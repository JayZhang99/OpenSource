Fix #6954 uTVM, fix when building the runtime for native hardware (#6957)* Fix #6954 which when building the runtime for native hardware failswith -march= is missing.This fix:1) adds support for march2) picks a senable setting for f746 discoveryThere is an interesting downside to this fix involving scheduling that likely needs discussion.In the microcontroller world we really should be setting ex: -march=armv7e-m depending on whatcortex-m is being used.-mcpu isn't as important when it comes to a command line compiler.Signed-off-by: Tom Gall <tom.gall@linaro.org>* Fix #6954 which when building the runtime for native hardware failswith -march= is missing.This fix:1) adds support for march2) picks a senible setting for f746 discoveryThere is an interesting downside to this fix involving scheduling that likely needs discussion.In the microcontroller world we really should be setting ex: -march=armv7e-m depending on whatcortex-m is being used.-mcpu isn't as important when it comes to a command line compiler.Signed-off-by: Tom Gall <tom.gall@linaro.org>	2
[REFACTOR][PY][API-Change] Polish tvm.runtime, tvm.runtime.module API update (#4837)* [REFACTOR][PY-API] Polish tvm.runtime, tvm.runtime.module API updateThis PR updates the tvm.runtime to use the new FFI style.- Remove top-level tvm.module to avoid confusion between runtime.Module and IRModule- API changes wrt to runtime.Module  - tvm.module.load -> tvm.runtime.load_module  - tvm.module.enabled -> tvm.runtime.enabled  - tvm.module.system_lib -> tvm.runtime.system_lib- Remove dep on api_internal from runtime.* Update module.load in the latest API	3
[PROFILER] Fix percent compute bound calculation (#11542)* [PROFILER] Fix percent compute bound calculationSomehow the runtime was dropped from the percent compute boundcalculation. Tolerances on the test we bumped a little bit higher to tryand catch mistakes like this in the future.* forgot print	1
[skip ci][ci][AutoScheduler] Disable flaky test_mutate_parallel test (#11441)See #11440Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[COMMUNITY] @tkonolige -> Committer (#11626)	3
[AUTOTVM][TOPI] Use tunable templates for GPU (CUDA/OpenCL/ROCm/Mali) (#1638)	1
[TVMC] Add composite target passes for compilation and tuning (#7304)* Extend --target syntax to cover multiple targets for compilation and tuning * Add a new composite_target module to implement custom codegen passes into TVMC * Provide implementation to integrate TVMC, to target Arm Ethos-N NPU and   Compute Library for the Arm Architecture (ACL)Change-Id: Iaee53fe22f0c14eb4e4c8ec47e72bade0c5e32cc	4
[Texture] Add memory scope entity into graph JSON/runtime (#11875)This PR is a split part of origin PR #11357Co-authored-by: Chris Sullivan <csullivan@octoml.ai>	5
Move Ops in relay.op.contrib.* (#4942)* move contrib* lint* address comment* address comment	1
[TOPI][CUDA] schedule for group_conv2d (#3663)* [TOPI][CUDA] schedule for group_conv2d* Fix #flops	0
[AutoTVM][TOPI] AutoTVM incorrect measurement (#5511)* [AutoTVM][TOPI] AutoTVM incorrect measurement* create new placeholder with converted layout* update _schedule_winograd	5
[TOPI] Fix reduce behavior to be consistent to numpy (#1738)[TOPI] Fix reduce behavior to be consistent with numpy	0
Fix typos (#2367)	2
[TIR][USMP] adding the pass to convert to pool offsets (#9418)* [TIR][USMP] adding the pass to convert to pool offsetsThis commit adds a transform pass that consumesthe planned pool allocations using memory planning algorithmthat convertes them to pool offsets.* adds two test cases for a linear structure with two pools* adds test case with a single pool for residual structuresChange-Id: I9d31e854461b5c21df72d1452120d286b96791c0* [TIR][USMP] adding the pass to convert to pool offsets* Adding a toggle to produce TIR that is TVMScript printable for unittesting* Fixing the unit tests* Ensure deterministic pool variable ordering.Change-Id: I317675df03327b0ebbf4ca074255384e63f07cd6* [TIR][USMP] adding the pass to convert to pool offsetsFixing the references after changes in the memory planningalgorithm.Change-Id: Id7c22356fd5de43d10a2b4fc70e978af2c6d599d* [TIR][USMP] adding the pass to convert to pool offsets* fixing the lintChange-Id: I7ff920b92d14a9919c930a4b35a2169c77a57dd1* [TIR][USMP] adding the pass to convert to pool offsets* removing unnecessary defitinitions* remove global var map* adding explaination for let bindings to pointer typeChange-Id: I31bd1a9f3057ee7f06252263565b0f75c51e6d13* [TIR][USMP] adding the pass to convert to pool offsets* rebase changes* making imports absolute* fixing typos and removing unnecesary linesChange-Id: I4c94b9955b001513fecb39ca94f81b1ad99c7bfc* [TIR][USMP] adding the pass to convert to pool offsets* fixing typosChange-Id: I42c557fd394aefdf8c2e825c4e88770eb0732f9b	4
[TIR] Utility function to decide loop mapping for auto tensorization (#11050)* [TIR] Add TensorizeInfo and GetTensorizeLoopMapping* expose PreOrderVisit to python* add test case* add conv2d nchwc test* add mma test* add arm nhwc conv2d test* Revert "add arm nhwc conv2d test"This reverts commit eb147f33bb02d62a0eacc9cdfe777ac047ee1bc9.* refine* add doc* update* fixd condition* black* pylint* Update python/tvm/tir/schedule/analysis.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* run black* bring back logic in original code to support loop permutation* add comment* simplify* minor fix to testCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[LANG/PASS] Support Vectorize (#37)	1
[Torch] Upsampling op support and enable registering a user defined op conversion map (#4961)* add custom conversion map* add roi align test using custom convert map* refactor test* add support for upsampling op and test on segmentation models* remove redundant no_grad* add upsampling test case* make the default custom map None, instead of empty dict* updated tests, remove packaging and drop PT 1.2 support* add better support for aten::to and tests* add a note on dilation in x86	1
Non_maximum_suppression and get_valid_counts add new parameters (#3335)	2
Remove unnecessary std::cout (#6072)* Remove unnecessary std::cout* Trigger CI	4
[AutoScheduler] Add sampling to dispatcher (#7376)* [AutoScheduler] Add sampling to dispatcher* address comment* make measurment configurable	5
[runtime][Hexagon] AOTExecutor implementation for C Codegen (#10311)* Hexagon AOT tests work* fix and address comments	1
[Arith] ExtendedEuclidean merge impl to int_operator (#5625)	1
[COMMUNITY] Yaxing Cai -> Reviewer (#12683)Please join me in welcoming Yaxing Cai (@cyx-6) as a new reviewer in TVM. Yaxing has brought the PackedFunc into TVM object system ([RFC-051](https://github.com/apache/tvm-rfcs/pull/51)), designed and implemented the new parser infrastructure for TVMScript and meta-programming ([RFC-079](https://github.com/apache/tvm-rfcs/pull/79))- [Commits History](https://github.com/apache/tvm/commits?author=cyx-6)- [Code Review](https://github.com/apache/tvm/pulls?q=reviewed-by%3Acyx-6+)	5
[DOCS] Change some tutorial text (#6514)	4
[CI] Shard Qemu python tests (#12258)This PR shards the QEMU python testsFixes #12180	3
[microNPU] Add unary elementwise operator infrastructure with ABS (#9530)* [microNPU] Add unary elementwise operator infrastructure with ABS* Added unary elementwise ABS legalization support and tests* Added unary_elementwise Relay to TIR lowering and tests* Added TIR to Vela translation and tests* Added codegen testsCo-authored-by: Rishabh Jain <rishabh.jain2@arm.com>	3
Cleanup comments (#6951)	4
[TOPI]Support dim-0 tensor in topi broadcast/reduce (#731)* support dim-0 tensor in topi opsrevert transform* revert	4
[ci] Remove `Prepare` step (#11082)This step doesn't do much on its own and triggers another `CPU` node allocation. This PR combines it into the `Sanity Check` step and renames that to `Lint`Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[COMMUNITY] anijain2305 -> reviewer (#4036)	3
[BugFix][TIR] Error check: Inline Block with Init Stmt (#11033)Should fix #10900	0
rpi4b target (#4445)	1
Fix default pytorch divide behaviour (#10727)Co-authored-by: Aleks Knezevic <aknezevic@tenstorrent.com>	0
[AutoTVM][RPCRunner] timeout is not passed correctly (#6924)* [AutoTVM][RPCRunner] timeout is not passed correctly* like @merrymercy suggests, scale timeout with (n_parallel + 1)* Apply suggestions from code review* Apply suggestions from code reviewCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	4
support dilation in conv2d (#439)	1
[Target] enable -arch=sm_xx for assigning cuda target arch and deprecate autotvm.measure.set_cuda_target_arch api (#9544)* [Target] enable -arch=sm_xx for assigning cuda target arch and deprecate autotvm.measure.set_cuda_target_arch apiSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Format] fix format error in CISigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] add warnings to target.cuda and fix errors in ciSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] fix docstringSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] amend warning conditionSigned-off-by: ZQPei <ziqiangpei@foxmail.com>	2
[BYOC][TRT] Allocate GPU data buffers and transfer data when needed (#6872)* Allocate data buffers for gpufix* Rename AllocateDeviceBuffer, update docstrings* Remove unneeded cast	4
Propagate tvm target through graph tuning setup (#9248)* Propagate tvm target through graph tuning setup* Don't append -device if it is already present in tvm_target* Make sure target string has device tracing only* revert accidental reformat* Update per review comments* fix lint issue* Use string for tvm target* Update python/tvm/autotvm/graph_tuner/utils/traverse_graph.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Cleanup testsCo-authored-by: Cody Yu <comaniac0422@gmail.com>	3
[Relay] IndexedGraph improvements in preparation for Collage (#11481)* [Relay] Odd's 'n ends changes to help Collage. - Complete the implementation of WithFields.   (Unfortunately they appear to be without unit tests and I continue this tradition...) - InferTypeExpr for InferTypeLocal but return the expression rather than the type. - Remove python binding of InlineComposites since C++ impl was removed some time ago. - Make IndexedGraph<Expr/DFPattern> more robust as stand-alone datastructure, and avoid unnecessary copies.   This will become a fundamental datastructure in Collage rather than just a helper for DFPatternMatcher. - Extend IndexedGraph with a notion of 'basic block' on every dataflow node. Needed by Collage to   avoid impossible partitions.* - Revert non IndexedGraph changes.* - Stick to 'Indexed graph' terminology- More tests* - Stick to 'Indexed graph' terminology- More tests* - Remove silly unit test	3
[DOCS] Introduce how to add hardware backend to FAQ (#4898)	1
Add ekalda to reviewers. (#11061)	1
[LLVM/CG] Sort PrimFuncs when creating LLVM module (#8958)* [LLVM/CG] Sort PrimFuncs when creating LLVM modulePrimFuncs are stored in a map where the order of iteration is notdeterministic. This can cause a different llvm::Module to be createdeach time, which can defeat debugging tools like -opt-bisect-limit.Add function CodeGenLLVM::AddFunctionsOrdered that takes a range ofPrimFuncs or objects convertible to PrimFuncs, and adds them to theLLVM module in a deterministic order.* Empty commit to restart build* Add testcase	3
[CI] Pin numpy version in image build (#10611)Tensorflow `2.4.2` expects the version of numpy to be `~=1.19.5`,however pip installing numpy will attempt to install `1.21.5` bydefault. Therefore, pinning the version of numpy when building thedocker image to be `~=1.19.5` until the version of Tensorflow isupgraded.Change-Id: Ia44e183afb660cac67fc4274ff70b23d28fc3e3e	4
[Frontend][TensorFlow] Improve TensorFlow control flow nodes ordering (#6387)* Improve TensorFlow control flow nodes ordering* Fix Lint	0
`tvm` crate stage 3 of Rust refactor  (#5769)* Adapt to new macro* Add tvm crate* Fix out of tree pass with new bindings* Super slick API working* Add examples* Delay egg example and add ASF headers* Move array.rs around* Remove outdated tests will restore in CI PR* Fix some memory issues* Fix ref counting issue* Formatting and cleanup* Remove out-of-tree for now* Remove out-of-tree	4
[TIR] Add structural error printing for TensorIR (#9306)* add structural error printing* remove old code* address comments* address comments* add test* fix test case* fix nested loop* rm print* change simple loop cond* address comments* fix test* address comments* remove msg* add override* address comments* address comments	1
[microNPU] Flatten after allocates have been removed in HoistAllocates pass (#10890)Fixes a small issue which caused SeqStmts to get left behind in thebody of the inner allocate after hoisting the allocates. Flatteningnow happens after the mutation has happened, which consequently helpssimplify the pass.The `test_outer_seq_stmt` test case already had this behavior, so a newcheck has been added to catch this case.Change-Id: Ia9e8a12088bc87dbf931c2535b648c49e676ea20	5
[VTA][Chisel] scale dram base address in hardware instead of runtime (#3772)* [VTA][Chisel] scale dram base address in hardware instead of runtime* remove trailing spaces	4
#1592 [PASS] Fix missing mem CHECK in storage_rewrite (#1616)	0
[CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo (#11013)* [CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo- Downloads a quantized (int8) person detection model- Uses tvmc to compile the model for Cortex(R)-M55 CPU and CMSIS-NN- Downloads an image to run the model on- Creates a C header file inputs.c containing the image data as a C array- Builds the demo application- Runs the demo application on the FVP- Application reports whether a person was detected e.g. "Person detected"Change-Id: If58d02ed0c4d2a85c0100398f65e6915a86f6546* [CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo- Downloads a quantized (int8) person detection model- Uses tvmc to compile the model for Cortex(R)-M55 CPU and CMSIS-NN- Downloads an image to run the model on- Creates a C header file inputs.c containing the image data as a C array- Builds the demo application- Runs the demo application on the FVP- Application reports whether a person was detected e.g. "Person detected"Change-Id: Ic20ceed80bc6e48d5c96ff0d5ca6c85e7f19174b	4
[ci][docker] Update GPU image (#12265)This includes a fix to `tlcpack-sphinx-addon` to fix broken card linksCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][AlterOp] Improving support for broadcast layout alteration. (#4040)	1
[Hexagon] Add test for registered schedules (#11016)* add hexagon schedule tests* moved tests to sub-directories	3
Added MaybeAlign to CreateAtomicRMW calls to fix build for LLVM13 (#7617)	0
Remove unused variable in topi cpp test (#8549)* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Remove unused variable in topi cpp test	3
[Hexagon] Initial support for Hexagon codegen (#6261)* [Hexagon] Initial support for Hexagon codegenThis commit does not support parallel execution or prefetch.LLVM 7 or later is required.* Set native_vector_bits_ based on target features* Initialize hvx_bytes* Remove commented out line	4
[TOPI] Bitserial low-precision convolution (#1332)	5
[FIX,AUTOSCHEDULER] Fix auto_scheduler to run with multiprocessing's spawn start method (#6671)* Fix multiprocessing with spawn issues* address reviewer feedback* Fix tutorials* formatting* undo autotvm work* Undo tutorial changes* Add spawn tests* fix test	3
[TIR][Hybrid] Hybrid Script Improvement (#6507)* [TIR][Hybrid] update* [TIR][Hybrid] python formatting	5
[TOPI] Fix x86 schedule for conv out_dtype (#1072)	0
[Bugfix][VTA] PkgConfig cause crash in PYNQ board due to link library (#3257)* [Bugfix][VTA] PkgConfig cause crash in PYNQ board due to link librarynot exist.Symptom:When run vta_get_started.py with pynq board, host crash andcomplain "cannot find -lsds_lib" and "cannot find -l:libdma.so"Reproduce:At pynq board, delete the ./build/vta_config.json, then run rpcserver.In host machine run vta_get_started.py, issue would reproduce.Analysis:This issue caused by 'PkgConfig' function  still using pynq2.1library which not exist in pynq2.4 anymore, when a "reconfig_runtime"logic of rpc_server.py get triggered , the compile would failed due tolink library not exist.Solution:change the link library to libcma.so.* [Document Change][VTA] Change pynq version from 2.3 into 2.4.Issue:pynq 2.3 image not available anymore from pynq download page and pynq2.4 is the current latest image which available in the said website, afterverification, currently VTA work good with pynq 2.4 image, hence updaterelated document from pynq 2.3 to 2.4.	2
Fix Xcode 10 metal compile error (#2836)	0
[community] @areusch -> PMC (#9604)	3
[nnpack] Preallocate workspace buffer (#2369)	1
Force CMake targets in top-level Makefile to run (#8840)This is a bug I introduced in https://github.com/apache/tvm/pull/8809, because the built binary is now named `build/cpptest` when `make` checks that artifact it finds it exists already and skips running `make -C build cpptest`. This ensures all nested `make` calls are forced to run from the top-level `Makefile`.	2
[LLVM/CPU] Terminate basic block after "ret" instruction (#6036)* [LLVM/CPU] Terminate basic block after "ret" instruction"Ret" is a terminator in LLVM IR and there should be no instructionsin the basic block following it. When generating a "ret", end thecurrent block and start a new one.	1
Remove sccache from Rust install (#3728)	4
Fix -Wreturn-std-move and -Wself-assign-overloaded (#2669)	4
[Topi] Breakdown topi.cc into smaller files (#5253)* [Topi] Breakdown topi.cc into smaller files* add missing file	2
lint: add opencl .cl file type (#6092)	2
[skip ci][ci][docker] Pin Pillow version (#11348)A recent release depends on some things we don't have installed, so don't use it.e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-11319/5/pipeline/Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[COMMUNITY] New committer -- @mbaret (#6873)	1
[TVM] Move check_numerical_grads to tvm.testing_ (#2314)	3
Turn on USE_SORT by default (#2916)	1
[TVMC] Add test for quantized pytorch model (#9467)As a follow up to #9417 and now that #9362 is resolved, this PR adds atest to check quantized pytorch mobilenetv2 is converted correctly.Change-Id: Iaf2d38ce71c008e0141a4a2536bd54c2c9f3fe3d	4
fix RPC waiting for device (#10255)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>	1
Automatically close open files to prevent ResourceWarning (#10526)Python will emit a `ResourceWarning: unclosed file` if filesare not properly closed. This removes this warning by enclosingfile-handling blocks in `with open...` that will automaticallyclose open files upon leaving the block.	2
[BUILD] Fix LLVM static/dynamic link issue (#1461)This patch prevents libtvm.so from both containing static parts of LLVMand link with libLLVM.so by removing the latter from link list.	2
[MetaSchedule] Keep Task / Trial / Iter / Postproc Number Consistent in Log (#10478)This PR fixes some inconsistency in log printing and make sure all numbers start from zero for tasks, trials, iters and postprocs. I think it's better for debugging if any task or trail went wrong in the future.	0
typo: Xlinx => Xilinx (#2283)typo: Xlinx => Xilinx	2
Expose workspace size in tvmgen_default.h (#9510)This PR exposes the workspace size as a macro TVMGEN_DEFAULT_WORKSPACE_SIZE in tvmgen_default.h (or TVMGEN_<MODEL_NAME>_WORKSPACE_SIZE in tvmgen_<model_name>.h in the case that the model name is not default).This functionality is useful for microTVM/AOT use cases where it's useful to know the workspace size at compile time.	1
Add smmla/ummla support in quantized Conv2d (#6802)* Add smmla/ummla support in quantized Conv2dThis introduces support for `smmla`/`ummla` instructions in TVM:- Added `is_mmla_available` function in `arm_utils.py`- Added the tiling node + tensorization schedule in `conv2d_gemm.py`- Added the intrinsic support in `tensor_intrin.py`- Added the test-case in `test_topi_conv2d_int8.py`Change-Id: Iff48c77f16fe1e64ecb733da965a879651ce635f* Address review comments and test failures* Fix linting* Rebasing	0
[Frontend] [Tensorflow2] Added test infrastructure for TF2 frozen models (#8074)* added test infrastructure for frozen TF2 models* linting with black* removing some comments* change in comment in sequential test* addressed the comments* refactored to place vmobj_to_list in a common file* Added helper function in python/tvm/relay/testing/tf.pyCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Refactor tf according to CI errorCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Added docstringCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* removing printCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>	4
[RPC] default use spawn for fork safety (#1240)	1
[Bugfix] Fix winograd nnpack fp16 (#3046)	0
Improve numerical gradient check (#3856)	1
[Relay][Topi][AutoTVM] Winograd support for Conv3D (#5186)* Functional conv3d winograd working.* Formatted python code.* registered conv3d winograd compute and started adding relay without_weight_transform operator.* Add topi testing for conv3d winograd.* Format file.* small tweak to unrolling to prevent build sticking.* Refactoring convolution ops in relay.* Refactored relay convolutions.* Bug fixes.* Fixed static bug in convolution.* Added conv3d alter op layout and related support.* Bug fixes and testing done.* Fix a few autotvm bugs.* Drop silly debug print.* Removed debug_skip_region.* Add variant of conv3d_winograd that doesn't transform depth.* initial infrastructure done for depthless conv.* Fix no_depth schedule bugs.* automatic topi switching between depth and depthless winograd.* Fixed bug in schedule.* lint fixes.* Removed indents in convolution.cc* missed a few indents oops.* fixed flop count.* One more small tweak.* Change kernel pack inner axes order.* Style changes.* Comment fixes.	0
[TIR][Schedule] Transform layout quality of life (#11269)* [TIR][Schedule] Added Schedule.transform_layout_sugared* [TE][TIR] Reduced duplication in TE/TIR layout transformationsPreviously, the implementations of `tir.IndexMap.from_func` and`te.Stage.transform_layout` had significant duplication to handleargument parsing.  This commit extracts the shared logic into`tir.IndexMap`.* Enabled *args in Schedule.transform_layout_sugared* Fix lint error* Allow Schedule.transform_layout_sugared to set axis separators* Merged transform_layout_sugared functionality into transform_layout* Fix lint errors* Fix lint error* Fixed docstring errors* Updated/tested TransformatLayoutTraits::UnpackedAsPython* Disabled exec-used check for running trace.as_python()* Updated SetAxisSeparatorTraits::UnpackedAsPython* Updated unit test that was added in merge commit* Fixed the argument name for TensorizeTraitsThis wasn't checked before, but was the only other issue caught by theupdates to verify_trace_roundtrip.* Re-enable type checks of transform_layout/set_axis_separatorDisabled while waiting for https://github.com/apache/tvm/pull/11289,which was required for the `Tuple` argument.* Updated a few additional transform_layout usages from main	1
[DOCS] Fix the docker binary cache location (#6390)	2
[COMPILER] Upgrade to meet latest TVM IR pragma convention (#32)	3
enhance cache write to support multiple tensors generated by ONE computeOp (#1042)	1
[TVMC][FIX] Compiler supports input with a slash (#8481)	1
[x86 schedule] Fallback schedule for Int8 depthwise. (#4733)	5
[MetaSchedule] Add MultiLevelTilingTensorCore rule for auto-tensorization on CUDA (#12059)* [MetaSchedule] Add MultiLevelTilingTensorCore rule for auto-tensorization on CUDACo-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>* address comments* update intrin registrations* fix tests* address comments* add warning when storage align doesn't work* remove printCo-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>	1
Update contribute.md	5
[COMMUNITY] mikepapadim -> Reviewer (#11276)	3
[AUTO_SCHEDULER] Add feature extraction directly from PrimFunc (#10455)* [AUTO_SCHEDULER] Add feature extraction directly from PrimFuncAllow users to directly extract features from a PrimFunc. Extractedfeatures can be used to get an estimate of flops, memory load size, orarithmetic intensity from a PrimFunc.Also fix feature extraction to correctly measure the number ofarithmetic operations width vector datatypes.* fix param name* log scale in cc instead of python* rename functions, remove load/store* forgot rename in tests* forgot to commit rename	3
[AOT][Stack Allocator] Fix Initial Memory Misalignment (#8487)* add flag* fix and test* format* fix memory memory_align function* fix and address comments* format* fix crt aot test* comments* fix test* trigger* trigger* trigger* trigger* triggerCo-authored-by: Mehrdad Hessar <mhessar@ip-172-31-20-199.us-west-2.compute.internal>	3
Rename tvm.hybrid.script to tvm.script. (#6522)	5
[RUNTIME] Support nop (#913)	1
[RELAY]sch & comp for ops in nn.py (#2092)	5
[Relay/Topi][Op] Conv1D (#4639)* added conv1d operators to topi.* Started to add python testing.* Added python conv1d implementation for testing.* Wrote test but need to add cuda schedule :(* Cuda schedules working for both conv1d layouts.* All topi tests passing.* Formatting topi.* Removed pad_method option as its probably overkill.* Added relay op definition of conv1d.* End2end conv1d working with onnx.* Lint fixes.* Formatting fixes.* Rebase fix.* Switched to array based attributes for consistency across convs.* Improved onnx parsing and testing for convolutions.* lint fix* Tiny tweak.* Bug fix* Rebase fix.* Add group ignore to onnx conv1d frontend.* Unified MakeConv and fixed documentation.* improved autopadding* Addressed feedback and simplified onnx frontend.* Format fix.* Basic X86 NCW schedule working.* Added nwc schedule.* fixed name* Added more tests and basic x86 schedules.* Format fix.* Added non power of two shape tests.	3
Sync with upstream and use CreateDSOLibraryObject. (#9376)	1
Fix error in fuse_ops.cc (#2098)	1
[DOCS] Add docs of logical snd right shift (#1834)	2
Fix bias_add gradient (#4516)* Fix bias_add gradientA change caused collapse_sum_like to reject implicit dimensionbroadcasting for bias_add gradient, so switch to explicit sum reductionon the non-bias axis dimensions.* Lint fix	0
delete init part when keeping trivial loop (#1031)	5
[Hexagon] Support both 1-d and 2-d VTCM allocations (#10846)* [Hexagon] Support both 1-d and 2-d VTCM allocationsPreviously, all VTCM allocations were assumed to be 2-d buffers.  Thiscommit extends `HexagonDeviceAPIv2::AllocVtcmWorkspace` to allow both1-d and 2-d VTCM allocations.  Matching the semantics used in`CodeGenHexagon::CreateBufferPtr`, allocation of 1-d buffers returns a`void*`, and allocation of 2-d buffers returns a `void**`.Co-authored-by: Adam Straw <astraw@octoml.ai>* [Hexagon] Distinguish between 1-d buffer and single-alloc 2-d bufferPreviously, HexagonBuffer represented 1-d buffers as 2-d buffers with`nallocs==1`.  Since this is used to determine the return type of thedata pointer exposed to the generated code, the ambiguity between`shape=[N]` and `shape=[1,N]` must be avoided.  This commit replaces`HexagonBuffer::nallocs_` with `HexagonBuffer::ndim_`, avoiding thisambiguity.* [Hexagon] Treat "global" scope allocations as 1-dThis updates `HexagonDeviceAPIv2::AllocDataSpace` to follow thesemantics of `DeviceAPI::AllocDataSpace`, to avoid breaking callerassumptions in `tvm.nd.array` or graph_executor/aot allocation.* Updated C++ unit tests for HexagonBuffer* Remove commented GetNumAllocs and unused GetBufferDimensionCo-authored-by: Adam Straw <astraw@octoml.ai>	5
Revert "upgrade ci lint docker file (#11734)" (#11787)This reverts commit 7bfbc74c65684d1e25e235335da41c94372a561a, as itgenerates near 500 code violations when PyLint was updated from 2.4.4 to2.9.3.Issue #11785 for details.	0
minor style edit	2
TRT Dynamic Reshape Fix (#7412)* Dynamic Reshape* Changes* Add test cases* Add test cases* PR COmments* CI Error* EmptyCommitCIErrorCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>	0
[QNN] Convolution 2D Implementation. (#3580)Rebasing. Empty commit.Clang-format styling.	5
Fix vmlal.s16 code generation for int8 x int8 -> int32 (#2748)	0
add converter for MXNet slice in nnvm and relay (#2662)	1
[Hexagon] Register basic strategies and schedules for common operators (#10919)These are just placeholders to enable building full models.	0
[ci] De-duplicate retry functions (#12325)	1
Fix tutorial to follow the conv2d change (#1390)	4
[Target] Introduce Target Id Registry (#5838)	1
[MetaSchedule][M4a] Mutator: Mutate Parallel (#10096)	5
[Frontend][Paddle] Fix op in paddle did't transmit layout information (#12658)[Frontend][Paddle] Fix adaptive_avg_pool2d in paddle did't transmit layout information	5
[CODEGEN][OPENCL] Explicitly cast min/max operands (#5090)* [CODEGEN][OPENCL] Explicitly cast min/max operands* retrigger CI	5
Fix LLVM initialization again (#2399)	5
[Relay][Quantize] Use fixed point mulplications (#4160)	0
Minor addition to graph runtime debug (#3129)* print op names in graph runtime debug* fix lint	0
[PASS] Use likely tag & enable LoopPartition by default (#132)* [PASS] Use likely tag & enable LoopPartition by default* [PASS] Support thread_axis partition* Take IfThenElse branch method* [PASS] Insert branch at the innermost thread scope* [PASS] Select candidates before trying to partition & add test for select* [PASS] Clean code* Fix* Remove print & assert vectorize happens	3
Add missing annotation for requires_gpu in test_topi_dense.py Requires GPU (#8387)	1
[DOC] Improve Pattern Language Docs (#5676)* [DOC] Improve Pattern Language Docs* address comments* address comments	1
[DOC] MXNet frontend tutorial (#2688)	2
[Lint] Fix Pylint Issues (#10358)	0
[RELAY][PASS] add a relay pass to count #macs of a model (#2609)* add a relay pass to count #macs of a model* remove unnecessary includes* fix end-of-file issues* address review comments* remove test resnet* address more review comments* use data layout string to locate the input channel* fix bug in conv 2d output tensor checking* C must exist	0
[Relay] add some check for the ad algorithm (#3585)* do* fix test	3
[COMMUNITY] @hzfan -> reviewer (#7360)	3
[Relay][QNN] QNNtoRelay & QNNLegalize Pass utility using Relay Legalize API. (#3838)	1
[SPIRV] Declare int64 capability by default (#7681)	5
[FIX] Bug fix for batch_matmul parameters mismatch (#8785)	2
[RELEASE] Update NEWS.md for v0.7 (#6613)	1
Initial commit	5
Fix `std::locale("")` in profiling.cc (#11846)std::locale("") would failed  in some env like: LC_ALL=zh_CN.UTF-8 `tvm._ffi.base.TVMError: locale: :facet::_S_create_c_locale name not valid>`locale has its default constructor function, no need to set input to empty string `""`	1
[CUTLASS] Initial conv2d support (#9595)* Add initial conv generator* added conv2d pattern* profile by gemm profiler* remove conv2d profiler for now* remove unused code* add default* minor fix, profiling working* start codegen* generated code compiled* fixed layout initialization* matched with autotvm tensorcore result* test refactor* minor cleanup* remove iteration algo "Analytic"* add test for dynamic batch conv2d* pass dl tensor as output too* support conv2d dynamic shape in codegen* test working* lint* simplify codegen* fix weird formatting* typo fix* check if cutlass is enabled in the test* simplify gen_conv2d.py	3
[microNPU] Determine block configs using the cascader (#10695)The cascader needs to be able to choose the block config for operations in order to accurately model their performance. The cascader must attach the chosen block config to the te.Schedule. This is done using a pragma. The chosen block configis also added to the TIR spec. If the cascader hasn't set a block config, it defaults to the existing block configselection behaviour.Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>	5
support using pointer with an original offset (#826)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.* Re-organize the tensorize test cases, and add a new case for none-resetmode.* Fix a typo.* Delete the unit case because merged it into test_schedule_tensorize.py already.* always use new tensor in its stage when rewrite for cache read* revert previous changes to sync up with master* support using the ptr with an original offset* update test case and fix CI error	0
[TF][Op] Op where (#4045)* [TF][Op] Add TF op Where* improve tests* add tests for vm	3
Fix build for llvm newer than 9.0 (#4515)	1
[Relay][External Codegen] Support data types for CSourceModuleCodegen args and output (#4934)* Support int args and no extra buffers* Fixes* remove testing code* fix style* more style* use const args* styleCo-authored-by: Jon Soifer <jonso@microsoft.com>	1
[DOCS]fix typos in autotvm tutorial (#4585)	2
vgg16 workload error fixed (#598)	0
[VERSION] Update mainline version to 0.7.dev0 (#4720)	5
[Relay to Onnx][LRN] (#8323)* Added support for LRN operator* [Relay to Onnx]* Added unit test case for LRN* * reformatted* * reformatted (2)* * fixed formatting issues in relay to onnx conversion script* * fixed formatting* change single quotes to double* set space to 4 instead of 2* * reformatted onnx.py: corrected spaces* [Relay to Onnx] LRN* Assert if axis != 1* * fixed formattingCo-authored-by: zxy844288792 <zhoxingy@amazon.com>	0
Don't multiply by constant 1 uselessly in dense (#5911)	1
[Frontend] Unified LSTM cell (#8599)* fuse dence sum* remove excess copying* dev LSTM in ONNX* alternative implementation of LSTM in onnx frontend. It is quicker than current one without tuning* LSTM_dev2 was implemented in onnx frontend* LSTM dev in pytorch frontend* LSTM cell implementation was transferred to common place. Unneccessary code was removed* lint fixes* Weights permutation for LSTM layer in onnx frontend* LSTM cell description was added* arguments and values were renamed. descriptions of some methods were added* LSTM output shape and actvations input format were fixed in onnx frontend* empty. tvm-ci test* unbind method was transferred from onnx frontend to common.py* unbind method was transferred from pytorch frontend to common.py* lstm cell was transferred from op/layers.py to frontend/common.py* clean up weight dictionary initialization* fix pytorch frontend wrapper over unbind method* minor fix of comments* empty. tvm-ci test restart* empty. tvm-ci test restartCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
[TIR] VNNI and ARM dot product intrinsic for tensorization (#10925)	2
[Metaschedule] Auto tensorization for CPU / GPU dot product (#11088)* [Metaschedule] Auto-tensorization for CPU / GPU dot productCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* doc update* add vnni conv2d test* add dp4a test* adding tests for rewrite_tensorize* add rewrite_tensorize test* add missing pydoc* black* more doc* adding auto tensorize integration test* add dp4a test* fix target name* fix dtype in test* skip bert test* replace hard-coded llvm intrinsic id in test with look up* remove unnecessary include, add doc for the rest of params* update postproc.h* update doc* fix shape in te matmul workload* fix newline in cppdocCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[COMMUNITY] @yongwww-> reviewer (#3997)	3
[DOC] minor language use improvements (#3317)	1
API call to get symbol output count (#270)* Symbol __getitem__ using list_outputs() is too expensive, when it only cares about the output count in most cases* Add cython cmake* GetNumOutputs() and __len__ changes per PR comments* set commit for tvm	1
[REFACTOR] collections->container, RPC returns func, time_evaluator r… (#244)* [REFACTOR] collections->container, RPC returns func, time_evaluator returns struct* fix executor	0
[FIX,AUTOTVM] More descriptive error message when an autotvm task is not (#6652)found.	0
Prevent host Vulkan SDK blocking cross-compilation (#7609)	5
[Relay][Op] Add instance norm op (#4004)* [Relay][Op] Add instance norm op* mend[Relay][Op] Add instance norm op	1
[Runtime][PipelineExecutor] Fix CPU affinity setting issue. (#10781)Found the CPU affinity setting not work in pipeline executor, thesymptom is that there is no perf change after doing cpu affinitychange. the reason is that only the ConfigRuntime stored the cpuaffinity setting but the BackendRuntime class not.	1
[AUTOSCHEDULER,FIX] Calculate arithmetic intensity without log scale (#12079)* [AUTOSCHEDULER,FIX] Calculate arithmetic intensity without log scaleIn autoscheduler's featurization, arithmetic intensity was incorrectlycalculated as log(FLOPs) / log(bytes). This change removes the logs soarithmetic intensity is FLOPs / bytes.* slog arith inten	2
[LLVM] Make changes needed for opaque pointers (#9138)* [LLVM] Make changes needed for opaque pointers- Pass value type to all Create.*Load and Create.*GEP functions.- Create type TypedPointer to keep both the address and the pointee's  type when buffer pointers etc. are created.- Eliminate calls to getPointerElementType, except one in creating  debug info (that seems necessary for the time being).* Fix typo in CodeGenCPU::CreateStructRefPtr* Fix type extraction in CodeGenLLVM::AddAliasInfo* Fix types in ramp-1 vector loads/stores* Fix getting intrinsic name in error message* Return valid pointer from PackClosureData when no data to pack	5
[FIX,RPC] Skip RPC tests when using multiprocessing's spawn method (#6858)The rpc tests are broken when running under pytest with multiprocessingusing spawn. I suspect this is because pytest tests each function in aseparate process and does not import the full module.	2
skip example json runtime test when config is not set (#4614)	1
[Apps] [howto_deploy] fix cxx-flags order and build directory (#2888)	0
[ci] Enable pylint for tests/python/ci (#11666)This fixes up the pylint issues as part of #11414 for the CI tests	3
fix doc warning (#4959)	2
[ONNX] Add Clip importer to handle when min/max are provided as inputs. (#6251)* [ONNX] Add Clip importer to handle when min/max areprovided as inputs.* Use relay.op.minimum/maximum to handle dynamic bounds for Clip.* Update test to new testing standard	3
[DOCS] Misc docs improvements (#5222)- Reduce CI docs task log size.- Update the relation to halide to the latest state.	3
[µTVM] Fix paths in the reference VM tutorial and add vbguest recommendation (#7015)* Add recommendation to install vbguest plugin.* Update directories to match checked-in.	5
[CI][DOCKER] Fix cuda11 nvidia-docker support for non-Tesla gpus (#8163)Starting cuda11, libcuda can be linked to a version of libcuda in/usr/local/cuda/compact. The particular linked librarydoes not work for non-Tesla GPUs, causing "no CUDA capable devices found"even though nvidia-smi shows available GPUs.This PR makes makes sure we always prioritize linking/usr/lib/x86_64-linux-gnu/libcuda.so.1so the nvidia docker cuda11 images works for non-Tesla GPU envs.	1
[REFACTOR][RELAY] move fallback_device to config (#5690)	5
[CODEGEN] Generate main compute function separately with alias info (#253)	5
Add dmlc-core to the list of installed header directories. (#4035)There are dependencies on dmlc-core in TVM public API headers(e.g. some headers include dmlc/logging.h) so it needs to be installedas part of TVM for TVM headers to be actually usable.	2
[Relay] Add expr_visitor, fix expr_functor exponential blowup problem (#2988)* save* lint	0
[Relay] Change Default "opt_level" of Sequantial from 2 to 0 (#8634)	4
[Relay] Fix dataflow_pattern.rewrite() hang if Match in IR (#5680)rewrite() quits only if graph stop changing, but ExprMutator  always creates new Match node. This patch fixes this.	0
[MetaSchedule] Post Processor: Rewrite Unbound Block (#10027)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
Add a missing header in cuda_device_api.cc (#3621)	1
[RFC][RUNTIME] Introduce new object protocol. (#4115)* [RUNTIME] Introduce new object protocol.This PR introduces a new object protocol to unify the node and object.We also updated the existing runtime::vm code to make use of the new system.Update to the node will be done in a follow up PR.Other changes:- Remove object related code in json serializer as that code logic was not complete  and we have a separate serializer for VM, can revisit later.* address review  comment* Fix the child slot logic	2
Add rocblas_sgemm_strided_batched impl. (#6579)	1
[Hexagon] Select qaic executable based on Ubuntu version (#10891)* [Hexagon] Select qaic executable based on Ubuntu versionAllow users to override the selection via QAIC_PATH_OVERRIDEenvironment variable, for example on non-Ubuntu systems thatcan still run Ubuntu binaries.* Address review comments- Remove repeated call to _check_path_exists.- Return qaic_path-NOTFOUND is qaic is not found.- Change SEND_ERROR to WARNING when qaic is not found, since not finding  of other properties is not an error.	0
[DOC] Argument name correction. (#1765)	2
[TFLite] CI recipe. (#2371)* [TFLite] CI recipe.* * Custom bake tflite package and install.	2
Fix eltwise alter op layout for broadcast axis (#11337)* Fix eltwise alter op layout for broadcast axis* Add tests on boradcast blocking over already blocked layout	3
[Relay][Op] Add group conv2d dispatch to topi function (#2870)* [Relay][Op] Add group conv2d dispatch to topi function* Rerun tests	3
[TOPI] Isolate padding option, improve decl of depthwise/conv2d/pool (#332)	1
Fix parsing of different exception string formats (#4785)	0
Update submodule dmlc-core (#1920)	5
Add ci_riscv image, update all to 20220810-060142-fae79bbc3. (#12369)	5
Fix MSVC build error with container.h (#4455)	0
[Relay][QNN] Support for non scalar zero points in qnn.conv2d (#8620)* conv2d working, fixing conv2d_depthwise* Depthwise conv2d working.* Make convinteger work on cuda.* Simplify code and add tests.* Formatting.* Fixed fallback broadcasting.* Fix fallback broadcasting.* Formatting.* Fix lint* Merge with new test parameterization.	2
[VERILOG] VPI Mem Interface/ VPI MMap (#73)* [VERILOG] VPI Mem Interface/ VPI MMap* fix test issues	0
use packed func macro for external codegen (#4710)	1
[NNVM][TENSORFLOW]Local Response Normalization added for tensorflow (#1522)	1
Enable the sparse schedule (#3651)	0
Support vector operations for AMD (llvm IR) (#623)* Support vector operations for AMD (llvm IR)* fix whitespace* update comments, docstring	2
[OpenCL] Fix vthread_extent for warp size 1 case (#10199)	0
Documentation Refactor (#9203)* Documentation Refactor - Stage 1RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 1 of the documentation refactor reorganizes the docs structure,moving files (without content changes) and adding new scaffolding togenerate the proper document tree.It does not address naming, style, content, links, or other existingcontent in documents that were moved. State 2 will address fixing theseissues with existing content.Major changes include but are not limited to:* Dividing the existing tutorials into two sections:  * Tutorials  * How Tos* Moving all of the existing tutorials out of the `/tutorial`  directory and into the more general `/gallery` directory.* Breaking up how-tos into individual sections for more  flexibility and more consistent rendering.* Moving content into new classifications:  * `/docs/arch` for architecture guides  * `/docs/reference` for API guides and other reference material  * `/docs/topic` for topic specific guides such as microTVM and VTA  * Restructuring `/docs/dev`* Adding a table of contents to the doc index* Adding instructions on how to install using third-party tlcpack* Documentation Refactor - Stage 2RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 2 of the documentation refactor fixes naming and linksin the documentation to be consistent with the overall structure.Major changes include:* an update to how to contribute to docs.* several updated index pages with title changes to match  the organization style and bring consistency to the sections* expanded descriptions of some page collections* fixed links* Documentation Refactor - Stage 3RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 3 of the documentation refactor adjusts CI for the new structure.The CI build script takes into account the new gallery format. Italso prevents deleting existing documents, and takes advantage of the`_staging` and `_build` directories to clean out previous builds.	4
remove duplicated cast op when lowering qnn.requantize op in float mode (#12234)	4
temp checkin of schedule	5
[Relay] fix format in ty.py (#1948)	0
Fix a lint issue. (#10245)lint.sh complain for an addtional space line in 'utils.cc', just fix it.	0
Fix Tile, add a few more test cases on bound inference	5
docker: Drop caffe2 download progess bars (#5359)Change-Id: Ia15c3c8f41f75423814e559f6fdb062098f19464	5
[AUTOTVM] Improve tutorial and logging (#1544)	2
[AOT] Initial implementation of --unpacked-api (#8023)* [AOT] Initial implementation of --no-typed-operatorsBased on the discussions in the AOT embedded improvements RFC, this adds a flag to the target which changes the internal operators to an unpacked API. The unpacked API spreads the input buffers across the operator function, for example:int32_t operator(void* arg0, void* arg1);As opposed to the traditional packed API:int32_t operator(void** args);Uneffected is the entrypoint function, which retains a packed API forcompatibility with other parts of TVM. This is done by changing thepasses taken by none entrypoint (CallingConv::kEntryPoint) functions.* Move entrypoint generation outside of main passesThis removes the logic for deciding the entrypoint from the compilerpasses and instead moves it into the metadata code generation. By movingthe generation, we can generate a variety of entrypoints on top of thecompiler output (such as the micro entrypoint discussed in the RFC).* Use buffers in make_unpacked_api tests* Enable --no-typed-operators for llvm* Change --no-typed-operators to --typed-operators=0 to match other options* Refactor typed-operators lookup into use_typed_operators_(Also contains minor clean up of output variables)* Rename --typed-operators to --unpacked-api(Also moves the entrypoint name to a constant)* Move all properties into init list to avoid double init* Remove AutoTVM breaking default and improve clarity	1
[PYTHON] Enable cython ndarray API (#113)	0
[PatternLang] Add If pattern (#7282)* Add if patterncommit 1ee052fd494a5bdd881c242c3ea0c95cf2a613e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 22:19:17 2020 +0900    add commentcommit c846a6999e9c9e48fbc019780e705a990f46cb22Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 21:14:20 2020 +0900    max_out_size rewrite added to the testcommit 2c7c7fbd0e6563aba694e7fb6baa7bda8e4fadcaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 20:57:55 2020 +0900    max_out_size rewrite workingcommit 319e930acb8162c1ec4a5d4fb71d134580a68f13Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 20:43:16 2020 +0900    refactor dyn strided slice patterncommit fb6917b703440748800bde624bc20efaf5798b8aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 11:21:33 2020 +0900    update NMS pattern following frontend changecommit 255a98f1da8f300d4fe417cce3587c0d71e38ed3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 24 05:19:31 2020 +0900    add some comment to explain the patterncommit 52cea1cc2bff533ca60acfc2416477fc8b058428Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 08:35:14 2020 +0900    revert tutorial changecommit d3e0e0d7e2427c40067d6ad2680ec5b3f0076223Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 08:02:29 2020 +0900    test fixed by setting force_surpress=Falsecommit 2fa1a574f932001be2d8f601338a342dab92f79cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 07:22:32 2020 +0900    fixed coord_startcommit 6ba88f27dec1bdb0b0ba746c268591a59264088eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 06:50:46 2020 +0900    add doccommit 8d386b6a1c92ce4fe3349ff20e320199a1b5b310Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:27:26 2020 +0900    updated tutorialcommit 3206b49ecfdd874e0ff8feb0fa586c4c4282f705Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:04:44 2020 +0900    update object detection test to add rewritecommit 74bebb2f4376aeb67d8c4aad395f9f2661fe6b3eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:02:15 2020 +0900    add a pattern to rewrite nms to batched nmscommit f410e6dde0ed949b90312c5a7ddbb6c234f9acc1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 22:20:16 2020 +0900    add commentcommit f1e078b0724bd22e7be0a812055e1c7c650d94daAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 19:54:22 2020 +0900    Add if pattern* add doc* add test* doc formatting* cpplint fix	0
[TFLite] Added check for dynamic range quantization (#7114)* [TFLite] Added check for dynamic range quantizationAdded check to prevent optimized with "dynamic range quantization"tflite files to be loaded as the optimization is not fully supported.https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization* linter* linter* unit test fix	0
[RELAY] Partition graph codestyle fixes (#5202)* [RELAY] Codestyle fixes for Graph Partitioner*ran through clang-format* *formatting comments* *further codestyle changes (after clang-format)	4
Make the behavior of data nullptr check of pooling layer same as others. (#3322)	5
[CPP_RPC] allow user supplied work dir (#7670)* [CPP_RPC] allow user supplied work dir* clang format	1
[autotvm] fix typos in comment (#4591)	2
update include regex to work for path with symbol (#1354)change regex to match all symbol except for space.	4
[Relay][TopHub] Add switch to disable TopHub download (#4015)	1
Make docs build again when not everything is enabled (#6386)	0
[TIR][Schedule] Transform layout (#10538)* [TIR][Schedule] Transform layout* address commens* fix* doc* Address comments* remove unused* Use BufferIndexType enum* lint* support *args* lint* lint	1
[PASS]Treat Halide call_type as pure expression (#2404)	4
Tutorial: Use Python 3 (#3498)	1
[COMMUNITY] @wpan11nv -> Reviewer (#5790)	3
[CI] Use Python 3.6 variant of pypa.io (#10114)Using pypa.io directly now results in:```ERROR: This script does not work on Python 3.6 The minimum supported Python version is 3.7. Please use https://bootstrap.pypa.io/pip/3.6/get-pip.py instead.The command '/bin/sh -c bash /install/ubuntu1804_install_python.sh' returned a non-zero code: 1```Also see: https://github.com/apache/tvm/issues/9703	0
Unpack NMS inputs into bbox, scores and class ids (#7257)commit fe8fda81774c2e1a4d434179f62e3a299e084cb7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 30 20:31:29 2020 +0900    fix write by a single threadcommit 0c21e36d58f81adeedec1749aeb04ed4e93a7f36Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 04:32:18 2020 +0900    minor improvement when topk is availablecommit 68c686617c818a81f31c6696c99c5dae68405becAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 04:10:24 2020 +0900    finish concat outputcommit 37d7a198010a7bfef85158bbc22b6673e43b2973Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 03:59:28 2020 +0900    fixed topk handlingcommit 1913f9764dc5987deb2c6228112c18b98533831cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:34:24 2020 +0900    more refactoringcommit 70c65f099da7cf8a18ffbaadadbd6dc814a804feAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:27:15 2020 +0900    unpack input datacommit 3a273975b1456991fd3f70e055cd5f7c2cdd79feAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:22:16 2020 +0900    slight change to initializationcommit 9b42008b42004f5f05cdaa51e2f6feeadf99abb1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:50:36 2020 +0900    add some comments, remove check the check on negative class idcommit 0aa375d67ad14cae8431958e17d1901dd94d1f6bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:39:49 2020 +0900    leave a TODO on write by only one threadcommit d75ee0a62b8e2fb8912ff226ea8bedb8ed78764dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:13:04 2020 +0900    temp disable write by only thread 0commit 20b563031adf56f93a7bcfe5b853c477175f4f80Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 10:06:43 2020 +0900    use one block two avoid global sync issuecommit dd1e23068f6fdadc5cb3c3a1872c3fff42f4e2eaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 07:59:19 2020 +0900    make NMS inner loop parallelfix write by a single thread	0
[TOPI]Add where operator (#1416)	1
[LINT] Improve robustness in task_lint.sh logic (#3315)The existing RAT ASF license auditing logic ignores any failure in theshell pipeline rather than just the exit code of the final grep.Adjust the logic such that failure of the various tools in thepipeline are not elided away.	0
[MXNET]broadcast and logical op support (#5461)* [MXNET]broadcast and logical op support* Review comment fixed	0
[FRONTEND][ONNX] fixed operator converter for Split in onnx frontend (#2038)	1
[ci] Skip failing tests in wheel (#11705)Some python tests are failing in the wheel. This PR skips them if the environment variable `WHEEL_TEST` is set.This PR is related to https://github.com/tlc-pack/tlcpack/pull/115.	1
disable stacked bidir test (#6585)Co-authored-by: masa <masa@pop-os.localdomain>	3
[OpenCL Textures] Fix memory management in texture pool (#10938)Previously, the size of the memory which should be allocated wascalculated as multiplication width on height. It doesn't work well incase when one texture has big size in height and the next one big sizein width. We tried to reuse the allocated memory and every time whenthe next texture with big size was used we reallocated the previousone. It has huge impact on the performance.Now we check two dimensions independently. So, in this case we willcheck both dimensions and it helps us to avoid the situation withcyclic memory reallocation.	1
Enable QNN primitives for DNNL runtime (#11642)* [DNNL] Enable QNN primitivesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [DNNL] add qnn testSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* typo fixSigned-off-by: Alexander Peskov <peskovnn@gmail.com>	0
fix memory leak (#4811)	0
[Bugfix][AutoScheduler] Fail to register ComputeDAG when deserializing tasks (#7395)* [Bugfix][AutoScheduler] Fail to register ComputeDAG when deserialize tasks* fix test* trigger ci	3
[Topi, x86] Using MKL blas for quantized dense (#6115)* [Topi, x86] Using MKL blas for quantized dense* Typo* CBLAS_OFFSET only available for MKL* Skipping tests as GPU CI uses Openblas* RetriggerCo-authored-by: Ubuntu <ubuntu@ip-172-31-0-202.us-west-2.compute.internal>	1
[NNVM] Add symbol for inception v3 (#1604)	1
[NNVM][FRONTEND][ONNX] Fix PReLU conversion (#3813)	0
Make "none" DataType explicit (#5491)* Make "none" DataType explicitThe None data type is created when converting an empty string to DataType.Add functions to create it and recognize it. Convert it to the "void" LLVMtype in LLVM codegen.* Rename "none" to "void"* Map VoidType:Type -> Void:DataType in GetRuntimeDataType* Map Void:DataType -> VoidType:Type in GetType	1
[TIR Pass] decouple flatten buffer to lower opaque block pass and flatten buffer. (#12172)	4
[LINT] Remove unnecessary copyright message for files with ASF header (#4409)* [LINT] Improve the check tool to handle ASF copyright message.* [LINT] Remove unnecessary copyright message as per ASF requirement.* Fix codegen hybrid* [LINT] Broaden license checks to include html, xml* [LINT] Fix rest of the files* Fix notice* [LINT] Improve check file type error message	0
Add contribute page about CI (#9906)* Add contribute page about CIThis adds some docs with a description of the TVM CI and some usage instructions to both make contributing more friendly and educate existing developers about how CI runs. Bikeshedding on content is welcomeNote: The TODOs are blocked on some other PRs and will be done before landing* docker instructions* Comments* RebaseCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Build] Explicitly link to cublasLt if it exists (#4776)* Explicitly link to cublasLt* Only link cublasLt if it's foundCo-authored-by: Jon Soifer <jonso@microsoft.com>	2
[Hexagon] Update Readme (#11283)* move conv2d readme* Update README	5
[TOPI][RELAY][PYTORCH]Conv3d_transpose op support added (#5737)* [TOPI][RELAY][PYTORCH]Conv3d_transpose op support added* Test cases in topi/relay* conv3d_transpose_ncdhw_python added* Review comments fixed	0
[TIR][TVMScript] specialize (#8354)	5
[COMMUNITY] ashutosh-arm -> Reviewer (#11101)	3
[TIR] StmtFunctor RenewDefs (#10843)* [TIR] StmtFunctor RenewDefsIn this PR, I introduce a StmtFunctor `RenewDefs` for deep copy all definition nodes in PrimFunc (including Var, Buffer, and IterVar). This functor can create a new PrimFunc with the same behavior as the old one but contains different Nodes.This Functor may help TIR fusion or inline multiple PrimFuncs* add ut* address comments* address comments* lint* lint	1
Add printer for Layout/BijectiveLayout (#3582)	1
Adding support for QNN subtract op (#5153)* Adding support for QNN subtract op* Fixing typo.* Fixing typo.* Fixing lint.* Addressing review comments.* Renaming variables as per convention and renamed QnnBinaryOpTypes -> QnnBinaryOpType* Renaming QnnBinaryOpType to QnnBinaryOpTensorType which now takes the index you want to extract to make the code more readable.* Fixing lint.* Moving common code to macro.* Fixing alignment.* Fixing typo.* Fixing lint.* Renaming method to pass CI.	4
[Relay][Op] Add type check to dense (#4724)	1
Fix some tiny spell error (#10693)	0
[RELAY] Refactor FoldConstant to skip TNonComputationalOps (#6720)* add TNonComputational to qnn ops and change FoldConstant* remove comments* check if op in nonComputational map* forgot to mark device_copy op as TNonComputational* hacky fix to fuseops pass* fix typo* manually skip device_copy in fold_constant* Update src/relay/transforms/fold_constant.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
Fix keras frontend elementwise-ops for lists with len>2  (fixes issue #325) (#326)* Added elementwise-add test* Fix typo* Fixed elem-wise ops for lists with len>2	0
Use shallow clone (#9864)to save bandwidth and docker image size	2
Use ci.py explicitly in docs building instructions (#9971)This adds `ci.py` to the docs to make it more clear how to easily build the docs locally. This also re-arranges CI following the merging of all CI steps to run concurrently since there's no need to run the Sphinx precheck during GPU unit tests. This still preserves it though in the docs step as a way to quickly bail out if there are formatting errors so the full tutorials don't get built.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TE] Promote substituted variable to iter_var's dtype (#10571)* [TE] Promote substituted variable to iter_var's dtypeThis fixes a bug where an iteration variable and its associated loopvariable have a mismatched dtype.* add check to iter var constructor. fix two bad uses* proplem is more complicated then I thought* one more fix* remove old comments	4
[ARM] Fix concat (#3061)	0
[TIR] CSE-TIR Pass - More deterministic behavior (#10663)* iterate through sorted keys* masa comments -- simplify iteration* test* tests* simplify vector construciton* jostle ci	3
Add -Bsymbolic-functions to linker option (#1244)	2
[APP] enhance android ui (#441)	5
[PYTHON][FFI] Cythonize NDArray.copyto (#4549)* [PYTHON][FFI] Cythonize NDArray.copyto* Cythonize the shape property	5
Update HalideIR (#1890)	5
[REFACTOR][TIR] Migrate Low-level Passes to Pass Manager (#5198)* [TIR][TRANSFORM] Migrate LowerIntrin* LowerDeviceStorageAccessInfo* Migrate LowerWarpMemory	5
[ATTRS] change AttrFiledInfo->Node (#1634)	5
[Doc][Tutorial] Add the instructions how to use contrib_spatial_pack (#2427)* [Doc][Tutorial] Add the instructions how to use contrib_spatial_pack* Update the code according suggestions	5
Link system library needed for LLVM (#1282)	5
[CODEGEN] Support cuda tensorcore subbyte int data type in auto tensorcore (#4546)* support cuda tensorcore subbyte int data type in auto tensorcore* add lisence* pass cpplint* fix code review comments* merge the int4/int1 codegen tutorial into the existing auto tensorcore tutorial* using master's new API* disable tuning when cuda is not enabled* address cr comment* do not run the tuning* fix test failure* fix cpplint error* fix bool type reduction bug* 1. fix a index bug 2. fix returned bytes value of int1/int4/uint4* fix typo	2
[METAL] set MTLBuffer purgeable state (#6376) (#6438)* [METAL] set MTLBuffer purgeable state (#6376)When using manual reference counting, MTLBufferpurgeable state should be set before releasing.* Fix lint error from tvm-ci	0
[RPC] support tracker in proxy (#1082)	1
Bump up tophup cuda version (#6908)	5
Add initial support for quantized transpose convolution in Relay (#6899)* Add initial support for quantized transpose convolution in RelayThis work is based on @jainris initial PR: https://github.com/apache/incubator-tvm/pull/6523I added a relay.qnn.conv2d_transpose node. The strategy I followed is toconvert to int16 and invoke nn.conv2d_transpose (which already exists inrelay). Main changes:- The node declaration lives in relay/qnn/op/convolution_transpose.cc- Cast int8->int16 and subsequent offset removal is in tvm/relay/qnn/op/legalizations.py.- I added and tested the operator in the tflite front-end- I added a unit-test in Relay for qnn.conv2d_transposeCo-authored-by: Rishabh Jain <jainris@users.noreply.github.com>* Fix linting* Addressing review commentsCo-authored-by: Rishabh Jain <jainris@users.noreply.github.com>	1
[Android] Update gradle version and other changes in android apps, CI modification to auto-build Android apps and upload artifacts (#11241)* Update gradle version in android_rpc app* Support latest gradle, bump versions, replace ndk build script with gradle tasks* [android_rpc] Fix linter errors, disable weird ones* [android_deploy] Support latest gradle, bump versions, fix linter errors, disable some of them* [android_camera] Support latest gradle, bump versions, rewrite readme* [android_camera] Fix linter errors* Fix sanity check errors* Add Android jobs for Github Actions* Add python requirements for TVM and android_camera, use preinstalled NDK* Revert to build with make* Add minrpc include (PR #11232)* Remove relative paths	4
[DOCS] Detailed contributor guide, doc refactor (#1220)	4
Parallelize cumsum in get_valid_counts (#7123)* Parallelize cumsum in get_valid_counts* make the scan loop exclusive* switch to directly using exclusive scan* perform inner loop of final writes on anchor threads* fix flaky testfix lint* remove final cuda kernelCo-authored-by: masa <masa@pop-os.localdomain>	4
[OPENCL] Workaround for zero size allocation (#9379)	1
[AutoScheduler] Update layout rewrite option setting for measuring (#7156)* Add layout rewrite options for measure* Update schedule for inserted transform stage* Set layout rewrite when tuning for network* Update the log version	2
Split adaptive_pool2d_avg into sum and div (#4186)	5
Fix onnx round import with float64 inputs. (#11685)* Fix onnx round import with float64 inputs.* Fix lint and optimize dtype mapping.	0
[PASS] PrintGraphIR, SimplifyBatchNormInference (#19)	5
[RELAY][PASS] Enable switching CanonicalizeOps in pass_enabled (#2696)	4
Fix lint, temporary add amaga back (#170)	1
add (#4311)	1
[RPC][CPP] Add support of cpp RPC-server for Apple (#8224)	1
dicrease the complexity of CalcDep from exponential to linear (#4053)	5
[TensorIR] Update VerifyGPU (#10405)* update VerifyGPU* address comments	1
fix dependenci and improve doc (#1535)	2
[Target] Rename target_id => target_kind (#6199)	1
Conv2d modified for better performance (#516)* conv2d tweaked for better end-to-end performance* syntax changed	4
[TVMC] Add support for the MLF to 'compile' command (#8086)* [TVMC] Add support for the MLF to 'compile' commandAdd support for the Model Library Format (MLF) to 'tvmc' so users canoutput compilation artifacts to a MLF archive passing the new flag'--output-format mlf'. For instance:$ python3 -m tvm.driver.tvmc compile ./sine_model.tflite --target="c" --output sine.tar --output-format mlfwill generate a sine.tar archive that is serialized accordingly to theMLF.Since the MLF is currently meant to be used only on micro targets, anerror is generated if one tries to run a MLF outside a micro context.The micro context does not exist yet but will be later introduced aspart of the [RFC] "TVMC: Add support for µTVM".That commit also adds 3 pytest tests to test tvmc + MLF.Finally, it also fixes some missing periods in the 'compile' commandhelp sections and renames export_format to output_format so there isno confusion with flag '--dump-code', which contains "formats to export"in its help section.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix missing importorskip in the import_package testFix missing importorskip() in the import_package test allowing thetest in question to be skipped when 'tflite' is not installed in thetest environment, otherwise the test will fail with:[...]>       archive_path = exported_tvmc_package.package_pathE       AttributeError: 'str' object has no attribute 'package_path'	0
[Hexagon] Clean up Hexagon device APIs (#11119)- Rename device_api.hexagon.v2 -> device_api.hexagon- Rename HexagonDeviceAPIv2 -> HexagonDeviceAPI- Rename hexagon_device_api_v2.* -> hexagon_device_api.*This concludes the removal of offload support from the Hexagon runtime.	1
[DOCS] Add some notes about LLVM (#373)Add some notes about LLVM before building TVM	1
[Target] Add python binding to new JSON target construction. (#6315)* Add python binding to new JSON target construction.* Added json string parsing and new test.* Add error type.* Add error type in json decoding check.* Fix sphinx formatting.	0
Remove warning which is adding too much noise (#7975)	1
[CI] reintroduce docker stage for wasm tests (#5565)* [DOCKER] Introduce ci-wasm* Add Jenkinsfile* Rename prepare to prepwasm so it won't run by default	1
Use std::move to avoid warnings on clang-13 (#12196)	2
Fix license URL (#451)	0
enable tsim and fsim for GPU build (#5352)	0
[RELAY][PASS] Fix expr subst and CombineParallelConv2D (#2218)	0
Fix LocalBuilder on macos with python 3.8. (#6083)Python 3.8 changes the default way multiprocessing creates new processeson macOS from forking to spawing. Spawning requires all objects to bepicklable. Nested functions and lambdas are not picklable, so thiscommit fixes the one instance of nested functions in the codebase thatwas causing issues.	0
Fix tvmc tuner for cases when uTVM is not enabled (#8153)	0
Add group_conv2d_transpose_nchw to CUDA backend (#10423)* add group_conv2d_transpose_nchw to CUDA backend* simplify significantly, just add groups argument to conv2d_transpose_nchw	1
Fix intersect of modular set (#2904)Fix comment bugs and code style	0
[Relay][Op] concatenate, reshape, transpose, copy (#1847)	5
[USMP] Fix assert condition for TVMBackendAllocWorkspace (#11270)* Fix test condition* fix	0
[AutoTVM] Enhance tuning space of split (#3949)* Refine policies for define_split- Rename policy "all" to "factors"- Add policy "verbose" and "power2"* Refine search space* add doc	2
[DOCKER] Fix: install script regarding get-pip.py during docker build (#7579)	2
[MetaSchedule][Runtime] Enhance Runner RandomFill (#11758)	1
[EXECUTOR] PruneGraph pass (#274)	4
[Hexagon] Run single RPC server on Android in each testing session  (#11547)* Reuse hexagon launcher in test session* separate random name generation* revert get_aot_executor* Fix launcher for simulator case* add stop server for simulator	1
[TUTORIAL] Improve opt_gemm tutorial (#757)* Improve opt_gemm tutorial* Addressed comments	1
typo in c_api.h (#290)	2
[OP] right_shift (#1832)	5
[Relay] [Training] Allow gradient to return a tuple (#3600)	1
add support for half_pixel_centers in resize (#8689)	1
[DOCKER] make demo images consistent with ci images when possible. (#4024)	1
[BUILD] Windows build pass on LLVM/CUDA/OPENCL (#57)	4
[Datatypes] Custom datatypes (#2900)* Register and use custom datatypes in TVMThis patch adds the ability to register and use a custom datatype from Python,using the `register_datatype` call. The datatype can then be passed as the`dtype` parameter using the syntax `dtype="custom[<type_name>]bitsxlanes"`.* Removes extra file* Register custom datatypes with TVM; specify Cast and Add loweringThis commit adds functionality for registering custom datatypes with TVM, andfurthermore adding custom lowering functions to lower those custom datatypes.This commit only adds lowering for the Cast and Add ops; more ops will be addedsoon.Check out some custom datatype samples in my repository of samples:https://github.com/gussmith23/tvm-custom-datatype-samples* Register and lower casts from Python* Formatting* Fix include; was including too much* Add comment* Add DatatypeRegistered* Add storage size field to custom datatypesThis field indicates the bitwidth of the opaque block of data into whichinstances of the datatype will be stored, when TVM compiles. For example, if Icreate a datatype with a storage size of 16, then- Constants of that datatype will be created as unsigned 16-bit ints- Calls to external functions taking that datatype will pass the data as  unsigned 16-bit ints- External functions returning that datatype will be assumed to return unsigned  16-bit ints.* Change how lowering funcs (Cast and other ops) are named in registrytvm.datatypes.lower.<target>.cast.<dst-type>.<src-type>becomestvm.datatypes.lower.<target>.Cast.<dst-type>.<src-type>And fixes some sloppy code around how the other ops were being formatted.* Update Python register_datatype to accept storage size* Oops, left out one cast->Cast change* Look up storage size when parsing `custom[typename]`When we encounter this type string in Python, it will be parsed into a Halidetype object in C++. Some of my original code supported this parsing, but we nowhave to attach the storage type to the type (by setting the bits field).* Change how external calls for casting/other ops are doneFirstly, we now use the storage size of the custom type when determininginput/output types; e.g. a cast to a custom type with storage size 16 is seen asa call to an external function returning an opaque uint of size 16.Secondly, write a macro to handle the other ops. Originally I thought I couldhandle these at runtime, with a single `_register_op` global. I transitionedinstead to using individual `_register_Add` etc. calls generated with a macro,but I don't remember why.* When encountering a custom type immediate, generate UIntImm* Translate custom types to LLVM type* Generate correct return type in CastsOriginally I was assuming that the result type from casts was always a customdatatype, and so I was making the Call return a UInt type.* Use TVM-idiomatic recursion style in DatatypesLowererThis was actually a bug, I'm pretty sure; we wouldn't have recursed deep on anycomplex programs. As a result of making this change, I also uncovered anotherpotential bug, where the datatypes lowering pass would attempt to lower a Loadof a custom type. By commenting out the `Mutate_` for Load, I was able to stopthe error from cropping up, but frankly, I'm not satisfied with the solution;how is it that we are able to run codegen when Loads of custom datatypes arepresent in the IR? I have not written any code, to my knowledge, that willsupport this. Perhaps Load does not care about the underlying datatype?* Use CHECK* Add comment about which Mutate_s are needed* Add comments* Add GetCustomDatatypeRegistered as an extern C function* Formatting, comments, casting* Change how datatype string is formatted* Use bits() instead of GetStorageSizeUse bits() instead of GetStorageSize* Change comment* Add datatype.py* Change registered function name (datatypes->datatype)* Remove GetStorageSize* Format custom datatypes like any other datatypeSpecifically, we now print the bits and lanes after the `custom[...]` string.* Correctly implement datatype lowering in Python* Remove unneeded include* Make function naming consistent* Use CHECK instead of internal_assert* Rename macro* Formatting* Rename functions* Implement Cast lowering`_datatype_register_op` is now able to lower both binary ops and Casts.* Formatting* Formatting* Clang format, google style* Fix std::string/extern "C" warnings* Formatting* Formatting* Lower Allocates and Loads during datatype loweringThis should ensure that there are no custom datatypes remaining once datatypelowering is done. This will allow us to remove the code in the LLVM codegenwhich deals with custom datatypes.* Revert additions to codegen_llvm.cc which are now unneeded* Pass cpplint on lower_datatypes.cc* Add clarifying comment* Remove datatype lowering registration funcs from C++* Add CHECKs* Remove TODO* Remove all references to storage size* Move and rename function* Rename function* Remove done TODOs and other handled comments* Remove irrelevant Load code and comments* Comment out the IR node types I'm not sure about yet* Add bfloat16 datatype unittest* Fix MakeConstScalarMakeConstScalar for a custom datatype will now call out to a function which canbe registered on a per-datatype basis. The function will take a double andreturn the equivalent value in the custom datatype format.Note that these code paths are not actually used or tested at the moment. I havenot yet written an example which uses const scalars of a custom datatype.* Formatting* Change pass name* Allow users to register whatever lowering function they wantTianqi pointed out that users should be able to register whatever loweringfunction they want, and should not be constrained to registering loweringfunctions which just call out to external libraries.I still provide a function for making lowering functions which call out toexternal libraries, for convenience.* Add clarifying comment* Remove unneeded comment* Remove unneeded function* Rename file* Undo unnecessary change* Undo unnecessary change* Make naming consistentRename "datatypes" to "custom datatypes" in most contexts.* Revert an artifact of old code* Fix build warnings, add TODO* Lint* Remove unnecessary use of extern C by separating decl and impl* Error checking* Remove TODO* Missed a name change* Lint* Python lint* Correctly format datatype* Move bfloat16 to 3rdparty* "custom_datatypes" --> "datatype" in most placesI left the pass as "LowerCustomDatatypes" to indicate that we're not loweringanything other than custom datatypes. Otherwise, everything else has beenchanged.* Upgrade datatype unittestI used a float calculator to generate some real testcases for the unittest.* Separate public includes and private implementationSpecifically, create cleaner decoupling between datatypes stuff in packed_funcand the datatype registry implementation.* Formatting* Limit custom datatype codes to >128* Add TODOs* Fix comment* Formatting* Clean up datatype unittest* Remove un-exported functions in public headers; UIntImm->FloatImmMore places where I accidentally was using implementation-only functions inpublic headers.Additionally, store custom datatype immediates as FloatImms. A later change willadd new lowering logic to lower these FloatImms to UIntImms.Plus formatting change.* Lint* Use FloatImm (not UIntImm) to hold immediates of custom datatypesThis change switches from using UIntImm to FloatImm for storing immediates ofcustom datatypes. The value of the number is stored in a double, which should beenough precision for now, for most custom types we will explore in the immediatefuture.In line with this change, we change the datatype lowering so that FloatImms arelowered to UInts of the appropriate size. Originally, this was going to be doneby allowing the user to register a double->uint_<storage size>_t conversionwhich would be called at compile time to convert the value from the FloatImm toa UInt and store it in a UIntImm. After discussions with Tianqi, we decided totake the simpler route, and lower FloatImms just as we lower all other ops: byreplacing them with Call nodes. In this case, presumably the user will Call outto a conversion function in their datatype library.The justification for this decision is due to the functionality added in #1486.This pull request adds the ability to load LLVM bytecode in at compile time.This applies in our case as follows: 1. The user writes their custom datatype programs and registers their lowering    functions in the same way we've been doing it so far. All operations over    custom datatypes are lowered to Calls to the datatype library. 2. The user compiles their datatype library to LLVM bytecode. 3. At TVM compile time, the user loads the LLVM bytecode. Depending on how the    datatype library is written, Clang should be able to perform constant    folding over the custom datatype immediates, even if their conversions are    done with calls to the library.Additionally adds test to test the FloatImm codepath.* Re-add a change I removed accidentally during rebase* Cleanup* Remove unnecessary TVM_DLLs* Add custom datatype utilities source file to Go runtime pack* Revert "Remove unnecessary TVM_DLLs"This reverts commit 4b742b99557fd3bf0ce6617f033c8b444b74eda4.* Mark bfloat code as TVM_DLL* Moves custom datatype runtime utilities to c_runtime_api.cc* Revert "Add custom datatype utilities source file to Go runtime pack"This reverts commit aecbcde0b2cc09a2693955b77037fe20f93b5bfd.* Move datatype parsing to its own function* Change comments* Remove unneeded function* Formatting* Formatting* Documentation* Add kCustomBegin, use it for checking for custom types* Documentation* Formatting* Move static definition to implementation* Remove comment* Decide toBeLowered before lowering arguments of ExprIn the past, e.g. when lowering custom datatypes for an Add, we would lower aand b first, and then decide whether the resulting new Add needed to be loweredbased on the (new) types of a and b. Now, instead, we need to check the types ofa and b first (to see if they're custom types), and then lower them (so they'llbecome non-custom types), and then lower the new Add.* Revert "Move datatype parsing to its own function"This reverts commit d554a5881afcf69af1c070d882a7651022703a09.This broke parsing. Will figure this out later. There isn't a really clean wayto separate this out given how the rest of the function is written.* Replace comment* Documentation* Remove comment and TVM_DLL* Better error messages* Remove artifact of rebase* Separate datatypes parsing to its own function* Add \returns* Comment changes; add TODO* Refactor tests	3
[Frontend][PyTorch] Add: Relay stft operator (#11190)* Add: Relay stft operator* fix doc* address PR comments* address addtional comments	1
Fix match case in Python-side expr functor (#4037)	0
[BYOC][TensorRT] TensorRT BYOC integration (#6395)* TensorRT integration using JSONRuntimeSupport input nodes with multiple data entriesFix failing testsSupport layout transform, add engine cachingAdd commentAdd PruneSubgraph passUse prune_subgraph pass, make params member of trt runtime classHide deprecation warnings coming from TRT headersRemove general prune subgraphSave/load use_implicit_batch and workspace sizeClean upFix cpp lintAddressing review commentsRefactor testsUse relay.bind instead of VarReplacer. Improve some annotation functionsAdd TRT docsUse DLOG, formattingUse logging.info instead of printalso  refactor integ testsalso  refactor integ testsFormattingFormattingFormat pythonfix python formatFix pylintFix sphinx precheckAdd tensorrt.rst to toctreeAllow codegen to be tested when TRT runtime is not available. Enable TRT codegen in CIlintyAddress more commentsFormattingFormatting* Documentation changes* Address comments* Rename USE_TENSORRT->USE_TENSORRT_CODEGEN and USE_TENSORRT_GRAPH_RUNTIME->USE_TENSORRT_RUNTIME* Fix comment typo* Test CI without TRT codegen enabled* formatting* Enable USE_TENSORRT_CODEGEN in CI* Change file_util.h -> file_utils.h	2
Add the SYSTEM keyword to all cmake include_directories commands for 3rd party or external headers. Warning flags should only be applied to code within the tvm repository. (#6531)	2
Mxnet parser for Qnn dialect (#4714)* - Additional util methods needed for mxnet frontend for qnn dialect.* - Fixing call to quantize.* [QNN] MxNet-MKLDNN parser support for QNN* [QNN] Relax conv check.* - Merge from origin* [QNN] Channel wise changes* [QNN] Dense changes* Dense fix for QNN ops.* - Removed non-mkl code from utils.- Small refactoring- Remove "with_sum" from conv- Simplified code* - Fixing ring buffer name.* - Fixing pylint issues.* - Fixing lint- Removing redundant commented code.* - Adding test cases- Removing unused methods.* [WIP] end to end test case for mxnet qnn parser* Changes to parse large CV models.* Pylint issues.* Fix Conv2D with sum and quantized pooling.* Reverting the changes made for mxnet-mkldnn test cases. Because of #4753, mxnet could not be updated to mxnet-mkldnn.Co-authored-by: Animesh Jain <anijain@umich.edu>	5
[Hexagon] Cleanup, remove obsolete comment (#10931)Should have been removed as part ofhttps://github.com/apache/tvm/pull/10581.	4
[MetaSchedule][Refactor] Introduce TuneConfig (#10986)This PR unifies the existing `EvolutionarySearchConfig`, `ReplayFuncConfig` and `ReplayTraceConfig` into `TuneConfig`, and refactored the logic in `meta_schedule/tune.py`	2
[Autodiff] Optimize and eliminate the Jacobian tensor for te.autodiff (#6078)* [Autodiff] Optimize and eliminate the Jacobian tensor for te.autodiffCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* fix lint* fix clang-format* add comments and magic number* clang-lint* address some comments* remove FreeVarsVisitor* fix constexpr lint* fix lint* fix lint* add Map.Merge* lint* change Array::Concat & Map::Merge to global functions* fix lint* move functions to global* static -> inlineCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>	1
Add ONNX LinearRegressor operator support (#10477)	1
Initial Implementation of TIRToRuntime Target hook (#9190)* Initial Implementation of TIRToRuntime Target hookThis is the initial implementation which wires in a test case for TIRToRuntime, in order to get this working I re-used `CodegenCHost` as it implements all of the `Op`s required from the lowered `PrimFunc`.Currently, the `IRModule` is non-unified but in future work it should definitely do so, I wanted to implement the basics here to get the infra in place.* Fix heterogeneous compute with multiple kDLCPU targets* Remove rogue te_compiler.h include	4
Add rust runtime (#1597)	1
[Hexagon] Remove sim_options from tvm.target.hexagon() (#11293)We no longer run simulator automatically, so this is not necessary.Also, the only way to pass options to the simulator was by settingan environment variable. That variable (HEXAGON_SIM_ARGS) shouldbe set independently by the user from now on.	1
[AutoScheduler] Support layout rewrite for whole networks (#6987)* [AutoScheduler] Add layout rewrite pass in relay* fix* fix lint* fix attrs* trigger CI* Apply suggestions from code review* trigger CI* Update python/tvm/auto_scheduler/relay_integration.py* Update python/tvm/auto_scheduler/relay_integration.py* Update python/tvm/auto_scheduler/compute_dag.py* Trigger CI* Apply suggestions from code review	2
[TVMC] Compose target options from target registry (#9218)* [TVMC] Split common tvmc test file into more specific filesThe `test_tvmc_common.py` file was becoming a bit of a mixed bag oftests and as we now want to extend the `Target` processing logic it madesense to split each out into its own file to make it clearer what eachdoes.`test_common.py` has also been renamed before we start using it for all thetests instead.* [TVMC] Compose target options from target registry[The RFC for this is still under discussion](https://github.com/apache/tvm-rfcs/pulls), but doing this before splitting the registries makes the most sense. This enables the `tvmc` driver to re-combobulate Target options from arguments:```tvmc --target=llvm \    --target-llvm-mcpu=cortex-m3```	1
[Relay] Prepare for merging context_analysis.cc and device_annotation.cc (#9077)* [Relay] Prepare for merging context_analysis.cc and device_annotation.cc- Improve construction and deconstruction of "on_device" and "device_copy" calls since they will be center stage.- Move "device_copy" support out of memory.h into own module to mirror "on_device".- Clearing out some DLOG -> VLOG changes I found helped me debug.- Clearing out some whitespace-only changes I accumulated.* [checkpoint] Address Christopher's comments.Some stray py formatting changes snuck in since I just run black . at the root.	1
Added option to build android rpc app with vulkan support	1
[Meta Schedule][M3a] TuneContext (#9053)* Add TuneContext class.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add tune context test.* Add meta_schedule to cmake.* Add type.* Rebase.* Disable MyPy for ethosu.* Add new line.* Remove duplicate line.* Minor fix.* Add comments.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[Frontend][Pytorch] Improve Pytorch frontend for object detection models (#6449)* Improve Pytorch Frontend* Add tests* Fix pylint* Improve data cast* Use int64 for slice axis* Fix lint* fix roi_align(..., aligned=True)* Minor fix* Add e2e test* Add asf header* Minor change* Use dynamic topk* Improve test* Rollback topk* py format* remove print* More improve* Fix test* Improve addmm* Fix test* Fix format* Fix format* Fix test scatterCo-authored-by: q.yao <streetyao@live.com>	3
[PYTORCH]Tensor creation ops support (#5347)	1
[AutoScheduler] Allow device specification for AutoScheduler Runners. (#10123)* Changed the python api to support device.* Finished implementation and updated tests.* Fix typo.	2
[NHWC] InferShape Layout conversion fix. (#372)	0
ONNX bitshfit (#7800)	5
[AutoScheduler] Improve tuning with random cost model (#6835)* fix* more fix* fix* revert* format* Update sketch_policy.cc* increase measure trial to avoid flaky	1
[Frontend] [PaddlePaddle] Add split operator (#11354)* suuport split op of paddlepaddle* black formatting	1
[TVMScript] Allow T.Buffer[] arg annotation to use int as shape (#11454)* [TVMScript] Allow T.Buffer[] arg annotation to use int as shapeBoth the function `tvm.tir.decl_buffer` and the TVMScript`T.match_buffer` expression allow a `PrimExpr` to be passed as the buffershape, which is interpreted as a 1-d buffer of that size.  This allowsthe same behavior to be used in the `T.Buffer` syntactic sugar.(e.g. `A: T.Buffer[16, "float32"]` instead of `A: T.Buffer[(16,), "float32"`)* Fixed round-trip when buffer size contains an expression	0
[Target] Tags, Composite Target, Unified Interface (#6369)* Add `set_attr_preprocessor` to TargetKind registry, which is used to pre-process attribute maps.* Use `set_attr_preprocessor` for NVPTX and ROCm backend to check and add mcpu and mtriple.* Add TargetTag registration and retrieval on C++ side and python side. Allow creation of Target using the tag name.* Unify target creation on C++ side, replace Target::Create and Target::FromConfig with the constructor.* Unify target creation on python side, deprecate tvm.target.create and encourage direct use of the constructor of tvm.target.Target instead.* Add initial support for composite target.	1
fixed #841 (#845)* Update workspace_pool.cc* Update workspace_pool.cc	1
[DOCS] Integration guideline (#1135)	2
[Relay, TF] Support converting TF combined_nms using Relay all_class_nms (#8174)* import from branchcommit c86bcf48fa6acd19647a7a096b9e1a5d4e56cc74Merge: 0fa88051b da75b2a52Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 12:13:29 2021 +0900    Merge branch 'tmp' into all_class_nms_tfcommit 0fa88051b3d30337674a07e579b78e8cb254cd66Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 06:24:57 2021 +0900    Revert "handling case when num detections is smaller than max_total_size"    This reverts commit 61e70b82f338300224b22f4d6bdda349e7aa5aca.commit 67251504c652e36106718617c5ae8b42c61deffcAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:06 2021 +0900    handling case when num detections is smaller than max_total_sizecommit 39549aa25267617671ca2a82ca517442065afe97Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:32:37 2021 +0900    simplify frontendcommit ca9470ba68e68c81902b0a3bad4bf5b5f0aa311eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:25:13 2021 +0900    update op definitioncommit 47bdef9e0fcdbab4671dd46044be5acac24b2f2bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 19:47:04 2021 +0900    remove unnecessary maskcommit 445a7daf1afb794be5f03473c70b172f06556d05Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:54:19 2021 +0900    remove in_buffercommit 71879b115b3bfe8087b73618bdab16fd61fbed86Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:48:22 2021 +0900    minor fixcommit 72e055a721ee7d698e7b3a3f58ab074ab78b57b2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:45:37 2021 +0900    make it more readablecommit a1fe7c46d6bb77da51de24b46bec881ed19d4cb3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:44:14 2021 +0900    clean upcommit 0c659bf27f9b90dd53455abd0d24b42f86e802bbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:33:54 2021 +0900    improve sort on cpucommit 480f6b782427dd46514efbf7028de4fb9f5ff9aaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:29:53 2021 +0900    collect indices and scores in one kernelcommit 2b441c391a25930092b55f12dadc31832400277bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:47:31 2021 +0900    initialization bug fixed in cudacommit d43e801289621e71a79c71308dedeef0969264beAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:23:09 2021 +0900    cpu nms bug fixedcommit 025010e42110388d0de2bc2ffcd76fbe14a188fbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 11:09:47 2021 +0900    add cpu implcommit 787d8399ff160694ecd2c4a9721a5825ca945d81Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:38:20 2021 +0900    refactoringcommit 05404305d1323b475829de10aa68f1f8791686ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:03:51 2021 +0900    initial import    commit 5ff0985625ec75f117af37017ebf4089dafb8a46    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:02:45 2021 +0900        cleanup    commit 199f9b67c2d471a761f743e6ea5fa414c899bd3f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:00:15 2021 +0900        Revert "add gather_nd shape func"        This reverts commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855.    commit 47a05c4c8f5a56a1685848210229aaa083b92880    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:53:00 2021 +0900        format    commit 9dcd0f02b25d658c94fa23e2cc65a9424ed8a1a5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:48:43 2021 +0900        make it static    commit eb06393939f1b8d8130f3815dc0f66223c9aa4f3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:14:31 2021 +0900        restore old impl and use it for q != 1 case    commit 115a5dfcf9b552fb2682534d82bbf638e661c0aa    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:00:40 2021 +0900        fixed score gathering    commit d2035626a72f8df71c514e3293337f6fda723353    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 08:53:14 2021 +0900        minimum fixed    commit 3fe91e8846b6d2075ae1d9a162c4b70b08cc8024    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 06:59:39 2021 +0900        batch issue fixed    commit 19e3e84690c0289c85001597046969d0c8dc92c2    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:29:15 2021 +0900        zero padding working        This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.    commit ce7848ba7def5a22659b09de039b2df12c0114f9    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:12:47 2021 +0900        pylint, do not use -1 for default value    commit 968f3bd230ed4855b45fd739dfc86edc2335ec80    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:07:31 2021 +0900        rename to index_rank and make it Optional    commit 9e06b8491e0ce1c981a5059f28135319f96978d0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 18:01:59 2021 +0900        fix pylint    commit 81dc6050dcbe59915a8f9b4f78b4aaf9fdba89a6    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:57:03 2021 +0900        minor fix    commit 54297b6128863d07e7dded71cc40077726faf2db    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:54:16 2021 +0900        support dynamic scatter nd    commit e25c225ce747c4e84452e6e7b32eeb0d71b2995d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:33:19 2021 +0900        gather_dim -> num_indices_per_tuple    commit aaa6211e7ef3ce520b8711a78cf7eb2af52e7acc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:23:46 2021 +0900        add dynamic gather_nd test    commit 3a9fe5dfa5faeadbcdb882ff70039ea7bccb61a3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:18:26 2021 +0900        refactor gather_nd ref funcs    commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 14:36:34 2021 +0900        add gather_nd shape func    commit b0200643a184294a2f2b3cce7208c4d257987424    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:01:11 2021 +0900        working on zero padding    commit 456741790dd5e73f3f76ef7a5ede6e1014de8b2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 03:21:52 2021 +0900        working    commit 7f5c76d0090950985888781f071ca341e2fa5695    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 02:37:50 2021 +0900        relay type inference works, debugging topi    commit 4a4b8dfbfdc65d7a6e77ed0a6e8b09af162b77ad    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 15:08:16 2021 +0900        add max_total_size to attributes    commit 7218b2f7b4de0c796d69f23084cd688e28f7b461    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:50:58 2021 +0900        tf frontend update    commit cde4a1fdd15ed898b1f7299e99377cceaaee2732    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:17:14 2021 +0900        all class nms tf mode first cut    commit 5f349f77c9c230ee636aceb52547502319c8ad77    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 06:54:34 2021 +0900        begin supporting per batch output    commit 0044365affac6667a02d15791a59040702f8990b    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Mon May 3 19:46:28 2021 +0000        initial    commit 168a617e48b062417b766d6400b0c6b856084cfa    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Fri Apr 16 20:31:32 2021 +0000        initia;        lcommit da75b2a52e9a8daa322168d1b6026e144d42d5bbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:58:19 2021 +0900    do minimum in topicommit 52c5e8a5bca56f93778990d4faa87c7e7b342ba7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:54:49 2021 +0900    more simplifycommit 44d88cdecd87468b630fd16a7d1e1214e86eabfaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:51:39 2021 +0900    simplifycommit 74e19174f1b2d40ee8f4d08c7a61667bc3dd69b5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:39:37 2021 +0900    blackcommit fc3a38e1cb699b66340c7742cb74188fdbe92bf5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:37:30 2021 +0900    minor changecommit f88e2a3a98a7ee283622e57712e28634374e5e2cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:14:54 2021 +0900    minor refactorcommit f2d7ed410a0b835586929706873ee1f448d1f955Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:08:47 2021 +0900    support the case when there is not enough boxcommit 0f184a6bf6e533c91d26c98a7a17f8d7970364ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 06:24:16 2021 +0900    Revert "handling case when num detections is smaller than max_total_size"    This reverts commit 61e70b82f338300224b22f4d6bdda349e7aa5aca.commit d7180f27cfaffbbd1ab1ce970ca605133bc812eeMerge: 61e70b82f 06ac2052aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:37 2021 +0900    Merge branch 'gather_nd_shape_func' into tmpcommit 61e70b82f338300224b22f4d6bdda349e7aa5acaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:06 2021 +0900    handling case when num detections is smaller than max_total_sizecommit 453a79bd05f67653be8b90db80ecde12d343aea6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:32:37 2021 +0900    simplify frontendcommit 2fc5f1ed3de49266f1eb72aed25d26457da78491Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:25:13 2021 +0900    update op definitioncommit 8afbd30c0fbbd40902acc4196a18b448f2a93266Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 19:47:04 2021 +0900    remove unnecessary maskcommit ff870f7e972e289953ca0e5daa444c09e5095efaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:54:19 2021 +0900    remove in_buffercommit e71b922b6cdf129ef51e91928635374a1f02a6fcAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:48:22 2021 +0900    minor fixcommit b02faaead24d2d14d3b67bf04ee23f9df9bfecbeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:45:37 2021 +0900    make it more readablecommit 6baee99ed1b57be8da06c00e17d6b92083668ac0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:44:14 2021 +0900    clean upcommit 7a2a2df8b696faf7c4280fd9a3f9fbdf8f5c3e03Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:33:54 2021 +0900    improve sort on cpucommit afad2a2e920c98d269c6000035f31392cff7b6a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:29:53 2021 +0900    collect indices and scores in one kernelcommit c5718e299a82ffe5e60bc1fee679b2b0405346e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:47:31 2021 +0900    initialization bug fixed in cudacommit 5623e3f8f71de1dbec55c83a300fa4131cd82aadAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:23:09 2021 +0900    cpu nms bug fixedcommit c40eaecd87513a6869098ed95c03b8553c350414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 11:09:47 2021 +0900    add cpu implcommit 6c7aaeb44f5586b57e7b1bfd7772d1b78a9eae1fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:38:20 2021 +0900    refactoringcommit 7b87922279121f06cdcc77a41ac6c8f59b6d5549Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:03:51 2021 +0900    initial import    commit 5ff0985625ec75f117af37017ebf4089dafb8a46    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:02:45 2021 +0900        cleanup    commit 199f9b67c2d471a761f743e6ea5fa414c899bd3f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:00:15 2021 +0900        Revert "add gather_nd shape func"        This reverts commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855.    commit 47a05c4c8f5a56a1685848210229aaa083b92880    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:53:00 2021 +0900        format    commit 9dcd0f02b25d658c94fa23e2cc65a9424ed8a1a5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:48:43 2021 +0900        make it static    commit eb06393939f1b8d8130f3815dc0f66223c9aa4f3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:14:31 2021 +0900        restore old impl and use it for q != 1 case    commit 115a5dfcf9b552fb2682534d82bbf638e661c0aa    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:00:40 2021 +0900        fixed score gathering    commit d2035626a72f8df71c514e3293337f6fda723353    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 08:53:14 2021 +0900        minimum fixed    commit 3fe91e8846b6d2075ae1d9a162c4b70b08cc8024    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 06:59:39 2021 +0900        batch issue fixed    commit 19e3e84690c0289c85001597046969d0c8dc92c2    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:29:15 2021 +0900        zero padding working        This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.    commit ce7848ba7def5a22659b09de039b2df12c0114f9    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:12:47 2021 +0900        pylint, do not use -1 for default value    commit 968f3bd230ed4855b45fd739dfc86edc2335ec80    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:07:31 2021 +0900        rename to index_rank and make it Optional    commit 9e06b8491e0ce1c981a5059f28135319f96978d0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 18:01:59 2021 +0900        fix pylint    commit 81dc6050dcbe59915a8f9b4f78b4aaf9fdba89a6    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:57:03 2021 +0900        minor fix    commit 54297b6128863d07e7dded71cc40077726faf2db    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:54:16 2021 +0900        support dynamic scatter nd    commit e25c225ce747c4e84452e6e7b32eeb0d71b2995d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:33:19 2021 +0900        gather_dim -> num_indices_per_tuple    commit aaa6211e7ef3ce520b8711a78cf7eb2af52e7acc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:23:46 2021 +0900        add dynamic gather_nd test    commit 3a9fe5dfa5faeadbcdb882ff70039ea7bccb61a3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:18:26 2021 +0900        refactor gather_nd ref funcs    commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 14:36:34 2021 +0900        add gather_nd shape func    commit b0200643a184294a2f2b3cce7208c4d257987424    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:01:11 2021 +0900        working on zero padding    commit 456741790dd5e73f3f76ef7a5ede6e1014de8b2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 03:21:52 2021 +0900        working    commit 7f5c76d0090950985888781f071ca341e2fa5695    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 02:37:50 2021 +0900        relay type inference works, debugging topi    commit 4a4b8dfbfdc65d7a6e77ed0a6e8b09af162b77ad    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 15:08:16 2021 +0900        add max_total_size to attributes    commit 7218b2f7b4de0c796d69f23084cd688e28f7b461    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:50:58 2021 +0900        tf frontend update    commit cde4a1fdd15ed898b1f7299e99377cceaaee2732    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:17:14 2021 +0900        all class nms tf mode first cut    commit 5f349f77c9c230ee636aceb52547502319c8ad77    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 06:54:34 2021 +0900        begin supporting per batch output    commit 0044365affac6667a02d15791a59040702f8990b    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Mon May 3 19:46:28 2021 +0000        initial    commit 168a617e48b062417b766d6400b0c6b856084cfa    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Fri Apr 16 20:31:32 2021 +0000        initia;        lcommit 06ac2052ab843be950ff3abf6ce8d52803adc5e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 28 13:12:47 2021 +0900    pylint, do not use -1 for default valuecommit 2adc42618580c967bd49d53c0724382f9cf87772Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 28 13:07:31 2021 +0900    rename to index_rank and make it Optionalcommit c458da6e80b0ff7b6e2ca729a49755f42dfe3702Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 18:01:59 2021 +0900    fix pylintcommit b7faf0f93bd3ba4fc0eb88f1fac31c8d9525c883Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:57:03 2021 +0900    minor fixcommit c03164116046670963f1d04529bfe94c5030ad17Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:54:16 2021 +0900    support dynamic scatter ndcommit 56f3f0ea3fae4ba049101fcb4571b8999a3bda1cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:33:19 2021 +0900    gather_dim -> num_indices_per_tuplecommit 081823b0129093602bb7f512f326eeb10bfb1906Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:23:46 2021 +0900    add dynamic gather_nd testcommit 6b2655baf867b4d08e7d21ffe5f854228ced57e9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:18:26 2021 +0900    refactor gather_nd ref funcscommit f9f5dfbe2a65eff8aa6718bf05fd8a843c5df08fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 14:36:34 2021 +0900    add gather_nd shape func* make combined nms converter public* do topk on smaller score tensor* update tests* remove max_total_size attribute, do minimum in relay side* fix topk* update relay doc* update doc* fix pylint* update shape func for tf mode and add test* name change* reject dynamic inputs* revert gather_nd change* do not try to support dynamic batch size in tile rep* check batch_size is int* fix dtype issue in scan* fix slicing before topk	0
[RELEASE] Release note for 0.2 (#853)	5
TFLite failures resulted from TF latest version upgrade resolved (#6774)* TFLite failures resulted from TF latest version upgrade resolved* [1] Review comments handled	0
[BUGFIX] Fix for quantize. (#2573)	0
[TOPI] Support int4/int8 conv2d tensor core with HWNC layout (#6121)* int4 tensorcore* a draft for new int4 schedule* update layout* add inline option* clean code* increase search space* fix kernel shape* update intrinsic* update intrinsic* support int4/int8 hwnc layout* remove useless code* remove useless code* remove useless code* remove useless code* fix int8 transpose* fix assert* add asf header* CI* CI* CI* fix bugfix bugCo-authored-by: Leyuan Wang <laurawly@gmail.com>	0
[Relay][AlterLayout] Broadcast with scalar shape (#4577)	5
[ARITH] normalize iter affine map expr to PrimExpr (#7759)	5
Support mode=instance, spatial for MXNet l2_normalize (#7062)	1
[microNPU] Fix layout assignment in layout optimizer pass (#10143)Fixes the layout optimizer incorrectly assigning layouts for graphs withmore complex topologies than previously considered. Specifically, thiscommit now ensures that intermediate layouts match (e.g. parent output =child input) and that all consumers are taken into account when alteringthe output layout - something not done previously due to an incorrecttraversal order.Previously, the input layout was always altered if the producer was anNPU operation without regard to the output layout of that operation.Additionally, is was possible for the output layout to be incorrectlyset due to a depth-first post-order of traversal of the graph, meaningit was possible for not all consumers to be taken into account whenaltering the layout.Now the `AnalyzeConsumers` pass is run before `LayoutOptimization` whichdetermines a mapping from NPU operation to list of boolean values thatrepresent whether or not each consumer is an NPU operation. Since thisis completed before `LayoutOptimization`, all consumers are guaranteedto be taken into account when altering the output layout. In turn, theinput layouts can correctly be determined by checking whether the outputof the producer will be altered.Change-Id: I04e9605da65fa9f12801109dd50c5e3f08cbc73c	4
Adding aten::unsqueeze_ to PT Frontend (#7231)* Added Ops* Regular* Remove copy* Remove copy* Tests* BlackCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>Co-authored-by: Ubuntu <ubuntu@ip-172-31-19-34.us-east-2.compute.internal>	3
[CODEGEN] Bugfix multiple condition generation (#558)	0
[TESTING] Fix the error when running tests with default targets (#6394)	1
[ONNX] Add imports for BERT contrib operators (#10949)* EmbedLayerNormalization, Attention* fix Attention* SkipLayerNormalization* fix dtype bug in GeluCo-authored-by: An Wang <anwang2009@gmail.com>* missing parameterize_targets* lint* lint* comments* fix small thing* factor out layer norm computation* layernorm func* add optional args to test* upgrade onnxrt version* no upgrade onnx* fix tests* int32* fix testsCo-authored-by: An Wang <anwang2009@gmail.com>	3
[CMSIS-NN] Initial operator support for Mul (#9163)This is largely as it says on the tin, it adds Mul support to CMSIS-NN	1
Generate compile_commands.json by default (#9763)* Generate compile_commands.json by defaultThis is low overhead (it only affects the make-generation step, not the actual build) and makes developer tooling like clangd work much better, so we should have it on by default instead of off (which is CMake's default). If necessary, it can be disabled with```bashcmake -DCMAKE_EXPORT_COMPILE_COMMANDS=0 ...```* Add skip when run in a subproject* Update CMakeLists.txtCo-authored-by: Andrew Reusch <areusch@gmail.com>Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>	1
update dmlc-core (#728)	5
Fix VTA Tutorial for more strict graphrt check (#1737)	0
[Bugfix][Keras] axis of softmax (#3834)	0
Flip operator (#505)	1
[µTVM] Minor fixes to the Reference VM tutorial (#7012)* Add recommendation to install vbguest plugin.* Update directories to match checked-in.	5
Implement relay nn.bias_add compute in C++ (#3027)* Implement nn.bias_add compute in C++* Address comments* Remove unnecessary check	4
[Community] @elvin-n -> Reviewer (#9321)	3
[RELAY] [AST] Add virtual_device as a first class field in Relay (#9641)	1
[VTA] Move compiler related registry items to vta/build_module.py (#6012)	1
[TIR] Regression test for PrettyPrint/IterMapExpr bugfix (#11418)Follow-up from https://github.com/apache/tvm/pull/11412, adding aregression test for the bugfix.	0
Add API `get_input_info` to graph_executor (#9889)	5
[ci][docker] Use CMake 3.20.0 for cortexm (#12744)The Zephyr project builds require 3.20.0 to work correctlyCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[CODEGEN] Refactor common codegen, Verilog Codegen (#74)* [CODEGEN] Refactor common codegen, Verilog Codegen* fix make* fix mk* update enable signal* change function name to at neg edge* Move test to correct place	3
Fix typo in error message in CMakeLists.txt (#9251)	5
[Relay][Frontend][darknet] Solve tvm parsing darknet resnext failure bug (#3778)* test_darkent_bug* test_darkent* add resnext tests	3
[ETHOSN] int8 support for tanh operator (#10813)	1
[PyTorch] Add aten::new_empty (#12591)This PR intends to add `aten::new_empty` which is used for model like `hf_Longformer`.cc: @masahi	1
[TOPI] Allow conv definition to have custom kernel layout (#11936)* [TOPI] Allow conv definition to have custom kernel layout* add tests* fix* fix	0
acquire gil while finalizing PackedFunc (#6378)	5
Fixed process termination routine in windows (#4844)* Fixed process termination routine in windowsaddresses and Fixes AttributeError: module 'os' has no attribute 'killpg' error in #4821* Update server.py	5
[MetaSchedule][Test] Add unittests for GMM (#12243)	3
[Bugfix] Add check to avoid calling back() on an empty container (#8930)* fix bug of calling back() on an empty container scope_[op->buffer_var.get()]* add test case for ConvertSSA* update the style to fix ci error* add annotation for function test_convert_ssa* update the style of test_convert_ssa to fix ci error	0
Remove unnecessary bracelet around make_int (#7907)Turning from `((make_int4)(exp))` to `(make_int4(exp))`. The former case is incompatible with "macro-defined function".	1
[Relay][Op][TF] Complete tensor array unstack with all ranks support (#4309)	1
[CI] Upgrade LLVM envs (#3590)	5
[Topi] [Hexagon] Conv2d slice op initial version (#11489)	5
Add parameter to allow caller to supply a Runner (#8747)* Add parameter to allow caller to supply a Runner* Add unit test for passing in runner to graph tuner	1
[Relay][MXNet] Support broadcast_like (#6561)	1
[OpenCL] Remove redundant visit statement in CodeGen. (#9144)Fixes regression with some models on which compilationdoesn't terminate.	0
[Relay][fix] Stack should take exprs that evaluate to tuples (#7130)* Fix stack to take Relay exprs that evaluate to tuples* Doc tweak* Linting fix	0
Add deps for Relay (#1463)	1
[Relay, OpFusion] Fix handling TupleGetItem for nested tuples (#2929)	1
[TF][TEST] add test_forward_reduce_any back (#4301)the test case was removed in #4181 for some reason@tqchen @soiferj @zhiics	4
Stronger type checker during conversion	5
register auto-scheduler to more ops (#6879)	5
fix build in windows (#256)	0
[TEST][FLAKY] fix random fail (#6312)* [TEST][FLAKY] fix random fail* increase size and error check range	0
[Meta Schedule][M3a] Instruction and Trace (#8615)	2
[CI] Fix global pip cache disable change (#8590)* The fix to disable cache needs to run after pip is installed* This is quick follow up fix after #8575	0
[METAL][RUNTIME] Fix bug of memcpy into metal buffer (#428)	0
[RPC] Make tracker jupyter friendly (#7961)This PR uses the PopenWorker to handle the tracker start upand makes the tracker jupyter friendly.	1
Fix bugs with C++ TOPI flatten and relu (#869)* Fix bugs with C++ TOPI flatten and relu* Added regression tests. Fixed typo in CMakeLists.txt. Fixed topi cpp import removed.	4
Add another MKL name alias for MKL (#3853)Installed through pypi	1
[RELAY][FRONTEND][CAFFE2] add Mul and ConvTranspose operator (#5302)	1
[Torch] Support Python list, more realistic recurrent networks (#5306)* use funcs from prelude, pass around convert_map* get relay input type from user ishape* handle tuple unpack* experimenting with static tensor array* use prelude concat instead of cons + rev* minor clean up* fix layer norm conversion bug, unwrap tensor array* add infer shape on tensor array* pass around prelude for now* compile worked but runtime error* fix tensor array wrapping* begin list dynamic test* is_list_dynamic first version* finish dynamic list test* a few fix* use shape_of function if Any is found* improve size conversion* working on adding free vars to loop block* fixed inlined inner loop issue* clean up free var handling* add support for tensor array concat* adding ta concat on last axis* fix concat, but got runtime error* disable concat on axis -1 for now* add lstm tests* revert unrelated change* fix stacked bidir test* minor fix to test* relax tol a bit, revert dnnl change to avoid conflict* simplify infer type, use input tensor shape rather than concat shape* more shape fix	0
[ci][wip] Upload docs with folder structure to S3 (#11528)Keeping the files as-is lets us serve them from S3 + CloudFrontCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[skip ci][wasm][ci] Fix WASM build and JS doc build (#11299)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[MetaSchedule] disallow_dynamic_loop (#9997)* [MetaSchedule] disallow_dynamic_loopCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Update src/meta_schedule/postproc/disallow_dynamic_loop.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[AutoSchedule] Support multiple cache read and fix bugs (#6686)* Add shape to DAG print* avoid useless cross-thread reduction* Fix stage order* support multiple cache_read* lint* fix* fix* address comment* fix ci* Trigger CI & Update doc stringsCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	2
Update setup.py (#803)fix errors when running `python3 setup.py sdist bdist_wheel`	1
[ONNX] Add CumSum operator to ONNX frontend (#7391)* [ONNX] Add CumSum operator to ONNX frontend* Fix lint and add attributes to CumSum* Fix CumSum test* Add support exclusive attribute* Add support reverse attribute* Fix clang-format* Fix lint* Move reverse calculation to ONNX frontend and add exclusive to GPU* Add test for int type	3
support custom IP address from rpc server to tracker (PUT) (#1243)	1
[TFLite] Using real image for QNN testing. (#4816)* [TFLite] Using real image for QNN testing.* Setting seed for SSD mobilenet for fixed input.* Support quantized Pad op.* Remove unnnecessary line.* Ina comments.	4
[TIR] Make compact buffer and get access region aware of conditions (#9372)* Support condition bound awareness in compact buffer and get block access region* remove intset difference usage* fix to visit match buffer's access region* change method to distinguish annotated opaque access regions	4
[TOPI] Update depthwise conv2d schedule on rasp (#500)	5
[ARITH] cleanup the indexmod/div on python side (#4028)	4
[ONNX] Disable failing tests on AArch64 (#12256)Change-Id: I170d2a8032dcb19d6ba3f67d9b0441944def84b8	4
[TOPI] Enhance Conv2D for More Data Type (#922)	5
[WIP] [TOPI] Depth wise Conv for NHWC (#325)* rename the nchw and pass the unit test; going to do it for nhwc depthwise* bug with fusion* nchw works fine; nhwc float32 problem remains* still cannot bind them together* fusion works* syntax fix* all bugs fixed; test cases pass* minor fix on nn.h	0
Improve if_parser_enabled (#2372)	0
[Relay] Densenet benchmark (#2154)* Port densenet to Relay* Invoke densenet test in __main()__* Even the spacing in the IR text format tests* Forgot to import densenet in init* Correct reference to densenet in test	3
[RELAY][Convert Layout] Specify additional layouts in convert layout pass (#5422)* [RELAY] Specify additional layouts in convert layout pass* This patch means that you can specify an additional layout, rather than using the layout chosen by default during conversion.* This is specifically useful for external codegen when a 3rd party library needs to target a specific kernel layout for example.Change-Id: I3ef9cf45ead574801870a38af9768f93e29aab10* Use mapping of op name to list of desired layoutsChange-Id: Ibd691a3cb93e73a394f36112668ad52a84c7d5a2* Fix issue with code blockChange-Id: Ibb4e38c05ad4312b7dea845be699b8d5d57e0a94* Address comments, Improve tutorialChange-Id: Ib824eead329d551c338234de3b2d814693afd0ec* Fix lintingChange-Id: Ie9e1891f590b3a7496a56ff8362cdda9d4b5fa75* Test uses NCHW default layout. Unrelated issue with NHWC.Change-Id: I1c16f0db73db56f5e9536db3fe5eb2624c3b595c* Fix mistake in tutorialChange-Id: I944041245d27af262dc96f1cd8117f1f19272062* Address multiple commentsChange-Id: If33a1e34acd8fc37d1c7797ee189a6448a392672* Improve tutorialChange-Id: Ib04142c94c7958ab5067947d2ff4c84354e3d0c5* Fix Clang-formatChange-Id: Ieff39e3f0817d22579c68b3287e972a3b0fcfbc8	4
fix PostOrderVisit signature (#3048)	0
[COMMUNITY] Yuanjing Shi -> Reviewer (#12345)	3
[Frontend][PaddlePaddle] Support more common operators (#9428)* update ci-gpu to v0.78* add some common operators* code format* add transpose and swish* add unitest	3
Update ISSUE_TEMPLATE.md	0
[AutoScheduler] Fix FLOPS estimation (#8695)	0
travis mac (#38)* chomo* try mac options	1
[RUST] Add conv3d transpose Rust bindings (#11471)* Add conv3d transpose Rust bindings* Fix typename* Add base	1
Fix int32 range overflow by using int64 (#3870)	1
Add Scatter to Topi/Relay/ONNX via hybrid script (#5619)* I can construct scatter but not embed it in a Relay Graph* working 1-4 dimesion scatter* add scatter to ONNXfix lint* isolate tests to cpu backend* Fix i386 test* fix gpu tolerance* use elemwise_shape_func for scatter* fix incorrect rebase	0
[OPENCL] Fix 32bit pointer size in OpenCL runtime (#809)	1
[CI] Recover Windows Mac Build CI via Github Actions (#4662)* [RUNTIME] Fix windows build after the latest dso module change.Switch to shared_ptr to get around a problem in latest MSVC.* [CI] Add github action for win mac build.	1
[Texture support][Part 1] TIR lowering and OpenCL support (#7686)* Add support for kTexture storage rank.* Add scaffolding for texture_flatten pass.* Add scaffolding for texture allocation.* Implement 2d texture flattening to builtin tir.text2d_alloca.* Lower BufferStore/Load to builtin texture store/load.* Add vectorizable attribure to texture load and store.* Support auto-vectorization on the innermost (RGBA) axis.* Add read/write_imagef opencl codegen for builtin texture load/store.* Add TextureType support.* Add InferTextureAccess pass to deduce __read_onlyand __write_only access qualifiers for texture vars.Also refactor use of restrict keyword to be var dependent.* Implement texture allocation as external function in TIR lowering.* Remove commented lines.* Add nd->2d texture flattening.* Bug fixes in opencl codegen (row<>col, access quals.)* Improve texture codegen by explicitly allocating local vectorfor the texture load. Also support indexing individual elementsof the RGBA vector.* Remove automatic vectorizationcode as it is no longer needed.* Improve SSA local use when storing texture read to scalar buffer.* Define texture flattening convention suchthat the outer Nd-1 axes are stored as rows,and the last axis is stored as columns.* Add tir lowering and opencl codegen support for float16 textures.* Disable SSA when texture load is immediately casted.* Allow RGBA extent to be of length 1.* Add pass to forward externally allocated texturesin place of textures realized from cache_read. Fixto better follow indexing spec.* Add buffer_common.h to house buffer offset simplification routines.* More refactor and clean up in texture lowering.* Add IsTextureType to tir and allow buffervar type annotation to be TextureType in additionto PointerType.* Bug fix in texture access qualifier inference pass* Step toward handling external texture buffer forwardingwhen external buffer is not stored directly to cache_read realized buffer.For example when it is conditionally stored via an IfThenElse node whenpadding is used.* [Part 2/3] Support texture:weight lowering convention for externally providedtexture buffers. Need to propagate this to allocated textures whencache_read(texture) is used for weights.* Bug fix in texture access qualifier inference pass* Tighten constraint on external buffer forwarding --cache_read(texture) cancellation -- to avoid incorrectprograms. Currently only forward through if_then_else nodeand direct external loads. For if_then_else, still needproper analysis of structural equality between buffersand access patterns to determine if an external buffercan replace the texture buffer realized via cache_read.* Use texture lowering convention from texture runtime util.* Use updated texture lowering utilities* Use inherited visitor overloads in texture flattener.* Add check in codegen for float/half untilread/write_image codegen supports other types.* Rename tir texture builtins* Remove codegen and tir runtime dependence on for TVMBackendAlloc/FreeTexture.* Dispatch texture allocas via target specialized tir.tvm_call_packed* Remove kTexture scope and use kGlobal with texture tag.* Remove TextureType.* Remove TextureType from OpenCL codegen.* Remove TextureType from TIR lowering.* Remove dependency on MergeMulMod.* Revert "Add buffer_common.h to house buffer offset simplification routines."This reverts commit 027628259229aaee051dbf1dfbed4e63ef820544.* Prune include list* Add more documentation to texture flattening.* Add TextureFlatten transform to refactored tvm lower API.* Apply clang formatting.* Blacken python APIs.* Apply cpplint changes.* Attempt to extract storage scope from pointer scope.* Remove ExternalBufferForwarding (cache_read cancellation) for now.* Apply MyPy.* Clang format* Only visit RealizeBuffer body for texture storage.* Fix bad merge.* Utilize OpenCL preprocessor to switch betweensampler-less and codegen provided sampler fortexture reads depending on whether the openclruntime is 2.0 compliant.* Add texture codegen test example.* Refactor tests to use pytest parameterization.Blacken tests.* Respond to CRs.	3
[3/10] Moved TIR generation from Python to C++ for CMSIS-NN (#8951)* [CMSIS-NN] Moved TIR Generation to C++* Deleted self import for cmsisnnChange-Id: I2cdcd7a90aa4749877c48bc6c7c4d27328856860* Reusing CodeGenC VistiExpr for softmaxChange-Id: Ie41b695fa06468cd3b0bfe428c360e98438a9180	4
[PTX] Support mma.sp to use Sparse Tensor Cores and refactor mma codegen (#10339)* init* upd* upd* lint* lint again* upd* add m16n8k32 testcase* format* use make_tuple instead of initializer list* add metadata offset* upd* docstring and sanity* add u8s8s32 back* improvement* compatible #9727	1
[RUNTIME] Enable auto conversion String->DLDataType (#6214)	5
Disable tensorflow v2 behavior in all unit tests (#10204)This should resolve the issue posted here https://discuss.tvm.apache.org/t/tensorflow-2-0-test-failures-while-running-the-tensor-flow-frontend-test-forward-py-function/11322 based on [this comment](https://discuss.tvm.apache.org/t/tensorflow-2-0-test-failures-while-running-the-tensor-flow-frontend-test-forward-py-function/11322/3?u=driazati).This is a necessary precursor to #10198 since it relies on invoking the tests individually.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][VM] Relay VM memory liveness/lifetime analysis (#10026)* WIP VM memory planning* tuple projection* support if* lint* remove old comment* WIP check in attempt at CFG analysis* rewrite CFG analysis in stages, support ADTs* lint* fix small bug in alias elimination, try fix VM profiler error* update DCE tests since allocations can be DCE'd* optimize worklist to reduce runtime* add docs, rename pass to ManifestLifetimes* add tests, more comments, proper VM profiler fix* lint* ci please* address nits* retry ci again* retry ci once again :)* fix sneaky memory leak due to cyclic refs* fix didn't work but retry ci anyway* slightly reduce size of large pretty printer test	3
[FIX] Fix doc_string of reducer (#292)	2
update android rpc docs (#1479)	2
[TOPI] Add ops compute (#323)* [TOPI] Add ops computeRemove 'compute' and add assert for safetyAdd documentfix lintfix softmax* fix batch norm	0
Handle uint8 in ConstantNode visitor in LowerToTECompute (#10894)	0
add symbol::GetChildren (#104)	1
[TensorIR] [Script] adding support for opaque block (#7829)* change complete tag* add parsing support for opaque block* address and add testcase* address* address	1
[DOC] More detailed installation instruction (#262)* [DOC] More detailed installation instruction* fix lang	0
[TF frontend] add support for StridedSlice to input a single constant (#6949)* [TF frontend] add support for StridedSlice to input a single constant* add test for strideslice with a single number input* fix bug	0
[Bugfix] Fix visit_attrs error if its function pointer is equal to nullptr (#8920)* fix visit_attrs equals nullptr on python container object* add a test a for python container object about function dir and getattr* change test_ir_container.py to the pytest style* update the style to fix ci error* update the style of ir container to fix ci error	0
[TVMC] Add codegen args to tvmc (#10190)* [TVMC] Add codegen args to tvmcThis enables external codegen arguments similar to those for `Target`s:```tvmc compile --target=cmsis-nn,c --target-cmsis-nn-mcpu=cortex-m55```* Add CMSIS-NN decorator to dependent tests	3
[WINDOWS][MSVC] Fix MSVC warnings (#6450)* [WINDOWS][MSVC] Fix MSVC warningsThis PR fixes various warnings bought by MSVC.TODO: deprecate `__tvm_main__` symbol and updatetestcase so windows works as normal.* Fix unicode problem in data_layout	5
[Hexagon] Add hexagon_posix.cc to TVM/RT sources in the right place (#5346)This file was added before the variable with TVM/RT was initialized.The initialization overwrote the addition.	1
Fix dmlc-core path in nnvm Makefile (#1829)	2
[RELAY]reshape_like (#1950)	5
[CI] Re-introduce redirect follow and update hash for Boost download (#10343)Looks like we did need the redirect in (#10247), otherwise you get ablank redirect response and `tar` doesn't like that very much:```tar: This does not look like a tar archivegzip: stdin: unexpected end of file```	2
Fix compilation failure with latest LLVM (#1208)* fix problem with the latest LLVM* add if-defs to support older LLVMs	1
[COMMUNITY] @trevor-m -> reviewer (#7352)	3
[Tophub] Race condition fixed in folder creation (#7940)* [Tophub] Race condition fixed in folder creationTophub download routines switched to Pathlib's `Path.mkdir`in order to avoid race conditions in creation of folders	1
[DOCKER] Fix Dockerfile.demo_android (#6361)* [DOCKER] Fix Dockerfile.demo_android* fix	0
Add CacheItem2Schedule Extension (#338)* add CacheItem2Schedule extension* fix lint* move function position* make cache item visible to frontend	1
[RUNTIME][VULKAN] Seg fault in WorkspacePool's destructor (#5632) (#5636)* [RUNTIME][VULKAN] Seg fault in WorkspacePool's destructor (#5632)* fixed this issue by changing WorkspacePool's destruction order* make line < 100 charactors long	1
[VTA][TSIM] parallel TSIM hardware compilation with macOS and debug support (#3797)* [VTA][TSIM] parallel hardware compilation with macOS and debug support* simplify	1
[RELAY][FIX] Fix hang in MergeCompilerRegions (#5227)For certain network topologies, MCR could hang.This patch fixes that case.Change-Id: I3edd8a8a6b452b2b838b777720adea22a3b995b4	4
[AutoScheduler] Support early_stopping per task (#7377)* [AutoScheduler] Support early_stopping per task* address comment* fix test* Update python/tvm/auto_scheduler/task_scheduler.py* Update python/tvm/auto_scheduler/task_scheduler.py* trigger ci* trigger ci	5
[Vulkan][Runtime] Uniform buffer bugfix, minor cleanup (#7966)* Bugfix, missing decoration on uniform buffer arguments.Caused segfault when running on NVidia GPUs, with models that requireduniform buffer arguments for constants.* Updated test_target_codegen_spirv.py to use @tvm.testing.requires_vulkanPreviously, these tests would show success if USE_VULKAN=OFF.  Now,they correctly show that they are skipped instead.* Minor cleanup on the vulkan runtime.- Explicitly require int64 support at device creation time, since the  TVM-generated shaders require it.- Allocate an appropriate pool size for the buffer inputs, including  both uniform and storage buffers.* [Vulkan][Tests] Merged test_target_codegen_spirv.py into test_target_codegen_vulkan.pyCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Runtime] Enable set_input_zero_copy in GraphRuntime (#3416)* Enable set_input_zero_copy in GraphRuntime* Fix LoadParams* Fix* lint* Fix remote context issue* Fix* Remove LOG* Remove unused variables* Add tests* works* More test scenarios* make it simpler* Remove unnecessary changes* Address comments* More comments* Address comments* Fix build	0
fix (#6902)	0
[COMMUNITY] Xiyou Zhou -> reviewer (#9361)	3
[Runtime] Make runtime compatible with android ndk api 15 (#2446)	1
[OP] Initial Stucture of Op Library (#198)* [OP] Initial start of op library* add gtest	3
[SCHEDULE] More reliable bound inference on threading. (#84)	5
[QNN] Lowering for Depthwise Convolution. (#4351)	5
[QNN] InferType changes that missed CI. (#3779)	4
Change the meaning of conv3d_transpose output_padding to match conv{1,2}d_transpose (#6065)* Change the meaning of output_padding to correspond to conv{1,2}d_transpose* Fix long lines* Fix the relay test* Add missing doc.* fix size ordering problem	0
[FFI] Fix global free destruction (#985)	0
[iOS] Add tracker support into ios-rpc application (#7876)* [IOS-RPC] Missprint in flag valueSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] custom_dyld up commit id. Fix mem leakSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] build without schemeSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Add tracker support into ios-rpc appAlso containes:* Links with tvm_runtime.dylib* Minor improvements from UX perspective* Add cli args support* Add caching for url/port/key attributesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] lint fixSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Uniform serversAlso:- Disabled bit-code- Enabled ARC- Use custom DSO loader by default- Single button to connect/disconnect- Add verbose flagSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS_RPC] Min changes. Fix warningsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Fix review commentsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix RPC connection to tracker* Fix typo* Fix build for local developer profile* Add tvmrpc xcode scheme* Revert unnecessary change* Remove old mechanism of reloading libs* Display ip and port for PureRPC mode* Update tests* Remove tvmLauncher from ios_rpc* Add updating tvm_build_dir in init_proj script* Update default bundle* Update README.md for ios_rpc* Fix lint* Apply comments* Rename PureRPC to StandaloneCo-authored-by: Egor Churaev <egor.churaev@gmail.com>	0
[QNN] Dynamic scale, zero point in qnn.op.dequantize (#6849)* add dynamic dequantize* register quantize and dequantize as opaque* make tests better* black* remove main fn* fix black again* move tests* fix import* fix import again* try again* fix import	2
[TOPI][PYTORCH]Logical & Bitwise operator support (#5341)	1
[ONNX] [Test] fix GRU modification and reduce tolerance for RNN tests (#8923)* fix high tolerance for RNN tests* random seed was added to GRU test reproductionCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
Support CombinedNMS in TF frontend. (#7520)	1
[Matmul] Add matmul op (#8234)* Add Matmul Op* Recover DenseAttrs* Add grad for matmul & some update* Update matmul cuda default schedule* Add blas support for matmul* Lint fix add update doc strings	2
update gpu and cpu (#8853)	5
[tvmc] Introduce 'tune' subcommand (part 3/4) (#6537)* tvmc: introduce 'tune' subcommand (part 3/4) * introduces a subcommand to drive auto-tuningCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>* [tvmc] address code review comments* adjust --min-repeat-ms default value logic* re-arrange rpc arguments to be --rpc-tracker=hostname:port and --rpc-key=str* use a local reference of the tvmc logger* add --target-host, default to llvmCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>	5
Small refactor for clarity in arraycopyfromto (#960)	4
[docs] microTVM model training tutorial with Colab support (#10921)* First draft of micro train tutorial* unit test code* Fix obvious formatting issues* Linting* Proof of concept showing that "Open in Colab" is possible* Make test Python script more readable* Fix formatting* Ready for review* Import pyserial only when neededChanges from code reviewUse official sphinx-gallery repoCorrectly specify versionImport pyserial only when necessary* Add warning to ignored listTry to avoid throwing warningFix linting, try verbosity filterTry adding to ignore fileRemove fix attempts* Grammar fixes* Address code review commentsInclude full git hashes* Rerun tests* Rerun again	1
[FRONTEND][TENSORFLOW] Helper function to add shapes into the graph. Use tmp folder for model files and clean it. (#1697)	4
Use LIB_SUFFIX (#526)	0
[REFACTOR] tvm.hybrid -> te.hybrid (#5223)Rationale: The current hybrid module is more aligned with the te part.We might consider add a new varient of hybrid script that support the unified IR later.This refactor paves for the potential later changes.	4
Update have_int8 condition to run on compute capability 7.x devices (#4214)	1
[topi][CuDNN] Removed requirement for GPU from topi conv2d_cudnn.cuda and conv3d_cudnn.cuda (#8276)Previously, `conv2d_cudnn.cuda` would use cudnn's benchmarkingfunction to select a forward convolution when `cfg.is_fallback`, and`conv3d_cudnn.cuda` would use cudnn's benchmarking at all times.After this commit, both expose the cudnn algorithm choice as anoption.  If `cfg.is_fallback`, the local device will be benchmarked ifpresent, otherwise will select a default cudnn implementation.In the future, to better support RPC use-cases, the fallback configshould be based on cudnn-specific parameters saved in the Targetobject.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
adding vvchernov to contributors file (#11649)	2
[FIX] disable cuda test for argwhere (#7042)* disable cuda test for argwhere* Fix lintCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	0
[TOPI] Raise exception group_conv2d_nchw not supported (#3195)	1
Fix cuda nms handling of additional per box features (#7483)	1
[Tutorial] Fix vta vision detection tutorial 'sphinx' style error. (#9279)Issue:   Some bash code in this tutorial does not get syntax highlightingbecause of the format errors.Solution:   Fix the 'sphinx' 'rst' style error.	0
[WIP] [Relay] [NNVM] [Frontend] implement MaxPool-8 and MaxPool-10 (#3114)	5
[RELAY][PASS] FuseOps, fix input fusion rule for conv2d (#2110)	0
fix error message (#90)* fix error message* fix	0
[CUDA] Do not emit vector load on unaligned base offset (#9731)* [CUDA] Do not emit vector load on unaligned base offset* fix alignment check condition* black* improve test* improve the vectorization condtion to avoid error in yolo5 (thanks to vinx13)* replace coeff != 1 check by coeff % lane == 0	0
support matching attributes with more complext objects (#8240)	1
[ARITH] fix zero iter bug in arith (#8494)* fix* black	0
[ci] Fix aws s3 cp command in the Jenkinsfile (#12341)Looks like #12332 broke [the CI](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/3992/pipeline/389) due to a misuse in the `aws s3 cp` command. This PR fixes that.	0
WIP: Add how_to readme to install tvm with nnpack support (#610)* feat(docs) add how_to for tvm install with nnpack support* feat(docs) change python package paragraph* feat(doc) remove unsure sentence* add comments on nnpack usage vs TVM* remove mxnet nnpack tips for nthread change	4
[MetaSchedule][UX] Convenient Object Creation (#12643)This PR introduces a set of `.create` methods making it easier to createMetaSchedule objects.For example:```pythonms.database.JSONDatabase(...)ms.database.create("json")ms.runner.RPCRunner(...)ms.runner.create("rpc")```Besides, this PR allows `JSONDatabase` to be created via `work_dir`:```pythondb = ms.database.create("json", work_dir="/path/to/db/")db = ms.database.create(work_dir="/path/to/db/")  # or even simpler```	5
[uTVM] Initial BYOC support with c-source module (#6950)This commit mainly introduces a byoc c-source moduleexample to uTVM. Moreover, it carries certain modificationsto the example codegen_c external module generator codeto generate utvm friendly c-source.Change-Id: I09f3a42017d518dd5b6c89e3fe0a0332b80088b0	4
[Relay] A Normal Form Canonicalization (#2251)	5
[Compilation Warning Fix]Enum constant in boolean context (#365)The below compilation warning is fixed.In file included from /mnt/D_DRIVE/work/nnvm_8_feb/dmlc-core/include/dmlc/any.h:16:0,                 from include/nnvm/./base.h:11,                 from include/nnvm/graph.h:15,                 from src/compiler/fold_scale_axis.cc:6:src/compiler/fold_scale_axis.cc: In function ‘nnvm::Graph nnvm::compiler::FoldScaleAxis(nnvm::Graph)’:src/compiler/fold_scale_axis.cc:155:39: warning: enum constant in boolean context [-Wint-in-bool-context]     CHECK(kind == kPassTroughFirst || kMulConsumer);                                       ^/mnt/D_DRIVE/work/nnvm_8_feb/dmlc-core/include/dmlc/./logging.h:110:9: note: in definition of macro ‘CHECK’   if (!(x))	5
[PASS][RELAY] polish pass infra (#3319)	5
fix comment/doc in TensorLoad (#3646)	2
Fix (2/2) [TOPI] conv2d schedule code (#3648) (#3717)* Fix the tile_rx and tile_ry issue.    Note that this patch depends on pull request #9 in tvm-distro.	0
[Relay] Convert a fake quantized or QAT graph into QNN ops (#8126)* Convert a fake quantized or QAT graph into qnn ops* fix pylint* fix typos* use an identify function for some ops* rename the pass from quantize_fake_quantization to fake_quantization_to_integer* add definition for affine	5
[ci] Add sccache to remaining Docker images (#10751)* [ci] Add sccache to remaining Docker imagesThese just need sccache installed and available on the `PATH` to startusing it in CI* Remove docker/ from glob skip listCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix the runtime raise error (#5586)	0
[BUGFIX] Change debug_runtime to represent times in seconds internally (#7227)* Add FrontendTestModule, a Module which can have Python functions.* fix units and use of scientific notation in debug_runtime variable names* remaining updates to formalize debug_runtime returns time in sec* Add test for debug runtime output* black format* git-clang-format* pylint	1
[ci] Add tests for PR linter (#12680)This adds some checks for the current usages of the PR linter and fixes the case where the script would error uncleanly when a PR body was `null`.	4
[M3c][MetaScheduler] Add ReplayFunc Search Strategy. (#9799)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Add ReplayFunc and EvolutionarySearch strategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix optional task name.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Remove extra files.* Fix things.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[Hexagon] Less aggressive adb state clean up (#10909)* Only remove port forwarding applied in a sessionto avoid affecting global adb state.* Send SIGINT to attempt to allow remoteserver to cleanup and undbind port indeconstruction* Only attempt to forward ports not in use byadb or the system.	5
[WIP][Frontend] Scala/Java package (#176)* JVM package skeleton* [JVM] link libtvm.so and list function names* [JVM] Function & NDArray skeleton* [JVM] TVMFuncCall in JNI* [JVM] handle string arg in TVMFuncCall* [JVM] get module function* [JVM] entry function for Module* [JVM] construct Module from function return value* [JVM] TVMContext, TVMArray attributes* [JVM] NDArray from / to java array* [JVM] load so and compute on cpu* [JVM] move PackedFunc to individual modules* [JVM] assembly package & native library loader* [JVM] unit test & codestyle check settings* [JVM] NDArray from & to different dtypes* [JVM] NDArray from native double array. Add linux-cpu profile.* [JVM] modify Makefile* [JVM] add linux-x86_64-gpu profile* [tvm4j] delay load libtvm_runtime.so* [tvm4j] refactor to pure java* [tvm4j] remove scalastyle-config.xml* [tvm4j] remove link HalideIR, remove Shape, remove scala binary versions* [tvm4j] only allow convert from/to same type array* [tvm4j] make NDArray api more readable* [tvm4j] refactor for c api* [tvm4j] add Jenkins tests* [tvm4j] fix duplicate Dockerfile cmd* [tvm4j] fix ut script filename* [tvm4j] add module load tests* [tvm4j] add javadoc, remove types package* [tvm4j] fix test script* [tvm4j] remove ut temp dir* [tvm4j] fix missing package types* [tvm4j] java code style check* [tvm4j] fix java lint* [tvm4j] downgrade checkstyle plugin for JDK7* [tvm4j] add stylecheck in jenkins tests* [tvm4j] specify source file encoding* [tvm4j] lazy init function; add Function.call() api; allow manully release Module,NDArray,Function* [tvm4j] fix ModFree* [tvm4j] cache Function in API	1
[llvm] fixed issue with llvm 5 vs 6 (#1167)	0
[BUILD] rename build.py to avoid conflict name of build (#284)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed	0
[Adreno][OpenCL] Get rid of extra memory copy (#12286)* Add annotation pass for device_copy where we get buffers but expecttextures* Fix issues with running device_copy* Get rid of extra memory copy* Fix build after cherry-picking* Fix lint* Fix CI* Apply commentsCo-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>	0
Deprecate NNVM warning msg (#4333)	2
[CI][TEST] Temporary disable nmsv4 test (#6151)	3
Skip flaky tensorflow tests (#10276)See #10275cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[MetaSchedule] Enable AutoTVM-style template-based search space (#10461)* [MetaSchedule] Enable AutoTVM-style template-based search space* Fix lint* suppress mypy	0
fix flaky test (#11663)	3
Update index.rst	5
[TIR][REFACTOR] std::string -> String Migration in TIR nodes (#5596)* [TIR][REFACTOR] std::string -> String Migration for Var node and SizeVar Node* update json_compact.py	5
[RELAY][FRONTEND] Initial MXNet frontend support. (#2163)	1
[release] Add script to gather PRs for a release (#11987)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[AutoScheduler] Use VM to extract tasks for dynamic models (#7173)* use VM for dynamic shape* make it work* add test* finalize* finalize* format* address comment* comment* improve task extraction	4
fix (#9205)	0
[QNN][Legalize] Specialize for Platforms without any fast Int8 arithmetic units. (#4307)	3
[DOCS] Phase out nnvm tutorials (#2783)	2
fix mxnet amalgamation (#96)	0
dockerfile cpu changes (#2191)	4
[TOPI] Add proper scheduling for dense on CUDA (#3923)* add proper scheduling for dense on CUDA* add fallback config and fix unit test* fix corner cases* refactoring* fix bias and add testcase* let fusion happen	3
[LLVM] Do not use x86_vcvtph2ps_256 intrinsic with LLVM 11+ (#5267)This intrinsic was removed in LLVM 11.	4
support adb-shell style cpp_rpc (#8223)* support adb-shell style cpp_rpc* fix review problems,  #8223* add comment & use /data/local/tmp dir in shell terminal case* fix spelling errors* fix spelling errorsCo-authored-by: rqg <ranqingguo90@qq.com>Co-authored-by: rqg <ranqingguo318@gmail.com>	0
Increase bss section size. (#5660)* Likely broken in PR 5590.	1
[RELAY] Add structural hashing for Relay (#1977)	1
Update link (#6838)	2
[Relay] Pass manager (#2546)* initial commit* add python frontend and module tests* add unit tests for function pass and optimize interface* add ExprPass* remove PassState and pass context for run* add required_passes* return module* remove move* fix minor reviews* remove optimizer, optimizer->pass_manager, make pass a the base class of all* remove deleted files* move resolvedependency to sequential pass, use ir_pass namespace* add todo* add disabled passes in sequetialpass* fix minor* fix currying doc* remove pass_kind from passnode* remove pass kind from test* fix doc* fix per @tqchen's comments* remove pass_manager.py create separate classes* simplify pass_func* inline using passfunc* update doc* disable test_quantize_pass for now* create PassInfo class to contain the meta data* flatten passinfo for interface* retrigger ci* remove required method* make Pass python class lighter* create pass -> decorator* make the api consistent for all classes	1
Optimizing autotvm task extraction speed (#4138)* Optimize task extraction speed* correct pylint errors* Delete unused function* remove unnecessary argument* resolve code review comments* corrent cpp lint errors* remove one more graph_json return value* fix test bugs	0
fix bug for rdiv	0
[MetaSchedule] Implement ScheduleFn as a C++ class (#12513)	5
Softmax operator migrated to topi (#366)* softmax migrated and test added* pylint error fixed* pylint error fixed	0
[ci] Re-run failed tests on failure (#12055)* [ci] Re-run failed tests on failureThis uses [pytest-rerunfailures](https://github.com/pytest-dev/pytest-rerunfailures) to retry failed tests. This should help alleviate flakiness for issues like segfaults in ethosu tests or flaky numerics. This obviously isn't as good as fixing the tests themselves, but it's an easy way to fix the most pressing issue (the cost of developer time to wait for, check, and re-run CI) while keeping the signal from the times the tests do fail consistently.* Remove changes to pytestCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[DOCKER] Pin flatbuffers checkout to the last release tag (#2823). (#2879)	2
[DOC] Fix typos in tutorials (#287)	2
[DOC] Codebase walkthrough with vector add example (#2273)	1
Add unidirectional sequence lstm (#11183)* UnidirectionalLSTM added* fixed missing import* fixed pylint warnings* black formatted tflite.py* corrections according to reviewer comments* fixed black formatting* just to trigger the CI again* assertion now tests that there are exactly 24 input tensors.* black formatted tflite.py* added explanatory comment regarding unused imports* removed unused import* nothing* nothing* added some details in a comment about the differences in unbind regarding to the version in common.py* improved comment on unbind* fix of black issue	0
Fix a issue when running with graph_runtime_debug in python (#2271)* fix a issue when running with graph_runtime_debug in python;* add support to `debug_get_output` in python;* comply with the linter;	0
Port from_nnvm to NNVM as to_relay (#2144)	5
[FRONTEND][TENSORFLOW] Support AttrValue that has different types of value in a list (#2177)	1
[RELAY] Add resnet-3d & Update network definitions for NHWC layout (#5945)	5
[Torch][Quantized] Fix converting serialized quantized models (#5839)* [Torch] Fix converting serialized quantized models* clean up dtype check* comment clean up	4
[BUILD] Upgrade build system to default python3 (#1260)	5
[FIX] Fix cublas batch matmul (#6715)* Update batch_matmul.pyUpdate batch_matmul.py* fix	0
[Frontend][Tensorflow] Sparse dense matmul adjoint option added (#7267)* [Frontend][Tensorflow] Sparse dense matmul adjoint option added* [1] Review comments handled* [2] Review comments handled* [3] Review comments handled	0
[TIR] Prevent loop binding over-simplification (#11578)@vinx13 @jinhongyii and I observe a recent regression on TVM mainline: over-simplification in`Schedule.split` leads to information loss that negatively impacts search space generation.**Impact.** This affects common operators like `softmax` and even simpler reductions.**Example.** Consider splitting a simple reduction loop:```python@T.prim_funcdef main(    A: T.Buffer[2, "float32"],    B: T.Buffer[2, "float32"],    C: T.Buffer[(), "float32"],) -> None:    for i in T.serial(2):  # <= split `i` into `i_0` and `i_1`, where `i_0` is a trivial loop        with T.block("C"):            k = T.axis.reduce(2, i)            with T.init():                C[()] = T.float32(1)            C[()] = T.min(C[()], A[k] / B[k])```Splitting loop `i`  by factors `[1, 2]`, we get:```python@T.prim_funcdef main(    A: T.Buffer[2, "float32"],    B: T.Buffer[2, "float32"],    C: T.Buffer[(), "float32"],) -> None:    for i_0, i_1 in T.grid(1, 2):        with T.block("C"):            k = T.axis.reduce(2, i_1)  # <= i_0 is not part of the binding,                                       # so the system cannot tell if i_0 is a reduction loop            with T.init():                C[()] = T.float32(1)            C[()] = T.min(C[()], A[k] / B[k])```In this case, loop `i_0` will be considered as a spatial loop, even it’s the outcome of splittinga reduction loop. However, if we change the factors from `[1, 2]` to `[2, 1]`, loop `i_0` becomesa reduction loop. This means the loop iteration property depends on the loop extent.**Why is it problematic**? MetaSchedule has an assumption: extremely seldomly, a loop extent wouldimpact the iteration property of the loop itself, i.e. no matter the extent is 1 or 2 or anything,the fact that the loop is a reduction loop should rarely change.As an example, `Auto-Bind` finds the outer `k` spatial loops, which are fused together and bound tothread axis. In the trace, the number (`k`) of the outer loops has to be a constant.However, if Auto-Bind thinks there are `k=3` outer loops to fuse during search space generation,where the last loop happens to be a reduction loop with extent 1, as shown below:```pythonfor spatial_loop_0 in range(...):  for spatial_loop_1 in range(...):    for reduction_loop in range(1):  # <= Auto-Bind mistakes this loop as spatial, because extent==1```During evolutionary search, the extent of reduction_loop will change and become larger than 1.In this case, the binding strategy will consistently fail because it considers fusing `k=3` loops- which means the entire search strategy will fail with almost no valid candidates.Thanks @MasterJH5574 for figuring out the root cause of the issue,and @jinhongyii for valuable pointers to the right fix!	0
[Doc][Relay] Add VM doc (#3188)* [Doc][Relay] Add VM doc* Add Apache header* Apply suggestions from code reviewCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>Co-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>Co-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>Co-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Junru's comment* More fix* More fix* More fix* last fix* Apply suggestions from code reviewCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Apply suggestions from code reviewCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Add code links* Remove unused bp* Update docs/dev/virtual_machine.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Explain TODO* Yong's commentCo-Authored-By: Yong Wu <55wuyong@163.com>* Comment	2
Add tophub for x86 (#1955)	1
[DOCS] Document cloudpickle dependency in tutorials (#7049)	2
Fix tvm.target.generic_func runtime detection (#4910)	1
[CODEGEN] Multiple parallel in one launch (#399)	5
Install rust for all users (#1856)	1
update topi schedules (#1556)	5
[clflush] Enable x86 cpu cache flush (#5914)	0
[ci][docker] Tag tlcpackstaging images to tlcpack (#11832)See #11768, this PR changes the deploy workflow so that after a successful build with fallback images (see #11775), they get moved over to tlcpack automatically. Since the images are moving repositories we can't just rename the blobs in docker, so there needs to be a full `pull -> tag -> push` for each image	2
[REFACTOR] Unified IR base types. (#4616)This PR moves a few base types from relay to the ir sub-folder.These types will serve as a common type system across the stack.Notably, we want to be able to use the same FuncType for all function signatures.I tried to make a minimum move to bring the necessary dependencies for a FuncType.We can discuss what additional things we want to move as a follow-up.Notably, because the TensorType will have a dependency on low-level Expr,we will need to break the type.h into two files and introduce atensor_type.h(or leave them in relay for now).	2
[runtime] Add Metadata classes for AOTExecutor (#10282)* Add new Metadata classes and base implementation. * These were autogenerated in the original PR, but checking them in   as plain code until we can revisit the auto-generator approach.* address masa comments* Add documentation per Manupa's comments, and move kMetadataVersion namespace.* remove get_name function, used for debugging* clang-format	0
[FIX] Remove leftovers from check_correctness (#7272)* [FIX] Remove leftovers from check_correctness* remove unused numpy import	2
[FRONTEND] Correct the use of `concatenate` operator (#181)* Correct the use of `concatenate` operator* Optimize	1
add dependency of compilation with LLVM (#4117)	1
[Doc] Fix Relay pattern rewrite (#8425)	0
[TEXPR][PASS] Fix thread all reduce to avoid write after read hazzard (#2937)	0
Add support for multiple OpenCL platforms (#1345)	1
[TOPI] add conv2d_transpose_nchw (#586)	1
Enable shape hints during infer_shape pass (#107)* enable shape hints during infer_shape pass* fix comment	0
[BugFix][Opencl] Explicitly cast min/max operands (#9374)* [BugFix][Opencl] Explicitly cast min/max operands* enable test_opencl_max	3
[Tutorial] Fix formatting, grammar, dead link (#9281)* tutorial: preprocess.py: Fix leading whitespaceThis fixes the indentation of metadata in `preprocess.py` in the TVMC tutorial, removing the leading whitespaces in the HTML rendering[^1].[^1] https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html#preprocess-py* tutorial: Add missing code block escapes* tutorial: Grammar fixup* README.md: Fix link to introductionCo-authored-by: Martin Kröning <martin.kroening@neclab.eu>	2
[DOC] Update ssd doc to avoid confusion. (#3677)* intel graphics conv2d bugs fixed for inception_v3* intel conv2d api updated, nn input size 4 condition added* review addressed* move conv_tags to attributes* ssd doc updated* address comment	1
fix hardware-makefile for osx, bugfix chisel-RegFile, and rename driver (#3371)	2
[TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue	0
[Arith] Support dtype promotion in TIR comparison expr creation (#10584)	1
[CUDNN] Support gradient kernels (#9986)* Dgrad nchw, nhwc, fp16 workingcommit 426e5dca446a27da49270f45171b58f1bfa21fa9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:48:53 2022 +0900    blackcommit 211a58b80f4d0f0b5b0230720e41f35e50cb1eafAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:43:52 2022 +0900    fp16 also workscommit c2a34d473b063873628bff00e51a44cd8e4d0e4fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:36:36 2022 +0900    nhwc test also workedcommit c0609ab147fef30c230a94d16b6c1ba35f7dd9c0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:21:23 2022 +0900    nchw test workedcommit 2bf68c72763708151e9f49f09916a210b2547be8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 10:41:35 2022 +0900    add test stubcommit c86b1288d5e371f12cba4e1b1866966cb9264401Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 10:32:09 2022 +0900    add python definition stubcommit 3166952f9673376801bf4b5b39eeb6f89452f30aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:57:18 2022 +0900    bwd filter compiledcommit e311ba3d05c5f9424ecb952cb5a520ce81a0828aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:27:55 2022 +0900    dgrad compiledcommit 47f35beb5eeeb7cbf9f6ec7cf8f5c80c65e8da46Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:16:43 2022 +0900    add dgrad stubcommit ebed032d15b1c3895f541c46ce5d80b6dd769034Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 17:01:56 2022 +0900    cpplintcommit 834f54a8c13512130e7d91ca0f54268dc06c5481Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 16:55:58 2022 +0900    remove cudnn get outputcommit dcbd9c95fdb8ffef9db9c2350430b270461a31c3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 16:28:07 2022 +0900    more refactorcommit 146464e8496fff972bdb1687c4e9d432fe3278d5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 15:57:35 2022 +0900    Introduce SetConvdescriptors to refactor cudnn/conv_forward.cc* add python function for cudnn wgrad* adding wgrad test* black* wgrad nchw and nhwc worked* remove bwd algo name stuff* compute output shape properly* swap arg order in wgrad* add kernel size arg in test* black* cleanup* more fix* fix dgrad test* support running relay conv2d_backward_weight directly with cudnn* black* refactor reference function to support nhwc* removed unused function* lint* enable offloading conv2d_transpose to cudnn dgrad* relax tol* name fix, remove print	4
[Pattern] add optional pattern to C++ syntatic sugar (#10872)Add an optional pattern syntatic sugar to the C++ pattern language to match python. While writing the tests, I noticed that the const definitions on the methods weren't quite right (I couldn't do nested syntatic sugar calls), so I fixed that as well.	0
[REFACTOR][TIR] Introduce PrimFuncPass. (#5139)* [REFACTOR][TIR] Introduce PrimFuncPass.- Introduce PrimFuncPass- Convert one pass to the unified Pass API.* Address comments* Fix comments	0
[RUNTIME][NDArray] Allowing External Libraries to Subclass NDArrays (#2613)	1
Update README.md (#9798)	2
[ROCM] MIOpen contrib for convolution kernels (#722)* fist working miopen support* do FindFwdAlgo during build time* fix lint* update doc string* import topi after checking if rocm is enabled* add miopen namespace* fixed descriptor overwrite bug* add use_miopen option* fix lint* better miopen option handling* fix typo* fix options handling	0
init (#3476)lintupdateaddress commentcomment out breaking test	3
[Fix] Add more pad_mode support for onnx converter (#4029)* [Fix] Add more pad_mode support for onnx converter* robustness fix	0
[NNVM][TEST] Test against numerical grad (#1505)* [NNVM][TEST] Numerical gradient testing* [NNVM][TEST] Make some tests a little faster* Fix the failing test_top_level3* Target exclusion for the check_function* Try to ignore singularities* grad_input_vars now can't contain shapes* Don't pass unnecessary grad_input_vars to check_function* Multiple outputs; fixes; testing of check_function* Use numerical_grads_params to pass parameters to numgrad checker* Fail when no action is requested excplicitly* Pass additional params to functions* Silence the linter issue* Simplified numgrad checking* Improved docs for check_function* Fixed the error message when no dtype is provided* Several fixes* Tests with shape/dtype inference for inputs* Don't check dense's grads on cuda* Raise an error if output dtypes haven't been inferred* Moved shape/dtype inference into a separate function; use float32 as fallback* Remove redundant dtype=float32* Fix multiple outputs* Use check_function in the rest of the test_top_level1	3
[ETHOSN] Add support for Requantize (#12384)This commit adds support for the requantize operator for the Arm(R) Ethos(TM)-N NPU.	1
Add ACL testing to the CI for AArch64. (#7122)Add testing for ACL to the CI for AArch64. A PR followsto add this to the Jenkinsfile once the docker changes land.We also need a separate script to run the tests as the fullintegration tests are currently broken.	3
Add bindings for StaticMemoryPlan and DensePackAttrs (#9034)* add missing python binding for DensePackAttrs* make bindings for memory planning usable in python	1
[ci][docker] create Dockerfile.ci_riscv (#12230)Move RISC-V related content from ci_qemu to ci_riscv	4
[CODEGEN/RUNTIME] Cross Compile Test (#160)	3
overview (#36)* overview* fix	0
[CMSIS-NN] Separated symmetric and asymmetric padding tests for Conv2D (#9963)	3
[TensorIR] GetProducer, GetConsumer (#506) (#9464)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[TOPI] Example for convolution in GPU (#212)* [TOPI] Example for convolution* update conv ex* fix submodule HalideIR* update conv impl* python3* minor fix* fix pylint error* Add test code* x* fix* fix* move python helper function into topi.testing* fix pylint	0
[Community] Add reviewer Balint Cristian (#3935)	1
Fix error when compile tvm with latest llvm14git (#8682)	3
cuda imagenet inference benchmark timing updated (#158)	5
change the doc to reflect previous code change (#8970)	4
Fix issue relating to serialization of reducer (#282)	0
[QNN][Relay] Calling Dialect passes from inside Relay Build API. (#3971)	4
[PASS]unroll loops with extent=1 (#2027)	4
[PyTorch] Fix neg indexing issue for `aten::flatten` (#10796)	0
minor fix (#313)	0
[UnitTest] Parametrized test_conv2d_int8_intrinsics (#9143)Parametrized it to get more detailed information while debuggingfailures in https://github.com/apache/tvm/pull/9091, but isn'tsemantically part of that PR.	0
[UnitTests] Parametrized test_topi_argwhere.py (#11651)Refactored while debugging breakage of tests inhttps://github.com/apache/tvm/pull/11646.  Submitting as a separatePR, as it isn't necessary or related to the primary changes in thatPR.	4
[AutoScheduler] Check duplicated names in the compute dag (#6973)* [AutoScheduler] check duplicated names in the compute dag* fix lint* fix pooling* fix pooling	0
[TOPI][RELAY][ONNX] Scatter ND (#7927)* passing topi tests* passing relay tests, needs better shape checking still* support ONNX operator* add shape checking back in* fix lint* update docstring	2
Add AArch64 frontend dependencies to Dockerfile (#10676)Preparing to run more of the frontend tests on AArch64 by installing all the dependencies in the container	3
[Meta Schedule][M3a] Traced Schedule (#8623)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[TFLite] Strided slice handling of shrink_axis_mask improved (#6998)* [TFLite] Strided slice handlig of shrink_axis_mask improved1. Added removal of dimensions if result is a scalarto mimic TensorFlow behaviour. E.g.:    tf.strided_slice([1,2,3], [0], [1], [1], shrink_axis_mask=0)    <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>    tf.strided_slice([[[1,2,3],[4,5,6],[7,8,9]]], [0, 0, 0], [3, 3, 3], [1, 1, 1], shrink_axis_mask=7)    <tf.Tensor: shape=(), dtype=int32, numpy=1>2. Added extra check to assert_allclose to check shape equalitiesas np.testing.assert_allclose() does not distinguish between cases like:    np.testing.assert_allclose(1, np.array(1))    np.testing.assert_allclose(1, np.array([1]))    np.testing.assert_allclose(np.array(1), np.array([1]))* unit tests fixed	0
Add schedule for conv3d NDHWC layout (#4775)	1
[Rust] Fix the existing test cases before refactoring.  (#5122)* Fix up the final pieces* Tweak build.rs	0
[TIR] Allow converting `BufferRegion` to vectorized `BufferLoad` (#12420)	1
[Bugfix][IR][ATTRS] Fix AttrEqual for Array and StrMap, double (#5054)- Use fuzzy comparison for double.- Removed the hack for BatchNormAttrs and DictAttr.Also removed a warning from text printer printing.	2
[Torch] Better support in-place variant of ops (aten::relu_ etc) (#9851)* [Torch] Better support in-place variant of ops (aten::relu_ etc)* add warning* black	2
[Topi & Relay] Add quantization support for the vision transform model in GPU (#7814)* Add cuda batch matmul int8 support for quantized vit model* Fix for combine parallel pass with dense and batch_matmul* Reformat based on lint* Add plevel & update the file download method	2
timing closure fix for default VTA config (#1489)	5
Add BatchNormAttrs Rust bindings (#6678)	1
Fix fusion bug when call symbol that is not an operator. (#2630)	1
Add support for overloading comparison operations in relay (#2910) (#3168)	1
Fix the FInplaceIdentity (#2572)	0
Allow microTVM Reference VM to be launched when TVM is a submodule. (#7854)	1
[AUTOTVM] Misc bug fix (#1467)	0
Fix flaky NMS test by making sure scores are unique (#9140)	1
[Relay][Topi][Op]Advanced indexing (#6388)* Add Relay adv_index op* Support single index tensor dynamic shape* Support more dynamic index* Fix lint* Minor fix for comment* Fix lint* Fix lint* Fix test* Fix	0
Replace 0.0.0.0 with 127.0.0.1 for client connections (#7766)* Rename references to 0.0.0.0 to localhost. Also change references to 127.0.0.1 to localhost so that all references are consistent. 0.0.0.0 is not the same as localhost.	4
[AutoTVM, Auto scheduler] Always use VM compiler for task extraction (#9069)* Always use VM compiler for task extraction* remove unused import* update task weights reference* Call op weights update callback from VM compiler* Revert "update task weights reference"This reverts commit 61ca552da3c25ed239e8c84c599a3f0f7923c0ff.* add doc	2
[BUILD] Enable cudnn in gpu build (#333)	0
Fix compilation on XCode 10 (#2731)	0
Fix init_proj.py: Team ID expected (#2824)	5
[TOPI][CUDA] Fix 0 valid boxes case for NMS when return_indices=False (#7700)* Handle 0 box case for return_indices=False case* Add unit test for mx NMS	3
[Relay] Prepare for new plan_devices.cc (part II) (#9130)* Prepare for new plan_devices.cc (part II)These changes came from changing https://github.com/apache/tvm/pull/9038 to usetvm.parser.fromtext instead of manual AST construction.- Demote FunctionOnDeviceAttrs to just a pair of DictAttrs entries so  that the parser will understand them on Function definitions.- Connect some special operators to their attributes so parsing understands them  at call sites.- Don't silently ignore attributes during parsing.- Implement OptFunctionOnDevice so won't add device annotations for kUnknownDeviceType.- Allow the parser to be given an initial metadata map to support examples which  need constants.- More DLOG -> VLOG conversions to reduce debug clutter.* [checkpoint] Keep existing ParseModule ffi to simplify rust bindings* [checkpoint] Address Christopher's comments.* [checkpoint] Andrew's comments from #9038* [checkpoint] Jared's comments from #9038* [checkpoint] Woops, forgot rename.	1
simplify expr in get_const_tuple (#795)* fix upsampling output shape* simplify expr in get_const_tuple	1
[ci][lint] Consolidate image lookup logic (#12206)This makes `run_docker` just pass through the image name to `bash.sh` without preprocessing it at all so that `bash.sh` can do its own determination of which image to actually use.	1
[ETHOSN] Use pytest parameterization for integration tests (#12688)Using pytest parameterization helps identify the particular parameter combinations that are failing for a given test. Additionally, it can be useful when parallelizing the tests. This commit makes sure that "trials" have been replaced by parameterization as well as completing a general cleanup.	4
[Relay] Allow partial virtual device annotations. (#12107)* [Relay] Allow partial virtual device annotations.Previously CompilationConfig::CanonicalVirtualDevice requiredthe argument virtual device to contain a device type. Howevernow that virtual devices may contain memory scopes that'sunnecessarily strict.With this change it is possible to write virtual deviceannotations with just memory scopes, and let PlanDevicesflow those constraints along with the usual device constraints.* - Make sure CanonicalVirtualDevice reuses FullyUnconstrained	1
[RUNTIME] Store nullptr PackedFunc as nullptr for better error propagation (#5540)	0
Correction in C++sample code. (#238)Variable declaration correction.	5
[TEST][KERAS] convert tvm output to channels_last format (#1733)	3
Few docs fixes (#2703)	0
Update README.md	2
[Hexagon] Add contrib tests for blocked conv2d and maxpool2d (#8960)* Add hexagon contrib tests for blocked conv2d and maxpool2d* Restructure based on review comments	3
Apply CPPLint to C++ Unit Tests (#8827)This change enables `cpplint` for the tests in `tests/cpp` and corrects any current linting errors. I had to use `NOLINT` in some of the PackedFunc tests due to a bug (see: https://github.com/cpplint/cpplint/issues/131) in CPPLint where `int(int)` is picked up as a cast rather than a nameless argument.	0
[Torch, QNN] Support dynamic quantization flow to enable importing quantized transformer models (#6782)* add stub and test* per channel quantize* calculate qparam correctly* import qbert working* support batched qdense* test batched input* fix mkl offloading of batch matmul* reduce range become True in torch 1.6* fix for 1.6* Revert "fix mkl offloading of batch matmul"This reverts commit cd90aa783688c68e1b12633eea4d2690d9e3a5a5.* fix merge* fix* lint fix* fix black* more black fix* fix version check for 1.5.1* disable assert on v1.4 (strange pytorch issue)* minor fix* use dequantizeCo-authored-by: masa <masa@pop-os.localdomain>	1
fix segfault when op is unset (#15)	1
[Relay] WithFields method for Call, Function, Var, TupleGetItem, If, Let, RefCreate, RefRead, RefWrite, Match, and Clause (#9569)* Implement WithFields for Relay exprs* lint	1
[ATTR] Introduce Integer container (#1994)	5
[Frontend]Add TensorFlow FloorMod (#4308)* Add tf FloorMod* Add floor_div/mod into topi and relay* Add to rst* Fix test	3
Add list attr recursive (#89)* Add list attr recursive* fix	0
[NNPack] Fix automatically cast fail on some platforms (#388)	0
[ONNX] [#8838] QLinearLeakyRelu contrib op  (#9063)* [ONNX] QLinearLeakyRelu contrib op* Add comment* jostle ci* jostle ci	1
Add shape backward inference (#58)	5
[CI] github_cc_reviewers: Catch all exceptions so all reviewers can be processed (#12578)In a recent change, `github.post` throws `RuntimeError` instead of `HTTPError` when the requested reviewer isn't a project collaborator. This prevents other reviewers to be added to the PR, for example, https://github.com/apache/tvm/runs/8001367110?check_suite_focus=true.This PR changes the caller to catch any exception so the execution won't be interrupted.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>	1
BUG #8013: Remove register_alter_op_layout example from dev/use_pass_infra.py (#9076)* BUG #8013: Remove register_alter_op_layout example from dev/use_pass_infra.pyThis tutorial registers a global layout transformation for conv2d for alltargets which is not well-formed. Later uses of conv2d in the tutorialspick that layout up then assert fail in the conv2d type-relation.Better would be to register a transform for an entirely fake target, butthat is beyond my current level of expertise.In general our use of sphinx/sphinx_gallery for running and rendering thetutorials is highly suspect since there is no inter-example isolation: - Examples using tensorflow will gobble up GPU memory and not give it back. - Any examples which use any of the (many!) global registration mechanisms   need to ensure the registrant is safe across all tutorials.I recall seeing a thread with the sphinx_gallery where they said they'd prefernot to work on process-level isolation, but it's probably worth pinging again.While digging into this I noticed we had a slicing cast in AlterOpLayout dueto a derived class of ObjectRef introducing virtuals. I moved the virtuals tothe corresponding Node classes. In this case we got away with it since theObjectRef happened to not get copied but we were on very thin ice.* [checkpoint] Woops, forgot there was an extra AlterOpLayoutI should have run locally, there goes 6hrs of CI.	1
[CI] Torch 1.7 update to mainline (#6828)	5
[RELAY][OP] conv2d, ShapeExpr->IndexExpr (#1798)	5
[Runtime] Only initialize required module (#5926)* init required modules* trigger ci* trigger ci	1
[TIR][Arith] Additional Simplifications Inside Conditionals (#11524)* [TIR][Arith] Use equality constraints in analyzerPreviously, constraints with inequalities were recognized and used forsimplifications by `ConstIntBoundAnalyzer` and `ModularSetAnalyzer`,but constraints with equalities were not.  This adds equality-basedconstraints.  (e.g. Inside the then-case of `if i==5`, the value of`i` is known to be 5.)* [TIR][Arith] RewriteSimplifier, apply literal constraintsPreviously, constraints were only checked within a `tir.likely`annotation.  After this change, constraints are used forsimplification of all boolean expressions.  (e.g. Within a conditional`if i==n`, the expression `(i==n) and (j==m)` can be simplified to`j==m`.)* [TIR][Arith] Do not apply literal constraints to BufferLoadIf a literal constraint relies on the contents of a buffer, theconstraint may not be assumed to hold.  This prevents the incorrectrewriting of `A[i]==n` to true within a `if A[i]==n` conditional, asthe value of `A[i]` may have changed.* [TIR][Arith] Use each independent constraints in RewriteSimplifierInside a constraint `if i==n and j==m`, both `i==n` and `j==m` may bereplaced with true, even in separate expressions.This commit uses a new internal utility function`tvm::arith::ExtractConstraints`, which breaks up a boolean expressioninto a list of true statements.  This may be used to reduceduplication elsewhere, such as `const_int_bound.cc` and`iter_affine_map.cc`.* [TIR][Arith] Check for negation of literal constraintsWhen inside a conditional of `i!=n`, in addition to the previousreplacement of `i!=n` with true, we can also replace `i==n` withfalse.* [TIR][Arith] Added unittests for new simplifications* Fix lint error* Fixed handling of negation of non-boolean types* Removed extra asterisk	4
[Minor] Fix typos in Ansor (#6425)	2
[Relay][Frontend][Tensorflow]Add conv2d_transpose (#4300)* [Relay][Frontend][Tensorflow]Add conv2d_transpose* add transformation from NHWC to NCHW to compatible with TVM conv2d_transpose implementation* remove 'dilations' paramater to compitable with TF1.3	2
[TFLite] Fix padding calculation in Transpose Conv (#9089)* [TFLite] Fix padding caculation in Transpose Conv* [TFLite] Fix padding calculation in Transpose Conv* [TFLite] Fix padding calculation in Transpose Conv* remove unused variables	1
Fix typo (#2839)	2
[Relay][Pass] Add ExtractOperators pass (#8996)* add extractor* extract to array* add comments* lint* Update tests/python/relay/test_analysis_extract_operators.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* op freqs* add comment* Update python/tvm/relay/analysis/analysis.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* oops* mixedmode visitor* oopsCo-authored-by: Cody Yu <comaniac0422@gmail.com>	5
Update README.md first init	5
[Pytorch] Disable failing tests on AArch64 (#12255)Change-Id: I234a4486a4a5ffac03d478e65cbfcee3efe30df1	4
[CMSIS-NN] Fix extension detection for CPUs (#10200)	0
Implementation of Common Subexpression Elimination for TIR (#9482)* Initial implementation of Common Subexpression Elimination for TIR (#703)The goal of this PR is to implement a Common Subexpression Elimination (CSE) pass for TIR, which aims at identifying redundant computations (both within statements and within expressions), and to replace them by a new fresh variable, introduced before the first occurrence of the redundant computation.Note that it does not only try to do commoning on full expressions, but it is also able to do it on subexpressions. For instance, if the program computes the expression (w+x) + (y+z) and the expression (w+x)+u, it will introduce the subexpression (w+x) into a new variable.If we want so, it will be easily possible in the future to make the notion of equivalence between terms more flexible, allowing for instance to identify expressions modulo commutativity (identifying for instance (x+y) with (y+x)), modulo associativity (identifying for instance (x+y)+z with x+(y+z)), etc. Replacing only the function bool EquivalentTerms(const PrimExpr& a, const PrimExpr& b) will be the only thing needed in order to do that. The typical way to rewrite it for such extensions would be to compute a canonical representant of a and a canonical representant of b and to then compare them with the strict syntactical equality.The main CSE pass is declared and implemented respectively in the files common_subexpr_elim.h and common_subexpr_elim.cc.The function Stmt CommonSubexpressionEliminator::VisitStmt(const Stmt& stmt) is a good entry point as it contains many comments about what the pass is doing.The general idea of this pass is that it tries to introduce at the current level (the current root) the computations that are redundant and which are possible to introduce there (they should only contain variables that are in scope). This notion of variables in scope is implemented with a context, which is a vector of pairs (var, MaybeValue). The context is not only used for checking that variables that appear in candidate computations are known at this point, but also for checking if a computation has already been introduced into a variable.For a greater flexibility in the future, there is a strong distinction already in place between :    - Syntactic computations, which are maintained in a hashtable which associates expressions (the computations already seen) to size_int (the number of times the computation has been seen).    - Semantic entities, which are obtained from the syntactic computations by merging equivalent computations (where this notion of "equivalent" is customizable). Semantic entities are stored into a vector of pairs (expr, size_int) where, again, the number is the number of times that expr or equivalent computations have been seen.The VisitStmt() method starts by computing the syntactic computations (implemented in an auxiliary analysis), then it merges equivalent computations to obtain the semantic computations. Then it sorts these semantic computations from biggest to smallest in order to always consider first the biggest computations. The rest will essentially be a loop over all these candidates, which will stay sorted.When dealing with a candidate computation, there are three cases that can happen:    1 - Rare case A variable in the context already contains this computation. This variable can't have been introduced by the CSE, as we would have performed the replacements at the same time (see case 2). So this is the case where the user himself (or the previous TIR passes) has written something like "let x = A in ...A...A...)"    -> In this case, we simply perform the replacements of A with x in the current result. These replacements are done by an auxiliary transform/Mutator, declared and implemented in replace_expr_selected.h and in replace_expr_selected.cc.    2 - Case where we need to introduce the current computation inside a new variable This is the case where all the variables used by the current computation are within scope (i.e. are present in the context) and where our internal heuristic/predicate tells us to introduce this computation into a new variable.    -> In this case, a new variable new_var_i is generated, all the locations that use this computation in result are replaced by this fresh variable (using the same auxiliary Mutator mentioned in 1.), and the current result is replaced by let new_var_i = currentComputation in result.    3 - Case where we can't or don't want to introduce this computation inside a new variable This is the case where we either can't introduce the current computation inside a new variable (because it contains variables that are not yet in scope there) or because our internal heuristic/predicate did not want to introduce it.    -> In this case, we will compute the direct sub-expressions of the current computation (implemented by an auxiliary analysis), and we will add them to the vector of semantic computations so that they have a chance to be considered later. Note that they are added while still preserving the order.    Note that we do not add all the sub-expressions of the current expression but only its direct subexpressions given the fact that we always consider them from biggest to smallest, and given that some candidates are mutually exclusive. Otherwise it would be computationally more intensive and it would pose the problem of cleaning the vector of candidate computations when one of them gets introduced into a variable. Evaluating them lazily by only looking at the direct sub-expressions is at the same time more efficient and simpler.Once the entire vector of semantic computations has been tried, the main function VisitStmt() calls the general dispatcher , which will in turn call the appropriate handlers. The only specific task of overridden handlers will be to update the context appropriately as new variables are introduced into scope (via Let-In, via For loop, etc) or leave the current scope. Thus, they will update the context appropriately before and after the calls to VisitStmt() and VisitExpr() on the child nodes.* Added empty newline at the end of every new file* Rolled-back the pointer to the submodule vta-hw* Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too.* Spelling and comment* Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too.* Revert "Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too."This reverts commit c4138d9afc28e79f107a4eccf988a6d93221eb5a.* Fixed reference used for no reason instead of normal variable.* Added comment explaning why we do not need the union/intersection over N tables at the moment (because we would only use it for N=3)* Did most of the changes suggested by upstream* Continued to work on the remarks given on the public repo.* Final remarks addressed, small formatting things, and fixing things reported by the linter* Last linter fix.* Fixing newline* Adding newline missing.* Minor commit for style fo conform with clang-format* Removed trailing space at end of line* And more minor style changes* Fixing style of the python test files* And one more for style in python tests!* This linter is very annoying to force the style of indentation in a comment, in a test file. It makes it harder to read in this case! And that incitates people to not write comments* Deactivate the CSE pass for the lowering tests as it would otherwise do some commoning, and improve the way the CSE recurse + test added for cascade commonings* Fixing new lint offenses* Removing debug statement* Restore other test file to its previous state* One more for the linter...* Linter again, this time for the new test...* again* again...* Deactivating the CSE pass for another lowering test as it does some commoning* Disabling the CSE for the a test for GPU too* Trying to fix a VTA test by disabling the CSE pass for it, as it probably does some commoning* Complying with the linter* Restarting the CI 1/2* Restarting the CI 2/2* Restarting CI 1/2* Restarting CI 2/2* Slightly reduce size of large pretty printer test, copied from https://github.com/apache/tvm/pull/10026/commits/ae98f9e7809cbf8d910fa16bfeac8364196e57d7* Trying to resolve the problems on the weird tests* Linter.* Restarting CI which has skipped the MacOS build for no reason 1/2* Restarting CI which has skipped the MacOS build for no reason 2/2* Commented buggy tests* Linter...* Restore the VTA tests, and use trick kindly given  by Masa to disable the CSE pass for the VTA tests, as vta.build() overwrittes the config* New fix, which this time does not break the doc (VTA uses a set with {} for the disabled passes instead of a list with [] for some reason* More VTA fixes* vta tutorial fixCo-authored-by: Masahiro Masuda <masahi129@gmail.com>	0
[AutoTVM][TOPI] Fix bifrost spatial packing conv2d auto tune (#5684)* [AutoTVM][TOPI] Fix bifrost spatial packing conv2d auto tune* [AutoTVM][TOPI] Putting placeholder replacement in compute* Fix winograd kernel replacement* Fix sanity check: Line too long	0
[Relay][Pass][Docs] Update the doc for adding a Relay pass to mention the pass infra (#3583)* Update the Relay adding pass doc to reference the new pass infrastructure* Correct pass nameCo-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Align header equals signs	1
[Relay] [Op] Zeros, Ones (#1885)	5
[Hexagon] Add flags to control floating point support in HVX (#10644)Add explicit parameters to `tvm.target.hexagon()` to control LLVMcode generation for floating point vector instructions.	1
[TIR] Fix rewrite_simplify tir::builtin::shift_left (#6555)	0
[Relay] Flexible shape dispatch transformation (#11199)* Added pass that creates a semi-dynamic dispatcher around a relay module.* Added automatic padding feature.* Output slicing working.* Multiple input support working i think.* Added test file.* Improve comments.* Fix lint.* Allow default values.* Fix docstring.* Improved documentation based on feedback.* Add extra check for record loading.* Improve variable names.* Add type inference to make sure things worked.* Added support for multiple outputs.	1
[TOPI] Enable standalone wheel build (#3657)* Fixed topi bdist_wheel build to include libraries.* Removed unneeded imports	2
Require explicit version for llvm_config. Update doc build requirements. (#129)	1
[VirtualMachine] fix raw pointer using by VirtualMachine (#9980)Use ObjectPtr<Executable> instead of Executable* to solid saving of Executable object for correct work of VirtualMachine on c++ sideCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	1
[TOPI] Improve CUDA softmax scheduling (#5600)- Do not use multiple kernels- Schedule with warp reductions- Fixed a bug on the lower warp memory pass- Fixed warp shuffle intrinsics for the nvptx backend.Signed-off-by: Wei Pan <weip@nvidia.com>	0
Support quantised SQRT operator in TFLite (#9258)	1
[TOPI] CUDNN integration (#730)* add target.libs to target str representation* integrate cudnn into topi cuda* append target.libs to target.options	1
Further clarify CI docs (#11980)	2
Switch off global barrier detection by default (#211)	5
[TE] Support mixing normal and cross-thread reduction (#5193)* Support mixing normal and cross-thread reduction* minor improvements	1
[Relay] s/SEScope/VirtualDevice/g (#9759)* [Relay] s/SEScope/VirtualDevice/gNobody liked 'SEScope', and 'DeviceMcDeviceFace' is too verbose, so itseems 'VirtualDevice' has the popular vote.	5
[VTA] update 3rdparty submodule (#7081)* update vta* remove tvm	4
swap pytorch and tvm import order (#7380)	2
[Rust] Allow convert Context to ArgValue (#6544)	1
[TIR] Fixed LowerThreadallreduce not remapping Store buffer var (#8931)* Fixed LowerThreadallreduce not remapping Store buffer var* reenable warp reduction schedule for softmax with fused opsCo-authored-by: masa <masa@pop-os.localdomain>	1
Create loops according to storage scope and thread hierarchies (#5190)* Set IterVar index to 0 for local thread bound IterVars.* Lint fix* Use rank instead of scope name to predicate.  Add tests.* Handle cases other than local/threadIdx.* Turn warp to the old behavior.* Modify test to cover global/blockIdx.* Fix a typo.* Update test_te_schedule_ops.py with more testing coverage in test_local_stage_predicate; remove test_schedule_schedule_ops.py which was added by mistake.	1
[hexagon][testing] filesystem-friendly test IDs (#12195)- Change the formula used to compute pytest test-ID strings.  The previous formula included '(' and ')' characters, which  can cause the filename to require escaping / quoting on the  Bash command line.	1
[arith][BugFix] Fix simplify input PrimExpr of DetectClipBound (#12150)	0
update python code style to 3.6 (#8199)	5
[microTVM] Arduino: Fix f-strings on flash warning/error messages (#12175)This commit fixes two f-strings on flash timeout exception and runtimeerror so the proper variables (like port, number of retries, and timeoutvalue) are correctly printed to the warning / error messages.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	0
[TOPI] Add winograd for mali (#898)* add winograd for mali* fix lint* add padding* fix comment	0
[Bugfux] wasm32-standalone app repaired (#8563)	0
[microTVM] Zephyr: Set 'choices' for ProjectOption 'verbose' (#8968)Set 'choices' tuple returned for ProjectOption 'verbose' as a doubleof True and False so TVMC and other interfaces can easily determineit's a boolean option.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[ETHOSN] Match config for is-supported with compilation target (#9160)The Ethos-N variant configuration for the is-supported functionality is nowthe same as the variant configuration for the actual compilation	5
[MetaSchedule] No explicit for spatial PrimFunc (#11534)	5
[AutoScheduler] Improve CPU matmul tutorial (#7037)* [AutoScheduler] Improve matmul tutorial* fix	0
[Hexagon] Add mobilenet test (#11104)* Add mobilenet test on Hexagon* Address comments* fix import and remove extra function	1
[TIR] Encode conditional accesses info into block read/write regions (#9880)* encode conditional accesses info into block read/write regions* compare ir after simplify	5
Add RESIZE operators to realy TFLite frontend (#3370)	1
[IOS] Fix build error of iOS RPC (#5621)* [IOS] Fix build error of iOS RPC- Update to C++14- Use the latest RPC protocol- Resolve CoreML dependency* Fix clang-format error	0
[CODEGEN] Remove incorrect check for LLVM in C codegen test (#3921)	3
[COMMUNITY] Altan Haan -> Reviewers (#11205)Please join us to welcome @altanh as a new reviewer to TVM. Altan has made contributions to relay language.- [Commits History](https://github.com/apache/tvm/commits?author=altanh)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:altanh)- [Community Forum Summary](https://discuss.tvm.apache.org/u/altanh/summary)	1
[Vulkan] Improved error message for extern calls passed to SPIR-V codegen. (#8332)Previously, the codegen indicated that there was an extern call.  Now,also indicate what that extern call is, to aid in debugging.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TOPI x86] Adding unroll_kw config option for depthwise conv2d. (#5197)	5
[MXNET]Softmin, trunc op support added (#5715)	1
[ONNX] Enable GPU in ONNX importer tests (#7438)* remove hardcoded target and ctx* fix c-codgen for floating point mod* MDisable onnx gpu test for argmin / argmax so we can get this fix merged. Matt or myself will fix later but we don't have time right now.* lint* fix black* Add flag to task_python_frontend.sh to only run GPU enabled tests on GPU* black again* Enable GPU for test_nonzero* Respond to comments* Don't test batch matmul on CUDA* Turn cuda off for dynamic batch matmul test* Fix task script* Flaky test* another flaky testCo-authored-by: mbrookhart <mbrookhart@octoml.ai>	5
fix onnx frontend softplus bug (#413)	0
typo fix (#11958)Co-authored-by: Terrance Liang <tailin.liang@outlook.com>	0
[python][docs] fix docstring / comment typos (#11608)	2
Allow to use negative index of array in python (#2069)* Allow to use negative index of array in python* Support negative index in array slice* Print index and array size in IndexError* Fix style	0
Revert "Change function def to Node ref for more flexiblity" (#29)	1
[Bugfix] Fix qnn.quantize type func with incomplete type (#11124)	0
Fix docker/lint.sh after #10933. (#11541)	2
[MLF] Add support for multiple modules in Model Library Format (#11464)	1
[Auto Scheduler] Make the opt_level of task extraction adjustable (#8288)* fix bugs in the auto scheduler record:* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code* fix some typos* fix the port number typo* fix a typo* make query_rpc_tracker show the correct port and the customized address* disable the pycharm reformat* reformat the code* make the opt_level of extract_tasks adjustable* Update rpc_server.py* fix a typo* Update tracker.py* support checking the port and customized address* reformat the code* fix a typo	2
[CI] always rebuild sphinx-gallery docs from scratch (#1742)	2
[COMMUNITY] @FrozenGene -> Reviewer (#2544)* [COMMUNITY] @FrozenGene -> Reviewer* Fix	0
[TOPI][x86] Legalize - Support int8xint8 convolution to use VNNI instructions. (#4196)	1
[LLVM/String] Remove conversion operator of String to llvm::StringRef (#11807)* [LLVM/String] Remove conversion operator of String to llvm::StringRefWe should not be declaring LLVM data structures in headers unrelatedto LLVM. There are only a handful of places where such a conversionwas used, it was replaced with a more local solution.* Rebase to restart CI* Restart CI	1
[TOPI] TE implementation of LSTM using scan (#11531)* TE implementation of LSTM in TOPI* docstring* lint* add injective tags where applicable	1
[DOC] add json spec intro (#62)* add json spec intro* fix error* fix name attrs* rename and move to top* address boolean values* update title	5
[microNPU] Use TFLite tests for strided_slice (#10165)Simply migrates the Relay style tests to TFLite for unification amongother tests.Change-Id: Iaab3ba27ad1534145fe94302be29f386f08af58e	3
[ONNX] enable the onnx tests after PR #8274 merged (#9019)* enable the onnx tests after PR #8274 merged* fix lint	0
[RPC][REFACTOR] Use PopenWorker to handle RPC Server. (#7889)Previously the rpc server relies multiprocessing to start a new process and does not work under jupyter.It also have a popen mode that does ensure the socket start listening before returning the port number.This PR switches the implementations use PopenWorker. The port number is returned after the socketget binded, which resolves some of the RPC flaky issues(need sleep to wait the server to start).It also makes the RPC server jupyter friendly.	1
[CI] Improve docker/build.sh to accept a docker tag parameter. (#7707)* This adds a new '--tag' parameter so that we can   build docker images on a particular tag, not only ':latest'   as given by Docker * It opens up the possibility of generating "staging" images on   a different tag, in the same servers as we keep the production   images * By default it keeps previous behaviour of using ':latest' tag.	3
Add vlogging for type-table registration. (#11041)	2
[UnitTests] Minor fixes to unit tests for cudnn/vulkan targets (#8462)- Marked tests as @requires_cudnn to avoid failure on platforms  without cudnn.- Replaced target "vulkan" with "vulkan -from_device=0"Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[random] support random fill (#5913)	1
Add several op mapping in PyTorch frontend (#6472)* Add copy_ and clamp_ in PyTorch frontend* add true_divide in PyTorch frontend* more test cases for copy_* fix format* remove copy_* fix format* skip true_divide for torch < 1.5	0
[ci] Fix condition for skipping tests in i386 (#10698)The Python and base image update for the i386 container changed the results of the various functions in `platform` as found in #10687. This updates them to work correctly with the new container and updates the relevant parts of the codebase to use the new check.cc @masahi @mosiusCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Make loop unrolling in LoopPartition optional (#6823)* [TIR] Make loop unrolling in LoopPartition optionalFor certain analysis/tensorization, it can be usefulto keep the loop structure when partitioning loops.The current behaviour removes For loops of length 1.This change introduces the option to preserve theseloops with the 'unroll' flag.	4
Update softmax.h (#1057)	5
[ETHOSN] Add support for concatenate with negative axis (#12686)Supports offloading concatenate with a negative axis to the NPU. In addition, parameterized the concatenate unit tests.	3
disable fopen64 in dmlc-core (#443)	5
[AutoTVM] Add index boundary check in ConfigSpace.get() (#7234)* [AutoTVM] Add index boundary check in ConfigSpace.get()* Fix unit testCo-authored-by: Yanming Wang <yanmwang@amazon.com>	3
Fix bug of generate-unmatched-brackets in CodeGenC::PrintSSAAssign (#6887)	0
[µTVM] Add serial transport, parameterize µTVM Zephyr test, run on physical HW (#6789)* [BUGFIX] Respect infinite-timed session start timeouts. * When debugging, the intended behavior is to set the session start   timeout to infinite to allow the user to configure the debugger. * At present, if a session start retry timeout is defined, the   current logic will bail after the retry timeout expires. * This change makes the session start logic retry forever, once per   retry timeout.* Document RPCEndpoint::Create.* Add stm32f746xx to tvm.target.micro() call; fix parameter name. * This API is expected to just be used with positional args, not   kwargs, so this change isn't expected to cause any breakage. * model is more inline with the rest of the file, given TVM Target   Specification RFC.* [BUGFIX] If session start fails, exit transport context manager. * If an error occurred during session setup, then complex transports   e.g. DebugWrapperTransport would not de-initialize.* Align transport writes/reads in TransportLogger* fix syntax errors which were not exercised in previous PR* Remove microTVM logic from standard RPC server, add debug shell. * microTVM uses the host RPC server as a way to launch a debugger in   a dedicated, separate terminal window. microTVM needs to be able to   launch the debugger itself, because its model of the device   flash/debug flow separates these two things into distinct   operations implemented by shell commands (for maximum portability   across frameworks). * microTVM can be configured to launch the debugger (e.g. GDB) in the   same terminal as is used for flashing, but this is sub-optimal   because then it hides any logs emitted by the device. * Using the standard RPC server was hard because GDB expects the user   to issue SIGINT to interrupt program flow, but due to the RPC   server's necessary use of multiprocessing, multiple signal handlers   needed to be SIG_IGN'd, and further, because libtvm.so is   intentionally frontend-agnostic, it's difficult to include signal   handling directly in that binary (Python expects you to call   PyErr_CheckSignals, but we don't require and don't want to require   python-dev to compile libtvm.so, and this is the only such case   where libtvm.so is expected to block the main thread for a long   period of time). * Here we implement a separate microTVM debug shell python script   using the non-blocking server implementation.* Add serial transport, parameterize test_zephyr to work on real hardware* add pytest test fixture, missed from previous change. * this test fixture helps to parameterize the test case* address leandron@ comment from #6703	1
[Vulkan][UnitTests] Compatibility fix for test_vulkan_unique(). (#8186)relay.unique return values changed in 6baccc13, updating vulkan unittests to match.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TOPI][ARM] Improve injective schedule (#2801)	1
Update main.yml (#6002)	5
[PASS] More storage sync. (#297)	4
[WEB] WebGPU support (#5545)This PR introduces WebGPU support to tvm.The WebGPU runtime is directly built in javascript(as WebGPU uses JS as the first class citizen API)and exposes back to the tvm's runtime via PackedFuncs.One important note is that `ctx.sync` is not async.This is due to the fact that WebGPU is a purely async API and we cannot block in the web environment.So the current best way to use the js api is to wrap things in an async function.When copy a GPU array to CPU, `await ctx.sync()` need to be called to wait for copy completion.We use a AsyncIO rpc server to serve the async functions to the clients.	1
[Frontend] Add Span filling for frontends to Relay (#9723)* [Frontend] Add Span filling for frontends to Relay* Add a common span filling feature for tf1/2, tflite and pytorch.* Add test case for Span filling in each frontend.* Expose Tuple and TupleGetItem to python end* [Frontend] Add Span filling for frontends to Relay* Fix lint errors* Change default string of scope_part in Pytorch* Reorder the span position for one to many conversion* [Frontend] Add Span filling for frontends to Relay * nit fixed * Add a bool flag to control print span * refactor pytorch get span to a birefer way* [Frontend] Add Span filling for frontends to Relay* Add one more condition for spanFller* Refine the format for those pytorch node without scopeName* [Frontend] Add Span filling for frontends to Relay* Fix lint	0
[PatternLang] Add a relay LetPattern (#7332)* Add a relay LetPattern* fix If copyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix If copyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[NNVM][TENSORFLOW] Sigmoid op support #1367 (#1369)	1
[Pass] Add MaxPool, AvgPool to FoldExplicitPadding (#11494)* fold first steps* spitballing* check pad is really optd away* new pool test passes* stuff* refactoring midway* things actually kinda work* complete tests* lint and complete tests* clean* fix comments	0
[Tutorial][Quantization] Fix incorrect name of calibration mode (#5150)	0
[TIR] Support fold constants in specialize process (#8803)* support fold constants in specialize* replace Substitue() with VisitExpr() in specializer.	1
[microNPU] changing region 'tvmbaw*' to 'dynamic*' (#10338)As a follow up to #10022, this is a follow PR toperform name change of the region as discussed inthat PR.	4
[Rust] Fix memory leak #2 (#8725)* Add C++ API for computing type key from type index* Try and isolate leak* Rewrite the bindings to fix the ArgValue lifetime issueThere are still quite a few issues left to resolve in this patch, but I believe the runtimechanges stablize memory consumption as long as the parameters are only set once. ByteArrayalso has some totally broken unsafe code which I am unsure of how it was introduced.* Finish handling tvm-rt issues due to ArgValue lifetimeThis patch further refactors the bindings to better handle thelifetime issues introduced by detecting the argument memory leak.* WIP memory leak* There is issue using TVMCb function which is breaking refcount* Fix fallout from the lifetime refactor* Another tweak* Follow up work from the memory leak, attempt to clean up ByteArray* Add some todos for future work* Fix doc string* Clean up the changes* Format	4
Add initial support for Intel FPGA SDK for OpenCL (AOCL) (#1474)	1
Yolo2 operators (#911)	1
[BugFix] resolve integer 32. ~ 64. mismatch by casting (#9582)	0
Append null terminator when converting JS string to c string. (#931)	5
[TEAM] Huyuwei -> committer (#2139)	5
add relu (#1849)	1
[LLVM] Refactor MakeCallPacked, NFC (#9118)Change the interface for `MakeCallPacked` in `CodeGenCPU` and in`CodeGenHexagon` to encapsulate the multiple returned values intoa single structure. This should help readability, but also it willmake the upcoming adoption of opaque pointers a bit easier.	1
[ETHOSN] Support multiply conversion to depthwise (#12403)Multiply can be supported when offloaded to the NPU by a conversion to a depthwise convolution operation. This is only supported when the multiply operation has a single single variable input with the other being a constant of shape [1, ..., C]. This commit adds a new pass "ConvertEquivalents" (name subject to change) to handle this conversion before codegen.	0
[Relay, Torch] Clean up and refactor PyTorch frontend (#4944)* The initial import of refactored implementation, all tests passed* enable mobilenet v2 test* minor cleanup* reorg* fix lint* use input names that come with torch IR* fix typo* introduce parse_operators* fix lint* add _ prefix	0
[Torch] Add initial control flow support  (#4964)* Add support for prim::If and prim::Loop with test cases* rebase and fix tests* add some comments* simplifying, fix float cast* parse -> convert* recursivly retrive ops in get_all_op_names* use multiple return values from block correctly, simplify loop convert* choose dtype properly for zeros and ones* simplifying, replace convert_inputs with _get_relay_input_vars* fix for while loop with non input dependent init cond* add assert on loop var update* move the condition around* better testing for seg models* rebase fix, disable inception v3 in quant test as it is too slow toload with torch-1.4 + torchvision 0.5* simplify and add more comparison op converter	1
Restore integration test on Mac and Windows (#11538)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>	3
[TIR] Add a new intrinsic count leading zeros for LLVM and SPIR-V (#7825)	1
[Relay] Restore dominator check (#11616)It is ok to match a sub-graph which has dataflowoutside of the sub-graph, provided all such flowseventually come into the sub-graph.	1
Alter op layout for group_conv2d on CUDA (#2148)	5
Improve comments (#4633)* Improve commentary for operator fusion.* Attempt to clarify what well formed checker is doing	1
[RELAY][IR] Move type_annotation to Var, remove Param (#1900)	2
[RPC] IOS RPC (#261)	5
Fix type checking annotation for Union type (#11430)* Fix type checking annotation for Union type* Update _type_checker.py	5
[RFC] [VTA] [TSIM] Enabling Cycle-Accurate Hardware Simulation for VTA #3009 (#3010)* merge files* move verilator to the right place* change name to tsim* add default rule to be build and run* add README for tsim* Update README.md* add some structural feedback* change name of VTASim to VTADPISim* more renaming* update comment* add license* fix indentation* add switch for vta-tsim* add more licenses* update readme* address some of the new feedback* add some feedback from cpplint* add one more whitespace* pass pointer so linter is happy* pass pointer so linter is happy* README moved to vta documentation* create types for dpi functions, so they can be handle easily* fix pointer style* add feedback from docs* parametrize width data and pointers* fix comments* fix comment* add comment to class* add missing parameters* move README back to tsim example* add feedback* add more comments and remove un-necessary argument in finish* update comments* fix cpplint* fix doc	2
add rule for clean (#4364)* add rule for clean* Update clean ruleSeems like lib/ directory is not made by the makefileSo don't delete directory, just the contents of it.	4
[TVMScript] Parser for Lambdas, Parser/Printer for `CommReducer` (#9358)* CommReducer Parser/Printer* update argmax unit test* update doc* lint fix* add unit tests with multiple reducers	3
[FRONTEND] fix same var used in single op (#383)* fix same var used in single op* revert to older version	4
[Relay] [PyTorch] Add aten::tril and aten::triu (#11890)* add trilu* update triu and tril; fix empty* fix lint	0
add luis as reviewer (#3909)	1
[UnitTests] Require cached fixtures to be copy-able, with opt-in. (#8451)* [UnitTests] Require cached fixtures to be copy-able, with opt-in.Previously, any class that doesn't raise a TypeError in copy.deepcopycould be used as a return value in a @tvm.testing.fixture.  This hasthe possibility of incorrectly copying classes inherit the defaultobject.__reduce__ implementation.  Therefore, only classes thatexplicitly implement copy functionality (e.g. __deepcopy__ or__getstate__/__setstate__), or that are explicitly listed intvm.testing._fixture_cache are allowed to be cached.* [UnitTests] Added TestCachedFixtureIsCopyVerifies that tvm.testing.fixture caching returns copy of object, notthe original object.* [UnitTests] Correct parametrization of cudnn target.Previous checks for enabled runtimes were based only on the targetkind.  CuDNN is the same target kind as "cuda", and therefore needsspecial handling.* Change test on uncacheable to check for explicit TypeError	0
Spelling mistake corrected (#945)	5
[RELAY] Add multiref trigger to ForwardRewrite (#2168)	1
Expose backtrace symbols in Debug mode (#3001)	0
[DOCS] Update link loc (#4257)	2
[PASS] More reliable error message for lower warp (#1065)	0
add squeeze (#52)* add transform* fix* update doc* Update tvm	5
[CI] Temporary disable rust test (#5029)	3
[REFACTOR] Polish ffi convention. (#4912)* [REFACTOR] Polish ffi convention.- Remove the src/api, keep registration local to the c++ function.- Remove the api_internal as it is no longer needed.* Update the codebase walk through	5
[FIX] Fix temporary allocation size in threefry (#7709)* [FIX] Fix temporary allocation size in threefry* bump sizes	0
Fix docs issue (#59)	0
[NNVM][TESTING] Add two testing symbols: dqn and dcgan (#1294)	3
[Relay] support i64 indices (#6143)	1
[FQ2I] Support Conv2dTranspose FQ2I (#9347)* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* fix things* cool keras fix	0
Corrected TVM autotuning on GPU (#5432)Added missing "tir" in tvm.tir.analysis.verify_gpu_code(f, kwargs)	1
Add missing #!/bin/bash directive. (#2951)	1
[Quantization, Calibrate] Fix context creation when current_target is explicity set (#4582)	1
[MetaSchedule] Add the missing HasWorkload interface to the Database (#9756)	5
[ci] Break out test steps for Hexagon / microTVM (#10946)Since we gate all tests on all builds currently in Jenkins, the longest running build is a bottleneck for overall runtime. This moves them to their own test steps so that the longer-running GPU/CPU tests can start earlier. This should shave off another 30 minutes or so of CI time.As a follow up we can investigate per-platform parallelism, e.g. the CPU tests only wait on the CPU build, but Jenkins doesn't have good support for this so we might have to work on the UX a bit first.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Frontend][Tensorflow] Add unique operator (#7441)* Initial commit of the unique operatorAdd unit tests for unique operator* Add tensorflow unique op* Refactor unique to use sort-based algorithm* Change relay.unique test to run only on cpu* Change topi.unique test to run only on cpu* Change range to parallel for parallelizable loops* Add return_counts option for relay.unique and topi.unique, add pytorch frontend* Fix pylint* Patch pytorch frontend* Initial support of topi.cuda.unique* Refactor to use ir_builder directly* Modularize adjacent difference* Refactor to simplify* Fix typo* Combine _unique and _unique_with_counts* Reuse indices_ptr to remove arange_ptrCo-authored-by: Yanming Wang <yanmwang@amazon.com>	4
CI: Install apt-transport-https (#5053)The ubuntu_install_llvm.sh script started failing because of a http tohttps redirect.  This patch adds the package that allows apt to handlehttps transport.Change-Id: I70bcba32a9fc75d02c54f4f21f288b2f46226689	4
[TVMScript] ExprDoc (#12048)This PR addes:- All ExprDoc subclasses- Their Python bindings- Support of ExprDoc in PythonDocPrinter- Unit tests for ExprDoc in PythonDocPrinterTracking issue: https://github.com/apache/tvm/issues/11912	0
[VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity (#3605)* support for different inp/wgt bits, rewrote dot for clarity* [VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity* [VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity* change back to sim* fix index* fix index* fix indent* fix indent* fix indent* fix trailing spaces* fix trailing spaces* change to more descriptive name* matric->matrix* fix spacing* fix spacing & added generic name for dot* better parameter flow* spacing* spacing* spacing* update requirement (tested) for dot, spacing* function call convention* small edit	2
[HEXAGON] Auto-vectorization (fp16) for v68 (#12397)* Auto-vectorization (fp16) for v68* use tvm.testing.main in fp16 test of tanh_slice op	3
[Meta Schedule][M3a] SearchStrategy (#9132)* Add c++ side SearchStrategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add python-side code & test.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add docs.* Minor fix.* Add workflow.* Add docs.* Fix docs.* Add notes.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[Relay][Topi] Use SimplifyInference for L2 Normazlization. (#4795)	5
[microTVM] Add Arduino CLI support to ci-qemu (#8504)* Add Arduino CLI support to ci-qemu* Install latest version of Arduino SDK* Remove unnecessary --fix-missing* Tweak to clarify what URLs go with what* Retrigger CI* Temporarily replace buggy Spresense core	0
[Meta Schedule][M4a] Local runner (#9153)* [Meta Schedule][M3a]Local runner (#479)* localrunner* localrunner init* linting* address comments* exception handling* single run testcase* two more cases added* add exception case* one case with AddModule added* address comments* address comments* remove unused dependency* optional arguments* linting* add utils* linting* address comments* remove non-ascii commennt* add sanity check* address comments	1
[Frontend] [PaddlePaddle] group_norm adjusts test accuracy (#11450)* group_norm adjusts the check accuracy* remove test skip	3
[skip ci][ci] Fix outdated Jenkinsfile (#10822)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CUDA] Support memory reuse for dynamic shared memory (#9341)* reuse shared dyn* minor* format* format* address comment and fix* address comment* address comment	1
sort axes (#10985)Co-authored-by: Margaret Qian <mqian@octoml.ai>	5
[Hexagon] Generalize builtin for Nd memory alloc with storage scope and add lowering for VTCM / Hexagon (#10558)* repurpose texture flatten for vtcm; TIR lowering correct* clean up remaining code in texture flatten pass* add Alloc and FreeTexture, but failing to run over rpc* test passing with malloc in the device api* cleanup* fails in very reliable way with memory corruption* working with non-HexagonBuffer vtcm alloc* cleanup* do not pass scope through mem_copy api* [Hexagon] Resolve breakage in test_hexagon/test_cache_read_writeBreakage was caused by https://github.com/apache/tvm/pull/9727, whichdidn't account for the new `builtin::mem_copy()` when computing thestack size in `StackSizeChecker`.* use HexagonBuffer in Alloc and Free packed funcs* Added comment indicating need for StackSizeChecker::MakeMemCopy.* add AllocVtcmWorkspace and FreeVtcmWorkspace* cleanup* Updated unittests to run all contrib/test_hexagon at CI.* create separate vtcm alloc lowering pass and transform* reset texture_flatten.cc* comments* CI bump* Fix lint formatting error.* Updated fix to remove StackSizeChecker entirely.* pass device and type to device api* Bugfix, verify the precheck's allocations, not own.* Bugfix, pass context information to the precheck.* pass order and shape to device api* working* fix up types and arg passing* pass scope to device api* common builtin for texture / vtcm* add scope to freend api* format and lint* fixed missed format error* restart ci* fix test random value issue + code review feedback* fix test hang* restructure lower vtcm pass per code review feedback (option a)* format error* global.vtcm + tvm_stack_make_shapeCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[PASS] Support buffer reuse for different types (#891)[PASS] Support buffer reuse for different types	1
use ubuntu18 in docker android demo  (#10222)* use ubuntu18 in docker android demo + remove useless directives in script* remove redundant apt-get update* revert install python3.7 + use install_python3 for ubuntu1804Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>	1
[hexagon][topi] add sliced max_pool2 (#12169)Add TOPI implementation of sliced max_pool2d,with basic scheduling.	1
[AutoScheduler] Register workload when deserializing tasks (#6927)* [AutoScheduler] Register workload when deserializing tasks* fix name* format* merge* fix test* more checks	3
[TESTS] Jenkins test flow (#152)	3
Added tesnorizeation for avx2 based gemm. (#3982)* Added tesnorizeation for avx2 based gemm.Summary:Tensorized the same region as avx512. Names produce 16x1 int32 results.Does by doing two sets of AVX2 instructions to do reduction on 8x4 int8kernel with 1x4 data.Test Plan:on avx2 machine:python tests/python/contrib/test_gemm_avx2_acc32.pyReviewers:Subscribers:Tasks:Tags:* Fix lint errors. Removed commented out code.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:	3
Partition fix with rfactor, simplify and likely predicates. (#3444)	0
[TensorRT, BYOC] Handling a corner case in TRT RemoveDropout pass (#8506)* [TensorRT, BYOC] Handling a corner case in TRT RemoveDropout pass* changing visit logic	2
[Relay][Frontend][TFlite] Add parses support for UNPACK tflite operator (#4447)* use SPLIT & SQUEEZE = UNPACK as implemented in tensorflow parser  Relay doesn't support UNPACK* tflite 1.13: UNPACK doesn't work as exepcted -> copies the values from  1st unpacked tensor to the other unpacks* tflite 1.13: doesn't accept negative axis	1
[Relay] Roundtrip part of pretty printer and parser (#3460)* initfix rebaselintfix cmaketry againfix ci* add gitignore* fix format* do not include .interp and .tokens	0
[CUDNN] Add partitioning support for fused conv2d+bias+act (#10997)cuDNN has kernel support for the pattern conv2d+bias+act,although as of v8 only relu is supported as the activation.	1
refact: rm unused variable (#8290)	1
[AutoScheduler][FIX] Fix exception handling in measure.py (#8754)* fix exception handling* fix linting* stringify the exception from MapResult* use repr instead if str	1
[PASS] Enhance LiftAttrScope (#632)* [PASS] Enhance LiftAttrScope* update vt	5
[Arith] Parse > and >= bounds in ConstIntBoundAnalyzer (#12457)Previously, only `<` and `<=` bounds were parsed, as these are thecanonical form produced by the `RewriteSimplifier`.  However, theconstraint may also be supplied by the user through the Python API,and may not be canonicalized prior to parsing.	1
Follow up from CMSIS-NN pooling failure (#9708)This commit fixes few comments in TIR2Runtime pass of CMSIS-NN target.These comments specify layout used by CMSIS-NN API for input and filter shapes.Another fix was done to the filter layout calculations. Instead of hard coded values for dimensions, filter_shape.find("H") was used to locate a particular value.Third fix was done to the padding API used by Conv2D and Pooling tests.It was made generic for TFLu's "SAME" padding type.	1
[microNPU] Add support for conv2d running on two cores on U65 (#10251)* [microNPU] Add support for conv2d running on two cores on U65The 512 mac variant has two cores that processes the weights inparallel, so we need to split the weights and biases into twoand encode them separately.Change-Id: I53791f614288ac4df181b9462fc632d35b934a86* Changes due to rebase* Rebase, improve DivideConstants and expand testingMake the DivideConstants to operate on non-flattenedtensors to support two core execution in U65.	1
Update find cublas so it search default path if needed. (#9149)	5
[MKL] Fix offloading of batch_matmul to MKL (#6752)* fix mkl offloading of batch matmul* name fix and add doc* add doc for lib argCo-authored-by: masa <masa@pop-os.localdomain>	2
[Hexagon]Refactor Hexagon_SDK_PATH (#11282)* refactor HEXAGON_SDK_PATH and remove HEXAGON_GTEST	3
Fix typo (#9740)	2
[RELAY][Refactor] TextPrinter, move ret_type after body in Function. (#1918)	1
[Relay][Frontend][TFlite] Add add parser support for relational ops (#4695)Add support for: greater_equal, less, less_equal, equal, not_equalAdd tests for the elemwise relational ops	3
[Relay][Convert Layout] Enable layout transformation for image.resize op (#8205)* Enable layout transformation for image.resize op* Change str map function to str and index retrieval* Fix for pytorch frontend segmentation models test	3
[VTA] Bug fix for padded load with large inputs (#4293)* bug fix for padded load with large inputs* Update TensorLoad.scala* Update test_vta_insn.py	3
[TOPI] upsample operator 'NCHWinic' format support. (#4791)* [TOPI] upsample operator 'NCHWinic' format support.some hardware accelerator ask packed format data like NCHWinic to fit thehardware resource, here add upsample NCHWinic format support to helpsuch requirement.* address review comments, add assert for 'else must be NCHWxc' logic.	2
[COMMUNITY] @wweic -> committer (#4636)	3
[UMA] UMA v1.0 (#12087)* Add minimal working structure for generic interface* Separate target definition from codegen* Update file structure to support multiple NPU targets* Add scheduling and pass support to codegen* Update schedule function and pass registration* Add generic partitioner for relay graph partitioning* Add pattern-based relay graph partitioning and AOT codegen* Update API* Add UltraTrail relay passes and schedule function* Update UltraTrail relay passes* Add tir_to_runtime hook for UltraTrail* Add operator strategy registration to lowering* Add option to pass constants as attributes* Refactor naming: Generic to UMA* Change API to single user-facing backend class UMABackend* Add initial codegen API* [UMA] add a generic packed function to register targets* Restructure files and add initial codegen* Minor code cleanup* Add UMA config and MergeCompilerRegion example* Move UMA configuration to init parameters* Add python hooks for C-codegen. Still has known restrictons* Fix relay_to_tir hook to keep virtual device in main function* Remove register schedules, scheduling is moved to passes for now* Remove extract constants since non-scalar constants are now supported by TVM* API documentation and some code fixes and cleanup* Fix typo* Fix UMA lowering* Prototype for UMA-based target attribute registration* Add default option and type deduction to register_target_attr* Change pass phases to enum* [Relay] Plumb external codegen target via Target.current() for all external codegen paths(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).We want both old-style (via relay.ext.$toolchain) and new-style (via "RelayToTIR" Passattribute on target kind) external codegen to be able to access the current 'external codegen'Target instance via Target.current(). - For old-style, plumb the true Target through TEComplier and push it on the context   stack before calling relay.ext.$toolchain. - For new-style, pass the CompilationConfig to the RelayToTIRTargetHook pass, make the jump from   "Compiler" attribute value to Target via the new CompilationConfig::FindPrimitiveTargetForKind   method, and push on the stack before invoking the custom "RelayToTIR" pass.While working on this discovered RelayToTIRTargetHook was incompatible with the VM's compilationflow since RelayToTIRTargetHook assumes all "Compiler" attributed functions are inlined. Generalizeit to support both inline and global function styles.Extend Target::IsExternalCodegen to recognize target kinds with "RelayToTIR" attributes asexternal.Update target hooks unit test to exercise new support for outline-style, picking up the current target,and compiling via the VM.* Use current target in lowering* Use attr:kRelayToTIR* Remove erronousely commited quick fix* Towards test cases for uma* Add test_uma* Initial UMA structure for version 1* [UMA]: conv2d unit test* [UMA] update of tutorial* [UMA] update of pass format, still issue with conv2d c code* [UMA] refactoring of test_uma_lowering_with_umalower.py* [UMA] refactoring of test_uma_lowering_with_umalower.py* [UMA] Adding backend, codegen, patterns, strategies and run file for MyAiHw* [UMA] update towards my_ai_hw usecase* [UMA] working testcase for conv2d with uma* [UMA] testcase* [UMA] uma lower.py: replaced outdated function create_prim_func_from_outputs to be compatible withe latest content of "main"* UMA: Move torch import to top to avoid free(): invalid pointer error* Add stub files for targets* Add tests for ultratrail codegen* Adopt my_ai_hw accelerator for new target definition* Add unit test for target attributes* Test string arguments* Extend target test* [UMA] tutorial first versin* [UMA] moved unit tests to contrib* [UMA] renaming interfaces* Fix umalower_tests in ci* make uma a python module* [UMA] Update of UMAv1 API + added testcases + tutorialV1* [UMA] UMAv1* [UMA] cmake file updated* AOT test infrastructure adapted* UMA: add __init__.py for uma.api* Finish uma tests* Use upstream version of dmlc-core* [UMA] tir_to_runtime documentation update* [UMA] cleanup* [UMA] fix for test_partition* [UMA] lint fix* [UMA] lint fix* [UMA] lint fix* [UMA] lint fix* [UMA] fix of build scripts for arm and i386* Fix remaining linter errors* [UMA] CMakeLists.txt added UMA tvm_option* [UMA] added UMA tvm_option* [UMA] guard against multiple registrations* [UMA] fixed comments as pointed out in PR 12087* [UMA] fixed comments as pointed out in PR 12087* [UMA] skip uma tests if uma is not available* [UMA] added UMA rst* [UMA] Moved tutorial to RST file in gallery* [UMA] moved uma cli to apps* [UMA] change requests according to PR-12087* [UMA] update and sync of uma_cli and tutorial* [UMA] update of template passe: remove Pad block of Conv2D* [UMA] lint updates* [UMA] Test updates* [UMA] fixes according to comments from PR 12087 discussion* [UMA] lint updates* [UMA] moved UMA _template file to apps* [UMA] lint* [UMA] Remove exceptions when dispatching over targets* [UMA] vanilla pattern update* [UMA] added mobilenet integration test* [UMA] clang lint* Remove tir to runtime* [UMA] Use sequential for UMA relay passes* Use comparison against BYOC flow in test_partition* [UMA] tutorial update: moved code blocks to RST* [UMA] tutorial update and lint fixes* [UMA]  removing UMA from i386 build, as there is a fail in the CI pipeline due to missing CLANG for i386* [BYOC-DNNL] covered case for sum node without attr* [UMA] pylint* [UMA] pylint* [UMA] aot fix* [UMA] Changes PR review* [UMA] cc lint* [UMA] cc lint* Use better function name for te_lowering and annotate current target at TE functionsCo-authored-by: Paul Palomero Bernardo <paulpb@outlook.com>Co-authored-by: Christoph Gerum <christoph.gerum@uni-tuebingen.de>Co-authored-by: mbs-octoml <mbs@octoml.ai>Co-authored-by: Christoph Gerum <gerum@informatik.uni-tuebingen.de>	5
Fix a few OpNode argument field descriptions when registered (#7140)	0
[MetaSchedule] Misc update for e2e workloads (#10776)	1
[Hexagon] Enable broken tests (#12073)	3
[Relay] [Training] Fix ad for concatenate (#3729)* reproduce error* fix* lint* lint	0
[VTA] [APPS] Update README on tsim example (#3409)* update README* fix typo	2
Aligned CMSIS-NN SHA in TVM to CMSIS top of tree (#12723)Aligned CMSIS-NN SHA in TVM to top of tree of CMSIS.-Aligned buffer size APIs to CMSIS implementations.-Updated the tests to match new CMSIS context buffer sizes.-This change needs updates to cortex-m docker image.Change-Id: I13f1ad29fe0ef02f08660eca4c818b5d66145ffc	4
[Arith] Update BufferDomainTouched to support vector access. (#11722)* [Arith] Update BufferDomainTouched to support vector access.* Add test checking that domain touched works on IR containing RampNodes.	5
Disable MicroTVM on i386 CI (#3569)	5
[skip ci][CI][Fix] Fixing lint (#10445)A linting issue was introduced in #10423, fixing this up.Change-Id: I06c518194e30dcaa755005f06b8b7280c237d386	4
Update README.md	2
[µTVM] Add TVMPlatformGenerateRandom, a non-cryptographic random number generator. (#7266)* [uTVM] Add TVMPlatformGenerateRandom, and use with Session nonce. * This change is preparation to support autotuning in microTVM. It   also cleans up a loose end in the microTVM RPC server   implementation. * Randomness is needed in two places of the CRT:    1. to initialize the Session nonce, which provides a more robust       way to detect reboots and ensure that messages are not confused       across them.    2. to fill input tensors when timing AutoTVM operators (once       AutoTVM support lands in the next PR). * This change adds TVMPlatformGenerateRandom, a platform function for   generating non-cryptographic random data, to service those needs.	5
fix error report on Store (#8895)	0
[QNN] More doc fix on quantize and convolution (#4874)* [QNN] Doc fix on quantize and convolution* update test	3
[BACKEND] Vulkan Runtime and SPIRV Codegen (#861)* [BACKEND] Vulkan Runtime and SPIRV Codegen* fix doc	2
[Relay] [Pass] Add mixed precision (e.g. FP16) model conversion pass  (#8069)* Initial skeleton for fp16 pass.initial green gray and red listsmove fp16 conversion to own fodlersecond pass examplesplit up files a bit morecool nodes broinitial transofmr pass* Working python version of fp16 pass.fix topi conv2d not casting kernel to output typeworking resnet, but conv2d topi intrinsics need worktests for resnetadd more tests, extend coverage for converterupdate tests, ensure red ops convert back to fp32clean up code a bitsimplify fp16 output dtype examinationfix passupdate testsinitial coloring* Rewrite python passes in C++inspect arg fieldsadd propagate colors pass"private -> public inheritance"rewrite draftfull transformation in c++remove printsfp16 pass the proper wrappinginsert extra cast to pass type checkingfix previously broken test by removing cast in wrong scenarioremove old python_files* Extend support to things besides CallNodes. E.g. tuples and letsfp32 invalidate typing instead of cast addingbasic testsskeleton code outStash work -- casting based on checked typesworking let statementsadd more ops, handle functions more generallyadd multiply, fix broken casesupport TupleNodes properly, move hash function for datatypes into data_type.h"update simple let test with structural expectationcleanup p1remove old file* Rewrite how and when casting is done by checking types directly.add support for GPT2, BERTadd some more commentsnew single pass versionformattingmake a lot of things const referencesclean up testsmore cleanupmore commentsfinal commentadd newline* linting and formatting* add AST header* remove todo* lint errors2* remove i386 incompatible features* Trigger CI again* set seed* lint* address animesh's initial comments* mutate attributes only if they were originally floats* initial comments from matthew* add comment on hashing strat* add missing ;* edge case when mutating attrs* Cody's easy to address comments* add test to show green-red casting works* remove np.random seed from each test* remove as many references to fp16 types in favor of generic mixed types* rename RED, GREEN, GRAY to MIXED_PRECISION_ALLOW, etc.* skeleton for supporting arbitrary mixed types* cool tests* Using MixedModeMutator* rename things ToMixedPrecision* rename passes to amp.cc* rename tests to match transform* clean up typos* rename even better to_mixed_precision* don't insert into cache when dtypes equal* new python interface for registering ops* cleaner registering ops* add fp64 structural test* clean up and comments* make copy of attributes* asf header* pylint* remove TODO which is solved* Apply nits from code review (comaniac)Co-authored-by: Cody Yu <comaniac0422@gmail.com>* change cast_node_cache --> cast_node_cache_* add check for returned vals* better error msg* docstring for pass in python* fix default behavior to be proper* better error reporting via single flag* priority to 0* address more nits* fix story telling slightly* restart* correct docstring* change class fields to have _ at end* add class docstring* add comment on accumulation dtype hack* ADT warnings* add todo* fix linterCo-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[COMMUNITY] areusch -> Reviewer (#6637)	3
[ShapeFunc] Handle weights in shape func (#6912)* [ShapeFunc] Handle weights in shape func* Comments	0
[CUDA] Various int8 fix (cublas, cutlass, etc) (#10596)* [CUTLASS] avoid tile size 256 for int8 + align1 case* allow selecting int8 dense strategy for vulkan* fixed cublas batch matmul for int8* fixed int8 dense tensorcore strategy* add cutlass conv align1 + int8 case* support int8 mixed precision cublas bmm* black	1
[DOCS] Bring relay docs to the top-level flat view (#5343)- Changes most of the relay docs to use autosummary.- Bring relay API docs to the top-level flat view for easier discovery- Removed a few cases of re-exports.	4
Legalize - Use Non-recursive Rewriter. (#5296)* Legalize - Use Non-recursive Rewriter.* Cleanup.	4
[TOPI] fix docs errors (#4973)	0
[TOPI] Add C++ implementation of elementwise operators (#1306)	1
[PASS] RemoveNoOp. (#68)	4
[Fix Bug]fix the bugs of keras frontend when parsing LSTM, GRU, RNN layers. (#9850)* [Fix Bug]fix the bugs of keras frontend when parsing LSTM, GRU, RNN layers.* Reformat files with black formatter.Co-authored-by: AndrewZhaoLuo <andrew.zhao.luo@gmail.com>	2
[Target] Creating Target from JSON-like Configuration (#6218)* [Target] Creating Target from JSON-like Configuration* Address comments from Cody* fix unittest* More testcases as suggested by @comaniac	3
[ARITH] Bugfix: check arg positiveness for mod rules (#3279)	0
[TVM] Fix warnings (#3817)transform.h:118:3: warning: 'const' type qualifier on return type has noeffectattrs.h:68:3: note: expanded from macro 'TVM_DECLARE_ATTRS'node.h:244:3: note: expanded from macro 'TVM_DECLARE_NODE_TYPE_INFO'transform.h:95:3: warning: extra ';' after member function definitionattrs.h:68:62: note: expanded from macro 'TVM_DECLARE_ATTRS'	5
[skip ci][ci] Fix Jenkinsfile (#12387)This got out of date after merging #12178Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Misc minor updates (#10335)	5
[COMMUNITY] New committer -- sslyu (#7968)	1
updates (#25)* [FIX] Remove extra move* [MEMORY] Add inplace index	1
[Frontend, Tensorflow2] Adding TF2 frontend code with support for control flow ops  (#8142)* adding tf control flow ops with a different frontend codeCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Some minor fixes* Fixing output order in TF2 outputsCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Using black* RefactoringCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* resolving a bug with passing output tensors for Functional Graphs* fixing multi output for graph runtime* adding docstring edits* linting + black* linting + black* linting + black* removing unnecessary output propagation across function* addressed comments in PRCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>	5
[TEST] Add rocm library to library path (#381)	1
[SCHEDULE] Fix cross thread schedule after refactor (#85)	4
correct error (#4093)	0
Add aten::mv support (#9894)* Rebase* Fix bad mergeCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Change function def to Node ref for more flexiblity (#27)* Remove warning in g++5* Change function def to Node ref for more flexiblity	1
[Docs] Update stale links (#8111)	2
add docstring skip in hybrid script (#1668)* add docstring skip in hybrid script* fix lint	0
[SCHEDULE] Fix schedule for big array (#1340)	0
Replace '> >' in templates with >>, NFC (#12615)The problem with greedy lexing of >> as an operator was solved inC++11, and now templates no longer require spaces between >'s.	1
[DOCS] Code quality comment (#1069)	2
Support export ADT value in Python (#3299)* Support export ADT value in Python* Cache original functions* Cleanup* Cleanup	4
[microTVM] Add timeouts for CI tests (#10295)These shouldn't take longer than 5 minutes but since they have to poll they can end up running for a long while (e.g. this failure: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2534/pipeline).cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Add sugar method `Schedule.work_on` (#11999)This PR introduces `Schedule.work_on`, which instructs`Schedule.get_block` to find the correct PrimFunc to retrieve fromwithout having to specify `func_name` in every time if the PrimFunc'sname is not `main`.	1
[AutoScheduler] Improve test cases (#6657)* Improve test cases* update* fix lint* fix lint* trigger CI* address comments* trigger CI	1
[RPC] Include rpc session info into context (#458)* [RPC] Include rpc session info into context* add type checker in return converison	1
[DOCS][COMMUNITY] Committer guide and tips (#2468)	2
[TIR] Fix Tensorization IR-Comparator for Annotations (#10498)This PR fixes the way of comparison in which the tensorization IR-comparator deals with annotations.Prior to this PR, the comparator requires the annotation values from LHS and RHS to be exactly the same, which is, in fact, never possible. And this PR removes this comparison requirement (with a regression unit test).```c++bool TensorizeComparator::CompareAnnotation(const std::pair<String, ObjectRef>& lhs,                                            const std::pair<String, ObjectRef>& rhs) {  if (lhs.first != rhs.first) return false;  if (!lhs.second.same_as(rhs.second)) return false;  // <== The values would never be the same.                                                      //     Thus this line should be removed.  return VisitExpr(Downcast<PrimExpr>(lhs.second), Downcast<PrimExpr>(rhs.second));}```	4
[Topi, Relay] Add cumprod (#7722)* make cumbinop, refactor cumsum, add cumprod* cumsum exclusive test* Add cumprod + flesh out cumsum testsadd cumprod and testsreinstate testsrethink* add rudimentary scan implementation* add attributes of cumprod node* add cumprod strategy* add cuda strategy* python relay node construction* change attrs to be reusuable* add cumprod nodes* complete tests* Fix some typos about sum --> prodtypos fix sum -> prodmore typosmore typo fixesmore typosadd doc strings* Use Bool instead of int to represent exclusivemake exclusive a bool up and down stackfix xfix bool errit is a bool nowfixfix thingformatting to pass linterlint pythoncumprod pylintfix attributefix orderingadd exclusivity tests for end to endfix thingscuda identity_value* Overall improve formatting, add doc message correctionssimplify constructionclang-formatmore testsundo simpler construction due to function passing stufffix docsmore exclusive doc changesmore fixins"* merge cumsum and cumprod to scan, merge testsfix stuff* remove other mentions of cumbinop -> scanop* lint formattingCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@Andrews-MacBook-Pro.local>	4
Fix PyTorch matmul conversion when given (2-dim, N-dim) input pair (#7845)* [AutoScheduler] Fix incorrectly array context device and hide info at the beginning* Lint fix* Lint fix* update repo* Fix Pytorch matmul conversion when given (2-dim, N-dim) input pair* update measure.py* Lint fix* fix bug && add ut for pytorch matmul* update ut* Lint fix* update commit* Lint fix	0
[TFLite] Support depthwise convolution multiplier greater than 1 (#3922)	1
[Fix] Fix errors in error checking and reporting (#12423)	0
[TFLite] Mimic the TFLite's 2.4 reader's behaviour (#8538)In TFLite 2.4, the builtin code value can be either in"deprecated_builtin_code" field or "builtin_code" field (as longas the value is less than 127) and similarly to the TFLite'sreader, we should use the higher value of the two.Change-Id: I0d738f9257187903b4c5b4cc5a8733a451ddc02e	4
[CI] Increase the number of shards for Cortex-M from 4 to 8. (#12334)Co-authored-by: Florin-Gabriel Blanaru <fgb@system76-pc.localdomain>	5
add missing dependency (#6375)	1
[TIR][LowerMatchBuffer] Fix lowering strides when source region has higher dimension than the buffer (#9145)* [TIR][LowerMatchBuffer] Fix lowering strides when source region has higher dimension than the buffer* use int instead of size_t	1
[TVM][LANG] Add eager simplification for operations with FloatImm (#2615)* Add eager simplication for FloatImm* fix* fix lint* Fix gcc warning* fix* Add test case	3
[docs] Getting Started with TVM: TVMC Tutorial (#7640)* Getting Started with TVM: TVMC TutorialAn update of the TVMC tutorial, follows the introductionand installation sections of the new getting started tutorial* Update tutorials/get_started/tvmc_command_line_driver.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Style and formatting fixesCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>	0
[Tutorial] Deploy Quantized Model on CUDA (#4667)* [Tutorial] Deploy Quantized Model on CUDA* update* update* address comments	1
prune dnnl subgraph, and add related test case. (#10835)	3
Upgrad oneflow version (#11052)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* upgrad oneflow version to 0.7.0* fix* continue push* continue pushCo-authored-by: hhhfccz <hjk1938927583@163.com>	0
[ci] Re-run flaky tests on failure (#12108)This is a follow up to implement the library added in #12055Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Improve error message for custom tflite operators (#3284)	1
[Relay][Transform] Support Dumping IR to help debugging (#3493)* [Relay][Transform] Support Dumping IR to help debugging* debugprint->printir	0
[CODEGEN] More storage alignment info aware generation (#186)* [CODEGEN] More storage alignment info aware generation* fix* fix* fix warning	2
[Perf] Enhance cudnn and cublas backend and enable TensorCore (#4353)* add half and mix precision support to cublas backend* add TensorCore support in CuDNN* enhance CuDNN support* address comments and fix lint* fix* add fp16 test	3
[TOPI] Add support for groupped conv3d (#9873)* [TOPI] Add support for groupped conv3dChange conv3d to use generic conv implementation which supports grouppedconvolutions. Also, remove support for non-float16 tensorcore operationsas they cause large degradation in accuracy. Generic conv now supportsautoscheduler.* correct none check* add tests for floordiv simplification* fixed incorrect test for autoscheduler* formatting* add groups to winograd* fix tensorcore* manually simplify index instead of relying on simplifier* formatting* add groups argument to conv3d_ncdhw_winograd_without_weight_transform* formatting	1
[DOCS] Added casting to hybrid script doc and fixed pass infra doc (#6174)* updated hybridscript docs and pass infra docs* forgot uint16	2
[microTVM] Zephyr: refactor _find_openocd_serial_port (#10346)Refactor _find_openocd_serial_port() as a generic USB serial portfinder since other runners beyond openocd use it (e.g. jlink runner).Also instead of using redundant hardcoded values in BOARD_USB_FIND_KWdict, use idVendor and idProduct from boards.json. And don't use 'usb'module to first find the serial number of the port and then pass it to'serial' module to obtain the port path, instead search for the portpath directly via 'serial' module using the serial number (if provided)or use idVendor and idProduct values taken from boards.json.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	5
minor fix after loading trt engine from disk (#11614)	0
[REFACTOR][ARITH] Remove legacy compute_expr.h (#5738)Replaces most of the ComptuteReduce using foldl.	1
[UnitTest] Disable ptx mma tests on unsupported nvcc versions. (#10229)* [UnitTest] Disable ptx mma tests on unsupported nvcc versions.- Modified `tvm.contrib.nvcc.get_cuda_version` to return a  `(major,minor,release)` tuple rather than a float.- Implemented `tvm.testing.requries_nvcc_version` decorator to specify  the minimum `(major,minor,release)` version needed to run a unit  test.- Applied decorated to unit tests in `test_tir_ptx_mma.py` that fail  on earlier nvcc versions.* Fix lint errors.* Updated a few of the cuda version checks.* More lint fixes.* Only compare major/minor in find_libdevice, not release version.	0
[RPC] Don't use existence of USE_HEXAGON_SDK as enablement check (#11080)* [RPC] Don't use existence of USE_HEXAGON_SDK as enablement checkUse USE_HEXAGON to check if Hexagon support is enabled or not.This fixes https://github.com/apache/tvm/issues/11059.* Restart CI	0
Relay C++ Build Module (#3082)* [Relay] C++ Build module* asdf	5
Unit test for DFPatternRewriter on deeply nested sub-graph with attributes on call. (#10533)* Unit test for DFPatternRewriter on deeply nested sub-graph with attributes on call.* - newline, disaster averted	1
[microTVM] Update pyproject to python3.7 (#11634)* Update to python3.7 and add poetry.lock file	2
[ci][tvmbot] Search more users when checking usernames (#12491)To figure out a user's association with the repo this code beforesearched the associations in the repo filtered by the relevant username.GitHub doesn't return the exact match only though, so we have to insteadcollect many results and search through all of them.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix prelu bug in onnx frontend. (#7208)	0
[ci] Disable failing hexagon conv2d test (#10666)See #10665. This doesn't disable just the parameterized version that is failing since our parameterization is buried within TVM and shared among many tests.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
keras.layers.ReLU added (#1530)	1
add stage to log (#9249)	2
Remove mutable defaults in mlp_model (#12546)	4
[TF][Relay][Op] Pass module when infer shape (#4287)* [TF][Relay][Op] Pass module when infer shape* Fix lint* Improve style* Add test	3
[TOPI] Fix CUDA pooling schedule (#8957)	0
Fix typos in target warn of dnnl (#11678)	1
[Runtime] Special Memory Scope Support (#7488)	1
[CMSIS-NN] Convert scalar constants to tensor constants (#10100)	5
[Relay] tflite frontend, keep underline with comments in same length. (#3363)	5
[nnvm] Add caffe2 frontend (#1981)	1
[ci] Fix diff condition for docker builds (#10684)Docker builds weren't triggering on main since it was diffing changes from `main` (and there weren't any), so this fixes it so the diff is checked against the previous commit for builds on `main`.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[MIPS] Fix CALL16 reloc at 0x290 not against global symbol (#7634)	0
[Relay] Add support of conv2d with NHWC for Mali (#8422)* [Relay] Add support of conv2d with NHWC for MaliAdded template schedule for conv2d NHWC reusing similar strategyas for NCHW layout. The schedule is also added to thecorresponding test that can be run to verify correctness.* [Relay] Fix issue from pylint in conv2d for Mali	0
[skip ci][ci] Skip actions on forks (#10468)	5
[MetaSchedule][Minor] Stability Improvements (#12014)* Fix tuning util for uint8.* Change to check runner_result.* Revert change to let cost model learn.	4
[ONNX] Fix cast op to/from bfloat16 (#11171)* fix cast from bfloat16* fix cast to bfloat16 test as well* clean up comments* lint* add commentCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
[relay][external codegen] outline and inline lifted functions for external codegen (#4996)* outline and inline lifted functions for external codegen* add batch_norm test* test batch_norm inline	3
[Graph tuner]Add opt out operator for has_multiple_inputs for graph tuner (#5000)* consider layout_transform in has_multiple_inputs* refactor code* remove debug info* remove subclass assignment* refactoring a little bit* remove default value* remove trailing whitespace* modify test for has_multiple_inputsCo-authored-by: Ubuntu <ubuntu@ip-172-31-40-194.us-west-2.compute.internal>	3
to fix issue Target llvm is not enabled[followup] (#3404)	0
Undefined name: Typo in variable name sotrage_order --> storage_order (#3439)Discovered via: __flake8 . --count --select=E9,F63,F72,F82 --show-source --statistics__	2
[Test] Add tvm.testing.requires_libtorch (#12737)Create a specific test dependency to map to USE_LIBTORCH, whichis disabled by deafult, and is independent from torch beinginstalled on the underlying machine, so it causes problems inmachines that have torch installed but TVM is build withUSE_LIBTORCH OFF.Mark tests.python.contrib.test_libtorch_ops.test_backend withthis new decorator.	1
[microNPU] modify the demo to use USMP (#10511)* [microNPU] modify the demo to use USMPThis commit changes the demo to use USMP.Change-Id: I87e85fdd9b2efea9559dda186c1644e4d63509ff* [microNPU] modify the demo to use USMP* removing a repeated TVMC CLI argumentChange-Id: I3e1ec328155bfc4c9ce813224a7f06b09e66232c	4
[BYOC-DNNL] suppport more dnnl ops (#11823)* support dnnl.global_avg_pooling2d* fuse pad-avg_pool2d* fix lint	0
Move SimplifyConvPad to a new pass and don't enable it by default (#7603)* Move SimplifyConvPad to a new pass and don't enable it by default* rename pass* move files* fix lint* adjust test tolerance	3
Checking the correct dtypes for choosing the Intel int8 instructions. (#3516)	2
Add a testcase of dilated conv2d int8 (#2065)	3
[FIX] Remove leftover instances of USE_GRAPH_EXECUTOR_DEBUG (#8796)* [FIX] Remove leftover instances of USE_GRAPH_EXECUTOR_DEBUGsingle flag, USE_PROFILER. This PR cleans up the last few remaining usesof USE_GRAPH_EXECUTOR_DEBUG.* formatting* Update CMakeLists.txtCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
Use int for endch to fix portability issues regarding signed/unsigned char (#4668)	0
[Relay] Use target_host determined at Relay level instead of recalculating it (#9499)	1
Add simplify pass in the Relay interpreter (#2417)	4
[Relay] Clip gradient: grad * (select(x < min || max < x, 0, 1)) (#3509)	5
fix case when offset is odd and size is even (#3643)	1
[Relay][RFC] Relay IR Text Format (#1781)	5
Update README.md	2
Pin python pillow to "<7" due to torchvision 1.2.0 dependency issue (#4632)* As a result of backwards incompatible changes released in pillow 7.0,   torchvision crashes if you just "pip install pillow", as we do in   a few places. * This patch sets pillow<7 to be installed in Dockerfiles and support   material as tutorials and documentation.	2
[QNN] Requantize - Optimize lowering for some corner cases. (#3864)	5
Update precision in the ONNX strided_slice, update precision of ToScalar (#6272)* Update precision in the ONNX strided_slice, update precision of ToScalar* fix tests	3
Improve the tensorflow frontend _test_spop_resource_variables to support tensoflow 2.6 (#9978)On tensorflow 2.4 the test is expected to fail as the generated graph is not forzen.On tensorflow 2.6 the generated graph is identified as frozen, therefore the test is not needed	3
[BYOC] Remove ext params stored in metadata from params to avoid duplication (#7977)* Remove ext params stored in metadata from params to avoid duplication* Add test for duplicate params	2
[CodeGen][CUDA] use hrint for cuda half rounding (#10460)When cuda c codegen generate `tir.round` for fp16, there is no function named `hround`, but `hrint` for cuda half arithmetics. https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH____HALF__FUNCTIONS.html#group__CUDA__MATH____HALF__FUNCTIONS_1gbbf7a989130edcbdbfbb4730f61c79b1Testcase to reproduce:```pythonimport tvmfrom tvm import relayfrom tvm.ir.module import IRModulex = relay.var("x", shape=[16], dtype="float16")y = relay.round(x)f = relay.Function([x], y)m = IRModule.from_expr(f)m = relay.transform.InferType()(m)relay.build(m, target="cuda")```	1
[RELAY] Remove re-exports of tvm.transform (#5337)	4
Expose list of PassContext configurations to the Python APIs (#8212)* Expose C++ PassContext::ListAllConfigs via its Python counterpart   tvm.ir.transform.PassContext.list_configs() * Add unit tests for the C++ and Python layers	3
Get tags of saved model automaticallyRemove exception trail in tf parser error messageFix lintFix comments	0
[Relay] Start porting pass to the pass manager (#3191)	4
[RELAY] Fix type info after mutation in simplify inference (#2093)	5
[Hexagon][Codegen] Implement CodeGenHexagon::CreatePrintf (#10710)* [Hexagon][Codegen] Implement CodeGenHexagon::CreatePrintf`CodeGenHexagon` inherits from `CodeGenLLVM`, but debug messages sentthrough `printf` calls do not make it back across the RPC server.Instead, the `FARF` preprocess macro provided from the Hexagon SDKshould be used.  This implementation of `CodeGenHexagon::CreatePrintf`generates the same `HAP_debug_v2` function call as would be generatedby the `FARF` preprocessor macro, using the `ALWAYS` print level.* Updated following review comments* Updated from const llvm::ArrayRef& to llvm::ArrayRef.	5
Add Check about negative uint constant (#10484)* fix InferType bug* fix InferType related bug* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative	0
[FIX] Don't add $TVM_HOME/.. to the include path when compiling code. (#7342)If the user has a dmlc-core directory next to the tvm directory, thisdmlc-core directory will be incorrectly used when compiling files withcc.py.	2
Error msg update (#5818)	5
fix version comparison (#200)	0
Makes sure g_last_error is null terminated. (#7190)This addresses GCC 10 error:```"src/runtime/crt/common/crt_runtime_api.c"include/tvm/runtime/c_runtime_api.h: 在函数‘TVMAPISetLastError’中:src/runtime/crt/common/crt_runtime_api.c:42:3: 错误：‘strncpy’ specifiedbound 1024 equals destination size [-Werror=stringop-truncation]   42 |   strncpy(g_last_error, msg, sizeof(g_last_error));      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~cc1：所有的警告都被当作是错误```	0
[Relay] clean up hd, change tl (#2917)	4
Fix the Type bug in ConvertSSA. (#6709)Co-authored-by: YushengMa <yusheng.ma@streamcomputing.com>	0
[OPENCL][RUNTIME] Fix race condition of modules (#2018)	0
[Relay][Op]Support symbolic TopK, Ones, Zeros and Full (#5459)* Support symbolic TopK, Ones, Zeros and Full* Fix pylint* Add docstring for topk shape func* Fix grad* Fix lazy_gradient_init* Fix parser* Fix print ir text* Fix lint* Improve pattern_util* Fix topk* Fix build* Use Optional for attribute* Fix clang-format* Minot fix* Fix pylint* Fix build warning* Fix parser* Move ToScalar* Fix lint* Fix lint* Make topk shape func as data independent when k is constant.* Fix lint* Minor fix	0
[microNPU] Move optimization passes to be a module pass and ensure they (#9831)are runningMoves LayoutOptimizer and LUTOptimizer passes to be a module pass,rather than a function pass. This is because it was found that thesepasses were not running in the NPU compilation flow. In addition, atest for both LayoutOptimizer and LUTOptimizer has been added to checkthat the passes are running in the compilation pipeline of the NPU.Change-Id: I5145c6f02eeb0daea3cdba56198e0804ec32f351	5
Export tvm::relay::OpRegistry::OpRegistry (#3711)	1
[CI] Add log check to the sphinx gallery docs (#5643)* [CI] Add log check to the sphinx gallery docsThis PR add log check to sphinx gallery tutorials to preventthe case when sphinx failed to capture the error in tutorials.* Fix the status	0
[TIR][PASS] dtype rewrite for indexing variables (#5092)	4
[BUILD] warning fix: new does not have an alignment parameter     (#1478)	2
Fix the shift column for scale_shift_nchw and scale_shift_nhwc in C topi (#5679)	0
Add Action to add cc'ed people as reviewers (#9934)* Add action to label mergeable PRsDevelopers often have to ping a committer once their PRs are both passing in CI and are approved. This helps facilitate this process by marking such PRs with a label `ready-for-merge` so committers can easily filter for outstanding PRs that need attention.* Fix lint and add tests* Add Action to add cc'ed people as reviewersThis provides a mechanism for non-triager/reviewer/committer PR authors to request reviews through GitHub. Anyone that is referenced by `cc @username` in a PR body will be added as a reviewer (GitHub will limit the reviewers to those with actual permissions to leave reviews so the script to add can be simple).* remove merge bot stuff* Fix target triggersCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TVMC] Python Scripting Init Files (#7698)* add to init files for clean tvmc python* black reformat init.py* adjust tests to new imports* black test files* tell lint ignore defined-builtin error for tvmc compile* add colon to match lint syntax* change import so must use tvm.driver.tvmc instead of tvm.tvmcCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>	1
[Frontend][TFLite] Add MIRROR_PAD operator (#4822)	1
[Relay] Fix BatchMatMulRel typerelation (#3032)return false mean retry in the future, and in the case of error, it should be report ASAP, not retry.	1
[microNPU] Update Conv2D Tests to Use TF API to Gen Test Cases (#9508)* Current conv2d tests compare the conv2d operator against tvm's execution of the default schedule of conv2d as defined in TOPI and that is not bitexact with tflite runtime's implemention. Therefore a tolerance of "1" in quantized 8-bit domain is used.* Converts the current conv2d tests to use TensorFlow APIs to create a test cases for conv2D and compare against TFLite runtime.	1
[Pass] Finish infershape testcase (#16)	3
[IR] Minor cleanup to tvm.ir.instrument.PassInstrument (#9392)Allowed users to subclass from PassInstrument directly.  Themotivating example was adding a `@classmethod` to a class decoratedwith `@pass_instrument`, but the class method wasn't passed through.	4
[Pass] Fix printer formatting for PassInfo (#10844)* [Pass] Fix printer formatting for PassInfoThe format of the printed PassInfoNode was difficult to read as therewas no separation between the different attributes of the nodeBefore this change, PassInfo is printed as:The meta data of the pass: pass name: tir.ApplyLayoutTransformsopt_level: 0required passes: []After this change, PassInfo is printed as:The meta data of the pass - pass name: tir.ApplyLayoutTransforms, opt_level: 0, required passes: []* Restart CI	4
[FIX,ROOFLINE] Handle mismatched compiled and TIR hash (#12219)Fix a bug where roofline analysis would crash if the provided tirfunctions do no match the profiling report.	1
[RELAY][Parser] Optimize relay parser to restore calls attrs (#7347)* [RELAY][Parser] Optimize relay parser to restore attrs for non-Operator calls* To avoid too much modification to the native class, only print out the attrs  type key of non-Operator Call in relay printer. Then reconstruct the attrs object  after parsing this attrs type key value in Relay parser.* fix lint* fix ci* add test case	3
[TIR] For-kind inheritance in decompose-reduction (#9814)	5
[microNPU] Add transform matrices and part matcher to identity op (#11453)* [microNPU] Add transform matrices and part matcher to identity op* Address comments* Enable cascader in identity tests* Address comments	1
[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)* fix factor divide by zero* add a test case in test_tir_transform_lower_warp_memory.py* fix type* make it more elegant	1
[Metaschedule] Add demonstration of selectively tuning relay ops with TIR schedules  (#10793)This demonstrates how to selectively extract and tune tasks from a whole relay mod, and apply the tuned schedule during the final `relay.build(...)`. This flow is entirely different from existing tests in `test_meta_schedule_tune_relay.py` where ALL ops are extracted and auto-scheduled by MS. My test extracts only int8 `dense` op, applies a manual TIR schedule on it, and leaves int8 `batch_matmul` to be scheduled by TE. This also serves as an example of autotvm style manual template + tensorization. The manual TIR schedule is equivalent to TE VNNI `dense` schedule in https://github.com/apache/tvm/blob/ce335c3a74185df6cc1152e53c60695d8a418d8e/python/tvm/topi/x86/dense.py#L366-L375	4
Document CMSIS-NN Options. (#9647)	2
[microNPU] enable USMP (#10022)This commit enables USMP in the microNPU codegenand tests. The microNPU codegen is modified tosupport Let nodes that are produced as from USMP.	1
[logging] LOG(FATAL) calls [[noreturn]] functions (#11310)Ensure that `LOG(FATAL)` always resolves to calling`[[noreturn]]` code.  This has two benefits:- Helps developers more quickly understand the intended/required  behavior for `LOG(FATAL)` calls.- May eliminate spurious compiler warnings based on control-flow  analysis.  E.g. gcc's / clang's `-Wno-return` warnings.	2
Issue in nnvm is fixed (#1176)	0
[Android][RPC] Add missing RPC sources after refactor  (#6113)	4
[Backend][Verilator] Multiple fixes (#6995)* bump vta-hw submodule version* fix cmake related stuff	1
[Relay] [Op] Squeeze (#1858)	5
[GraphRuntime] remove print from GetInputIndex (#7027)* remove print* retrigger CI, flaky test failure	0
Fixed additional deprecation warning in file (#10318)	2
Add per channel quantization to QLinearConv and fix related bugs (#10354)	0
Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 software (#9672)* Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 softwareChange-Id: I22685c117f3b6e9bc53c25ede14bb91c2b9f85f3* Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 software- Allow CI tests to pass with existing FVPChange-Id: I67117de96c525fa01dae1d402a5f08743681a246	4
Cleanup more uses of np.bool and np.int. (#8399)In a similar vein to previous pull requestsreplacing deprecated use of np.bool and np.int fromnumpy with bool and int.https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations	2
[TIR][Schedule] Support annotate dict typed value (#12288)* tir schedule support annotate dict typed value* fix lint* fix comment issues	0
[CI] Set main as default in github actions (#6669)	1
Fix typo in relay.vm.Executable (#7543)Co-authored-by: Yanming Wang <yanmwang@amazon.com>	2
[LANG] CommReducer (#103)* [LANG] CommReducer* Reorganize c_api* Remove InitValue and Combine; refactor Functor* Make CommReducer an Expr* Make comm_reducer type independent* Make CommReducerNode a Node* Small fix* Refine* Refine front api; add integration testcases for min/max* Fix python* Refine* Fix lint and add example	1
Let CUDNN choose the best algo (#734)* use cudnn findalgo to choose the best algo* fix lint	0
Change color channel from BGR to RGB for darknet preprocessing (#4794)	4
[Relay] SpaceToDepth and MirrorPad Operators (#3718)* Added relay and topi mirror_pad operator.* Added mirror_padding to tensorflow frontend.* Added mirrorpad testing in tensorflow frontent.* Added space_to_depth in tf frontend.* Added tests for spacetodepth.* spacetodepth bug fix.* Lint fix* Added mirror pad python attrs.* Pad code formatting.* Syntax improvement* Hopefully last lint fix	0
[TVMSCRIPT] Misc error message improvements (#9543)* [TVMSCRIPT] Misc error message improvements* only prevent indexing into handles with multiple indexes* lint	0
[µTVM] Try to fix qemu hangs in the CI #7590 (#7769)* Try to fix qemu hangs in the CI. * Remove __pycache__ directories only underneath checked-in   subdirectories to hopefully avoid long find runtime.* try just removing the check	4
[Relay][Refactor][std::string --> String] Relay updated with String (#5578)	5
[AUTOTVM] Fix a bug in generating the search space (#4779)- Do not use numpy.prod which ignores integer (64 bits) overflows.  This leads to an incorrect number of points in the search space.	1
[Pytorch]layernorm bug fix and testcase updated (#5257)	5
[Relay] Alpha equality tests for Relay exprs (#1871)	3
Add BN support with run-time mean and variance calculation (#4990)	1
fix buffer elem_offset calculation (#1762)	1
[Hexagon] Deprecate USE_HEXAGON_DEVICE, introduce USE_HEXAGON (#11025)The new cmake flag `USE_HEXAGON=[ON|OFF]` enables/disables Hexagonsupport in TVM and TVM runtime. It should be turned on _whenever_Hexagon support is required, even when compiling TVM runtime forHexagon itself.This is one in a series of commits intended to remove offloadsupport, and make the whole-model support the default mode ofoperation.With `USE_HEXAGON_DEVICE` deprecated, offload runtime is not builtanymore, so register `device_api.hexagon` to be same as `.v2`(presence of device API is taken as evidence of support for thedevice in TVM, so this step is necessary).	1
register depthwise conv2d as generic function (#1108)	1
[Hexagon] Correct use of wrong cmake variable (#10769)The code should be checking DSPRPC_LIB_DIRS instead of REMOTE_DIR.	1
[DOCKER] Update docker protocol (#2793)	2
[TIR, Relay] improve bfloat16 support (#10112)* update AMP table to enable ResNet50 conversion* add runtime datatype dispatch for BFloat16* skip asserts for uint16 for bf16 compatibility* add bf16 cast for the unary intrinsic operators* enable "bf16<-->fp32<-->any dtype" casting* support inconsistent input for bf16 BIOP legalize* add treatments for bfloat16 in if statements* add bfloat16 dtype casts in binary OP* delete unnecessary treatments for bfloat16* add test for bfloat16 building* code style* restore the modifications in .gitignore* restore the changes to AMP lists* fix typos* fix lint errors* fix typo	2
[PYTHON] Enable constructors in Node (#1647)	0
[TESTING] Mark CMSIS-NN test in TVMC tests (#10674)Currently failing with `USE_CMSISNN` set to `OFF`	1
Fix LLVM version for Hexagon (#9711)	0
[Relay] Type Relation Fixes (#7362)* fix an error in the dynamic Full Type Relation* Add Diagnostic Errors to Broadcast Type Relations	0
raise right error in tensorflow split op (#5951)	0
[CI] update ci-gpu to the latest (#5469)	3
[TensorFlow] Support NonMaxSuppressionV5 (#6933)	1
[Relay] Add TopPattern to nn.dropout (#7685)	4
Checkin IR Visitor and tests	3
add a testcase for #5674 (#5677)	3
Keras Frontend (#273)* vgg16 success* remove six.PY2, use sys.version_info;convert_activation() accepts activation type name(str, e.g. 'relu') as input;* add convert_merge* fix convert_batchnorm;improve tests* fix lint* add numpy-style pad operator* deal with asymmetry padding* resnet50 success* fix pool_convert; xception passes test* update tvm* fix bias error; all tests pass* use > >, not >>	1
add tvm.micro pydoc to sphinx (#5661)* add tvm.micro pydoc to sphinx* making build pass and addressing tqchen comments	1
[MAINTAINER] Add zhreshold as reviewer (#1287)	1
support Torch all and any op (#9185)	1
Cache PrimExpr instead of raw pointers in bound analyzer (#5533)The objects that the raw pointers point to can be deallocated and newobjects can be allocated at the same address, all while these pointersare still in the cache. This can lead to unexpected behavior, forexample to calculated bound conflicts with previously cached values.Caching PrimExpr will prevent the objects from being deallocated whilethe cache is active.	5
Fix typos and format in comments (#8132)* Fix typos and format in commentsFix typos and format in comments about the registry manager ofpacked functions.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix lintNo more than 100 characters per line is allowed.	1
[BugFix][MetaSchedule] Fuse only serial loops in rewrite-unbound-block (#10883)	1
[Relay][Quantization] Fix out-of-date realize (#3790)	5
Fix the building error in android_deploy (#1262)* Fix a link in android_deploy/README.md and a error while building android_deploy.* revert and change APP_STL in Application.mk	4
[REFACTOR][API-Change] Migrate all Object construction to constructor. (#5784)This PR migrates all the remaining object constructions to the new constructor stylethat is consistent with the rest of the codebase and changes the affected files accordingly.Other changes:- ThreadScope::make -> ThreadScope::Create- StorageScope::make -> StorageScope::Create	1
[COMMUNITY] Nicola Lancellotti -> Reviewers (#11226)* adding new contributor* edit* Update CONTRIBUTORS.mdCo-authored-by: Nicola Lancellotti <nicola.lancellotti@arm.com>Co-authored-by: Nicola Lancellotti <nicola.lancellotti@arm.com>	5
[Relay] Fix dynamic case for Squeeze and Split (#6739)* [Relay] Fix dynamic case for Squeeze and Split  Squeeze: Allow removed dimension to be dynamic and check it in shape  function  Split: Fix negative axis* Fix comments	0
[CI] Update actions miniconda (#6926)	5
[Relay][Op] Make Type Relation catch more errors (#3899)* save* init* move type_relations	4
[NIT] fix relay invariant error message (#3011)* [NIT] fix common error messageExtremely minor issue, but this is one of the most common error messages people see...* Update type_solver.cctrigger CI	5
[TFLite] Quantized unary elemwise ops (#10566)* [TFLite] Quantized unary elemwise ops* fix cos	0
[microNPU][2a] Add CascaderGraph for cascading analysis (#9469)A CascaderGraph augments a TE graph with additionalinformation needed by the cascading algorithms. Thisincludes defining a strict ordering on the operatorsas well as including all the Propagators needed todo the affine analysis of cascades.The CascaderGraph consists of two object types, Partsand Tensors. A Part is an augmented operator whichincludes the Propagators and a Tensor is similar to aTE tensor but stores additional information likecompression ratio.	5
[hexagon][testing] add TIRScript elemwise-add (#11490)Replace TE-based elementwise-add benchmark witha TVMScript-based one.Update Hexagon target architecture from v68 to v69.As a result, the benchmark now requires a version ofHexagon SDK newer than 4.4.0.1.  Version 4.5.0.3 isknown to work.	1
winograd_nnpack (#2721)	5
[Hexagon] Add schedule and test for conv2d_transpose_nchw (#11175)* Add test for registered scheduales - depthwise_conv2d* added more test to depthwise_conv2* adding new line at the end of the file* reformatted the file* resolve comments* add schedule and tests for conv2d_transpose_nchw* registering conv2d_transpose strategy and clean up test	3
[DOCS] Add install docs, fix Jenkins (#57)	0
perfom full rpc tracker handshake (#1500)	5
Added equality check and upgraded concatenate op (#1172)	1
[TOPI] Fix atlest1d for reduce and squeeze (#2147)	0
[WIP][AUTOTVM][TOPI] Port x86 NCHWc to AutoTVM for Task Extraction (#2664)[AUTOTVM][TOPI] Port x86 NCHWc to AutoTVM for Task Extraction	4
[DOCS] Fix tag_scope example (#581)	0
Add UIntImm to select rewrite (#909)	1
Temp checkin c++ code.	5
[VTA] pynq v2.1 -> v2.3 (#1945)	5
Add the Arm(R) Ethos(TM)-U NPU identity operator (#9457)* Add the Arm(R) Ethos(TM)-U NPU identity operator* Add the ethosu.identity operator which returns the input tensor* Add an opportunity to requantize the tensor* Add legalization for reshape and strided slice* Add a pass that puts an indentity op after a no-opChange-Id: I0adb5ca269f8529c79e0e7681ca4b5147d8f53c8* Fix the pylint errorsChange-Id: Icc9b6507f164681a5d6b1fcff2ae4a5051d44734* Changes in response to review commentsChange-Id: I63f30f84ad481789fc047ad8c2107f5313562f7f	4
AArch64 base algorithm refactoring in LLVM (#6907)* AArch64 base algorithm refactoring in LLVM- I refactored the assembly in arm_cpu/tensor_intrin.py to use LLVM+TIR- Removed the `interleave` boolean parameter in the intrinsic to switchamong two different interleaving modes. LLVM will now take care ofinterleaving the instructions- Applied the changes accordingly to conv2d_gemm.py to call the rightinstrinsicNote: I found LLVM very sensible to the choice of the `-mcpu`.So, in order to preserve performance, it is important to specify theright `-mcpu` when creating the LLVM target* Fix linting* Fix linting -2* Fixing comments* Address review comments* Fix spaces around ':' in docstrings	2
[KERAS]RepeatVector, Conv3DTranspose op support added (#5833)	1
[Frontend][PaddlePaddle] Support conv2d_transpose/rnn/fill_constant_batch_size_like (#9564)* add conv2dtranspose, rnn, fill_batch_size_like* fix conv_transpose and add RNN test case* Update paddlepaddle.py* Create paddlepaddle.py* fix scale attr of convert_interpolate* black codeCo-authored-by: heliqi <1101791222@qq.com>	0
[PATCH] Fix undefined __floatdihf in libtvmruntime.so on aarch64. (#4119)Arm architecture provides optional FP16 floating point support in two alternative formats, IEEE and an an alternative Arm format.The ACLE (Arm C Language Extension) defined preprocessor symbol __ARM_FP16_FORMAT_IEEE can be used to distinguish between implementations providing IEEE and the Arm alternative format, but cannot, on its own, be used to determined if FP16 HW support is actually present.Testing this preprocessor symbol can lead to undefined __floatdihf at runtime on an aarch64 target where no FP16 HW is present.The relevant preprocessor symbol to determine whether FP16 HW support is present in the target is __ARM_FEATURE_FP16_SCALAR_ARITHMETIC, this symbol implies  __ARM_FP16_FORMAT_IEEE.The relevant preprocessor symbols are defined by the ACLE standard, section 5.5.21 16-bit floating-point data processing operations, https://static.docs.arm.com/101028/0008/Q2-ACLE_2019Q2_release-0008.pdf	2
[PyTest] Sort by test location, but not parametrization (#9353)A follow-up from https://github.com/apache/tvm/pull/9188.  The`item.location` tuple contains `(filename, line_number, test_name)`,where the `test_name` includes a string representation of allparameters.  This change preserves pytest's sorting of parametrizedvalues within a parametrized test, rather than sorting by strings.	3
[TVMC] Workspace Pools Parameters (#11427)* [TVMC] Workspace Pools ParametersAttributes from tvmc are now passable into the created PoolInfo objectsinside WorkspaceMemoryPools. This is passed in to relay.build that getattached to IRModule attribute.* [TVMC] Workspace Pools ParametersAddress comments, fix linting. Testing improved.Change-Id: Iea79329b6b9ec1cbc51e5c293449bf6dd43b00c5* [TVMC] Workspace Pools ParametersUpdate workspace pools test namingChange-Id: Ib698d6248be1e6f44340f27db3641c985bc5c5d8* [TVMC] Workspace Pools ParametersAdd test for parameter overrides.Change-Id: I67d5470dcfbfbc9ab27f34e20a9269d2070193ca* [TVMC] Workspace Pools ParametersRebasing over #10189Updates to the way a WorkspaceMemoryPool object is createdChange-Id: I1f0e1d240343af311ddb3ed5c564cc1ab329f463* [TVMC] Workspace Pools ParametersFix linting, fix CIChange-Id: If75f8709ac4ad925655eca54b3e5c1bb09d025e8* [TVMC] Workspace Pools ParametersAdd mcpu and mattr to target registry for cmsis-nnChange-Id: I15257b8d01624c071c738cab6d12ecb84ed6cb16* [TVMC] Workspace Pools ParametersAdded test for override on single pool when multiple pools are presentUpdated functionality of parsing multiple attributesChange-Id: I2c0745051b7a923dd7f75040bfb89bbc99376a11	4
[FIX] Fix target in tutorial (#347)	1
Update Makefile	2
[PASS] Canonical form simplify (#34)	4
[CODE] Halide attributions (#3824)	5
Fix a bug of flatten in ONNX to Relay converter (#3180)* fix onnx frontend flatten bug* Update onnx.py* Update onnx.py* Update onnx.py	5
Require LLVM >= 9 for AMDGPU backend (#4253)LLVM 8 will crash when loading the bitcodesThis is a runtime check as the file will be compiled in even whenUSE_ROCM OFF is used in the configuration if ROCM is installedin the default location.Fixes: #4087	0
[CUTLASS, Eazy] Cache profiling result and support compiling generated kernels in parallel  (#9402)	1
[Doc] Fix link error in pipeline executor tutorial (#12185)Fix the link error.	0
Fix GraphRuntime with -link-params over RPC (#6985)* Fix GraphRuntime with remotely-linked params. * Previous test did not exercise this correctly.* fix incorrect function name	1
Allow cuDNN in non-CUDA non-system dir (#7608)cuDNN is not a builtin library of the CUDA toolkit package.The user can install it in the CUDA directory, the systemdirectory, or anywhere else. This patch relax the restrictionof locating cuDNN in the CUDA directory. This is helpfullwhen trying out different versions of cuDNN.	1
[Hexagon] Remember to add common sources when building TVMRT for Hexagon (#10290)	1
[NNVM][POOL] bug fix. Remove the hardcode. (#1600)	4
[MetaSchedule] Developer Ergonomics Enhancement II (#11727)Follow-up of #11622, per discussion with @Kathryn-cat- [x] Allow using a string `"default"` in `TuneContext` to quickly specify a set of target-specificrules- [x] Enhance detection of `ScheduleFn` in `TuneContext` to make it easier for users to quickly tryout template-driven scheduling on TIR.Next PR:- Add `TuneContext.tune` to allow directly tuning without task scheduler.Co-Authored-By: Kathryn (Jinqi) Chen <65606304+Kathryn-cat@users.noreply.github.com>	1
[REFACTOR] IRPrinter->NodePrinter, move to node/printer.h (#4622)Rationale: printer is a common infra that is shared across all nodes.	5
[TOPI] Added support for Mali Bifrost target (#4047)	1
[Doc][Fix] Fix a typo in hybrid script tutorial. (#6525)	2
Docker updates (#2702)* [DOCKER] Switch from yes|apt-get to apt-get -yThe yes | apt-get idom guarantees that the 'yes' process always existswith exit code 141 (pipe broken).  This is fine while the scriptgenerally ignores failures but won't work when the script behaviour istightened to robustly catch errors.* [DOCKER] Turn down the wget/curl volume	1
[TOP] Level1 complete (#3)	5
[ci] Add mechanism for trust on certain CI scripts (#12604)This makes it so changes to certain files from users not listed in`CONTRIBUTING.md` are not tested in CI. This is necessary since thesescripts run on the baremetal EC2 instances and not inside Dockercontainers, so they can affect other builds and potentially grab Jenkinssecrets. This checks out the version from the upstream for the listedfiles after running `git checkout`. Tested in CI: [positive](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12604/6/pipeline/) and [negative](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12604/9/pipeline/)	3
Change op function pointer to std::function, enable mutation (#6)	0
Improve x86 roi align (#3296)* Improve roi_align performance for x86* Change test	3
[Relay] Dense alter layout fixed for packed input (#8669)* clean up typerel* add layout transform when input is 3D* add test* update doc to clarify that only 2D input data is supported* add weight_layout attribute in dense* remove explicit layout transform from dense_alter_op.py* Add DensePackInferCorrectLayout to insert layout transform* relax type rel* revert type rel relax and add check on dim* introduce DensePackAttrs to avoid breaking dense op* try fixing arm compute lib test* Update tests/python/contrib/test_arm_compute_lib/test_dense.pyCo-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>* formattingCo-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>	1
Fixed minor misspelling (#7499)Co-authored-by: mshr-h <mshr-h@users.noreply.github.com>	1
[TOPI] Remove cpp upsampling and resize op (#4769)* remove cpp upsampling* remove cpp resize	4
Fix NormalizeError to allow colon inside CHECK (#9670)	1
Improve the keras frontend to support tflite 2.6 (#9562)This code change keeps compatibility with tflite 2.4	4
[TensorIR] Primitive "SetScope" (#9738)* Main code* Reorder steps* Unittests* Docstring* Check the input storage scope* Docstring for `CheckStorageScope`* Import header	2
[JVM] Support overriding RPCWatchdog termination behavior on Android and other platforms (#6216)* Instead of performing a system exit and leaving unhandled items onthe activity stack, finish the RPCActivity and return cleanly to theMainActivity where the RPCActivity can be restarted automatically.* Update doc. string for checkstyle.	2
[TOPI,RELAY][TFLITE] Sparse to dense operator (#5447)* [Relay][Frontend][TFLite] Add parser support for shape and rangeSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* [TOPI,RELAY][TFLITE] Sparse to dense operatorSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* use param name in documentationSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* sphinx doc errors fixedSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* incorporated review commentsSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Missing a blank line...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>* use get_tensor_exprSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Accidently removed this function in the rebase...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>* support default value for default_valueSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* clang format fixesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* topi pylint fixesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>	0
[microTVM] Add support for the Raspberry Pi Pico via Arduino (#11694)* Add RP2040 support	1
[TOPI] add squeeze (#494)* add squeeze* should be squeeze	1
Rename FInferLayout -> FCorrectLayout (#453)* rename FInferLayout -> FCorrectLayout* correct stupid IDE* update submodule tvm	5
Reset sphinx-gallery version to 0.4.0 (#9280)Seeing:```ERROR: Could not find a version that satisfies the requirement sphinx-gallery==0.4.1 (from versions: 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.10, 0.0.11.post1, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.9.0, 0.10.0)ERROR: No matching distribution found for sphinx-gallery==0.4.1```This was changed in https://github.com/apache/tvm/pull/9115	4
posix_memalign appears in API 17, not 16 (#3532)	5
[RELAY][REFACTOR] Mix mode context analysis (#6403)* mix mode context analysis* add uses_gpu decorator for more tests* revert visit counter* relax visit limit* lint* bump visit limit to 19* typo	2
dilation fixed for (1, 1) case (#477)	0
Vulkan2 Runtime API (#3849)	1
[DOC] Fix typo (#6920)	2
nn.batch_flatten is a reshape op (#11367)	5
[CODEGEN][COREML] Call InferType explicitly in coreml test (#6676)	3
[RUNTIME] Implement TVMDSOOp(TensorFlow custom op) for TVM runtime (#4459)* Add implementation of TVMDSOOp* feat: Update cmake script to work with c++11 and in-repo build* feat: Use libtvm as oplib dependency* fix: Add missing link dependency to libtvm* feat: Update tf tvmdso op by review comments* fix: Update with pr comments* fix: Fix lint* feat: Add test script and fix gpu shape* feat: Add test script and fix gpu shape* fix: Conditional build tftvm op for gpu* fix: Conditional build tftvm op for gpu* fix: Fix pylint of tf_op module.py* fix: Fix pylint of tf_op module.py* feat: Conditional enable gpu test for tftvm op* feat: Conditional enable gpu test for tftvm op* feat: Add tf_tvmdsoop test script as an app test* fix: Fix gpu/cpu enabled check on tvm in test script* fix: Make tf tvmdso op test script runnable with pytest* remove unused test script test_tfop_module.py* fix: Remove pushd & popd in tfdsoop test script* fix: Upgrade tftvmop use python3 to find TensorFlow* fix: Upgrade tftvmop use python3 to find TensorFlow* fix: Change target_link_options to target_link_libraries* fix: Add tftvmop build script's c++ option* fix: Add tvm library path to tf op test library path* fix: Debug ci build for tftvm dso op* fix: Fix cmake error and skip tfop test* fix: Fix typo and indentation issues* feat: Use TF list input op def* fix: Fix style and unexpected changesCo-authored-by: baoxinqi <baoxinqi@4paradigm.com>Co-authored-by: Chen Dihao <chendihao@4paradigm.com>Co-authored-by: wrongtest <wrongtest@4paradigm.com>	3
[TOPHUB] Set vulkan as alias for opencl (#2230)	1
[microTVM][Tutorials] Add tutorials to run on ci_qemu (#10154)* add python files to script run* add python files to script run* fix issues with formating	0
Fix conv2d_transpose layout transform issue in trt (#9668)* fix* change default in teh transform instead	4
[Auto Scheduler] Mali Support (#7132)* [Auto Scheduler] Mali Support* Fix doc* fix lint* address comments* fix doc	2
[ONNX] Onnx node tests (#7720)* WIP* some fixes* more fixes* fix some conv_transpose tests* fix out of bounds slice* fix flatten import* fix logsoftmax and softmax tests* fix Error in Upsample* fix onehot* normalize errors* fix gather with negative indices* parameterize test* skip unsupported tests* clean up* fix rebase* fix lint* add an error message when we find an un-identified tensor	0
[Relay] Fix interpreter for dyanmic shape input of ndarray_size (#6086)	0
[BUG][TVMScript] fix block range error (#9574)	0
[CMSIS-NN] Fixed the network hash to avoid type inference failure (#9887)Co-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>	0
Add Quantize/Dequantize Partitioning (#5940)* Implement quant/dequant partitioningon our wayget clooooooserclean up (part 1)clean up (part 2)clean up (part 3)clean up (part 4)clean cleancleaanaannanaaananaananaananaanclkjsdflkjlfsjdflkjrevert parser changesadd docsroll lintroll lint* add option to toggle fully integral check* convert dtype collector to C++* remove need for `with_dtype`* remove unused imports* roll lint* partially address feedback* roll lint* upgrade to new parser* retrigger CI* roll the dice again	1
prevent starting of RPC server w/o RPC support (#962)* prevent starting of RPC server w/o RPC support* fix indent	0
[Relay][OP] Fix bias_add default axis (#2829)* Fix bias add default axis* update* Fix canonicalize ops for bias_add	1
Fix what looks like bizzare copy-paste issue (#6010)	0
[MAINTAINER] add masahi as reviewer (#1277)	1
[ARM,TOPI] Allow auto scheduler layout rewritting in dense (#10699)* [ARM,TOPI] Allow auto scheduler layout rewritting in denseAuto scheduler was already rewritting the layouts of inputs to dense,but the dense operators was not passed the correct flag to takeadvantage of these inputs. This could cause a crash when the rewritteninputs did not match the size expected by dense.* formatting	1
[Rust][CI] Restore Rust CI (#5137)	5
add test to irbuilder for gpu execution (#1228)	3
add support for subgraphs. (#1221)* add support for subgraphs.* fix.* fix.* Fix compilation error* Fix compilation error* add comments.* update comments.* Sanity check on subgraphs when creating IndexedGraph* avoid the overhead of sanity check.* Stop using non-recursive DFS* Trigger CI* trigger CI	1
[Hardware][Verilator] Integrating and simulating hardware accelerators in TVM (#6971)* add files* update interface* update* fix comment* fix fmt* remove widget repo* remove	4
[NNVM][TOPI] Add gradients for broadcast_* ops (#1234)	1
[TVMScript] Enhance printer (#8934)	5
[TOPI] Support grouped conv1d (#9832)* [TOPI] Support grouped conv1dGeneralize the conv2d compute statement to a generic convNd thatsupports any layout and groups. Replace some existing conv2d and conv1dcompute statements with this generic compute. Also add a topigroup_conv1d compute that uses the generic convNd compute. Existingschedules for conv1d work with group_conv1d, so they are reused.* permute reduction axis order* formatting	1
Add Community Page (#1063)	1
[OP] Introduces auxiliary attrs into compute (#1293)	5
Support aten::flip (#8398)* Support test aten::flip* Support aten::flip	1
Fix typo (#2467)	2
[RUNTIME] v2: runtime support for rocm (#386)* v2: runtime support for rocm* fixed coding space errors* removed kROCM from c_runtime_api.h	1
Fix a typo for Target class (#951)	1
Fix llvm-enabled build by adding missing intrinsics headers (#4575)	1
add another default location to verilator (#3324)	1
SSA Pass	4
[Frontend] ONNX frontend v0.2 support (#202)* update* fix* use generated onnx model* fix tests* fix lint* remove log filter* add vgg* fix tests* update tests* fix download* fix ci* fix tutorial url* clean cache	4
Skip tensorflow test `test_forward_ssd` (#10231)Since this test is launched in a thread, errors aren't propagated up to the main thread and thusly pytest doesn't detect / report them, so this test will always succeed. Given that this single test can take [upwards of 30 minutes](https://ci.tlcpack.ai/job/tvm/job/main/2499/testReport/cython.tests.python.frontend.tensorflow/test_forward/), this PR disables it until someone lands a proper fix (given that this test takes so long some consideration should be given to reducing its runtime before enabling it again).Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Hexagon][Runtime] Add QuRT thread pool backend (#11018)* Initial take on adding QuRT thread support to TVM's thread pool. WIP; crashes* Allocate QuRT thread stacks automatically* Remove duplicate stack in QuRTThread* Add more logging to QuRTThread* Use QuRT mutexes and condition variables* Get QuRT thread pools working perhaps* Sleep for a little bit to let race condition bugs shine through* ayeee it works!* Remove custom hexagon implementations of std::mutex and std::condition_variable* threading_backend.cc code cleanup* Formatting changes* remove hexagon debugging* Initial take on adding QuRT thread support to TVM's thread pool. WIP; crashes* Allocate QuRT thread stacks automatically* Remove duplicate stack in QuRTThread* Add more logging to QuRTThread* Use QuRT mutexes and condition variables* Get QuRT thread pools working perhaps* Sleep for a little bit to let race condition bugs shine through* ayeee it works!* Remove custom hexagon implementations of std::mutex and std::condition_variable* threading_backend.cc code cleanup* Formatting changes* remove hexagon debugging* Add hexagon thread pool test* style fixes for tests/python/contrib/test_hexagon/test_thread_pool.py* Fix some style issues* Address some reviewer comments	1
[2/6] Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op (#8795)* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D opThis commit adds mainly the relay passes and ethosu_conv2doperator to relay. The relay passes include the legalizationsand preprocessing of the relay graph in the paritioning.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* skipping the test if vela is not in the container.Change-Id: I68cc4259dc33e1473e460956978f364fbf6596d8* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* addressing Jared's commentsChange-Id: Ief669f788c6bd1a1be1004cbce5129ed06b63c3c* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* addressing Elen's commentsChange-Id: Iad6315bb63f12ba318deb9c5c9eff7459ff58c48* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* cleanup passesChange-Id: I8e1cbedd2c4d3d0cdff481d775d9eb0577e44456* Update TE commentsChange-Id: I7e65c2714d017c8a4b64986b111a6b51d128c963* Address ekalda's comments in TEChange-Id: I55cfbb3787c0aacdadf46c4859dff39287e65ddc* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*addressing chris's comments*addressing Nicola's commentsChange-Id: Id02788ddcdbc3679e0da37b2fa614cded0a4c1f5* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*addressing missed 'hidden' comments of Chris*addressing one missed comment of Nicola*adding type annotationsChange-Id: Iadf4907b311e195731dbbed571e95a266341db8f* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*further type fixes and one missed commentChange-Id: I6da69fd95d17dfeaf5940da4f8d8c8ea142b39d2* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*missed comment split_oChange-Id: I4a4b19ff2cd18e8f568a63ae827f44358ed85b8e* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* adding mypy checkChange-Id: Iaf58dbba2a9d8e1098a10c589d91b63c7efe646d* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*removing premature insertion of get_accel_type utilityChange-Id: I210512e00a5eb46adf23d1d72eb16432db526d25* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* rebase fixesChange-Id: I06c9b536a7598646efce2b664fcc405aa6008203Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>	4
[TensorIR] introduce Block and BlockRealize (#312) (#7553)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[Caffe Frontend] extending Eltwise to handle multiple inputs (#8136)* [Caffe Frontend] adding Reduction op* reformatting Reduction op test script* reformatting Reduction test script* [Caffe frontend] Reduction op- adding more test cases; handling '0 < axis < num_axes - 1' case to give the result equivalent to Caffe framework- skipping Relay multiplication if coeff is 1Signed-off-by: zotanika <zotanika@gmail.com>* linting test script* linting* [Caffe Frontend] Supporting multiple grouped(channel-wise) Deconv op* Handling group > 1 cases, assuming group == output channels* Decomposed into Relay split, transposed conv, and multi-leveled concatenation.* Added some test cases.Signed-off-by: zotanika <zotanika@gmail.com>* [Caffe Frontend] supporting variable number of inputs for Eltwise* extra handling of rest inputs for PROD, SUM, MAX operations* extra testcasesSigned-off-by: zotanika <zotanika@gmail.com>* formatting fix* [Caffe Frontend] reverting codes related Reduction for splitting PR* Revert "[Caffe Frontend] Supporting multiple grouped(channel-wise) Deconv op"This reverts commit 43e25e552b790ce9a38fdbcfb3ddf2075c253e20.* instant fix against docker format error* instant fix against docker format error* instant fix against docker format error	0
[TENSORFLOW]Sparse2Dense support (#5767)* [TENSORFLOW]Sparse2Dense support* Formatting issues fixed	0
[AutoScheduler] Re-organize logs files for tutorials (#6768)* reorganize logs files* fix lint	0
[Topi] Allow batch_matmul to broadcast along batch dimension. (#6616)* Allow batch_matmul to broadcast along batch dimension.* Added typerel checking.* Fix style issue and respond to feedback.* Fix style.* More formatting issues :(* Fix issues after merge.* Comment update.* Small tweak.	5
Add methods to get and set late-bound constants. (#12664)* Add methods to read and restore late-bound constants on Executable.* Add bindings for new functions* Cleanup* Fix function name* Add tests for python API to access new load/save functions* Add another tests for python API to access new load/save functions where there are no constants	1
Add IdentityN operator for TF Frontend (#7452)* Add frontend code and tests* Add Frontend CodeCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>	1
[ARITH] Introduce iterator (quasi)affine map detection. (#6667)* [ARITH] Introduce iterator (quasi)affine map detection.The loop transformations (split, fuse) create bijectivemaps from a collection of source iterators to target iterators.DetectIterMap is a function that detects such bijective mappingsfrom the lowered index expression.We choose the term quasi affine to be consistent with theterminology used by in polyhedral compilation.DetectIterMap can handle symbolic integers(in split/fuse) to some extent.The utility can be useful in detecting loop transformationpatterns and data layout change patterns in TIR.* Update per feedback	5
Replace RuntimeError in _lookup_task with deferred error. (#8421)* Replace RuntimeError in _lookup_task with deferred error.This allows unknown tasks to be created (e.g., when parsingautotvm log files) but not invoked.* Format.* Update python/tvm/autotvm/task/task.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Matt Welsh <mdw@mdw.la>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
[PASS] SimplifyBatchNorm->SimplifyInference, remove dropout (#24)	4
[LLVM] VectorType::get with two parameters is deprecated in LLVM 11+ (#5984)In LLVM 11+ the distinction between fixed and scalable vector typeshas become more explicit. Before the introduction of scalable vectortypes VectorType::get(e,n) created what is now a fixed vector type.With the addition of scalable types, it is recommended to useFixedVectorType and ScalableVectorType classes directly. Alternatively,there is a VectorType::get that accepts a 3rd parameter indicatingwhether the type should be fixed or scalable.Using the older VectorType::get that implicitly assumes the fixed typeis deprecated and LLVM now generates a warning.Change calls to VectorType::get to FixedVectorType::get to avoidcompilation warnings.	2
[Bugfix] Preserve IRModule type definition and imports in NameMangleExtFuncs (#8523)* bug fix and add tensorarray with partition pass test case* change test function location and address comments* Update tests/python/relay/test_pass_partition_graph.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* trigger CICo-authored-by: Cody Yu <comaniac0422@gmail.com>	3
[BYOC][TensorRT] Add nn.batch_matmul, nn.layer_norm, erf (#8005)	1
Fix typo in packed_func.h (#4219)	2
µTVM RPC server and Part 1 of AutoTVM compilation infrastructure (#6334)	5
[UnitTests] Removed unnecessary file creation from unit tests. (#7998)Some of the unit tests produced output when run, even for a successfultest.  Edited these tests to either write to a temporary directory, orto suppress the file creation entirely.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[DOCS] Add docs for Pass Instrument (#8220)* Fix AttributeError when TEST_DATA_ROOT_PATH is setInitiate a Path object from TEST_DATA_ROOT_PATH to fix the error:AttributeError: 'str' object has no attribute 'mkdir'* [DOCS] Add docs for Pass Instrument - Add a tutorial about how to use pass instrument. - Add related sections in Pass Infrastructure documents.* Fix ir.rst, the length of separator.* Fix unused local name* Fix linting errors* Fix linting errors* Fix linting errors* Address code-review feedbacks* Fix linting* Fix the order of tutorial.* Add exception handling. Address feedbacks.* Fix CI error -- clearing instruments in global pass_ctx* Clarify section hierachy.* Emphasize to use decorator instead of subclassing* Add a sentence to explain Pass Instrument. Fix typo.* Shrink python docs a little.* Fix tag name.* Address feedbacks.	5
Bump version to 0.9.dev0 (#9581)	5
[CMSIS-NN] Support for asymmetric padding in Convolutions (#9886)	1
[MetaSchedule] Fix Summary Format for Invalid Runs (#11584)Previously for invalid tasks, MetaSchedule prints a huge number inlatency which is aesthetically unacceptable. For example,``` 69 |  fused_cast_add_cast_3 | 16777216 | 2 | 0.0000 | 10000000000000000019156750857346687362159551272651920111528035145993793242039887559612361451081803235328.0000 | 20000000000000000038313501714693374724319102545303840223056070291987586484079775119224722902163606470656.0000 |     64 |```This PR fixes this behavior and turns the huge number into "N/A".	0
[docs][bug] Add redirects for moved pages (#9394)The documentation refactor moved many pages that have outstandinglinks from search engines and other sources. Because we don'thave direct access to the .htaccess file for the TVM docs webserver,this patch automatically creates manual http redirects froma list of tuples that provide the old page with the relativelink to the new page.	1
[Bugfix] Fix other div zero errors also in rewrite_simplify (#8983)* fix div zero error in rewrite_simplify* update the style to fix ci error* remove useless code and comment* fix div zero error of mod, floordiv, floormod in rewrite_simplify* rewrite the test case of divison by zero to fix ci error* remove useless tab* retrigger ci* remove useless blank to retrigger ci	1
[Frontend][TFLite] PreLU alpha can be an expr (#11879)* [Frontend][TFLite] PreLU alpha can be an expr* [Frontend][TFLite] handle both cases of PreLU alpha param	2
Revert "[Relay][QNN] Add unit test for int8 (#4159)" (#4192)This reverts commit 6f9d028b80f9e41fd577b5c6a7229cafcfc72173.	4
Run verifier during LLVM code generation (#2211)	1
[RUNTIME] Support setting CPU affinity (#1403)	5
[Relay][TensorFlow] Remove 'input_0d_mismatch' special handling (#3087)* [Relay][TensorFlow] Remove 'input_0d_mismatch' special handling* Add more tests.* Cover the case that strided_slice outputs a scalar	3
[docs][ci] Add CI reproducability docs (#10912)This adds info about `ci.py` to the docs and also re-arranges things a bit to consolidate/de-duplicate informationCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix conda packages (#642)* Make the tvm conda package build with in-place source and use cmake from conda.* Add a package for topi.	1
[NNVM][TOP] broadcast versions corresponding to topi: mod, max, min, pow, left_shift, right_shift greater, less, equal, not_equal, greater_equal and less_equal. (#1383)	5
[Metaschedule, Refactor] Move MultiLevelTilingNode decl to a header (#11020)* [Metaschedule, Refactor] Move MultiLevelTilingNode decl to a header* cpplint* Update src/meta_schedule/schedule_rule/multi_level_tiling.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Update src/meta_schedule/schedule_rule/multi_level_tiling.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* cpplintCo-authored-by: Junru Shao <junrushao1994@gmail.com>	5
Reorder dynamic to static and simplify inference, lower DynamicToStatic Opt Level (#7213)* reorder dynamic to static and simplify inference, add a dropout unit test* lower dynamic to static opt level* autoformat test* raise DynamicToStatic to opt level 2 to match Constant Folding	3
[TOPI] Fix cpp library dependency on MAC (#852)	0
[FIX][CI] hotfix check_grad perf regression (#8581)* hotfix check_grad perf regression: lift compile out of hot loop* hoist interpreter creation out of python closure, fix weird conv2d bug on arm cpu* lint* try one more fix	0
add conv2d transpose and fix bugs (#1566)	0
[Relay] Add printer for op strategy objects (#9923)	1
add einsum in pytorch frontend (#9651)* add einsum in pytorch frontend* add einsum in pytorch frontend	1
Place device now compatible and tested (#33)	3
[Frontend][Pytorch] Add axis N when maxpool3d layout is (C,D,H,W) (#12467)* Add axis N if input is (C,D,H,W) layout.* Add (C,D,H,W) test case.	3
[codegen] heterogeneous build for c++ (#3144)* heterogeneous build for c++* merge relay buildmodule to codegen build* use module split* use target_host* remove sse3* retrigger ci	4
[CI] fix Python dependency required by cpp tests to work standalone (#6639)	1
Add Handling of Zero Len Arguments (#6923)* Update tensorrt.py* Update tensorrt.py* Update tensorrt.py	5
[PYTORCH]Repeat, Reciprocal & Reshape Op support (#5280)	1
[Onnx] Turn off flaky nllloss test for now (#8919)* turn off flaky test* jostle ci* jostle ci	3
[REFACTOR][IR] tvm::Expr -> PrimExpr(Primitive Expr) (#4669)* [REFACTOR][IR] tvm::Expr -> PrimExpr(Primitive Expr)As part of unified IR, we will need to unify relay::Exprand the current tvm::Expr under the same base type.From the techinical point of view. tvm::Expr is a "primitive"expression that only contains POD types and handles and doesnot do life-cycle management.This PR renames Expr->PrimExpr to clarify that.We will send a subsequent PR to introduce the base expr class.* Remove legacy VarExpr and ExprHash/Equal	4
[Relay] Add Defunctionalization Pass  (#6400)* type args not automatically inferred...* working on type arg infer* fix type arg infer* WIP* wip* wip* revert type_infer* working* fix up test* fix* remove DeGlobal* lint* fix std move* comments* fix comments* review* style	0
[TFLite] Model importer to be compatible with tflite 2.1.0 (#5497)	2
Support quantised RSQRT operator in TFLite (#9165)The commit tests _convert_unary_elemwise function for the quantised andnon quantized tensor for the RSQRT op.Other operators will be tested in future (separated )commits.	3
Handle Select in IntSetEvaluator (#2687)	1
[DEP] Remove HalideIR from submodule (#3535)	4
[Arith] Merge surjective/non-surjective iter mapping detections (#11287)* simplify (x * 96) % 64 to (x * 32) % 64* adapt merge mulmod opt for OffsetOf computation* merge DetectIterMap and DetectIterMapPadded* adjust related interfaces for IterMapLevel* - check incompatible left paddings- determine case like x % 16, x in [0, 5) to be non-surjective, since usages may treat the region extent as 16 by mistake.- skip second round of rewrite when there is no padding- fix some typo in comments* rebase upstream	2
[OpenCL] Change winograd priority and extend split (#11908)	4
[Runtime] Allow parameter sharing between modules (#3489)As GraphRuntime does not provide control-flow logics, we have to splitour model to two parts. While we need to share parameters between themto save memory usage.Solution:1) add "lazy_init_input" in graph's attributes   "attrs": {     ... ...     "lazy_init_input": [       "list_str",       [         "p0"       ]     ]    }2) allow un-allocated NDArray entry in SetupStorage3) utilize "set_input_zero_copy" function to set parameters	2
[INTRIN] Add support for floor and ceil (#1267)	1
[TOPI/TEST] Add Testcase folder for TOPI (#225)	3
[TOPI] [Hexagon] Reshape slice op (#11983)* Reshape slice op. This patch adds the initial python implementation reshape slice op for hexagon.* Add tests for reshape op	3
"Resolved deprecation issue in test_op_qnn_conv2_transpose.py" (#10228)	3
Fix demo dockerfile build failed (#4744)	0
[TEST] Remove script that references previously removed content. (#2481)	4
[Bugfix] fix android rpc app undefined reference problem (#8530)	0
[Bugfix][Vulkan] Call VulkanDeviceAPI destructor on program exit (#7997)Most of the TVM Global() functions allocate with "new" and donot deallocate, as the OS can clean up any leftover buffers atthe end.  In this case, we need the VulkanDeviceAPI destructorto call vkDestroyInstance, to prevent a segfault on exit whenusing some nvidia drivers.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Torch] More graph rewrites for Faster RCNN / MaskRCNN (#7346)* add post nms topk to max_out_size rewrite* add argsort conversion* scatter pattern first cut* matching seems to working* dup matching fixed* add converter* conversion seems working* add reshape, use take* remove pytorch argsort converter* update test* add doc	2
Don't use alternative linker for static libraries (#10870)Otherwise we may end up with things like this (make VERBOSE=1):/usr/bin/ar qc libtvm_runtime.a  -fuse-ld=lld CMakeFiles/tvm_runtime_ob.../usr/bin/ar: invalid option -- 'e'Usage: /usr/bin/ar [emulation options] [-]{dmpqrstx}[abcDfilMNoPsSTuvV]...       /usr/bin/ar -M [<mri-script]...	2
fix llvm dependency bug (#2198)	0
[Relay] Fix index order in conv2d computation for Arm CPU. (#8361)When dilation is larger than value 1 in conv2d with NHWClayout, the ordering of indexes when accessing data arrayin computation of convolution appears to be incorrect.'data_vec' is defined aslambda n, oho, owo, kh, kw, ic, ohi, owi:But accessed asdata_vec[n, oho, owo, kh, kw, ohi, owi, ic]This patch fixes the order of indexes and modifies the testso that it is suitable for running on an AArch64 CPU.	1
Add demo_android Dockerfile (#1646)	2
[Bugfix] Simplify reduce expression in te.gradient (#6611)	0
[COMMUNITY] lhutton1 -> Reviewer (#6461)	3
[BACKEND] initial llvm codegen for amdgpu (#402)* added initial llvm codegen for amdgpu* fixed whitespace* fixed hsaco gen from ir* fixed targetmachine for rocm and added GetSource for rocm* fixed whitespace issues* changed statement to use less than 100 lines* added intrinsics for workgroup - rocm* whitespace - newline error fix* fixed error msg for workitem-workgroup intrinsics* added llvm ir dump for rocm codegen* [ROCM] changed codegen to emit proper amdgpu kernel header* fixed whitespace error* fixed whitespace error- 2* fixed AddFunction to not to use extra arg1. Changed AddFunctionInternal to not to take extra arg for target type2. Use Target from CodeGenLLVM to check for AMDGPU target* fixed whitespaces* fixed whitespaces 2* fixed codegen for AMDGPU - now generating valid IR* fixed codegen depending on code review* reviewed alignment for amd devices* added code to dump code object to file* fixed cpplint errors* print out IR after pass manager* added code to dump asm, obj to file and std string* fixed whitespaces* Update codegen_amdgpu.cc* used registry for amdgpu llvm* Fixed whitespaces* added code for calling linker* fixed formatting errors* added rocm link python interface* fixed pylint issues and added more body to the function* added doc string* added doc string for module* fixed python code after review, fixed llvm object codegen* fixed linker to generate code object* removed dumping to output file and debugging log out* fixed lint for python code* added fault check after running linker* removed print statement in rocm.py* changed rocm lld linker to raise runtimeerror than emitting error log to stderr* changed the way linker command line is pass to subprocess.popen* removed redundant code and reuse tvm utils* removed commented out code* removed cloning of unused modules, and put IR into string	1
[TOPI] Slice operator (#1165)	1
[COMMUNITY] kazum -> committer (#2831)	3
[RANDOM] Init contrib.random Library (#684)* [RANDOM] Init contrib.random library* [RANDOM] Add uniform* [RANDOM] Fix lint* [RANDOM] Add comments and tests* [RANDOM] Fix lint	0
[ETHOSN] Add support for Ethos-N 21.02 driver stack release. (#7628)- Updated default Ethos-N driver stack to 21.02  - Fixed some test failures associated with this change	4
Rev ci-qemu to v0.08 (#8776)* Remove synr from pip-installed package list * synr is installed by task_ci_setup* rev ci-qemu to 0.08	1
[RUST] Fix typo (#2681)	2
[Hexagon] Expose gtest output through runtime exception (#12502)Expose Hexagon gtest output in CI by raising it as a runtime exception rather than printing it to stdout.	1
Undefuned names: import os for line 324 & import re for line 308 (#6003)	2
Improve NHWC depthwise convolution for AArch64 (#6095)* Improve NHWC depthwise convolution for aarch64We created a default schedule (no auto-tuning or tensorization) nameddepthwise_conv2d_nhwc which does a decent job at optimizing depthwisefor NHWC layouts (on aarch64).Change-Id: I01e32903f6c1950623f33eae18484e70244fe0af* Add tuning knobs in depthwise scheduleChange-Id: I15080e7f12b16e6c6aba99a04e42023845eeabf1* Introduce padding policyChange-Id: If12a6d05dce9153861550ddef1ee5216809dd1e1* Vectorize paddingChange-Id: I7e2062a40358bf111c0366a449945eb077fb2e30* Legalize depthwise convolution (2x improvement) and fix tuning issueChange-Id: I4b82c58b167e40b0b7747d28293bbb488c505dd9* Adding assert on paddingChange-Id: Idf8eeaaface5eb7799109cd00f437e404778b9cd* Fix python lintingChange-Id: Iac16a8daea1268f0eb331fe4ec18a62408106cf9* Removing commented codeChange-Id: I1412f22ad9864273d77a7bf38a6768694339b7f0* Revert test file to make CI passChange-Id: Ica3eff8f9f0fd4c6f32f7ae80adc922f8b16cec9* Enabling only arm_cpu testsChange-Id: Icbaafcb39e892a5d1a4685133c1699e4d1a8e07e* RebasingChange-Id: Ibb23f1d4e0d0107e4e3b3571437161cdc2ee2909	4
[AutoTVM] Temporary fix to the stack overflow issue in autotvm task extraction (#5019)* Temporary fix to the stack overflow issue in autotvm task extraction* fix lint* fix graph tuner test	3
Complete register op from python (#8079)* Complete register op from python* fix lint* fix lint* fix lint* fix comments* fix* fix* fix comments* fix lint* fix lint* add comments* fix build* fix* add exception case* fix* fix comments* fix* fix* fix* fix* fix* fix* fixCo-authored-by: xiaoqiang.dan <xiaoqiang.dan@streamcoputing.com>	0
[PatternLang]Conditionally Embedding Constants in Partitioned Functions (#5693)* Embed constants in the partition function if the pattern explicity requests constantsfix rstfix pylint* improve comments based on Cody's feedback	5
[Relay][Layout] Add FInferCorrectLayout for L2 norm layout transform. (#12497)* [Relay][Layout] FInferCorrectLayout for L2 norm layout change.* [Relay][Layout] Test for L2 norm layout transform.* [Relay][Layout] Re-edit test to add multi-dimensional axis list.* Fix cpplint errors* Use clang-format-10 rules.* replace uint with size_t.	1
[Fix] Fix some errors in unittests (#12245)- test_aot_legalize_packed_call.py: `T.preflattened_buffer` returns `void`- test_tir_intrin.py: `type` here should be `buffer_type`- test_tir_transform_flatten_buffer.py: `extents` should be `list`- test_tir_transform_hoist_expression.py: change `tir` into `T` and register `Let` expression in `script/tir/intrin.py`- test_tir_transform_storage_flatten.py: `T.allocate` has no argument named `strides`	3
[CUBLAS] Add support for nn.dense and nn.batch_matmul (#10826)* [CUBLAS] Add support for nn.dense and nn.batch_matmulThis commit includes a fix for cublas.batch_matmulwhen mixed precision is being used.* Specify args in dense	1
[Runtime][Relay][Cleanup] Clean up for memory pass to enable heterogenous execution support. (#5324)* Cleanup type pack and unpack for tuples.* Clean up the memory_pass using common helpers* Clean up memory.cc* Refactor pass* Add doc strings* Fix CPPlint* Fix PyLint* Fix* Apply suggestions from code reviewCo-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Fix typoCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>	1
Grammar fix (#7622)	0
[LLVM CodeGen] Solve LLVM CodeGen br instruction accept not-i1 type issue (#2381)	0
save (#3901)	5
Fix recast of relay ops without attributes (#8043)* Fix recast of ops without attributes* fix test for pylint pass	4
[CUTLASS] Initial support for dynamic shape dense (#9419)* dynamic dense branch importcommit ee790ad64dfaec9d28f27726074a53a1daf3cf3eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 21:32:32 2021 +0900    dynamic dense working on ref build and execcommit 2449b667727e247c5b04e5fce12fa9d284444c38Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 21:27:46 2021 +0900    add vm build APIcommit 57036806b0533e5b039cc17f8da81983fba3ef9cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:57:33 2021 +0900    fix testcommit 4a7a5033b1f8c34b7f08b0a2882d6fa8bb5f44abAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:55:33 2021 +0900    fixed profile_all = False casecommit fb915195c122e9036ea3283e7fc286fa19222709Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:34:15 2021 +0900    add export_lib compile kwargscommit 3ad61e5379b486a0af010143ae8c7065b1c649d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:41:45 2021 +0900    fixed hardcoded cutlass include pathcommit 1def1d34ad042f9859307fdbd1f732186115d556Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:36:19 2021 +0900    add vm build and execcommit 570f9dd2cdc9b366348043ecc9c52a593ae4e273Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:15:01 2021 +0900    adding test* start new jit impl for updated signature* generate signature that takes DLTensor as input* choose default kernel, hit shape func problem* dynamic dense ran but the result is a bit off* the result agrees with cublas* support dynamic N* run black* clang format* add doc for build_cutlass_kernels_vm* add doc to gen_gemm.py* add sm75 dense kernels* black* replace print with logging, add sm check for DEFAULT_KERNELS	1
[TE/TIR] Fix create_prim_func to properly handle rank 0 tensors. (#8128)We handle lowering rank 0 tensors to rank 1 buffers with a singleelement.	0
[RUNTIME] NDArray CopyFrom/To Bytes always synchronize (#6586)* [RUNTIME] NDArray CopyFrom/To Bytes always synchronizeThe previous behavior of non-sync can be unsafe for GPU devices.In particular, the need for an explicit synchronization couldleads to confusion behavior e.g. asnumpy does not immediately returnthe right content for vulkan.Also brings the requirement of array being contiguous.Right now we encourage compact array since they are easier for optimization.We can consider bring support later by introducing a compactify PackedFunc(which might need be jitted).	1
[Doc] Fix broken link (#4438)* [Doc] Fix broken link* [Doc] Fix broken link* [Doc] Fix broken link	2
[MetaSchedule] Add Per-Store-Feature (#9860)* [MetaSchedule] Add Per-Store-FeatureCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* fix lint* fix lint* Update per_store_feature.py* address comments* fix lintCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
Update code_review.rst (#5923)editorial pass with corrections	4
[REALY][OP] fix typo (#5315)Signed-off-by: windclarion <windclarion@gmail.com>	2
add the --net=host cmd line arg to the docker/bash.it script (#7780)	2
Initial NHWC layout support (#376)* initial NHWC layout support* remove layout param from softmax* more nhwc support* fix typo* add nhwc layout test* fix lint* update tvm* update for c++ topi* fix lint* update tvm	5
[Relay] Add logical operators (#2743)	1
Revert ci-cpu due to nnpack issue (#4124)	0
[ONNX] enable more `*_expanded` tests  (#9051)* enable more sce tests* more testsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[TOPI][Relay][OP] support dynamic NMS(Non Maximum Suppression), symbolic begin, end, and strides for strided_slice (#4312)* [TOPI][Relay][OP] Dynamic NMS and strided_slice* Incorporate comments* fix nnvm compatibility issues* fix InferCorrectLayout* Minor fix* fix for fuse* Workaround to pass batch_size into hybrid function to handle dynamic shape* Seperate rearrange* fix lint* fix ci, comments* change attr to Optional<T>* clang format* remove empty lines* partial ignore for end of strided_slice* pylint* add out_indices for gpu get_valid_counts* change to slice_mode* clang-format, fix comments* fix comment* change slice_mode to string* fix CI* update docstringCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>	2
REGION op removed from topi and added in darkent frontend (#2275)	1
add min_repeat_ms to other CUDA tutorials (#2526)	1
[BUGFIX]bugfix in tensorflow space_to_batch_nd (#5175)* [BUGFIX]bugfix in tensorflow space_to_batch_nd* Test case added	1
[PASS] Improve GraphFuse to include five patterns (#26)	1
Fix compiler warnings (#939)	2
[RUNTIME] Support module based interface runtime (#5753)	1
[TOPI] Example for depthwise convolution (#197)* first commit* move to topi/recipe* refactor, almost rewrite* 2-D sum reduction; implement SAME pad; improve schedule* add util.py; separate test script* conv + bn + relu fusion* auto fusion* separate declare and schedule; using op tag* divide large image into blocks* move to topi; improve blocking schedule* restructure* add doc* using time_evaluator	1
Add TVMC Frontend for PaddlePaddle (#9083)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black format* Add Paddle Frontend for TVMC* refine code format* add test case for tvmc* fix pylint check* gen_requirements add paddlepaddle* Trigger CICo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
[ci][docker] Regenerate Jenkinsfile on each run (#11886)This makes it so the generated PRs pass CICo-authored-by: driazati <driazati@users.noreply.github.com>	1
[REFACTOR][PY][API-CHANGE] Remove legacy python files. (#4943)* [REFACTOR][PY][API-CHANGE] Remove legacy python files.Remove legacy python files.Use the te namespace for most of the tensor expression primitives.- tvm.create_schedule -> tvm.te.create_schedule- tvm.placeholder -> tvm.te.placeholder- tvm.compute -> tvm.te.compute* Remove top-level exposures.	4
[TIR] Handle axis_separators during FlattenBuffer (#12652)* [TIR] Moved tir.FlattenBuffer to occur before tir.LowerOpaqueBlockFor buffers with more than one physical axis, the `axis_separators`are required in order to know which groups of logical axes to fuseinto each physical axis.  The implementation in `tir.FlattenBuffer`assumed that all buffers were being flattened to a single physicalaxis.  Because `tir.LowerOpaqueBlock` replaces the`BlockNode::alloc_buffers` with `Allocate` nodes, `tir.FlattenBuffer`no longer has access to the axis separators and performs inconsistentflattening for `Allocate` as opposed to `BufferLoad`/`BufferStore`.This was introduced in https://github.com/apache/tvm/pull/12172, whichdecoupled the lowering/flattening steps.The commit reorders the `tir.FlattenBuffer` to occur before`tir.LowerOpaqueBlock`, to make use of the axis separators.  Any`Allocate` nodes that exist at that point (e.g. from hand-writtenschedules) are still flattened to 1-d physical buffers, but the`BlockNode::alloc_buffers` are flattened according to the axisseparators.* Add unit test to validate non-flat memory after tvm.lower* Explicitly write T.reads for test on BufferRegion updates* Update incorrect docstring for test* Use DeclBuffer information in FlattenBufferThe DeclBuffer node can be inserted during LowerOpaqueBlock, thenprovide the missing Buffer information required to flatten theallocation.* Use T.allocate in unit testsWith the insertion of `DeclBuffer` nodes, `LowerOpaqueBlock` no longerneeds to be before `FlattenBuffer`, and has been moved back to itsoriginal position.  Revering the tests to use `T.allocate` instead of`T.alloc_buffer` more closely represents the functions as they arebeing lowered.* Fix usage of T.decl_buffer in updated tests* Update LowerOpaqueBuffer to expect the DeclBuffer nodes* Strip DeclBuffer annotation in FlattenBufferThe DeclBuffer annotations aren't yet supported in all passes.  Thisrestricts them to being introduced in LowerOpaqueBuffer, thenimmediately removed in FlattenBuffer.* Strip out all DeclBuffer nodes in FlattenBuffer* Update unit tests to remove expectation of DeclBuffer nodes	4
[Frontend][PaddlePaddle] Remove unused parameters and fix doc string (#9283)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes* Remove unused parameters and fix doc string for paddle frontend* remove blank line* fix code error* modify test_forward.py	3
[MetaSchedule] Logging Interface Unification (#11157)* Implement new logging interface.* Major interface usage update.* Functionality fix.* Switch logging conditions.* Tweak logging interface.* Minor fix.* Feature updates.* Logging usage.* Linting.* Fix linting.* Fix handler type.* Fix issues.* Nits.* Address issues.* Add DEBUG level fall back.* Minor fixes.* Allow parameterized configuration.* Linting.* Polish interface.	1
Add `extern "C"` to C Interface API header (#9094)This is to provide the hint to C++ compilers that these functions are C linkage.New header looks similar to:```c++extern "C" {/*! * \brief Input tensor pointers for TVM module "default" */struct tvmgen_default_inputs {  void* y;};/*! * \brief Output tensor pointers for TVM module "default" */struct tvmgen_default_outputs {  void* output;};/*! * \brief entrypoint function for TVM module "default" * \param inputs Input tensors for the module * \param outputs Output tensors for the module */int32_t tvmgen_default_run(  struct tvmgen_default_inputs* inputs,  struct tvmgen_default_outputs* outputs);}```	1
fix some pass docs (#3767)	2
Change as graph.input can be either ValueInfoProto or string (#186) (#192)* graph.input can be either ValueInfoProto or string* pylint	5
[BUGFIX] Thread related bound (#86)	0
[PASS] Assign unique names to variables in ConvertSSA pass (#18)* [PASS] Assign unique names to variables in ConvertSSA pass* revert change to ConverSSA pass	4
[PASS] Refactor thread storage sync to a common visitor (#296)* [PASS] Refactor thread storage sync to a common visitor* Fix the sync scope check behavior	0
[Topi] pass-by-value -> pass-by-const-reference (#5783)	4
[TIR] Allow `tir.Buffer` converted to `BufferLoad/BufferRegion` with `__getitem__` (#12422)	1
[TOPI][x86] Injective schedule improvement (#4786)* [TOPI][x86] Injective Schedule Improvement.* Add tiling.* Vectorize when there is an axis.	1
[Relay][Compilation] replace relay.build_module with C++ BuildModule (#3174)	5
[DOCS] Fix tvm.build API doc layoutThe newline in the pydoc breaks the layout of parameter inputs in API `tvm.build`	2
[DOCKER] Only pass pythonpath for ci images (#6005)	4
raise error when cannot get shape in reshape (#467)* raise error when cannot get shape in reshape* fix pylint	0
Added missing include file (#7808)	2
[TIR][TRANSFORM] Return value support in tir.tvm_call_packed (#7932)This PR fixes the return value support in tir.tvm_call_packed- Clarified the semantics of the intrinsics- Fix a problem when lowering call packed with nested scopes(let bindings)- Added regression tests to cover the changes	4
Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger	5
[TIR][Schedule] Allow named block and buffer arguments in Schedule (#11624)* [Schedule] Allowed string argument as block argThis has previously been implemented for `Schedule.transform_layout`in https://github.com/apache/tvm/pull/11296, extending to allow forblock arguments in all `Schedule` methods.This change was only made for arguments that must be a `BlockRV`.  Forarguments that may be either a `BlockRV` or anothertype (e.g. `Schedule.get_child_blocks` accepts either `BlockRV` or`LoopRV`), this sugar is not implemented, to avoid ambiguity.* [Schedule] Allowed string argument to Schedule.reindexSimilar to https://github.com/apache/tvm/pull/11269, which added thisfunctionality to `Schedule.transform_layout`.* CI test update	5
[MicroTVM] fix compile error when the compiler implements char as unsigned (#12519)When compiling tvm with micro on the compiler which implements char as unsigned(such as arm-linux-gcc), there is an error:`src/runtime/crt/graph_executor/load_json.c:218:12: error: result of comparison of constant -1 with expression of type 'char' is always false [-Werror,-Wtautological-constant-out-of-range-compare]``    if (ch == EOF || ch == '\r' || ch == '\n') {`The reason is because the implementation of char is undefined, so it's better to specify here that it is signed.	1
Trivial fix, up the rodata section for the discovery board to 512 bytes. (#6259)This is more reasonable as the trivial tflite example module needs 208 bytes.Signed-off-by: Tom Gall <tom.gall@linaro.org>	5
[ci] Reinstall junintparser after zephyr deps (#12226)Fixes #11749	0
[PatternLang] Remove unnecessary check (#6958)Thanks @mbrookhart	4
fix vec*mat in PyTorch converter (#11347)* fix vec*mat in PyTorch converter* Trigger CI	0
[ARITH] Constraint-aware ConstIntBound, Enhance CanonicalSimplify (#3132)	5
[CI] Update rust format version (#2550)	5
[CUDA][TVM] fix constructing invalid command line string for nvcc (#1674)	0
[CMAKE] Windows build instruction (#161)	1
Add using directives for otherwise hidden virtual functions, NFC (#12561)This silences warning```warning: 'foo' hides overloaded virtual functions [-Woverloaded-virtual]```typically caused by overriding only some overloads of `VisitExpr_` froma set defined in the base class.	1
[TOPI][IMAGE][RESIZE] Bilinear interpolation for resize and upsampling. (#1181)	5
[TEST] Fix testcase to make them more compatible to zero-rank (#3612)	1
[RUNTIME][REFACTOR] Use object protocol to support runtime::Module (#4289)Previously runtime::Module was supported using shared_ptr.This PR refactors the codebase to use the Object protocol.It will open doors to allow easier interpolation betweenObject containers and module in the future.	1
[microTVM][CMSIS] Add CMSIS libraries/sources to Zephyr CMake file (#11835)* Add CMSIS libraries to cmake build model with CMSIS	1
[CI] Update wasm emcc to latest (#6755)	3
update_document_after_repository_renamed (#4398)	5
[Relay][Text Format] Text Printer Refactor and Debug Printing (#2605)	0
[Relay][Frontend][Onnx] Allow A to B broadcasting of batch_matmul and reverse strided slice (#6681)* slice and batch_matmul fixes.* Bug fix in shape inference.* Test backwards strided slice.* Fix batch_matmul dynamic shape function.* formatting.* Fix edge case for implicit broadcast	0
[CI] Further open up Rust permissions (#10115)Tested this with `./tests/scripts/task_rust.sh` to ensure it builds.	3
link the math library by default (#4713)	2
[LANG/SCHEDULE] Reduction factor, predicate in reduction. (#77)	5
[LLVM] Support atomic for GPU backend (NVPTX, ROCm) (#7051)* support atomic add on llvm* make atomic builtin intrin* test bincount on nvptx* use builtin::atomic_add* add atomic llvm codegen test, only works on int8 input somehow* supports fp32 atomic* drop support for cpu atomic* add comment* add atomic gpu unit test* reenable other tests* add doc string* run black* fix build with llvm 8 and older* fix format* do not run float32 atomic test on ci* do not run scatter_add 1d with float inputs on CI* fix typo* add todo comment for cpu backend* fix build on ciCo-authored-by: masa <masa@pop-os.localdomain>	0
Update HalideIR and dmlc-core to the latest (#2809)	3
[RUNTIME] Improve memory usage for RPC (#1741)	1
Expose `Struct/Tuple`-related TVM Builtins (#12452)This PR exposes the following TIR operation in python:`tvm_tuple`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L554)`tvm_struct_get`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L200)`tvm_struct_set`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L2432)Co-Authored-By: yongwww <[yongcale@gmail.com](mailto:yongcale@gmail.com)>cc @junrushao1994	3
xcode.py: Decode bytes before output (#2833)	5
[ARITH] Subspace division (#7760)	5
[Op][Topi] 5 ops can accept unsigned integers as indices (#10098)* tests passed* reformat* add uint test for unravel_index	3
add additional field into node for autograd (#145)* add additional field into node for autograd* check inputs reachable* fix	0
[WIP][µTVM] Add OpenOCD Low-Level Device (RISC-V Support) (#3756)	1
[microNPU][4] Add the cascader Proposal generator (#9959)* [microNPU][4] Add the cascader Proposal generatorThe Proposal generator takes optimal Plans and combinesthem to find optimal 'Proposals' - sets of disjointPlans that cover every Part in a CascaderGraph. Itultimately produces a Pareto-frontier of 'optimal'Proposals in terms of estimated cycles and memory usage.Change-Id: Id42099819a596496a5769bae22f08eeb75ec69b6* FixesChange-Id: I4f5f2a298bd3bb379c7c8d179150358923b0dd66	4
TFLite: Add fused_activation_function for ADD, SUB, MUL, DIV (#3372)	1
Fix leakyReLU support for CoreML (#6651)The original implementation failed with the following error:File "../include/tvm/runtime/packed_func.h", line 372TVMError: Check failed: type_code_ == kDLFloat (8 vs. 2) : expected float but get Object	1
Fix broken rat install (#3527)http://www.trieuvan.com/apache//creadur/apache-rat-0.12/apache-rat-0.12-bin.tar.gzgives a 404 so point the installer at archive.apache.org hopefull thatis more reliable.	0
[NNVM]Keras SimpleRnn and GRU support (#1729)	1
[PYTHON] addon->contrib add docs (#107)	2
[ci][docker] Add Jinja2 to image (#11265)commit-id:207f3fb7Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[IR] Add body to AssertStmt (#220)* [IR] Add body to AssertStmt* fix lint	0
[Object] Throw AttributeError if the object doesn't have a reflection table (#9919)	0
[Bugfix][Relay][Frontend] Fix bug in mxnet converter for slick_like (#2744)* Fix bug in mxnet converter for slick_like* More tolerance for topi_conv2d_NCHWc	0
[RUNTIME][REFACTOR] Use new to avoid exit-time de-allocation order problem in DeviceAPI (#6292)	0
[MetaSchedule][Test] Add unittests for TBG (#12262)	3
Calculate CMSIS-NN buffer size with respect to architecture extensions (#9338)This correctly calculates the buffer sizes for a variety of targetsbased on the `-mcpu` and `-mattr` flags passed to the `cmsis-nn` codegenerator.Added for Conv2d, Depthwise Conv2d and Average Pool.	1
Update tune_simple_template.py (#4778)fixed a spelling mistake.	0
[SCHEDULE] Detect duplicate IterVar in reorder (#575)	5
Pytorch Conv Transpose Padding Fix (#7958)* fix conv transpose import from TF* fix String::fromwe() to String::from()* * fixing pytorch converter to take into account the output_padding parameter for conv transpose operations* updating pytorch converter to correctly convert conv1d to conv1d in tvm inestead of a flattened conv2d unless under circumstances of grouped convolution* updating pytorch converter to correctly convert conv1d transpose to conv1d transpose in tvm instead of a flattened conv2d transpose* added tests to cover these latest additions* * removing print statements used for debugging* * fixing typos and formatting* * fixing formatting* * fixing grammar* * formatting fixes* * updated formatting after running pylint and python_format checksCo-authored-by: Mikael Sevenier <mikael.sevenier@sima.ai>	1
[ARITH][IR] Introduce FloorDiv/Mod (#3479)* [ARITH][IR] Introduce FloorDiv/Mod* Address review comments* address review comments, fix div sub rule	0
Changes to make tensorize work. These changes also fix the previously broken test. (#3981)* Changes to make tensorize work. These changes also fix the previouslybroken test.Summary:Tensorize was breaking  for a few reasons.1)Assert at: src/op/tensorize.cc:234 CHECK(is_one(e.region[j]->extent))In some cases this cannot be proven, e.g.:expected shape=[16, 4], given region=[range(min=((ax1.outer*16)/16), ext=(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer)), range(min=((k.outer*4)/4), ext=(((((k.outer*4) + 3)/4) + 1) - k.outer)), range(min=0, ext=16), range(min=0, ext=4)]The unprovable one is: ext=(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer)).This can be simplified but it is not because to simplify divide, it mustprove ax1.outer > 0 and since it is var it cannot. The fix for this tojust find all the vars in expr in relace them with some const value.2) Equivalence between tensorized expr and one being asked to tensorize. For example,the error would be.TVMError: Check failed: Equal(lhs, rhs):Failed to match the compute with TensorIntrin tensor_intrin's declarationprovided= reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[(int16)0]), source=[(int16(data(k))*int16(kernel(((((((((k.outer.outer*64) + (k.outer.inner*2)) + k)/2)*128) + i) - (k.outer.inner*128)) - (k.outer.outer*4096)), ((((k.outer.outer*64) + (k.outer.inner*2)) + k) % 2))))], axis=[iter_var(k, range(min=0, ext=2))], where=(bool)1, value_index=0),intrin=  reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[(int16)0]), source=[(int16(data(k))*int16(kernel(i, k)))], axis=[iter_var(k, range(min=0, ext=2))], where=(bool)1, value_index=0)Difference is mainly in the source part:source=[(int16(data(k))*int16(kernel(((((((((k.outer.outer*64) + (k.outer.inner*2)) + k)/2)*128) + i) - (k.outer.inner*128)) - (k.outer.outer*4096)), ((((k.outer.outer*64) + (k.outer.inner*2)) + k) % 2))))]source=[(int16(data(k))*int16(kernel(i, k)))], axis=[iter_var(k, range(min=0, ext=2))]This was not being simpifiled due to compute_intrin_iter_space (map foriter var to range) not containing leaf iter vars.3) Here it fails with:Check failed: is_one(Simplify(value->shape[i])): Argument b_buffer shape mismatch[16, 4] vs [(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer), (((((k.outer*4) + 3)/4) + 1) - k.outer), 16, 4]This is in buffer binding where it thinks expected and buffer boundshape is different. Although if we could simplify expr, this would notbe the case.Test Plan:On skylake avx512 machine:python tests/python/contrib/test_gemm_acc16.pyReviewers:Subscribers:Tasks:Tags:* Implemented bounded analyzer which traverses tree and for reduce/forstatements binds the bound of the analyzer. Later this is used tosimplify expressions. Inspired from ir_mutator_with_analyzerSummary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Addressed comments.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Added ASF header + define macro for the header file: TVM_ARITHMETIC_IR_VISITOR_WITH_ANALYZER_H_Some lint fixes as well.* Relax the assumption that dom_map must always contain all leaf itervars.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Disable copy constructor and move to raw ptr.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:	3
[LLVM] Represent alignment information in LLVM IR (#5598)	5
[TEST] Hotfix CI outrage after TF in docker update (#2781)	5
Add back missing __init__.py to unbreak CI. (#9052)	4
[Relay][Pass] Clean up DCE tests in preparation for refactoring.  (#7029)* Clean up DCE tests* Format* Fix* Fix	0
[ETHOSN] Upgrade NPU driver stack to v22.05 (#11759)* [ETHOSN] Upgrade NPU driver stack to v22.05In updating the driver stack to v22.05 some additional things neededchanges:* Prevent split being offloaded to the NPU which is not supported in  v22.05.* Removes compile algorithm configuration option since this was removed  in v22.05. Versions before v22.05 will use the default option.* Managing some API changes.* Updating network compile hashes.* Updating expected error message for overall scale bounds check.Change-Id: I09343c398a1f47dec44e135ff8252a6315a9b63f* fix decorator evaluation orderChange-Id: Ib1a34093b4011bdc20fca47d474eb1786218de98* Return none if version doesn't existFor some reason PyTest evaluates the second skipif decorator evenif the first one marks the test to be skipped. Thus, meaning testcollection fails when we want to check the version. The workaroundis to return None when the version is not available.Change-Id: I7cdd8cc70a9ee3c193e9a900f1011829538d975b* Update resnet hash after rebaseChange-Id: I7555c4a4d7db4f6c7aa8d476e39277fc5cba2f0d	5
[TOP] concat, sigmoid	5
fix ci-arm build process (#8377)	0
Update TVM_LOG_DEBUG for IR tracing. (#9278)* Update TVM_LOG_DEBUG for IR tracing.Forgot to do this when I switched to VLOG, sorry.* Woops, remove src/ prefix.	0
Handling duplicate NodeEntries on the edge of the gradient graph (#122)* Handling duplicate NodeEntries on the edge of the graph* Fix docs and segfault* Suggestions from review* Added attr_parser check	1
[PROFILER] Theoretical roofline models (#11066)`tvm.analysis.roofline_analysis` adds estimated roofline performance to aprofiling report. The roofline model measures how close an operator getsto best possible memory bandwidth or FLOP/s depending on whether it ismemory or compute bound. This computation uses the runtime of theoperator along with two numbers extracted from the TIR code: bytes ofmemory touched and number of floating point operations. Because thesenumbers are extracted from TIR, they may not be 100% accurate. The bestpossible memory bandwidth and FLOP/s are measured by running smallprograms that are memory and compute bound respectively.For now, this function only works with llvm cpu targets, but it shouldbe possible to extend to GPU targets.	1
Update search for bitcode files for rocm 3.9 (#6865)rocm 3.9 moved the bitcodes, we adapt to that.As this gives opaque error messages that are hard to debug(loading the module fails with could not initialize shared objectbut does not tell you about the missing symbols), we tightenthe checks at this stage:- we become more strict with missing bitcodes,- we let the linker fail loudly for unresolved symbols.	0
add use_mt option, default to use md as mt won't work with current c++ api (#484)	1
[RUNTIME] Add device query for AMD GcnArch (#4341)* add gcnArch query* kGcnArch query for cuda is a no-op	1
Move Var back to Expr, add format str test	3
[DOC] Add install prerequisites (#358)Add install prerequisites of customized building	1
[Relay] Fix memory leak in the interpreter (#4155)* savelint* address reviewer comment	1
Remove leading "./" from include paths (#1640)	4
[Hexagon] Don't use cmake glob for auto-generated source files (#10259)* [Hexagon] Don't use cmake glob for auto-generated source filesGlob treats inputs as patterns: if the file with a given namedoes not exist (is to be generated later), it won't be added tothe output.* Restart CI	1
[ci] Mark some ehtosu tests as flaky (#10301)See #10300Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[NNVM][KERAS] Fix keras model converter and improve tutorial (#1716)	1
Fix repository URL in ubuntu_install_rocm.sh (#9425)* Fix repository URL in ubuntu_install_rocm.sh* ROCm dependency installation process was following outdated procedures.  This PR makes the installation script to point to the correct repository* Installation documentation is at:  https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html* Fixes #9413* Update docker/install/ubuntu_install_rocm.shCo-authored-by: Christopher Sidebottom <git@damouse.co.uk>Co-authored-by: Christopher Sidebottom <git@damouse.co.uk>	1
[PatternMatcher] Support matching tuples, call nodes, and functions with variable numbers of inputs (#7754)* Allow TuplePattern to have null fields and match any tuple* support matching functions and call nodes with variable numbers of parameters* remove development code that was commented out* add docs for fuzzy matching	2
[Relay] Fix a bug in tensor_array_scatter (#6890)* [Relay] Fix a bug in tensor_array_scatter  tensor_array_scatter constructs helper functions according to dtype  and shape of element. When there are multiple scatter operations with  same dtype and element shape but different indicies_shape, there will  be name conflict in prelude.* Refine get_name	1
add in-place methods used by Tacotron2 to pytorch frontend (#8692)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>	1
Fix issue with importing models using Tensorflow Lite 2.4.x schema (#8375)Tensorflow Lite has changed the opcode for BuiltinOperatorsto be represented as 32 bit integers instead of 8 bit integersin the schema.This is an attempt to fix this in a way that is clean to handlemultiple versions of tensorflow lite in the frontend.	0
[IR] Initial stab at std::string->String upgrade (#5438)	5
dynamic to static use infer_type_local (#9869)	5
[Minor] Typo Fixes (#10000)* Fix typos.* Missed funtion -> function.	1
[Relay][TF] Keep node name in span (#6885)	5
quanitze operation expanded to take const argument (#6127)* quanitze operation expanded to take const argument* amendmentsused get_tensor_expr, added _test_forward_quantize_dequantize_const test	3
[BYOC][ETHOSN] Introduce further operator support (#6355)* [BYOC][ETHOSN] Introduce further operator supportThis PR introduces support for the following operators: - Quantized Fully Connected - Quantized Addition - Depth-to-space - Max/Avg Pool 2D - Quantized Relu (Clip) - Reshape - Quantized SigmoidCo-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>* Skip tf imports if not availableChange-Id: I11bcf4a78014fa63e7b8e3b0cb00eecfd6cb7760* ethos -> ethosnChange-Id: I1fb1a11d0765f6d69f04c24b9c24e08665b8af6a* Reduce random testing in test_additionChange-Id: Id06063a0a0cf5f01356df23dc5d4bbbcb47cfa99* Reduce random testing in test fullyconnectedChange-Id: I330408dfabc4bd804373f100581ce909ff724052* Fix dumb mistake with renameChange-Id: I2c5007be485b323116a0e8bab0f9106ea5ec834b* Added comments to update the hashes in network tests when necessaryChange-Id: I13828c918c959daa492b9ed942a882c86d6690d1* Fix github nameChange-Id: Idaa70ab9c2ec8db2828d51d15e7c23f28670ec82* Use black formattingChange-Id: I538171bd547a16395bef155a1dad28e8b3e347f2Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>	4
[TensorIR][UX] Type annotation-based runtime type checking (#9559)	1
use python3.7 install script in ci-qemu (#10799)* use python3.7 install script in ci-qemu* update pyton venv to 3.7* setuptools is just python3...* don't use apt-add-repository (breaks with python3.7 as python3 on ubuntu 18.04	4
[TOPI] Update conv schedule on rasp (#497)	5
[typo] fucn => func (#2240)	2
[Relay][QNN] Relax simulated qnn tests to prevent flakiness. (#7684)* Relax simulated qnn tests to prevent flakiness.* Change name of helper to make pytest happy.	3
[Relay][Frontend][ONNX] Support auto_pad in Conv and ConvTranspose (#4563)	1
[CUDA] Initial support for dynamic shared memory (#8466)* send dyn shmem size to runtime* add dyn shared storage scope* associate buffer var and its storage scoe in split_host_device* tried NVPTX but failed with INVALID_PTX error* test stub* dynamic shmem reduce working* log2 issue fixed* nvptx working* refactor llvm shmem allocation* make linkage argument* support rocm too* send dyn shmem param to hip runtime* remove alloc map from split_host_device.cc* remove attr::storage_scope from split_host_device* lint fix* formatting* update calling convention doc* minor update to test* remove log* remove kDynShared, dyn.shared -> shared.dyn* support backward compat* update json/binary reader/writer* thread_axis_tags -> launch_param_tags* ThreadAxisConfig -> LaunchParamConfig* remove use_dynamic_shared_memory from FunctionInfo meta data* revert change in test_tir_ir_builder.py* make sure kUseDynamicSharedMemoryTag is the last tag* remove continue* update doc string following name change* more comment update following name changeCo-authored-by: masa <masa@pop-os.localdomain>Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>	4
Add support for missing uint types. (#272)	1
[TOPI] migrate global_avg_pool, fully_connected (#472)* migrate global_avg_pool, fully_connected* fix pylint* enable fusion of pooling schedule* rename fc->dense, enable fusion* improve dense schedule* unified global pool	1
[ci] Remove apt cache from the docker images (#11470)	2
[IR]  eager constant folding in operator overloading (#1789)	1
[microTVM][autoTVM] Follow up fixes to #9003 (#9018)* fix test and cleanup* fix tutorial doc* fix verbose for tutorial* fix tune check* address comments* address comments	1
[TVM] Rewrite simplification rule to eliminate unnecessary conditionals. (#4076)The current bounds checking infrastructure inserts checks like:```for (i, 0, bounds[n]) {  if (likely(i < bounds[n]) {     ...  }}```into the TVM IR which is currently not removed by simplification infrastructure.This is a little unclean, as these are trivially true since for a loop var `i`with a given min and extent, we are guaranteed that `i >= min` and `i < min +extent`. Thus, we can insert these checks into the IR and use them to eliminatetrivial bounds checks early on.	1
Update the docs stale links (#7169)	2
Fix typo (#9156)Co-authored-by: JoshuaZhong <joshuazhong@etekcity.com.cn>	2
Remove minimum seed constraint on XGB Tuner (#7992)* remove minimum seed* reset 3rdparty dep* add items to 'visited', parametrize min seed records* add comment* fix lint* add tests	3
[cpplint] Fix C-style cast linting errors from cpplint 1.5.5 (#8106)	0
fix debug mask param check typo (#9586)	2
License as BSD for now before we finish approval for apache (#118)	5
[Frontend][TFLite] Fix fully_connected converter when batch size is not 1 (#6038)* Fix fully_connected when batched* Remove unused variable	1
[TE] Fix bug in AutoInlineElemWise and implement AutoInlineBroadcast (#7602)* [TE] Fix bug in AutoInlineElemWise and implement AutoInlineBroadcast* [TE] Add AutoInlineBroadcast API to schedule_pass.h	4
Fix winograd infer tize (#7092)	5
[Tutorial]NLP Sequence to sequence model for translation (#1815)* [Tutorial]NLP Sequence to sequence model for translation* Review comments* Review comments updated	5
[AutoScheduler] Delete deprecated file auto_schedule.py (#7071)	2
[ci] Fix build android rpc failure (#12216)android_rpc build problem: https://github.com/apache/tvm/issues/12191The problem with the build appeared due to the fact that the `ANDROID_NDK_HOME` environment variable was removed in the current version of github actions. https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-Readme.mdBut this variable is used here:https://github.com/apache/tvm/blob/ee319d9d23c80091da9c4fb764b1e6d49d462714/.github/workflows/main.yml#L122-L127Now only `ANDROID_NDK_LATEST_HOME` is available for ndk.	3
[TFLite] Implemented PADV2 Operator for TFLite and added support for constant values in PAD. (#6167)	1
Add FlattenAtrousConv transformation (#10996)	1
[TARGET] Phase out WebGL (#5570)The graphics API is moving towards next generation.Vulkan/Metal on the native and WebGPU on the web.Due to the limited programming model, we cannot get the best compute performance in WebGL.Now that the mainline already have both WebGPU and vulkan support, this PR phases out WebGL.	1
[Relay] Improve more operator mxnet frontend importer (#2772)	2
[Relay][Pass] Meta-Schedule-Layout-Rewrite (#11845)	4
[Relay] QLinearMatMul allows 1D weight_scale, weight_zero_point inputs (#10047)* fix after cr* fix after cr 2* emptycommit* emptycommit 2nd try	1
[DOCS] Fix C++ example:graph_runtime.cc:151: Check failed: data->ndim == data_out->ndim (2 vs. 1) (#1987)	5
Bug fix for Android platforms (https://github.com/dmlc/tvm/pull/971) (#986)	0
[Relay][Vm] Some performance improvement to VM (#5901)* make alignment constant* tweak copyto and loadscalarint* some safety check* x* lint* fix	0
[Analysis] Readability/Deduplication in Analyzer CanProve/Simplify (#11130)	5
[COMMUNITY] Siju Samuel -> Committer (#5817)	3
Frontend before tensor expression	5
[DOCS] update titles to reflect tutorial content (nnvm vs. relay) (#2597)* update titles to reflect tutorial content (nnvm vs. relay)* move things around* fix typo	2
[FRONTEND][MXNET] support elemwise logic ops (#5361)	2
[ARITH] Bugfix: int bound analysis for mod (#3288)	0
[CUDA] Improve injective schedule to enable half2 (#8457)* [CUDA] Improve injective schedule to enable half2* lint* fix* trigger ci	0
Fix dependency problem of reducer condition (#712) (#721)* Make duplicated function name checker working* Fix dependency checking problem for reducer condition (#712); add test* Fix dependency checking problem for reducer condition (#712); add test* Specify R to be computed inlined	3
LRN only supports 4D tensors, remove it from alter_op_layout (#5520)	4
Add dense base scheduler (#887)* Add basic dense scheduler* Revert to put back cpp dense registration* Fix lint	0
[Relay] Port LSTM to Relay for testing (#2011)	3
Update parsed kernel sources check. (#8257)	5
Port build_module.py to C++ (#667)* Port build_module.py to C++* Fix lint errors* Fix more lint errors* Fix more lint errors* Fix more lint errors* Fix build error* Implemented style fixes* Fix lint errors* Added function to construct target from stringlower now returns array* Fix lint error* Implemented review changes - style & Target options -> std::vector* Fixed lint, argument alignment and added unit test* Changed test to target LLVM, fixed sign compare warnings* Reverted unit test to CUDA, changed Jenkinsfile to enable GPU for C++ tests* Slight change to Jenkinsfile* Changed build_module test from CUDA to LLVM* Added function var() to construct a Var instance.Changed implementation of LLVMEnabled()* Reverted Jenkinsfile	2
Add a default warp size 1 for vulkan and opencl (#8109)	1
[microNPU] Fix bug in channels extraction in the matcher (#11335)* [microNPU] Fix bug in channels extraction in the matcherIf the input tensor layout is in NHCWB16, we were passing W valueinstead of the channels to get_valid_block_configs.* Add test for conv2d	3
fix name bug in test_pass_inject_double_buffer (#678)Change the parameter 'C' name	2
[BYOC-DNNL] add support for more ops and fusion patterns[BYOC-DNNL] add support for more ops and fusion patterns	1
[AUTOTVM] typo (#2478)* [AUTOTVM] typo* trigger CI	2
Allow tvmc to compile models with AOT executor in MLF (#8331)* Allow tvmc to compile models with AOT executorThe tflite_compiled_model fixture was getting duplicated a few times soI've added a parameterized fixture tflite_tvmc_compiler which combinestmpdir_factory setup with compile_modelNested targets broke a basic string split, so in cases where we usenested targets I replaced the string split with shlex split* Clarify that graph JSON is required only for graph executorPlus other clean ups* Change parametrize fixture to use string instead of list	1
[Arith] Inequalities solver (#5618)	5
export builtin_fp16 on Windows (#4731)	5
[REFACTOR][PY] Establish tvm.runtime (#4818)* [REFACTOR][PY] Establish tvm.runtimeThis PR establishes the tvm.runtime namespace that contains the core runtime data structures.The top-level API are kept inact for now via re-exporting.We will followup later to cleanup some of the top-level APIs.* Fix ndarray name	0
[DOC] Release note (#340)	2
[DOCS] Various sphinx related fix. (#5168)* [DOCS] Various sphinx related fix.- Use :ref: for reference.- Use :py:class: to refer to API docs.- Update installation guide to also refer to the download page.- Only move html contents in doxygen.* Address review comments* Update wording	5
[MetaSchedule] Support grouping in the cost model (#10811)	1
[DOCS] Update has_dtype/has_shape to pattern lang doc (#5847)	2
[TVMScript] Printer VarTable (#12336)This PR:- Adds VarTable for the new TVMScript PrinterCompared to the prototype version, this:- Removes unnecessary public methods.  - GetObjectName  - GetUniqueName- Add Frame parameter for `Define` methods. VarTable will add callback to Frame to remove variable when Frame exits.- Changes DocFactory from `ExprDoc(ObjectPath)` to `ExprDoc()` to simplify var definition.Tracking issue: https://github.com/apache/tvm/issues/11912	0
Mark zephyr install world-writable in docker image to unblock #7995. (#8037)	2
[ONNX] Reshape op (#11047)* hitting bug while running the reshape unit test. currently trying to reproduce error in script* unit test passes* ran make format* removed print statements* edited commentary* moved the zero check outside of the ravel unravel and into the topi reshape defn* ran cpplint* changes from andrews comments* derp* black* ran black on test_forward.py* fixed test expected output* retriggering CI due to hexagon test failure	0
[Relay][VM] Add more passes to VMCompiler (#4058)* [Relay][VM] Add more passes to VMCompiler* Check build config* Add todo	2
Adding support for TFLite QnnSub operator. (#5230)	1
[ARITH] More caninical simplfy (#561)* [ARITH] More caninical simplfy* [DEBUG] Use HalideIR with trace logging	2
[TFLite] pack operation extedned with const args (#6984)pack operation now accepts constant arguments	5
[Relay][OP] Gather_nd exposed to relay (#2945)* gather_nd added* gather_nd test added* more test added* fix lint* fix build error* fix lint* comments addressed	1
[M3a][Meta Schedule] Add Sampling Primitive SampleCategorical. (#8817)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>	1
[COMMUNITY] Matthew Brookhart -> Reviewer (#5886)	3
[LINT] Add more allowed file type	2
[Relay][Frontend][Onnx] Fix GEMM converter when C is not a parameter. (#7509)* Fix onnx gemm with non parameter C.* Add gemm tests for C.* Fix formatting.	0
[PYTORCH]ReflectionPad2d op (#5624)	5
[Relay] Add fast_softmax (#7163)* [Relay] Add fast_softmax* fix* fix	0
Update expr.h (#3031)	5
[TOPI][GPU] Mergepath sort with odd-even block sort (#7611)* Mergepath sort with odd-even block sort* fix lint, add test* respond to review comments* speed up tests by reducing dtype skews* fix bad rebase* change threading to support vulkan* fix lint* only sort if the data is non-empty* fix lint again* fix for vk* move if to higher scope* fix typoCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	2
[COMMUNITY] Adam Straw -> Reviewer (#12480)	3
add rpow & fix 'Number' to 'Number_' (#76)	0
[RELAY] Fix the FoldConstant Regression for VTA (#6377)* [RELAY] Fix the FoldConstant Regression for VTA* [CI] Fix error guard that was missed in VTA.This PR fixes an error guard during the documentation build step.- Temporary disables VTA frontend tutorial due to  the regression of deploy_detection	2
[Hotfix] Black format (#10482)	0
[BugFix][Meta Schedule] Fix meta_schedule.testing.local_rpc (#9172)	3
workaround typing.Deque import error for Python 3.5 (#4254)	0
os.path --> osp to match the import (#4681)	2
[TOPI] operator support: logical_and, logical_or, logical_not (#3929)* [TOPI] operator support: logical_and, logical_or, logical_not* [TOPI] operator support: logical_and, logical_or, logical_not* [TOPI] fix test cases for operator support: logical_and, logical_or, logical_not* [TOPI] fix test cases for operator support: logical_not	2
[Runtime][ThreadPool] Handle the default value of affinity mode. (#10434)* [Runtime][ThreadPool] Handle the default value of affinity mode and acorner case of function 'SetMaxConcurrency'. 1. Handle the default value of affinity mode. 2. After calling the function 'SetMaxConcurrency' with a non-zero value,    if calling the function 'SetMaxConcurrency' again with a zero value ,    then the second setting can not correctly set the max_concurrency value    into zero.    use new logic to fix this issue.* address review comments.* polish the warning message.	2
Rename runtime-config to executor-config and add documentation for Model Library Format (#8270)* Rename runtime-config to executor-config.* Add documentation.* address comments, make tests pass* fix unit test* fix sphinx doc errors* address manupa comments	1
[VTA][OpenCL] add device_annot support in graphpack (#6125)* add device_annot support in graphpack* on_device annotation* lint* typo* fix lint* fix lint	0
Add LLVM-13 installation to Docker setup (#9498)* Installs LLVM 13 in for Ubuntu 18.04 Docker images * This is needed as a requirement for #9425	1
enable rocm target for topi/recipes. add timing util to gemm test. (#554)	3
Bumped Ubuntu version to 18.04 for ci_gpu (#7970)Change-Id: I8b13fda08ab002c16a082baaaedd973a063fab99	4
[NNVM] Make param file python version agnostic	2
avoid flaky testing errors in test_topi_sparse (#1706)	3
[RELAY] BiasAdd, MLP, Resnet testing (#1969)* [RELAY] BiasAdd, MLP, Resnet testing* fix review comments	0
[TFLite] Added ability to infer shapes for arguments (#7293)Added an ability to infer argument shapes if shapes are not present inTFLite files. The set of networks on which the patch was tested isinternal to Arm. Any help with creating unit tests would be appreciated.	3
[APP] fix gradle build for Android build (#685)	0
update block syntax (#9286)	5
[RELAY][BACKEND] CompileEngine refactor. (#2059)	4
[SimplifyExpr] Simplify consecutive adds with constants (#9671)	1
Fix cuDNN call for NHWC layout (#9600)	0
Add softplus operator conversion to Onnx. (#7089)	1
Fix bug with non-fp32 gemm in onnx frontend. (#8011)	0
[PASS] Improve vthread injection. (#411)	1
[TOPI] Upsampling op support (#772)* add upsampling cpu op* add upsampling gpu schedule* add doc for upsampling opadd more doc* cleanup upsampling test* add doc* fix lint* fix lint* fix lint* remove unused import* remove skimage dependency* remove skimage import* remove schedule_upsampling	4
[TEAM] Add Zhi Chen as a reviewer. (#2040)	1
Fix end to end benchmark with rpc devices (#9175)* Ensure that device used in end to end rpc is a local device* fix vm; add actually failing tests* bump roi_align test tolerances	3
[DOCS][RELAY] Sync up ops with code base (#2532)	2
Improve x86 Inception (#1506)* Improve x86 pooling and concat* Fix* Fix test concatenate correct layout* Add conditional vectorize* Fix lint* Modify schedule for global pooling* Fix* Fix warning* Fix alter layout test* Remove vectorization for pooling when using 4D layout* Remove vectorization for 4D concat* Fix concatenate layout* Fix concatenate schedule* Fix concat* Fix lint* Fix concat* Simplify pooling logic* Update docstring* Fix test topi pooling* Small changes	4
[Relay][ONNX][Fix] Flatten in OnnxConverter (#10593)* fix flatten* fix: python tuple to list	0
[CI] Pre-build Reference System Dependencies (#9270)Building these dependencies from scratch in each test was taking muchlonger than really necessary.Before:```$ time python3 -m pytest tests/python/contrib/test_ethosu/test_codegen.py::test_tflite_depthwise_conv2d[strides0-dilation0-SAME-kernel_shape0-relu-ifm_shape0-ethos-u55-256]real    0m19.982suser    0m13.255ssys     0m3.403s```After:```$ time python3 -m pytest tests/python/contrib/test_ethosu/test_codegen.py::test_tflite_depthwise_conv2d[strides0-dilation0-SAME-kernel_shape0-relu-ifm_shape0-ethos-u55-256]real    0m10.963suser    0m5.516ssys     0m2.232s```	5
Support additional architectures beyond x86_64 in ubuntu_install_java (#3546)* Support additional architectures beyond x86_64 in ubuntu_install_javaWhile attempting to get a development environment going for TVMon my AArch64 desktop I ran into some hardcoding of relevant architectures.	1
[TIR] Expose MMA-related PTX builtins (#12623)Expose MMA-related PTX builtinsThis PR exposes the following TIR operation in python:`ptx_mma`: tested`ptx_mma_sp`: tested`mma_store`: add new unittest`mma_fill`: add new unittestCo-authored-by: yongwww <yongcale@gmail.com>Co-authored-by: yongwww <yongcale@gmail.com>	3
[VTA] [CMake] hotfix tsim rules (#3650)	0
[TIR] Expose `shift_left` and `shift_right` to Python (#12584)This PR exposes the following TIR operation in python:- `shift_left`: tested [here](https://github.com/apache/tvm/blob/1afd0593956066635ee49297b731726c9218c91c/tests/python/unittest/test_tir_transform_simplify.py#L487)- `shift_right`: add new unittestCo-authored-by: yongwww <yongcale@gmail.com>	3
[TIR] Reduced duplication in op.h (#11129)* [TIR] Reduced duplication in op.hPreviously, `is_positive_int`, `is_negative_int`, `is_const_int`, and`as_const_int` had nearly duplicate type-checking logic.  This allowedhandling of Broadcast nodes to be diverge between theimplementations.  (e.g. `is_const_int(Broadcast(4,1), 4)` returnstrue, but `is_positive_int(Broadcast(4,1))` returns false.)This changes `as_const_int` to contain the type-checking logic,including the handling of Broadcast nodes, with the other threefunctions implemented in terms of `as_const_int`.* Test case, removing BroadcastNode handling from as_const_intRather than extending it to apply in more cases, seeing if it is safeto extract this functionality out to a separate function.	1
[Relay] Add compiler pass tutorial docs (#2746)* Add Relay compiler pass tutorial docs* Add Python API hook wrapping step* Incorporate feedback* More doc iteration* Mooooore iteration* Rewrite `runtime.md` in rst	1
[RUNTIME] Move device_api to include (#185)* [RUNTIME] Move device_api to include* fix doxygen* fix device api* fx	0
Use channels from attrs if possible (#7011)	1
[TVMC] Add vulkan to targets of tvmc run. (#8359)This allows to run compiled models via Vulkan.	1
Relay Op sprint (part 2) - Level 1 - log_softmax (#2128)	2
add scalars	1
[microTVM] Arduino retry on flash failure (#12114)* Retry on flash failure* Add unit test* Style improvements for Arduino api server	1
[uTVM][AOT] Adding workspace byte alignment (#8019)* Adding workspace byte alignment* This commit adds byte alignment support for workspaces* Updating AoT tests to use calculate workspacesChange-Id: I88380d875269e1ffa4a51a9cceefd51b3042f1a7* Adding workspace byte alignment* fixed aot_memory cpp tests* add new error type for stack allocator bad freesChange-Id: Iadb4770ac761ef5edb80308e18120443d269c83d* Adding workspace byte alignment* addressing comments + LIFO changeChange-Id: I1e8ad47e11e220f879bf936da2abb3d111db89f0* Adding workspace byte alignment* addressing comments furtherChange-Id: Idb07d28b55520d8897d7dbcb9ef4aad5e3e7b35c* Adding workspace byte alignment* addressing comments - add a default constant to alignmentChange-Id: Id3f486bfdc0bd57d54b3c4097885cb54675196ca	4
[AutoTVM] [TOPI] Support AutoTVM for int4 tensorcore (#7831)* initial* int4 asnumpy* remove* random test* format* random* remove unused import* change dist range* add fuse_pack in* random engine* reformat* remove import* add cuda context* refactor code	4
[TOPI][OP] Support Faster-RCNN Proposal OP on CPU (#4297)* Support Proposal operator on CPU.* PyLint space issue* PyLint space issue* Pylint singleton-comparison issue	0
Windows VS2015 Compile error - 'conversion from 'double' to 'float' requires a narrowing conversion' (#1329)	1
Generate Lower Bound Conditions for issue 1014 (#1091)	0
Fix Get Valid Counts when the number of boxes is zero (#7229)	1
[PYTORCH]LayerNorm support added (#5249)	1
[RPC] Terminate worker's childs first. (#3669)	1
[RUNTIME] Use weak link for fp16 functions  (#1769)	1
Fix Hexagon build using ci.py (#11304)* Add output directoryadd post build for hexagonfix -net=host for docker* remove --net by default	4
[REFACTOR] Polish runtime (#4729)- Remove operator bool from base object ref macro  - Raitionale: operator bool can be dangerous for sub-classes    that also overloads other operators(e.g. ==).  - If bool is still needed, use explicit operator bool.- Use absolute include when necessary- Move type related util to data_type- Isolate stackvm code from compiler	5
[5/10] Code generation for Depthwise Convolution via CMSIS-NN (#9409)This PR adds support for depthwise convolution via CMSIS-NN.	1
Add docs/dev/relay_add_op.rst to docs/dev/index.rst (#1790)	2
[CMAKE] Add option to enable custom logging (#10531)* [CMAKE] Add option to enable custom loggingThis option just passes -DTVM_LOG_CUSTOMIZE=1 to the compiler.* propagate compile defintions to tvm_allvisible* manually propagate compile definitions	5
[Hexagon] Reenable compilation of TVM runtime for Hexagon (#7784)- Add support for Hexagon SDK 4.x (different directory structure)- Conditionally disable functions not present on Hexagon (popen, etc.)- Bump sim_dev architecture target to v65 (older versions can still be  used with older compilers).Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>	1
[RPC] microtvm: fix RPC large transfer size issue (#7838)* fix rpc for microtvm* apply feedbacks* bundle deploy fix* fix func registry size* mv constant* fix copyfromremote* address comments and fix error* change rpc default max size* Trigger Build* add checks* Trigger Build* fix ICHECK	0
[Hexagon] Update launcher cmake flags for Android (#11213)Don't build graph executor for Android (it's not needed).	1
Enable custom images to be set in TVM Jenkinsfile (#8721)* This work is needed to enable automatic testing of our   newly built Docker images as part of CI * The default value is set by variables in the same   Jenkinsfile and are used when no custom values are   provided	1
[AutoScheduler] Accelerate feature extraction for winograd (#6981)* [AutoScheduler] Accelerate feature extraction for winograd* fix an overflow in feature.cc* address comments* address comments* Update include/tvm/te/schedule.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Use a smaller min_repeat_ms* Use a smaller min_repeat_msCo-authored-by: Cody Yu <comaniac0422@gmail.com>	1
Section names for TVM generated constants (#9524)This PR places the TVM generated static const arrays in a memory section named .rodata.tvm.The arrays default to 16-byte aligned but a constants-byte-alignment parameter can be added to the target to set alignment to a specific value.This allows the linker script to optionally place TVM generated constants in particular memory regions.	2
add expr simplify and canonical	1
[TEST] Fix flaky topi/tests/python/test_topi_pooling.py:test_adaptive_pool (#5736)	3
[AutoTVM] Add batch_matmul to tunable operations (#4242)* Batch matmul tuning running but with errors.* Default x86 schedule as good as before.* Code Cleanup* Remove unused argument.* improved template documentation.* Silly lint fix* Removed leftover comment.* Moved cfg declaration to schedule for batch_matmul* Moved x86 dense cfg declaration to schedule.* lint fix* Removed duplicate cfg declaration in dense.* Reverted changes to dense.	4
[DOCKER] Fix dependency for autotvm (#1398)	0
[TOPI] conv2d nchw gpu scheduler (#315)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed* conv2d gpu scheduler for two layouts moved to tvm* changes made according to CR* conv2d_nchw formating updated, conv2d_hwcn tests updated* lint error fixed* element wise operator schedule fusing fixed for conv2d* conv2d_nchw topi test added, all resnet workloads now pass* conv compute lint error fixed* fixed python 3 compatibility problem* conv2d tensor input support added, test typo fixed, ir_pass.Simplify changed to util.get_const_int	1
[Bugfix] Repeat and tile bug fixed, relay tests added (#2804)	1
[Frontend][TFLite] support for FILL and SPLIT_V operators (#5330)* tflite spliv ops* TFLITE fill and splitv ops* TFLITE fill and splitv ops* TFLITE fill and splitv ops* remove unnecessary operator check	1
[Runtime][Pipeline executor] Global parameters group name and runtime modules parameters map. (#9846)* [Runtime][Pipeline executor] Global parameters group name and runtimemodules parameters map.Solution:To support on the fly parameters setting for each runtime modulein pipeline executor, we create a feature that use global parametersgroup name to map the runtime module parameter, after such maprelation get created user can do the on the fly parameters settingby using the parameters group name.trigger build.fix ut issue.polish comments.Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update src/runtime/pipeline/pipeline_executor.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update src/runtime/pipeline/pipeline_struct.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>address review comments.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix plint issue.Co-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[PyTorch] Guarantee data input is the first argument (#7592)	5
[Profiler] Do not aggregate frames with different devices (#9290)	2
[COMMUNITY] comaniac added as new PMC member (#8470)	1
enable negative values in TShape (#125)	0
[microTVM][ARM][Zephyr] Add CMSIS dependencies in Zephyr project build (#11362)* Test with CMSIS build addeddisabled conv2d_nhwc_dsp.arm_cpu for non integers workloadsadded debugging feature to TempDirectory* revert arm_cpu strategy changes* Address Andrew comments* change copy to include* add cmsis_path only as project option	1
Make target and build module more pythonic (#1089)	1
[doc][fix] fix sphinx parsing for pass infra tutorial (#4337)	5
[DOC] Fix doxygen comments (#247)	0
[Doc] add KEYS to downloads.apache.org (#6581)	1
[REFACTOR][IR] Polish ir/type (#4705)- Use consistent constructor style to construct objects.- Move env_func to ir as it is mainly used to construct IRs.- Make docs consistent.	2
Apparently, ONNX Conv with no 'pads' defaults to zero padding (#5548)	1
[VTA] Fix TSIM compile error in Linux (add missing -fPIC flag) (#3876)* [VTA] Fix TSIM compile error in Linux (add missing -fPIC flag);* [VTA] Fix TSIM compile error in Linux (add missing -fPIC flag);* fix indentation problem;	0
Merge pull request #5 from ZihengJiang/masterAdd tile operation	1
Fix curand. (#11901)	0
Fix GraphModule.load_params to allow passing parameters that are not an expected input (#7665)	2
skip aot checks when USE_MICRO=OFF (#8772)	1
[TensorIR][PASS] CompactBufferAllocation (#7923)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
Add runtime.ModuleGetFormat method enabling export of BYOC generated sources which require a .cpp/.cc file extension (#9243)* Allow export of C++ kernels using correct file extension* [WIP] Set module_key=c for CSourceCrtMetadataModuleNode to temporarily fix failing testsI realized that the module format `cc` is currently already used by the `CSourceCrtMetadataModuleNode` declared in `src/target/source/source_module.cc`.This needs to be discussed first to decide if either the module_key should be changed or the test cases expecting the systemlib kernel (e.g. `default_lib0.c`) to have a `.c` extension.* Update Makefiles used by tests/python/relay/aot/ to support C++ file extensionsAOT: Add c++ support to aot_test.mkAOT: Add c++ support to corstone300.mk* Add missing definition of GetFormat to cmsisnn and ethosn codegens (WIP)* Resolve PR comments* lint python/tvm/runtime/module.py* fix EthosUModuleNode for CI* Fix: detect empty module.format* Add error message to assertion* Lint python/tvm/runtime/module.py	1
[PASS] InjectDoubleBuffer (#405)	4
[CMSIS-NN] Increase partitioning accuracy for pooling (#11229)This ensures that CMSIS-NN is only used when the batch size and layout are correct for the library calls.	1
[Ansor][AutoTVM v2.0] Part 0: Ansor minimum system for auto schedule generating (#5962)* Code migration Start (#1)* Init commit: Code migration Start* Add loop_state.cc/h* Add ComputeDAG basic test* Split transform_step out & Update more UTs (#3)* Split transform_step out* Update GetProducers & GetConsumers* Update UTs* Add UT for CacheReadWrite & Some bug fix* Add search_task, measure and serialization (#4)* Add FollowSplit & FollowFusedSplit tests* Update dag.InferBound & its UT* Add search_task, measure and serialization* Update Serialization UT* Add MetaTileRewritePolicy (#5)* Add feature* Add cost_model, meta_tile_rewrite_policy* Add MetaTileRewritePolicy basic UT* Basic Python API for State (#6)* Add Basic Python API for State* Add UTs for State* Add Python API: Measure & Task (#7)* Update the return value of state operation* Add task* Copy measure.py & utils.py* Fix LocalBuilder* Fix LocalRunner* Add ansor.auto_schedule() API; First AutoSchedule working version(#8)* Add basic Python support for ansor.auto_schedule* Update AutoSchedule API* Bug fix for get the attach point of a fused iter* Update UT after infer bug fix* Bug fix & Add python serialization API (#10)* Delete C++ UT hack since Python is ready* Add ndarray.non_empty* Update Serialization python API* Improve code style, python wrapper and test cases (#11)* Update c++ code style and unit test* Update python State wrapper and test cases* fix unit tests* Add RPCRunner & OpenCL/CUDA test (#12)* Add RPCRunner & OpenCL search test* Add CUDA search test* Add RPCRunner test* rebase to upstream/master* Add Ansor basic tutorial (#13)* Add basic tutorial* migrate feature extraction (#14)* Add XGBModel & RPCRunnerWarpper (#15)* Add XGBModel & RPCRunnerWarpper* Revert "Add Parallel Granularity Mutation"* Migrate workload_registry.py (#16)* add workload registry* update* update* add task scheduler (#17)* Add conv2d cuda tutorial with workload registry (#18)* add tune_test.py (the old tune_wkl.py) (#19)* add tune_test.py (the old tune_wkl.py)* update* fix measure* fix for gpu* Code refine for tune_test.py & Add a pre load callback (#20)* Bug fix for tutorials* Add PreLoadMeasuredStates* Add search_callback support for task tuner* Code refine for tune_test.py* Update* Update* Update* Update* Bug fix* Add python custom sketch rule (#21)* Add custom sketch rule* Bug fix* Ansor Relay Integration (without layout rewrite) (#22)* relay integration* Add tune_op_subgraph.py & Some code clean for tune_network.py (#23)* Add single op tune scripts* Add tune subgraph support* Merge all op & all subgraph to one file* Rename file* add explicit_unroll_max_extent (#25)* Add Index simplification & API update (#26)* Add vectorized cooperative_fetching test* Update math simplify for vectorized CF* File rename* Update tune_network* API update* Update PreLoadMeasuredStates & Some bug fix (#27)* Add a threading wrapper to fix the test bug* Set default TVM_USE_AUTO_SCHEDULER to false* Update PreLoadMeasuredStates callback* Add tensorize step for loop_state (#31)* Add tensorize step* State python api update (#33)* Start to update api* Add compute_dag to state* API update* kernel layout rewrite (#28)* kernel layout rewrite* remove some hacks* add defuse_ops pass and move kernel_layout_rewrite pass after fuse_ops pass* set TVM_RELAY_DISABLE_BUILD_CACHE for task extraction and prepare_layout_rewrite* [cache flush] port cache flush to ansor (#32)* Improve relay integration (#34)* tmp checkpoint* Improve relay integration* Improve relay integration* Fix xgb error & Simplify dispatcher (#35)* Rename "MetaTileRewritePolicy" to "SketchPolicy". (#36)* Rename "MetaTileRewritePolicy" to "SketchPolicy".* Add a new class for auto_unroll_max_step, storage_offset in StageNode* fix tune_op_subgraph.py* rebase* Migrate all node::make to noderef's construct function (#37)* Start to move xxxnode::make to noderef()* Update* Update* Finish transform_step* Finish comute dag & auto schedule* Update* Update* Update* Update* Update* Code refine* Code refine* Code refine* Update* Update* Some lint fix & Recover the double constructor of tvm::PrimExpr (#39)* lint fix* clang-format-fix* pylint fix* Update* Recover the double constructor of tvm::PrimExpr* Fix pylint* pylint fix* pylint fix* Add MutateComputeLocation and MutateParallel in evolutionary search (#40)* Add MutateComputeLocation and MutateParallel in evolutionary search* fix lint* Improve loop state python API (stage_tensors -> stage_ops) (#41)* improve loop state python API (stage_tensors -> stage_ops)* fix* ComputeDAG bug fix & Add Custom TensorCore Matmul Example (#42)* Bug Fix* Sample example of Custom TensorCore Matmul* Rever Commits, Start to build minimum Ansor system* Code clean for minimum Ansor system* Bug fix & Delete AccessAnalyzer* Delete attachmap & Code clean* Doc updateUpdate statenode::stages from vector to Array* Headfile update & Python doc update* clang-format fix* pylint fix* Update* Doc update* Update* Bug fix after code merge to the new master* clang-format fix* Update* Update* Update std::vector to Array; Update verbosity setting; Some commemtsaddressed* std::vector->Array & std::string->String* Add init_state to ComputeDAG* Update* Update some unordered_map to Map* clang-format fix* Comments addressedDelete ReplayAndInferBoundDelete ReplaySteps & InferBoundCommon* Lint fix* Update* Update* Update* Update* Update* Update* Update* Update* Update* Rename ansor namespace to auto_schedule* Update* Rename ThreadPool to ParallelFor* Add parallel_for* Remove ThreadPool* Update python/tvm/auto_schedule/auto_schedule.py* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>Co-authored-by: Minmin Sun (孙敏敏) <minmin.smm@alibaba-inc.com>Co-authored-by: Zhao Wu <zhaowu@apache.org>	5
[MetaSchedule] Add target field to MetaScheduleContext (#10169)* Add target field to MetaScheduleContext* fix linter issues	0
[TOPI] Enable scatter_add on GPU  (#6856)* enable scatter gpu test on cuda* adding update_func arg* pytorch scatter_add gpu tests working* update 3d and 4d scatter* enable scatter_add gpu testCo-authored-by: masa <masa@pop-os.localdomain>	3
Avoid runtime exception when file doesn't exist (#2441)* Avoid runtime exception when file doesn't exist* Update the check based on feedback* Revert the old fix	0
use LLVM linker (#2713)* use LLVM linker* error message improved in case of filenotfound* linting error fixed	0
Add rocm target to topi tests (#548)* add masahi to contributors* enable rocm target in topi tests	3
fix group_conv3d caculate error (#12500)	0
[bugfix] fix the bug caused by test_any (#3528)	3
[Relay][Op] Trilu operator implementation (#12124)* Added topi trilu implementation* Implemented and tested full Trilu op.* Fix test type.* Add tril zero tests.* Add pytorch trilu integration.* Clean up torch integration.* Readded skip for zero tests.	3
[DOC] Generalize the get_started script for beginners with different environments. (#798)	1
Enable aux data (#24)	5
[Hexagon] Enable running CI tests via simulator (#10473)	3
[TIR] Add schedule primitive TransformBlockLayout (#11485)* [TIR] Add schedule primitive TransformBlockLayout* fixup! [TIR] Add schedule primitive TransformBlockLayoutFix doc	2
Update contribute.md	5
[TFLite][Python 2] Solve TFLite frontend python 2 compatibility and Modify TFLite whl files path (#2529)	2
[TOPI] Fix traverse function not inline zero-input op (#3623)* Fix traverse_inline not inline zero input op properly* Add where to python and set tag to broadcast* Fix inline* test* fix test target* fix	0
cleanup (#21)	4
[Hexagon] Updated incomplete docstring (#10879)As a follow-up from https://github.com/apache/tvm/pull/10846,completing a docstring that unintentionally ended in the middle of asentence.	2
[Relay][Module] Refactor the way we interface between different modules of Relay. (#3906)* Module refactor* Add load module* Add support for idempotent import* Tweak load paths* Move path around* Expose C++ import functions in Python* Fix import* Add doc string* Fix* Fix lint* Fix lint* Fix test failure* Add type solver* Fix lint	0
[AUTOTVM] Use opt level 3 when extracting tasks (#10065)* [AUTOTVM] Use opt level 3 when extracting tasksAutotvm was implicitly ignoring opt_level when extracting tasks becausepass opt_level is a thread local variable and extraction happens in anew thread. Not having opt_level 3 causes alter op layout to notfire, which in turn prevents tuning from finding all possible kernels.* disable alter op layout	1
[Vulkan] Remove dependency on Target from -from_device functionality. (#8171)The `tvm.target.Target("vulkan -from_device=0")` functionality wasinitially implemented by generating/returning a Target.  This brokeusage of libtvm_runtime.so, since Target is only defined in libtvm.so.This commit reimplements the functionality without the dependency onTarget, Integer, Bool, or IntImm.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Revive the Rust + SGX refactor (#4976)* Add Nick's changes's squashed* Fix frontend compilation* Re-enable Rust CI* Add changes with conflicted badly* Restructure import_module! macro in order to avoid unstable features* Kill old unstable feature enablement* Refactor common to use new APIs* Move the code to stable* Fix warningCo-authored-by: Nick Hynes <nhynes@oasislabs.com>	2
[RUNTIME] Add Function, Unify TVMTypeCode and TVMArgTypeID (#24)	1
Make expressions in the DynamicToStatic pass tests more dynamic (#8989)	3
Compare all outputs in TFLite test_forward_ssd_mobilenet_v1 (#4373)	3
[Hexagon] Refactor tvm.contrib.hexagon, NFC (#10616)* [Hexagon] Refactor tvm.contrib.hexagon, NFCBreak it up into multiple files.* Restart CI	2
[CUBLAS] Remove deprecated CUBLAS_TENSOR_OP_MATH flag (#8130)This flag is causes CUBLAS to use tensore cores on all operations. Withf32 or f64 operations, this leads to loss of accuracy.	1
[Vulkan][Codegen] Spir-V codegen, correct labels/blocks in WhileNode. (#8013)Previously, the WhileNode assumes that evaluating the loop conditionwill not introduce any additional labels.  If this assumption isviolated, such as for a WhileNode whose condition is an if/elsestatement, then the OpLoopMerge instruction appears in the wrongblock.The unittest added exercises this code path, but doesn't yet trigger afailure.  Once spvValidate is enabled for all vulkan codegen, thenthis unit test will catch the failure mode.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[PYTHON][WINDOWS] More robust dll loading behavior after python3.8 (#6707)The dll search directories need to be manually addedby os.add_dll_directory after python3.8.	1
[DEPRECATION] Remove NNVM compiler (#4571)* Remove NNVM compiler	4
[BUGFIX][IR] Fix String SEqual (#5275)* fix String SEqual* retrigger ci	0
[microNPU] Add MergeConstants pass (#12029)* [microNPU] Add MergeConstants passChange-Id: I1ff51d8147fba8c66d442a370b9f058e9b2758d8* Fix errors and warningsChange-Id: I29f68f83a73fa00ca34ed0ab2321c53c6b761137* Address commentsChange-Id: Iad59107d5abdec6b079c6fd4ab48c6bffbb5e0bb* Fix lint errorChange-Id: Ie5caf506337de01e169d6f422e4682eefbd93241	4
[AUTOTVM] Refactor measure build func (#2927)	4
fix missing ffi binding of relay.attrs.DequantizeAttrs (#7054)	0
[TIR][BugFix] Do not bind non-index type value of lets in CompactBufferAllocation (#11828)	0
fixing typo in apps/README.md (#538)	2
[MetaSchedule] Refactor MultiLevelTiling state to allow subclassing (#11931)This PR made `State` in `MultiLevelTiling` inherit `Object`, to allow future subclassing of `State`. Making `State` an `Object` allows instances of `State` and its subclasses to be stored in `std::vector<State>`.	1
[Contrib] Added default non-verbose to download_testdata(), pass to download() (#8533)* [Contrib] Added default non-verbose to download_testdata(), pass to download().Minor cleanup as well, while in the file- Using tempfile.TemporaryDirectory instead of explicit cleanup.- Pass through verbose/retries arguments if replacing a corrupted  copy.* [Contrib] Switched download.py from print statements to logging* [Contrib] Added shutil.copy2 fallback after downloading file.Initial implementation using tempfile.TemporaryDirectory assumed thatthe tempdir and output location were on the same drive, and could berenamed.  This update falls back to copying from the temporarydirectory, in case the tempdir is on a different drive.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[KERAS]Minimum & AlphaDropout op support (#5380)	1
Use local complete block and local reduction block to identify compact dataflow (#10705)* inint* upd* upd* remove redundant print* upd* change the reads/writes region for argmin/val* fix wrong push	0
[TIR] Move UnifyThreadBinding to earlier stage (#9365)* Move unify thread binding to earlier stage* Unify thread binding support AttrStmt	1
[TOPI] Numpy consistency: always broadcast binary op. (#1321)	5
Bump ci-cpu and ci-arm container versions. (#7745)	5
[HEXAGON][TOPI] This PR adjusts schedules so >64 length vector loads/stores are not generated at LLVM level. This is a workaround for an instruction selection issue in current version of llvm for hexagon (#12471)	0
[Python] Replace os.path.exists with try...except...else (#4784)	1
Restore the use of ONNX_DEFAULT_CONFIGS["use_nt_batch_matmul"] (#9925)	5
[Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.	1
[TIR] Add pass to check for out of bounds memory access (#12352)* [TIR] Add pass to check for out of bounds memory accessThis is a conservative static analysis that checks to see if any out ofbounds array access occurs. It is not enabled by default.* formatting* manually construct local irmodule* update comment* fix bug in int_set	1
Expose array to python	5
[Formatting] Fix python formatting issue (#6491)After #6442 was merged, black started complaining about formatting of test_pass_convert_op_layout.py. Format here.Change-Id: I04346fa06e22b722b619488b895b47c6943e3fd9	4
[TOPI] Fix for pooling (#673)	0
Allow datatypes besides fp32 in conv2d_transpose for cuda. (#6593)	5
Update README.md	2
[CI] Limit number of threads in all jobs (#5815)	5
[TE] Fix bug if find a loop in compute_at attach path (#7898)	0
Community guideline in effect (#1261)	3
[TIR][Schedule][UX] Beautify TIR Trace Printing (#12507)Following https://github.com/apache/tvm/pull/12197, this PR introduces`Schedule.show()` which convenience the user experience in the followingtwo aspects:- Python syntax highlighting- Outputs a schedule function instead of standalone instructions so thatit's easier to follow.To demonstrate this change:- Before `Schedule.show()` is introduced:<img width="555" alt="image" src="https://user-images.githubusercontent.com/22515877/185713487-03722566-1df7-45c7-a034-c1460d399681.png">- After this change:<img width="583" alt="image" src="https://user-images.githubusercontent.com/22515877/185713564-c54f3a9d-cd52-4709-a8b8-d8a61361e611.png">	1
[Bugfix] Fix #8536 Get Target When Heterogeneous Execution (#8537)	1
Update comments for the API tvm.lower (#2193)tvm.Schedule ==> tvm.schedule.Schedule	5
Improve the x86 auto-tune tutorial (#3609)	1
[RUNTIME][GOLANG] TVM runtime for golang v0.1 (#1470)	1
[UnitTest][Vulkan] Runnable relay unit tests on Vulkan (#8947)* [UnitTest] Added ids argument to tvm.testing.parametersThis matches the usage in `tvm.testing.parameter`, and allows forparameter sets to be referred to by a single name.* [Pytest] Fixed ordering issue of tvm.testing.parametrize_targets and known_failing_targetsIf an explicit list of targets is given, then the`known_failing_targets` decorator would fail to apply.  This commitresolves the issue, and cleans up all target-specific marks to applyin `tvm.testing.plugin._add_target_specific_marks`.* [UnitTest][Vulkan] Runnable relay unit tests on VulkanThis commit allows the relay test suite to be run targeting Vulkan with`TVM_TEST_TARGETS="vulkan -from_device=0" pytest tests/python/relay`.  Alltests that require a specific environment are skipped if that environmentisn't present.  All tests that are known to fail when running on Vulkanare marked as expected failure, and will be tracked inhttps://github.com/apache/tvm/issues/8903.- Failures during code generation  - Type mismatches, boolean vs int8    - tests/python/relay/test_any.py::test_any_reduce    - tests/python/relay/test_op_level3.py::test_sparse_reshape    - tests/python/relay/test_op_level4.py::test_reduce_functions    - tests/python/relay/test_vm.py::test_cond    - tests/python/relay/test_vm.py::test_simple_if  - Incorrect strategy selection, picks NCHWc implemenation for NHWC layout    - tests/python/relay/test_op_level2.py::test_conv2d_run  - Unresolved CallNode operation    - tests/python/relay/test_op_level1.py::test_unary_op[erf/tan/atan]    - tests/python/relay/test_op_level3.py::test_scatter_add    - tests/python/relay/test_op_level3.py::test_segment_sum  - Generates 64-bit calls to GLSL that have only 16-/32-bit support    - tests/python/relay/test_op_grad_level1.py::test_log_softmax_grad    - tests/python/relay/test_op_grad_level1.py::test_softmax_grad    - tests/python/relay/test_op_grad_level1.py::test_unary_op    - tests/python/relay/test_op_grad_level10.py::test_cross_entropy_grad  - Codegen raises error for variable size    - tests/python/relay/test_any.py::test_any_batch_matmul    - tests/python/relay/test_any.py::test_any_conv2d_NCHWc    - tests/python/relay/test_any.py::test_any_dense- Failures when running  - Numeric differences (observed on GTX 1650 with NVIDIA driver)    - tests/python/relay/test_op_level3.py::test_take    - tests/python/relay/test_op_level5.py::TestCropAndResize    - tests/python/relay/test_op_level5.py::TestResize1D    - tests/python/relay/test_op_level5.py::TestResize2D	3
Update QemuTransport#write() to match new write API contract. (#8761)* suspect this should fix #8278	0
Rename bound to schedule, add graph related utils	1
[CI][DOCKER] Update ci-gpu to v0.60 (#4827)	5
[DOCS] Improve review guide, improve cmake llvm build (#1295)	1
checked buffer / schedule / code gen	5
[REFACTOR][TIR] Migrate low-level pass functions to Pass Manager, (#5213)- Migrate LowerTVMBultin- Migrate inferFragment, LowerThreadAllreduce- Migrate ThreadSync- Refactor target::Build to directly take IRModule.- Remove un-used legacy functions.	1
[Target] Add target_parser to TargetKind (#12119)This adds the `target_parser` as described in https://github.com/apache/tvm-rfcs/pull/71, which parses an incoming `TargetJSON` and produces a new configuration for generating the final `Target` object from.Marks `set_attrs_preprocessor` as deprecated and errors if both `set_attrs_preprocessor` and `set_target_parser` exist together.	1
Fix a bug in inject-virtual-thread (#2039)	0
[Relay][Frontend] Keras Support (#2336)	1
[Rust] Impl IsObjectRef for Array (#7138)* impl isobjectref for array* array test* cargo fmt	3
Introduce Model Library Format export format (#7533)* Introduce Model Library Format export format. * This function produces a stable on-disk representation of TVM's   compiler output. * It's intended just for use with the C runtime for microTVM right   now. It could be expanded for other use cases. * This PR implements the Model Library Format RFC, which ultimately   is intended to support the Project Generator API (RFC   forthcoming). * There may be some changes to the format without revving the version   number until downstream consumers are known. The Project Generator   API is the first such known downstream consumer. * There are no plans currently to support generating old Model   Library Format from TVM. The version number is intended as a   compatibility check between the generator and downstream consumers.	1
[TE][BuildModule] Fix import in dump pass ir (#5327)	4
[Relay][Dyn] Dynamic full operator (#6260)* moved full from other branch* fixed some typos* fix lint* add final newline* fix int64 test	3
[Relay] add python doc for function in ir_pass (#1877)	4
[Relay] Merge analysis/context_analysis.cc and transforms/device_annotation.cc (#9038)* [Relay] Merge analysis/context_analysis.cc and transforms/device_annotation.ccCurrently LowerTEPass (backend/te_compiler.cc) is a 'special' pass because itdepends on a side-input DeviceMap. We'd like to remove that side-input, andinstead recover the Device (and, ultimately, Target) for each (fused) primitivecall from the AST alone.By doing so we also avoid needing to perform device planning twice: - It needs to be done before lowering so we know which primitives need   to be compiled for which devices. - It then needs to be re-done after lowering and optimization as a prelude   to memory planning.By baking the device plan into the AST we can simply do device planning beforelowering, and run memory planning later, both as ordinary passes.While working on that issue we realized we currently have 3 'device planners': - transforms/device_annotation.cc, which supports only a small subset of Relay   and uses a simple top-down algorithm to assign a device to every   sub-expression. - analysis/context_analysis.cc, which makes a galant effort to support most of   Relay, is based on unification rather than a top-down algorithm, but handles   higher order functions by ad-hoc and fragile inlining. - transforms/annotate_target.cc, which works on Targets instead of Devices, but   is otherwise like 'device planning'.We'd like to bring these together.In this PR we introduce a new transforms/device_planner.cc intended to replacetransforms/device_annotation.cc and analysis/context_analysis.cc. We don'tdelete those two just yet since we need to switch all users off of them in thenext PR. We also leave transforms/annotate_target.cc alone pending a proper RFCto bring devices and targets together sensibly, but have it firmly in oursights.transforms/device_planner.cc is based on analysis/context_analysis.cc, butis heavily reworked to: 1. Handle starting from existing "on_device" annotations as well as existing    "device_copy" calls. 2. Be idempotent, with the idea we'll probably need to re-run it to 'refine'    device planning to account for storge scopes. 3. Robustly handle all of Relay, particularly higher-order functions. For that    we replace the inlining approach in analysis/context_analysis.cc with a    higher-order unification domain. 4. Be a little more systematic with defaulting. 5. Capture the result of the analysis within the AST as new "device_copy" calls    at device boundaries, and new/replaced "on_device" calls wherever the device    for a sub-expression is not already 'obvious' from the sub-expression's    lexical scope. 6. Provide helper visitors for passes which need to ask for the device for    any sub-expression they are processing and/or preserve device information    on rewrites. Those passes include:     - backend/aot_executor_codegen.cc (AOTOnDemandAllocator)     - backend/graph_plan_memory.cc (StorageAllocaBaseVisitor etc)     - backend/te_compiler.cc (LowerTensorExprMutator)     - backend/vm/lambda_lift.cc (LambdaLifter)     - transforms/memory_alloc.cc (DialectRewriter)     - transforms/to_a_normal_form.cc (Fill)     - backend/vm/compiler.cc (VMFunctionCompiler)    However we won't change any of those in this PR.See the draft https://github.com/apache/tvm/pull/8788 for the end game.* [checkpoint] Use Relay script for all unit tests.* [checkpoint] Hoist out DeviceDomain and DeviceDomains.* [checkpoint] Hoist out visitors* [checkpoint] Woops, left debug-only code in	0
[FFI][BUGFIX] Fix memory leak when Pac callback argument is NDArray (#6744)* [FFI][BUGFIX] Fix leak when Packed callback arg is ndarray.Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>* Fix for rust ts and jvm* Update rust/tvm-rt/src/to_function.rsCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
[CI] Temporary disable CRT test (#5297)	3
Bugfix #1692. Constant folding and result comparision allowance. (#1708)	1
[PASS] Enable StorageRewrite before virtual thread lowering (#880)* [PASS] Enable StorageRewrite before virtual thread lowering* update* fix testcase	3
ReshapeAttrs no longer has reverse (#7205)	5
[TOPI][CUDA] Fix nms block extent and type mismatch in multibox (#2320)	0
[Bugfix][Relay] Crash in match_exhaustion.cc when given an empty tuple pattern or constructor with no args (#7459)* [match_exhaustion] Fix cartesian product to handle empty tuple patterns or constructors with no args* Test cases do not actually exhibit the fixed bug* Mistake in comment	0
[BYOC] Fix DNNL Conv2D in JSON runtime (#9043)* dnnl memory dim is wrong for conv2d* change test case for dnnl conv2d* trigger CICo-authored-by: sunway <sunwayforever@hotmail.com>	3
[RUNTIME] Cleanup build for libbacktrace (#7706)* [RUNTIME] Cleanup build for libbacktrace- Introduce TVM_USE_LIBBACKTRACE value macro to be consistent  wth other value macros(instead of relying on disabled flag).- Introduce AUTO mode for libbacktrace- Temporary disable MacOS support in light of recent bug report.- Refactor out the libbacktrace.cmake to libs- Properly use TVM_DLL so that code is cross platform.- Fallback to the weaker dmlc impl when backtrace is disabled.* Update Logging.cmake* Update the macro check order to be consistent with the rest.	5
Add debug mode to tempdir() (#5581)	0
init mxnet converter (#27)graphbackupupdatefinish mxnet converterfixfix variousadd testsfixadd multi networksuses model_zoofix testsminor fixfix graphfix	0
Refactor to use iterVar	1
[TIR] Simplify indices in InjectVirtualThread (#12259)If the injected index expressions can be simplified to Rampnodes (e.g. `Ramp(0,1,4)` resulting in `Ramp(vthread*4, 1, 4)` insteadof `Ramp(0,1,4) + Broadcast(vthread*4, 4)`), these can be identifiedas vector access in later passes.  Simplifying at the time ofsubstitution avoids requiring all downstream passes to perform thesimplification.	4
Revert "[Relay] add test for second order ad (#2754)" (#2926)This reverts commit f5ca9915ab163364c885de0b103579e4d85460eb.	4
Remove incorrect extension registration of tvm::Target (#1272)	1
[iOS][RPC] Enable iOS simulation in public CI to cover basic tuning capabilities (#9212)* init class for launch of server with ios simulator* init infrastructure of tests* added functionality for automatic loading of the simulator* add error handling for simulator interaction* extend tests for connection configurations* add test for pure rpc connection* init test for remote call* add wrappers for connect configurations* remove duplicate code* add tests for simple remote call* change policy for tests of connect configurations* added tests to check basic functionality of rpc session* add test for remote graph executor* add test for auto schedule tuning* remove hardcode parameters* add success criterias for auto schedule tuning* fixing problems related to running tests through the pytest* expand the workflow for new iOS RPC tests* update GH workflow for iOS* update GH workflow for iOS: conda shell* add parser for iOS RPC console log* add depends for ios tests* set verbose flag for rpc server* changes related with main checkout* add context manager class for ios rpc server launcher* extend pythonpath* add watchdog for start ios rpc server* clean up GH actions workflow* clean up GH actions workflow* rename enum SimulatorSystem to OSName* fix python format black* fix bash syntax* add doc strings for API* skip tests, because this type of connection was broken* fix lint* add check that current environment has required environment variables* code review fixes* replaced call os.system with call subprocess.check_call* add description for messages from iOS RPC Server	1
[NNVM] : Function header corrections. (#324)	1
[LINT] Fix clang-format (#6264)	0
[TVMC] Add configuration `tir.add_lower_pass` to option `--pass-config` (#9817)	5
[TOPI] Update pre-tuned parameters for TX2 and fp16 on Mali (#1892)	2
Consider pad value and input zero point in FoldExplicitPading (#11127)This commit adds the following:Do not fold `nn.pad` and `qnn.conv2d` if padding value is notequal to input zero point of qnn operation. Added unit testto check such behaviour.	3
Usability fixes to CI runner script (#9752)* Usability fixes to CI runner script* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay] Make check stricter by using Feature. Fixed multiple bugs. (#6326)* savelintlintlintfix lintlintupdatelintsavesavesavelintformatformatsavesavefixuse a form more suitable for numeric checksave* save* save* lint* save* lint* fix* fix	0
[TEST] Address flaky error in test_any (#6705)	3
add memoized expr translator for use by backend codegen (#5325)	1
[1/3][AOT][DeviceAPI] Connecting devices structure to relevant operators (#9395)* [AOT][DeviceAPI] Connecting devices structure to relevant operatorsThis patch adds support for passing the device context via the unpacked API in AOT, generating an additional struct if necessary:```c/*! * \brief Device context pointers for TVM module "default" */struct tvmgen_default_devices {  void* npu;};```Which is then added as an argument to the entry function:```c/*! * \brief entrypoint function for TVM module "default" * \param inputs Input tensors for the module * \param outputs Output tensors for the module * \param devices Device context pointers for the module */int32_t tvmgen_default_run(  struct tvmgen_default_inputs* inputs,  struct tvmgen_default_outputs* outputs,  struct tvmgen_default_devices* devices);```I've temporarily added the collection of external code generators to the TE compiler pending proper annotation of the eventual functions.Co-authored-by: Grant Watson <grant.watson@arm.com>* Correct "use_device_api" attribute name on TargetCo-authored-by: Grant Watson <grant.watson@arm.com>	1
[ci] Add a tag to generated Jenkinsfile (#10825)This adds a timestamp to the generated Jenkinsfile that is ignored when `--check`-ing. This line should generate merge conflicts for updates that would not pass `--check` in CI on main, so PRs will need to be rebased and the Jenkinsfile regenerated.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Add pass ManifestSharedMemoryLocalStage (#12355)Added a pass to insert local (cache) stage for the shared memory. It's similar to cache read but bypasses the limitation of int set analysis for compacting buffer region by inferring the buffer shape from the loop extents.	5
Update NEWS to include v0.8 change log (#9580)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	2
Add PAD operator to relay tflite frontend (#3310)	1
fix symbol output compose (#166)	0
[BUGFIX] fix illegal memory access bug in reduce op schedule by constriant threadIdx.y (#8566)Signed-off-by: ziqiang.pzq <ziqiang.pzq@alibaba-inc.com>Co-authored-by: ziqiang.pzq <ziqiang.pzq@alibaba-inc.com>	0
[TEST] CI infrastructure (#30)	5
Add ROCm docker (#7422)	2
Fix infer type of kernel in dense. (#4125)* Fix infer type of kernel in dense.* - Moving the check of weight being nullptr up as it is needed in both the branches now.- Adding test case for validating that data dtype and kernel dtypes can be different.* - Fix the dtype check for weight. If the weight is not present then we will use the data dtype.	5
Add caffe2 nnvm frontend to CI (#3018)	1
[FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py	3
[FRONTEND][TENSORFLOW] Fix a typo in _matmul (#2152)	2
[LLVM CodeGen] Partially disable unsafe fp math (#2422)	5
[DNNL][Relay extern-schedule] DNNL Conv2D Kernel enable by assigning "-libs=mkldnn" (#11571)* enable oneDNN conv op by using -libs=mkldnn* add channel last format support and let oneDNN chose blocked format.* remove unnecessary changes* reformat 3 files* reformat 1 file* change the argument name* change the argument name* rename the arguments* fix cpp lint issue* fix cpp lint issue* fix cpp lint issue* clang reformated* adjust .py import for testing* function existence check in test	3
[TOPI] Relu and schedule elemwise (#390)* convolution compute typo fixed* relu activation migrated to topi* reviews addressed* elemwise schedule added* relu compute deleted	4
Install rust in ci-lint so cargo fmt can move to lint stage. (#8727)	4
Enable copy on write in graph attrs (#31)* [INFER] Enhance backward op policy* [SYMBOL] add list inputs* relax graph attr to enable copy-on-write	0
upstream (#6436)	5
CPP support for region and reorg operators (#1115)	1
[TOPI] add basic scheduling for conv2d_transpose on x86 (#3491)* initialize cond 2d transpose scheduling on x86* refine the scheduler a bit* fix for lint* address review comments; remove duplicate code* fix lint	0
Install curl in ubuntu_install_core.sh (#8310)Rationalize the installation of curl among all the docker install scripts.	2
[Vulkan][Refactor] Pull out vulkan initialization into VulkanInstance and VulkanDevice (#8188)* [Vulkan][Refactor] Broke out VkInstance setup/teardown into managed class.- Previously, the VkInstance was directly owned by the  VulkanDeviceAPI.  Now, VulkanDeviceAPI owns a  tvm::runtime::vulkan::VulkanInstance that does setup/teardown of the  VkInstance.  This way, the teardown is done even if a later  initialization step throws an exception.* [Vulkan] Renamed VulkanContext to VulkanDeviceRenaming to match with the tvm.context to tvm.device rename.* [Vulkan][Refactor] Extracted VulkanDevice initialization into VulkanDevice class* [Vulkan][Refactor] Removed the VkPhysicalDeviceProperties member variable from VulkanDevice- Now that there is a separate VulkanDeviceProperties class, the  redundant VkPhysicalDeviceProperties can be removed.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Topi] Fix direct SIMD conv2d schedule name (#9225)* feature tested* change name	4
[CUTLASS] Support more kernels: int8, tf32, and 3xtf32 (#9899)* add int8 type in library* wip* adding test and plumbing data and weight dtype* adding 3xtf32 support and refactor tile description enum* add 3xtf32 test* update gemm generator too* int8 test worked* 3xtf32 also works* int8 and 3xtf32 gemm works* clean up test* support int8 in sm75* refined int8 alignment constraints* black* support 3xtf32 in default kernel* remove log* refine dtype check* support tf32* leave TODO for alignment modification on int8 kernels* tf32 test working* fix default kernel for tf32* workaround for compilation failure* lint	0
[CuDNN] Remove GPU dependency from tvm.contrib.cudnn.conv_output_shape (#8275)Previously, if the local server couldn't initialize a CuDNN-enabledGPU, it couldn't generate code that uses CuDNN's forward conv.  Thiscommit adds a python implementation of conv_output_shape, along withtests to verify that the outputs are matched to CuDNN's output.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TOPI] Update TopHub and benchmark (#1796)	5
Fix tune_relay_cuda.py (#6001)	0
[microNPU] Fix incorrectly calculated stride when converting NHWC to NHCWB16 (#9560)Fixes an issue that causes strides to be incorrectly calculated when thenumber of channels in the input is less than 16 and involves a conversionfrom NHWC to NHCWB16. This is due to TVM being 'too smart' when analyzinggenerated TE and removing compute that is deemed unnecessary. Consequently,strides over data are incorrectly calculated leading to an outputmismatch.The PR uses a reduce sum operation to trick TE's data dependencyanalyzer into looping over a whole block (16), rather than the numberof channels actually used (< 16). This causes the calculated strides tobe a multiple of 16 which is required for NHCWB16 format.Change-Id: Ibf76a94a12cebf51fa716fcac1de932a271c4a6d	4
[Relay] fix 'please use input parameter mod warning' triggered in build_module (#3452)	2
NOTICE (#2203)	5
Revert "[CI] Fix build android rpc failure in CI" (#12277)Reverts apache/tvm#12216Addition to my previous changes.After the previous changes, the android camera build failed because by default `PYTHONPATH` is empty, and after `set -eux` it is fails:https://github.com/apache/tvm/blob/759a648cd5237885a8205b1ee4508dabcc3af2d5/.github/workflows/main.yml#L152-L156This error was not noticed because the flag `continue-on-error` is true.	0
[TOPI] Add dilation argument to conv2d and depthwise_conv2d (#1970)	1
[PYTORCH]Padding support (#5638)	1
[CODEGEN][CUDA] Fix vector load (#5226)* Fix high-low bit bug in __pack_half2* Fix vector load* Add unit8 support for PrintVecElemLoadExpr and BroadcastNode	1
Allow long type values in shape list (#1806)* Allow long type values in shape list* Update build_module.py	5
Add EtaExpand to transform API (#3406)* Add EtaExpand to transform API* Add test case	3
[BUILD] Simplify after bind device type (#2670)	5
Adds SEScope (Storage/Execution Scope) for use as new unit of planning in 'device' planning. (#9313)[Target] Adds SEScope (Storage/Execution Scope) for use as new unit of planning in 'device' planningThis is the first step in https://github.com/apache/tvm-rfcs/pull/38 to bring devicesand targets together when doing device planning. I've gone ahead and also included amemory scope in this object since we will also need to propagate memory scopes acrossRelay expressions once this basic preparation is in place. In the meantime that field will beleft as "".Once device planning works in units of SEScopes it will be possible to directly read offthe device and target for any Relay sub-expression without the need for TargetMaps ortthe construction of default Targets.SEScopes also support 'Join' and 'Default' operations needed when constraint solving inthe device planner. You can see those in use in my scratchpad branch:  https://github.com/mbs-octoml/mbs-tvm/tree/mbs-scopesThis PR also brings some duplicated and the ad-hoc 'default target' handling logictogether into a CompilationConfig class. (Again, see the scratchpad branch for how thatwill end up being used). I've placed that next to SEScope since it's main purpose is to  a) establish the default SEScope for primitive ops  b) establish the SEScope for the 'host'  c) feed a definitive vector of Targets into device planning so it can resolve all     "on_device" and "device_copy" device references to their full SEScope form.* Reworked to avoid global SEScopeCache.Realized while working through unit tests in the sequel that it's reasonablefor folks to call build multiple times with distinct Target objects, in whichcase the global cache would grow without bound.So instead placed the cache in the CompilationConfig class. Since that classnow has everything the device planner needs to do its job, promoted it tobe an FFI-able Object, which is now in compilation_config.{h,cc}.I think we can do much better with CompilationConfig, but for now keeping itto the minimum I needed to prepare for device planning from all the executorcompilation codepaths.	5
[TOPI] Conv2d Added and Optimized for Intel HD Graphics (#1290)	1
Add RoiAlign to Onnx frontend (#5454)	1
[Tensorflow] Support for Crop (#2285)fixesfixes	0
[Target] Add support for target object with host field compatible with previous api (#7534)* Fix legacy code on target host* Modify legacy code for target host change* Add tests and fix merge issue* Add condition for same host* Modify all files for new target host api compatibility* Add newline* Change import format* Optimize test file* Add match error info for unit tests* Fix for heterogeneous targets* Fix format for dict iteration* Fix target host type error* Skip one testcase for tvm infinite loop bug* Fixed bug for target map compatibility* Fix another TargetsMap issue* Fix typo and infinite loop error* Temporary fix for handle issue* Fix vm target* Add condition support for str case* Add GetHost function and fix previous bugs* Fix measure_record.cc* Fix search_task.cc* Fix compiler.cc, memory_alloc.cc* Fix driver_api.cc* Fix format* Fix bugs and GetHost function usage* Fix clang format* Fix bug* Modify python tests* Change python unit tests to new target api* Fi test_runtime_heterogeneous.py* Modify tutorials & remove extra print* Update more tests to new api* Refine the tutorial target usage* change argument name for Target constructor function* Fix target export function* Fix and validate all tutorial usage* Remove unused argument* Fix format* Fix bug in driver/build_module.py for heterogeneous target* Fix bug in driver/build_module.py for heterogeneous target more* Fix target host type error* Fix cudnn target host bug* Fix according to reviews, add helper function in python* Refactor code as helper function* Expand helper function* Fix bug add and update python helper function* Update target hosts* Fix format & refresh function* Fix unit test bug* Fix bug in refreshing host* Fix bug* Add SetHost function* Update export function* Fix format* Fix export bug in target* Fix bug on host referencing* Addtional tests* Address review issues* Fix format target.py* Fix issues and format* Add some 3rd party dependencies* Merge main branch* Fix target.h format* Remove redundent import* Fix function name* Add parameter name* Fix new code bug* Fix bug in lowering	0
[TOPI] Fix resize nearest with fractional scaling (#3244)	0
Improve tvmc error message from lazy-loading frontend imports (#9074)When installing TVM from the python package, the Frontend frameworks dependencies such as TensorFlow, PyTorch, ONNX, etc, are not installed by default.In case a user tries to run tvmc using a model whose framework was not installed, it will be presented with a very raw Python exception in the output.The aim of this commit is to implement a better error messages for errors related to lazy-loading frontend frameworks in tvmc.Change-Id: Ida52fac4116af392ee436390e14ea02c7090cef0	4
[team] add Yizhi's pgp key (#4380)	1
[tvmc][docs] Getting started tutorial for TVMC (#6597)* [tvmc][docs] Getting started tutorial for TVMC * Include a tutorial, demonstrating basic capabilities of   TVMC, by executing a full pipeline (tune, compile, run)   on a ResNet-50 model.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* apply suggestions from code reviewCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>* adjust text according to code-review* improve reading flow into tuning sectionCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[AutoTVM][Autoscheduler] Default build funcs inherit PassContext (#11632)* init commit* lint* empty commit* test results* reset progress* lint* fix	0
[CI] Update rust docker (#5141)	2
[LINT] Enable clang-format. (#5572)* [LINT] Enable clang-format.* Add more docs	2
Enable attribute key in LetStmt	0
[RUNTIME] Remove runtime, rely on tvm only (#38)	1
[microNPU] Refactor base address determination to codegen (#9929)This commit introduces BaseAddress ObjectRef to determinebase addresses in the codegen for microNPU. This isrequired when multiple memory pools become available. Thus,base addresses could not be statically determined in thesource module.	1
[Caffe Frontend] Add support for Permute layer (#9157)* Add support for Permute layer* Add test for Permute layer* Fix alignment	0
[CI] Clarify RAT exclude patterns. (#3328)	5
[RELAY][PASS] FoldScaleAxis Backward (#2024)	4
Update Symbol and C API (#22)* Update tuple to be compatible with mshadow* Move set error message to C API* simplify with using* updates to shape inference* Add unnamed namespace to the implementations* [SYMBOL] Enable inference of Auxiliary data, rename list_arguments to list_inputs	5
Docs: Fix links (#2118)	2
Add SGX random engine (#1113)	1
Tvmc python tutorial (#9633)* finish rpc and shape_dict in tut* added more to rpc* tutorial edits* added tutorial to docs in howto* accidentally had two copies of tutorial* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Apply suggestions from code reviewCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* added Leandro's suggestions* added example model at top* added example model, blacked it* trying to get docs to build* underline too short for title* forgot Jetson info, added Chris H comments* reformatting text* black* hitting code block issue, trying to debug* added spaces after the python codeblock* black* changing formatting* touching up more edits'* more touchups* changed location of file to tutorial section* changing doc location* broke the order of the docs somehow* fixed it yayy* added additional indentation* black'dCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>	1
[BUGFIX] Fix for NoneType Target (#3792)	1
[TIR] Minor fix to tensor intrin description (#12356)	0
[Refactor] Expose meta-schedule related packed func in a header and call it directly (#10470)* [MetaSchedule] Expose CreatePrimFuncFromOutputs in a header and callit directly* add include guard* exposed ContextQueryInsideWithScope too* oops* add tir namespace for clarity* address comment	1
[Codegen] Use "target.build.$TARGET_KIND" for all codegen functions. (#8071)* [Codegen] Use "target.build.$TARGET_KIND" for all codegen functions.- Removed special case for "micro_dev" target.  Instead, register  BuildCHost as both "target.build.c" and "target.build.micro_dev".- Renamed "target.build.build.aocl_sw_emu" to  "target.build.aocl_sw_emu".  Appears to be a typo introduced in  #841725cc585* [micro_dev] Removed references to non-existent micro_devdevice_api.micro_dev was removed in745e542e4deaf44f3d6e5665299aa85ef8f4a6b9, but several references stillremained.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Fix graph_tuner ancestor duplication (#7704)A diamond dependency currently generates a duplication of ancestors:  A / \B   C \ /  Dunder some conditions, this scenario leads to the following dictionnaryentry: "D":[A, A] instead of "D":[A].This results in failures when subsequently trying to transpose nodestates based on this data.Change-Id: I72f9b19286bbab0581b851c228b9d0e79ead400f	5
[EZ][Typo] Correct gather, scatter type rel error message (#10023)	0
[Bugfix] GetReduceAxes accept empty axis (#11643)* emptycommit 2nd try* codeCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>	1
Add ceil shape registration (#11533)	1
[ci][2/2] Shard `frontend: GPU` job into 2 jobs (#10414)	5
[CI] Added llvm-12 to ubuntu1804_install_llvm.sh (#8008)Change-Id: If614604ae6f5dd8d29ee3acf1635c38a30bd85ff	4
[Rust] Unify types between bindings and pure Rust impl (#2616)	5
Add parser support for CAST tflite operator (#4096)This implementation provides cast to limited number of dtypesthat tflite currently supports for placeholder op. Add INT64 in thepossible dtypes as it appears to be supported accrording to tlfite schema.	1
Python security issue about mktemp() and abspath() (#2202)	0
TensorCore Support using Intrinsic (#4136)* add tensor core support* avoid memory bank conflict* fix thread sync & better performance* better performance* add schedule test for conv2d* extend into BatchMatMul* support config fragment shape and layout using intrinsic* add TensorCore tutorial* add int support and fix lint* address comment* add 32*16*8 TensorCore test* fix wmma include logic	2
[Arith] Handle mod/floormod in modular set analysis (#10453)	1
[AutoTVM] Re-enable `ref_input` (#8113)* [AutoTVM] Re-enable ref_input* add ref_input on measure_option* add ref_input unittest* fix: test reformat* [autotvm] [ref-input] refine test and description* [autotvm] [ref-input] revert arg on measure_option	4
[Profiler] Fix graph_executor_debug hang (#12382)For some operations such as `__nop` or `__copy` the measured inferencetime is equal to 0. In this case we are in infinite loop and we won'texit from it. Added new parameter `limit_zero_time_iterations ` which specify themaximum number of repeats then the inference time is equal to 0. Whenwe exceed this value then we will exit from a loop.	5
[CODEGEN LLVM GPU] Initialize llvm before lookup for the target (#2683)	1
[FIX] Fix Typo (#164)	2
[PYTORCH]expand bug fix (#5576)	0
Update task_cpp_unittest.sh	3
Allow Linker script files to be committed (#8745)This is a source file type needed for https://github.com/apache/tvm/pull/8744Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Grant Watson <grant.watson@arm.com>	2
[rust][tvm-graph-rt]: maintain error sources when propagating errors, swap Mutex for RwLock (#6815)	0
[microTVM][tvmc] Add TVMC Micro tutorial for Zephyr (#10024)	1
[PyTorch] Fix pad_common for float pad_value (#12134)* fix pad* fix constant padding and handle float infinity* revert change to pad_width* fix constant pad value	0
[RUNTIME][OPENCL] Create program lazily when the program is built (#1408)	1
[FIX,CMAKE] Only compile runtime files once (#7417)* [FIX,CMAKE] Only compile runtime files once* copy defines to tvm_runtime_objs	1
[Hotfix] A line is accidentally removed in `Verify-GPU-Code`	4
[COMMUNITY] @srkreddy1238 -> Committer (#2339)	3
[Frontend][Darknet] L2 normalization support in darknet (#1916)* l2 normalization* retrigger CI	1
Add parses support for zeros_like tflite operator (#4042)The tensorflow zeros_like operation provided in array_ops.py produces directly a tensor with zeroswithout a graph, using only the shape and type of the input. This imposes the use of gen_array_ops.pythat produces both a tensor and a graph so a comparison between tflite and tvm can be done.	1
[ONNX] Add OpSet 13 implementation for Hardmax (#8924)* Add opset 13 impl for hardmax* Format	1
[Doc] Introduction to module serialization (#4564)	2
Fix] Avoid Directly Pass Python Context Object (#201)	4
[Relay][VM] Add autotvm context when compile (#4062)	1
fix lint (#1284)	0
[microTVM] Add wait to QEMU Setup   (#8236)* add wait* address comments* address comments* test added* add suggestions* fadd message assert for test	3
[RPC][BUGFIX][BACKPORT-0.6] Fix bug in rpc ring buffer shrink (#5516)	0
[LLVM][TIR] Propagate variable names to parameters. (#10514)* [LLVM][TIR] Propagate variable names to parameters.To aid in debugging, generate the variable names of function/closureparameters based on their TIR names.  These function/closure names canthen be observed in the generated LLVM IR.* Use name parameter in CreateLoad.* Fixed unit tests that check the parameter name.	2
TVM SGX (#919)* Update DMLC core most recent version* Modify runtime to minimize io when building for SGX* Add SGX example app* Prefer streaming versions of packed_func function	1
[PYTHON] Rename the namespace (#105)	5
[CI] Hotfix Jenkinsfile (#9739)	2
[COMMUNITY] junrushao1994 -> committer (#6719)	3
[RUNTIME] Simplify dynamic library and code path. (#27)* [RUNTIME] Simplify dynamic library and code path.* reword the readme	1
[C++][API] Consistent RAII scoping API. (#3231)	5
[TOPI][HEXAGON] Implement depthwise conv2d slice op. (#12218)	5
[Community] Zhi Chen -> Committer (#3572)Let's welcome Zhi as a new Apache TVM Committer!	1
[ETHOSN] Update compilation defaults to Ethos(TM)-N78 (#9563)The default compilation target is now changed to target Arm(R) Ethos(TM)-N78 NPU.	1
[Object] Add String container (#4628)	1
Do not use ICHECK in nnvm (#7255)	1
Change tir::GetPointerType to return std::optional<DataType> (#12458)It was returning a std::pair<bool, DataType> to emulate the behaviorof std::optional.	5
Fix test-test-set (#2384)	3
[Relay, TOPI] Refactor strided_slice and add axes argument (#8165)* Initial importcommit 667011f10320918e4dcd47ac2b57fe49849e5440Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 16:28:57 2021 +0900    Squashed commit of the following:    commit 95242d86ea5de96925430c0a74b6e91e299fb5ab    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu May 27 15:45:19 2021 +0900        Add function attribute for shape func for profilingcommit b8ede24ff987eb152bde7cc15afce004a88aeb5fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 10:21:06 2021 +0900    layout transform support completecommit 5782b7070288eb0de122f5dab91b38c26166a7d7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 08:31:11 2021 +0900    support layout transform part1commit e94aa6b2a916607234c89eddcd07afdfa8085786Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 19:47:46 2021 +0900    moved utilities to its own filecommit 8bf88913b9bc02730120a0695138ed3fb8ed49aeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:39:50 2021 +0900    fix formatcommit e89d599d6a10021167feb241483693260aa535f2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:33:02 2021 +0900    ToVec -> ConvertToVeccommit 001982ce1419504f1f0e1d116d57dd34f0180008Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:26:56 2021 +0900    formatcommit fae57f9bd67b29880a7552b5149d03120924cdacAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:24:35 2021 +0900    use Any for relay type rel pathcommit 053eee2e6f58749af0b68cca52fd530afc0f6454Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:14:44 2021 +0900    fixcommit fbb099c8e66caf846c773e180c66a2b336bd64a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:39:37 2021 +0900    refactor type relcommit ecfe3cd43e3968375505d5393959ec19da4b5c01Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:23:47 2021 +0900    workingcommit b357c2f8825603d8ba9ee2424a7f572e12c29852Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:07:07 2021 +0900    refactoring output shape calccommit f69ef407cf003c7977a4564185949d3f6b5c0219Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 14:23:36 2021 +0900    bug fix end param initcommit a5611c9a1f243f4b9a56539e7e8a15661374920cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:42:31 2021 +0900    fix test shapecommit e79931a264f0d8ed63a333ec4ab10a72cff22a84Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:42:03 2021 +0900    dyn slice tests left as todo now workcommit 7db4cea31378eed85dfae1cb03fb5a97394f7fe3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:36:30 2021 +0900    remove dynamic input specific opcommit 510bce6a181604e5eb3f2bd1951ae035a4090700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 12:52:30 2021 +0900    refactoring dynamic slicecommit 1b3969ade9ee98651b8157ecab1c675410a84ee5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 09:06:46 2021 +0900    fix slice axes dispatchcommit 9a795606fb71ec08cefe5bfa904f1ab32c18da4bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 08:32:54 2021 +0900    refactor computecommit 80442f86bbf9f0582823d5903021e3bae61a4662Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 08:11:18 2021 +0900    fixed output shape, refactored version workingcommit d2538ae980a1c731b646467c615d742efeb65e25Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 07:56:05 2021 +0900    restore another slice variantcommit 36aa777eacd8426a850d08b528e9addcd36a4894Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 06:41:50 2021 +0900    refactoring slice with axescommit 32698b74df211829777e5493e82bf7425364acb4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 13:11:01 2021 +0900    fix axes null checkcommit 54fb723d23d351551b75d879198aafb1eac2dedeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 12:52:18 2021 +0900    Revert "[Bugfix][Vulkan] Call VulkanDeviceAPI destructor on program exit (#7997)"    This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.commit 37eaf579d47190bc42ad64f9ac34c93a9dac3ce5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 04:30:37 2021 +0900    remove wip layout transform support for slice with axescommit 9bcb2ada60fadadd1f29a6d09e6b4fc5104efd3fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 18:01:59 2021 +0900    fix pylintcommit 7063a09ef1b98849e98194e8a9e47455cd1b5fa3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:57:03 2021 +0900    minor fixcommit 96c9231b5b2cbf2f36b4096d54f1f5ac4033d361Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:54:16 2021 +0900    support dynamic scatter ndcommit d4a4db8a8b518b1ef9e6abacfa23a9e1b76fd1b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:33:19 2021 +0900    gather_dim -> num_indices_per_tuplecommit a489375f0b31948a13e41f5967960305453c7049Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:23:46 2021 +0900    add dynamic gather_nd testcommit 533854a006c16359842451b8690cb8639b47635dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:18:26 2021 +0900    refactor gather_nd ref funcscommit 36a4501a151070760559f6ce4cfa574202b4d0c8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 14:36:34 2021 +0900    add gather_nd shape funccommit 1853c35d883e501e484d5f74adb3081f916761d5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 04:20:39 2021 +0900    add eyelike supportcommit 150e945290cbc595bd370dcae7e96e24597fbf04Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 04:08:37 2021 +0900    migrating inlined topi compute to topi/transform.hcommit 763ac37f725c2cb89a3221621b69da0e6ac39ed8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 03:45:37 2021 +0900    strided slice with axes support* fix bad merge* fix cpplint* fix pylint* more cpplint fix* fix compiler warning* add doc* add tests* typo fixed* support axes argument in topi cpp strided slice* Properly test axes argument in relay tests* fix bad merge (revert vm change)* fix tests	3
[ci] Add more shards (#11402)This adds a bunch more CPU shards and moves everything to CPU-SMALL.Some Java limitations required splitting up the logic in the templates abit as well.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Port StorageInfo and StaticMemoryPlan data structure (#8297)	5
Remove dmlc headers, add optional DMLC_CORE_PATH variable for build path (#102)* Remove dmlc headers, add optional DMLC_CORE_PATH variable for build path* travis.yml doesn't need to clone dmlc-core anymore	5
[VTA][TOPI] Conv2d transpose (deconvolution) operator support (#3777)* initial conv2d_transpose* correct select operator* cleanup* fix* fix correcness check* conv2d transpose declaration fix* autotvm conv2d_transpose tuning script* ir pass fix* fix tuning script* deriving params from env, adding bias* removing bias comp from deconvolution* lint* fix* lint* lint* turning off cpu* lint, ops* lint* import fix* removing hard coded values* lint	4
[tvm][any] broadcast with values other than one (#3967)* [tvm][any] broadcast with values other than 1* Add test for incompatible runtime values* Remove hybrid script compact buffer binding* retrigger ci	4
[RELAY][OP] expand_dims (#1819)	5
ROCm: use GcnArch for mcpu and ApiVersion to select code object version (#6447)	1
Enable USE_CUSTOM_LOGGING for Hexagon Launcher (#11185)* Enable USE_CUSTOM_LOGGING for Hexagon Launcher* Fix clang-format error* Remove "ERROR:" from LOG messages	2
[OP] enable binary op	0
[FRONTEND] Fix mxnet multi outputs (#271)* fix mxnet multi outputs* add test case for multi outputs* fix* fix* fix* fix index* use json hack* fix test cases* fix test cases* fix test cases* fix test cases* fix test cases* fix test cases	3
[ATTR/SYMBOL] Expose op_name attr to python (#132)* [ATTR/SYMBOL] Expose op_name attr to python* fix xcode	0
[MODULE/DSO] Support pack everything into one shared library. (#133)* [MODULE/DSO] Support pack everything into one shared library.* fix osx load	0
Add Onnx Pad v11 (#5539)	1
Simplify full broadcast (#7423)* convert argwhere(full(const)) to reshape(arange())* Add IsWildcard syntatic sugar* add a simplify expression to fold full into broadcast ops* Allow constant folding of full-like ops after SimplifyExpr* fix a bug with the Attr Pattern matching* remove skip_list	4
[Torch, Quantization] Necessary workaround to prepare for 1.6 update (#6602)* add support for 1.6 quantized models* fix lint* move version check function to a common utils* fix lintCo-authored-by: masa <masa@pop-os.localdomain>	0
[Frontend][Paddle] Fix pool2d op (#11029)* fix pool2d op* [frontend][Paddle] Fix pool2d Op* reformat files	2
[microTVM] Increase host memory size (#7933)	1
[Relay] Strict mode in pattern matching (#3620)* add fatallintlintlintdomake completeness check an errorlintremove fatal* fix test* reset parser file* remove unneeded import* Update python/tvm/relay/adt.pyCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update include/tvm/relay/adt.hCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Eliminate trailing whitespace (my fault)	5
[Hotfix][MetaSchedule] Importing from test foldeer (#11695)A concurrent merge breaks the unittest which imports directly from`meta_schedule.testing`.	3
[ARITH] Add Lowering rule for FloorDiv/Mod (#3976)* [ARITH] Add Lowering rule for FloorDiv/Mod* add comment about constant folding	1
Document missing qnn operators (#10077)The following qnn operators were missing from the relay documentation.	2
[Relay][Docs] Fix local variable code example. (#2435)	0
[Logging] Bring back the stack size optimization (#7756)	2
[Hexagon] Fix cmake files for Hexagon launcher (#9343)* [Hexagon] Fix cmake files for Hexagon launcherThe update to build the launcher automatically accidentally brokebuilding it separately. This patch fixes that.Also included are a few minor fixes and an update to the README.md.* Clarify support for Hexagon codegen	1
[ConvertLayout] slice_like support (#7184)	1
[RUNTIME] FastRPC interface for Hexagon runtime (#5353)* [RUNTIME] FastRPC interface for Hexagon runtimeCo-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>* Explain store offset in a comment in launcherCo-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>	1
Remove seemingly invalid SoftPlus (#7189)- `Softplus` is added in 12/10/2020 from this https://github.com/apache/tvm/pull/7089- However, I see that there were `SoftPlus` (not the P is in capital) was already in.According to [Onnx spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md), it is `Softplus` not `SoftPlus`.	2
[TFLite] Reshape - support different qnn params for input and output (#7159)	2
[hexagon][testing] add max_pool2d benchmark (#11720)- Add benchmarking framework for Hexagon maxpool-2d kernels,  and one (simple) kernel.	1
Update auto_tuning_with_python.py (#8158)	5
[TEAM] were -> Reviewer (#1705)	5
[Runtime] [ThreadPool] Make SpscTaskQueue::Pop(..) spin_count configurable (#3577)In cases where we have multiple models or threadpools active, spinning around`sched_yield()` may not be desirable, as it prevents the OS from effectivelyscheduling other threads.Thus, allow users to conditionally disable this behaviour (via an environmentvariable `TVM_THREAD_POOL_SPIN_COUNT`, similar to existing environment flags forthe thread pool such as `TVM_BIND_THREADS`, etc).This substantially improves tail latencies in some of our multi-tenantworkloads in practice.Unit tests have been added - on my laptop, running:```TVM_THREAD_POOL_SPIN_COUNT=0 ./build/threading_backend_test;TVM_THREAD_POOL_SPIN_COUNT=1 ./build/threading_backend_test;./build/threading_backend_test;```gives https://gist.github.com/ajtulloch/1805ca6cbaa27f5d442d23f9d0021ce6 (i.e.97ms -> <1ms after this change)	4
[FIX] Fix target warning (#560)* [FIX] Fix target warning* [FIX] Deduplicate options* Fix* Fix	0
[TIR] Fix pragma_loop_partition_hint attrs should check it's value (#12699)Current LoopPartition doesn't check the value of attribute key "pragma_loop_partition_hint". Whatever I set pragma_loop_partition_hint to True or False, the result is same, which is confused for debug.This PR fix pragma_loop_partition_hint attribute key should check it's value.	0
[CI][AArch64] Enable ONNX installation in ci_arm image (#12438)This patch enables ONNX and dependencies installation on ci_arm asa way to enable ONNX and Torch testing.Change-Id: I818db28dea2a3d4ae66e775aa15f7ed2f059d673	5
[ci][docker] Add retries for docker pull (#12306)	2
Enable IRFunctor based IRMutator	0
[REFACTOR] Move support related code to include/tvm/support (#4716)* [REFACTOR] Move support related code to include/tvm/support- tvm/logging.h -> tvm/support/logging.h- remove tvm/base.h, move with into tvm/support/with.h* src/common -> src/support	1
[AUTOTVM] Fix GATuner and improve error message (#1605)	0
[topi] fix strategy for sparse dense cuda (#5782)	0
1) Make unroll code reusable 2) reduce non-determinisim in CanonicalSimplify (#701)* 1) Refactored some parts of the unrolling code into their own methods so we can reuse unrolling functionality in other parts of the code. E.g., to explicitly unroll loops with count of 1 when they are programmatically created.2) Reorder based on top operator before resorting to pointers, which causes non-determinism.* Fixed lint errors	0
[BUILD] Include rocm cross compile env (#367)	5
[NNVM][DARKNET]Logistic activation added (#1477)	1
[Meta Schedule] Refactor meta schedule testing utils (#10648)This PR moves some utility testing classes into `meta_schedule/testing/utils` and updated the following tests involved:- test_meta_schedule_integration.py- test_meta_schedule_measure_callback.py- test_meta_schedule_search_strategy.py- test_meta_schedule_task_scheduler.py	3
Mark test_op_int8 as flaky (#10215)As per the docs [here](https://github.com/apache/tvm/blob/main/docs/contribute/ci.rst#handling-flaky-failures) this PR marks `test_op_int8` as a flaky test so it will run but failures won't be reflected in the Jenkins jobs. Once #10213 is addressed this PR can be reverted.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to "* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build	1
[TUTORIAL] gemm tutorial image add! (#276)* image add!* image path move to web-data	5
[TIR] Fix VerifyGPUCode for vectorized halfx8 store (#9420)	0
[Target] Fix empty target and host for autotvm task (#7791)	1
[Relay] Add list update to prelude (#2866)	5
[Relay, OpFusion] Better tuple fusion implementation  (#3092)	1
[microNPU] Add support for TFLite FULLY_CONNECTED (#10345)* [microNPU] Add support for TFLite FULLY_CONNECTEDThis is primarily a legalization to an NPU Conv2d operator. Thelegalization target is Conv2d with 1 1 I O (HWIO)* [microNPU] Add support for TFLite FULLY_CONNECTEDTest TVM runtime against TFLite for codegen and operator legalization.* [microNPU] Add support for TFLite FULLY_CONNECTEDFix linting* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, update codegen test, fix linting.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress more comments, ensure qnn.dense is lowered to NPU, fix linting* [microNPU] Add support for TFLite FULLY_CONNECTEDFix linting, update legalization test and codegen test for completeness.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, fix linting. Certain legalization test assertions wereupdated.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>* [microNPU] Add support for TFLite FULLY_CONNECTEDFix assertion in legalization test.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, fixing assertion on ifm and ofm shape.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>	3
[Relay] Fix PE (#3482)	0
Simplify enclave lifecycle management (#1013)	5
Fix bulleted lists in TVM documentation. (#8268)* These currently do not render due to https://github.com/readthedocs/sphinx_rtd_theme/issues/1115 * Breakage was likely caused due to https://github.com/apache/tvm/issues/7995	0
Refactor the compile engine into a cleaner interface. (#7518)Duplicate the CompileEngine interface.Refactor the graph_runtime_codegen to invoke the new LowerTE passMore changesThings appear to be workingSome tracing to get Relay code to flow through too.Disable some assertions as exp.Tweak printing for nowFix a few bugs: (#13)1. Don't add relay main function to list of lowered TIR functions2. Don't skip visiting call to relay function in graph runtime codegenRemove debug prints.Start refactoringSplit out shared data structuresFix implicit duplicate decl of IsDynamicClean up handling of name + global prim fnClean up the code and debug issue introduced by previous hackClean up the debuggingDo C++ lint clean upUpdate src/relay/backend/graph_executor_codegen.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Clean up handling of external functionsAdd more error messagesMore clean upUpdate src/runtime/graph_executor/graph_executor.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Update src/runtime/graph_executor/graph_executor.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Update src/relay/backend/te_compiler.hCo-authored-by: Haichen Shen <shenhaichen@gmail.com>Update src/relay/backend/te_compiler.hCo-authored-by: Haichen Shen <shenhaichen@gmail.com>FixCRMore CRFormatFix lowering path for C++Fix testsRemove uncessary changeClean up a few more thingsCI fixFix the default contextFixFix broken test casesUpdateFixWIPClean up storage data structuresWIPWIPFix build errorsRemove TVMLowerFix lintLint againfix blackMove UpdateMainWorkspaceSize into te_compiler.ccFix link errorsFormattingChange UpdateMainWorkspaceSize to return Map<String, FunctionInfo>Workaround for GCC 5 error caused by enums in maps (GCC 5 is on i386 CI)Testing how functions should be namedLintChange how function metadata is updatedAttempt to update aot_executor_codegen to use new StaticMemoryPlan instead of storage_device_mapPass memory plan through LowerTE into UpdateMainWorkspaceSize so that we don't need to run GraphPlanMemory an extra timeFix return in UpdateMainWorkspaceSizeLintTry to fix UpdateMainWorkspaceSizeFix construction of static memory planClean up code while debuggingAdding UpdateWorkspaceSize backAdd closure + call to UpdateFunctionMetadata (WIP)UpdateFunctionMetadata builds; weird error with device ctx map though. Not sure if it came from this change or something elseAdd some debugging of UpdateMainWorkspaceSizeStarting to move UpdateFunctionMetadata call to use process_fn infraUWhat target should be passed to UpdateFunctionMetadata?UpdateFunctionMetadata is not workingggAdded some comments about UpdateFunctionMetadata for JaredFix the creation of function metadataTry another stab at cleaning up the informationFixPort StorageInfo and StaticMemoryPlan data structure (#8297)Restoring reshape optFix testsCaught a nasty typo from Lily, Map::Set does not mutateFormatDisable stupid Google style warningRebase cleanupFormattingAdd docstring for storage infoBlackPost rebase fixRemove printsDisable assert that doesn't make sense for nowFix lintAdd copying attrs from relay node to graph node; still need to figure out how to do this in the case of global varsWork with Lily to fix graph attrsTry to figure out where extra arguments are coming from; fix mergepasses the profiling testClean upFix profile testRemove debuggingAdd attributes for BYOC uTVM caseFormatDumb typoAnother fix for byocFormatFix last 3 failing testsFormatFix final two test casesFormatFix lintFix againFixFix auto scheduler codeFix issueAddress CR commentFormatCo-authored-by: Jared Roesch <roeschinc@gmail.com>	0
logsoftmax reusing the softmax function (#11141)Co-authored-by: caizihua <978497756@qq.com>	1
[COMPILER] GraphHash based cache system, allow dump and query duplicated functions. (#30)	1
[TVMC] Support dot inside of TVMC input shape name arguments (#9294)* [TVMC] Support dot inside of TVMC input shape name arguments* dot -> dots	1
[CUTLASS] Add parallel split-k support to wgrad (#10185)* [CUTLASS] Add split-k support to wgradcommit 60b73a91b79d644d8c95f682eedaf47a89abba0dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Feb 8 10:43:11 2022 +0900    pylintcommit ae2e7187256316c48c915c3c187feb5cd4d4dbd4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:51:52 2022 +0900    Add split-k support for wgrad    commit 43820d50055b0bd17b736f5c5830321c7509a20a    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sun Feb 6 10:07:34 2022 +0900        fix and add doc    commit 446a95b0aabc5ab69cdd2e414b812aab1c557f42    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sun Feb 6 09:48:38 2022 +0900        dw conv2d properly supported for wgrad    commit adc4e22d2e03a99f30ebb6a5e956a1749de693f0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:32:42 2022 +0900        fix overwriting template    commit 040eab000bc5f162c6e9aca70ae6d29378fe65bc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:06:27 2022 +0900        black    commit e5a07c24b7463552b8e545710d25472159bcc127    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:03:10 2022 +0900        add reduction in profiler    commit be89334ab981d536d010dd765c9cf601dbdae5e0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 06:58:03 2022 +0900        adding split k reduction to conv2d profiler    commit ae09b0fbdc3a472eb320d866c054f73b3142f21c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 11:52:59 2022 +0900        fixed conv2d_backward_weight typerel for dw conv2d        commit 16fe5313fd1219e2e7d531ef9b36f64bb557e5e7        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Feb 3 12:59:22 2022 +0900            wip        commit 2167c2543340a285bb1985e8fe37e11aed51fb9b        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Feb 3 04:22:19 2022 +0900            fix conv2d type rel for depth wise and grouped conv2d    commit 14b12e5dd84fc34691d585213387198f091eefc5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 05:01:03 2022 +0900        remove split_k.py    commit b14127179c43f71c3ce5ccc7b4ca678a099e5497    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 04:48:21 2022 +0900        workaround for invalid split_k_slice    commit 6e4c7e1d77d89f124abc77dbcdab69eff8a5d961    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 02:43:58 2022 +0900        support split k in profiler    commit 2eb1cf43c7f56f0537cf249855054b5cbd357b13    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 02:31:03 2022 +0900        improvement    commit 0bce8f3778a6bb05607232a0997d25681e55ce7c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 18:20:12 2022 +0900        fixed for fp16 output    commit 30df1bd5282a4d326856382726d4e63ee8c27e8e    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 17:50:33 2022 +0900        fp32 output works    commit 7a519956b8d103464dff83b4f01b75973f4a33b0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 14:30:22 2022 +0900        fix    commit 4a383e2c7c37148a563e9cf34968fb7da3aaf91f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 14:05:24 2022 +0900        update c++ codegen    commit 6206e388cc7062cbef0b3c8c47fcd228b44b6818    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 13:46:05 2022 +0900        wip    commit 0ece49b53e773ebc1ea71c7667abc0cbb29d91bf    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 03:05:21 2022 +0900        wip    commit 08a6147940d9911fd65a890a4d90beb68176fc03    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 13:10:21 2022 +0900        test worked with fp32 output    commit 084d5c47666df92ba6c2c1445d5a23de0193a119    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 12:35:18 2022 +0900        fix compile error for fprop    commit 31f25436c5aca1a75336fa1a8d1c8a25a4936ee8    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 12:18:06 2022 +0900        compiled    commit c2098e79ade47117f2c32132da864b1fa73fce4a    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 11:11:43 2022 +0900        wipcommit a14585020151d0e09bb9bac549285dceb13e55e1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:46:16 2022 +0900    fixed for sm75commit 61515062ef4576bf5b4e7e9e800f7f705738809cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:32:46 2022 +0900    all tests workcommit 041c094b3646e0f521f5bd2c4f6f6b5b1cff7b97Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:19:09 2022 +0900    dw conv2d properly supported for wgradcommit 2191918743a4e9ffb8254f3786d817be57ff49ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 09:14:05 2022 +0900    wgrad tests now work under pytestcommit 78f76df1eb1602f66cacb888a97b6b267f8600a7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 07:31:54 2022 +0900    run blackcommit 0a82149fe0586b0bf449fc7f3a1fa9809e9b38d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 06:12:39 2022 +0900    [CUTLASS] Add wgrad support (without split-k)* pylint* add more doc* more doc clarification	2
Squeeze bug fix. (#506)	0
[VERSION] Update to 0.5.dev (#1623)* [VERSION] Update to 0.5.dev* Update the docs to include all intrins	2
[TensorIR][M1b] Schedule class (#7847)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Jared Roesch <roeschinc@gmail.com>	1
[DOCS] improve document of symbol; (#456)	2
[ARITH] iter_affine_map bug fix, stride generalize (#6753)	0
[DARKNET]RNN Support for darknet (#1443)	1
[Relay][Parser] Improve Relay parser and pretty printing, including CMAKE (#2377)	1
[TVM][AutoTVM] cast filepath arguments to string (#3968)	2
[BUILD][LLVM] Support LLVM mainline 5.0 6.0 (#356)* [BUILD][LLVM] Support LLVM mainline 5.0 6.0* Reduce parallelism	1
Add `is_floating_point` and `div_` PyTorch ops (#7128)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* Revert changes to tests* reintroduce newline* Fix style	0
[CUTLASS] Refactor cutlass kernel generation and selection (#9800)	4
[Ansor][AutoTVM v2.0] Part 1: Rename namspace form auto_schedule to auto_scheduler (#6059)* Rename namespace auto_schedule to auto_scheduler* Update* Lint fix	0
Fix device name and Issue #874 (#1096)	0
[TVMC] Runner.py Updates (#7779)* change runner to ms instead of s, consider reformatting* adjust formatting and test in test_runner.py to be more realistic* change device in run_module runner.py to be mandatory* make hostname optional in run_module, in runner.py* update order and doc string* remove print statement* black files* device error lint* argument order was incorrect* arguments funkiness attempt fix 2* Fix merge with main.Co-authored-by: Jocelyn <jocelyn@pop-os.localdomain>Co-authored-by: Josh Fromm <jwfromm@uw.edu>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>	5
Add MicroTVM tutorial using the STM32F746 discovery board (#5655)* Add MicroTVM tutorial using the STM32F746 discovery boardwith a sample tflite modelSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fix: add a reference to the new turtorials/micro directorySigned-off-by: Tom Gall <tom.gall@linaro.org>* fix: Cosmetic, align Micro TVM text with dividerSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fixes to remove warnings, spaces for readability, code blocksSigned-off-by: Tom Gall <tom.gall@linaro.org>* remove use of dload in favor of requests for obtaining the TFLite modelSigned-off-by: Tom Gall <tom.gall@linaro.org>* add setup for CMSIS_ST_PATHcomment out portion of tutorial that will not run without a physical board availableSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fix warning due to ** in python but part of a comment blockThe block is commented out since it can only run on deviceSigned-off-by: Tom Gall <tom.gall@linaro.org>* Numerous reworks to address feedback.Within docs/conf.py place the microTVM tutorial prior to the VTA tutorialsWithin the micro_tflite  - rework section headers  - reorder code so model prep code is all in one place as well as code    for running on device  - address indentation feedback  - remove '' '' usage which I mistakenly thought was getting around a    sphinx issue involving **Signed-off-by: Tom Gall <tom.gall@linaro.org>* Change disable_vectorize to use current approach with tvm.transform.PassContextChange to pull example model from github with download_testdataAdd 2.5K tflite modelCouple of small changes following https://sphinx-gallery.github.io/stable/syntax.htmlSigned-off-by: Tom Gall <tom.gall@linaro.org>* remove use of relay.build_config in favor of PassContextSigned-off-by: Tom Gall <tom.gall@linaro.org>* Couple of minor 4 space fix upsSigned-off-by: Tom Gall <tom.gall@linaro.org>* Change to use tvm.transform.PassContext for disable_victorize and disabling FuseOpsSigned-off-by: Tom Gall <tom.gall@linaro.org>* Remove binary module from repoChange download_testdata back to pull model from linaro serverSigned-off-by: Tom Gall <tom.gall@linaro.org>* Couple of small cosmetic changes. (spaces and extra lines)Signed-off-by: Tom Gall <tom.gall@linaro.org>* Convert link to tf docs to examine a tf lite model to use RST syntaxSigned-off-by: Tom Gall <tom.gall@linaro.org>	1
[Relay] Fix Type Arguments not Attached (#6385)	0
[ci] Add --docker-image option to ci.py (#11118)This allows users to specify what Docker image they want to use manually (i.e. for debugging if a user has built their own image)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CODEGEN] Add LoweredFunc, MakeAPI to build a C API function (#23)* [CODEGEN] Add LoweredFunc, MakeAPI and SplitHostDevice* update halideir	5
Pin pylint version 2.2.2 (#2698)	5
[PYTORCH]celu, gelu, selu activations (#5263)	5
Remove nnvm (#4565)	4
Update hybrid_script.rst (#3799)	5
typo (#6352)	2
[Meta Schedule][M3a] TaskScheduler (#9154)* Add docs.* Add TaskScheduler.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI after hotfix.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
Add optimizer (#334)	1
fix group conv3d pack kernel shape error (#12523)	0
Add USE_ETHOSU for the config.cmake (#9162)This commit adds the cmake variable to enable/disablebuilding with Arm(R) Ethos(TM)-U NPU codegen support.Change-Id: I891a1cb0619dd24d749353fe7b156c45356d453f	4
[VM] Avoid round-trip Target->str->Target conversions (#8161)Currently, in some cases this round-trip cannot be completed.  Forexample, if an Integer value has a value outside a 32-bit signedinteger range, or if a String value contains spaces.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
tir printing (#10805)	5
[Relay] First order reverse mode Automatic Differentiation (#2321)* initstaging onfinal save before stagingsaveinit verinit verll stuffsaveadd failing testadd filepass testfix errorhuh?saveAdd test changesFix fusion with nested tuplesFix reverse mode testMore hackingClean up ADHacking on reverse modeFix issue in reversesavefix lintfix lintfix lintsavesavefixsupport negaddress some commentadd back filesavesave* save* save* save* lint* fix lint* save* fix	0
[RPC] Bugfix. Removed server forcing IPv4 protocol (#7953)Removed forcing IPv4 protocol from python RPC server implementationto be in correspondence with the RPC client implementation which isused `platform default`. This had led to situation when "localhost"was translated as 127.0.0.1 for the server (IPv4 protocol was used),but the client translated it as "::1" and was trying to connect toserver using IPv6 protocol and was getting "ECONNREFUSED 111 Connection refused".Change-Id: I44802eb1ea78f3b36ac664f0be7237e62084c234	4
Fix PRelu layout in Relay (#3013)* Fix PRelu layout in Relay* Fix cpplint* Add PRelu test case	3
[PASS] Remap thread axis. (#1122)	4
fold const or empty iter partition (#12080)	5
Optimize the implmentation of scale (#10884)	5
[CI] Update to LLVM 14.0.0 for ci_hexagon (#11539)	5
[ARITH] Add CombineInterval<Div> in IntSet (#48)* [FIX] add CombineInterval<Div>* fix error message and add comment about rounding* fix comment	0
improve infer shape/type error message (#4)* improve infer shape/type error message* fix dense infer shape	5
Move to new style issue template system (#8898)* Move to new style issue template systemThis lets us have a template for each type of issue, notably this includes a template for requesting a CI image update.* Fix checkboxes* Codify the use of Discourse rather than raising issues* Change CI to CI Image and introduce CI Issue template* Fix poor english* Add more tags where we have them* CI Issue -> CI Problem	0
[CI,DOCKER] Bump gpu image to cuda 11.0.3 (#8119)	2
update the type of return value (#9603)	5
Add docker/lint.sh, for running dockerized lint scripts locally (#6333)* Add -i option to docker/bash.sh * Allows scripts to invoke dockerized commands interactively, for   better Ctrl+C.* Add docker/lint.sh to run lint step locally in the docker VM. * This allows developers to run lint using the official versions of   the lint tools without needing to lookup the docker image name. * Move all lint scripts to tests/lint/ * Point Makefile to those new scripts. * Update apache rat script to filter untracked/gitignore'd files when   run with `docker/lint.sh`.* fix bash_source[0]* explicitly set the author for CI* try environment variable override* try config option* remove =traditional from ignored option to increase git compat* address comments, fix behavior under git worktrees* address cppdocs comments* address lint.sh comments* address zhi comments, update pull_request rst	5
[Hexagon] Use single allocation to back 2-d arrays (#10903)* [Hexagon] Use single allocation to back 2-d arraysCurrently, each allocation allocates an entire page, so even arelatively small number of allocations can use very large amounts ofVTCM.  This commit changes calls to `AllocVtcmWorkspace` of shape`[N,M]` from performing `N` allocations of size `M`, to 1 allocationof size `N*M`.  Since `N` is usually much smaller than a page, thisreduces the total amount of memory required.This is an intermediate step, where the long-term solution is to usestatic planning for VTCM allocations.  This returns the same `void**`type as the static planning eventually will, but avoids excess memoryuse in the meantime.* [Hexagon] Maintain alignment of allocationsPreviously, when a single monolithic allocation is used to back a 2-dHexagon buffer of shape `[nallocs, nbytes_per_allocation]`, theallocation itself is aligned, but each individual region is not.  Thiscommit ensures that each individual region also followed the alignmentspecified.	1
[REFACTOR][TYPE] Remove un-necessary var sub-field in GlobalTypeVar and TypeVar (#4615)Currently, we use a tvm::Var to represent a placeholder for shapes in generic types.This is not necessary for GlobalTypeVar(as we never parameterize by shape var),and is a bit twisted for TypeVar.As we move to a unified type system, we want to break the dependencyfrom the base TypeVar(which is shared across the languages) from the expression.Note that it is fine for TensorType to depend on Expr.One alternative solution to embed the Var would be to introduce a TypeVarExpr,which can wrap a TypeVar as Expr. However, this new alternative won't benatural until we migrate the type to the global scope.Lucikly, we have not yet start to depend on the shape parameterization heavily yet.This PR removes the tvm::Var from the typevars. We will follow up with anotherPR to migrate the types to a base location. After that, we should be able touse the more elegant approach via TypeVarExpr.	1
[FRONTEND][TENSORFLOW] GPU support for tensorflow models. (#1718)	1
Add test case: Create a static WebGL library and run it in the browser. (#932)* Add test case: Create a static WebGL library and run it in the browser.* Add documentation for loadModuleFromFile* Modify emscripten.createjs	1
[TEAM] merrymercy->code owner (#1581)	5
[FQ2I] Add mean op to FQ2I (#10607)* add mean op* clean up* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
[Autoscheduler][VM] Autoscheduler layout rewrite pass to VM (#7516)* fix type inference for conv2d* fix* adding the autoscheduler layout rewrite pass to VM compiler passes* revert edits applied in other PR* minor fix* fix* formatting fix* lint	0
[DOCS] Remove incubating from docs (#7525)	2
[Relay][TOPI] Gluncv SSD support on the GPU (#2784)* ssd gluoncv gpu op updated* ssd gluoncv gpu op updated* tutorials and testes modified* tutorials and testes modified* fix lint* fix lint* address comment* multibox bug fixed* space line added* use less threads per block* use less threads per block* less threads per block for get valid count* less threads per block for get valid count* merge with master* Revert "less threads per block for get valid count"This reverts commit 08896cfccc34b0b2a1646d01d01ea4cad73941c4.* Revert "less threads per block for get valid count"This reverts commit 08896cfccc34b0b2a1646d01d01ea4cad73941c4.* typo fixed* elem length made to a variable* fix lint error* fix lint error* lint fixed* bug fixed* bug fixed* lint fixed* error fixed* error fixed* test ci* test ci* seperate argsort to be an independent op* seperate argsort to be an independent op* fix lint* fix lint* remove unsupported models* typo fixed* argsort added to realy* solve conflicts with master* fix lint* fix lint* test push* Revert "test push"This reverts commit 6db00883fab6cc06bddf564c926bb27c874397d8.* fix lint error* fix more lint* cpu test_sort udpated* debug ci* nms fixed* expose argsort to relay frontend* test ci* fix lint* sort register error fixed* fix nnvm* nms type fixed* adaptive pooling added to relay* Revert "adaptive pooling added to relay"This reverts commit 1119f1f2c055753e0cc5611627597749134c5c8c.* fix lint* expose argsort op* fix lint* fix lint* fix lint* sort test updated* sort bug fixed* nnvm error fixed* fix argsort default data type returned to be float insteaf of int* fix lint* fix lint* test fixed* fix valid count* fix titanx bug* tutorial add both targets* titanx error fixed* try to fix CI old gpu error* try to solve CI GPU error* get_valid_count added* reverse get_valid_count* get valid count optimized* address comments* fix ci error* remove unessesary block sync* add back one sync* address comments* address more comments* more comments* move sort to be indepent algorithm* typo fixed* more typos* comments addressed* doc updated* fix pylint* address final comments* apache license added	1
[RUNTIME] Add libbacktrace for backtraces with line numbers (#7153)* [RUNTIME] Add libbacktrace for backtraces with line numbersCo-authored-by: Robert Kimball <bobkimball@gmail.com>	1
[RUNTIME][IR] Allow non-nullable ObjectRef, introduce Optional<T>. (#5314)* [RUNTIME] Allow non-nullable ObjectRef, introduce Optional<T>.We use ObjectRef and their sub-classes extensively throughout our codebase.Each of ObjectRef's sub-classes are nullable, which means they can hold nullptras their values.While in some places we need nullptr as an alternative value. The implicit supportfor nullptr in all ObjectRef creates additional burdens for the developerto explicitly check defined in many places of the codebase.Moreover, it is unclear from the API's intentional point of view whetherwe want a nullable object or not-null version(many cases we want the later).Borrowing existing wisdoms from languages like Rust. We propose tointroduce non-nullable ObjectRef, and Optional<T> container thatrepresents a nullable variant.To keep backward compatiblity, we will start by allowing most ObjectRef to be nullable.However, we should start to use Optional<T> as the type in places wherewe know nullable is a requirement. Gradually, we will move most of the ObjectRefto be non-nullable and use Optional<T> in the nullable cases.Such explicitness in typing can help reduce the potential problemsin our codebase overall.Changes in this PR:- Introduce _type_is_nullable attribute to ObjectRef- Introduce Optional<T>- Change String to be non-nullable.- Change the API of function->GetAttr to return Optional<T>* Address review comments* Upgrade all compiler flags to c++14* Update as per review comment	5
[Docs] Add Commit Message Guideline (#12689)This commit adds the Commit Message Guideline text to Apache TVMdocumentation in ./docs/contribute/pull_request.rst, under section'Submit a Pull Request', below subsection 'Guidelines', as a subsectionnamed “Commit Message Guideline”. The text in the second-last item insubsection 'Guidelines' that mentions PR tags is also updated to referto this guideline.This documentation will help guide contributors on how to write goodcommit messages when submitting code / creating Pull Requests, inaccordance with RFC-0088:https://github.com/apache/tvm-rfcs/blob/main/rfcs/0088-commit-message-guideline.md	1
[tutorial][benchmark] nnvm -> relay (#4368)* [tutorial] nnvm -> relay* use relay workload* delete movbilenetv2 option	4
[MetaSchedule] Integration test for CUDA AutoTensorization (#12142)* [MetaSchedule] Integration test for CUDA AutoTensorization* cleanup* fix	0
[Relay][Frontend] CoreML Support (#2476)* [Relay][Frontend] Add CoreML Support* pip install six in CI* remove triggering nnvm coreml test* set opt_level=2 for nnvm coreml test case	3
Updated documentation error (#1001)	0
Android RPC README improvements (#3500)- Fix APK path- Add ADB install/uninstall instructions	5
Rename GraphRuntime to GraphExecutor (#7653)	1
[RELAY][OP] Move computes to cxx, enable concat as injective (#2166)	0
fix typo (#12115)	2
[COMMUNITY] @anijain2305 -> Committer (#4921)	3
[Relay][Training] Add gradient for cast (#3894)savefixfix grad	0
use sizevar when convert any to tir (#8555)	1
[NNVM] Example NNVM integration. (#182)	5
[REFACTOR][TIR][API-Change] Migrate BuildConfig to PassContext. (#5668)* [REFACTOR][TIR] Migrate BuildConfig to PassContext.This PR migrates the TIR configurations from BuildConfig to thePassContext used by the unified IR.Moving forward, PassContext will be the unified way to configure passes in the TVM stack.Changes- Refactored TVM_PASS_REGISTER_CONFIG_OPTION to take in the reference type.- Removed BuildConfig.- Migrated the passes to use PassContext.* Update include/tvm/ir/attrs.hCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>	1
Update docs to clarify minimum Visual Studio version. (#10715)	5
[TVMC] Run module once by default (#12713)* [TVMC] Run module once by defaultCurrently executing `tvmc run module.tar` will run the input modeltwice. For benchmaking this is to be expected as the first run is usedto prime caches etc before taking a measurement. However, this seems abit unintuitive to have as default, especially when benchmarking is notalways intended. In this sense, this commit aims to amend thenumber of runs for the default: `tvmc run module.tar` to a single run.After inspection, this seems to be down to the use of the `.benchmark()`method which runs (1 + repeat * number) executions in total. This meansthat at least two runs are required (i.e. when repeat=1, number=1). Italso seems that it is only necessary to benchmark the model when`--print-time` has been set from the CLI POV. From the python interfacePOV, benchmarking is always run, but this may not always be necessary.This commit makes use of the `.run()` method to singularly execute themodel by default. From the CLI this will be used when `--print-time` isset to False whereas from the python interface this will be used when`benchmark=False`. Otherwise, the `.benchmark()` method will be usedas before. Complementary to this change `repeat`, `number` and`end_to_end` parameters are only used when either `--print-time` or`benchmark` are set to True - and the documentation has been updated toindicate this.Change-Id: I18a38a9d430d660264f7fce5caf0779aa059fed3* improve documentation with number of exectuions when benchmarkingChange-Id: Iecf557594420fcc9f3abcec5ce7d952db2c94271	5
Fix version check bug (#6784)* Fix version check bug* Update pytorch_utils.py* Update pytorch_utils.py* Update pytorch_utils.py* Update pytorch_utils.py	5
[TensorIR][Schedule] Inherit block anotation upon creating new blocks (#9573)	1
[TEST] Fix Some Failed Test Cases and Tutorials of The Issue #6453 (#6454)	0
Introduction tutorial formatting fixes (#9539)* Introduction tutorial formatting fixesThis fixes some rST issues I noticed while going through the getting started tutorial. Some of these shouldn't be too controversial like using ` instead of `` and consistency fixes. I noticed lots of the `.. note:`s have titles even though they don't really render as anything special in Sphinx, but the base `.. admonition:` does render the title in the top line. This looks nicer IMO and wastes less space but it could go either way, I didn't change it in all places yet either.* Convert the rest of the tutorial `.. note::` directives to `.. admonition::`* Fix compilation for matmul tutorial + some random formatting fixes* Fix ambiguous links and remove namespaceCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[FRONTEND][DARKNET] YOLO V3 model support (#1734)	1
[Executor][Bugfix] Properly return and unflatten outputs from GraphExecutor (#7604)* properly return and unflatten outputs from GraphExecutor* lint* cleaner approach, not sure what I was thinking before* remove unused import* forgot copyto cpu* make solution even cleaner using iterator	1
[FastMath] Add cuda & x86 schedules for fast_softmax (#8150)* Add cuda & x86 schedules for fast_softmax* Bug fix* Re-trigger CI	0
Add a quantized conv2 unit test for the tflite front-end (#5558)Signed-off-by: Giuseppe Rossini <giuseppe.rossini@arm.com>	5
[GraphRuntime] Support parameter out in the graph runtime debug (#4598)* [GraphRuntime] Support parameter out in the graph runtime debug* Dummy commit to trigger build	0
[CodeGen] Generate blob use LLVM directly (#4657)	1
[TVMScript] refactor (#6734)* [TVMScript] refactor* [TVMScript] pylint* [TVMScript] pylint	4
[OP] Initial checkin of nnvm core op folders	5
[Ansor][AutoTVM v2.0] Phase 1: Add RPC Runner (#6077)* Add rpc runner* Update* Update* Add clflush & non-empty ndarray TODO hints* Update* UT Update* Update timeout in UT	5
Add gluoncv installation (#2464)	1
[AUTOTVM] tweak `sample_int` implementation (#2677)* check in* lint* cleanup* Update util.py	5
[RUNTIME][TRACE] Support trace primitive. (#1973)Add trace call expr to allow trace Tensor dataat the runtime. By default the default handleris enabled which prints a tracing data to stdout,otherwise user should specify a call_back asglobal_function (aka @tvm.register_func).The issue is related to:https://discuss.tvm.ai/t/idea-trace-expression/945	0
image tranform runtime wrong on python (#354)x = np.transpose(img, (2, 0, 1))[np.newaxis, :]TypeError: an integer is required	1
Make cpptest build on Ubuntu (#2798)	3
Add scripts for automated build and testing (#10194)	3
print ast w/o metadata (#2533)	5
Android demo: Docker improvements (#3499)- Install OpenCL headers- Set ANDROID_HOME environment variable	1
[RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph (#5616)* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraphIf the annotated compiler region contains multiple outputs wheresome of the outputs are tuple output, the current PartitionGraph willcreate tuple of tuples. This will not be handled by the runtime.This commit flattens the such tuples and re-create them after thecall site of the partitioned function.Change-Id: I4e7ccbda73c129a9f4ae8705d5c9f2af6ab99ef6* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph    *code refactor : extracted the passes as a sequentialChange-Id: If4bc00b00a96fa244358d602fc1a361498342f46* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph   *further refactorChange-Id: I69ddd0e835e88ef97da8a3a3b949be3f7b619c02* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph    *class description comment amendedChange-Id: I55720bf0467c96e979e1ab56c40d9d209e0f9456	4
Fix memset of memory pool in PageMemoryManagerCreate (#12437)	1
[ATTR] More robust attr parsing (#33)	5
[Relay][Text Format] Fix Pretty Printing Annotations (#3041)	0
[DEV/IR] Python IRBuilder (#102)	5
[Runtime][PipelineExecutor] Getting the asynchronous output (#10723)This patch create a new GlobalRuntime to check whether the output dataready and poll global output of pipeline, it also removed the sequencepipeline execution logic as the asynchronous logic already done.	2
[Bugfix] Fix the issue that function pass modifies original module (#3712)* fix* fix interpreter	0
[Docker][Hexagon] Add docker file and scripts (#10263)* Hexagon docker files added* trigger	1
Add QEMU setup to uTVM tutorial. (#7296)	1
[microNPU] Cascader performance model bugfixes (#10510)* [microNPU] Performance model bugfixes* Fixed incorrect num_blocks calculations for both BufferModes.* Fixed similar issues with Read/Write byte calculations.* Fixed an issue where the 'partkernel' flag was not propagated to  the performance estimation code.* Fixed single buffering check incorrectly used output shape and  block rather than the input shape and block.* Fixed block config not aligned to micro block for Elementwise.Change-Id: Ide6b231bc1a17c65bed20129d2179a215ada14b2* Address review commentChanged incorrect usage of 'max_width' to 'max_depth'.	4
[Torch] Frontend update to support PyTorch 1.10 (#9664)* wip* fixed converting maskrcnn* fixed nll los* fixed linspace* fixed deformable conv2d* control flow and rnn test had no problem* swap more import orders* qmv3 test is having weird segfault* cleanup* black	4
[NNVM] Fix gradients for broadcast_div (#1512)	0
[TOPI][OP]change float multiplication of resize op to integer division (#12315)* [TOPI][OP]change float multiplication of resize op to integer division* add unittest.* 1. add docstring2. rename param	2
add Verilator to CI (#7098)	1
[RUNTIME][API] Graph runtime API enahncement to support NDArray (#1659)	1
[TOPI] Automated schedule in conv2d TOPI lib, moving to GEMM intrinsic (#35)* removing programming out of end to end example for now* updating TOPI library to use gemm tensor intrinsic* bug fix, autoschedule in TOPI conv lib* removing the deprecated GEVM intrinsic* refactoring, fixed lint test* fix for integer division bug* python3 bug fix for non matching types due to float division* comment	0
[TVMScript][TIR] Clarify scope of BlockNode::iter_vars (#12726)Previously, it was ambiguous whether `BlockNode::iter_vars` werein-scope for `BlockRealizeNode::predicate`.  `ConvertBlocksToOpaque`treated them as in-scope, and applied a mapping from `iter_vars` to`iter_values`.  Similarly, TVMScript printing places `T.where`statements below the `T.axis` statements, where `T.axis` definitionsare in scope.  However, `BlockRealizeNode::SEqualReduce` and`BlockRealizeNode::SHashReduce` do not visit the block and `iter_vars`until after visiting the predicate, placing the `iter_vars` out ofscope.This commit updates the printing of `T.where` to be above `T.axis`,and updates `ConvertBlocksToOpaque` to report an error if thepredicate contains references to `BlockNode::iter_vars`.  After thiscommit, these three usages all consistently treat`BlockNode::iter_vars` as out of scope for`BlockRealizeNode::predicate`.	0
[Relay][Pass] Only allow Module -> Module for opts managed by pass infra (#3430)* [Relay][Pass] Only allow Module -> Module for opts managed by pass infra* revert gradient pass	4
[TOPI] Fixed nms max_output_size loop (#4541)One of the loops in hybrid_nms used forperforming the max_output_size reorderingwas incorrectly designated as parallelresulting in incorrect behaviour. This patchchanges that loop to a serial loop.Change-Id: I97184f5887f5f028d8ab339fa2808eb7630a4017	4
preserve input option order (#1068)	5
fix expand onnx conversion (#11278)	0
[PROFILING] Use PAPI to collect hardware performance counters on CPU and CUDA (#7983)* [PROFILING] Use PAPI to collect hardware performance counters on CPU and CUDAThis PR adds an optional dependency on PAPI(https://bitbucket.org/icl/papi/) in order to collect hardwareperformance counters on CPU and CUDA. These performance counters includedata like total cycles, instructions executed, and cache misses. Userscan control which performance counters are collected by setting theTVM_PAPI_${DEVICE}_METRICS environment variable to a semicolon separatedlist of metrics.* Update CMakeLists.txtCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* move thread pool reset out of crt* add docs* comments* formatting* forgot one doc* kDLGPU -> kDLCUDA* Refactor API to more closely match pass instrument's.* forgot files* formatting* more lint* fix docs* optional loading of papi metric collector in python* more formatting* fix check* update docs and default value* formatting* addressing andrews comments* fix docs* address comments* move shared initialization code into private function* move most definitions from papi header to implementation fileCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>	2
[PYTORCH]Unary Ops (#5378)	5
[Relay] Modify create_executor to pass params (#8418)* Overload create_executor to accept params* [fix] Add stringdoc for new param in create_executor	1
cleanup: removed a piece of code that is redundant now given updates to HalideIR submodule (#3169)	5
Add support for QLinearMul ONNX op (#8773)* add qlinearmatmul* noop* mul not matmul* refactor some common qlinear op test code	3
[Docker] Turn on Rust docs and MxNet based ResNet (#6640)* Enable ResNet and Rust docs* Tweak* Format* Fix issue with overwriting	0
[PASS/SETUP] Fix minior issues (#663)* [PASS/SETUP] Fix minior issues* fix lint	0
Fix compilaton of bfloat16 on Windows (#4415)	0
[git] Ignore auto-generated micro_tvmc.py example. (#10729)This file is generated automatically by`tests/scripts/task_convert_scripts_to_python.sh`, added inhttps://github.com/apache/tvm/pull/10555, and can be generated whenrunning the `ci.py lint` script locally.	1
[Pass] Enable BackwardOp	0
Fix CUDA int8x4 vectorize (#3928)* Fix int8x4 vectorize* Fix gpu shared/local memory accumulate* Add test_shared_memory for int8x4* Adjust test format* Fix cpplint	0
[DOCS] How to deploy TVM Modules (#499)* [DOCS] How to deploy TVM Modules* More comments	2
[ci] Skip broken android_rpc failures (#12192)See #12191Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[JSON] attr->attrs in node attribute, keep backward read compatible (#152)	5
VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings	2
[FIX] Fix howto_deploy (#7841)We were missing files in tvm_runtime_pack.cc	1
Extend AttrPattern to support CallNode and FunctionNode attributes (#5637)* Extend AttrPattern to support CallNode and FunctionNode attributes* Update tutorial and add breaks* add func attr test	3
add parser support for GREATER tflite operator (#3963)add test for GREATER	3
[OpenCL] Implement conv2d_winograd algorithm for Adreno (#11543)* Implement conv2d_winograd algorithm for Adreno* Implement gtest for OpenCL texture pool* Implement conv2d_nhwc_winograd for Adreno* Minor refactoring* Fix lint* Apply comments* Apply comments* Fix lint	0
[Community] Add @Hzfengsy as TIR/Auto Codeowner (#8912)	1
[VM] Minor refactor for C++ memory alloc (#7413)* started moving things to header* directly call InvokeTVMOp* done all memory op* also refactor AllocTensor* declare Prod* remove cached func for Add, Multiply, Divide* lint fix* revert test change* remove tensor.h and declare Prod in pattern_utils.h	4
[ci] Add guards to pytest_wrapper (#11553)This should fix #11544 and adds some more logging in case the issue persists. Unfortunately it is difficult to test for real since the case data in that PR is thrown away after Jenkins is done (Jenkins does store test data but it marshals JUnits into its own format)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
fix things (#9146)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
Zhi's key for ASF release (#6554)	5
[Frontend][TFLite] Dynamically calculate input_stats of any fake_quant range (#4789)* [TFLite] Dynamically calculate input_stats of any fake_quant range* pass the input range to the convertor and caclulate (mean, scale) there* change the range of the second tensor in elemwise operations  so that we test inputs with different quant params* change the possible output range for elemwise ops wrt the updated ranges* update the comments for (m, s) calculations* add input range dict to reduce_mean op* Apply requested changes* add exception handling for zero division in input_stats* fix range of the input tensor in elemwsie	0
[TVM] const auto p -> const auto &p (#4861)	5
Add version 11.1 in finding CUDA libdevice (#7033)* Add CUDA 11.1 libdeviceMaybe we should have a >= check instead.I also added a fallback to detect the version if version.txt ismissing. Calling nvcc for this has been inspired by what PyTorchdoes when compiling extension modules.	5
[TVMScript] Report error if add attr to implicit root block (#9507)* fix implict root block attrs* lint	0
[SCHEDULE] Mutate dataflow in schedule, refactor Stage (#44)	4
rebase (#10525)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[AOT] Name mangling in AOT (#8014)* [AOT] Name mangling in AOTMini-RFC is here: https://discuss.tvm.apache.org/t/mini-rfc-name-mangling-in-aotWith this change we'll mangle the name of global symbols so that we can bundletogether multiple models in the same application.The relay.build interface has been left unchanged, which means I amresuing mod_name as a prefix for all functions. If mod_name is None thena "_tvm" prefix is used.I had to add two different compilation functions:- _CompileEngineLowerWithModuleName to mangle all the operators with the mod_name- PartitionGraphWithModName to mangle all the operators produced by BYOCI could have changed signature of both, but that would have meant a veryinvasive refactoring.I refactored the aot test utils and added some tests for multiplemodels.Change-Id: I30e93fa075f660054577ea36cf9268ec0c6eebcb* retrigger CIChange-Id: I4f11da7fce1327ad89bb25f25209b57077b2c6a3	4
[BYOC][TRT]Fix groups cannot divide output channel count error for deconv when groups>1 (#7595)* trt num_outputs* asdf* fix lintCo-authored-by: Leyuan Wang <leyuan.wang@bytedance.com>	0
Enhance upsample operator to adapt onnx opset version 9 (#2840)	1
[Relay][Quantization] Fix add_rewrite and UnifyDTypeScale (#3534)* [Relay][Quantization] Fix issue introduced in #3135* Recover StopFusion* Fix fmultiref* Fix lint	0
Address review comments on Arm(R) Ethos(TM)-U PR 3/6 (#9159)* Address review comments on Arm(R) Ethos(TM)-U PR 3/6Change-Id: I22961885a503be31f6a72622ae0b5f874cc6f463* Fix rebasing errorChange-Id: I3e2fde786096ea331fcb366080fa779ec4ea4a5d* Fix more rebasing problemsChange-Id: I1026e3ccee33a3fdec9ebbf6456bae244ad4f1d5	4
[RELAY,TOPI] Threefry PRNG: splittable and stateless (#7083)* [RELAY,TOPI] Threefry PRNG: splittable and stateless* Fix sphinx?* Lint fixes* sphinx fixes round 2* fix inputs for tests* reorganize to random, fix uninitialized memory bug* silence linter* silence linter even further* s* strengthen Threefry key type checking, add tests* replace static variable with function for Threefry key type* lint fix* Remove old todos, improve assert messages* describe how random number is generated* add tests for incorrect output size. also vary test sizesCo-authored-by: Altan Haan <ahaan@octoml.ai>	5
misc fixes for ROCm (pointer lifetime, runtime::String refactor) (#5431)	4
Update CONTRIBUTORS.mdmake name alphabetical	1
[Fixbug] Report duplicated param names of relay function when bind params (#9350)* [Fixbug] Report duplicated param names of relay function when bind params* add test* lint	3
[DOCS] add a solution to installation error because of the old submodule (#384)* [DOCS] add a solution to installation error* Fix the command for updating submodules	5
Docker env for Arm® Ethos™-U55 Port (#8514)* Docker env for Arm® Ethos™-U55 Port* Added Arm® Corstone™-300 Reference System for testing* Added Arm® Ethos™-U driver stack* Added installation of Arm® Vela.Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>Change-Id: Ie3cc43943c876d95618a39887aa666da20bcb1e4* Docker env for Arm® Ethos™-U55 Port* Removes /opt/arm/cmake/bin from the path* Parameterizes Arm® Ethos™-U55 driver stack version numberChange-Id: I2162b40f82241fd013643cbfa8847b60d7f4f5a1* Docker env for Arm® Ethos™-U55 Port* Adds ethosu as an extra to /python/gen_requirements.pyChange-Id: I2162b40f82241fd013643cbfa8847b60d7f4f5a1* Docker env for Arm® Ethos™-U55 Port* Added comment explaining why Vela version needs to be pinned to 2.1.1Change-Id: I1ade280faa5274cca78899f4dae9e596b16fb5df	4
This patch is to fix some minor typos in project. (#9852)	2
[DOC]Update doc in _api_internal.py and ir_pass.py (#2514)	4
[Autotvm] Support override (#3292)	1
[TIR] Expose WMMA-related TensorCore builtins (#12589)This PR exposes the following TIR operation in python:`tvm_load_matrix_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L711)`tvm_store_matrix_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L913)`tvm_mma_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L860)`tvm_bmma_sync`: add new unittest`tvm_fill_fragment`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L571)Co-authored-by: yongwww <yongcale@gmail.com>cc: @junrushao cc @Hzfengsy @junrushao1994Co-authored-by: yongwww <yongcale@gmail.com>	3
[Ansor][AutoTVM v2.0] Phase 1: Add cache_read/cache_write steps (#6107)* Add cache_read/cache_write step* Update* Update* Update* Update state->current_compute_dag to Optional* Update* Update doc* Update* Update* Doc update* Update	5
fix Android and OpenCL docker install (#4363)	2
[docs] Add a tutorial for the pass manager (#3515)* [docs] Add a tutorial for the pass manager* address comment* address more comments* retrigger ci* address steven's comments* address comments* retrigger ci* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>	1
Add a JSON converter for 0.7 -> 0.8 and 0.8 -> 0.9 (#9874)* Add default to serialization* revert changes in serialization.cc* update 0.6 converter* json updater working, except for cycles* clean up code* Fix tests* formatting* format:* nit	3
[VTA][OpenCL] intelfocl (#6126)* intelfocl support* disable tsim test* bugfix to vta autotvm* disable tsim test in task_python_vta_tsim.sh* fix integration test* update vta submodule and re-enable tsim tests* remove unnecessary comments	4
[NNVM] Initial mixed precision support of conv2d (#1356)	1
Remove support for run-time linked-params from codegen (#11144)Linking parameters via a runtime lookup function no longer happens aftercommit b5f1dabce4 (PR#8509): "Tir constants integration into compilationpipeline". Now, in cases where the runtime lookup would have happened inthe past, the parameters are embedded into TIR, removing the need for aruntime lookup.There is still plenty of code around that implemented the original runtimelookup. This patch removes the unnecessary leftovers from TVM's codegen.	4
[Contrib] Support fp16 input in cpu sort (#8672)	1
[Python] Populate setuptools description with README.md (#11078)* [Python] Populate setuptools description with README.mdAdds the description metadata for the setuptools descriptor file`setup.py` with the contents of our existing README.md, which isa common practice.* Update python/setup.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Update python/setup.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Import pathlib and apply black formats.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>	1
Remove settings about SGX in config.cmake (#6530)removed settings about SGX since SGX is removed from TVM core	4
remove limit on simple type	4
[MetaSchedule] Enhance CPU auto vectorization (#11404)	5
[ci][actions] Add more HTTP retries for conda (#11360)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Arith] linear system and equation solver (#5171)* [arith] linear system and equation solverCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* avoid constructing analyzer every time* generate random test cases and address commentsCo-authored-by: Sergei Grechanik <sergei.grechanik@gmail.com>* rename linear_system to int_constraints* add comments and use random seed* message for reporting failure with seed* add SEqualReduce to IntConstraints; allow variables & ranges to be NoneCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>Co-authored-by: Sergei Grechanik <sergei.grechanik@gmail.com>	1
[UnitTests] Enable minimum testing on Vulkan target in CI (#9093)* [UnitTests] Enable minimum testing on Vulkan target in CI- Include the Vulkan runtime in the GPU build.- Run test_target_codegen_vulkan.py as part of the `python3: GPU` CI step.* [CI] Added a dummy task_config_build_gpu_vulkan.sh, to be removed later.The CI builds use the Jenkinsfile located in the ci-docker-stagingbranch, but the scripts in the PR that is being run.  Temporarilyadding back a task_config_build_gpu_vulkan.sh, which just calls therenamed task_config_build_gpu_other.sh.	5
[TOPI, Relay] Add half_pixel option to Resize op (#4610)* add onnx resize converter* update frontends* updating topi* adding onnx resize tests* fixed NHWC test by casting size dtype to int32* fix tests* fix lint* update existing test cases* fix tensorflow frontend* fix lint* remove NHWC stuff* update topi resize test for half_pixel* update doc* fix doc* remove onnx resize bits	4
[TIR][LowerMatchBuffer] Fix lowering strides when source buffer has non-empty strides (#9166)	0
[DOCS] Improve docs naming, fix docs warnings	2
[FIX] Infer input shape in sparse_dense_padded's alter_op if one does not exist (#7308)* [FIX] Infer input shape in sparse_dense_padded's alter_op if one does not existIf there are multiple alter_ops in a model, the first alteration doesnot run type inference for the subsequent ones. In this case, we don'thave the shape information, so we run the inferencer manually.* add todo	2
Check iter_type in vectorize (#1921)	5
make injective ops's opt schedule applied to every output tensor (#11820)	1
[TIR] Add spans to all ExprNodes (#6860)	1
add missing gradient check to gradient pass (#4169)	4
[Community] @ganler -> Reviewer (#9346)	3
[BYOC-DNNL] Enhance GetRootCall function (#10836)* enhance dnnl codegen to support different topology of partition graph* fix lint* add test case	3
Add int8 gemm recipe (#1614)	1
[ci] Test pytest-forked boxing (#12312)This wraps all tests in pytest-forked (with n=1) so that segfaults and otherprocess-terminations are properly reported by pytest. With this reporting on issues like #12311 should be much better (segfaults become like any other test failure)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix the build error for wasm-standalone app (#6862)	0
Auto-tuning a Convolutional Network for ARM CPU (tutorial error, bug reports)  (#8103)* tune_relay_arm.py tutorial modify* Lint fix* Re-trigger CICo-authored-by: Chenfan <jcf94@outlook.com>	0
[skip ci][Docker, CI] Update DGL installation, temp disable DGL tutorial (#10067)	5
Make the TVM targets list available in Python (#7427)* Make the TVM targets list available in PythonChange-Id: I8602723fe57aaf32cee5392d4387a637115dd363* Rename the APIs to get target kindsChange-Id: I2e6e32e025e3614a148a30a31e5a2c52fd3563cc	4
[ETHOSN] Get buffer sizes from the compiled network (#12160)The NPU support library compiler sometimes adds padding to inputtensors which means the buffer sizes calculated at runtime cansometimes be smaller than necessary. Instead, buffer sizes are nowcollected at compile time and passed to the runtime so that they matchthe sizes expected by the compiled network. This was seen when runninga fully connected operation with an input that is not a multiple of1024, so testing has been added to cover this case.Additionally changed the fully connected test case to use pytestparameterization as part of a general cleanup, and fixed an issuewith specifying a different output shape and weights with more than 1output channel.Change-Id: Iad319d75326b9ac41950de982603660a084dc27b	4
[VM] Memory alignment check for `set_input` in Virtual Machine (#11391)* add memory alignment check* add accounting of byte_offset* transfer NDArray generation method to NDArray class instead of VM* describe conditions. check IsContiguous for external DLTensor* hide safeless method in private* fix lint* fix lint* check conditions for correct creation of NDArray from external DLTensor* lint fix* update API after review* empty commit. restart CI tests* empty commit. restart CI tests once moreCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
Onnx Gather operator added (#1513)	1
[Vulkan][Refactor] Split out vulkan.cc into separate distinct functionality. (#8157)This is in preparation for additional refactoring.  Functions areorganized according to group similar functionality together, tominimize the amount of file-to-file transfers needed later.  The maindivisions are between VulkanDeviceAPI,VulkanModuleNode/VulkanWrappedFunc, VulkanThreadEntry, andVulkanContext.Other than minimal renaming of private functions and addition of somecomments, this commit should have zero changes to the functionsdefinitions themselves, only to their arrangement within thesrc/runtime/vulkan directory.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[NNVM][OP] Allow two input tensors with different type in reshape_like op  (#2052)	1
[TF frontend][bugfix]Avoid making a new node when already has span info (#7789)* Avoid making a new node when already has span info* add test* add test* add test* fix* fix* move test to test_forward.py* fix* fixCo-authored-by: xiaoqiang.dan <xiaoqiang.dan@streamcoputing.com>	0
[BUGFIX] Seg fault in memory planing for symbolic shape (#2317)	0
[TVMScript] Printer Registry (#12237)This PR:- Adds the registry of printing function (traced_object_layered_functor.cc)Compared to the prototype version, this:- Consolidates the implementation into a single class, since this class is only for the TVMScript printer.- Deduces the TObjectRef when calling set_dispatch.Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>	5
[SCHEDULE] Normalize returns a new schedule (#94)	1
[APP] Improve GraphExecutor (#216)* Remove 'final' in GraphExecutor for extension* Dynamic num of inputs/outputs for tvm_op	4
[Cuda] Updated bfloat16 math defs. (#10258)Required to pass `test_cuda_bf16_vectorize_add` in `tests/python/unittest/test_target_codegen_cuda.py`.	3
[Hexagon] 2-d allocation cleanup (#10786)- Added device validity check in allocation. HexagonDeviceAPI should  only be called for CPU/Hexagon types.- Check for "global.vtcm" scope instead of "vtcm".  The ccope of N-d  allocations produced by `LowerVtcmAlloc` should be `"global.vtcm"`.  The previous check allowed unsupported scope such as `"local.vtcm"`.- Remove `vtcmallocs` entry after calling free. Previously, the vtcm  allocation map kept dangling pointers to `HexagonBuffer` objects  after they had been freed.- Rename N-d alloc and free packed functions.  Since most of the  similar device functions use snake case, renaming `*.AllocND` to  `*.alloc_nd` and `*.FreeND` to `*.free_nd`.Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Adam Straw <astraw@octoml.ai>	5
Fix error message in Buffer::vstore, NFC (#6056)* Fix error message in Buffer::vstore, NFC* Fix whitespace in comment as well	0
[CI] Set test python.contrib.test_onnx.test_resize as xfail (#12568)`python.contrib.test_onnx.test_resize` is failing due to a numericalaccuracy issue, reported in #12567. This patch marks that test asan xfail, so that other tests can be enabled, while this one isinvestigated separately.	0
[Relay][Tensorflow] Allow an op as loop var. (#3056)	1
[GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc	2
[runtime][cublas] fix typo (#6230)	2
Constant name prefix added (#11509)This is a proposal to fix the bug reported here: https://discuss.tvm.apache.org/t/problem-with-allocateconstnodes-in-cmsis-nn-code/12806Bug report: #11394A prefix has been added to the "constant_" in te_compiler_cache.cc to distinguish from the constant naming generated in aot_executor_gen.cc	1
Force a gc between sphinx-gallery items to reclaim GPU memory. (#8722)GPU memory is only released once the PackedFunc for evaling the model is gcedby Python. In CI we're noticing intermittent 'CUDA: Out of memory' failureswhile processing the tutorials, and tracing showed there was no gc happeningbetween items. Not confident this will solve the problem but worth a try.	1
[RUNTIME] Fix graph runtime for gpu (#491)	1
[DOCS] Move git_howto to rst, add Stage documents to te (#5055)	2
[TOPI][CUDA] batched int8 conv2d (#1961)	5
[Target] Migrate data structure of TargetNode (#5960)	1
Consider variable range information during simplification of tensorize expressions (#674)	5
[4/10] Code generation for Conv2D via CMSIS-NN (#9331)This PR is for support of Conv2D via CMSIS-NN.	1
fix duplicated symbol bug in external codegen (#7383)Co-authored-by: 袁航剑 <yuanhangjian@bytedance.com>	0
barrier fence added for warp mem (#1174)	1
Update plan_memory.cc (#2574)	5
[Hexagon] Add default constructor to struct Optional in session.cc (#10517)It's needed for older compilers.	1
[REFACTOR][IR] Introduce SeqStmt to replace ir::Block (#4627)* [REFACTOR][IR] Introduce SeqStmt to replace Blockir::Block was used to represent a sequence of Stmts in the original low-level IR.The nested ir::Block structure is not really friendly for recursive visits,especially when the statements are unrolled.This PR introduce a SeqStmt that directly stores a sequence of statements in an Array container.The new SeqStmt will be used as a replacement of the original Block structure.* [REFACTOR] Migrate use of Block to SeqStmt.* [REFACTOR] Remove Block* Add more comments per yizhi's comment	1
[QNN] Fix qnn.dequantize scale and zp shape (#10880)* [QNN] Fix qnn.dequantize scale and zp shape* Rework* Add review feedback	5
[RELAY] Refactor type inference to use type solver (#1779)	1
[RUNTIME] Initial implementation of Hexagon runtime support (#5252)* [RUNTIME] Initial implementation of Hexagon runtime supportThis is only the TVM runtime. The FastRPC libraries, simulator driver,etc. will be provided in subsequent commits.* Fix pylint complaints* Fix some more pylint complaints* Add link to the Hexagon SDK website* Extract VTCM marker into a common variable* Implement device->device memory copy* Disable unsigned PDs by default* Ensure that --hvx_length is present in sim_args if HVX is enabled* Remove the line about clang from README.mdApparently things work with libstdc++.* Mention to set USE_RPC=OFF when building libtvm_runtime.so for Hexagon* Remember to use codegen_hvx in validate_hvx_length* Add a line about minimum version of LLVM	5
Adding t-vi as a reviewer (#6149)	1
[Relay] More type alpha equality test coverage (#1823)	3
[Codegen] remove fp16 function override for cuda  (#4331)* add volatile override back* [codegen] remove fp16 function override for cuda	1
[ONNX] fix reduce crash on scalar inputs (#10780)* fix reduce crash on scalar inputs* fix uncovered cases.* fix on different opset to pass ci	4
[Flaky] Skip test_qlinear_average_pool (#10030)	3
[Vulkan] Broke out implicit device requirements into SPIRVSupport (#8048)Codifies the current requirements that are implicit in the shadersbuilt by CodeGenSPIRV (e.g. can read from 8-bit buffers).  The nextsteps for this development are (1) to query driver/device supportinformation from the device, (2) to pass these query parametersthrough the Target, and (3) to ensure correct shader generation evenwhen features are not supported.Step (3) will require exposing the target properties to relayoptimization passes.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Allow rust tvm build configuration through cargo features (#8665)	5
[Hexagon] Rewrite AllocateNodes with global.vtcm scope after FlattenBuffer (#11429)* Add test to ensure AllocateNodes for buffers with global.vtcmare rewritten.* Move test_cache_read_write.py out of topi testing directory.	3
[Auto Scheduler][Auto TVM] Fix infer tile size for NHWC winograd (#7068)	5
[TVMScript] Add ObjectPath class (#11977)Motivation:Same IR node object can be referenced in several different contexts inside a larger IR object. For example, a variable could be referenced in several statements within a block.This makes it impossible to use an object pointer to uniquely identify a "location" within the larger IR object for error reporting purposes. The `ObjectPath` class addresses this problem by serving as a unique "locator".Tracking issue: https://github.com/apache/tvm/issues/11912	0
Fix a crash in android_deploy demo. (#2073)	0
[TOPI] improve elemwise schedule (#393)* [TOPI] improve elemwise schedule* modify fuse	1
[microTVM][Zephyr] Fix missing BOARD in CMakeCache file (#12338)This bug was caught in the microTVM hardware in the loop CI, becauseonly in the HW in the loop CI it's possible to test 'open_transport' andZephyrSerialTransport() method, i.e. run a model via the serialtransporter.                                  This commit changes the microTVM Zephyr template project to read BOARDfrom cmake file instead of cmake cache file.	2
Sort functions (#11814)	1
use checked_type instead of type_annotation (#7522)	1
[RELAY][DOCS] Core Operator docs (#1821)	2
[VTA] bugfix parameter derivation (#1521)	2
[Relay] Support large constants saved/loaded outside of VM executable (#9734)* [Relay] Support large constants.This allows constant tensors at or above a given byte limit to be marked as'late bound' and saved/reloaded to a file independently of the overallexecutable. Since the executable is often embedded in the data segment ofgenerated runtime Modules this avoids problems with external tools which can'thandle multi-gigabyte data segments.[ACE-466 in OctoML JIRA]* [checkpoint] fix latent bytecode/code bug	0
[AutoScheduler] Fix flaky test (#6307)	3
Add Relay option to link parameters into runtime Modules (#6917)* refactor RPCSessionContext utils* Make TVMLogf platform-independent. * Some platforms need to use an alternate printf() to support basic   things like %zu. Since %zu is platform-specific, we prefer to   use a printf() that supports it or allow the platform to fix it up   as needed.	0
[Relay][Frontend][Keras] Fix ReLU in Keras Converter missed the case (#3917)* [Relay][Frontend][Keras] Fix ReLU in Keras Converter missed the case* [Relay][Frontend][Keras] Add test case for ReLU in Keras Converter missed the case* [Relay][Frontend][Keras] Add test case for ReLU in Keras Converter missed the case	3
[Relay][Frontend][Tensorflow] Fix GatherV2, Add StopGradient (#4238)* Add StopGradient. Add batch_dims attr to ignore list for GatherV2* Trigger CI	1
[HEXAGON] Slice ops added - add, subtract, multiply (#11529)* [UPSTREAM][HEXAGON] Slice ops added - add, subtract, multiply* Change to v68* Change transform_numpy function call* Do not disbale pylint errors and fix them* Fix variable names* Move the test file to topi* Resolve conflict* Modify init	5
[Relay][AutoTVM] Bug Fix for ARM CPUs. Lower strict assumption. (#5063)	0
Merge Java unittests into GPU unittests (#9732)These run on the same node but the Java unit tests take < 1 min (whereas the node setup alone takes 3-4 min), so it seems like it would be good to just run the Java tests as part of the general GPU unit testing rather than allocate and set up a separate node for it.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TensorIR][Bugfix] Disallow fusing loops with dependency (#9112)* check dependency for fuse* blank line	1
[PASS] Schedule Ops init working version (#6)* [PASS] Schedule Ops init working version* bugfix in PassUp	4
[ConvertLayout] Support transpose (#7214)* [ConvertLayout] Support transpose* format* fix ci* fix axes missing* fix* fix NCHW[x]c* Update src/relay/op/tensor/transform.cc* fix negative* fix	0
Add MyPy to lint (#1301)	1
Remove developer facing api from frontend exports. (#5375)	4
[docs] Add instructions for uploading CI resources to S3 (#12476)These were missing the final step to use the uploaded resources	1
[DOC] Add link to release blog (#342)	2
[REFACTOR][PY] Establish tvm.tir- Move related files into the corresponding location as in C++- Keep the top-level TVM API backward compatible to make minimum changes in topi	4
[REFACTOR] Establish printer in the source folder (#4752)* [REFACTOR] Establish printer in the source folder.As we move towards the unified IR, we will eventually want to build a unifiedprinters for both relay and TIR.This PR isolate the printer component into a separate folder in src as a first step.- Refactored the Doc DSL using Object, clean up APIs.- Isolate out the meta data into a header.- move printer into relay_text_printer, add comments about further TODos.* Rename NodePrinter -> ReprPrinter to distinguish it from other printers	2
[AlterOpLayout][x86] NHWC to NCHWc conv support. (#4080)	1
[microNPU] Remove remaining UnsupportedLayout checks (#9791)* [microNPU] Remove remaining UnsupportedLayout checksIn #9508 the decision was made to remove the UnsupportedLayout exceptionand the checks that throw it, this PR is cleaning up some that remained.Change-Id: I83bfe233381b83af886343c9569db753e33f9059* fix lintChange-Id: I67c1a5371f0b2e51b6cd39435ef4073d8d17af51	4
[COMMUNITY] New reviewer -- wrongtest (#9400)	3
enable amd_apu device on vulkan target (#5659)	1
Fix comment (#115)	0
[DOCS] fix link (#2157)* fix fname in comment* fix	0
[Relay] Support deformable Conv2D NHWC (#7075)* [Relay] Support deformable conv2D NHWC* add test case* fix lint* lint	0
[Rust][CI] Move CI over to new Rust crates and try to fix flaky test. (#6011)	3
[FRONTEND][TENSORFLOW] Bugfix (#2326)	0
[NNVM] Bugfix operator fusion for residual block with layout transform (#1760)* Bugfix operator fusion for residual block with layout transform* add a test case* update error message	0
Document Project API server. (#9654)* Document Project API server.* address leandron comments* Update docs/arch/microtvm_project_api.rstCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>	2
Testcases of onnx (#2274)	3
Support for Tuple Inputs of Reducer and ComputeOp (#175)* Support for batch ComputeOp* Support for batch ComputeOp* Fix CrossThreadReduction* Fix lint* Add UpdateArray, remove support for batch reduce* Tuple input support for reduce* rfactor works with multiple reducer; support multiple reducers with different types* Small fix* Small fix* Change return type of rfactor to Array<Expr>* Fix lint* Improve* Add tutorial* Improve tutorial* Improve tutorial	1
[QNN] Conv2D type checking for kernel per-channel scales. (#4732)* [QNN] Conv2D type checking for kernel per-channel scales.* Address commments.* Address comments.* - Adding safety checks for downcasts.Co-authored-by: shoubhik <shoubhikbhatti@gmail.com>	1
Update task_python_vta.sh	5
[COMMUNITY] Reviewer: wyc-ruiker (#8328)	3
adds antlr4 to python3 package list (#1560)	1
Asymmetric padding and dilation in conv2d workload (#7142)* added asymmetric padding to conv2d workload* fixed depthwise conv2d padding* Added fix to include dilation in workload output width calculation* Added missing dilation to arm_cpu/conv2d_int8.py workload* Fixed dilation for x86 conv2d* Improved dilation workload integration in x86* Fixed x86 conv2d_alter_op to add dilation* Local linting not always producing same output as CI, probably my fault* Fixed bug, tested locally* Abusing CI until I can figure out how to reproduce the same behaviour of running integration tests locally.* Ammeded conv2d_int8 test* Updated workload, improved unit tests* Added depthwise conv2d workload test	3
[Codegen] Remove compile_enginer header (#8471)* [Codegen] remove compile_enginer header* fix lint	0
escape tvmscript's string literal (#10954)	5
[Relay][Frontend][Onnx] Auto extract onnx input shapes when possible. (#7115)* Auto extract onnx input shapes when possible.* Remove shape dict definition in tvmc.	5
Drop trailing whitespace (#3331)	4
[Support] Add libinfo into the runtime build (#9310)* Move libinfo into the runtime build* put libinfo back into libtvm* limit microtvm imports when we only have the runtime lib* fix lint* try conditional for micro import	2
[RELAY] Enable registering op with python (#8002)Add a new API register_opNote: Implementing a op by pure python is still limited:  1. Custom type relation (add_type_rel()) is still not     available in python.  2. Setting number inputs (set_num_inputs()) needs     plevel > 128 in python.     (see tests/python/relay/test_ir_op.py)	3
Avoid downloading when TOPHUB_LOCATION is NONE (#5720)	5
fix (#138)	0
More on source reference (#466)	5
[VTA][Config] hotfix denano10 (#3918)	0
[BYOC] Add example of Composite + Annotate for DNNL fused op (#5272)* merge change from dev branch* fix string issue* bring comanic's change back	4
[DOCKER] Start docker infratructure (#1402)	5
[TE] reverse-mode autodiff without any optimization (#5121)* [TE] reverse-mode autodiff without any optimizationCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* address review comments* add comments and retrigger CI* move unittest to debug ci* move test back and add seedCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>	1
Fix code to work with cmake 3.2 (#6952)	1
[AVG POOL] Asymmetric padding (SAME) support. (#1346)	1
[BYOC] Added "include_non_call_ops" parameter to AnnotateTarget pass (#6655)* [BYOC] Added annotate_non_call_ops parameter to AnnotateTarget passAdded annotate_non_call_ops parameter to AnnotateTarget pass to preventnon-call to be promoted to previously annotated operationsThis is useful in case if you are not running MergeCompilerRegionspass after AnnotateTarget.* linter* Tuple and TupleGetItem handling* resored transform.py, added missing tests to main* requested changes	4
Update topi/cuda schedules to use target.max_num_threads (#577)* update topi/cuda schedules to use target.max_num_threads* allow num_thread to be larger than cuda.max_num_threads* remove get_max_num_threads and make it inline	1
[AutoScheduler] Relay integration : Task extraction (#6710)* add task extraction* fix evo search* fix tests* fix test* fix docstring* fix docstring* update workload registry* fix warning* fix test* fix fallback* fix lint* fix tests	3
[TF] Fix a bug in _stridedSlice() (#6829)When stride < 0, the slicing range for whole demension should be  [-1, -(dim+1))	0
sync logging improvement from dmlc (#86)* fix glog* sync logging improvement from dmlc	1
[DOCKER] Pin keras version (#6032)	2
[microTVM][ARM-DSP] Fix pool schedule  (#12653)When I built keyword spotting ONNX model, there was an issue with the pool schedule because certain schedules like broadcast and elemwise do not have input tensors.	1
allow const_range allocation; preprove if-then-else (#2419)	1
[Relay][Frontend][keras] added interpolation method of Upsampling2D (#2854)* [Relay][Frontend][keras] added interpolation method of Upsampling2D.* added testcase* small fixes	0
[Bugfix] Fix a memory leak in OpManager (#3263)	0
[CI] Solve occasional CI issue when pad value is all 0 (#3801)	0
Fix comments. (#12220)	0
[DOCS] Update to reflect the repo name change (#6967)	4
[Bugfix] Missing headers (#3392)	0
[Frontend, Tensorflow] Support for broadcasting in batch_matmul when shapes differ (#8251)* Support for broadcasting in batch_matmul when shapes differ* refactor* refactor logic for reshape in conditional* refactor	4
[Autodiff] Deterministic gradient compute (#7321)* fix unstable compute* fix* fix* lint* sort linear equation* sort inequalities* fix* fix find* lint* fix find* lint	0
[RELAY/PASS] Fix the extent for the post_stmt in the loop partition (#3734)	0
[DOCS] Update API docs to reflect the status after the refactor. (#4907)	4
[Fix] Fix CompilerAttrs (#5109)* fix CompilerAttrs* retrigger ci	0
[skip ci][ci][docker] Prune all non-relevant images (#11491)Before this would leave around any image that could be used in CI. ThisPR changes it so that the `docker rmi` knows exactly which image isbeing used in CI so all others (even those that are being used in thesame build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on andshould help keep disk usage down.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Minor][MetaSchedule] Remove Unused Imports (#10577)Remove two unused imports.	2
update halideIR (#1515)	5
[BYODT] fix CMAKE flag name + update documentation (#6567)* documentation fixes + better name* fix up comments	0
[cleanup] Remove task_sphinx_precheck.sh (#10196)This is empty following #9971 and is no longer neededCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Op][Topi] Gather, GatherND, Take can accept unsigned integers as indices (#10080)* take rel* gather and more tests* gathernd case* lint* remove test which invalidates take preconditions* re-add test* fix dumb test failure oopsie	0
[Vulkan] Support uniform buffer object for passing many scalar arguments (#7717)* ubo codegen first cut* begin runtime change for UBO* allocate and bind ubo* query memory type for uniform* refactor* do not use float64* trying an approach similar to push constant* add more log* do not delete ubo when not using it* cumsum and nms test working with ubo* remove log* cleaning up* formatting* revert BufferArgument change* refactored codegen* minor fix* introduce value kind for ubo* fix cpplint and revert float64 change* query push constant size using runtime API* let vkmap/unmap allocate and delete host_buf* doc update* fix typoCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	2
check in pass	4
[ETHOSN] Add support for resize (#12535)This commit adds support for the `resize` operator forArm(R) Ethos(TM)-N NPU.	1
During tensorize, call Simplify on algorithm and intrinsic definitions before CanonicalSimplify. This will prevent a number of false tensorize mismatches. (#718)thanks, this we can use this solution for now	1
[Relay][BYOCG] Propagate constant to subgraphs (#5094)* bind constant to subgraphs* con -> constant	5
[DOC] Fix typos in tutorials (#4066)fix some typos	2
[TOPI][Hexagon] Add test and schedule for uint8 resize2d (#12559)* [TOPI][Hexagon] Add test and schedule for uint8 resize2d* Fix correctness issue* Reformat* Remove cubic from testing* Remove unnecessary else	4
[COMMUNITY] @areusch -> Committer (#7679)	3
Fix gather_nd in Relay (#3442)* Fix gather_nd in Relay* Add test cases for gather_nd.	3
Allow all enum int to be exposed	1
[PYTORCH]Gather op support added (#6013)* [PYTORCH]Gather op support added* retrigger	1
Fix missing import in bifrost schedule (#6479)	2
Update SGX cmake (#1763)	1
Make version.py to rely on repository metadata to generate version string. (#9472)This change removes the need for a hardcoded version string whengenerating a TVM release from a git repository. The long term goalhere is to be able to have reproducible release versions withoutmanual intervention on files such as version.py.It also removes assumptions of what the last valid tag should bein order for the version string to be generated. This informationshould be encoded in the repository, as we are using git tagsto retrieve the information.	5
[REFACTOR][IR] Streamline ir/op Registry (#5609)* [REFACTOR][IR] Streamline ir/op RegistryThis PR refactors the attrregistry mechanism in the ir/op intoa separate common base. The common base will provide a foundationfor other attr related registries such as target and pass.We also streamlines the terminology of the registry API.- Use AttrMap for the column maps returned by the registry- Use RegEntry to refer to the registry entry.* Address review comments	1
Fix makedirs() condition in contrib. (#2942)	1
[CI][DOCS] Fix the sphinx doc style for sphinx4 (#8198)	2
[ROCM] Fix conv2d (#3107)	0
fix plan memory add inplace_identity option (#124)* fix plan memory add inplace_identity option* comment	1
Support automatically Name Loop Variable in IRBuilder (#716) (#741)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop* [IRbuild]Support automatically Name Loop Variable in IRBuilder (#719)    * add idx_num in class* using typical index [i, j, k] first, then i_suffix* keep inputs names* fix lint* improve comment of name* fix lint	0
[PROFILER] Add shape, structural hash, and layout information to profiling (#7894)* [PROFILER] Add shape, structural hash, and layout information to profilingAdd a new pass that which inserts the layout and structual hash of the op into theattrs of Functions.* includes* fix gcc5 issue* old gcc fixes	0
Unite cmake builds (#1248)	1
[PyTorch] [Relay] Add l1 and mse loss function for pytorch frontend (#11978)* add l1 and mse loss function for pytorch frontend* fix CI	0
[RELAY][VM] Enable heterogeneous execution for Relay VM (#6337)* vm heterogeneous execution* context analysis on module* fix profiler* fix memory plan* add more unification* add serialization* add gpu tests for test_adt* cache visited functions* path compression* C++ context analysis* remove python context analysis* add tests* clean* lint* fix* enable gpu test for dynamic namespace* remove GetParamsContext* fix comments and add doc for context analysis* cache context* cache allocator* rebase and fix comments	0
add black-format to docker/lint.sh, suppport in-place format (#6601)	2
[VTA] Support for batched inference (#3661)* fix in IR pass to support padding on 6-d tensors* support for both N>1 and N==1 for padding* batch size > 1 tuning and base config* output formatting* batch conv2d* print all category results* revert to single-batch config* pick record best* fix conv test* improving reporting* address batching bug in fast simulator* fix	0
[LINT] Remove scalalint from lint deps (#5269)	4
[FIX] Update rasp benchmark example (#225)	5
[PASS] add plan memory (#19)	1
[microTVM][zephyr] Increase stack size for zephyr host-driven AoT tests (#11777)* set zephyr stack size to 4096 for qemu_* and zephyr_board targets* use smaller stack size for HW targets	1
[cpptest] Reset op attributes before registering them (#9202)Prior registration of attribute XYZ can result in an error whentrying to register the same attribute again:```Check failed: (p.second != plevel) is false: Attribute XYZ of opis already registered with same plevel=10```Reset the attributes before registering them.	1
[TOP] Level 3 complete (#7)	5
Support dequantizing scalar inputs (#8207)	1
[Fix] post-fix incre/decre should not return reference (#12128)	0
[FRONTEND][ONNX] Some bug fixes and Shape operator fixed for relay. (#2850)* [FRONTEND][ONNX] Some bug fixes and Shape operator fixed for relay.* * test cases* * ci error	0
[BUILD] restrict runtime include headers (#1444)	1
It's gpu not cpu. (#4832)	5
[Rust][Fix] Memory leak (#8714)* Fix obvious memory leak in function.rs* Update object pointer	5
[AOT] Introducing AOT in TVM (#7785)* [AOT] Introducing AOT in TVMThis change adds the code generation and minimal runtime API to use theAhead Of Time (AOT) compilation flow. The main logic is contained in:- src/relay/backend/aot_codegen.ccWhich produces a TIR PrimFunc traversing the Relay graphThe runtime interface (authored by @mousius) leaves a gap for futureiterations using platform-specific features from RTOS.Currently AOT runs successfully on x86 in a host OS, running thesetests on micro is coming soon.This PR is based on the RFC described here: https://discuss.tvm.apache.org/t/implementing-aot-in-tvm/9206Co-authored-by: Christopher Sidebottom <Christopher.Sidebottom@arm.com>Change-Id: I9f731c953231f129e1472298915dddc01788efd7* Rebasing 2Change-Id: Ia0a533a49960f1cb4bf3c3833511e539cf7c459f* Applying comments/refactoringChange-Id: Iea1832355f8b1d4c921d02c6b4ceec7db3a681c1* Fixing comments + refactoring - 2Change-Id: I7200cc17b297e42bf67dcdef6f643e86991ca0a8* fix lintingChange-Id: Iba6544ac7101595696b352b8702345cf916625f6* fix linting - 2Change-Id: I7f80d16005f2c621d37a9aae2cbbd61df0277cbe* fix linting - 3Change-Id: I7a1ba40afeea46d5f122563a20cd4b2f08751a1e* fix testsChange-Id: I1297ccc54dd6d93647f421e0beb226f410bf73f5* Addressing comments - 3Change-Id: Id25d1382c30d6d0a0013b5e8986fb8cd886666dc* Addressing comments - 4Change-Id: Ibe29676abe3b75161b5a0903e007118a8318d862* fix tests - 2Change-Id: I2117f9d4392bfd87102ecbef0993c8b320f479a0* fix tests - 3Change-Id: Ic0373543b0f9a54dbd4dc32d428272f7293200ba* fix tests - 4Change-Id: I8a6f229c9a3a9e169779c8d49cbfa3f473348b1f* Addressing comments - 5Change-Id: Ib9ccd07c87392034a21b2eb70955d0b091b780f1* fix tests - 5Change-Id: I4b13c3b548ced414991e83072e9e6fc99b64f939* fix tests - 6Change-Id: Id5af1f778ae25bc60849cc054a605181c1b7a765* addressing comments - 6Change-Id: Id94a2bbcaae891f9498d41be538f13a952f55b81* fix linting - 4Change-Id: I371a0aa5b81824b5a3a1278fac22ace57832027a* add missing fileChange-Id: If359bef96dd0773ead4f75f0d9f5234276347e2d* fix buildChange-Id: I73fc1feb6f7b5d454a528e3289228484dc2b07d5* addressing comments - 7Change-Id: I7f908f3908ffc77e408391f62edcc06f2600c6c2* addressing comments - 8Change-Id: I90bced4e18259a6d42e6a406d93958e204f3859e* rebasingChange-Id: Id28751b069bd046f00faee301b2b446b2ea4fab8* Addressing comments - 9Change-Id: I06c9f280de0a9bf0ca5545bbbbfcc70cb66831b3* fix tests - 7Change-Id: I739f29779862f05def36e5f3e0722019596d17f8* Addressing comments - 9Change-Id: Ie736f40a5225f4e56e79006753d7732127da5408* Applying comments + fixing testsChange-Id: I83e16068b93aaccc7a86b79d42f13328bc76b53d* Applying comments - 10Change-Id: I443d72f53913849f3c28fd6e416162d1ca99e647* Addressing comments - 11Change-Id: I7fefbd0076949b9c38d0abbf2759ebf1502de330* Addressing comments - 11Change-Id: Iad028144d7b394b2dd2fce41a35ca689d1680200* fix tests - 7Change-Id: I14286e665dcdba1e9bc10bb5a27dd6ced50372b0* fixing tests -8Change-Id: I7b4c966da9680870ceda1704c749ee3bdc751926* fixing tests - 9Change-Id: Icf62128a604998ed1b7d5af4cbeadf7d39196d0bCo-authored-by: Christopher Sidebottom <Christopher.Sidebottom@arm.com>	4
Fast exponent (#4790)	5
Customize SI prefix in logging (#5411)* Customize SI prefix in logging* Include unit test	3
Modified pick best to accumulate the best configurations from both the input and output file. (#3225)	2
[Hybrid Script] Add `max_num_threads` (#2672)* i think it works for now?* fix lint* fix 2/3 compat* fix py2 again* fine, i gave up	0
[DOCS] Remove stale Auto TensorCore CodeGen tutorial (#7924)	4
JVM NDArray fatal exception fix (#1022)	0
[Metaschedule] Add test case for multi-anchor subgraph (#10856)This adds a demonstration of extracting, scheduling, and e2e-compiling relay subgraphs with multiple anchor ops. Since task extraction is not associated with TE scheduling anymore, extracting a subgraph with multiple anchor TE compute just works.The test case manually creates a simple fused mod with two `relay.dense`. But in the future, an effort like https://github.com/apache/tvm/pull/9628 should make it easier to construct multi-anchor subgraphs.The extracted TensorIR block corresponding to two TE `dense` compute looks like this:```@tvm.script.ir_moduleclass Module:    @T.prim_func    def main(placeholder: T.Buffer[(128, 128), "float32"], placeholder_1: T.Buffer[(128, 128), "float32"], placeholder_2: T.Buffer[(128, 128), "float32"], T_matmul_NT: T.Buffer[(128, 128), "float32"]) -> None:        # function attr dict        T.func_attr({"global_symbol": "main", "tir.noalias": True})        # body        # with T.block("root")        T_matmul_NT_1 = T.alloc_buffer([128, 128], dtype="float32")        for i0, i1, i2 in T.grid(128, 128, 128):            with T.block("T_matmul_NT"):                i, j, k = T.axis.remap("SSR", [i0, i1, i2])                T.reads(placeholder[i, k], placeholder_1[j, k])                T.writes(T_matmul_NT_1[i, j])                T.block_attr({"layout_free_placeholders":[placeholder_1]})                with T.init():                    T_matmul_NT_1[i, j] = T.float32(0)                T_matmul_NT_1[i, j] = T_matmul_NT_1[i, j] + placeholder[i, k] * placeholder_1[j, k]        for i0, i1, i2 in T.grid(128, 128, 128):            with T.block("T_matmul_NT_1"):                i, j, k = T.axis.remap("SSR", [i0, i1, i2])                T.reads(T_matmul_NT_1[i, k], placeholder_2[j, k])                T.writes(T_matmul_NT[i, j])                T.block_attr({"layout_free_placeholders":[placeholder_2]})                with T.init():                    T_matmul_NT[i, j] = T.float32(0)                T_matmul_NT[i, j] = T_matmul_NT[i, j] + T_matmul_NT_1[i, k] * placeholder_2[j, k]    ```	5
Dynamic ONNX Importer (#6351)* Change onnx importer to use dynamic upsampling3d (#3)fix pylint* Refactor ONNX frontend to be dynamicMake OneHot dynamicSupport BatchMatMul with dynamically shaped inputsfix dynamic broadcastAdd null checks to broadcast_to rel functionsfail more isolated broadcast_to testuse StructuralEqual instead of pointer comparisions in dynamic_to_static passadd an optional weight freeze argument to onnx importerconvert onnx resize to dynamic opadd dynamic expand to onnx importeradd a shape_func for powerfix BERTSquad, linthandle onnx graph initializer parameters more intelligently* Dynamic ONNX importer: Upsampling and Pad (#2)fix lintfix Call referencefix a type issue with expandfix a bad test refactorrespond to review comments, fix batch matmul tests* black format* fix batch matmul test* add dynamic strided slice to the onnx importer* fix clip importer* fix qnn tutorial* fix bad merge, respond to review comments* add a simple dynamic model test* Add dynamic-shaped autopadding to convolution and pooling ops* fix dynamic issues in a few ops* fix pylint* disable tests onnxrt doesn't support* fix pytorch test* respond to review comments* add documentation about partially supporting dynamic shapesCo-authored-by: Lily Orth-Smith <lorthsmith@octoml.ai>	5
[TUTORIAL] add extern lib tutorial (#331)* add extern lib tutorial* do fix	0
[TOPI] Setting workload correctly for Depthwise conv ARM. (#5182)	1
[MetaSchedule] Add utility API to ease using manual schedules  (#10876)As discussed in https://github.com/apache/tvm/pull/10856#discussion_r840324560, add a utility under `meta_schedule/testing/utils.py` to clean up the database boilerplate. Also using `DummyDatabase` instead of `JsonDatabase` for further clean up, as suggested by @junrushao1994 .	4
[PYTHON] Add __init__ to the generated grammar so that it can be installed properly (#4223)	5
[CI] make sure submodule checkout in clean state (#7228)	4
[HybridScript] Capture constant external python variables (#3157)	5
[CI] Avoid content-length request in test data download (#4375)	5
[Relay] fix error in ANF (too agressively inline atomic expression and create free variable). (#2665)	1
[VTA][Chisel] Change Scala Linter scalafmt => scalastyle (#4998)* scalafmt => scalastyleChange-Id: Ifc590e7cb63585f35dfdc9efcf3c6287b1afb1dd* scalafmt => scalastyleChange-Id: I8aff2632dadda05d2896e28bdaf6f780a160a15a* add indentation constraintChange-Id: Ibeb00c11a5718ea47322ea2b82e757828af8af91* trigger ci again	4
revert SET_LLVM flag (#7657)Co-authored-by: Lei Wang <34334180+NjtechPrinceling@users.noreply.github.com>	1
log_softmax added to topi (#483)	1
[µTVM] Zephyr: Add MPS2-AN521 board as a test platform (#7864)Now that MPS2-AN521 board is supported as a µTVM target, add it as testplatform so tests can run against it by using:$ pytest test_zephyr.py --microtvm-platforms=mps2_an521Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	3
remove dtype in model symbol (#310)	4
[AutoTVM][BugFix] Fix autotvm on the conv2d_nchw_winograd.mali operator (#6130)* [AutoTVM] Fix conv2d_nchw_winograd.mali* Fix pylint errorCo-authored-by: Yanming Wang <yanmwang@amazon.com>	0
[Metaschedule] New relay backend for meta schedule task extraction (#10578)* New relay backend for meta schedule task extractioncommit 501fac65291c51710911ca49af1577ea1794bcb2Merge: 076fa33fc ce8c563d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:16:47 2022 +0900    New relay backend for meta schedule task extractioncommit ce8c563d09eaba2a6b03189d1d3452f7565f4c69Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:12:30 2022 +0900    fix cpplintcommit dfa4fb0c20c17049e8ac2c135200074b872ce1ecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:09:11 2022 +0900    update expected op list in    test_meta_schedule_integration_extract_from_resnet to remove dep on Ansorcommit a98182eed3b85e477c5f2527d5d21ce545bd5c18Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:56:35 2022 +0900    fixed test_meta_schedule_integration_apply_history_bestcommit 40d52a15b4c1ac9b8d4eac16f98ccec5e2a3e966Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:50:43 2022 +0900    uniquefy task namescommit dfaf4964bf3a0b542ead5f11f356c2ec592be725Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:45:30 2022 +0900    dedup taskscommit e49d500299c9c884497410046421853266b60cd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:59:45 2022 +0900    return reversed listcommit 74636beae0878cdda7dd03aa2b09ab2821c86477Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:39:58 2022 +0900    refactorcommit 99f1701eb71d77a85bb0f8457841739dc586a168Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:34:14 2022 +0900    clean up integration.cc and Query interfacecommit 3f93a1e7645118c002aa10e5b7ff14b71b3f837aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 11:54:57 2022 +0900    check in minor vnni-related changecommit af3e98867f91f99522fee4da2e170dc87311466cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 07:36:35 2022 +0900    Removed TaskExtraction nodecommit 7b4d35eb00852db6397d43e0aa6b1fedabae3f63Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:42:56 2022 +0900    add doc to util functionscommit 3c5a3184fb42e69ef10619b05b9b9f128f7ea618Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:27:53 2022 +0900    rename to task extractioncommit 57f2882a5ed5615ef8eee96cd7284d495f908449Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:24:37 2022 +0900    fixed constant param bindcommit f099537d3630d268ad0700c75e93bbdc67831837Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:10:44 2022 +0900    remove unused stuff from python extract_tasks_from_relaycommit 4a5e4aae48a7bdc8c24c8f7ae7bd5484034837e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:10:30 2022 +0900    move BindParams function to cc filecommit efecceaea3958e184de7ef0ff6cb5f3988640afaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 03:56:05 2022 +0900    refactor param bindingcommit 109187fc0463728cd44171389e8fc91fb0ac8cf9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 02:21:58 2022 +0900    New relay backend for meta schedule task extractioncommit 6f019014a4614f43aefcf642981bfb15d64b09f3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 11:25:44 2022 +0900    fixed anchor impl selectioncommit be6c25893dd0546db71b8472415303fc5be9d67fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 10:57:02 2022 +0900    Forgot visiting arg in ScheduleBuilder CallNode vsitcommit 0c6d4a603335ae2cba2771e939eff1ddeb98fbe3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 10:45:08 2022 +0900    add public, fix include path conventioncommit 4cd3a1657c4e2e13abe7281b7cdef5dff73b37eeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 18:43:15 2022 +0900    removed create_schedule stuffcommit eb1bc7e789b66eaf3d4fe01d5154c135ab275dc2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 18:13:42 2022 +0900    fixed merge conflictcommit 6e68fd9aff9f86412f8b7150b18ae1b374927f86Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 14:27:34 2022 +0900    Decouple TE compute and schedule lowering in ScheduleBuilder* update integration.h doc* remove unused import* fix mypy check* use_meta_schedule restored, now extracts the same task as Ansor* python doc update* unused import* cache_ -> cache, suppres "Cannot find workdload" warning* Update src/relay/backend/task_extraction.cc and te_compiler_cache.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* removed unnecessary include* fixed build* drop relay.const on params* updated comment in integration.cc* update schedule_rule name to prepend "metaschedule"* typo fix* more nit change* make the output of Query Optional* update py doc* remove TODO comment on parse_modCo-authored-by: Junru Shao <junrushao1994@gmail.com>	2
Allow log_softmax on explicit trailing dim (#1684)	2
[relay][tensor_array] test tensor_array in vm (#4608)* [relay] test tensor_array in vm* add tensor_array scatter test	3
[RELAY] Turn reshape into nop in graph executor backend. (#7945)* [RELAY] Turn reshape into nop in graph executor backend.Previously we are generating the function calls for reshape.This PR updates the optimization to turn reshape into nop:- Tag a fused function as reshape only if it only contains reshape.- Update memory planner to force input output to share the same piece of memory- Update the graph runtime codegen to emit nop when reshape only function is encountered.* Address review comments.* Additional comment and TODOs on the rationale	2
[BUILD] Clean the HalideIR submodule during the make clean (#163)	4
[PASS] Improve double buffer (#413)	1
Add same_as to NodeBase (#550)* Add same_as to NodeBase1. Most class inherited from NodeBase(Schedule, Stage, etc) still havethe convenience of using '==' for object identity. And this is the rightbehavior for non-Expr classes.2. subclasses of ExprOp now create EQ expression when '==' is used.`__nonzero__` and `__bool__` in EQ and NE is a comprise that in some casesobject identity semantics is still useful, like in unit test. For instance:````assert a == b````"a == b" will create EQ expression, assert then calls `__nonzero__` of theresult expression. `Expr.__nonzero__` throws exception since it prohibitsevaluating IR expression.More complex case like:````assert a in b # b is dict````it will call `__eq__` on a and all keys of b, then `__bool__` on the resultexpression. This could not easily be done by same_as.* Retain __hash__ from NodeBase in Python3	3
[Relay] Conv2d grad (#3636)* [Relay] Conv2d grad* Fix test* Fix first order gradient	0
[Topi][Op][PyTorch][Vitas] Fix inconsistent kernel layout conventions for conv2d_transpose (#9336)* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* jostle* fix keras* fix another frontend test* fix things* jostle ci	0
Added support for CoreML Permute layers (#262)https://apple.github.io/coremltools/generated/coremltools.models.neural_network.html?highlight=permute#coremltools.models.neural_network.NeuralNetworkBuilder.add_permute	1
[Relay][Text Format] Reverse CallNode Print Order (#2882)	5
[CMake] Split out libinfo.cc into a separate target. (#8520)Every `*.o` file in the cmake-generated makefiles have a dependency onthe target's `flags.make` file.  The `flags.make` file contains thecompiler flags for all objects in a target, not just the `*.o` filecurrently being compiled.  As a result, even though `libinfo.cc` isthe only file that has the `TVM_GIT_COMMIT_TIME` and`TVM_GIT_COMMIT_HASH` definitions, every file in the `tvm_objs` targetwas recompiled whenever the commit id was changed.By splitting `libinfo.cc` out into a separate target, no other filesneed to be recompiled when committing or changing branches, unlessthere are actual changes to the file.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Hexagon] Add hexagon launcher to apps and add to TVM's build system (#9220)* Add USE_HEXAGON_LAUNCHER cmake configuration to build the androidhexagon launcher along with a standard TVM build.* Update hexagon launcher README.md to includeinstructions on how to build the launcheralongside TVM.* Move Hexagon launcher into top-level apps directory.* Refactor hexagon launcher cmake directorystructure and group common code intocmake/HexagonLauncher.cmake.* Address CRs from @kparzysz-quic.	1
Typo: Tensorflow --> TensorFlow (#3249)	2
[docs] Getting Started with TVM: Auto Scheduler and matmul (#7644)Moves the auto scheduler with matmul example into the tutorial,expands to follow the flow of the larger getting started tutorial.Indended to follow the AutoTVM tutorial on matrix multiplication.	1
[NODE][REFLECTION] Support NDArray as field (#1452)	1
Fix clang llvm 12.0.1 warnings (#10744)* Fix warning: zero as null pointer constant [-Wzero-as-null-pointer-constant]* Fix warning: private field 'first_for_' is not used [-Wunused-private-field]	1
[VTA][Xilinx] Update to Vivado 2020.1 and Pynq 2.5 (#6402)* vivado version update* update docs	2
[DNNL] Add bfloat16 type support for dnnl conv2d kernel (#11902)	1
[TOPI] Add GPU SSD (#1397)	1
[Relay][Op] Enhance Upsample Operator to support float scales   (#4206)* :add scale2 for upsample* update unit test for upsampling* support latest upsample op for multiple frontend* fix lint* fix lint* fix lint* fix lint* update scale description and rebase	5
[Arith] Inverse affine map (#8384)* [Arith] Inverse affine map* [Arith] Inverse affine map* Update iter_affine_map.h* Update iter_affine_map.h* Update iter_affine_map.py* Topology order visit* doc* fix* address comments* lint* remove print	4
[ci] Don't diff when running clang-format  (#10933)* [ci] Don't diff when running clang-formatThis takes about 15-20 extra seconds but has the benefit of allowing users to replicate and fix clang format issues locally with ease.* format files* Add --fix flag* CommentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
LowerWarpMemory: remove unneeded shuffle when accessing from the same thread (#8681)	4
[TOPI] Group conv2d NHWC op implementation (#6510)	5
[TOPI] add binary broadacst (#456)* add binary broadacst* fix testing* revise testing threshold	3
[REFACTOR][TIR] Migrate all low-level passes to the Pass Manager. (#5233)* [REFACTOR][TIR] Migrate all low-level passes to the Pass Manager.This PR migrates the tvm.lower to return IRModule of PrimFuncsinstead of the LoweredFuncs.* Remove LoweredFunc.	4
[FRONTEND][TFLite] Fully connected op conversion made in sync with TFLite (#5510)* [FRONTEND][TFLite] Fully connected op conversion made in sync with TFLite* [1] Test case added* [2] Review comments handled* [3] Prints removed	4
[typo] fucntion ==> function (#2239)fucntion ==> function	1
[TIR] Register CUDA WMMA tensor intrinsics (#11677)* Register CUDA wmma tensor intrins* Meta programming to generate wmma intrin* format* fix* fix wmma_store* lint* Update cuda.py	5
Bring Your Own Datatypes (#5812)* Add ChangeDatatype pass and unittest* [WIP] Jared's work on FriThis was work that Jared did on my computer, trying to get Inception v3 running.* Fix simplify inference to work over different data types.* Formatting* Copy setup code from other test file* Logging in Relay* Remove duplicate TVM_DLL* Add Sub, Mul, Div, Max to bfloat lib* Fix previous broken rebased commit* Remove line* Add LowerCustomDatatypes to build passes* Upcast ints to custom datatypes too, as well as to floats* Add and use convert_ndarray* Lower Call* Relay: create constant scalars of custom dtypesWe use the same method we use in TVM: store the value in a double.* Custom datatype formatting in Relay* Update unittests* Add simpler example that's not working yet* Add Python unittests to Makefile* Fix bug* Fix function name in GetPackedFunc call* convert_ndarray makes its own executor* Add simple test case* Move setup() calls* Use convert_ndarray* Change import to make it more specific* Fix another Registry::Get call* Allow users to register minimum functions for custom datatypesThis commit allows users to register global functions named`tvm.datatype.min.<type name>` which take the number of bits in the custom typeand return the corresponding minimum value (as a double).A similar commit will need to be created for max, whenever that ends up beingneeded!* Remove check for float* Add test* Fix inception test* Add MobileNet* Lower custom datatypes before intrinsics* Add exp and sqrt bfloat functions* [buggy commit] Lower intrinsics like sqrt, expThis commit has bugs in it, I'm fairly certain.* Formatting* Fix bug* Add lowering for new ops in test* Add int to bfloat* Remove print* Add all tests* Correct image size* Add TODO* Add "notbfloat" typeThis type is for testing purposes. It just stores a float in a uint32. It wasused to confirm the fact that my bfloat "implementation" is very numericallyunstable and was causing issues when running the model.* Convert argumentsNot sure how necessary this actually is.* Rewrite custom datatype constants in Relay* Add test_ops* Print constants in Relay* Use topi.testing* Test conv2d* Add test_model* Comment out model tests* Register notbfloatThis could be unregistered at some point later* Add commented codeRemove later* Add posit tests* test_ops_same_function* [temporary] move incomplete commit to macbook* Add more to tests* Formatting* Uncomment add* Remove bad tests* Change comments* Change function name and docstring* Change main function* Restructure tests* Fix visibility of posit functions* YAPF* Switching keywords around to resolve build errors on some systems* Improve test by running smaller mobilenet* Add test_cast* Change datatype name; add simple test* Rename to posit32* Merge 3 posit types into one file* Add a nop type* Remove bfloat* Refactor test comments* Refactor conv2d test* Add optional tolerance arguments* Add posit8 and posit16* Add comment about posit8* Whoops -- actually add noptype to CMakeLists* Add rtol, atol to run_workload* Add noptype to tests* Run noptype over other models, too* Pass correct arguments to calls* Fix line length errors* Raise tolerances (again) to avoid flaky test* fix style* add test for tanh, log, sigmoid* Remove references to bfloat, notbfloat* Change comments* Remove old test file* fix min func* refactoring unit test file* use posits es2* cleanup* comment* coment if_then_else* support different bit widths* use random seed to create stable tests* update documentation* removed nop-type and code consistency* add batchnorm test* rebase and update* fix tests and format* pylint* change order of include* include order* fix style* remove posit c linkage* update universal* fix style* fix test* fix overflow error with minfunc and posits* style* use change_dtype to convert params* update universal* fix fatal error* fix constant repr* minor update to posites2* update universal* fix rst* fix invalid import and sqrt* update universal* comments* comments and expand testing* increase atol/rtol for custom[posites2]32* Re-add newline* Remove comment* Remove opt level and comment* Change docstring* Add TODO* Add file header and newline* Update docstring* Update file docstring* Update docstrings* Delete todos* create_min_lower_func* add better debugging message* docs* add BYODT tutorial* add todo* Reformat some of tutorial to RST, plus code fixes* tutorial notebook runs now* fix hyperlink* rebase* add to tutorial* fix mobilenet model* add skip tag* black lint* add compiler flag and add dummy float* myfloat and posites2 test* remove universal* lint* lint* add setup* build with USE_POSIT for CI/CD* fix posit cmake* add cd /* undo docker changes* change tutorial to use myfloat* move files* lint* fix* remove filter* fix lint* fix suggestionsCo-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Andrew Liu <andrewlliu@gmail.com>	0
reduce input size to fix oom (#4653)	0
[Relay][Frontend][TF] Fix slice when begin or size is not Const (#4372)* fix slice bug when input is param* use _infer_value rather than _infer_value_simulated	5
[TOPI] bitserial_conv2d move to autotvm template and updates (#2819)	5
[MetaSchedule] Support ApplyHistoryBest Direct Dispatch (#12016)This PR introduced a new argument for `ApplyHistoryBest`'s `Query` interface to allow direct dispatch without querying the database, would be useful for debugging and benchmarking without interference.	0
[REFACTOR] relay::Module Def -> TypeDef (#4665)* [REFACTOR] relay::Module Def -> TypeDefThe term Def was not very clear about what is the object of interest(could be function def or type def).Changes the term to TypeDef to be more explicit.* Update include/tvm/relay/module.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>Co-authored-by: Wei Chen <ipondering.weic@gmail.com>	5
Adjust Hexagon conv2d schedule to split channel out (k) and move to outer loop (#9287)* Adjust Hexagon conv2d schedule to split channel out (k) and move to outermost loop* add missing reference data verify	5
Turn on Compute library testing in CI for AArch64 (#8291)* Turn on Compute library testing in CI.This pull request turns on compute library testing in CI by1. Handling import errors in Compute Library Integration.2. Setting the configuration to the right path for ACL.This handles import errors for packages in Compute library integration.This pull request allows for the AArch64 CI to pick up nativecompute library testing and tests the operators being offloaded atruntime.* Fix typo* Fix up use of ubuntu_install_arm_compute_lib.sh in Dockerfile.ci_arm* Move to using pre-built ACL binaries for ci_arm* Fixup the path for installation to be /opt/acl as it originally was.* Fix up the issues with paths.Once this is done ci_arm will need to be rebuilt though will continueto work seamlessly.	1
[FRONTEND] [HYBIRD] [TEST] Add GPU shared memory test! (#1338)	3
[CI] Update ci-cpu to the latest (#6164)	3
Fixed bugs for conv2d (#1465)	0
[TENSORFLOW]Conv3d Transpose OP added (#5775)* [TENSORFLOW]Conv3d Transpose OP added* Testcase updated, tf cpu supports only ndhwc	1
[microTVM] Add Nucleo stm32l4r5zi board to zephyr (#8386)* add stm32l4r5zi_nucleo* add parameter for test qemu* file type check* fix test* change order* revert	4
[Frontend][TFlite] use qnn helper function in softmax (#4840)	1
Add cuda tags and unit test (#7410)* Add cuda tags and unit test* Add missing space* Remove extra indent* Modify macro def position* Fix clang format* Fix clang format for set_config	5
[CI] Remove topi from the CI cache (#6188)	4
[skip ci][microTVM] Add pytest-xdist to pyproject.toml (#12478)	5
[Frontend, Tensorflow, Tensorflow2] Tensorflow frontend op refactor (#8179)* Refactoring the ops from Tf1 frontendCo-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>* Resolving an import bug and refactorCo-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>* Tf2 frontend importing from common ops* linting and unused imports fix* Applying changes from commit id f4ec5fd4ae346dbdd8e915c048aeed94b44f6776Author: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>* Minor lintingCo-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>	5
[microTVM] Add wrapper for creating project using a MLF (#9090)Currently there is already a wrapper function for creating a new projectdirectory based on an ExportableModule, but there isn't one for creatinga new project directory based on an existing MLF archive, which is alsohandy. Hence that commit adds a new wrapper for creating a project usingan existing model compiled and kept in a MLF archive.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>Reviewed-by: Christopher Sidebottom <chris.sidebottom@arm.com>Reviewed-by: Andrew Reusch <areusch@octoml.ai>	5
Support mxnet dot and LogisticRegressionOutput (#6542)	2
[Documentation Changes] Parameter description change for reduction apis (#881)* [Documentation Changes] Parameter description change for reduction apiThe parameter description is updated for max, min, argmax and argmin* Lint changes on patch-3 of reduction.py	4
Fix reduce NCHWc infer layout (do not keep reduced inner c when keepdims=false) (#9821)* Fix reduce NCHWc infer layout (do not keep reduced inner c when keepdims=false)* black* lint	5
[IR] Move AttrStmt to HalideIR (#21)	4
[AUTOTVM] Core part of auto-tuning module (#1312)	5
[USMP] Hill Climb allocator (#9704)* [USMP] Hill Climb allocatorThis PR adds HillClimb allocator "tir.usmp.algo.hill_climb"to the memory allocation algorithm set.Change-Id: Ib7485df93757eb512da040528ec86c920db8d03b* requested changesChange-Id: I6700a24c1608d92f87be7dde33cc24f5de1f7063* Conda-related linter small fixesChange-Id: I0dac5c6d75ade8f813b077c8708aad59d2722933* Moved implementation from greedy.h to greedy.ccChange-Id: If8ed159eceef32d3f22b51e0252161d09222eb1e* Integrated into test_tir_usmp_algo.py unit testAdded "hill_climb" into test_tir_usmp_algo.pyAmended sorting to be consistent with "greedy" familyChange-Id: I8e9f5282f15baaab71d6d129aeb9643376b14763	4
[Relay, Quantization] Quantize all fields of concatenate (#2913)	5
[Utils] Handled Callable in tir.schedule._type_checker (#12633)Previously, `Callable` was handled as an atomic type.  This workedwhen it was included as last element of a `Union[]` annotation with nosubtypes, but raised an error for other use cases, including`Optional[Callable]`.This commit adds explicit checks for `Callable` type annotations tovalidate whether the argument is callable, but doesn't recursivelyvalidate the signature of the callable object, because lambdafunctions cannot have typeannotations. (https://peps.python.org/pep-3107/#lambda)	1
[TOPI] Add left_shift, right_shift, clip, cast (#504)* [TOPI] Add left_shift, right_shift, clip, cast* [TOPI] Add test* [TOPI] Fix	0
Fix int8 cuda kernels on older SM versions (#11389)* Fix int8 cuda kernels on older SM versions* Update target.py* Simplify initialiasation of do_tensorize* Simplify initialization of do_tensorize dense* Simplify initialization of do_tensorize in group_conv_nchw* Fix tensorize for conv2d_int8 as well.* Try to make linter happy* make linter happy* Fix wrong commit to auto_scheduler	0
Remove redundant item from langref/relay_op.rst (#2192)Remove redundant item `tvm.relay.sigmoid` from langref/relay_op.rst	4
fix a bug in convertSSA. (#6785)	0
[PRINTER] Fix the repeatitive cast in scripr printing (#8531)	0
Separate fusion and Compilation (#1564)* Separate fusion and compilation* fix description of graph_fuse.h* fix lint* fix @masahi 's comments, move fusion out of target* fix graph passing and make fused_entries singula in graph attr* fix typo* fix some comments* run test again* remove rvalue for graphfuse and graphfindfusiablegroups	1
Removed a manual file handler pitfall (#9435)	0
Add missing cmath header (#287)f9ed337552a26cc55c7c0fb22cd54839ee13f19c added a call to std::logwithout including cmath	2
Add test case for expr_functor.py (#2632)	3
[ci] Add filter to teams (#11455)This improves the parsing to avoid issues like in #11454commit-id:53a06ab3Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Frontend][MXNet] argmax, argmin ops support (#2048)	1
Add µTVM Zephyr support + QEMU regression test (#6603)* Split transport classes into transport package.* Introduce transport timeouts.* black format* Add metadata-only artifacts* Simplify utvm rpc server API and ease handling of short packets.* add zephyr test against qemu* Add qemu build config* fix typo* cleanup zephyr main* fix nonblocking piping on some linux kernels* don't double-open transport* validate FD are in non-blocking mode* gitignore test debug files* cleanup zephyr compiler* re-comment serial until added* remove logging* add zephyr exclusions to check_file_type* add asf header* lint* black format* more pylint* kill utvm rpc_server bindings, which don't work anymore and fail pylint* fix compiler warning* fixes related to pylint* clang-format again* more black format* add qemu regression* Fix paths for qemu/ dir* fix typo* fix SETFL logic* export SessionTerminatedError and update except after moving* fix test_micro_artifact* retrigger staging CI* fix jenkins syntax hopefully* one last syntax error* Add ci_qemu to Jenkinsfile* build in qemu* address liangfu comments* fix new bug with list passing* retrigger CI	4
Duplicate likely nodes added when loop axis split unevenly (#5084)* [TE][Schedule] Duplicate likely nodes removed* [1] Test case added* [2] Lint error fixed* [3] Review comments handled* [4] Review comments handled	0
[Analysis] Exposed Analyzer::CanProveEqual to Python API (#11102)* [Analysis] Exposed Analyzer::CanProveEqual to Python APIChecking for `analyizer.simplify(lhs-rhs) == 0` was a frequent patternin Python unit tests, and already had a utility function in the C++public API.  Exposing this utility function to Python allowed thispattern to be cleaned up.* Replaced more cases of .simplify with .can_prove_equal	4
[RPC] Refactor, introduce tracker (#1080)* [RPC] Refactor, introduce tracker* [RPC] Change RPC hand shake convention, always get remote key.* fix lint	0
Documentation correction (#665)Readability.	2
[APP] ROCM RPC (#1155)	5
[RELAY][OP] roi_align operator alter layout (#6443)* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layoutCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[CodeGen][CUDA] Vectorization for intrinsics (#5101)- This allows to emit vectorized loads/stores  for CUDA math intrinsics.- A few intrinsics should be lowered as CUDAMath not CUDAFastMath ones.- Fixed the code block identation.	0
update template (#1233)	5
add orderedset to Ubuntu python package list (#1491)	1
[BYOC] JSON Runtime with DNNL End-to-End Flow (#5919)* json runtime* json dnnl WIP* fix ArrayNode usages* Support composite functions* DNNL json runtime: conv2d/add/relu/dense/bn* add a more complex example* fix bias memory issue* rebase to upstream* merge to metadata module, remove the unused driver* handle constant* support composite functions* support DNNL constant* clean up* Simplify dnnl user code* GetDataSize* fix dense bug* improve cmake* zero copy* add unit test* move json to contrib/json* fix cmake* lint* max_digits10 for fp serialization* only keep base getfunction* fix lint* zero copy for all data entries* address comments* enable ci* address comment; fix bug* address commentCo-authored-by: Zhi Chen <chzhi@amazon.com>	1
[Relay][Pass] Add submodule extraction pass (#4960)* rebased* fix lint	0
[Relay] remove unneeded VisitExpr (#3239)	4
Fix compute library installation on AArch64 (#8371)* Fix compute library installation on AArch64* empty	0
[ONNX]Mod operator, bug fix (#6160)* Onnx mod, bug fix* Added comment for the mod/floor_mod behaviour difference between numpy & relay	1
[ci][docker] Send a PR to bump the Docker images nightly (#11813)See #11768 for detailsThis adds a GitHub Action to check for the latest images on Docker Hub via the Docker API and update the `Jenkinsfile` accordingly. It sends this in as PR for a committer to review and merge.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
finish tensor dom infer	5
[Pytorch] Add quantized::leaky_relu (#11729)* emptycommit 2nd try* add operator and test* example output* lint with black* register param index* remove assert as it is a warning in torch* fix algo bugCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>	1
[LLVM] Create fixed vector size according to latest LLVM12+ changes (#6717)The vector handling code in LLVM keeps evolving to accommodate scalablevectors. As a result, code related to vector sizes changes quite often.	4
[Adreno] Update conv2d_nhwc test to use winograd (#12214)	1
correct doc (#11439)	2
[Relay][Prelude] Use the Relay parser to define the Relay prelude (#3043)* Add ability to load Prelude from disk* Port over id* Define compose* Linting errors and style changes* Eliminate unnecessary parens* Rename identType to typeIdent (makes more sense)* Another unnecessary paren* Bump the version number for the text format* Ensure .rly (Relay text files) are permitted* Correct release number and simplify grammar rule* Correct load_prelude docstring* Corrections to _parser* Add Apache headers to prelude source file* Remove test_prelude (redundant)* Correct misleading error message* Add check that parser is enabled in Prelude* Commit pre-generated parser, ensure generated files are treated as binaries, and have parser tests always fire* Permit parser files and git attributes files* Exclude gitattributes and parser files from apache check* Another attempt at appeasing Apache audit checker* Corrections to rat-excludes* Apache should be truly appeased now* Ignore Relay parser files by name* Mark parser files as generated so they don't show up on Github* Add parsing helper function for tests* Mark parser files as not detectable	2
[Hexagon] Remove double ".hexagon.hexagon." from registered names, NFC (#10624)	4
[Build] Add CUDA_VERSION and GIT_COMMIT_TIME (#8372)* [Build] Add CUDA_VERSION to libinfo* add git commit time	1
[TOP] Add comments about optimizable ops (#10)	1
Gather operation with indices as tensor expr in TFLite frontend (#6168)* gather with indices as tensor exprAdded handling of indices as tensor exprto gather operation, unit tests amendedCode cheking out of boundary error refactoredin more "pythonic" way. Fixed bug in negativeaxis value normalisation* replaced with get_tensor_expr	1
safe to remove thread related headers? (#3713)	4
Default value for graph_runtime Init lookup_linked_param_func (#7676)	2
[Docs] improve the doc of release (#6091)	2
create function.py (#5087)	1
[ARITH][BACKPORT-0.6] fix a min/max simplify bug (#5749)* fix a min/max simplify bug* fix cpplint* turn into oposite when c1val<0 and add more case* fix c1=0Co-authored-by: xqdan <danxiaoqiang@huawei.com>	0
[CODEGEN] Fix vector element access in metal (#872)	0
[Relay][Frontend] Add slice axis op in mxnet converter (#2706)* Add slice axis op in mxnet converter* Fix lint	0
[CI] Fix Vitis-AI tests when USE_VITIS_AI flag set to OFF  (#10802)* Register relay.ext.vitis_ai.available function* Fix vitis-ai tests when running with USE_VITIS_AI OFF* Replace skip_test with pytest skipif* Add a function to see if vitis_ai is available* Use requires_vitis_ai function for running tests	3
[TFLite] Add transpose_conv to TFLite parser (#4440)	1
Fix pylint 2.2.2 gripes. (#2642)	0
Acknowledge related projects (#465)* [CODEGEN] Redo CodegenLLVM.* Add remarks about origin of the passProperly acknowledge related projects* Fix and expression	0
[RUNTIME] Add compile_shared option to linux compile utility fn (#5751)* feat: Add compile_shared option to linux compile fn* feat: Add compile_shared option for linux compile util fn* fix: Fix minrpc testcase use executable compilation* fix: Fix binutil case where call create_shared to create executableCo-authored-by: baoxinqi <baoxinqi@4paradigm.com>	1
[SCHEDULER, HW] Auto scheduler for conv2d, hardware generation (#20)* Hardware generation fixes/sweep, auto scheduling for VTA conv2d* Hardware generation fixes/sweep, auto scheduling for VTA conv2d* derive hw spec from config file* up to date hardware spec	5
[microNPU] Fix bug in microNPU demo app (#10930)* Fixes a bug in convert_image.py where the uint8 range image was cast to int8 without subtracting 128 first* Updates run_demo.sh to use an image that previously failed to be classified correctly but is now successully classified	0
Add 'get_num_inputs' to GraphRuntime (#6118)	1
[VTA][Refactor] Introducing VTA_HW_PATH for easier migration (#5163)	4
[Relay/TOPI] Added dilation_value attribute to dilate operator. (#6550)* Added dilation_value attribute to dilate operator of Relay/TOPI.  (Enables custom value for dilation, instead of always 0)* Added tests for dilation_value of dilate operator in Relay and TOPI.	1
Fix more ONNX URLs (#10220)PR #10218 was not enough for this fix so this should probably run through full CI to make sure it got everything.cc @mousius @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Change Integer Implicit Conversion Rule to C Standard Way (#8733)	4
Update jenkins (#890)	5
Add yolov3-tiny to the tutorial. (#3674)	1
[TVM][RUNTIME] dependencied update for ndarray. (#1431)	5
[Topi] Cortex-M DSP support (#9233)Co-authored-by: Sergey Smirnov <Sergey.Smirnov@mir.dev>Co-authored-by: Ekaterina Bern <Ekaterina.Bern@mir.dev>Co-authored-by: Mikhail Trubnikov <Mikhail.Trubnikov@mir.dev>Co-authored-by: German Tretiakov <german.tretiakov@mir.dev>Co-authored-by: Ilya Gozman <Ilya.Gozman@mir.dev>Co-authored-by: Alexey.Yazev <Alexey.Yazev@mir.dev>Co-authored-by: Ilya Gozman <92577591+ilyag-grovety@users.noreply.github.com>	1
[REFACTOR][TIR] Migrate most of low-level build to use the Pass Manager. (#5225)* [REFACTOR][TIR] Migrate most of low-level build to use the Pass Manager.- SplitHostDevice- ThreadSync- BindDevice- LowerThreadAllreduce- Provide a temp fix for printing IRModule with PrimFunc before the formal text printer.* Address comments, fix tests.* Fix relay tests* Explicit move	4
[µTVM] Rev ci-qemu to 0.02 (Introduce onnx python dependency) (#7728)* Fix ci-qemu build, add ONNX* rev ci-qemu to staging	1
[CONTRIB] TVM download utility based on urllib2/urlib.request (#1313)moving nnvm/testing/download.py to python/tvm/contrib/download.py to be used as a general TVM download utility	1
[TIR] Introduce BufferLoad/Store (#5205)Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>This PR introduces BufferLoad/Store to TIR. The new nodes will replaceProvide and Call with Tensor arguments in the subsequent refactors.	4
[RUNTIME] Only checks the custom data type if it is bigger than the specified range (#3471)	5
[OpenCLML] CLML Profiling fixes corresponding to OpenCL Timer recent … (#12711)* [OpenCLML] CLML Profiling fixes corresponding to OpenCL Timer recent changes.* [OpenCLML] Review comments.* * review comment	4
[DLPACK] fix flaky ctypes support (#2759)	1
[MetaSchedule] Task Extraction (#9382)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
Re-organize the test cases for tensorize. (#736)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.* Re-organize the tensorize test cases, and add a new case for none-resetmode.* Fix a typo.* Delete the unit case because merged it into test_schedule_tensorize.py already.	3
fix typo (#12183)* fix typo* fix typo	2
[RELAY][OP] Split (#1876)	5
[ci] Add branch protections to .asf.yaml (#10964)Moving these into the repo means we will be able to change them at-will.`tvm-ci/pr-merge` will change soon into `tvm-ci/pr-head` to fix anunrelated bug, but codifying it here means we can more easily coordinatethe change.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[NDArray] Update runtime.TVMArrayAllocWithScope to use ShapeTuple (#10728)`runtime.TVMArrayAllocWithScope` predates the introduction ofShapeTuple, and its use simplifies the `tvm.nd.empty` function.  Thetwo modified locations are the only occurrences of the string"TVMArrayAllocWithScope" in the repository, so no other call sitesshould need to be updated.	5
convert full-width characters to half-width characters (#11112)* `）` -> `)`* `】` -> `]`* `、`* `，` -> `,`	5
[Frontend][PaddlePaddle] Add some activation、elementwise  and reduce operators (#9370)* add activations and unary operators* revert modify of slice* add test cases* disable signal capturing in paddle framework	1
[AUTO_SCHEDULER] Only run rewrite layout tests on CPU (#10717)Set the layout rewriting tests to only run on "llvm" and "llvm-device=arm_cpu" as layout rewriting is only supported on CPUs.Currently, the arm test will not be run on arm CI because integrationtests are not enabled.	0
fix (#3769)	0
[Topi][Hexagon] Implement Cast F32ToF16 and F16ToF32 Slice Op (#11561)	5
Fix CallNode Rust binding (#9381)Was being constructed as VarNode.	0
[Docker] Move psutil installation to ubuntu_install_python_package (#10615)Previously, psutil was installed as part of `ubuntu_install_redis.sh`,which was included as part of the ci_arm, ci_gpu, ci_gpu, ci_qemu, andci_i386 docker images.  However, it was not included as part of theci_hexagon docker image.  Since this is a more general python packageused in terminating rpc server/tracker children, moving it to the moregeneral location.	4
[Docs] Convert Layout pass. (#4664)* [Docs] Convert Layout pass.* Address comments. Section 3 massaging.* Address comments.	1
[ETHOSU] Add early simplify to fix LoopPartition (#9387)* [ETHOSU] Add early simplify to fix LoopPartitionCertain loops aren't correctly partitioned if the loopcondition hasn't been simplified. This can happen whena copy loop is split by a non-factor. To fix this, anadditional simplify pass is added to the TIR pipelineprior to LoopPartition.Change-Id: Icd4ff14648ccaed41384da50c6d183a122b30048* Fix linting againChange-Id: I9c9dc2ee2c679861866b23531e88584b94198e51	4
[TUTORIAL] Add gpu instructions and results to deploy_sparse (#7298)	1
[CODEGEN] NVPTX backend. (#392)* [CODEGEN] NVPTX backend.* Fix pylint* use fix	0
Fixed a bug in the convert_fully_connected() function (#10371)In case we need to change the output shape, need to convert the output_shape tuple to list before the change.	4
[Hexagon] Allow execution on target or simulator from HexagonLauncher (#10454)Setting ANDROID_SERIAL_NUMBER=simulator will execute the tests onsimulator instead of a hardware device.This patch also introduces an environment variable HEXAGON_RPC_LIB_DIRto specify the location of the hexagon_api binaries. If unset, thecode will look for the binaries in the same way as before this patch.	1
[ci] Override Request in pytests (#11974)This follows on #11839 to apply it outside of docs and to tests running under pytest insteadCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[RELAY][GRAD] handle Tuple/TupleGetItem in first order gradient (#5946)* handle Tuple/TupleGetItem in first order gradient* Unify MultiOnes/MultiZeros.	1
Fix typos (#8787)Fix a couple of typos in comments about the IR/AST node reflection codeand a typo in a comment about the main member of the TVMModule struct.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	2
add DeviceName to ROCm api (#4437)	1
Revert "[Relay][TOPI]Fix meaning of conv2d_transpose output_padding parameter (#4318)" (#4708)This reverts commit dcf7fbf1f962569e78c624755b2d612fffa81ada.	4
[Relay][QNN] Moving Conv, Dense, Concatenate InferTypes to header for sharing. (#3783)	5
[Relay][Runtime] Implementation of Relay VM (#2889)* Implement the virtual machineCo-Authored-By: wweic <ipondering.weic@gmail.com>* Fix rebase build issues* Reorganize vm.py and fix allocator bug* Remove compiler* Remove tests* Remove backend/vm/vm.cc too* Fix docs* Fix doc* Fix doc* Add vm docs* Remove change to dead_code.cc* Remove Relay logging* Remove reduce* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Reformat* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Address feedback* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Apply suggestions from code reviewCo-Authored-By: jroesch <roeschinc@gmail.com>* Fix a couple outstanding comments* Last couple comments* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Address code review feedback* Fix final comment* Address comments* Error reporting and example* add Const* Explicitly delete copy assignment operator* Fix rebase* Pass 3rd arg to fusion	4
[FRONTEND][TFLITE] Add support for TFLite_Detection_PostProcess (#4543)* [FRONTEND][TFLITE] Add support for TFLite_Detection_PostProcessThis adds support for the custom operatorTFLite_Detection_PostProcess which is commonly used inobject detection networks such as SSD Mobilenet. Itonly adds support for when use_regular_nms = False.Change-Id: I819b253c0eb6f0fa55da65d2634e09359b888828* Added a test for the tflite custom opChange-Id: Ie5baa092deae9a8bcffd2ebd9f6d346b90e58afd* Removed trailing commaChange-Id: Ib08f02b5f1a59a883048bfb36e4321152cd2e7f2* Added spaces between divideChange-Id: If1171fc03d211a809cedeb800804394972af4060* Formatted commentChange-Id: I3ce7e69b8d2c73aec57369c1c64ea1eec07f087b* Reduced line length in testChange-Id: I49eaafc3369070f8f3e85fbb965ad20972096c68* Set random seed for testChange-Id: I542a787d11422ea83c52147b2cb1144fcef0dd77* Fixes to styleChange-Id: I2971b8ecebe08c882b2481a99f67cfbe515e0b1f* Assert for incorrect number of inputsChange-Id: I393f3b3b62be73e427498d98456fb1d5a214e0af* Change comparison to pass lintingThe linter was updated, so I needed to fixa small style issue as a result.Change-Id: Ia3c954565a00de92e7fb1912eae9ed9875d60c7c	4
[microTVM] [docs] Point micro_train tutorial links to official repos (#11715)* Point micro_train tutorial links to official repos	2
Fix doc after moving to unified IR (#4835)	4
[USMP] adding support for U2 and U3 usecases (#10193)This commit adds a MemoryPools argument forthe compilation flow according to RFC0029.Moreover, it is used to provide support forexternal pools from the application layerthat could be pinned for different memoriesand/or be reused between multiple inferencesof a model.	5
[TensorIR][M2a] Reorder (#8767)This PR is part of the TensorIR upstreaming effort (#7527), which adds a schedule primitive: reorder.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[BYOC] Two helper passes for external codegen using RelayToTIR custom pass machinery (#11474)* [BYOC] Two helper passes for external codegen using RelayToTIR custom pass machinery(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).For reasons explained in the above thread I'm moving CUTLASS to be IRModule-at-a-time external codegenusing a custom RelayToTIR pass instead of the traditional function-at-a-time external codegen usinga relay.ext.cutlass registered function. This means some of the rewriing done on-the-fly by LowerTEPass nowneeds to be done by the custom pass directly. This PR supplies two passes which ease that burden: - Before starting the CUTLASS-specific processing, make sure all "Compiler" attributed functions have   unique global definitions (ie are outlined). Though functions start in this form after BYOC partitioning,   under Graph and AOT compilation flows those functions are then inlined to pass through the 'codegen' keyhole   which assumes the whole model is just one self-contained main function. This pass will undo that. (I gave up   trying to just remove the inlining in the first place.) - After the CUTLASS-specific processing the now compiled "Compiler" attributed functions need to marked as   'extern'. The te_compiler.cc uses the "ExternalSymbol" attribute for that, but since a) the symbol name   is never needed, on the presense of the attribute is significant downstream and b) "ExternalSymbol" is   easy to confuse with "global_symbol", I just replaced "ExternalSymbol" with "Extern" with an Integer(1)   (cf "Primitive"). The outlining pass is a little more general than necessary because it (will also) be used by Collage to rewrite the IRModule into optimally partitioned form while making maximal reuse of partition functions. Hence the abstract GlobalSymbolCache.* - Andrew's comments	1
Adding support for dequantizing from int32 to float32. (#4130)	1
Fix tvmc run error message when inputs aren't found. (#10017)	0
[Bugfix] Fixed bug where shifting by out-of-bounds value results in no compute code being emitted. (#5115)* Fixed bug where shifting by out-of-bounds RHS values results in LLVM to codegen nothing. Added regression testcase* Updated testcase to be more precise.* Fixed testcase	3
[Profiler] Sort columns in table and csv output (#9300)This should fix some problems around flakey profiler tests based onorder of columns.	3
Remove MemoryPlan from VM passes (#7361)	4
cleanup (#11659)	4
Add unit tests for HexagonBuffer (#9736)* Add unit tests for HexagonBuffer* fix build error for signed / unsigned comparison	0
[Relay][TOPI]Fix meaning of conv2d_transpose output_padding parameter (#4318)* Add output_padding to generic* Add output_padding to the reference impl* Add output_padding to arm_cpu* Add output_padding to the test* Add output_padding for cuda* Add output_padding for x86* Make use of the new output_padding argument in Relay* Adjust conv2d_transpose Relay test* Fix lint errors* Fix the VTA declaration of conv2d_transpose* support for output padding in conv2d transpose* some output padding will break IR pass* Fix new conv2d_transpose test* Update tophub* Fix conv1d output_padding too.* Fix the conv1d_transpose reference function.* Fix the cuda impl* fix the topi test for conv1d* Update the versions in tophub.pyCo-authored-by: Thierry Moreau <tmoreau@octoml.ai>	5
fuse constant padding into conv kernels (#7515)* fuse constant padding into conv kernels* change the kernel to support other layouts* add channel-last test* add a comment about bailing early	1
[relay][frontend] clean up tf frontend (#3710)* clean up tf frontend* fix get_relay_op	1
Update CONTRIBUTORS.md (#8837)TVM is no longer in the Apache Incubator; moving mentors to the end of the doc.	2
[Test] Fix flaky LocalRunner test due to a small timeout (#9181)	3
[µTVM] Modify reference VMs to support new µTVM demo (#7001)	1
[CONTRIB] MPS DNN Dense (#615)* mps* update	5
Fix function annotation (#9474)	1
[TFLite] Add support to int16 data type in TFLite frontend (#10915)* [TFLite] Add support to int16 data type in TFLite frontendAdd support for int16 data type and int64 biases/accumulators inthe TFLite frontend.Adjusts TFLite tests to cover int16 convolutions and element-wise;Fixes a minor typo negtive->negative in the element-wise tests.* Update src/relay/qnn/op/convolution.ccCo-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>	5
Safe remove tmpdir (#4781)	4
[AUTOTVM] TOPI integration for ARM CPU (#1487)	5
[MetaSchedule] Add Gradient Based Task Scheduler (#10366)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[TVMScript] Enable assignment statement without type annotation  (#10736)* Add test* workaround mypy* replace assert with condition	3
[FRONTEND][TENSORFLOW] Optimized tensorflow testcases (#1546)* [NNVM][TENSORFLOW] Optimized tensorflow testcases* Replace Constants with Placeholder* Review comment fix	0
[AutoTVM] Compatibility improvement with XGBoost v1.3.0 (#7076)	1
[C++ RPC] fix typo to keep same with source code (#6220)Signed-off-by: windclarion <windclarion@gmail.com>	2
[TIR][USMP] Added buffer info extraction pass (#8468)* [TIR][USMP] Added buffer info extraction passThis commit adds a pass that takes the main (call graph of operators)TIR PrimFunc and each operators also as TIR PrimFunc. The pass willtraverse through all TIR PrimFunc starting the from main. Thereafter,it will extract information from tir.allocates. Among the information,the liveness conflicts are reported.* Added test for a linear model* Added test for parallel/serial mixed for loops* Added test for a substructure of inception-style model.* Exposed buffer_info creation to python* Added member functions to update pool info* Unit tests to cover functionality of buffer_infoChange-Id: I5e163ac3e83c830629a5d34ed4407c9962701c60* [TIR][USMP] Added buffer info extraction passSwap key-value pairs of returned values of the buffer_infoextraction pass.Change-Id: Ia4f7289592bc776ef6189a41a7891038751bf31f* [TIR][USMP] Added buffer info extraction passUpdating the USMP utility tests to include teststhat test creation of PoolInfo and PoolAllocationObjects.Change-Id: I5d349d0ffcac6b0160072d832dd9d5418699228e* [TIR][USMP] Added buffer info extraction pass* Removing the unnecessary header : include/tvm/tir/usmp/analysis.h* Some nits and cleanupChange-Id: Iac3ddd9428c56cd8ef49cf643e797bf6fdf4e97a* [TIR][USMP] Added buffer info extraction pass* Change the class data members to have a trailing underscoreChange-Id: I71809b3c73b0bc0cd133fad1392ae8c17c895ee4* [TIR][USMP] Added buffer info extraction passAdding more documentation for data structuresand the approachChange-Id: Ide2bfffaeff9add86853b6992017264e5d796299* [TIR][USMP] Added buffer info extraction pass* Added more documentation* Added functionality to handle multiple calls  for the same PrimFunc with a test.Change-Id: Ib7c27b3cf17f415067a224f1e57d8b928f4c7c6f* [TIR][USMP] Added buffer info extraction pass* Attaching targets to PrimFuncs in the util test caseChange-Id: I82960512659a346f6242b2b5789ec1120f8ea2cf	4
Docs and Readme updated as per new namespace change (#4989)	4
Adding support for Hexagon User DMA Engine (#10217)* initial hexagon user dma impl* Hexagon User DMA descriptor, instruction and register headers* Synchronous 1D DMA working* HexagonBuffer unit tests passing with memcpy* cleanup* comments and orgnanize code* format and lint* init function + other code review feedback* add ifdef hexagon around inline asm	1
[TVM] Fix segfault for CanonicalSimplify(x % -1) (#2194)	0
[TUTORIAL] add tutorial for mali gpu (#313)* add tutorial for mali gpu* update submodule version* fix doc error* fix server error* fix server in test	3
Jekyll (#3262)	5
[LLVM][WINDOWS] Recover windows support for the latest LLVM (#6698)Windows COFF requires comdat information to support weak-like linkage(via any).This patch fixes the windows LLVM support after LLVM-8.	1
Add support for sharing params of operators in tensorflow frontend (#5042)	1
Fix broadcast add and subtract grad (#2465)	1
fix missing qparams in aten::upsample_nearest2d (#7646)	2
Add CONCATENATION to tflite frontend, support Inception V3 (#2643)* Add CONCATENATION to tflite frontend* fix typo* Fix codestyle* Fix code style* simplify convert map* Update	5
[WIP] [RPC] clean up uploaded modules (#2121) [RPC] clean up uploaded modules	4
[Hexagon][LLVM][CodeGen] Make CodeGenHexagon a subclass of CodeGenCPU (#10908)* Initial pass at making CodeGenHexagon a subclass of CodeGenCPU* More cleanup of CodeGenHexagon* Remove unused func_handle_map_ from CodeGenHexagon* Remove CreateCallExtern and CreateCallPacked from CodeGenHexagon* Code style fixes* Run clang-format-10 over codegen_hexagon.cc	1
relax tolerance for dlpack test (#7325)	3
[TensorIR] fix region cover check (#9810)	0
delete declaration of unused op_node (#6102)	1
[CODEGEN] Allow link additional module (#559)* [CODEGEN] Allow link additional module* fix py3* add register back	1
[CodeGen][CUDA] Enhance CUDA codegen for SelectNode (#4983)- This patch allows CUDA backend to emit correct code for  selects with vector conditions, which may be produced  by floordiv op lowering etc..- This already works for llvm BE, as llvm select instruction  supports vector conditions.Signed-off-by: Wei Pan <weip@nvidia.com>	1
[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async* add missing doc* black* missing src* clang format* clang format* check against nested async scope	2
[DOC] API doc organization. (#90)* [DOC] API doc organization.* remove breathe for now	4
[ROCm][Auto scheduler] Support Auto scheduler and NHWC convolution on ROCm (#7038)* add nhwc + winograd support to rocm strategy* support rocm hw parameters in search task* run analysis pass for rocm too* run black* pylint fix* use IsGPUTask functionCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	1
enable shape inference with hint func (#84)	5
[ci] Use smaller ARM nodes for build/test (#11445) (#11457)This applies the new instances from https://github.com/tlc-pack/ci-terraform/pull/32Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: driazati <driazati@users.noreply.github.com>	1
ROCm installation guide (#1216)	2
[Runtime] Pipeline Executor Second patch, configuration load and executor export/import. (#9108)* [pipeline executor] Add configuration load function and pipeline executor export,import function.* address review comments.* polish comments and doc string.* address review comments.* address review comments.* Change mod_idx start from 0, remove mod_idx - 1 logic.* address review comments.* polish documents.* adress review comments* address review comments.* address review comments.* polish the document.* address review comments.* address review comments.* polish comments.* Triger build.* address review comments.* address review comments.* fix grammar issue.* polish documents.* add single global binding check.* address review comments.* trigger build.	1
[SPIRV] Support Bool buffer argument (#7591)	1
fix fuse reduce_axis error in pooling schedule (#482)	0
Ensure interpreted functions can take values that are not TensorValues (#3015)	1
[TVMC][Relay] Introduce executor and runtime parameters (#9352)* [TVMC][Relay] Introduce executor and runtime parametersThis introduces `executor` and `runtime` into the various entrypoints but also into `tvmc` as `--executor` and `--runtime`. This touchs a lot of files and I've tried to update anywhere as necessary.Notable, executor code generators now accept the initial `IRModule` rather than creatingit themselves so it can be annotated once.Validated the demo application continues to classify the tabby cat withnew CLI options.* Correct Graph Executor Python API	1
[hexagon][testing] add test-skip logic; fixes (#11737)- Skip Hexagon benchmarks whenever the env. var `ANDROID_SERIAL_NUMBER`  has the value `simulator`.  This is a temporary hack to prevent the CI pre-commit hook from  running benchmarks, due to the extra time required.- Fix a bug where the elementwise-add benchmark code was broken by  an earlier change to the `HexagonLauncherRPC` class.- Rename `benchmark_elemwise_add.py` to `test_benchmark_elemwise_add.py`  so that it's noticed by the CI test infrastructure.  (CI tests are sometimes run in contexts _other than_ the pre-commit  hook.)- Miscellaneous small changes to  `tests/python/contrib/test_hexagon/benchmark_util.py`.	3
[RELAY][PASS] Common subexpression elimination (#2639)	4
[Bugfix][TOPI] conv2d_transpose bugfix (#3138)* deconv tests* deconv bug fixed for certain cases tests added	1
fix fuse over functions that are handled by external codegen (#5365)	0
[DARKNET FRONTEND]Batchnorm added as part of Dense op for running rnn model for next wo… (#1385)	1
{relay,topi}.reinterpret support (#3599)= MotivationIt's useful to expose the tvm::reinterpret functionality to Relay/TOPI users, asthis allows them to build (fused) operators leveraging the bitwisereinterpretation of an operator. An example is approximate transcendentalfunctions, which can be implemented similar to:```.py    def C(x):        return relay.expr.const(x, "float32")    def approx_exp(x):        x = relay.minimum(relay.maximum(x, C(-88.0)), C(88.0))        x = C(127.0) + x * C(1.44269504)        xf = relay.floor(x)        i = relay.cast(xf, "int32")        x = x - xf        Y = C(0.99992522) + x * (C(0.69583354) + x * (C(0.22606716) + x * C(0.078024523)))        exponent = relay.left_shift(i, relay.expr.const(23, "int32"))        exponent = relay.reinterpret(exponent, "float32")        return exponent * Y    def approx_sigmoid(x):        # <2.0e-5 absolute error over [-5, 5]        y = approx_exp(x)        return y / (y + C(1.0))    def approx_tanh(x):        # <4.0e-5 absolute error over [-5, 5]        x = x * C(2.0)        y = approx_exp(x)        return (y - C(1.0)) / (y + C(1.0))```See unit tests for implementations of these approximate transendentals.	3
Add parser support for ReLU tflite operator (#4022)	1
add parser support for TANH tflite operator (#3996)	1
[Relay] Incorporate TypeRelations into more tests (#1792)	3
Fix tf parser (#5794)	0
[CUDA][PASS] conv2d NWHC/HWNC legalize tensorcore (#8222)* add conv2d leg* minor fix* fix pylint* fix pylintCo-authored-by: wangyucheng <wangyucheng@sensetime.com>	1
Add prim::device op (#5584)	1
Fix missing cublas libraries (#1281)	0
[Bugfix] Fix type code error for StringImm (#3050)	0
[TensorIR] CreatePrimFunc from TE (#7987)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	1
[AutoTVM] Use PopenPool in XGBoostCostModel (#8820)* replacd multiprocessing.Pool with PopenPoolExecutor* add initializer func* static init func* address comments* linting* fix tests* address comments	1
Add ShapePattern and DataTypePattern (#5760)	5
[TOPI] Using x86 schedules for ARM conv2d. (#5334)	1
[tvmc] unify all logs on a single logger 'TVMC' (#6577)	2
[TIR] Support Return in TIR (#7084)	1
[BYOC] Fix build with TensorRT 8 (#9047)* fix compile error missing noexcept in overwridden methods* remove depricated builder method call	4
[Cleanliness] [Easy] Make TVM leak-sanitizer and Wnon-virtual-dtor clean. (#2046)	4
[Test] Add missing test for fast erf (#6058)* add missing test for fast erf* trigger ci	3
DLPack Conversion API (#1573)	5
[microTVM] Add platform version check to template project (#9274)* add platform version to project template* fix arduino cli version on qemu* specify arduino/zephyr version everywhere* cleanup* address comments* fix warning message* fix typo* trigger* trigger* address comments* trigger* trigger	1
Misc refactor on graph runtime, layout node (#2557)	1
[Relay] Remove in-place modification of attributes in layout transform (#8309)* stub* mnist test working* porting InferCorrectLayout* compiles with new infer layout* remove log* fix qnn concat* do not run dense pack alter op test on gpu targets* cleanup* add test* cpplint* CHECK -> ICHECK* doc update* restore try catch* split inferred_layout into seperate fields* Update InferCorrectLayout functions following struct field change* fix cpplint	0
[TOPI] Treat undefined elements as constants in Array (#7232)* [TOPI] Treat undefined elements as constants in Array* Add a checker* fix* add test case	3
Update ci to new location (#1552)	1
fix onnx shape dtype (#4528)	0
[RELAY][FRONTEND][TF] Fix FuseBatchNorm output cast error if need_cast is True (#4894)	0
[RPC] Android RPC Performance Regression Fix, Update Android RPC to use Tracker (#1457)	1
[LLVM][Hexagon] Revert LLVM header change for version 14 (#10006)* Revert LLVM header change* Trigger	4
support false-positive fast math (#12702)	1
[REFACTOR][PY] Establish tvm.arith (#4904)	4
[ci][docker] Nightly Docker image update (#11857)This bumps the Docker images to the latest versions from Docker Hub.Co-authored-by: tvm-bot <95660001+tvm-bot@users.noreply.github.com>	1
[RUNTIME] update to make runtime copy type aware	1
[TVMC] Fix error while compile paddle model with tvmc (#11730)The tvmc command will throw a error while the passed path of model is not exist, But for PaddlePaddle model, it contains 2 file model_name.pdmodel and model_name.pdiparams, we only pass the prefix like inference_model/model_name.This pr is same with https://github.com/apache/tvm/pull/11108 Since the origin PR didn't update for a long time, I send this new PR	1
Fix InferCorrectLayout for dynamic upsampling and add a regression test (#6712)* add a regression test* fix dyn upsampling infer layout* fix lint	0
Support reshape for dynamic shape in tf converter (#4185)* Support reshape for dynamic shape in tf converter* Only allow reshape directly after shape function for symbolic input shape* Fix lint	0
More flexible conv2d_NCHWc_int8 generic operator. (#6714)	1
[microNPU] Add support for nearest neighbor and bilinear upsampling (#9841)* [microNPU] Add support for nearest neighbor and bilinear upsamplingAdds support for 2x2 nearest neighbor and bilinear upsampling. In thecase of bilinear upsampling with align_corners set to true, theupsampling size must be `2*input_size - 1` (as opposed to `2*input_size`).Change-Id: I95d215eabfaac983629dcdedcda2b90efb8e0ddf* rebase and add support for no-upsampling case.Change-Id: I840d8ee3671a40c5c99f22119442c349dbed39cf	5
[Relay, TF Frontend] Dilation2D operator support (#5033)* update docs for dilation 2d* dilation2d compute* dilation2d register* dilation2d rel compute* dilation2d strategy* dilation2d attrs* dilation2d generic schedule* dilation2d tf frontend support* dilation2d tf frontend test case* dilation2d test cases* pylint fixes* add exception for cuda target* Update docstring* Update docstring* change rates to dilations* removed unused param* merge master* Update nn.py* Update nn.py	5
Add instructions to run tests locally (#1868)	3
[TensorRT] Add transpose_a/b for TensorRT batch_matmul (#8607)* Add transpose support for tensorrt batch_matmul* Address PR comment* Refactor to add ONNX_DEFAULT_CONFIGS	5
[REFACTOR][IR] Allow Module to store BaseFunc. (#4678)Under the unified IR. We will allow a single IRModuleto store different function variants, such as relay::Function,ExternFunc, and low-level function.This PR changes relay::Function -> BaseFunc in the module fileto support multiple function variants.	1
Dynamic gpu tests, add dynamic strided slice to topi (#6870)* enable GPU tests for dynamic ops* strided-slice can't do 0-sized output tensors, remove test* move dynamic strided slice into topi* add python interface to topi dynamic strided sliceadd python interface, tests* autoformat* fix bad copy/paste* fix doc string* disable topk on gpu for now, remove invalid slice test	3
[microNPU] Remove xfail from test_clz (#10670)After #10640 we can re-enable the flaky clz test, mentioned in #10487.Change-Id: I35d474183239cad5f6c2eba35b55d4ca14869917	4
Small fix of the Depthwise Convolution example in python3 (#224)* fix for python3fix for python3* Update depthwise_conv2d_map_test.pyremove sys.append	5
[CI][CMSIS-NN] Running tests parallel using pytest-xdist (#12557)Introducing -n auto for CMSIS-NN tests to run them inparallel with pytest-xdist. This is needed because ofadditional parameterization done over cpu variants.Change-Id: I02e1b37ead0b0a562b5b1b2dacfeb3fdd7cc1ce3	4
[Fix Bug]fix the bug of tensorflow frontend when parsing Range layer (#9999)Co-authored-by: wangjiuyang <wang.jiuyang@intellif.com>	0
[MetaSchedule] Mark two tests as xfail (#12733)This patch marks two tests as xfail for further investigation:* test_meta_schedule_integration_extract_from_resnet_with_filter_func* test_meta_schedule_integration_extract_from_resnet	3
[Fix][TOPI] remove wrong fix in x86's dense_nopack operator (#8687)	1
fix (#788)	0
[Vulkan] Remove some interface block decoration (#8102)* Remove block decorator for shared/local variables* Fix lint	0
Add ssd op with ir builder (#1095)	1
[BACKEND] Explicitly allow specialization of FMA in llvm (#407)	1
[Frontend][TFLite] L2_POOL_2D operator (#5452)* TFLITE fill and splitv ops* l2_pool_2d op changes in comment* TFLite l2_pool_2d op added test case in main* TFLite L2_POOL_2D added check for quantized input	1
Fix Strided Slice Infer Layout (#6621)	5
[microNPU] Match requantize in min/max with activation pattern (#11010)* [microNPU] Match requantize in min/max with activation patternOptimizes a corner case where min/max + clip also produces a requantizeoperation. Previously the requantize was lowered separately as anidentity operation which is unnecessary. Now the quantization parametersfrom requantize will be used by the lowered min/max operation.Change-Id: Id740d975bd8ba2952f3444ce1061acef560d74d7* add random seed to legalization testChange-Id: Ic4ff78af94c3e8250dba8e3ce5c2775fcc7a17f6	5
[Relay][Onnx][Frontend] Add RandomUniform converter and tests to onnx frontend. (#8426)* Add RandomUniform converter and tests to onnx frontend.* Fix comments.* Remove weird import.* Add test against golden array.* Retrigger CI* Improve test comment.* Retrigger CI.	3
[EXECUTOR] Enable load executor remotely (#245)* [EXECUTOR] Enable load executor remotely* [EXECUTOR] Pipeline* Pass bytearray directly* Enable load dynamic library in rpc_server.py* Fix* lint* Return Module from remote side directly* Remove unused header file* Fix* fix	0
[TVMC] Updates TVMC tutorial with input shape information (#12031)The tutorial is currently broken, probably because updates in themodel, so we now need to pass input shape information.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>Co-authored-by: Liam Sturge <Liam.Sturge@arm.com>	5
[NODE][REFACTOR] Rename IRFunctor->NodeFunctor, use func pointer (#4247)* [NODE][REFACTOR] Rename IRFunctor->NodeFunctor, use function pointer for dispatching.Previously we used std::function for the functor dispatching.It introduces additional overhead and problems during dll destruction(of std::function).This PR changes the std::function to function pointers.This change a bit restrictions around the set_dispatch that we can get around,but will improve the general efficiency by reducing one level of indirection in the std::function.We also no longer need special marcos to register functions to the Functor.	1
use libtorch c++ distribution with c++11 strings in gpu image (#11346)* use libtorch c++ distribution with c++11 strings in gpu image* libtorch path* don't activate libtorch before merging the image	1
remove downloaded source code archive (#9879)	4
Contributing the STM32 port (#7742)* Contribute apps/stm32 application.* Removed a useless file.* STM32: Use Model Library Format in demo. Added test.* STM32: Added quantized mnist test.* STM32: Fixed apps/stm32/Makefile.* STM32: Added quantized models test.* STM32: Added tests to tests/scripts/task_python_microtvm.sh* STM32: Listed specific files with lint.* STM32: removed external copyright notices.* STM32: Added missing ASF copyright notice.* STM32: style fixes.* STM32: more style fixes.* STM32: fixed liny for C files.* STM32: Does extern C help with cpplint.* STM32: Fixed wrong LINT_C_FILE spelling.* STM32: Still some lint fixes.* STM32: more style.* STM32: More fixes lint+formatting.* STM32: cleanup.* STM32: style cleanup.* STM32: Moved ai_runner to the apps/stm32.* Alignment with PR 7742* lint cleanup.* STM32: Use crt_backend_api.c with standalone build.* STM32: Fixed the CI test.* STM32: style fixes.* STM32: Removed unused files.* STM32: Moved to crt_backend_api* STM32: style fix.* STM32: style fix.* Revert "STM32: Removed unused files."This reverts commit d72f8e5be3bae3c36da1758b6e61cd39ef7036c7.Undo changes to c_backend_api/c_runtime_api.* Revert "STM32: Moved to crt_backend_api"This reverts commit 6c0e66672cb636d47244e99e8efd893004acc382.Undo changes to the c_backend-api/c_runtime_api.* stm32: aligned to micro TVM structure.* stm32: improved the python style.* stm32: cpplint fixes.* stm32: Fixed the test* stm32: style fixes.* stm32: style fixes.* stm32: style fixes.* stm32: style fixes.	0
add reviewer - slyubomirsky (#3673)	1
[MergeComposite] Fix InferType when module contains Prelude (#5797)A function may refer to other resources in the same module, so keep  the content of original module when infering a function.	1
[LINT] handle more file types in ASF header (#3235)* Update add_asf_header.py* Update add_asf_header.py	1
[REFACTOR][IR] Initialize Unified IR Pass Infra. (#4702)Move the relay's pass Infra to ir.Keep FunctionPass in relay as it is local to the dialect.	4
[microTVM] Refactor RVM scripts and fix DNS network issue (#11741)* refactor scripts* address comments	1
[Runtime][PipelineExecutor] Setting CPU affinity for the Runtime of pipeline. (#10639)This patch add the function to set cpu affinity for the runtime of thepipeline. By using the said function, user for example can let the firstruntime of the pipeline to use the cpu [0,1] only, and the second runtimeto use the cpu [2, 3] only.	1
Ghost nodes in NNVM graph (#3290)	5
[MEMORY] Fix gcc 4.8 compact (#4461)	0
[CUTLASS] Support conv2d activation fusion (#9746)* Add cutlass conv2d activation (bias, relu, sigmoid)commit e4e273ae74a8e54ab1ae1414ce9b6bfcc2b3d530Merge: 0489d1418 77c938550Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:58:54 2021 +0900    Merge branch 'partition-constant-unbind' into cutlass-conv2d-fusioncommit 77c9385501595f804bd33b436aed0cc192059a10Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:58:18 2021 +0900    add testcommit ab01b3aae36ef88ec299ff5e9d1bbdf5fd7268f6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:55:06 2021 +0900    make constant binding in PartitionGraph optionalcommit 0489d1418c5f49b95ad220b8f0e57ba48443c6a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 21:52:29 2021 +0900    support sigmoid fusion (only fp32 accum for now)commit 3705bbd6b77ec8d8c416ab00738fe8b9c91e1535Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:50:58 2021 +0900    conv2d fusion test workedcommit 05b51c9f94c9a4f31afeb126c8749675be423892Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:34:10 2021 +0900    fix bias stridecommit 7cf40e719a8c1c464e2ee925018d8ed90b145dcdAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:01:21 2021 +0900    use nobetascalingcommit 274ec028845e296b012c23830a0be6f48cbee767Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 19:12:58 2021 +0900    adding fusion support to codegencommit 0de5ebdb2e318129a1a4d3a2e568849a993525c4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:39:08 2021 +0900    partition workingcommit c08bb38e3eea90168dc036cb4cbfbcdef288bac6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:24:42 2021 +0900    update testcommit 81bf9e600b1bbdbc27e0e8b54496fc09b33e0624Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:23:39 2021 +0900    add fused conv2d patterncommit 1c0bbb297da43dab75ad995afbcacd59e9fe4c87Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:29:03 2021 +0900    fix lintcommit 463574ce087ca0444b23d4f47baf8066b8fbd3dfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:28:38 2021 +0900    fixed conv2d checkcommit 588c5abe15abbf0339a7972d81b8235bc7460620Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 15:05:27 2021 +0900    update testcommit a447b57ade7c99da4efd38e1bdfc213a64a80fd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 14:54:52 2021 +0900    speed up profiling by removing initializationcommit 93cd039ba04dd80e887ec1a71f358cd86a1e5221Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:26:29 2021 +0900    fixed nhwc cudnn depthwise convcommit 6db71727f553ee2009c9d3feb2b019c24459f4d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:39:05 2021 +0900    add cachecommit f7d17a116acd80c57dfa04aa86577fa30898908dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:05:38 2021 +0900    removed im2col profiling for conv2dcommit b724f446d07030e45f30474a1e3c124f1541b496Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:57:54 2021 +0900    blackcommit fe4687b9f6d41eff66742cdde694680538a3d5e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:49:13 2021 +0900    fixed cmd arguementcommit ab114f5c3e1a3086255caccd6c0f5df09d7c3755Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:22:19 2021 +0900    conv2d profiler workingcommit 49ee61f5583f73f0d9b2c18c1861c90fa028f64aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 20:26:15 2021 +0900    add conv2d profilercommit 49e2c8918a1a2632f60c700675f12f9d83adef24Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:03:36 2021 +0900    do not offload depthwise conv2dcommit cd8367768229720020d81d597d751bfa95ec014eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:20:01 2021 +0900    lint fixcommit 870823c6d54114896aa9db91eb72c132cdef762fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:54:38 2021 +0900    add comment on IC == 3 casecommit 6b780db7f8059a9a58bfc257fbb9cf7cb60b72a2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:48:33 2021 +0900    check align on N dimcommit 308c4dac39f761ac157f032b9fe7028820980294Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:34:42 2021 +0900    fixed check functions for fused cases, run infer type before mergecompositecommit 8d6a1bfee26e9dfdeb1ab001aa89b43b9cd33d74Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:59 2021 +0900    test IC=3 convolutioncommit ffce47de724398222f672b220156b19c8f48a700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:16 2021 +0900    use align1 kernel for unusual channel cases (IC = 3 etc)commit 6cdf205a3451b5fa7ea0cbff0228ae1da775573aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:06:56 2021 +0900    add dtype and layout check in parttern matchcommit 7743cc6dde442b45d66b4be320268ed24f352422Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:53 2021 +0900    add sm75 kernels to sm80 profilingscommit efceccb994ec71bc4ba847c791fc4f696eb7f242Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:42 2021 +0900    skip legalize when batch size is dynamiccommit 65fbc0a0813cb4e980e53240f34f3ba18754ab99Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:36:36 2021 +0900    bug fix in im2col encoding* support batch norm fusion	1
make test_runtime_rpc use pytest.main() (#7482)	3
[Build] Reflect Compile-Time CMake Options into libtvm.so (#6280)* Initial comit* Address comments from @tqchen	1
Change rust installation profile to minimal (#9878)	5
[Torch] Fix converting torch slice op with dynamic slice length (#7549)* Fix converting torch slice op with dynamic slice length* use isinstanceCo-authored-by: masa <masa@pop-os.localdomain>	1
Add type code and bits to AllocWorkspace. (#831)	1
[CMSIS-NN] Fixed the case with repeating operands in the QNN binary ops (#11732)	0
[Relay/TOPI][Op] Add TopK operator (#3256)* init impl for topk* Fix cpu for topk* init cuda impl for topk* Add cuda for topk* fix* Add doc* update doc* lint* lint* lint* x* fix warning* [Relay] Add TopK in tf converter* Add frontend converter* fix	0
[AutoScheduler] Tutorial on auto-scheduling a network for GPU (#6882)* add a tutorial on auto-scheduling a network for cuda* fix typo* fix training time printing* fix lint* fix* upload logs* fix* use weighted sum as the default objective function* update ci logs* fix the bug in kill_child_processes* fix test* address comments* add early stopping in task scheduler & fix a stuck issue in measurement* fix lint* trigger CI* fix early stopping	0
[AutoScheduler] Fix the conflict of thread pool in measurement (#7166)	5
[Relay]resize op compute and schedule (#2172)	5
unroll condition changed for cuda (#232)	4
TVMC: Don't divide trials by zero tasks (#10164)If there are no tasks to tune, the number of trails is meaningless.Co-authored-by: Martin Kröning <martin.kroening@neclab.eu>	5
bump PyTorch version to 1.11 (#10794)* bump PyTorch version to 1.11* disable some caffe2 ci* Fix sub conversion in PyTorch frontend* use fuse_modules_qat if available, fallback to fuse_modules for older PyTorch* Re-Run CI	1
[Hexagon] Introduce new DeviceAPI (#9355)* Compile hexagon device api from runtime/hexagon/hexagon whenbuilding for hexagon and USE_HEXAGON_DEVICE is not set.* Add hexagon_common.h utilities including customtvm runtime logging for hexagon.* Introduce HexagonBuffer class to store hexagon allocation metadata.* Add HexagonDeviceAPIv2 for use on hexagon.* Add hexagon packed function wrapper.* Add custom linked param lookup for hexagonthat wraps params in an unmanaged HexagonBuffer.* Add custom hexagon module based of library module node.* Apply clang formatting* Apply cpplint changes.	4
android gemm for topi/recipe (#628)	5
[Conda] Fix a compatibility bug in conda scripts (#10201)Remove `max_pin` requirements for `cudnn` library to be compatible with `cudatoolkit>=11`	5
STM32: add as a new target (#9385)* STM32: add as a new target* STM32: Target takes a board ID rather then a series.* STM32: target series.* STM32: Fixed lint issues.	0
Fixed typo and type mismatch (#5259)Co-authored-by: Adrian Muresan <muresan.adrian.bn@gmail.com>	2
Add test_forward_index_put to __main__ (#7542)	3
Fix cmake for building with cuda in msvc (#1437)	1
[CI] remove unused environment var (#6824)	1
[CMSIS-NN] Assert correct amount of CMSIS-NN artifacts in MLF (#9480)Unsure why this wasn't picked up by the other PR, assuming some race conditions - there's now 4 artifacts in the archive:* Interface in lib0* CMSIS-NN in lib1* Linked params in lib2* Host code and executor in lib3	2
[RPC] Better handle tempdir if subprocess killed. (#3574)	0
[Target] Improve string interpretation in Target creation (#12152)- SplitString now preserves escape sequences, but still observes  quote characters.- Added function Interpret that transforms given string according  to interpretation rules:  - outermost quotes are removed (if present),  - escape sequences inside quotes are preserved verbatim,  - unquoted escape sequences produce the escaped character (the    escape character (\) is removed.- Interpretation happens every time a value of any type is to be  parsed from a string, e.g. Array<String> will first be parsed  as an array, then substrings of the input will be parsed as  individual elements of that array. In this case, some parts of  the initial input will be parsed (and interpreted) twice.- Implement corresponding stringification functionality.This new scheme enabled encoding nested arrays of string with anydegree of nesting. For example  "-field='\\'foo0\\',\\'bar0,bar1\\'','\\'zing0,zing1\\',\\'fred\\''"would correspond to the target kind attribute  Array<Array<Array<String>>>("field"))and have the value { { {foo0}, {bar0, bar1} }, { {zing0, zing1}, {fred} } }	1
tanh float16 (#12165)Co-authored-by: aakaverm <aakaverm@qti.qualcomm.com>	5
upgrade java style-check due to CVE-2019-9658 (#2817)	5
[Bugfix][Module] Fix recursive GetFunction in runtime::Module (#6859)	1
Update HalideIR submodule to include TVM_STATIC_IR_FUNCTOR_REGISTER (#857)* Update HalideIR commit to include TVM_STATIC_IR_FUNCTOR_REGISTER* Fix HalideIR to point to the right commit* Add missing using to C++ TOPI nn.h* Update HalideIR to include compiler error fix* Fixed error where broadcast_to fails if shape is tuple of IntImm* Change get_const_int to support int as input	1
[TIR] Expose Misc TIR operations to python (#12435)This PR exposes the following TIR operation in python:- `assume`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_transform_remove_assume.py#L34)- `undef`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_transform_remove_undef.py#L63)- `likely`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_schedule_compute_at.py#L849)Co-Authored-By: yongwww <yongcale@gmail.com>	3
[OpenCL] Fix profiling hang for OpenCL device (#12173)* Fix opencl timer logic to support nested calls to start and stop functions* Fix profiling for __nop functions that caused the profiler to hang. Now __nop functions are skipped for profiling* Fix linter* Add a test that checks the correct time for nested timers* Fix linter* Fix work with memory object in test for opencl timers	3
[Hybrid Script] Unify the symbol tables to one; support `tvm.container.Array` (#2366)	1
[AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci	3
[TOP] split, reshape, concatenate (#43)	5
[CODEGEN] use charp for voidp (#753)* [CODEGEN] use charp for voidp* fx	1
Add __declspec(dllexport) to class Node and class Op if build on Wind… (#296)* Add __declspec(dllexport) to class Node and class Op if build on Windows, so that it can be referenced in some test code in MXNet* correct typo* reuse the macro in c_api.h* revert changes on base.h* one more place to revert* use enclosing extern c block* revert a change caused by copy* remove a space	4
Update arm_compute_lib.rst (#6861)Updated correct path in readme	5
[TOP] Add dense, batchnorm (#22)* [TOP] Add dense, batchnorm* update tvm	5
[ETHOSN] Remove the compiler library from the runtime link (#10334)Due to some restructuring of the Ethos(TM)-N driver library it is nolonger necessary to link the compiler library (AKA Support library)into the runtime.	1
skip mps2_an521 for host-driven AoT zephyr tests (#11833)	3
Rename TShape::index_t to dim_t and change to int64_t (#114)* Change TShape::index_t to int64_t* Add comment* Make Tuple::Save&Load dtype generic* trigger update* Fix lint* Fix comment* Change index_t to dim_t* Remove legacy index_t	4
Implement [skip ci] for Jenkins (#9554)* Rebase* Fix missing skipCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR][BugFix] Fix a wrong use of T.exp in test_compute_inline_opaque_access_with_tvm_access_ptr. (#12117)Co-authored-by: sqing <qing.siqi@intellif.com>	3
[ci] Look for any tags in issues before adding new tags (#10685)Previously this searched for a specific `cc @abc` line by itself before looking for people to tag. This led to a double-tag in #10679. The change here updates it to check for `@`-ed people anywhere in the PR/issue body and filters those out.	0
[ARITH] Refactor intset eval with functor (#295)	1
[TFLite] Support TFLite FP32 Relay frontend. (#2365)* Support TFLite FP32 Relay frontend.* Fix lint issue* Remove unnecessary variables and packages* Add method doc string* Fix capital letter of method doc string* Add TFLite FP32 Relay frontend based on latest code* Merge the latest code* Solve cython issue with relay type inference* Modify code based on suggestions	5
[AUTOTVM] Simplify TopHub (#1630)	5
[ONNX] Add ReduceSum opset13 support (non-dynamic) (#11606)* [ONNX] Add ReduceSum opset13 support (non-dynamic)* Add check* Add support for constant axis* noop* Rework logic	2
[CodeGen][CUDA] Improve CUDA vectorizer (#4736)- Fixes issues to enable fp16 vectorizer. Now correct packing and  unpacking CUDA code will be emitted. Enabled more unit tests.- Do not emit code to read the first lane from an undef variable  int _3;  _3 = _3 & ~(0x000000ff << 0) | ...  and emit the following code instead:  _3 = (((0x000000ff & (_1 >> 0))+(0x000000ff & (_2 >> 0))) << 0);  Note that nvcc 10.2 is forgiving and emits the same code for both cases.  A warning appears in test_codegen_cuda.py.Signed-off-by: Wei Pan <weip@nvidia.com>	3
[AutoScheduler] Bug fix & Custom sketch support (#7260)	1
[microTVM] Autotuning performance tests (#11782)* Common autotuning test* Autotuned model evaluation utilities* Bugfixes and more enablement* Working autotune profiling test* Refactoring based on PR commentsBugfixes to get tests passingRefactor to remove tflite model for consistencyBlack formattingLinting and bugfixesAdd Apache license headerUse larger chunk size to read filesExplicitly specify LRU cache size for compatibility with Python 3.7Pass platform to microTVM common testsBetter comment for runtime boundStop directory from being removed after session creation* Use the actual Zephyr timing libraryUse unsigned integerAdditional loggingTry negationTry 64 bit timerUse Zephyr's timing libraryFix lintingEnable timing utilities	0
[TOPI] implement pool3d op (#4478)* [TOPI] implement pool3d op* use PoolInferCorrectLayout for both 2d and 3d pooling* unify MakeMaxPool and MakeAvgPool	1
[TFLite] added scalar axis value handling in reduce (#6970)Axis value in reduce can now be specified as scalar	1
reshape with non constant shape argument (#6411)	5
[microTVM] Fix timeout of -1 breaking Arduino transport (#12189)* Remove warning from Teensy boards* Use a real timeout* Skip assertion of whether a functional schedule exists* Don't specify least significant digits for Teensy boards	1
[ConvertLayout] Squeeze and reduce ops (#7835)	5
[lint] CHange docker lint message (#11767)	2
[RELAY][PASS]use attribute registration style in the mac count pass (#2645)	4
hotfix gcn tutorial fail (#4994)	0
Add unittest	3
[Relay] Remove DynamicToStatic pass from graph runtime build (#10691)Closes https://github.com/apache/tvm/issues/10692To solve this problem, we can either remove this pass from `relay.build(...)` pipeline or run `DynamicToStatic` in both VM and non-VM paths. I propose to remove it because  (1) usually `DynamicToStatic` is supposed to be applied after model import and (2) the only case running `DynamicToStatic` during `relay.build(...)` helps is when the input is entirely static but a frontend fails to produce a static mod AND a user forgets to run `DynamicToStatic` after model import. I hope the latter case happens rarely but if not, that's something we should fix in the frontend side. We should avoid relying on `DynamicToStatic` that runs during `relay.build(...)` since not all use cases of TVM use `relay.build(...)` (BYOC, for example).	1
Update gen_requirements.py with new onnx things  (#9541)* adds onnxoptimizer to gen_requirements.py	1
[CI] Move back Keras to 2.4.3 (#6810)* I mistakenly moved it back to 2.3.1, now fixing it	0
[CODEGEN] Fix code generation bugs for C/CUDA & Improve VerifyGPUCode pass (#6041)	4
[TEST] Not assuming HOME in tvm/download.py (#3803)* Not assuming HOME in tvm/download.py* Trigger notification	3
[FIX][TOPI][X86] schedule dense pack (#4539)	0
fix tvm relay testing tf.py typo error (#5977)	0
Fixed subprocess creation under windows (#4820)* fixed subprocess creation under windows this addresses  the issue #4819* Update server.py	5
Change TOPI ops to use C++ implementation where applicable (#357)* Updated TVM version. Implemented fix for nnvm_compiler crash on exit on windows. Changed TOPI ops from using python to using C++ where applicable.* Fix lint* Fix lint* Fix macro* Fix reshape* Update TVM to fix test fails	0
[relay] Changed 'name' field to 'registry_name' for Executor and Runtime (#10466)* [relay] Changed Executor and Runtime 'name' field to 'registry_name'Changed 'name' field to 'registry_name' for Executor and Runtime pythonwrappers as it clashed with tvm object attribute 'name' which made the latterinaccessible from PythonChange-Id: I917755753549edfe1d3090ca9ca4512de552c4bdchanged name to registry_nameChange-Id: I9feb5b33b7b6f6f8421902e5721167f585cc4193* more fixed unit testsChange-Id: Ie2e96297fda119e1b726b196a59deae95b263a07* typo fixedChange-Id: Id579c50ab58dfb25fa18436265e0701ebbd9d554* renamed registry_name to flag_registry_nameChange-Id: Iabbd81069959f05c073f9dbc8d10fb31dd05f7a3* bugfix	0
[RELAY][Frontend][TF] decompile tf control flow (#2830)* decompile tf control flow* Add docs* remove import relay* move tests under tensorflow frontend* minor fix	0
[MetaSchedule][Test] Add unittests for CAP (#12047)	3
fix pytorch frontend bug (#9884)* fix pytorch frontend bug* update* updateCo-authored-by: zhaojinxi <zhao.jinxi@intellif.com>	5
[DOCS] fix doc builder (#213)* [DOCS] fix doc builder* fix* fix* fix doc builder	2
conv2d schedule fall back warning fixed (#450)	0
fix (#2674)	0
[Target] 64-bit RPi4b target (#6211)	1
[DOCS] Cleanup the relay docs location (#2785)	2
Emit DWARF debug information (#3420)	5
reverse changes in pr #4849 (#4853)	4
fix crash issue in tsim backend (#4527)	0
[microTVM] Rework evaluate_model_accuracy into a more generic helper function (#12539)* Add workaround for #12538* Rework evaluate_model_accuracy into predict_labels_aot	1
[microNPU] Integrate the cascader (#10862)* [microNPU] Integrate the cascaderIntegrate the cascader into the codegen and optionally enable itwith the enable_cascader flag. Includes placeholder MemoryRegions untilintegration with the PoolInfos provided by a user.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* Fix linting and a docstring* Plumbing and testing improvementsPlumb the workspace memory pools into into the cascader and makethe tests to check for the memory reduction.* enable_cascader() -> is_cascader_enabled()* Check for the exact value of workspace size* Remove unused ACCEL_TYPES* Linting...Change-Id: If2d92846f05a7e8b21be767163841084538805a9* Rebasing...Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>	4
[FIX,AUTOTVM] Print warning when all autotvm tasks fail with errors (#6612)* [FIX,AUTOTVM] Print warning when all autotvm tasks fail with errors.* formatting* write errors to tempfile* wording* wording* don't duplicate errors* Ensure we have a string for an error	0
fix cudnn output shape (#708)	0
[HEADER] Add Header to Comply with ASF Release Policy (#2982)* [HEADER] ASF header dir=include* [HEADER] ASF Header dir=src* [HEADER] ASF Header -dir=python* [HEADER] ASF header dir=topi* [HEADER] ASF Header dir=nnvm* [HEADER] ASF Header -dir=tutorials* [HEADER] ASF Header dir=tests* [HEADER] ASF Header -dir=docker* fix whitespace* [HEADER] ASF Header -dir=jvm* [HEADER] ASF Header -dir=web* [HEADER] ASF Header --dir=apps* [HEADER] ASF Header --dir=vta* [HEADER] ASF Header -dir=go* temp* [HEADER] ASF Header --dir=rust* [HEADER] Add ASF Header --dir=cmake* [HEADER] ASF Header --dir=docs* [HEADER] Header for Jenkinsfile* [HEADER] ASF Header to toml and md* [HEADER] ASF Header to gradle* Finalize rat cleanup* Fix permission* Fix java test* temporary remove nnvm onnx test	3
[AOT] Get input name from module/prim func (#10731)The input name generated in each of these test cases changes dependingon the version of tensorflow being used. v2.4 = "x_int8", while v2.6= "x". Making these tests agnostic of input name so that they work withboth v2.4 and v2.6.Change-Id: I843a655b3bf4e018624e5757c653b1d85058991e	4
[TOPI,CUDA] Don't enable cudnn conv2d kernel if is not supported (#10021)* [TOPI,CUDA] Don't enable cudnn conv2d kernel if is not supportedSpecifically, check that layout is not NCHW if datatype is int8.* remove all conv2d_cudnn int8 support	1
[BUGFIX] Fix search path for libtvm_topi.so (#4467)	0
Move external codegen test helpers into utils (#9008)This is so they can be re-used as part of other tests which don't extend test_external_codegen.pyI've identified that `test_json_runtime.py` and `test_pass_partition_graph.py` use a very similar but slightly different variant of these functions for future iterations.	1
[ci] Migrate all test steps to macros (#10968)This moves all the tests in the `Jenkinsfile` to use the `test_step` macros so they all get the same timeout/condition/skipping behavior. This also adds 2 shards for i386 and GPU unittests, the 2 remaining longest jobs.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[PatternLang] Add Syntatic Sugar to the C++ pattern API and support DataType Attribute Matching (#7120)* Add Syntatic Sugar for C++ Pattern API, Support DataType Attribute match* add missing tests* fix lint* fix license edit* fix bad rebase	0
fix TVMRetValue move constructor not clear old value  (#144)* fix TVMRetValue move constructor not clear old value lead to repeat delete* fix	0
[TIR] CSE pass : Restrict the equivalence to be decided by a normal form - avoids comparison of terms (#11574)The CSE pass had been designed for potentially allowing comparisons (and commonings) of equivalent terms (like (x+y)+z and x+(y+z)), where **the notion of being equivalent was customizable, and no assumption was made about it**. That means that the implementation of the equivalence test function `EquivalentTerms()` - which was at the moment just calling the syntactical equality test `EqualTerms()` - could be replaced later by a cleverer equality test.However, having such a generic way of comparing elements meant that in the function `SyntacticToSemanticComputations()`, where we were going from a hashtable of syntactical entities to what I called a vector of "semantical entites" (which are just canonical forms/representants of classes of equivalence of terms), **the only way was to compare each pair**.That resulted in a quadratic behavior of this function, but there was no way around it as in order to merge equivalent entities into their class of equivalence, we had to compare them.**This PR essentially does the following:**- When computing the classes of equivalences of terms (therefore transforming a ComputationTable (i.e. a hashtable) into a vector of classes of equivalence) : **instead of comparing each pair of terms, relies on a normalization procedure to obtain a normal form for each of them**.That transforms a small part of the algorithm that was quadratic to n.logn. However, it's difficult to see improvements in practice, in particular for average sized programs, as that part was a "small" quadratic to a "big" n.logn (finding things in a hash-table, copying it to a vector, etc).It was probably going from a complexity of ~O(((n²-n)/2) + n.logn) to a complexity of ~O(3n + n.logn), so potential gains would only be expected for very large programs.- Completely gives the user the possibility to turn ON/OFF the semantical comparisons of terms. It is turned OFF by default (as it's quite longer to compile with it ON, unsurprisingly), which means that by default, the equivalence coincides with the (syntactical) equality of terms.    As the pass was written with the possibility to do these additional commonings (like (x+y)+z and x+(y+z)), it was a good time to fully plug that completely, up to the Python user who can now turn that ON if he wants to. But again, it is OFF by default, so no real change on that.To run it ON, simply do:`with tvm.transform.PassContext(config={'tir.enable_equiv_terms_in_cse_tir':True}):`before calling `build()`- When this boolean is set to ON, it uses a simple implementation of the normalization function with equivalences that uses `arith::Analyzer::Simplify` as noted by in https://github.com/apache/tvm/pull/10544 . Note that this is not a real normalization procedure as it is incomplete (i.e., it is not guarantee to converge to the normal form), but it is correct, and it works well with most properties : associativity of +, distributivity of * on +, etc.- Clarifies and enhance the test base for the pass. In particular, it adds the tests that were written in https://github.com/apache/tvm/pull/10544 but which did not make it through.- Also add the test ( https://github.com/AndrewZhaoLuo/TVM-Sandbox/blob/19284ddbd6bb28af61c0c2aa8bb334c5c53731a7/tir/test_inconsistent_tir_lowering.py#L1 ) demonstrating the (older) non-deterministic lowering and put it into a proper test, as I found it useful for making sure that this does not happen again. It has been copied from https://github.com/apache/tvm/pull/10663 and only slightly adapted (in particular for doing the comparison of hashes automatically instead of printing them and relying on a human to compare them).	1
[Bugfix][Target] Correct passing of target-queried bool/int parameters (#8660)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Relay] Fix for recursive let (#5757)* Make let processing iterative* Try again* Fix pretty printer overflow* cleanup* fix lint* Fix text printerCo-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Jared Roesch <jroesch@octoml.ai>	5
[FIX] Miss kUInt in TypeCode2Str & dir method (#130)* [FIX] Miss kUInt in TypeCode2Str & dir method* [FIX] Add regression test	3
Improve the error message in module.cc (#8694)	0
[ARITH] Fix lowering of floormod(x, y) != 0 (#4127)	0
[TIR][USMP] Augmenting the algo interface with memory pressure (#9649)This commit adds memory pressue to be an arugment tothe USMP algorithm interface as certain iterative algorithmscould use this as a guide determine the terminationcriteria.Change-Id: I3fb5eea3fe5ba43e68c23625d411e557f6dd89a3	4
[Relay/Topi][Op] Added native DepthToSpace and SpaceToDepth Operators (#4566)* Added tvm function stencil for subpixel operations to topi.* Topi subpixel operators added and tested.* Added subpixel attrs.* Added depth_to_space relay attributes.* depth_to_space fully working.* Fixed NHWC shape bug.* SpaceToDepth in and all tests passing.* lint fixes.* Added string include* Fixed topi formatting.* Added DCR/CDR mode to depthtospace operator.	1
[COMMUNITY] vegaluisjose -> Committer (#6582)	3
[BYOC] RelayToTIR custom codegen passes can still depend on dynamic shape functions (#11619)In #11474 I got ready to switch CUTLASS from function-at-a-time to IRModule-at-a-time compilation.However my approach didn't handle dynamic shape functions, so I adjust it here.The idea is still that such passes will leave behindcalls to 'extern' functions. However, converting thosecalls to 'call_lowered' form inMarkCompilerFunctionsAsExtern is too soon since onlythe TECompiler knows how to capture all the attributesnecessary to support dynamic shape functions.So stop doing that in MarkCompilerFunctionsAsExtern andinstead support this case properly in the TECompiler.While there try to chip away at the chronic lack of structure in te_compiler.cc. Every little bit helps.Add a basic unit test.	3
[PROFILING] Various fixes for profile_function (#10850)Check that the function to be profiled is actually defined.Check that the MetricCollector used actually can time the regionrequested.Default to using the module's entry_name instead of "main".	1
[VM] add removeUnusedFunctions pass in vm memoryopt (#8040)* add removeUnusedFunctions pass in vm memoryopt* fix lint	0
[Bugfix] Fix div zero error in rewrite_simplify (#8961)* fix div zero error in rewrite_simplify* update the style to fix ci error* remove useless code and comment	1
[QUANTIZE] Add nn.batch_flatten as quantizable. (#5805)* [ONNX] Skip ADD inside Gemm op when vector is zero* [QUANTIZE] Add nn.batch_flatten as quantizable.	1
[Relay] Improve reduction op layout propagation for packed input  (#9253)* wip* fixed packed dim size logic* fixed test* formatting* fix compile warning	2
make ThreadPool thread local (#1152)* make ThreadPool thread local* add return	1
[CMSIS-NN] Only run ScalarToTensorConstants pass on CMSISNN external functions (#10375)* [CMSIS-NN] Only run ScalarToTensorConstants pass on CMSISNN external functionsEnsures the `ScalarToTensorConstants` pass only runs on functions withthe "cmsis-nn" compiler attribute. Previously, it was possible for partsof the pass to run on all of the input. For example,`ReplaceScalarWithTensorVariable` was running on all OpNodes withoutregard to the containing function. As a result, this change meansthat the pass now visits each CMSIS-NN function separately, rather thanvisiting the body of the main function.Change-Id: Ia4da2246952aa7e3514264f1ed1a77a0916deb95* Change variable names so main function is not mentionedSince the main functions is no longer accessed, the variable namesmentioning main don't make sense. Correcting this.Change-Id: Ic1f8170b5a73453be27e70d533a41ef7eb58f485	4
[SCHEDULE] Add group, refactor thread bind api. (#82)* [SCHEDULE] Add group, refactor thread bind api.* fix doc* fix g++-4.8* More testscase* Remove graph context from fix pt analysis	0
Fix address and port reported by android_rpc to tracker (#8405)	1
[COMMUNITY] jcf94 -> Reviewer (#6241)	3
[PYTORCH]Abs, Arange, Softplus ops (#5295)* [PYTHON]Abs, Arange, Softplus ops* Review comments updated	5
[CODEGEN][OpenCL]: fix tir.erf codegen to opencl directly (#8756)* register tir.erf to lower opencl directly* add opencl codegen unit test* change erf opencl codegen unit test for checking there is erf in the source not erff	3
[AutoTVM]Improve graph tuner for multiple subgraphs (#3490)* Improve boundary nodes in graph tuner* Limit output node number* Fix test* Improve warning.* Fix test	3
[FRONTEND][TENSORFLOW] Some bug fixes for tensorflow NCHW data_format (#3514)	5
[Torch] Add an option to make imported models compatible with the Relay text parser (#9015)* [Torch] Add an option to make imported models compatible with theRelay text parser* py format	2
[PYTORCH]Matmul fix for batch_matmul (#5604)	0
[Relay] External codegen (#4482)	5
Complete docs. (#9070)	2
Fix typo in ir_pass.h  (#3741)	4
adding Jorn to reviewers list (#9105)	1
[C API] Make DSL API registerable, add copy from/to raw bytes (#222)* [C API] Make DSL API registerable, add copy from/to raw bytes* fix cython	0
[CODEGEN][AOCL] Add math intrinsic rules (#1653)* [CODEGEN][AOCL] Add math intrinsic rules* introduce aocl_emu target for AOCL emulation* rename aocl_emu with aocl_sw_emu* update docs	2
[TARGET] Add layout_transform, clip and expand_dims in onnx converter (#6366)* Add layout_transform, clip and expand_dims in onnx converter* remove _add_input and address comments* address comments	1
[TOP] Rename conv pool parameter back to 2d	2
Ensure loop count is a constant before trying to unroll. (#2797)	1
[SimplifyExpr] Add simplify for dq->arg funcs (#12580)* add simplify for dq->arg funcs* add comments, fix lint* move comments to the right spots	4
[Frontend] Onnx (#40)* init onnxfinish onnx frontendadd onnx testsfix variousbackupuse transformer[Frontend] graph passedadd test forwardtest forwardfix doc and lintfix test graph tuplefrom_onnx now take 2 args, output (sym, params)fix renamefix input namesfix multiplefix lintfix lint check* better doc	2
[COMMUNITY] Add DISCLAIMER, KEYS for ASF release (#4345)* [COMMUNITY] Add DISCLAIMER, KEYS for ASF release* Add file name spec	2
[Relay] Port param dict save/load from NNVM (#2620)	2
fix (#111)	0
[Relay] Remove memory planing from LowerTEPass  (#8974)* Clean up LowerTEPassAdd attrs to IRModule equal and hashMake LowerTEPass opt_level 0Copy IRModule attrs to per_target_modulesAdd ShallowCopy to IRmodule* Fix rebase* Remove comment* [TEC] Remove memory plan from LowerTEPass* Fix linting errors* Fix PR comments* Remove updated module with function info from LowerTe* Refactor UpdateMainWorkspaceSize to update func info independently from LowerTEPass* Fix aot failed tests* Revert whitespaces fixes* Remove obsolete function hoisting and minor cleanups* Address PR commentsCo-authored-by: electriclilies <lilyorthsmith@gmail.com>	1
[LLVM] Use llvm::FunctionCallee in IRBuilder::CreateCall with LLVM 11+ (#5338)The older variants of CreateCall have been deprecated and were recentlyremoved from LLVM. This caused compilation failures.	0
Fix some typos in api docs (#3309)	2
[TIR][CUDA] Fix sub-warp reduction using "max" (#12275)* upd subwarp unittest* fix range check in sub-warp reduction* upd: sub-warp max unit test	3
[microNPU] fix the CMake to keep utils.cc (#9630)If the build does not use USE_ETHOSU ON,it will report that Object definitions are missing.This change will keep the file that has the Objectdefinitions.Change-Id: I64776f941bf0475e10397ff2cdbfd73a909611a0	5
Slight optimize the default injective schedule (#7158)	5
Fix serialization of inf float value (#5912)	5
[docs] Fix incorrect command (#11630)build/html get moved into _docs, update related document.	2
improve doc for relay.nn.dense (#6508)	2
[CI][DOC] Fix incorrect commands in docs/readme.md (#11583)Fix incorrect commands in docs/readme.md	2
Fix issue when group attribute isnt defined in convtranspose. (#7655)	0
[Keras] Adjust Keras frontend for Keras 2.6 support (#10733)Add support for the Keras frontend to be tested and used withboth Keras 2.4 and 2.6, as we plan for migration.Co-Authored-By: Luke Hutton <Luke.Hutton@arm.com>Co-authored-by: Luke Hutton <Luke.Hutton@arm.com>	1
[RELAY] bugfix type functor caching (#2113)	0
[CODEGEN] Detect broadcast(cast(x)) pattern in FMA (#551)* [CODEGEN] Detect broadcast(cast(x)) pattern in FMA* [CODEGEN] Improve* [CODEGEN] Fix	0
[BYOC][Verilator] Refactor Verilator runtime (#7406)* new experiment* save* refactor* refactor library* add profiler* refactor* refactor* add docs* update comment* add deallocator	1
fix (#9021)	0
Fix the TF tutorial to run against TF2.0 and TF1.x (#4104)* WIP Run the TF tutorial on TF2* Remove debugger statement.* Complete the support for TF2.0's `resize`.TF2.0 adds a `half_pixel_centers` attribute to the `resize` function inthe image API. This commit completes the hooks in Relay's TF frontend.At the point of this commit, no new test yet. Also, this commitaddresses solely the `resize` change. Other commits address otherchanges in TF2.0.* Support TF2.0 in the tutorial by using the compat API.This looks cleaner than trying to detect the TF version.* Use the TF compat API, so as to support TF2.0.This is a direct change, relying on the compat API provided by the TFteam.This code will last as long as the compat API exists, so a"proper" support for TF1.x and 2.x will require more work in somefuture.* Partial support for EXPLICIT padding introduced in TF2.0.Explicit padding is a special case in TF2.0 (see reference linkedbelow). Some models are serialized with that mode, and break TF supportin TVM.Support is *partial* as EXPLICIT falls back to set padding on theRelay op, which only supports 2 values. At some point, padding may needto be extended to support 4 values, but that is out of scope of thissupport commit.Reference on EXPLICIT padding: https://github.com/tensorflow/tensorflow/commit/ec81825aaf7e848d9f8ddffdf1e0d20aebe9172c#diff-1d1c0bb0a880f85b6164f71dbb2f446e* Guard on checking for optional TF2.0 attribute.* Do not expect Relay to implement TF-specific attributes.The `half_pixel_centers` attribute is a new feature in TF2.0. Earliercommits of mine mistakenly introduce them in the Relay API. This isprobably not what Relay is expected to support, and the semantics of`half_pixel_centers` is unclear (to me, at least) at this point.* Remove unclear comment.CR https://github.com/dmlc/tvm/pull/4104#discussion_r338705742Addresses #4104* Changes after review.Complying without understanding the rationale for now.* Fix the arguments set mistakenly.An argument ignored for the wrong operation.	0
[DOCS] API doc update (#1136)	5
copy intrinsic now can include typecast (#855)	2
[DOCS] Use https link (#5183)* [DOCS] Use https link* use http for sphinx	1
[BYOC-DNNL] Support DNNL optimal layout (#10421)* enable dnnl optimal layout for supported ops* verfied cv models with onednnv1.7* rebase to the latest main branch* fix format related comments* remove unnecessary layout transformation* change deconv into conv_transpose* rename some variables and functions* simplify query_layout* add checkes for query_layout* fix lint* move partition_for_dnnl from dnnl.py to test_dnnl.py* remove unnecessary model test* add more dnnl layout* rename flag in convolution.cc* enhance dnnl layout	1
[Optimization] Warp level reduction support for CUDA (#5498)- Added the warp level reduction support- Upgraded shfl intrinsics to the sync version.- This is the building block for scheduling softmax like operations.Signed-off-by: Wei Pan <weip@nvidia.com>	1
init (#7943)	5
[auto_scheduler] metal default hardware params (#7022)	2
[TVMScript] Fix printing ForNode annotations (#8891)	0
conv_nchw parameter updated to the one generates mobilenet benchmarks, doc typo fixed (#345)* conv_nchw parameter updated to the one which generates mobilenet benchmarks, doc typo fixed* removed unused variables	1
Fix infer shape bug (#148)	0
Raise ImportError for XGBoost (#6969)	2
Improve CanonicalSimplify to handle Min, Max(#2248) (#2261)Also enable Mul caching for more cases	0
[DOCKER] Fix missing apt https transport support (#3735)* [DOCKER] Fix missing apt https transport support* [DOCKER] Drop superflous explicit sudo's	4
[TVMSCRIPT]Fix script printters StructuralEqual check failed (#8499)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	0
[PYTORCH]ReplicationPad support added (#5708)	1
[RUST][FRONTEND] Add rust frontend v0.1 (#2292)	1
add make/config.mk; add nnvm-fusion into plugin as a submodule (#80)	1
Update README.md	2
[TVMC] Add --opt-level to compile mode (#9722)	1
[DOCS] Add how to contribute TVM docs with images. (#10287)	2
[Target][Lowering] Update Op Intrinsic Lowering Mechanism And Intrinsic Lowering Pass (#7809)This PR updated the intrinsic lowering pass to support the new op registry and avoid overloading the global tvm registry. Meanwhile, it kept the fallback mechanism to find the most suitable lower intrinsic function, e.g., llvm.FLowerIntrinsic vs. default.FLowerIntrinsic. All previous op registration are ported to new functions, and some missing ops would be added in separate PR.	1
[Software pipeline] Fix hardcoded index in `access_ptr` rewriting, add a GPU test with depth 4 (#11495)* fixed hard-coded index in software pipeling* fixed three-stage pipeline test* add three stage pipelined gemm test* refactor mma test* use mma_4k schedule utility in test* apply pipeling annotation* black* require ampere in test	3
[PASS] Refactor a couple of TIR passes - BindTarget, AnnotateEntryFunc, Filter, LowerInitBlock (#11628)This PR fixes a few inconsistent pass registration and add testcases for them. - `LowerInitBlock` had mismatch between its pass name and ffi key.- `BindTarget`, `AnnotateEntryFunc`, `Filter` were not following the name convention of tir passes and they were not registered in FFI registry.	1
[TensorIR] change IntRV to ExprRV (#8077)	4
[Bugfix] Recover original layout when alter_layout function return None (#2101)	1
[Docker] Update to Torch 1.10.1  (#9781)* update pytorch to 1.10.1* fix missing import test only on llvm and cuda* Revert "[Docker][Onnx] Upgrade ONNX to latest version (#9519)"This reverts commit 3f5dca5f56da92942168bdf80ee8b7b24667ac1f.* skip testing if target is not enabled	0
[EXAMPLE/PYTHON] Improve extension type, add operator lib (#162)	1
[ROCM] remove fma dispatch (#591)* removed fma dispatch* added comments to explain why remove fma* fix lint* use fmuladd intrin for fma dispatch	1
[ETHOSN] Improved identification of driver library version (#10285)	1
[COMMUNITY] Add driazati key for release (#12076)As per https://tvm.apache.org/docs/contribute/release_process.html#id3	2
add dilation in x86 NCHWc depthwise conv support (#4962) (#6267)	1
[Tutorial] Add a tutorial for PyTorch (#4936)* Add a tutorial for PyTorch* Fix sphinx formatting, add version support* Remove space* Remove version check* Some refactoring* Use no grad* Rename input* Update cat img source	5
add aten::pixel_shuffle implementation (#6328) (#6468)	1
[JS][WEB][BACKEND] Javascript(webassembly) backend. (#239)	5
[Test] Add Test Case to Cover Bug Fix by PR#7432 (#7601)	0
Add SGXModule (#1019)	1
Fix minor issues in the tvmc tune CLI (#8039)* [TVMC] convert timeout flag to intfixes Check failed: type_code_ == kDLInt (11 vs. 0) : expected int but got strwhen setting the timeout option using the cli flag.* [TVMC] fix typo in tvmc tune help	2
[ONNX][TOPI][Relay]Support dilations in pooling operators (#7928)* change more pooling operatorsdilations -> dilation to match old field names in convfix python interface into new relay nodesfix order of argumentsupdate type relation for dilationschange topi interface to use dilations* spooky, there are two implementations! Change to 1 topiuse generic poolnd instead of 2d implementation for topiremove old pooling topi* rename pool --> pool2d in topichange pool -> pool2d, make topi tests work nowmake op level 2 pass with interface changesfix dilation being hardcoded to 1proper calculation for avgs among dilationsproper avg pool padding behaviorchange name of pool test to pool2d test* add poolnd baseline implementationmore fixes to edge cases for poolnd, delete old versionsreplace topi tests with new baseline python versionclean up testsmake tests more readable kind ofadd dilation topi tests FINALLYremove see_pool.pyremove dilation from grad* fix subtle implementation detail between topi and baseline python pool op* rewrite tests to be more generic for relay pooling opsadd relay dilation tests, FINALLYadd some comments to testing codelinting and formattingadd ASF headermake 10/10 for black formatting lolmore appeasing the formatting godswowadd parameters to documentationfix test importJostle CIfix more broken unit tests using old version of poolfix wrong var used for bound calcadd dilation to arm testsadd docstring to python make funcs* fix pattern utils out of place args* properly forward more tests to use dilations in poolingformattingmore formattingrelax constraints on test to make it passrelax more constraintsfix some pytorch frontend errorsfix errorbetter test conditionsjostle build* fix padding bug with ceil modejostle buildcleaner pool conditionremove see_pool.py again* add dilations field to onnx importerblacking filesblack file* address matthew's comments	1
[Relay] Fix TFlite frontend for unpack, stridedslice (#10333)We found this while converting an RNN model.The relay tflite frontend use squeeze at converting unpack, but when theunpack.axis=0, `None` is passed to relay.squeeze(), which would squeezeall dimensions with length 1, causing different results from TFLite.A possible fix might be, assign the unpack.axis as-is to relay.squeeze()As for stridedslice, when the tflite frontend handles shrink_axis_mask,the wrapped `begin` should be used, instead of the original one whichcan be negative. It can cause errors athttps://github.com/apache/tvm/blob/d65ff6594d4d6db0062537a1d43c0504173b8e5c/include/tvm/topi/detail/strided_slice.h#L140Related cases are also added to the python test.	3
#1159 bug fixed (#1164)* #1159 bug fixed* #1159 #1164 fixed	0
[ci] Add workflow to cc teams (#10322)As discussed in https://discuss.tvm.apache.org/t/rfc-remove-codeowners/12095/2?u=driazati, this adds a mechanism to auto-tag people based on PR/issue titles and labels. This should improve visibility across the project and make it easy for interested people to subscribe to various topics.Details on usage will be posted in the relevant issue: #10317Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Frontend][ONNX]support  Pool2D layout is CHW (#11034)* support  Pool layout is CHW* fix lint test* change the if condition	4
[ETHOSN] Fix requantize output conversion (#12540)Fixes a small issue when converting the output information to the support library API. The `requantize_info` output datatype needed updating with the output datatype from the relay function to ensure the graph is compiled correctly by the support library. Included a test to prevent regression in the future.	3
[RELAY] Stop_fusion annotation (#2624)	5
[Frontend] TF V2 sparse.todense() test added (#7473)* [Frontend] TF V2 sparse.todense() test added* [1] Review comments handled	0
Lily -> Committer (#10417)	5
[Tir]Adding detail error messages when MatchCopyPattern function is failed. (#10244)There is an error message to show the body when 'MatchCopyPattern' is failed,but the error message not give the information why this function get failed.Adding the detail error information to help trouble shooting.	5
[microTVM] Enable micro tvmc tutorial testing in CI (#10555)* Add script convertor* Address comments: added test* address comments	1
[COMPILER] Initial compiler infra (#12)	5
Clean up pass.h (#3312)	4
[BugFix] Fix bug in cast to bool (#3207)	0
[MetaSchedule][Minor] Fix EvaluatorConfig Argument Description (#11766)Pointed out by @sunggg that the description of `number` and `repeat` for evaluator configuration is not accurate, updated to a version more consistent with `TimeEvaluator`.![TimeEvaluator](https://user-images.githubusercontent.com/3203174/174385966-74d3dbf6-dcca-43ea-9c0b-a91b4a281687.png)	5
[docker] Update CI to Python 3.7 and Ubuntu 18 (#10247)This updates the docker image build to use Python 3.7 and Ubuntu 18 as discussed in #9703. The update is mostly straightforward except that the `apt` boost isn't built with 3.7 so we must now build it from source.	5
[Relay][Frontend][TF] Fix transpose when axes is not a param (#4327)* [Relay][Frontend][TF] Use _infer_value_simulated when axes is not a const to Transpose* uncomment tests* dummy change to retrigger ci	4
[Hexagon] Do not pass lookup_linked_params to graph executor (#10944)This function is no longer used or generated, so it comes from theregistry as an "empty" PackedFunc. If the lookup function is provided,the executor will expect it not to be empty, which leads to a failedassertion.	3
[MetaSchedule] Apply-History-Best Task Filtering (#11692)This PR enables task filtering in Apply-History-Best, which is used inRelay/Relax integration. Previously, even though a task is ruled outduring task extraction, it still shows up in Relay compilation due tothe lack of filtering on `Apply-History-Best`. However, TE-to-TIRconversion `te.CreatePrimFunc` doesn't support all cases with hybridoperators involved, which leads to post-tuning failure affectingmultiple models.	0
[TVMSCRIPT] Attach span information to tir nodes in tvmscript (#6910)	5
[Hexagon] Make local symbols visible to loaded modules in RPC server (#11611)The simulator library `libhexagon_rpc_sim.so` contains TVM runtime builtinto it, but since it's loaded as a "local" library these symbols are notvisible to shared libraries loaded by subsequent dlopens. (Same applies tosymbols from the C++ runtime.)To make these symbols visible, dlopen the defining libraries as "global".(Re-dlopeninig an already loaded library is a well-defined operation.)	5
update (#10306)	5
fix prelu, now can use on 2d input and add one test (#2875)	3
[Frontend][Tensorflow] Fix TF 1.15 conv2d_transpose parsing (#6589)* Fix conv2d_transpose parsing in Tensorflow frontend for TF 1.15* Add comments and convolution tests without AddShapesToGraphDef	1
[FRONTEND] [HYBRID] Augmented assign operator supported! (#1459)	1
Allow condition in if op to be an array. (#7215)	1
[NODE] Move op inside node attribute (#30)	4
[TVMC] Add an end_to_end benchmarking argument when benchmarking. (#10256)* Add an end_to_end benchmarking argument to TVMC run.* Add command line test.* Fix comment syntax.* Set device to cpu if end_to_end is on.* Tickle CI	1
[DOCS] Improve document in reflection (#5593)	2
[VTA] Fix an issue in updating uop_idx in the TensorGemm module (#4694)	5
Heterogeneous Runtime (#1695)	1
[CODEGEN][LLVM] Refactor cpu runtime related code to CodeGenCPU (#361)	1
[microNPU] Remove unused import and command stream printing (#10764)This is a follow up to https://github.com/apache/tvm/pull/10695.Change-Id: I7f2dc14826cefea81fe5ff69c6255cdb5dc7f5c0	5
[CMAKE] Remove unecessary rdynamic (#4613)	4
[DOC/PERF] Reduction Tutorial and GEMM (#96)* [PERF] Add gemm* [DOC] Reduction tutorial	2
[RUNTIME] Add fp16/fp32 conversion functions (#1766)	1
Fix broadcast shape (#6422)* Fix broadcast shape* Fix test* Minor fix	0
[Hexagon] Add unit tests executing 2-d VTCM usage (#10904)* [Hexagon] Add unit tests executing 2-d VTCM usagePreviously, the schedules in `test_2d_physical_buffers.py` werelowered and built into a `runtime::Module`, but were not executed.* Added a missing function definition in the PR branch.* Only run test_execute on simulator in CI	3
Specify argument to FastMathFlags setAllowContract (#9337)	1
[Hexagon] Return pathlib.Path from get_hexagon_rpc_path() (#9969)Type annotations don't do anything, the type conversion needs to beexplicit.	1
Don't add cast for TF batch norm when type isn't changing (#5731)	4
Fix bug check trt (#10600)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
Update inject_virtual_thread.cc (#806)This compilation warning is fixed.src/pass/inject_virtual_thread.cc:43:19: warning: ‘rw_mask’ may be used uninitialized in this function [-Wmaybe-uninitialized]       if (rw_mask & 2) {           ~~~~~~~~^~~	5
[TOPI] Move conv2d spatial pack schedule to dedicated file (#3972)More schedules are making the conv2d.py file too large, sowe'd like to move the spatial pack schedule to dedicated filebefore introducing NHWC schedule. No logic change in this patch.	4
[GRADIENT] Add backward operator to enable backward graph  (#276)* Update docs* Add backward operator to enable backward graph* Fix testing* Refactor top level1 test code* Fix format* Test* Added zeros ones op* Register fill_like operator* Fix unit test	3
Re-enable Compute library tests. (#8573)ci image v0.06 does not appear to have the flakiness shown in ci image v0.05.However what changed between the 2 remains a mystery and needs furtherdebugging. However for now re-enable this to see how this fares in CIFixes #8417	0
[BugFix] Copy intermediate result in debug runtime (#2520)	1
[RELAY] Add MergeCompilerRegions pass (#5134)* [RELAY] Add MergeCompilerRegions passThis pass is part of the flow to support creating compilerregions with multiple outputs. It should be called afterAnnotateTarget and will merge together regions that sharethe same target to create larger compiler regions that canbe off-loaded to external codegens.This pass implements an algorithm to ensure that during themerging, no data dependency issues are created. See the testsfor an example of this case.Co-authored-by: Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>Co-authored-by: Manupa Karunaratne    <manupa.karunaratne@arm.com>Change-Id: Ibd99083564608d888482f57c5080109f3eefec88* [RELAY] Annotate compiler_ends on each edgeThis alters the behaviour of the AnnotateTargetpass to enforce the property that all compilerannotations exist along a single data flow edge.Specifically, this means they should have exactlyone parent and one child.Change-Id: I0e74803a77767f4f377d17755a13a74a30909797* Fix comment* Rebase *Node::make* Moved block outside for loop* Code style* Update make API* Remove comment* Remove redundant 'else's* Make one line* Fix comment* RefWrite* Fix merge ordering* Add the RFC example as a test* [FIX] Fixed merging behaviour in AnnotateRegionSetDeleting items from a list while iterating it seems toresult in undefined behaviour which sometimes segfaults.This makes sure all the item deletion happens separately.* Added checks* Move comment* Update comments	5
[FRONTEND][MXNET] Add squeeze_axis support to split operator (#1288)	1
LSTM Memory Allocator Fix #5035 (#105)* Imbalance version of shared pool during plan memory* Bug fix for no shared_pool case* Auto search and updated shared mem pool* Cleanup unused code* Cleanup logging code* Add unit test for shared storage* Remove shared pool in PlanMemory. Fix lint warnings* Fix lint warnings* Use reference instead of ptrs	1
[CI] Add file type check (#3116)	2
update doc (#47)	2
[TOPI][CUDA] Support cuBLAS BatchMatMul (#3936)* Support cuBLAS BatchMatMul* Add test and check target name	1
[OP] Finalize Op registry	1
[TIR] Allow compute_at create block predicate for non-trivial bounds and support floordiv pattern (#9527)* allow generate block predicate in compute_at schedule* revert #9880 and add more testcases	3
[NODE] Enable global singleton object, allow set_body_typed in function registry, default fallback of IRPrinter. (#1652)	1
[QNN] Support 4D padding. (#5036)* [QNN] Support 4D padding.* Empty commit.Co-authored-by: Ubuntu <ubuntu@ip-172-31-38-96.us-west-2.compute.internal>	1
Added tflite frontend support for quantized mean. (#4339)	1
[BYOC] Make CUTLASS BYOC integration 'Collage friendly'   (#11631)* [BYOC] Make CUTLASS BYOC integration 'Collage friendly'(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).Currently CUTLASS has four entry points: - The usual 'partition_for_cutlass' partitioning function, using the   standard pattern table and pass machinery (see cutlass/build.py). - A 'tune_cutlass_kernels' function which augments CUTLASS partition   functions with the results of building and running test kernels (see cutlass/build.py). - A 'relay.ext.cutlass' external codegen function which inspects the   turning results and generates a CSourceModule for each partitions   (see cutlass/codegen.cc). - A 'build_cutlass_kernels_vm' function which runs 'export_library' with   all the nvcc compiler options needed to build all the CSourceModules   (see cutlass/bild.py).For Collage we'd like CUTLASS to have only two entry points: 'partition_for_cutlass',and 'relay.ext.cutlass' or equivalent. This makes the CUTLASS external codegen integrationcomposable with other integrations, which in turn helps Collage avoid having to understand anyexternal codegen APIs other than the global pattern table and the custom compilation function/pass.Collage also tends to end up requiring multiple partitions for the same backend since it ismore aggressive at mixing-and-matching smaller sub-graphs between backends. Thus we'd also liketo make sure all tuning, generated code and compilation overhead is shared between all such CUTLASSpartitions.So, in this PR: - We add all the CUTLASS-specific tuning and compilation options as new Target   attributes for the 'external codegen' "cutlass" TargetKind (cutlass/target.cc).   The user now has one place to provide those settings, and we've already done the   legwork to plumb the target instance. - We replace 'relay.ext.cutlass' with a 'RelayToTIR' custom pass hook   'CompileForCutlass' (see cutlass/codegen.cc). This pass obviously can see all   the CUTLASS partitions in the IRModule, so we can now share tuning results   between them all and can be sure to generate a single CSourceModule. The pass can   also invoke the compiler to yield a StaticModule, which we've also already done the   legwork to support. In this way all CUTLASS-specific steps are handled at once. - For convenience we supply 'finalize_modules' and 'finalize_modules_vm' which   invoke nvcc for final linking (using export_library as usual). However, there's now   nothing CUTLASS specific in those helpers other than their overriding of the 'compiler' to   be nvcc. - test_cutlass.py is updated to use the new API. Though this is a breaking change for existing users of the CUTLASS integration the change is pretty minor, as shown in test_cutlass.py.* - Masa's comments* - Remove unnecessary save.	4
Add RISC-V build/test pipeline to Jenkins. (#12441)	3
[TOPI] NCHWc added input shape 4 condition, intel graphics conv2d schedule debugged for inception_v3 workloads (#2265)	1
[TIR][REFACTOR] RewriteForTensorCore -> te/schedule (#5379)* [TIR][REFACTIR] RewriteForTensorCore -> te/scheduleRewriteForTensor depends on the schedule information, which makes it differfrom a typical pass(which should get all the information from the input TIR).As a result, we refactor it as a SchedulePostProc step for now.We should revisit it later as we introduce more support for tensor core patterns in the TIR.* Fix VTA to fit the new IR Pattern	1
Update CONTRIBUTORS.md	5
[MetaSchedule] Mutator: Mutate compute location (#10028)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[PASS][TENSOR] Use correct select semantics (#2394)	1
Fix fatal error <CL/opencl.h: No such file> when building with CMake (#1045)	1
[MetaSchedule] Introduce `Union` and `OrderedUnion` in Database (#12628)Following up #12520 and #12626, this PR introduces two database classes:`UnionDatabase` and `OrderedUnionDatabase`, both of which allow users toorganically compose multiple databases together, so that the high-levelIR (Relay, Relax) could select the best tuning records according torunning time or a preferred order given by users.To each query, `UnionDatabase` returns the best record among all thedatabases given; Instead, `OrderedUnionDatabase` returns he record fromthe first database that responds to the query.Used together, users may specify complicated dispatching patterns likebelow:Examples below demonstrate the usecases of and difference betweenUnionDatabase and OrderDatabase.Assumption:* db1, db2 do not have tuning records for the target workload.* Each of db3, db4, db5 has tuning records r3, r4, r5 for targetworkload respectively.```python#### Case 1. `UnionDatabase`:merged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    db4  # has r4)# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)### Case 2. `OrderedUnionDatabase`merged_db = ms.database.OrderedUnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    db4  # has r4)# returns r3merged_db.query_tuning_record(..., target_workload)### Case 3. Mix-use scenariomerged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    ms.database.OrderedUnionDatabase( # returns r4        db4,  # has r4        db5,  # has r5    ))# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)### Case 4. Another mix-use scenariomerged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    ms.database.UnionDatabase( # returns the better one between r4 and r5        db4,  # has r4        db5,  # has r5    ))# returns the best one among r3, r4 and r5merged_db.query_tuning_record(..., target_workload)### Case 5. Yet another mix-use scenariomerged_db = ms.database.OrderedUnionDatabase(    db1, # no record    db2, # no record    ms.database.UnionDatabase( # returns the better one between r3 and r4        db3, # has r3        db4, # has r4    )    db5,  # has r5)# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)```Co-authored-by: sunggg <49998730+sunggg@users.noreply.github.com>	1
[Relay][Doc] Separate arguments types formatting with comma (#2690)	2
[FIX] Fix clang12 warnings (#7593)	2
[CuBLAS] Support implicit broadcast in batch_matmul (#8229)	1
update dlpack (#382)	5
[TVMC] add the support of the cross compiler options (#7922)Add the possibility to provide the cross compiler options when using thetvmc compile functionality.With some cross compiler, toolchains --sysroot option (at least) need to bedefined.tvmc/test_compile.py as been updated to introduce simple tests to validatethe cross options functionnality.Signed-off-by: Vincent ABRIOU <vincent.abriou@st.com>	1
Do type checking for the input and kernel in the qnn conv2d (#3904)* [QNN] Convolution 2D Implementation.Rebasing. Empty commit.Clang-format styling.* Reformatting code.* Fixing lint issues.	0
[CI] Update docs style dependency. (#7034)	2
checkin stmt	5
[LANG] Include buffer semnatics, introduce pylint (#11)* [LANG] Include buffer semnatics, introduce pylint* Refactor inline add support for buffer indexing* fix doc	2
Fix the missing `dtype` attribute of `tir.Shuffle` in Python level (#9131)	0
[COMMUNITY] hypercubestart -> Reviewer (#6511)	3
[TVMScript] Parser `int64` support (#10789)## ContextWhen dealing with end-to-end models, we note that some tensors may have large shapes. Thus, when designing graph-level IR, we sometimes use `int64` instead of `int32` for the shape. Below is an dense GeMM example which has `int64` input tensor shape:```python@tvm.script.ir_moduleclass Module:    @T.prim_func    def main(rxplaceholder: T.Buffer[(1, 512), "float32"], rxplaceholder_1: T.Buffer[(T.int64(1000), T.int64(512)), "float32"], T_matmul_NT: T.Buffer[(1, T.int64(1000)), "float32"]) -> None:        # function attr dict        T.func_attr({"global_symbol": "dense", "tir.noalias": True, "op_pattern": 3})        # body        # with T.block("root")        for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 4, 1, 25, 8, 1, 10, 64, 1, 1):            with T.block("T_matmul_NT"):                i = T.axis.spatial(1, 0)                j = T.axis.spatial(T.int64(1000), i1_0 * T.int64(250) + i1_1 * T.int64(10) + i1_2)                k = T.axis.reduce(512, i2_0 * 64 + i2_1)                T.reads(T_matmul_NT[i, j], rxplaceholder[i, k], rxplaceholder_1[j, k])                T.writes(T_matmul_NT[i, j])                T.block_attr({"layout_free_placeholders":[rxplaceholder_1], "meta_schedule.tiling_structure":"SSRSRS"})                with T.init():                    T_matmul_NT[i, j] = T.float32(0)                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]```## ProblemThough our TVMScript printer can easily print `int64` constants, the parser had poor support for `int64`. So this PR introduces some parser support for `int64`, basically about the data type of loop variables, block iterators and block read/write regions.Besides the parser, most of the TIR schedule primitives didn't take `int64` into account in their implementations. These schedule primitives will be fixed and updated in recent future, in followup PRs.	5
Add ostream formatters for TargetPtr/TargetVal. (#5592)	1
[Vulkan] Remote target.h #include (#8813)Was added in #8127, should have been removed in #8171 along with therest of the references outside of libtvm_runtime.so.  This didn'timpact the Vulkan+g++ builds, because no symbols were accessed outsideof the runtime library.  However, it broke the Vulkan+Windows builds,which expected symbols due to the `__declspec(dllexport)` defintion of`TVM_DLL` on MSVC (see #8805).  This wasn't caught by the CI build onWindows, because it doesn't perform the Vulkan build.	1
fix coreml tools (#72)	0
Update SGX example (#1825)	5
[CMSIS-NN] Include clip in the qnn binary op patterns (#10548)* [CMSIS-NN] Include clip in the qnn binary op patternsChange-Id: I3406c4ff90d26392b92675f09f9d8c872ddd596f* Removed redundancies in extraction of clip node in binary opsChange-Id: If6472a3fed6a3df6fbc55615982b8cc5eb40c310	4
[NNVM][Keras] allow only tensorflow backend (#1392)	1
[CI] Update to PyTorch v1.10 in GPU image (#9866)* apply PT vs LLVM symbol conflict mitigation* update ci-gpu to v0.79 with PT 1.10.1* disable quantized mv3 test due to weird segfault from torch	3
Add proper cmake PATHS when multiple NAMES. (#6558)	1
[ETHOSN] Fix output tensor ordering (#12317)During compilation of an NPU subgraph, the input and output tensorordering is determined by creating a mapping of Support Librarybuffers to TVM buffers. The runtime had been using this mappingincorrectly by instead interpreting it as a mapping of TVM buffers tosupport library buffers. This can result in undefined behaviour. Theruntime now interprets the mapping as Support Library buffers to TVMbuffers.A test has been added to check the correct ordering of outputs isprovided correctly in the runtime.	1
[Relay] Minor fix for some TF OD models (#6729)* Minor fix for some tf od models* More fix* Minor fix* Fix lint* Minor fix	0
Improve tol to resolve flaky case (#4836)	0
[TVMScript] Add for loop syntax sugar (#9620)* add for loop syntax sugar* remove prints* better doc* finish thread binding* fix CI* fix CI* address comments* update sstub* fix CI* remove failed test* update stub* address comments* add decorator	1
[NNPACK] Add check for NNPACK being available (`nnp_initialize()` succeeding) (#2119)This fixes issues with failing tests on PowerPC.	3
[LINT] clang-format the h,cc,m files. (#5557)This PR prepares for our migration to use the clang-formatas part of the linter system.	5
Fix Python syntax error in start_rpc_server_to_tracker.py (#4682)[flake8](http://flake8.pycqa.org) testing of https://github.com/apache/incubator-tvm on Python 3.8.0$ __flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics__```./apps/vta_rpc/start_rpc_server_to_tracker.py:18:18: E999 SyntaxError: invalid syntaxPROJROOT="$( cd "$( dirname "${BASH_SOURCE[0]}" )/../../" && pwd )"                 ^```	0
[Container] Fix NDArray SaveDLTensor declaration and implementation signature different (#4586)	0
[Relay] Add support for TupleGetItem in op fusion (#2914)	1
[AutoTVM] Add support for text buffers to ApplyHistoryBest (#12521)Currently, AutoTVM's ApplyHistoryBest class does not support loading tuning logs from memory. This is a pet peeve of mine, as it requires you to work with a tempfile whenever writing autotuning tests. This is also just strange, as the rest of AutoTVM has support for text buffers (e.g. tvm.autotvm.callback.log_to_file supports passing in a text buffer, letting us write to but not read from them).Additionally, ApplyHistoryBest handles input arguments very unintuitively. Before this PR, it allowed users to pass string filepaths, a list of string filepaths, or an Iterable (such as a list) of input and result tuples. However, it did not support taking in StringIO objects as mentioned above, nor pathlib.Path objects, nor combinations of a filepath and an Iterable of tuples.In a perfect world, we would change ApplyHistoryBest to take as input a path-like object, file-like object, or an Iterable of input and result tuples (similar to what ApplyGraphBest takes as an argument). However, this would break the existing functionality to take as input a list of filepaths.To be backwards compatible, while fixing this issue, this pull request defines a new type inside dispatcher.py:Records = Union[    Union[str, bytes, Path],  # Path-like objects    TextIOBase,  # File-like objects    Iterable[Tuple[MeasureInput, MeasureResult]],]It then rewrites ApplyHistoryBest.load so it takes the following arguments:def load(self, records: Union[Records, Iterable[Records]]):This PR also adds unit tests for this new functionality, and fixes a relevant bug in tests/micro/common/test_autotune.py in which a StringIO object was passed to apply_history_best, causing it to appear to pass but not actually read any data.	5
[Topi] Fix GPU Dynamic Topk by Improving Dynamic Strided Slice in Topi (#7018)* Fix GPU dynamic Topk* Fix style* Minor fix* Simplfy dynamic checking* Fix lint* More improvements* Disable test any topk	3
Update CMakeLists.txt to be more flexible (#3354)	5
[MetaSchedule][M4b] Testcases for TensorRT builder/runner (#10055)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
[MyPy] Extend type checking and annotation for TIR (#8429)	5
[RPC] safe destructor when remote server get killed (#1086)	1
[NNVM][KERAS]LSTMCell support (#1686)	1
Make sure to visit the arguments of inlined functions (#4783)	1
[BUILD] Allow inject custom pass via phase (#408)	4
fix name (#3719)	0
Fix While Node StructuralEqual and StructuralHash issue (#11073)	0
[BUFFER/REFACTOR] Buffer byte_offset-> elem_offset, add buffer_bind_scope (#209)	1
kCustomBegin overlapped with kExtEnd; incr by 1 (#3250)This was a typo in the original custom datatypes PR.	5
support any shape and axis for log softmax (#11951)	2
Deploy docs to tvm-site/asf-site on main (#10494)* Deploy docs to tvm-site/asf-site on maincommit-id:59241556* Use oauth* testing codecommit-id:6cc27fceCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[BACKEND][CODEGEN] C codegen with tests (#2161)* Implement C code generation with tests* Code cleanup* Implement C code generation with tests* Code cleanup* tabs to spaces* make lint compliant* update export_library and reserve unique C keywords* move ReserveKeywordsAsUnique to codegen_c* some documentation and code cleanup* use tvm.contrib.util for tempdir in testcases	3
[Torch][AArch64] Skip test_load_model___wrong_language__to_pytorch (#12660)This patch makes test_load_model___wrong_language__to_pytorch to beskipped in AArch64 due to a bug that can be reproduced when enablingIntegration Tests in machines with Torch installed in TVM.```The error message seen is:OSError: /usr/local/lib/python3.7/dist-packages/torch/lib/libgomp-d22c30c5.so.1: cannot allocate memory in static TLS block```While the test needs further investigation, it is being set asskipped so other tests can be enabled and not to regress and allowtime for the investigation to be made.This relates to the issue described in #10673.	0
Add sse4/avx2 support for fast x86 int8 (vpmaddubsw/vpmaddwd/vpaddd) (#8897)* Add sse4/avx2 support for vpmaddubsw/vpmaddwd/vpaddd- Extend the list of different target for x86 topi- Extend tests for conv2d x86 int8 for fast i8 x86 platforms* fix code style* Change x86-64-v2 to nahalem in test to support llvm11* Change test target to get NCHW8c	1
[Relay][TOPI] Add rsqrt operator (#2949)	1
[RUNTIME] Fix compile errors of OpenCL FPGA backend (#4492)	0
[QUANTIZE] Refactor quantization codebase and fix model accuracy (#3543)* Refactor.* update* update* update* update* update* update	5
[RELAY][DYN] Dynamic broadcast_to, zeros, ones (#6007)* Dynamic BroadcastTo* fixed lint!* add test_one_hot() back* add one_hot registration back* Dynamic BroadcastTo* fixed lint!* add one_hot registration back* fixed lint.. again* fixed lint* lint* responding to comments* skipping cuda in dynamic test* skipping cuda in dynamic test* fixed i386 test and GPU test* lint* starting ones and zeros* fixed dynamic ones and zeros, wrote dyn ones and zeros test* added static version of zeros, ones and added a check for size of types to static BroadCastToRel* added dynamic to static pass for zeros and ones, dynamic test and dynamic to static test* removed op_str in dyn to static pass test* fixed lint* fix lint hopefully* removed import const* removed import that was actually used* copy all attributes from broadcast_to, ones, zeros, full* responding to comments* fixed build error* finishing rebase* fix lintCo-authored-by: Lily Orth-Smith <lorthsmith@Lilys-MacBook-Pro.local>	0
[TOPI] Update softmax compute and CPU schedule (#3680)* Update Softmax compute and CPU schedule* Add C++ compute* Fix schedule* Update CUDA and OpenGL schedules* Fix log_softmax* Fix hls and opengl schedules* Fix CUDA schedule	0
[Web][Emscripten] Update EMCC C++ standard to C++17 (#12693)As a follow-up to https://github.com/apache/tvm/pull/12337, updatingthe EMCC flags from `-std=c++14` to `-std=c++17`.	5
[TIR] Use PopenPool instead of multiprocessing.pool (#8492)Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[RELAY][OP] comparison (#1824)	5
[FRONTEND][TENSORFLOW] Add Transpose support. (#1665)* [FRONTEND][TENSORFLOW] Add Transpose support.* [FRONTEND][TENSORFLOW] Get parameter from inputs and fix document style.* [FRONTEND][TENSORFLOW] Handle the case that perm is not specified.* [FRONTEND][TENSORFLOW] Convert Rank and Range to param.* [FRONTEND][TENSORFLOW] Fix a pylint issue.* [FRONTEND][TENSORFLOW] Implement Rank and Range as normal op.	0
[AutoTVM] Fix a bug in simulated annealing (#3413)* [AutoTVM] Fix a bug in simulated annealing* Update sa_model_optimizer.py	5
Add tool to clear stale images. (#11772)	1
[BYOC][ACL] Update installation docs (#9426)Also updated CMake to proper casing	1
[Formatting] Fix black script for Python formatting (#6469)	0
[BugFix] Use shape dtype on ArgReduce to determine return type (#12083)Fix ArgReduce automatic return type inference by forcing it to use thedatatype of the shape of the Tensor instead of the fixed Int32.Including additional tests.	3
Only use thrust for cuda target (#6722)	1
Support runtime defined function wrapping of library module packed functions (#9342)* Expose DSOLibrary and add documentation.* Allow runtimes to specialize library module functionwrapping by providing a PackedFunctionWrapper objectat construction.* Apply clang formatting.* Use std::function.* Minimize DSOLibrary interface.* Add param and return documentation to PackedFuncWrapper type alias.	2
Update compile_engine.py (#4393)	5
[VERSION] Make script path invariant (#6766)	1
[REFACTOR][IR] Migrate IRModule ObjectRef to not-null (#5654)	4
Fix build break on Windows. (#1179)	4
Allow ubuntu_install_darknet.sh to work in both 18.04 and 16.04 (#5574)	1
[RELAY][[PASS] Consolidate ForwardRewrite pass. (#2124)	4
[Meta Schedule][M3b] Database (#9061)This PR is part of the meta schedule project (#8473) that adds a genericDatabase interface of tuning records, as well as a default implementationof using two JSON-files to mimic the database.This feature is future-compatible with dynamic shape auto-tuning.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
Fix an infinite recompilation loop in tvm-sys. (#9450)Including the CMake `build` directory in `reun-if-changed` was leadingto `tvm-sys` recompiling on every invocation of `cargo build` (alongwith every crate which depended on it).	5
[AutoScheduler] Add a tutorial on auto-scheduling a network for x86 CPU (#7019)* [AutoScheduler] Add tutorial on auto-scheduling a network for CPU* update* update* update* improve* improve* address comments* add help on layout conversion* add help for layout conversion* update target string* update cuda logs	2
[Relay] visit the span (#1990)	5
[Relay][Op] Clip (#1844)	5
[TOPI] Fix nn.lrn result dtype on fp16 (#11032)The buggy script as below:```pythonimport tvmfrom tvm import relayfrom tvm.contrib import graph_executorx = relay.var("x", shape=[1, 3, 224, 224], dtype="float16")y = relay.nn.lrn(x)mod = tvm.IRModule.from_expr(relay.Function([x], y))lib = relay.build(mod, target="llvm")f = graph_executor.GraphModule(lib["default"](tvm.cpu()))f.run()```The error I get is```Check failed: ret == 0 (-1 vs. 0) : Assert fail: (((tir.tvm_struct_get(arg.T_divide, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg.T_divide, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg.T_divide, 0, 7) == (uint16)1)), arg.T_divide.dtype is expected to be float32```	1
[NNVM][TENSORFLOW]Fix lstm testcase to support get_output without size input (#1731)* [NNVM][TENSORFLOW]Fix lstm testcase issue to support get_output without size input* removed redundant* Enabled inceptionV1 testcase	3
[Relay][Any] Add shape func for dynamic shape (#3606)* init shape func in interpreter and vm compiler* Update interpreter* fix* lint* lint* fix* remove hack* update* fix* fix* update* address comments & update for shape_of* fix lint* update* fix hybrid* lint* fix bug & add take shape func* lint* lint* update* fix flaky test* add todo	2
[RUNTIME] Support TVMContext (#1720)	1
[TEST] recover tflite test (#2788)	3
[AutoScheduler] Print the time used for measurement (#6972)* [AutoScheduler] Print the time used for measurement* address comments	1
Mark x86 specific test (#10672)Currently this test crashes pytest on any other architecture, this introduces a decorator which can be used to mark future x86 tests.	3
[OpFusion] Make the max number of fused ops configurable (#6327)	5
[LANG/CODEGEN] Intrinsics and Extern Math (#101)* [LANG/CODEGEN] Intrinsics and Extern Math* fix lint	0
[Hexagon][CMake] Propagate build type to external cmake calls (#10711)	1
[Bugfix]  fix the bug that occurs when the test_pass_ctx_exception() is (#9774)tested separately.In the batch test ,cause a previous function will specify the value ofPassContext.current() as None,so tests the test_pass_exception() will pass.but when testing it individually ,the result of PassContext.current() isa [] instead of None,it will not pass, this may happen when using theoption of "pytest -n 4"	3
[microTVM][Zephyr] Add recommended heap size for NRF and qemu_x86 (#12585)This PR sets recommended heap size for qemu_x86 and NRF board to fix memory size with models like VWW using AoT host driven executor.	1
[ETHOSN][CPP-RPC] Link NPU runtime in CPP RPC build (#11946)When building the CPP RPC package with the NPU enabled,`link_directories` fails to find the NPU runtime libraries. This ispresumably because the TVM runtime is linked with the PRIVATEoption in: https://github.com/apache/tvm/blob/main/CMakeLists.txt#L601.Therefore working around this by following the precedent of otherlibraries such as Hexagon and Open CL.Change-Id: Iba2fbc245df18147e3b564ba807ca78c9cc8461d	4
Fix long for windows in cuda (#700)* Use long long for platforms where long is 32 bits (like windows).* Make sure scalar chars are signed.* Re-add NOLINT marker.	1
[Relay] Fix typo in ChangeBatch (#3660)	4
Additional MXNet Convolution and Deconvolution tests (#4026)Add different batch sizes and channel numbers toMXNet Convolution and Deconvolution tests.	3
Register shape functions for some image related ops (#6373)* debugging* added three shape funcs* fix lint* address comment* resolve conflicts* resolve conflicts* resolve conflicts* resolve conflicts* resolve conflicts	5
Fix Error messages in tflite.py (#3320)	0
[FIX] Fix autoscheduler tuning on sparse matrices where there are multiple with the same shape (#7974)* [FIX] Fix autoscheduler tuning on sparse matrices where there are multiple with the same shape* formatting* remove unreachable code	4
[Relay][Training] Add more missing gradients (#6767)	1
[WIP] [CI] Bump CI GPU image version (#11637)* Bump CI GPU image version* Run generate,py	1
Add skip to flaky MacOS RPC test (#9753)* Add skip to flaky MacOS RPC test* Use flaky marker instead* link issue* trigger ci* trigger ciCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Hexagon] Reuse Hexagon SDK analysis across cmake files (#8822)* [Hexagon] Reuse Hexagon SDK analysis across cmake filesDifferent versions of the Hexagon SDK may have different directorystructures. Extract the directory identification code into a separatecmake module. Use that module in Hexagon.cmake and in the cmake filefor the FastRPC libraries.* Don't modify CMAKE_SHARED_LINKER_FLAGS, instead set target properties* Add quotes around ${...}* Add USE_HEXAGON_ARCH variable to cmake configuration* Restart build	5
[TRT][BYOC] handling dynamism in TensorRT to support OD models (#6905)* handling dynamism in TensorRT to support OD modelsrefactoring test tensort codeadded comments to dynamic check wrapperlog.warn changed to logger.infoTRT codegen taking slice_mode into accountTRT codegen to handle both stride_moderefactoring TRT codegenadding a test for dynamic offload[TRT] bug in codegen for slice_mode=endctx determined from target in test + io test was missing* Addressed the formatting/refactoring comments* Addressed comment in TRT codegenLint formatting* Lint error* using slice_mode during strided slice registration in tensorrt.py* removed a few blank lines* addressing cli comment on elif-return* Added decorator for tensorrt functions with dynamism checkskip_codegen added for test_tensorrt::test_dynamic_offload* addressed comments in PR + black linting* resolved import error in test_tensorrt* import mxnet location changed to pass CI* test_integration removed as components were run by pytest anyway	3
fix typo in resnet definition (#1995)	5
[Fix]use a more intuitive way to limit the #ops in a group (#4018)* use a more intuitive way to limit the #ops in a group* format	1
Fix the Makefile for howto_deploy (#4457)	2
Fix tests for tflite unary elemwise operations (#4913)* add TFLite version check for 'ceil' and 'cos'* fix name check of test_op for positive inputs* add error message for operator not found in the installed fbs schema	1
[TVMScript] Support inlined function call as a sugar (#11324)* [TVMScript] Support function call to help construct AST* add test* update test* more comment* fix for avoiding Buffer.vload(...) case* update parse error msg* wrap func call with try / catch, emit error msg* silence pylint	0
[RELAY]Slice_like support (#2014)	1
[BYOC][ETHOSN] Fix tests for new module API (#6560)* [BYOC][ETHOSN] Fix tests for new module APISome of the downstream variants of our tests hadbeen broken by a recent change to the API of build.This both fixes that and refactors a couple of testsso that they will run entirely in upstream CI andwe won't see this sort of failure again.Change-Id: I841266eef0e2e89cc76e0526fc6cd3fc8d1326d8* Only run mobilenetChange-Id: Ie41c6d2c13c4473ecaa5c50c33d2c1589c742796* Improve docsChange-Id: I2c8bde44278e4cbc9cea5c5cbd4bb3c316ec37ae* More docsChange-Id: Ia9973915eecea647689535cc1e6eef9228111324	4
[COMMUNITY] @hlu1 -> Reviewer (#3021)	3
[PROFILER,VM] Fix timer device type for reshape_tensor (#9518)* [PROFILER,VM] Fix timer device type for reshape_tensorThe index of the host device was changed in the VM's `devices_` array,but we forgot to update this index in the profiler.* formatting	2
fix	0
Use print() function in both Python 2 and Python 3 (#3440)Discovered via: __flake8 . --count --select=E9,F63,F72,F82 --show-source --statistics__Legacy __print__ statements are syntax errors in Python 3 but __print()__ function works as expected in both Python 2 and Python 3.	1
[INFA][IR] Build and Evolve Low-level IR. Remove HalideIR dep. (#3533)* [INFA][IR] Build and Evolve Low-level IR. Remove dep from HalideIR.* Update include/tvm/node/ir_functor.hCo-Authored-By: Jared Roesch <roeschinc@gmail.com>* Update include/tvm/node/ir_functor.hCo-Authored-By: Jared Roesch <roeschinc@gmail.com>	5
[Relay] GradientCell Relay Pass (#5039)* save* gradient.rly* fix* NOT WORKING: gradient cell pass* test gradient pass* fixed basic call ops* more tests* fix bug* transform calls to one ones_like zero zero_like* maintenance stuff* fix linting* linting* linting* throw default* remove unrelated changes* import gradent.rly in pass* comment* linting* remove changes to test files* move gradient_cell.cc to transforms* revert change* update files with new commits* type* wrapper function to main outermost function type* fix linting* fix unsigned and signed int comparison* review* GetConstructor definition in module and change op comparison* update node instantiations* increase code readabilityCo-authored-by: Marisa Kirisame <lolisa@marisa.moe>	1
[Autoscheduler] Add sparse conv2d(1*1) support for auto_scheduler (#8065)* add sparse conv2d support for auto_scheduler* add description* fix bug* fix annotation* Lint fixCo-authored-by: laiyin.lyc <laiyin.lyc@alibaba-inc.com>	0
declare a type name for each tuple type (#151)* declare a type name for each tuple type* generic way to declare type names for each tuple type* fix lint error* update submodule dmlc-core	5
sort.cc added to runtime for nms compatability (#7942)* sort.cc added to runtime for nms compatability* remove include* fix clang lint* sort includes in alphabet orderCo-authored-by: Alexander Serov <alexander@tech5.com>	0
[RELAY] Fixes to MergeCompilerRegions (#5195)* [RELAY] Fixed issues with MergeCompilerRegionsThis PR addresses a few outstanding issues withthe implementation of MergeCompilerRegions. Inparticular, it now handles TupleGetItem nodes properlyand other minor bugs related to region merging havebeen fixed.Change-Id: I07783afc56183a6f798a510209f23b0a5f252255* Fixed issue using pre-merged regionsChange-Id: I0a844ac59bda1089ae0c67cef52f0b0c7ab2cbd7* Removed some debugging logicChange-Id: Ib6f2eede6f38bbb270073eb8d4c4dc19f60832c6* Remove default annotationsChange-Id: I9b7696a51c95871491cbea33c40f92ec327e417f* Annotate default 'if'sChange-Id: I0098bd1bf6788dd6366810dcefa84f1ebbffaab0* Clang formatChange-Id: I944365cd3080a97a9261f643a8f1efa5a63cf82b* Use src/dest in mergeChange-Id: Ie43113492bda8f1ce63eaf9615cb645bb9e2ee86* Fixed partition testChange-Id: I46f9e349b1a813a9140f7e4f8a2241687e2df73b* Removed commentsChange-Id: I309afdd1951d7e796e41d13788aa487707e0ac4c	4
Revert "update dependency (#1495)" (#1499)This reverts commit f1f30c4c4ab5e8a47a0d76c8cd2c5344c0a9096a.	4
Fix a memory leak in SetParams (#7960)* Fix a memory leak in SetParamsToDLPack creates a DLManagedTensor instance, but nobody delete this i8nproper way. We can use operator-> for getting access to DLTensor.* Remove other usage of ToDLPack()	4
[Relay] Fix name of bias in testing.mlp (#2892)	3
Fixing package path in tflite test (#3427)	3
[Relay][pass] call graph for relay (#4922)* call graph for relay* CallGraphEntryNode->CallGraphEntry, __getitem__->print_var* fix typos	2
keep parameter names from PyTorch (#5887)	2
[Rust] Fixes "common" sub crate using nightly and master (#3965)	1
[Vulkan][Topi] Parametrizing additional topi tests, marking vulkan failures (#8904)* [Pytest] Fixed TestTargetAutoParametrization in cases where LLVM is disabled.* [UnitTests][Vulkan] Improved robustness of test_tir_intrin::test_clzPreviously, would fail during build since support for Int64 primitives wasn'tdeclared in the `"vulkan"` target.  Now, uses `"vulkan -from_device=0"` targetand marks the test as xfail if the current target doesn't support Int64.* [UnitTest][Topi] Parametrized several unit tests, identify vulkan failures- Parametrized topi modules  - test_topi_conv1d_transpose_ncw.py  - test_topi_conv2d_nhwc.py  - test_topi_correlation.py  - test_topi_loss.py  - test_topi_math.py  - test_topi_reduce.py  - test_topi_softmax.py  - test_topi_sort.py  - test_topi_unique.py  - test_topi_vision.py- Unit Tests fixed  - `test_topi_loss::test_nll_loss`, failure due to `supports_float64`    not being passed from the target to the codegen.- Known Vulkan failures (tracked in https://github.com/apache/tvm/issues/8903)  - test_topi_math.py::test_ewise, ["tan", "erf", "isnan", "isfinite", "isinf"]    Unimplemented CallNode operations  - test_topi_reduce.py::test_reduce_map, ["sum", "any", "all"]    Fails during codegen, unexpected size of data type.  - test_topi_vision.py::test_proposal    Marked test_proposal as xfail on vulkan, currently has a type error    between bool/int8.  - test_topi_conv1d_transpose_ncw.py::test_conv1d_transpose_ncw    Incorrect numeric output, a few elements outside of allowed    tolerance, only occurs on vulkan backend.  - test_softmax.py::test_softmax    Marked float64 operations as xfail in vulkan, because GLSL.std.450    only supports 16/32-bit floats.	1
Fix a bug in nnvm to relay converter. (#2756)	0
Use auto-tuner to improve conv2d_gemm performance (#6117)* Use auto-tuner to improve conv2d_gemm performanceThe following tuning entities have been introduced:- Unrolling and vectorizing input matrix transform- Reordering gemm to exploit parallel threads- Unrolling `gemm_quantized` intrinsic- Interleaving `gemm_quantized` intrinsicChange-Id: Icd3ab005663f78a80672e71ef368f6d0efa4a401* RebasingChange-Id: Id27b6de705b16b93df8e885868961fa0321497be* Fix python lintingChange-Id: I77d880424c3e7ce9de67c970ddb2cf2a92b52f79* Fusing batch into inner dimensions before parallelizingChange-Id: Ic58d1138ab96d58d12f5855f0e1044f10d9e6e9b	4
Remove AOT Executor header from Arduino project (#8857)	4
[topi] block sparse dense on cuda (#5746)	5
[CI] Keras version upgraded from 2.3.1 to 2.4.3 (#6793)	5
[Relay,Topi][OP] affine_grid and grid_sample (#5657)* [Relay,Topi][OP] affine_grid and grid_sample* lint	5
[Relay]fix heterogenous annotation bug (#2622)	0
[FIX,PROFILING] Add USE_PAPI configuration to config.cmake (#8567)	5
[Caffe Frontend] introduce caffe frontend for tvm (#6206)* [Caffe Frontend] introduce caffe frontend for tvm.* [Caffe Frontend] fix bugs for generating caption in tutorial.* [Caffe Frontend] delete statement for python2 and modify the function name.* [Caffe Frontend] change the directory which will hold the tmp fileswhen testing the caffe frondend.* [Caffe Frontend] delete tutorial about caffe frontend.* [Caffe Frontend] delete some print statementsCo-authored-by: fernchen <zifeng.cf@alibaba-inc.com>	4
[Bugfix] [tir] do not simplify 'Any() - Any()' to 0 (#8266)* fix* fix lint* remove* address comments	1
[CI] Update GPU image for oneflow v0.7 (#11085)	5
[PyTorch] Fix rsub type (#10090)* [PyTorch] Fix rsub type* fix	0
[QNN] Refactor fixed point multiplication in requantize (#4073)	0
fix (#134)	0
download fallback config file for search from tophub if it does not exist (#4671)	2
fix tutorial build (#390)	0
[Relay][TensorFlow] Add support for SquaredDifference (#3930)* Add support for SquaredDifference and StopGradient; minor fix in BatchMatMul* Remove stopgradient change* Resolve PR comment* Dummy change to retrigger CI* dummy change to retrigger CI	4
Improve dtype detection in loop to fix onnx tests. (#7934)	3
Fix type var docs (#4208)	2
[FRONTEND][TF] conv2d_transpose 'SAME' support kernel more than 1x1 (#4484)* [FRONTEND][TF] conv3d_transpose 'SAME' support kernel more than 1x1* revised per as review comments* add more fallback wolkaround to make all tests pass	4
[METAL] Fix codegen for inf and erf (#8054)* [METAL] Fix codegen for inf and erfFixed Metal codegen with using `inf` constant. Constant `INFINITY` isused now instead of `inf`.Also, Metal doesn't have `erf` built-in function. So, we are using`fast_erf` from tir. User will see warning message when we willgenerate `fast_erf` instead of `erf`.* Apply comments* Fix clang-format* Fix lint	0
[BUGFIX] Fix CRT static test bug (#5293)* [CI][DOCS] Make sure to refresh the cython part* [BUGFIX] Fix CRT static test bug* Fix demo_static* resolve review comment	0
[Relay] enable blocking format in x86 conv2d and fold scale axis (#5357)	0
[Relay] Install Relay Prelude program in package install (#4227)	2
[Frontend] Unnecessary default warning msg changed to debug (#7119)	0
[torch] Add linear operator support (#7569)	1
[ONNX] [Relay] Dynamic squeeze (#9095)* adding dynamic squeeze first steps* Matt B. implementing shape* squeeze implemented, dynamic_to_static and onnx importer next* add Squeeze op convert to onnx.py* dynamic to static* removed comments* removed comments* added comment* adjusted comment* black and lint* ran make format in root directoryCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>	1
[FIX] Fixes #6096 (#6131)Clear the compile cache between module builds so that schedule changeswill have an effect. Also, clear the warning cache so that schedulechanges properly list untuned ops.	4
Enable control deps in API (#55)* [SYMBOL] support control deps in API* enable more generic tuple list* fix* fix	0
Move infer_value to _get_list_param (#8051)	2
[Hexagon] Launcher modifications to make use of the new device API (#9356)* Use custom lookup_linked_params function inapp/hexagon_launcher.* Use new device api in hexagon launcher.* Ensure app/hexagon_launcher uses kDLCPU for external allocationsso that CopyDataFromTo knows how to handle the provided memory.* Use loadfile_hexagon in app/hexagon_launcher.* Update hexagon launcher's get_output method toutilize NDArray's for copying in order to exercisethe DeviceAPI.* Apply clang formatting	1
[Frontend][TFLite] Use axis.size instead of len(axis) (#8060)The variable axis is an ndarray.	1
fix typos in comments and relay tutorial (#5999)* [TypoFix]fix typos in comments and relay tutorial* retrigger	2
[cleanup] Log compile errors for AOT tests (#10214)* [cleanup] Log compile errors for AOT testsSee #10213* Update tests/python/relay/aot/aot_test_utils.py* removed the encode of msg that is already strCo-authored-by: lhutton1 <luke.hutton@arm.com>Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>Co-authored-by: lhutton1 <luke.hutton@arm.com>	1
[NNVM][TENSORFLOW] Fixed variable ops shape parsing issue (#1381)	0
[VERILOG] Basic Verilog Testflow (#70)* [VERILOG] Basic Verilog Testflow* fix build* fix the comment* fix lint in verilog	2
[microTVM] Add microTVM Template Projects to tlcpack pip Package (#9309)* zephyr lib fixed* restructure* readme* add arduino* add project template to setup.py* fix lint* fix tutorial* address comments from PR9274	1
[Relay] Handle ndarray_size in FoldConstant (#6156)* [Relay] Handle ndarray_size in FoldConstant* Use Optional	1
[REFACTOR] Add Types to IterVar, Isolate Operator (#62)* [IterVar/REFACTOR] Add types to IterVar* [ARITH/REFACTOR] Move IntSet to include* [REFACTOR/OP] Move Op detail to seperate folder.* fix test	3
Minimal example of tuning on hexagon. Fails in fast rpcs currently. (#11395)	0
[Heterogeneous][Bugfix] Fix bug of wrongly generated device_map (#2990)* fix bug of device_index* cpplint* nose* Update test_pass_annotation.py* fix name of testcase* delete comment	4
[Topi] Fast mode in take op (#3325)	5
[OP] PReLU Support (#394)	1
Update README.md	2
Fixed attribute parameter (#184)	2
Implement explicit IR representation of memory alloction (#3560)	5
[relay][topi] Add operation relay.nn.dilate() which calls topi.nn.dilate() (#5331)* Add operation relay.nn.dilate() which calls topi.nn.dilate().* Fix typo* Set op pattern to injective	1
[µTVM] Allow for platform-specific malloc in runtime (#6948)	1
[HEXAGON][TOPI]Slice Op Argmax uint8 (#12472)	5
[TensorIR] Support for match_buffer from subregion (#8585)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[VM] Bug fix for numpy scalar input in vm (#8553)* Bug fix for numpy scalar input in vm* Bug fix* Re-triggle CI* Update* Update UT* Re-triggle CI	5
[BYOC] Threadsafe initialization of JSONRuntime module (#11339)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>	5
[VTA] Fix RewriteForceSerial Function logic issue. (#3854)Issue:RewriteForceSerial is a debug function to force instructionsto be serialize instead of parrallel running, by doing so wecan isolate some parallel problem or do performance comparebetween parallel and serialize. But this function have someproblem, once get enabled by set debug flag, vta would stuckwhen running on pynq board.Analysis:once enable RewriteForceSerial, the dependency logic is differentwith default one, but we still use same logic to generate FINISHand other logic, this would cause dead lock.Solution:give a different dependency settings when enable RewriteForceSerial.	1
[Relay][Module] Make tags for ADT constructors and ConstructorValues more robust (#3369)* Use hash of ADT name and constructor idx to generate tag, add reverse mapping to module and use where appropriate* Lint and build fixes* Add round-tripping test for getting constructors by tag* Use int64_t everywhere for tags* Add additional identity check* Bring out _arg_to_ast again* Use 8-bit hash of GTV name as MSB of tag, index as LSB for more readable tags* Use int32 instead of int64 for tag	1
[DOCKER] Add DGL to {ci_gpu, demo_cpu, demo_gpu} docker images (#3692)* add dgl to docker file* add dgl to docker file	2
[RELAY] Codegen_c.h should include relay.function (#5093)Change-Id: I015b2c66a50b64d0eb2e9efe336f6c18ea1fdc67	4
[Relay] [Pytorch] Add aten::maximum and aten::minimum (#11864)* add maximum and minimum* cleanup	4
Fix CUDA library search (#339)	0
[Relay][Frontend][TF] fix _parse_param bug (#4711)	0
[TIR][Bugfix] Improved massive build times caused by tir.floormod and tir.floordiv. Fixed Topi testcase. (#5666)* Improved uncommon case of floormod and floordiv. Removed dependence on np floor_div and fmod.* Fixed clang-format complaints* Streamlined floormod and floordiv lowering logic* Improved build times by expressing int64 case of tir FloorMod and FloorDiv using let nodes* Updated use-def analysis and llvm codegen to support duplicated letnodes.* Corrected misuse of var_map_ in llvm codegen* Updated backends that support LetNode* Changed floormod and div lowering logic to avoid using FP on systems that don't support it.* Fixed formattingCo-authored-by: pankratz <pankratz@ualberta.ca>	0
Refine CMakeLists.txt (#2049)	5
[Pass][Bugfix] StorageFlatten, buffer var definitions in LetStmt (#10788)Previously, any buffers whose buffer var was defined in a Let orLetStmt would result in an error when running `StorageFlatten`,stating that the buffer var was undefined.  These are used forallocations in external calls, such as those produced by`LowerVtcmAlloc`.	1
[Relay] fix checkwellform (#2705)* do* address comment	1
[Relay] Support nchwc layout in ConvertLayout pass (#9681)	4
Update metal_module.mm	5
add instance infer layout (#11871)	5
[TIR] IndexMap Simplification Constraints (#11342)* [TIR] Added optional arith::Analyzer argument to IndexMap methodsSimplifications done when applying a transformation may requireiteration bounds from the caller scope.  This is a C++ only feature,because `arith::Analyzer` doesn't inherit from `ObjectRef`, and cannotbe passed through the FFI.* [TIR] Pass analyzer from TransformLayoutRewriter to IndexMapAvoid needing to simplify twice, now that IndexMap can accept theanalyzer from the calling scope.* [TIR] Added BlockNode handling to IRMutatorWithAnalyzerIteration variables defined in `BlockNode::iter_vars` may be usefulfor simplifications.  This functionality was extracted from`TransformLayoutRewriter`.	4
[RELAY][OP] take (#1863)	5
[TOPI] Use fixed thread block size in unique op for Vulkan (#7718)* [TOPI] Use fixed thread block size in unique op for Vulkan* forgot to add min for non vk backend	1
[REFACTOR] driver.h -> driver_api.h (#4760)"driver" normally refers to the "main" function.Rationale: the header exposes set of APIs to drive compilationand should be named as driver api to best reflect its usage.	1
[IR] Include PrefetchIR (#189)	5
[ThreadPool] Solve ARM BIG.LITTLE heterogeneous multicores (#4747)	5
Fold RTensor into tensor	5
[UnitTests][CMSISNN] Mark CMSISNN with skipif they are missing libraries (#9179)* [UnitTests][CMSISNN] Mark CMSISNN with skipif they are missing librariesShow test as skipped, rather than failing test.* Added tvm.testing.requires_cmsisnn	3
[CI] Update GoogleTest (#11162)This updates GoogleTest across all our images which also has the sideeffect of using the same version for any host OS.Rather than updating to a fixed version, I've followed the best practiceadvertised by GoogleTest itself which is the Live-at-Head philosophy:https://github.com/google/googletest#live-at-headCloses #11002	3
[CODE COMMENT] Comment BindBufferScope (#815)	5
Fix RelayVM for 32-bit platforms (#7605)	0
Add support for AOT in external code generation tests (#8591)This adds support for the external code generation tests to use AOT. Aspart of this the existing logic in check_result was split out intomultiple functions, this allows selectively disabling those that aren'tsupported such as JSON outputs not being supported in AOT. I've replacedexisting checks to skip tests with @pytest.mark.skipif macros as they'vebeen moved out of the `check_result` function.	1
[Auto Scheduler][fix] Add dense strategy for mali (#7181)Signed-off-by: leowang1225 <810916296@qq.com>	1
[Relay] Add ADTs to text format (#3863)* Getting closer to having ADT defs* ADT defs working probly* Match parsing basipally done* came to earth in a silver chrome UFO* match finished?* All tests but newest are passing* ADT constructors worknow cleanup?* Cleanup round 1* Cleanup round 2* Cleanup round 3* Cleanup round 4* Cleanup round 6* Cleanup round 7* Lil grammar fix* Remove ANTLR Java files* Lint roller* Lint roller* Address feedback* Test completeness in match test* Remove unused imports* Lint roller* Switch to Rust-style ADT syntax* Lil fix* Add dummy `extern type` handler* Add type arg to test* Update prelude semantic version* Repair test* Fix graph var handling in match* Revert 's/graph_equal/is_unifiable' change	4
[AutoScheduler] Use tempfile in tutorials (#6728)* Use tempfile in tutorials* address comment* Update tutorials/auto_scheduler/tune_matmul_x86.py* Update tutorials/auto_scheduler/tune_conv2d_layer_cuda.pyCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	5
[Keras] fix dropout bug (#399)	0
Fix Cmake error on Windows (#160)	0
[microTVM] make RVM memory and number of cores variable (#8154)* ram/cpu variable* tvm prefix	0
[RELAY]sch and compute for reduce ops (#2091)	5
[ROCM] DP4A intrinsic support for TE/TIR (#11009)* [ROCM] Support dp4a on AMDGPU by sdot4 intrinsiccommit 0225f2bfe3f413cd4764c2dba6c922af2520146bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 08:56:10 2022 +0900    share op strategy between cuda and rocmcommit 762c7e8611c9ec3cca3321428e2362c81fe89b9bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 08:28:34 2022 +0900    fixed rocm batch_matmul strategy for mixed i8i8i32commit ce53e8d141f7f901303ec6a91674337cbf2b2384Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 06:17:30 2022 +0900    add rocm sdot4 TIR intrincommit f4562b991f9180b61be7339b2890de1584656c10Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 06:03:44 2022 +0900    rocm sdot4 workscommit 6cc62805f82dd884a18a1c4c0e9bae5866e00da0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 05:32:07 2022 +0900    more wipcommit 0602f4a3157d4cb5a3f280a3a3c514bb6535aac8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 03:47:37 2022 +0900    Squashed commit of the following:    commit 65b8bcf955f44540d6a52c8416e60f3047c8366c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Apr 13 20:36:49 2022 +0900        [WIP] adding DP4A support to rocm    commit 4f8f308ab6bb85ef3bdcc2b8e846c2eea15f2167    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Apr 13 14:03:25 2022 +0900        Squashed commit of the following:        commit 1711be38a17e3b6171350009f1da05824cd0b340        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 13:11:40 2022 +0900            fixed condition for real        commit 8a48fb5262e80e318cd81d5ff51bf95fd5eb576e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 09:57:42 2022 +0900            Revert "Skip applying sch_rule when both ann and sch_rule are defined"            This reverts commit 4915c6a5a91ff87038e71f8aff9f31db684b4a95.        commit daea033d2cb06388ef27ddadb80fc5bce72181d2        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Mon Apr 11 09:31:05 2022 +0900            [Metaschedule] Support rocm and spirv        commit eb0cae2c779808cced074d189e8f487bf46ea89f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 07:25:04 2022 +0900            dp4a works        commit 4915c6a5a91ff87038e71f8aff9f31db684b4a95        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 06:13:45 2022 +0900            Skip applying sch_rule when both ann and sch_rule are defined        commit 7b3d71c6b21a9c5de9ef2b89d0a7db2800a5f3a2        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 04:40:31 2022 +0900            fixed intrin description        commit 7666cd7a5b0ce182791662673fbe45944c84d0ae        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 19:59:47 2022 +0900            add DP4A intrin        commit 7086bdb75546a2680d12dc8f80c040cea23f729a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 19:03:44 2022 +0900            works        commit db343974bfae86e51078e40e6170022a782d8e0a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 12:49:52 2022 +0900            more hack to tensorize loop mapping to make resnet50 e2e work        commit 2409674a7884a60beb50d7aa3345c4b907b8cd13        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Mon Apr 11 13:40:59 2022 +0900            wip support pad + qnn.conv2d folding        commit 613cb7ec33b6df41f1ebe0f0a0ac8eca7c73cff1        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 12:04:08 2022 +0900            hack to tensorize loop mapping to make conv2d work        commit 9e4f9df6a409396a8a4a20d967c4f51accf5d210        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 11:34:13 2022 +0900            wrap tensorize with try/catch        commit d4b496d858da0ae43063d47cb03a28b803d0269f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 11:33:39 2022 +0900            revert change in task_scheduler.cc        commit 476129be7b286f5d109402280aea585e89f6dc1d        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sat Apr 9 05:54:10 2022 +0900            try / catch in ThreadedApply        commit d8226ff26f25eba17d4000f25131822874bdc2cc        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 17:17:59 2022 +0900            filter out invalid candidate        commit 2632899a2759885d338e25f2a25ba0b2c555f0c3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 10:09:48 2022 +0900            try graceful exit in parallel_for_dynamic        commit 9d6741c3dd29c4dde861aa1d3b2ca85f560f5ac6        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 09:35:51 2022 +0900            [QNN] Fix broadcast for invalid axis        commit 6ccde0959343ce4246ef99505b4f54de469a1a5c        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 20:51:15 2022 +0900            refactor rewrite_tensorize        commit 2ce206699f10b03b9611c4683018f7e0c70c7eb5        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 20:48:17 2022 +0900            allow missing schedule_rule in post order apply        commit 3a69353a29abfc454e28d4e530d22a3e2043712e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 19:42:48 2022 +0900            refactor rewrite_tensorize        commit 43e0b2f7f98299679807aaf1ffb13cce2b5f5ce3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 18:25:14 2022 +0900            rewrite_vnni -> rewrite_tensorize        commit 823797e2627a9bfa812b72019468569ee79eb4c6        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 18:12:12 2022 +0900            VNNI -> WithIntrin        commit 4284a47e5933aa89c1c3362b15ad53b14782fc81        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:45:41 2022 +0900            introduce TileForIntrin        commit b87ef32e30e1e71b3f39789f7289976a8cba4ab4        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:34:04 2022 +0900            move TilingwithTensorIntrin to auto_tensorize.cc        commit 2fc118b3726586ba13f7de950beaa299b83a0af3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:28:45 2022 +0900            clean up headers        commit d8b2aa325c91b524bec22dc1ec2fc52c9f060fce        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:09:32 2022 +0900            clean up using namespace        commit eb05d25e2b71f4a1232a8796d1413011ec7629d3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:03:05 2022 +0900            refactored init        commit 5e6b0a08d447c0470c2c8a993e4bd62673e34fe3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 16:57:14 2022 +0900            compiled        commit 2b8c430e2fec7ceb285eed7bc7aa73bb9a74a997        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 12:51:55 2022 +0900            wip MultiLevelTiling refactor        commit 7c21a9fea0511c88bd82f49f799b5198252df40a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:58:33 2022 +0900            function doc string not supported by tvmscript        commit 40f9742bc9c3aa11e8c2c0551d1827ad47fc0f39        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:56:45 2022 +0900            update vnni intrin name        commit 4814f825a5315efd2a3da8c36d2ce6b5df5447cd        Merge: e0c5eb84b 07bbb38f7        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:44:47 2022 +0900            Merge branch 'tir-tensor-intrin' into auto-tensorize-vnni        commit 07bbb38f7fb52db4a2ecde3d5c87cf4d5cd000a1        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:24:56 2022 +0900            more lint fix        commit 15e60b42362cc64b1428b219c8eada414d1b8372        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:16:08 2022 +0900            black        commit 7a757fe53758e06418ea1367b348b47c8cd2dcf9        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:12:54 2022 +0900            pylint        commit 9a3e508b6f4529158e703b4617f2ddaa351a89eb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:58:52 2022 +0900            simplify import        commit d8e43ecf1c0a79a2c195ff31e1e699a447a11335        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:52:50 2022 +0900            use vectorlow/high in arm intrin        commit 625cd2774ec455307646b0c26bb3971d89613d1e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:34:57 2022 +0900            fixed offset factor        commit 69e72b6b612588e670937e003435afa647030ceb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:12:02 2022 +0900            Add ARM intrin        commit 1351fdea6b22f231a290a6c28e06732c9cf993cf        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:27:27 2022 +0900            use buffer syntax sugar        commit 0ced85fd097ed48aad8714912718d8735791e1fb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:17:43 2022 +0900            rename vnni.py to x86.py        commit 38a5aca87ec438446593a3af17760339211f5ad9        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:24:44 2022 +0900            add VNNI unittest        commit 88b763ec48c20cf68db8bc3bae3fa3ae78996ee8        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:10:06 2022 +0900            refactored existing test using VNNI intrin        commit 711a0076d9be2b9aa80ada67e1edda5ba1fdf1fd        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:04:58 2022 +0900            [TIR] Add VNNI dot product intrinsic for TIR        commit e0c5eb84bf6a0ad2ba0cddc4bdf22a799dc4b8a0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:42:26 2022 +0900            merge fix        commit b171748139e53f0cf75ff4b6fde436f9d8a5fe91        Merge: 71fe3bdf0 82e152a3c        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:33:59 2022 +0900            Merge branch 'tir-tensor-intrin' into auto-tensorize-vnni        commit 71fe3bdf02ae10ddbe090a4fd1020f545a05bb41        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:57:38 2022 +0900            move tensor intrin under tir        commit 0c51badef45af2a1025ab42fe38d1b3f07ab493e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:12:39 2022 +0900            remove log        commit fed910e03eb94c169d4a160b8f3cad406d04c6aa        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:11:22 2022 +0900            more revert        commit 7150aff9fba167d88dbfb40d48727de8a144b9c0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:10:44 2022 +0900            revert stmt_functor change        commit 155107b98b09c5e5cc7f19afbd327b0557a02843        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:10:09 2022 +0900            refactored RewriteVNNI a bit        commit ca15255e3a882b89b05bb83079640c929fb63096        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 05:41:13 2022 +0900            add RewriteVNNI        commit dc9f71d5e3122b50fa8ae6a4462f959f13870b05        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 05:38:56 2022 +0900            vectorized init loop        commit fcc31ee20ddfafd47f566bf98ff40a9f684d12eb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 04:55:36 2022 +0900            tensorize worked        commit 2b534377a45b9ab84bf35c3d7c03ecae7616d17f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 6 19:11:05 2022 +0900            TilingwithTensorIntrin works        commit 86baa31e773fc864f77dc113bc9a93b79f3fc652        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 6 08:58:27 2022 +0900            Ported auto-tensorization code        commit 82e152a3c91144041ade783116a50565ebb48b89        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:24:56 2022 +0900            more lint fix        commit 88d9bdd3b21302bc2dd068a990df15c375a1a8ef        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:16:08 2022 +0900            black        commit 31fe7eb8075445161d804d170772eac8e90d3425        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:12:54 2022 +0900            pylint        commit 7876754effc40ad089349534dacd75df19d38fc4        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:58:52 2022 +0900            simplify import        commit 56f2e9a85069426021e2872eb1da95bf134ac7e0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:52:50 2022 +0900            use vectorlow/high in arm intrin        commit 995cc8d6fcec70a3fadcfb1c6fee7b9f0b5a0951        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:34:57 2022 +0900            fixed offset factor        commit 86bbd4955b34257d68d957cb4a2536aea3ef9bac        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:12:02 2022 +0900            Add ARM intrin        commit 120fd96e80307b4301ee3fc93e6793e0b40485f0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:27:27 2022 +0900            use buffer syntax sugar        commit 0f0682d00c3961afd1f492ae55f180c5b5502767        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:17:43 2022 +0900            rename vnni.py to x86.py        commit f88c31ead1fa6db4bfd2c88eeaf5f665e4c6dddb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:24:44 2022 +0900            add VNNI unittest        commit 6cc80094adac398762924b0b31a4c741417ba9dc        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:10:06 2022 +0900            refactored existing test using VNNI intrin        commit 11a29c704cdaad96aeeca39c9c753ef006d27a50        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:04:58 2022 +0900            [TIR] Add VNNI dot product intrinsic for TIR* cleanup* black* update dot prod intrin* add mattr kind* conv2d topi test working* add dense and bmm test* add conv2d relay test* add tir intrin test* pylint	3
[ONNX] Fix issues for Clip and RoiAlign (#7237)	0
[Testing] Add model loader for int8 BERT (#10622)* add model loader for qat bert-base* add test* pylint* ignore mypy* Update python/tvm/meta_schedule/testing/tlcbench.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* use a dedicated process for converting* return input info* encode batch size and seq_len information in cached file pathCo-authored-by: Junru Shao <junrushao1994@gmail.com>	2
[BUILD] Add CMake for Windows build (#55)	1
[Relay][Pass] Simplify consecutive transpose/layout_transform (#7656)* [Relay][Pass] Simplify consecutive transpose/layout_transform* lint* fix* support negative* comment	1
[Misc] typo and nit fixes (#10145)	0
Fix JSON graph dumping. (#5591)* Previously this function placed a JSON-escaped string containing   the JSON-encoded graph.	5
rename aot_demo to aot_standalone_demo for clarity vs. host-driven aot (#11723)	5
[BUGFIX/TESTS] Bugfix of Tenso slicing. Union. (#66)	0
[MetaSchedule] Extract workload embedding (#11975)This PR enables extracting the embeddings of the workload in a tuning context, which further strengthens the feature extracting process. Workload embeddings are extracted based on names of each block in the IR module. If `extract_workload` is enabled, the extracted feature vectors will have length 164 + 8 = 172.	4
add support for mxnet smooth_l1 (#2905)	1
fix mistype (#3763)	0
[Hexagon] Remove uses of LLVM from simulator runtime (#8821)* [Hexagon] Remove uses of LLVM from simulator runtimeThe TVM runtime is not linked with LLVM libraries, so using LLVMin it carries a risk of referencing undefined symbols. This maywork for objects defined in header files, but it then relies onLLVM keeping them there.Replace uses of LLVM utilities in the Hexagon simulator runtime,with simple alternatives.* clang-format* Use dmlc::optional instead of implementing one from scratchMake detail::Optional be derived from dmlc::optional, and add some bitsto make it behave more like the C++17's std::optional. The goal is toreplace detail::Optional with std::optional, once the project switchesto C++17.	1
Add test_forward_ssd_mobilenet_v1 to tflite/test_forward (#3350)	3
fixenhance robustness of DefuseOps (#8564)	1
[1] Test case modified for int type (#5012)	3
Update ONNX versions (#8304)	5
[PERF/TIMER] Add builtin timing logic (#168)* [PERF/TIMER] Add buildin timing logic* fix lint	0
[TEST] Fix test_topi_batch_matmul_tensorcore.py:test_batch_matmul requirement (#7294)* this test current sets a requirement to "uses_gpu", which   causes it to fail in cpu-only machine * this patch changes it to be "requires_tensorcore", as per discussion   on issue #7277	0
[Fix][Runtime] Use flatBuffersBuffer_ in EdgeTPURuntime::Init() (#8034)* Use flatBuffersBuffer_ in EdgeTPURuntime::Init()* Specify target_runtime for tflite_runtime.create()* Add a comment for describing the dependent TF version	1
Fixed make no_optimization runs indefinitely when run the memcost example provided  (#126)* fixed make no_optimization runs indefinitely when run the memcost example provided* format plan_memory.cc	1
fix rpc server bug on VTA (#5607)	0
Improve SGXModule (#1104)* Improve SGXModule* Address code review comments	1
[Relay][VM] Fix loading late bound consts when none exist (#10087)* Fix loading late bound consts when none exist* Simplify comment* -mFix skipping of verilator tests* Skip loading late-bound consts if none present* Remove semi-related fix to verilator test skipping* Remove more test-skip fixing for pr hygiene* No-op for ci* No-op for ci	0
[TVMC] Fix to check whether a path passed to --target is strictly a file (#7663)* When we use file with --target, the validation in place was only   checking whether it was a valid path. For the case in which the   path is a directory, it causes a crash when tvmc then tries to   open the path. * This fix moved the check to be strictly for files, not only a valid   path	2
Update CONTRIBUTORS.md (#3130)	5
[ci] Build GPU libraries on CPU nodes (#10539)* [ci] Build GPU libraries on CPU nodesGPU capacity is more strained and expensive so we should stick to CPU when possible. This moves the GPU build to a CPU node (which is fine so long as the cuda libraries are present) and splits the C++ unit tests out to relevant areas (test steps where possible, otherwise it runs after the build)commit-id:d385b28c* Address commentscommit-id:dcb084daCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[microNPU] Refactor type inference data type checks (#10060)* [microNPU] Refactor type inference data type checksAims to improve readability, extendibility and error messageunification for data type checks across NPU operators.A follow up for the comments in #9576.Change-Id: I83fb89a56677003f7abebb7985ad60d92cfa8df1* unordered_set -> initializer_list and use new format for upscale checkChange-Id: Icf3d68d5cc7d5e1d5af42b1af193db89faea155e* remove unused header and use auto for initializer typeChange-Id: I10311b718c3abd0ed75dd88b5ec9de6e0742f047	4
Remove redundant FoldConstant pass (#2387)	4
allow annotation info for relay var (#8000)	5
[Onnx] Support Bidirectional RNNs (#8337)* modify lstm to be easily bidirectional* make it obvious some matriciies are packed via prime notation* fix var name* more var names* add op split* keyword arg names* missing implicit cls arg* deal with extra dimensions* last of the fixes* refactor rnn tests to support directions* bidirectional tests* test forward results* go backwards* more fixes* reverse tokens on reverse pass* parameterized directions* double up activations in bidirect* slow attribute forgetting* lstm interface is v. confus* test forward complete* add GRU outline* revisiion2* why was tehre a not* gru tests* missing bounds, copy pasta!* add comment* ensure all args fp	1
[Documentation] Document rewrite_once option (#8900)	2
[RUNTIME][COMPILER] Formal compiler pipeline, runtime wrapper module (#21)* [RUNTIME][COMPILER] Formal compiler pipeline, runtime wrapper module* more detailed comments	1
[Runtime] Export GraphRuntime in tvm_runtime.dll (#5002)Co-authored-by: Jon Soifer <jonso@microsoft.com>	1
[Test] Fix AutoScheduler test to cover Conv2D Winograd (#8539)* optimize resize vertor* tmp* DoMultiLevelTiling* modify size_t to int* modify* modify level fill* Update utils.ccmodify format_lower* format lower count* delete blank lines* delete blank lines* re-commit message* Update graph_executor.hadd set_output_zero_copy* add setoutputzero* add set output zero* Update graph_executor.cc* Update graph_executor.h* delete const_cast* add common function chechDltensor* Update graph_executor.h* Update graph_executor.cc* add output_ sort* Update graph_executor.cc* add a.nodeid == b.nodeid* add unit test for set output zero* add include <algorithm>* modify Setoutput zero copy* modify by clang-format* add unit test for set output zero* rrealy ut go back* rrealy ut go back* modify input->output* delete sort output input* modify build_module_test.cc* midify winograd UT* re-pr* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit	3
[Relay][Frontend][Onnx] If Operator Support (#6730)* If operator support in ONNX.* Small tweak.* Added uses_gpu tag.* Disable test on GPU until onnxruntime version is updated.* Use parametrize_target to specify CPU only.* Just dont use onnxruntime for now i guess.	1
[IRPrinter] Prevent multiple printing of optional info (#8279)* fix* test	3
[FIX,MICROTVM] Add requires_micro decorators to microtvm tests (#6747)* [FIX,MICROTVM] Add requires_micro decorators to microtvm tests	3
moving module import inside of download function (#1319)	1
[CI] Update all Docker Images to 20220505-060045-500703308 (#11219)This gives us GoogleTest for #11202 and blocklint for #11200 but most importantly it makes use of the new and improved tags from @leandron in https://github.com/apache/tvm-rfcs/pull/66Closes #11202Closes #11200	1
[QNN] Optimize requantize for power of 2 and fix dequantize for per-channel quantized input (#6675)* [QNN] Optimize requantize for power of 2 and bug in dequantize* Comments* Docs* Comments* Ethos	2
[Hexagon] Deprecate SDK 3.x, rewrite HexagonSDK.cmake (#10612)* [Hexagon] Deprecate SDK 3.x, rewrite HexagonSDK.cmakeAvoid setting global state, instead implement get_hexagon_sdk_property,which will set a user-provided variable to the value of the requestedproperty.* Restart CI	5
refactored GraphProto.from_onnx into smaller functions (#10267)* refactored GraphProto.from_onnx into smaller functions* black formatted file* removed line that does not seem to make sense. Is there a purpose that I missed?* just to trigger CI pipeline	1
check in (#1629)	5
Refine porting x86 NCHWc conv to AutoTVM (#1993)	5
[TVMC][VitisAI] Enable Vitis AI target through TVMC (#7577)* Enable Vitis AI target through TVMC & change PassContext API's* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Change Vitis AI  API to  & address comments & fix linter issues* Update docs/deploy/vitis_ai.rstCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Add Vitis AI initiliazation to separate init config in TVMC composite target registry* Lazy load pyxir package in Vitis AI codegen to avoid hard dependency for TVMC* Fix TVMC Vitis AI test for compiler.compile_model API change* Lazy load pyxir package in Vitis AI partitioning passCo-authored-by: Jorn Tuyls <jornt.tuyls@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>	4
Mention minimum version of python features one should stick to (#3588)	5
[llvm] switch to use Align for llvm trunk (#4051)	1
[CI] Temporary disable rust test (#3809)	3
fix build break for android_rpc (#7664)	4
[skip ci][microTVM] Update Zephyr RVM name and box version (#11655)* Use new Zephyr RVM version* fix box name* fix version to match zephyr* update version* Update README.md* Update README.md	2
[Relay] Parser Tests (#2209)	3
Fixing tensor not found issue in bitserial operator (#4095)	1
Fixes a link in doc. (#8064)Signed-off-by: Tao He <sighingnow@gmail.com>	2
Intel target added, sub group sync added (#1084)	1
Implementation of relay_to_tir target hook (#8423)This the first new hook proposed in the Additional Target Hooks RFC, longerterm the compilation should move to using `Target` proper but this unblocks our current work whilst illustrating the eventual interface via `Target` in `src/relay/backend/contrib/example_target_hooks/relay_to_tir.cc`Ideally the host target would be annotated onto the `IRModule` so as this `Pass` could use it instead of defaulting to C but this is fine for now.	1
[LIBINFO] Enable load .dylib on darwin (#142)* [LIBINFO] Enable load .dylib on darwin* Fix makefile	2
[DEBUG]Fix debugger message mess in display_debug_result (#2228)Signed-off-by: Zhebin Jin <zhebin.jzb@alibaba-inc.com>	0
Revert "[RUNTIME] Refactor extension type handling, now it is header only (#924)" (#925)This reverts commit 12d15704d7f5d30cff7540f1fd16be64c6baca68.	4
[testing] Remove wrapper from @slow (#11566)This makes it a normal pytest decorator so it doesn't incur test set up / tear down. This also makes the PR body the source of truth for skipping slow tests or not since it can be confusing sourcing it both from the PR and commit message.	5
[VM] Allow serialization of function attrs which are strings (#8485)* [VM] Allow serialization of function attrs which are strings* add test	3
[SETUP] Add optional dependencies to extras_require (#4428)	1
Update test_op_level1.py	3
Fix inconsistencies in graph_executor function names handling (#9255)* Clean up redundant code in graph_executor.ccHow did these lines ended up here?* Fix inconsistencies in graph_executor function names handlingUpdates value of `TVM_CRT_MAX_STRLEN_FUNCTION_NAME` from `80` to `120`Replace all occurences of `[120]` with `[TVM_CRT_MAX_STRLEN_FUNCTION_NAME]`to maintain consistency and make the array lengths user-configurable.Introduces `TVM_CRT_MAX_STRLEN_PARAM_NAME` used for parameter names onlyAdds comments to `kMaxFuncNameLength` variabe in src/relay/backend/te_compiler_cache.ccmaking sure that the values are kept "in sync". (sort of)See #8953 for more context. The actual bug reported there however canonly be fixed by increasing the TVM_CRT_MAX_STRLEN_FUNCTION_NAME to avalue larger than the maximum possible truncated function name length(including prefixes and suffices)Example:  6['tvmgen' prefix length]+ 7['default' model name length]+ 5['fused' fused function name prefix length]+ 80[truncated function name length]+ 19[length of appended hash]+ 4     [Number of '_' between components]= 121	1
Fix alpha_equal bug (#4897)	0
adding gromero as a reviewer (#8765)	1
[Ansor][AutoTVM v2.0] Phase 2: Basic CPU Sketch Search Policy (#6184)* Init commit to pass the compile* First commit to pass the test* Update* Add UTs for sketch generation* Update* Add ASF to new UT file.* Update rule for winograd* Update* File renamed* Lint fix	0
[REFACTOR][IR] Introduce include/tvm/target (#4721)As part of Unified IR infra.Introduce target folder to store all the compilation target related information.	5
Documentation errors updated (#392)	5
Add temp git dir to test_cc_reviewers test case (#10058)This decouples the test_cc_reviewers test case from the user's git configuration. The implementation reuses the TempGit structure from test_skip_ci to always use a fresh git environment.	1
[RELAY] Refactor AlphaEqual to support deep comparison of Attrs. (#1958)	1
[IR][Pass][Instrument] Pass instrument framework (#7952)* [IR][Pass][Instrument] Pass instrument frameworkThis commit provides utilies to instrument passes:  1. Add a new namespace tvm.instrument  2. Introduce PassInstrument and PassInstrumentor to PassContext     Example     -------    passes_mem = #... Impl of memory instrument    passes_time = tvm.instrument.PassesTimeInstrument()    with tvm.transform.PassContext(        pass_instrumentor=PassInstrumentor([passes_mem, passes_time])):        tvm.relay.build(mod, 'llvm')        passes_mem.rendor()        passes_time.rendor()  3. Integrate existing PassContext::Trace() and timing profile* [IR][Pass][Instrument] Fix python test_pass_manager.py* Fix comment* Fix lint* Fix test_pass_annotation* Fix test_pass_annotation.py* Fix lint* Fix test_pass_annotation.py* Fix test_pass_annotation.py* Fix review comments* Fix tutorial use_pass_infra.py* Fix review comments* Fix review comments* Fix typo* Fix review comments* Fix review comments* Fix unittest error: test_cow_pass* Fix unittest error* Add more test cases for exceptions* Fix nit* Doc override_instruments()* Fix review comments* Fix lint* Fix EnterContext exception behavior	0
Remove depreceated mointergration tests with mxnet zoo importers (#10772)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
Correctly enable architecture extensions in CMSIS-NN Zephyr Demo (#10458)* Correctly enable architecture extensions in CMSIS-NN Zephyr DemoWithout `CONFIG_FPU` being set the correct architecture extensions weren't being applied which means the buffer sizes didn't necessarily match up - this corrects it so that they align.* Fix memory allocation in demoThe stack allocator forcibly aligns memory by removing parts of it which causes there not to be enough memory and the CMSIS-NN integration uses more stack than the demo with pure TVM operators (we should look to remove some of our stack usage)	4
[AutoScheduler] Add winograd support in tuning networks (#6877)* add winograd in auto-scheduler* trigger CI* address comments* fix tests* fix test	3
[CI] Rev ci-cpu to v0.76 (#8786)- This includes changes up to commit 1a95f9bd0	4
[NNVM]Activations support added in Keras Frontend (#1210)* [NNVM]Activations support added in Keras Frontend* Helper for ELU added* All activations test cases clubbed to one	3
[MetaSchedule] Modify Profiler Timers (#11735)Minor modification to scoped timers to cover 99% of all the time cost during MS tuning. Allow `ApplyHistoryBest` and `TaskExtraction` time to be counted during tune_relay.	4
update vta-hw version (#7271)	5
[FRONTEND][Keras] Fix softmax axis (#503)	0
[CI] Add more info, per exec ws isolation (#4388)	5
Rename ml.dmlc.tvm to org.apache.tvm (#4290)	2
add hash function for tuple (#141)	1
[TESTS] Refactor tests to run on either the GPU or CPU. (#6331)Much of the time spent in testing is duplicated work between CPU and GPUtest nodes. The main reason is that there is no way to control whichTVM devices are enabled at runtime, so tests that use LLVM will run onboth GPU and CPU nodes.This patch adds an environment variable, TVM_TEST_DEVICES, whichcontrols which TVM devices should be used by tests. Devices not inTVM_TEST_DEVICES can still be used, so tests must be careful to checkthat the desired device is enabled with `tvm.testing.device_enabled` orby enumerating all devices with `tvm.testing.enabled_devices`. Alltests have been retrofitted with these checks.This patch also provides the decorator `@tvm.testing.gpu` to mark a testas possibly using the gpu. Tests that require the gpu can use`@tvm.testing.requires_gpu`. Tests without these flags will not be runon GPU nodes.	1
add github issue template for docs (#8982)	2
[CUDA] Support float16 erf,tan,atan (#10122)* [CUDA] Support float16 erf,tan,atan* fix* fix* Update src/target/source/literal/cuda_half_t.h* Update src/target/source/literal/cuda_half_t.h	1
[RELAY][PASS] CombineParallelConv2D (#2089)	4
[DOCS] Update to show github version (#7948)* [DOCS] Update to show github version* Remove dup code	4
Enhancement for fold_scale_axis and dnnl_json_runtime (#11815)* enhance WA in dnnl_convolution, support crop for tensor with mismatched groups and OC* add missing param checks for conv2d, conv3d* fix lint	0
[microtvm] Add mxnet importer and update pyyaml to fix poetry error (#11668)	0
[CODEGEN] update codegen for vector operation (#711)* [CODEGEN] update codegen for vector operation* update comment, fix for metal	0
[LLVM] Create LLVM scope object for use with LLVM libraries (#12140)This implements RFC 80. See https://github.com/apache/tvm-rfcs/pull/83.Summary of changes:- Created an `LLVMInstance` class. Uses of LLVM functions and data struc-tures should be contained within the lifetime of an object of this class.LLVMInstance object contains LLVMContext, and implements member functionsto deserialize an llvm::Module.- Created an `LLVMTarget` class. Once an LLVMInstance object has beencreated, an object of LLVMTarget class can be created from TVM targetstring, or Target object for "llvm" target. Once LLVM command line flagsare added to the "llvm" target, one of the goals of this object will beto save/restore relevant LLVM global state. Another objective for theLLVMTarget object is to be a single location for all LLVM-relatedcompilation structures and options (such as TargetMachine, FastMathFlags,etc.)	1
[AUTOTVM] Misc fix to document and style (#2035)	2
[CI] Add JVM Env (#237)* [CI] Add JVM Env* add update	5
[FIX,TESTING] Add tvm.testing to the docs (#6458)	2
[Hexagon] Disable broken test on physical device (#11960)	3
Fix ArgBinder assert order (#3794)	3
[Frontend][TFLite] Improve support for half_pixel_centers in resize (#11521)* add resize_nearest_neighbor op test* Improve support for half_pixel_centers in resize	1
[TIR][REFACTOR] Deprecate FreeStmt (#5890)Currently FreeStmt is not being used.While it can be useful to have an early free hintwe can always use an intrinsic instead of a first class statement.	1
bugfix function args order in alu instruction generation (#3592)	1
[CONTAINER] Introduce StrMap (#1292)	5
[git] Remove duplicate .gitignore entry (#10760)The ignore for `gallery/how_to/work_with_microtvm/micro_tvmc.py` wasadded in both https://github.com/apache/tvm/pull/10701 andhttps://github.com/apache/tvm/pull/10729.  This removes the duplicateentry.	1
[codegen][Build] it's more readable to move the if condition out of the loop (#4501)	4
[IR] Make iterators compatible with constructors of STL containers (#3624)	1
[REFACTOR][RELAY] Move invoke_tvm_op and shape_func to vm dialect (#5958)* [REFACTOR][RELAY] Move invoke_tvm_op and shape_func to vm dialect* address comments	1
[Relay] Target annotation for external codegen (#4933)* op based external compiler annotation* Use TVM register directly* Small fix* test graphCo-authored-by: Cody Yu <comaniac0422@gmail.com>	3
Scatter on Cuda (#6533)* working cuda scatterfix lintfix pylint again* cuda scatter with threading* add dynamic shape tests* remove unused variable	1
[BUILD] Fix VTA build in CI (#5165)	0
[microTVM][RVM] Reuse QEMU installation config and fix bug in RVM testing (#11808)* refactor* add llvm installation* fix testing	3
[VM] Move param bind to OptimizeModule (#7451)* [VM] Move param bind to OptimizeModule* add test to verify the number of free vars after opt* remove const from OptimizeModule	4
[Frontend][TFLite] respect out type of Shape op (#11877)* [Frontend][TFLite] respect out type of Shape op* tests: update for changes to tflite shape handling* lint fix	0
[Metaschedule] Enable continuing tuning after schedule application failure  (#10937)Currently, when there is a failure in schedule application during tuning (e.g. tensorize), the entire tuning session is killed with an error msg like `RuntimeError: parallel_for_dynamic error with ...`.  We should gracefully handle such errors and let tuning continue on other candidates.No test is added since I don't know how to get tuning to fail in a controlled manner.	0
check substitute	5
[Relay] remove redundant test cases in test_op_level4.py (#1905)	3
Incremental type inference (#6900)	5
[Relay] Fixed VNNI batch matmul op strategy for meta schedule compatibility (#10637)* Fixed VNNI batch matmul op strategy for meta schedule compatibility* update bert task extraction test	3
[EXPR] Expression-template based pattern matching. (#2589)	5
[Relay][Op] Dropout and batch_norm (#1870)	4
[ONNX] Add MeanVarianceNormalization op (#11444)* [ONNX] Add MeanVarianceNormalization op* Add pytest.main([__file__])	3
[COMMUNITY] @mbaret -> Reviewer (#5322)	3
[Relay] Conv2D padding representation (#4787)* enforce 4-way padding* add util with get_pad_tuple* delete unnecessary arguments* fix lint* add container.Array case* fix cudnn conv2d asymmetric padding logic* rename get_pad_tuple to get_pad_tuple2d* revert change for topi/python/topi/nn/conv2d.py* add get_pad_tuple2d for several contrib conv2d ops* add get_pad_tuple2d for all conv2d ops	1
[TUTORIAL] Deploy the Pretrained Model on Raspberry Pi (#61)* [TUTORIAL] Deploy the Pretrained Model on Raspberry Pi* [TUTORIAL] Improve* [TUTORIAL] Improve* [TUTORIAL] Improve* [TUTORIAL] Improve	1
[ROCm] Fix dense autotvm template registration (#3136)* Fix rocm dense autotvm template* suppres lint warning	2
[TIR] Fix Ramp int32~64 mismatch in VectorizeLoop and NarrowDataType passes (#10172)[TIR] Fix Ramp int32~64 mismatch in VectorizeLoop and NarrowDataType passes	4
[MetaSchedule] Evo Independence from TaskScheduler (#11590)Per discussion with @Kathryn-cat, we realized that the current APIdesign could be verbose if we only want to tune a single task, in whichcase a dummy task scheduler still needs to be established to supply`EvolutionarySearch` with proper `CostModel` and `Database`. This PRfixes this UX issue.	0
fix #802, create cache based on sugar tensor (#808)	1
[CI] Update ci-cpu to bionic (#5554)	5
[microTVM] AOT Demo (#8075)* initial* remove compare* temp fix* debugging* hack* hack for testing* both test pass* cleanup* fix tests and tutorials* restructure* cleanup* cleanup* fix check files* fixed for physical devices* address comments* reduce nrf stack size* update sample url* format	5
[QNN] Quantize - Fixing the sequence of lowering. (#4316)	0
[BUGFIX] Fix CanonicalSimplify change type of int constant (#269)	4
[ci] Default to n=2 for test parallelism (#12376)This decreases the test times for most of the tests except a few that did not run under pytest-xdist with 2 worker nodes. This also doesn't decrease overall runtime since CI is still bottlenecked on other jobs. However, this could lead to savings in compute which makes CI more sustainable so this is still worthwhile, though we should revert this if we start seeing "weird" errors like OOMs more often.	0
[COMMUNITY] @masahi -> PPMC (#5691)	3
[TVM][ARITH] Teach BoundDeduce to handle the case in which target var can appear in rhs of expression (#2795)* target variable can now appear in either lhs or rhs of the expression to be analyzed* removed extra spaces	4
[NNVM/TOPI][OP] Split : default axis to 0 and allow negative values - nump… (#1883)	1
Hotfix CI (black check not caught by PR CI) (#11056)	0
Return empty CSourceModule when no lowered_funcs exists in Relay mod (#4847)* Use dummy func when no lowered_funcs exists in Relay mod* Dummy func -> CSourceModule with empty code str* Added comments describing the empty CSouceModule* Always import external modules w/o assertions* Use CSourceModule as a fallback for LLVMModule* Changed cond for target == llvm* Create an empty LLVM module w/o using dummy func* Avoid using IR str concat to create LLVM module* Improved comments for codegen.LLVMModuleCreate* Satisfy the linter for LLVMModuleCreate	1
[RUNTIME] Add TypedPackedFunc (#1626)	1
[CODEGEN] Add CodeGenC (#22)	1
[docs] Update tlcpack-sphinx-addon (#11891)This integrates the fixes from tlc-pack/tlcpack-sphinx-addon#7	1
[TFLITE]FLOOR_MOD & FLOOR_DIV support (#4971)* TFLite Floor_div & floor_mod parsing code* Review comment updated	5
[CMSIS-NN] enable USMP with CMSIS-NN (#10224)This commit mainly enables the USMPwith CMSIS-NN codegen.In order to do that, CMSIS-NN functions neededto contain BufferMaps. This commit adds the necessaryBufferMaps as well.All the tests are modified to run with USMPwhile the networks tests run with and withoutUSMP.	1
[TVMC] Fix tvmc compile to extract target and target_host from --target (#8176)* [TVMC] Fix tvmc compile to extract target and target_host from --target * Removes validation to accept up to two TVM targets and   set them as target and target_host* Update python/tvm/driver/tvmc/common.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
[CI] fix docker group exists with different GID (#11184)This patch fixes an error while using the docker/bash.sh script toinvoke docker. If the same CI_BUILD_GROUP is present within dockercontainer but with a different GID, we try and fail to add the existinggroup This patch tries to fix this error	0
[Relay][VM] Fix code generation for packed functions + tuples  (#3287)	1
Custom dyld linker for iOS mach-o executable files (#7875)* [IOS-RPC] Fix compilation iOS_PRC appSigned-off-by: Alexander Peskov <peskovnn@gmail.com>	0
fixed tuple error (#10216)Co-authored-by: suhail <suhail@expedera.com>	0
Check that the node is not null, add contains to OpMap (#3037)	1
[Relay] add ShapeFunc for one_hot op (#7490)* [Relay] add ShapeFunc for one_hot op* fix pylint* add test for shapefunc of one_hot op	3
[TIR] Fix FlattenBuffer computing size for buffer with strides (#9195)	0
Enable identity layout (#238)	0
[TIR] Enhance Substitute, python bindings for Substitute/PostOrderVisit/IRTransform. (#5400)Substitute now takes a std::function to customize more replacing behaviors.Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>	1
[TOPI] GPU sort IR refactor to enable sort by keys (#7157)* sort refactor initial import* sort test working* scatter 1d with positive indices working* remove negatiev indices, using extern for now* minor fix* minor fix* add sort by key test* revert scatter change* add document* fix py formatCo-authored-by: masa <masa@pop-os.localdomain>	0
[DRIVER] Add simulator, unify testcase to unittest (#25)	3
[Fix] Fix the logic of the number of nodes checking in op fusion (#4074)* move the number of nodes constraint in op fusion up to the dom tree level* add test case of limiting the max number of ops to be fused* uncomment other test cases	3
Add Chris (#9532)	1
[Relay] use unordered_map instead of map in ANF (#3024)	1
[QNN] Concat - Refactoring to C++ (#3819)	4
[Frontend][Relay][Parser] fix unparsable yolo formals (#6963)* fix yolo formals* fix lint* move test to test_forward	3
[CI] Update Docker images for new CMSIS-NN (#11336)Updates ci_cpu and ci_qemu to pull in #11273	5
[BYOC][TENSOORT] Add support for FP16 on TensorRT BYOC flow  (#10388)* FP16 support for TRT* Cleanups on tests* Fix for typing on output tensor* Fix icheck* Add TRT inference builder auto-convert precision flags as attrs in the config* Address PR comments* Fix bug on passing the new config attrs to codegen for tensorrt partitionCo-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
Disallow copy to/from external HexagonBuffer (#9930)* Disallow copy to/from external HexagonBuffer* change nbytes -> allocation_nbytes for clarity* retrigger ci	4
fix temp array object reference in manifest_shared_memory_local_stage (#12516)	0
[TFLITE]GATHER_ND (#5508)Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>	5
Organize the CodeOwners file: (#8512)- Order by depth first- Always show the complete prefix	0
[skip ci][ci] hotfix Jenkinsfile (#10492)This was broken by #10456 since that did not have a case for ignoring the `main` and staging branches. This adds it at the Jenkins level so the further scripts won't even be run if not necessary. This is marked `skip ci` but since the changes all happen before lint this is being tested.commit-id:1655b8daCo-authored-by: driazati <driazati@users.noreply.github.com>	1
basic var	5
sequential cpp test (#5745)	3
[Relay][Training] Add checkpoint annotation for checkpointing memory optimization (#4146)* add checkpoint annotation for checkpointing memory optimization* add alpha-equivalence checkpoint test and fix gradient type issue* fix build issues* ignore checkpoint annotation when checking missing gradients* refactor, fix checkpoint compute for tuple and add tests	3
[CI] Update cpu docker (#4153)	2
[RELAY][ONNX]ReduceLogSumExp Operator support (#5453)* [RELAY]LogSumExp Op Support* [ONNX]LogSumExp Op Support	1
[Relay][Bugfix] Fix off-by-one error in BiasAddRel, use new reporting (#7467)* Fix off-by-one in BiasAddRel, use new reporting* No need to mark xfail if the exception is caught* lint	0
Update VTA schedule (#1464)	5
[BYOC] [DNNL] enable in-place post-op sum in dnnl json runtime (#12371)* enable inplace post-op sum in dnnl byoc* add inplace post-op sum test	3
Fix broken CI when git-merge needs to create a commit. (#11007)- This fixes an error seen when CI needs to create a merge commit   to sync the PR. CI started doing this recently to ensure that all   CI branches use the same commit for their regressions. - The error looks like:        + git merge e370ed459739f5312e45a2fb3a446b120f8ec5d1        *** Please tell me who you are.        Run          git config --global user.email "you@example.com"          git config --global user.name "Your Name"        to set your account's default identity.        Omit --global to set the identity only in this repository.	1
hot fix (#6434)	0
fix typo (#56)	2
Tighten split's extent (#4931)* Set split node's range to minimum of ext and split factor or split nparts, but only when PassDownDomain is called with allow_missing == false, i.e. by InferBound.  Add a helper PassUpThreadBinding() to get a map telling whether an IterVar has at least one leaf IterVar deriving from it binding to a thread. Add two unit tests.* Enhance LoopVectorizer for vectorizing by 0.  Found at least one case from testtopi/tests/python/test_topi_transform.py::test_tile.* Revert changes vectorize_loop.cc; when parent's ext is zero, set split's range to the factor or nparts.* Update with comments.* Refactor the ext tightening predicate.* Fix reference types.* Integrate tvm.te changes.* Trivial comment change to trigger CI.* Trivial comment correction to trigger testing.	3
Move Compute library to 21.11 (#9754)	4
[API] Move all RTTI related code to one place (#20)* [API] Move all RTTI related code to one place* add back rtti comment	1
[TOPI] Add tensor multiplication. (#2106)	1
[Relay/Topi][Op] 1D Pooling (#4663)* Added 1D pooling to Topi* Added 1D pooling relay op and tests.* Added onnx parsing and tests for maxpool1d and averagepool1d* formatting* moved partial import.* Fixed typo.	2
[TIR][USMP] Add a parallel to serial for loop converter pass (#8469)* [TIR][USMP] Add a parallel to serial for loop converter passThis is an optional pass to convert all parallel for loops in TIRto serial ones for different reasons such as executordoes not support parallel launch of for loops (e.g., AoT)or allocating space for parallel for loops might notbe desired.* Additionally adding FFI scaffolding for USMPChange-Id: Id5e8ccb90140d2d3ae113b20a3ca152a54497c45* [TIR][USMP] Add a parallel to serial for loop converter pass* remove unused importChange-Id: I29d5fdec92120418596f9dba1d6630f65620a603* [TIR][USMP] Add a parallel to serial for loop converter pass*moved the pass to tir namespaceChange-Id: I74720ca2f566066b3a4f22f504d8f0f684c99dc2* [TIR][USMP] Add a parallel to serial for loop converter pass* fixed docstringChange-Id: I73bb9867fe2ed6a86f65666493c5c6e3edf87b49* [TIR][USMP] Add a parallel to serial for loop converter pass* fixed mypy lint errorChange-Id: I226ef27d5536674fbe4b2d2c6ff47b8cb3b41431	4
[Metal] Remove matching Metal to OpenCL in tophub (#8211)	4
Resolve more warnings in msvc (#6702)	2
[microTVM][ARM] Add Relay tests for conv2d registered schedules (#11250)* Added conv2d relay test for each schedule* Enable relay tests in qemu* split aot test utils	3
Remove unused pylint 1.9.4 from docker installation script (#6538)	2
[fix] relax QnnConv2DTransposeRel constraint (#10716)* zp check* lint	0
[RUNTIME][DSO] Improve TVMBackendPackedCFunc to allow return val (#4637)* [RUNTIME][DSO] Improve TVMBackendPackedCFunc to allow return value.Previously the signature of LibraryModule's PackedFunc does not support return value.This wasn't a limitation for our current usecase but could become oneas we start to generate more interesting functions.This feature also start to get interesting as we move towards unifiedobject protocol and start to pass object around.This PR enhances the function signature to allow return values.We also created two macros TVM_DLL_EXPORT_PACKED_FUNC and TVM_DLL_EXPORT_TYPED_FUNCto allow manual creation of functions that can be loaded by a LibraryModule.Examples are added in apps/dso_plugin_module.The change to TVMBackendPackedCFunc is backward compatible,as previous function will simply ignore the return value field.* address review comments	1
[COMMUNITY] Add @abergeron -> reviewer (#5064)	1
[RUNTIME][CRT] support DLTensor whose ndim == 0 (#5344)Signed-off-by: windclarion <windclarion@gmail.com>	1
Revert "[topi][relay] add operation tan to TVM (#4938)" (#5017)This reverts commit d992468d80af816f0413fc43c2ee1c02f7fe19c3.	4
[Hexagon] Set target architecture when compiling for Hexagon (#10771)	1
fix ROCm strategy for winograd conv selection (#5001)	0
[CI][ACL] Enable ACL installation in ci_cpu docker container (#5916)This patch adds a cross-compiled ACL build to the ci_cpu dockerfile used for CI.Change-Id: I66e1521ab553306bc7367b65acc0363e750f0211	4
Always docker/build.sh with --no-cache. (#8038)* Almost every docker container starts with apt-get install foo bar * This is inherently not cacheable, meanwhile the lack of --no-cache   has bitten nearly everyone I've talked to who's tried to rebuild. * Adding now to address this repeated problem.	0
[DOCS] Fix vta tutorial (#4809)	0
Fix GetReduceAces (#1460)	1
[ONNX] Use take instead of min in NMS conditions (#7633)	1
Auto-discover C/C++ compiler instead of hardcoding g++ (#10007)Some platforms (e.g. FreeBSD) use clang as the default OS compiler,and there is no g++.	1
[RUNTIME][WEB] Cleanup logging for web runtime. (#7750)* [RUNTIME][WEB] Cleanup logging for web runtime.The log(info) won't work for web runtime due to the usage of timefunction.- Introduce TVM_LOG_CUSTOMIZE anf TVM_LOG_STACK_TRACE.- Reorganize the log customization code to wasm_runtime(so non-gpu usecase can apply).- Update the testcase to cover the logging.* Fix windows build and address comment.	1
Improve XGBTuner document (#8428)* chore: improve xgboost_tuner docstring* chore: remove whitespaceCo-authored-by: Siwa <siboon@sertiscorp.com>	4
[microNPU] Upgrade Vela to v3.2.0 (#9635)* [microNPU] Upgrade Vela to v3.2.0In addition to upgrading the version of Vela, this contains a bug fixand a fix due to an api change.The bug fix was found as a result of upgrading the version and ensuresthat block traversal mode is calculated before a set of block configsis found. As a result, valid block configs are calculated consistentlythroughout compilation.The fix for the api change is a cast to the datatype now expected byVela due to using strict casting rules.Change-Id: Icd40d11d37859f660527571d29cfeddd761d60da* adding version change to gen requirementsChange-Id: I45d2892951558e52c599af34e8d9d2d3fb5eb20b	4
update stm32mp1 arm_cpu target configuration (#7443)Add the -mcpu information to complete the picture.Signed-off-by: Vincent ABRIOU <vincent.abriou@st.com>	5
[Hybrid Script] Inter-function call supported! (#2287)	1
[PASS] More simplifier for mod and div (#1100)* [PASS] More simplifier for mod and div* fix testcase	3
Description updated for pooling attributes (#5091)	5
[TVMScript] Printer Frame (#12366)This PR:- Implement Frame for the TVMScript Unified PrinterCompared to the prototype version, this:- Removes the dependency of VarTable (SymbolTable) from Frame- Adds a callback array to the Frame base class so that VarTable can add callback to clean variable when Frame goes out scopeTracking issue: https://github.com/apache/tvm/issues/11912	0
[REFACTOR][IR] Unified IR IRModule structure. (#4699)This PR brings relay::Module as the unified IRModule structure.IRModule will be used as the basic unit for transformationsthrough out the stack.- Rename relay::Module -> IRModule- Move relay/module.h -> ir/module.h- ModuleNode::FromExpr -> IRModule::FromExpr- FromText -> IRModule::FromText	4
[AutoTVM] Fix hang/crash issues on feature extraction (#3689)* [AutoTVM] Fix hang/crash issues on feature extraction* Update xgboost_cost_model.py* fix lint	0
[UnitTest][TIR] Testing utility for before/after transform tests (#12264)This PR adds `tvm.testing.CompareBeforeAfter`, a generalization of the `BaseBeforeAfter` utility previously used in `test_tir_transform_simplify.py`, which performs unit tests that perform a transformation on a TIR function and compare the results to an expected TIR output.  This arose when minimizing the boilerplate required for unit tests in the implementation of https://github.com/apache/tvm/issues/12261.	0
[skip ci][ci][actions] Hardcode Python version to 3.7 for miniconda setup (#11136)Fixes #11131Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay] [Virtual Device] Store function result virtual device in virtual_device_ field (#9848)* VStore function result virtual devices in virtual_device_ field* Address Mark's 'mega nit'* Promote function result virtual device to first class* Add kVirtualDevice* move kVirtualDevice* Fix annotation test* Progress on parsing & printing* Fix printing of virtual device attribute* flake	0
support concat in recast (#8028)	1
[Relay] Add generic & informative Relay error reporting (#2408)	0
[Relay][vm] Small bug fix for DataTypeObject (#3604)* small bug fix for DataTypeObject* retrigger ci	5
[Hexagon][Docker]Add HEXAGON_SDK_ROOT ENV variable (#11291)	2
[TOPI] C++ doc (#320)	2
[RELAY] TextPrinter: Use Map Format (#2553)	1
Update legacy places from nnvm to relay. (#4535)* Update legacy places from nnvm to relay.This PR prepares the current mainline to remove nnvm compiler dep.* remove legacy stage	4
add nn.global_avgpool to fq2i (#9137)	1
[MetaSchedule] Postproc: Rewrite-Layout (#11884)	5
Add a combine batch_matmul pass (#5791)* Add a combine batch_matmul passContrary what you might expect, this doesn't share as much code withthe combine dense pass as it does with the combine 2d conv pass.This is because it concatenates the "output feature" dimensions.* fix docstring	2
Update install.rst (#5858)* Update install.rstminor cleanups/corrections* Update install.rstFixed broken link	2
[TOPI] Tunable Template for Conv2D HWCN on CUDA (#4168)* support conv2d HWCN in AutoTVM and Relay* fix lint* fix comments and unit tests	3
[NNVM][ONNX] Shape operator support (limited/differed) - #1297 (#1333)	1
[ci] Roll out teams-tagging to everyone (#10739)* [ci] Roll out teams-tagging to everyoneThis removes the check for opted-in users from #10317, making it so anyone can attach their names without having to also opt-in. Also included is a script to generate the list of owners from `CONTRIBUTING.md` which was used to update #10317.* Cleanup after discussionCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Hexagon] Resolve breakage in test_hexagon/test_cache_read_write (#10520)* [Hexagon] Resolve breakage in test_hexagon/test_cache_read_writeBreakage was caused by https://github.com/apache/tvm/pull/9727, whichdidn't account for the new `builtin::mem_copy()` when computing thestack size in `StackSizeChecker`.* Added comment indicating need for StackSizeChecker::MakeMemCopy.* Updated unittests to run all contrib/test_hexagon at CI.* CI bump* Fix lint formatting error.* Updated fix to remove StackSizeChecker entirely.* Bugfix, verify the precheck's allocations, not own.* Bugfix, pass context information to the precheck.	5
[TOPI] Update tophub according to the fix in schedule (opencl and rocm) (#3752)	0
[DOC, TVM] ResNet tutorial, updated TVM (#51)	5
[MetaSchedule] Schedule Rule: Cross Thread Reduction (#9994)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[RELAY]Ops Dense, leaky_relu (#1828)	5
[Relay][RFC] Implement type checking for Any (#3221)* Implement type checking for AnyRemove code generation related changesRemove compile changesRemove moreRemove unification hackAdd some code back that was needed, and clean up testRefactor test casesWIPImplement TypeHint ASTAdd test case which should failRemove unification changes, and fix bug with let recRestore unification for shapesImprove error reporting while debuggingAll examples type checkAll examples type checkWIPFirst version that works with hints, needs clean upRemove dead codeTweaksRemove type hintRemove unecessary type hint stuffRemove more type hintsClean upExpose Any expression nodeAddress CRFixFix solverKill unecessary codeFixPyLintFixRelocate loopsFix license and testLint againLint againFix loopsFix docstringFix template errorFix compiler issueFix compile errRemove more runtime changesRestore bufferFix segfaultFixFix arange* Address feedback* Fix typo* Fix arange* Fix op level3* Fix issue with Python wrapper	0
Enable reduction in front-end	0
[MetaSchedule] postproc: rewrite_parallel_vectorize_unroll (#10071)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
Introduce --interface-api={c,packed} parameter (#8280)* Introduce --interface-api={c,packed} parameterThis introduces structures generated to provide a documented and stable userfriendly interface to a TVM generated model, as can be seen in the AOTdemo application:```struct tvmgen_default_inputs inputs = {  .input_1 = input_data,};struct tvmgen_default_outputs outputs = {  .output = output_data,};int ret_val = tvmgen_default_run(&inputs, &outputs, NULL, NULL);```To facilitate this, some other changes are included:* Removed dependency on `aot_executor.{c,h}` in tests, pending thediscussion in the interface RFC as to whether we keep them.* Moved creation of test DLTensor's into the AOT test utils, in future thiscan be replaced by loading via the Python API or otherwise* Introduce `parametrize_aot_options` which can be used to testpermutations of AOT which work together - for now this filters Cinterface and packed operators* Updated demo application to generate the header for demonstrationpurposes, we should consider porting the demo application to ModelLibrary Format and using the toolchain in the Zephyr App via CMakeinstead?This patch builds upon the improvements @giuseros made to AOT testingand name mangling from #8014* Tweak metadata variable description and MLF target loop* Remove direct usage of `relay::Var` in meta_data.hThis looks like the only place that could be causing the Windows CI failures, so trying removing the additional header in meta_data.h* Linting fix* Post-rebase files fixingThese tests were somehow transmuted in transit, I've updated them to themost recent variant of the test helpers.* Strip back interface API to just inputs and outputsThis removes any speculative structures from the generated code and cleans up some of the documentation.* Add header guards and tweak documentation	2
Fix TRT weight conversion when first dim of weight shape is 1 (#7253)	0
[ci] Add conditionals for non-Python tests (#11438)These don't get sharded in any way so there's no point in running them multiple times.cc Mousius areusch	1
[Relay][VM]Compiling pattern matching (#3470)* [Relay][VM]Compiling pattern matching* Fix lint* Remove debug code* Move TreeNode definition* merge ifi and selecti, todo: remove them* fix lint* remove ifi and selecti* rename GetTagi to GetTag* fix dltype* fix more dltype* Generalize If and select, and rename to Ifi and Selecti* Fix lint* Rename Ifi to If* Change register default to match value* Remove bad specialization for Move* Stop use Select* Remove Select* TreeNode refactor* Change entry_func name* Remove Cmp due to rebase issue	0
docs: minor spelling tweaks (#4027)	2
[microNPU] Refactor Relay to TIR hook (#10599)* [microNPU] Refactor Relay to TIR hookRefactors the Relay to TIR python hook for the NPU so that optimizationscan be applied across the whole module and not just functions that willbe offloaded to the NPU. A pass `OutlineCompilerFunctions` is introducedto outline NPU functions, which now happens before optimization passesare run (this previously happened after the prim_func had been created).In addition, optimization passes that should only run on NPU functionsare now limited to running on outlined functions for the NPU (bychecking the "Compiler" attribute). To help avoid code duplication, ahelpful decorator `create_npu_function_pass` has been created for pythonpasses that should only run on NPU functions.This refactor helps move a number of passes in the microNPU codegen touse an IRModule -> IRModule philosophy.Change-Id: Icdea9ba43da0157d5ee17529d2b23b761396d112* add mixed compilers to testChange-Id: I3ca48738e096bb0f4dc362f0e9550317fc0d5afd* Address comments including renaming both npu_pass and RelayToTIRThis commit renames `npu_pass` -> `create_npu_function_pass`.It also renames the `RelayToTIR` pass created in Python to `LowerToTIR`,along with moving it to compiler.py to make it clear that this pass is awrapper around the `_lower_to_tir` function. In addition, to make itexplicit that the `lower_to_tir` func->func pass should not be useddirectly it has been renamed to `_lower_to_tir` - it is being maintainedsince it is used in many tests.Change-Id: I3a0a06801f029aeaa4a51c2d86d8703bb0d7afbb* address nit and small fix to exampleChange-Id: I44c64de15fa8680cc89ce0440ffa6c9e0ec62a50	4
Revert "[TE][Fix] Comparison of the output tensor (#9829)" (#10540)This reverts commit 73cf51b8246f1f0b5c0bff6fa206d154e053199b.	4
Support imagescalar operator for onnx (#448)	1
[Relay][VM]Fix debug statement (#3565)* [Relay][VM]Fix debug statement* Change debug statement	0
[Relay][Frontend][TFLite] Add parser support for squared difference (#4652)* [Relay][Frontend][TFLite] Add parser support for squared difference* fix some error* fix exp_type* add comment	1
Improve IntervalSet's floormod (#5367)	1
[Vulkan] Minor optimization for deferred token lookups. (#3960)Use a hash map keyed on the descriptor set to avoid bad asymptotic behaviour.	1
[FIX,TIR] Handle LetStmt in EstimateTIRFLops (#12138)* [FIX,TIR] Handle LetStmt in EstimateTIRFLopsAdd flops for the right hand side of let statements to the total flops.* assert flops	3
[TIR] Bugfix for zero number arguments tir functions. (#8515)* [TIR] Bugfix for zero number arguments tir functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[REFACTOR][IR] Move error.h into ir (#4701)We will use a single ErrorReporter to report errors duringprogram transformations.	0
fix pooling documents (#12201)	2
[Meta Schedule][M3c] Argument Info (#9059)This PR is part of the meta schedule project (#8473) that adds metadata of each PrimFunc's argument.This feature is necessary for dynamic shape auto-tuning.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
Update CONTRIBUTORS.md (#10972)	5
[TIR][REFACTOR][API-Change] Migrate the tvm/tir/expr.h to construct style. (#5773)This PR migrate tvm/tir/expr.h to the new constructor style that isconsistent with the rest of the codebase and changes the affected files accordingly.	2
[Node] Provide guide to user who has difficulty register SEqualReduce (#5300)	1
[Autotvm] Fix autotvm customized template (#5034)* init* fix template* tweak naming	0
[Compilation Warning Fix] in matrix_op.cc (#356)The below compilation warning issue has been fixed.src/top/tensor/matrix_op.cc:37:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   for (int i = 0; i < lshape.ndim() - 1; i++) oshape[i] = lshape[i];                     ^src/top/tensor/matrix_op.cc:38:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   for (int i = 1; i < rshape.ndim(); i++) oshape[i + lshape.ndim() - 1] = rshap	2
Add CMake summary (#9696)* Add CMake summaryThis adds a printout at the end of CMake's configuration that prints out all the user options (anything declared via `tvm_option`) plus some extra metadata about CMake itself (version, system, etc). This makes it easier to see what CMake is doing without guessing from config files (i.e. is the build release or debug mode, is CUDA on, ...)* Add SUMMARIZE=ON to CI cmake configsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Runtime][PipelineExecutor] Add the pipeline internal forwarding logic. (#10543)* [Runtime][PipelineExecutor] Add the pipeline internal forwarding logic.This patch use the SPSC lock free queue to forward the runtime output datainto the child runtime input interface.* remove debug logic.* address review comments.* correct a variable comments.* address review comments.	1
[DOCKER] inheritate javahome (#2210)	2
Run AOT tests against reference system (#8744)* Run AOT tests against reference systemThis introduces an alternative way of running AOT tests using the reference system added in https://github.com/apache/tvm/pull/8514. This gives us additional assurance that the AOT output runs successfully on embedded platforms in our core test suite.I've also changed calculate_workspace_sizes to debug_workspace_sizes and default to False in most cases as it only needs to be True for a few cases to check theoutput with the debug flag - this was discovered trying to allocate 16MB in an embedded test :scream_cat:Co-authored-by: Grant Watson <grant.watson@arm.com>* Skip AOT reference system tests in i386 container* Add comment clarifying the reference system runnerCo-authored-by: Grant Watson <grant.watson@arm.com>	1
[Frontend][Tensorflow] Gather nd bug fix for one dim support in tensorflow (#5588)* [Frontend][Tensorflow] Gather_nd one dim support added* Test case added* Doc error handled* Review comment handled: reverting new attr introduced* Check added at mxnet frontend* Doc error handled* TFLite test case failure resolved	0
[LANG] Enable json load/save and pickle (#10)	5
[Relay][Pytorch] Add aten::new_ones, aten::new_full, aten::fill_, aten::pad, aten::reshape_as and atem::empty_like (#11896)* add new ops* fix pad* fix pad* remove pad* fix CI* remove doc* fix fill_* add tests	3
[CI] hot fix Sphinx (#9998)	0
[MetaSchedule] Allow MultiLevelTilingTensorCore rule to specify multiple tensor intrin groups (#12113)	1
[ONNX] support additional nllloss tests (#9045)* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* working tests* add test, remove unneeded file* fix things* more lint* more lint* pick things* disable opencl tests* unsqueeze tests* clean up* dyn stuff* add num_newaxis* add support* black* doc string* remove bad merge* fix default axis behavior* rebase* fix squeeze* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
Compare relay and numpy outputs in graph runtime test (#2164)	3
[CI] Improve VTA build message and scripts. (#5170)* [CI] Improve VTA build message and scripts.* Use absolute path to set the env var	1
changed makefile to build rocm backend (#355)	2
[RPC] Fix cpp_rpc connection to rpc_tracker (#8388)* [RPC] Fix cpp_rpc connection to rpc_trackerFixed connection cpp_rpc application to rpc_tracker which was broken bythis commit: 0bbaf0e.Also, made it possible to create a linux shared library on MacOS.Without this change default tuning didn't work on MacOS.* Add explicitly check on dylib	1
Add @slow decorator to run tests on `main` (#10057)* Add @slow decorator to run tests on `main`This adds the infrastructure discussed in https://discuss.tvm.apache.org/t/rfc-ci-skip-slow-tests-on-prs/11910, but without affecting any tests. As we investigate reasons behind [slow tests](https://gist.github.com/driazati/e009f09ff44c6bc91c4d95a8e17fd6f1) in CI, this decorator will allow us to move these to run only on `main` and not PRs after checking with all concerned parties.* cleanupCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Pass] enable infer type (#17)	5
[LLVM] Add "cl-opt" attribute to target_kind "llvm" (#12440)* [LLVM] Add "cl-opt" attribute to target_kind "llvm"Add LLVMTargetInfo class that can be used to query the LLVMconfiguration without forcing an LLVMTarget to be created.There is no programmatic way to obtain the actual type of an LLVMoption. The type is necessary to obtain the value of the option,hence it must be provided as a part of the option string.See src/target/llvm/target_kind.cc for more information about thesyntax.* Fix lowercasing of bool value string* Use std::optional instead of std::pair<..., bool>* Treat malformed options as fatal errors* Fix linter* More unit tests for option parsing, have one case per test* Remove "option ignored" from fatal error messages	0
[RUNTIME] Scaffold structured error handling. (#2838)	0
[REFACTOR][OBJECT] Consoldiate NodePtr/Ref/Hash/Equal  to Object (#4603)* [REFACTOR][OBJECT] Consoldiate NodePtr/Ref/Hash/Equal and macros to Object.Historically, we have classes like NodePtr/Ref/HashEqual.After unified object protocol, these names are just alias of the object counterpart.Moreover, there are helper macros defined over the places for defining these object.This PR consoldiate the terminologies into the corresponding onesin the Object system so we have a clean and consistent API moving forward.* Update include/tvm/attrs.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>* fix compilationCo-authored-by: Wei Chen <ipondering.weic@gmail.com>	0
add is_entry_func tag for device function (#9436)	1
[NNVM] Fix grads for sum and expand_like (#1455)	0
support to keep trivial loops with extent of 1 (#877)	1
Compat for opencl mode between cpu mode and gpu mode (#655)some host opencl runtime may at cpu mode, but remoteclient opencl runtime at gpu mode, compat it	1
[Hexagon] capture gtest output and return over FFI (#11239)* [Hexagon] capture gtest output and return over FFI* rename to test_hexagon_unit_tests.py so it will run in CI* rename to run_unit_tests.cc* pass back gtest error code along with gtest output* skip Hexagon unit tests if gtest not enabled* pass gtest_args as pytest argument* change env variable to HEXAGON_GTEST* set HEXAGON_GTEST in the environment to enable Hexagon unit tests* add back try / except around get_function	1
[THRUST] Faster multi dimensional argsort by segmented sort (#7195)* remove sort nms* add segmented sort by key impl* bug fix, test pass* updated fast path condition to work for all dims	1
[FIX] make keras frontend py3 compatible (#283)	1
Fix newer GCC compiler warnings. (#6257)	2
Fix running gtest on Hexagon hardware (#11257)	3
[Relay] Fix format (#1957)* save* fix format	0
[MISC] Add miss Type2Str and remove compile warnings (#10430)* [MISC] Add miss Type2Str and remove compile warnings* fix lint	0
[Relay][Frontend[Onnx] Add testing for output datatypes and fix related bugs. (#7364)* Add testing for datatypes and fix related bugs.* Fix lint issue in onnx.	0
[DOC] Add setup.py, fix comments (#89)* Add setup.py, fix comments* Update installation document	2
Fix for issue #4831. The data_min_idx and data_max_idx were flipped. (#5136)Existing test cases cover this fix. In addition I have added an assert to make sure that the data_min is always less than equal to data_max.	5
[microTVM][RVM] Fix platform name in base-box-tool (#8612)Platform boards passed to base-box-tool.py need to be a subset ofplatform boards support by 'tests/micro/zephyr --microtvm-platforms='.Currently base-box-tool.py only accepts the 'stm32f746xx' ST board,which is not supported by 'tests/micro/zephyr --microtvm-platforms='. Asa consequence if one passes '--microtvm-platform=stm32f746xx' tobase-box-tool.py the 'tests/micro/zephyr' test will fail.That commmit fixes it by adding two new platforms to base-box-tool('stm32f746xx_nucleo' and 'stm32f746xx_disco') which are supported bytests/micro/zephyr and by removing the nonexistent 'stm32f746xx'platform. The new platform boards are quite similar and share the sameUSB VID and PID.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
Fixed nnvm issue #239 (#660)* scheduler tweaked for super resolution perf* conv2d_transpose schedule error fixed* nnvm issue #239 fixed	0
[Tensorize] Fix compute reusing (#7920)	1
[MetaSchedule] PostProcessor: Verify GPU Code (#9945)	5
[CODEGEN][CUDA][OPENCL] Handle INF and NAN (#3194)	5
[ONNX] Fix onnx convtranspose error (#9938)* fix mix up of channels with conv2d-transpose* add grouped convtranspose tests* turn off groups for non-llvm test	3
[RPC] Add Missing Command Line Option "through-proxy" of RPC Server (#10188)	1
fix string argument mismatch in GraphRuntimeCodegen (#5933)	1
[microNPU][2d] Add more Part matchers to cascader (#9785)* [microNPU][2d] Add more Part matchers for the cascaderAdds Part matchers for ethosu_depthwise_conv2d,ethosu_pooling and ethosu_binary_elementwise. Alsoadds additional testing for the CascaderGraphcreation.Co-authored-by: Jacob Bohlin <jacob.bohlin@arm.com>* Extended testing for block config* Add test guardsCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>	3
BUG #9216: Don't disable FuseOps pass since required by GraphExecutor (#9227)This tutorial disabled the FuseOps pass, but before #8788 that wasignored since FuseOps was applied directly.	1
[microNPU][1] Add affine analysis structures for the cascader (#9458)* [ETHOSU][1] Add affine analysis structures for the cascaderThe cascader relies heavily on being able to determinedata dependencies between operators. This is so that itcan calculate how stripes should be propagated through acascade.To do this, two data structures are defined: StripeConfigand Propagator. StripeConfig stores information for how atensor should be broken up into stripes and executed.Propagator transforms a StripeConfig using an affinetransform matrix, allowing an input StripeConfig for anoperator to be determined by 'propagating' the outputStripeConfig.By chaining together Propagators, we can analyse howdata dependencies vary throughout a cascade and thereforecalculate the memory requirements (and approximate theperformance).Change-Id: If7176fea961c631be4a6c195303da536030d957b* Add test guardsChange-Id: I1d7633e20daab33642fa5c4a12e474a4def4d8b8* Address review commentsChange-Id: Iff5f1effa08e0628de91f5577487d0cecebec824* Improve docsChange-Id: I508809d8c1a08d231e3a9b0fd9b3f2639cc2f0e3	4
[AutoSchedule] Fix a flaky test (#7580)	3
[µTVM] Fix problems with the debug flow (#6930)* Allow blocking read and write in micro transport, for debugging.* add support for None timeout to micro transport, add tests* fix GdbTransport and friends. * GDB itself was just busted (would not launch inferior properly) * GdbDebugger would kill the debugger without waiting for user   input. change to always wait for an explicit user quit. * immediately resurrect Ctrl+C handler when debugger dies. * remove on-terminate callback complexity, unnecessary	4
fix x86 pooling topi schedule (#1132)	0
[ARITH] Canonicalize comparison to move constant to one side (#3467)	4
Revert "[RUST][CI] Add rust frontend tests in Jenkins" (#2406)	3
[COMMUNITY] @sgrechanik-h -> Reviewer (#2732)	3
Fix compilation of tvm runtime for iOS (#8242)	1
[ARITH] Improve vector simplification for float operands (#6043)	1
Add synr==0.3.0 dependency for Docker images and Python dependency. (#8801)- PR #8776 removed `synr` as a dependency to be installed in the Docker  images, making the images to need manual intervention so that we could  run tests.- Thir PR reverts synr (with current constraint as observed in  tests/scripts/task_ci_setup.sh) to be part of the Docker image.	2
Support TensorFlow saved modelTF parser: return the consistent error message to error handler	0
Add option to specify flatbuffers location (#5425)	1
[TIR][Hybrid] Hybrid Script Support for TIR (#6227)	1
[ci] Fix slow test script permissions (#10457)This is failing silently, e.g.: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10359/4/pipelinecc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[FQ2I] Add leaky relu to FQ21 (#10378)* add leaky relu op + passing unit test* passing test* format* clean up* lekay relu qnn op* wip* qnn op* add comment* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
Fix AutoScheduler for anaconda python (#7387)In case of non cpython flavour of python, the task passed to measure processshould be serialized using pickle approach. The task includes workloadwhich is a list of Tensors. The list should be serialized and deserializedas an atomic object.	1
Build and test TVM under minimal configuration (#12178)This PR builds and tests TVM (running the CPP and unittests) under minimal configuration with some debug flags enabled:- `USE_RELAY_DEBUG=ON` in TVM- `-Wp,-D_GLIBCXX_ASSERTIONS` in TVM- `-DLLVM_ENABLE_ASSERTIONS=ON` in LLVMIt also adds this configuration to the CI. `tests/python/unittest/test_meta_schedule_task_scheduler.py::test_meta_schedule_task_scheduler_multiple_gradient_based` results in an array OOB access and a segfault due to `D_GLIBCXX_ASSERTIONS`. I disable this test for now and will open an issue to solve it ASAP.It should fix #11932 and address [this discussion](https://discuss.tvm.apache.org/t/pre-rfc-new-ci-container-ci-cpu-asserts/12536/9).	3
add document (#2714)lintlintsavesaveadd more casesaveerrorlintlintcommitdolintsavefix lintwrap it back as funclintsaveremove dead commentfix stylefix lintUpdate src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>address review feedbackpe now handle freevar. as a result preserving function is now trivial.testadd basic test, implement pretty printing for generic functiontestlintfix segfaultsavesavedotestfix another erroraddress commentcommitsaveaddress review feedbackadd test for invalidate, fix error in lookuprename cont to boduyfix error and add regression testfix error, add test caseUpdate src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>fix lintremove extra linesavesave	4
[LLVM] Include LLVM headers in files that use them, not in llvm_common.h (#11888)This is following the same principle we use everywhere else in TVM, thatis, every source file includes headers that it depends on. While includingunnecessary LLVM headers (which may happen by including llvm_common.h)is not actively harmful, it makes the header dependencies much less trans-parent.	1
Update Graph Support for Batching, Fix Swapping (#37)* fix graph transform for batch dimension* fix* fix	0
[RUNTIME] Enable NDArray type extension (#2598)	0
[Relay] Fix shape func for strided slice (#10418)* fix dyn strided slice* add tests* remove stuff* jostle ci* jostle ci* jostle	4
support slicing with out of order axes (#8959)	1
Fix #12039‘s broken cases (#12143)	0
Cleanup deadcode (#1991)	4
Case-sensitive CMake module for OpenCL (#537)* Case-sensitive CMake module for OpenCLCase-sensitive CMake module for OpenCL, see https://github.com/Kitware/CMake/blob/master/Modules/FindOpenCL.cmake .* fix in case Metal locates OpenCL as wellfix in case Metal locates OpenCL as well	0
Fix Travis on test (#59)	3
[Testing] Allow NCHW layout in `relay_workload` (#12656)	1
Correct function signatures for CreateXPass functions in docs (#8829)	2
LLVM 7.0 support (#1048)	1
[Relay][TF] add BatchMatMul (#3634)	1
Refactor IR Pass	4
merge extract_from_program and extract_from_multiple_progam (#4173)	4
checkin basic expr	5
[PERF] Persitent kernel (#87)* [PERF] Persitent kernel* fix doc	2
[ETHOSN] Update driver stack version to 22.08 (#12650)Updates the driver stack used by the NPU to the latest released version(semantic version 3.1.0), while maintaining backwards compatibility forthe previous version 22.05 (semantic 3.0.1) during the migration period.In addition, support for split is re-introduced as this is now supportedin 22.08.Change-Id: I86bce3469f0b8ad52e66461ae055dec6717b3527	4
[RUNTIME] Add workspace pool (#229)* [RUNTIME] Add workspace pool* fix doc* fix the free list* avoid zero size	0
TVMC - a command line driver for TVM (#6112)* Introduce a command line driver to compile, run and tune models, using TVM graph runtime * Include tvmc tests and integrate tvmc with linting, testing and CI * RFC: https://discuss.tvm.ai/t/rfc-a-tvm-command-line-interface/5165Co-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>Co-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>	5
[RUNTIME][OPENCL] delay device check (#1657)	1
[ONNX] LessOrEqual and GreaterOrEqual ops (#9066)* recreate things* jostle	1
[TOPI] Fix bug in Winograd on CUDA (#4260)* fix winograd* move get padding after kernel transform	1
[PYTORCH]Std op without specified dimensions support (#6226)	1
[RELAY][OP] Fix conv2d NHWC type inference. (#2019)	5
Fix line break in `setup.py` (#9029)	1
[LLVM] Add target feature string to function attributes (#6763)	1
[ci] Allow skip tag anywhere in PR title (#11714)	1
[PROFILING] Add ability to profile a single function_profiling (#9553)* [PROFILING] Add ability to profile a single function_profilingAdd a new function `tvm.runtime.profiling.profile_function` whichcollects performance metrics for a single function in an IRModule. Forexample, collecting performance counters using `PAPIMetricCollector`.This is helpful for optimizing kernels and schedules for a singleoperator.* fix docs* configurable number of warmup iterations. avoid allocating when stopping collectors	5
Cache object refs in loop partitioner instead of object pointers (#6004)* Cache object refs in loop partitioner instead of object pointersLoop partitioner modifies the IR, which can cause TIR objects tobecome dead and be destroyed. To avoid working on junk data cacheobject references instead of object pointers.* Fix format/lint errors	0
[Minor][Build] Fix compiler warnings hidden overloaded virtual function (#10485)	1
[FRONTEND][TENSORFLOW] Enhance with left over patches from NNVM. (#2757)* [FRONTEND][TENSORFLOW] Enhance with left over patches from NNVM.commit 76188a4Author: Siva sivar.b@huawei.com[NNVM][TENSORFLOW] bugfix. (#2444)commit 6737739Author: Ashutosh Parkhi ashutosh.parkhi@imgtec.com[Tensorflow] Support for Crop (#2285)commit f6c3f99Author: Alexey Romanov alexey.v.romanov@gmail.com[FRONTEND][TENSORFLOW] Use input shapes directly instead of 1-element lists (#2242)commit e5d92e1Author: Dominic Symes 36929632+dominicsymes@users.noreply.github.com[FRONTEND][TENSORFLOW] Bugfix (#2326)commit 00d509dAuthor: Alexey Romanov alexey.v.romanov@gmail.com[FRONTEND][TENSORFLOW] Support Unstack and Split (#2105)commit df9d3adAuthor: Siva sivar.b@huawei.com[FRONTEND][TENSORFLOW] Bugfix (#2267)commit d1a0c90Author: Zhebin Jin zhebin.jzb@alibaba-inc.com[FRONTEND][TENSORFLOW]Add Split and realdiv op support (#2123)* Add Split and realdiv op support* Fix the pad calculation in the case of dilated convolution* * review comments* * resnet fix.* * review comments	0
[TOPI][CUDA] Improve the performance of scatter_nd (#8479)* [TOPI][CUDA] Improve the performance of scatter_nd by:1. Split into 2 kernels, one does the "Init" and another does the "Update".   Thus they can have different Grid/Block configurations to better utilize   SMs.2. Use atomic_add instead of direct assignment, which could avoid the race   condtion when multiple indices point to the same location of the output   tensor. With this moidification, it's safe now to use more CUDA threads   to gain more parallelism.* Fix python code format.* FIX: [TOPI][CUDA] Improve the performance of scatter_nd #8479- Split ScatterND kernel into 2 sub-kernels using ib.new_scope()- Replace ib.for_range() with blockIdx.y- Using atomic_add when mode == "add"- Keep threadIdx.x less than max_threads of GPU* Comment added* Add fallback implementation when "mode=add" meets int64- Atomic_add from CUDA doesn't support int64 data type- Change "ind{i}" to "ind%d"%i, where names of relay.var could correctly display* Python format* Fix line too long* CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Exchange blockIdx.x and blockIdx.y* check for Vulkan or metal* Fallback to previous algorithm when mode==update* Update python/tvm/topi/cuda/scatter.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Assign TODO* Swapping then and else blockCo-authored-by: wenxizhu <wenxizhu@tencent.com>Co-authored-by: CaptainDuke <captainduke328@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	2
[Torch] fix unordered dictionary problem for python version under 3.6 (#4982)* fix unordered dictionary problem for python version 3.5* modify style	0
[CPP RPC] Fix the compile problem of cpp_rpc (#4725)	0
[Frontend,TOPI] Improve dynamism for BatchMatmul and Dense (#7496)* [TOPI] Dense cuda schedule support dynamic dimension* [TOPI] batch_matmul cublas te computation support dynamism* [Frontend] tensorflow frontend: dynamic support for BatchMatmul* [TOPI] nn batch_matmul te computation support dynamism* fix CI* Update python/tvm/topi/nn/batch_matmul.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/topi/cuda/batch_matmul.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* remove concat_dynamic_shape function* update topi dense op integer checking* fix ci* Update python/tvm/relay/frontend/tensorflow.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update batch_matmul.py* [Frontend] add test for batch_matmul in dynamic shaped caseCo-authored-by: Cody Yu <comaniac0422@gmail.com>	3
[Runtime][PackedFunc] Bring `PackedFunc` into TVM Object System (#10032)	5
[VTA] [Chisel] Improved Data Gen, Added ALU Test (#3743)* added alutest* fix indent* name change for cycle* improved data gen and infra* added alutest* fix indent* name change for cycle* improved data gen and infra* fix space* fix indent* fixes* aluRef* fix randomarary* add* Revert "add"This reverts commit 87077daebbe055dee11f80e37da3a6291138e0f0.* Revert "fix randomarary"This reverts commit df386c1e660eb6ebcff1a1f905610573676f1589.* Revert "aluRef"This reverts commit 8665f0d4a7b12b796b2cb1ca6bf9cfe5613ee389.* should fix dlmc-core	0
[CMSIS-NN] Fallback to native schedules for unsupported partiti ned functions (#10603)	1
[TIR][Analysis] Add SuggestIndexMap for layout rewriting (#10732)This PR added an analysis function `SuggestIndexMap` to analyze buffer access pattern and suggest index map for layout transformations.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
Add some tips for building on macOS. (#1215)	1
[AOT][BUG] Only include extra headers if the constants array is needed. (#12061)	0
[docs] Disable numpy intersphinx (#10752)These links usually 403 for us which `task_python_docs.sh` is set up to ignore, however sometimes it hits a 404 error which has been causing CI failures lately (e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2832/pipeline/). This disables intersphinx links to numpy entirely until we can sort out a better fix.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
remove unused variable (#131)	1
[RUNTIME] Quick fix PackedFunc String passing (#5266)	4
Support negative axis for gather (#7600)* Fix negative axis in gather* Clang Format* Black* Empty CommitCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>	0
[MetaSchedule] random compute location (#9940)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
Update include and src dir CHECK* to ICHECK* (#6745)	5
[TIR] Improved error messages for PrimExpr operator overloads (#12638)Previously, type-checks in boolean operators on `PrimExpr` wouldstate that the type is incorrect, but further investigation would berequired in order to determine what expression caused the error.After this commit, error messages for these type checks include theexpression that was used, and the dtype of that expression.	1
fix jenkins (#31)	0
[Relay][Hashing] Structural hash - incorporate the var type into its hash (#3267)Currently, the BindVar function does not take Var type into account. This causestwo same graph structures with different var shapes to have same hash.Structural hash is used for keeping track of which operators we havealready compiled. Because of this, two operators with different shapes end uppointing to same compiled code. The failure is encountered at runtime, where theexpected input shape asserts are not met.	3
[TEST] Add memoize to save test data (#424)* [TEST] Add memoize to save test data* Update comment* mark py version	5
[ci][tvmbot] Trigger GitHub Actions after merging (#12361)This fixes the issue where merging from GitHub Actions (i.e. with the default `GITHUB_TOKEN`) doesn't trigger post merge GitHub Actions on the commit it creates in `main`. Instead these jobs are triggered manually by a call to the Actions API after the merge has taken place.This also updates the tvmbot testing code (and by extension some of the other CI testing code) to remove the fixtures for each test in favor of constructing them from a single sample at runtime, this makes it a lot easier to add new tests and see what is different between each data sample and clean up the testing anti-patterns that were there before (e.g. `run()` instead of `pytest.mark.parameterize`, but none of the tests in `test_ci.py` have changed)Tested in https://github.com/driazati/tvm/pull/36 which ran https://github.com/driazati/tvm/actions/runs/2881047903	1
[microNPU] Add support for scalar values (#9794)* [microNPU] Add support for scalar valuesPR #9515 enabled support for scalar constants, but didn't consider thecase of a scalar value where the underlying constant data does not havea shape i.e. `constant.shape == []`. See the test case for a visualdifferece when the scalar value is 1.Change-Id: Id7a238cb5bf999dd5a8428c097202f9fb940a5f0* Fix failing test by removing constantBefore this PR scalar constants were handled differently so this testwas able to pass. Now that scalar constants are handled in the samemanner as tensor constants, the test fails since unexpected tir isproduced in the compilation pipeline. Since the relay used in this testcase is not expected to be produced by higher levels of the compiler,removing this constant for now.Change-Id: I4ea5155778809041339e6faac05af3f72c3e3ea5* clean up finding tensor from inputsChange-Id: Ideccf84f8c9149148ff23e2406229cf637c982a3	4
Fix Google Mock differences between Ubuntu 18.04 and 16.04 (#9141)I thought I got all of these, but turns out I didn't, there's another weirdism in how Google Mock and Google Test are packaged on Ubuntu where the `cmake` command fails due to directories outside of the build root.Double checked logs on Ubuntu 18.04 and Ubuntu 16.04 for this after enabling verbose copying:```shell$ ./docker/build.sh ci_cpu --net=host...'googlemock/libgmock.a' -> '/usr/lib/libgmock.a''googlemock/libgmock_main.a' -> '/usr/lib/libgmock_main.a''googlemock/gtest/libgtest.a' -> '/usr/lib/libgtest.a''googlemock/gtest/libgtest_main.a' -> '/usr/lib/libgtest_main.a'$ ./docker/build.sh ci_i386 --net=host...'libgtest.a' -> '/usr/lib/libgtest.a''libgtest_main.a' -> '/usr/lib/libgtest_main.a'...'libgmock.a' -> '/usr/lib/libgmock.a''libgmock_main.a' -> '/usr/lib/libgmock_main.a'```	3
[docs] Update minimum compiler requirements for building from source (#12405)	1
[TOP] level4 except argmax/min, correct split (#9)* [TOP] level4 except argmax/min, correct split* [DOCS] Add doc generator for top	2
Fix #8510 (#8511)	0
[Hexagon] Fix gtest flag in apps/hexagon_api/CMakeLists.txt (#11652)	5
[PYTHON] Allow no de-allocation when exit (#583)	1
[microNPU] Enable network tests for U65 256 mac variant (#10159)Currently we run the network tests only on one NPU variant, so thispatch is to enable all the currently supported variants.Change-Id: Ic18054fb4ba19eb28b99ff4439e5a36e57199763	4
[CMSIS-NN] Stop test generating 1x1 and 1xn Conv2d (#10784)I believe the flakiness in #10748 is the small chance of generating a1x1 or 1xn convolution which allows for a different buffer size:https://github.com/apache/tvm/blob/63461c0c97c307e581271708c3490f5275675a1a/src/relay/backend/contrib/cmsisnn/buffer_size.cc#L38-L47Therefore, careful selection of the distribution should alleviatethis issue.	0
Fix a spelling mistake (#135)	0
Conditions updated to cover better user scenarios (#4951)* Conditions updated to cover better user scenarios* [1] New test case added* [2] New test case added* [3] Proper variable name used* [4] Review Comments handled* [5] Review comments handled* [6] Review comments handled	0
[CODEGEN] Use correct math intrin for metal (#562)	1
upgrade ci lint docker file (#11734)	2
[M3c][MetaScheduler] Update TuneContext, TaskScheduler & Search Strategy Design (#9789)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Minor fix.Fix mypy.Fix mypy.* Retrigger CI.* Minor fixes.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[TOPI, Relay] A new NMS op variant for ONNX NMS / TF Combined NMS (#7796)* initial import* add c++ boilarplate* add python boilarpolate* update onnx frontend* fixing* update onnx frontend* fix shape* minor update* fix* fix shape func* fix for no box* more fix* made things 64 bit* int64 tweak* max_output_size doesn't need to be a callback* remove all_class_nms schedule* minor simplify* remove expand_dim* refactoring* simplify nms loop* cpu all_class_nms stub* updating ir for cpu* working with cpu* update cpu strategy, relay op also working* fix cpplint* fixing pylint* enable gpu test for onnx nms* tweak parallel* pyformat and lint* fix relay nms test* doc update for cpp relay* updating tests* updated tests* fix converting score_threshold to Expr* update doc* doc fixCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	0
[Hotfix] typo in Vulkan runtime change causing severe perf regression (#7871)Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>	1
[Relay] Fix operator fusion for multiple output (#3871)* save* add test* refactor* fix indent* save* refactor	4
[DOCS] Enable theme with header and footer. (#6834)Also fixed a sphinx warning in pytorch.	2
[DOCS] Reword community guide (#1494)	2
[TIR] Support AllocateConst nodes in TensorIR scheduling flow (#12489)* [TIR] Support AllocConstantNode in CreatePrimFunc* Handle AllocConstantNode in LeafBlockRemovalPlan* Properly handle AllocConstNode in BufferAllocationLocator* handle AllocateConst in EstimateFlops* remove NDArray printing* doc update* add test* cpplint* Removed dependency on link-params attribute from target* Restored NDArray printing to unbreak test	3
[Runtime] Allow for parameter sharing in GraphRuntime (#3384)Summary:In multi-threaded applications where we have multiple inferences on thesame model in parallel (consider e.g. a TTS system handling multiplerequests), it can be useful to share the parameters of a model amongstthese multiple instances. This improves the cache utilization behaviourof the system, as multiple cores can use the same set of weights insteadof evicting the identical copies of weights in a shared cache.As the underlying `NDArray` instances in `data_entry_` implement aref-counted based sharing system, this is a simple modification of the`GraphRuntime::LoadParams` logic to instead copy parameters from anexisting GraphRuntime instance. This is a little ugly in that we needboth the pre-existing GraphRuntime instance, as well as the 'serialized'params (since we need to know the set of names we should copy), butwithout imposing additional assumptions (i.e. storing the set of paramnames in GraphRuntime, and enforcing that shared param names areidentical to the parameters set in the preceding `LoadParams` call),this seems unavoidable.Test Plan:Unit test added.	1
[RPC] Tracker status query (#1081)	5
Frontend: add onnx GlobalLpPool op (#8845)* Frontend: add onnx GlobalLpPool op* update* fix for testCo-authored-by: xp224797 <xp224797@alibaba-inc.com>	3
[DOC]Errors corrected (#1767)	2
bumping vta version (#6977)	5
add committer (#2661)	1
[TIR] Fix data dependent indexing when lowering TE to TIR (#8217)A conversion pass was missing the recursive VisitExpr statement.	4
add reading of nRF5340 DK product ID to determine which COM port to use (#10304)	1
[TUTORIAL] use OpenCL on ARM board (#633)	1
[CONV] Asymmetric padding (#4511)* [CONV] Asymmetic padding* fix lint error* update for legalize, rocm and cudnn* add more test cases* change more symmetric padding* change conv2d winograd tests according orginal cases* remove 'alter_op_layout.h' header in bitserial.cc	4
[MetaSchedule] Sample-Perfect-Tile (#9449)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[Rust][IRModule] Flesh out IRModule methods (#6741)* WIP* WIP* WIP* WIP* Disable WASM and fix rebase* Work on finishing tests* Make entire object system printable* Write some more tests for IRModule* All tests pass* Format* Restore module.cc* Bump syn	4
[Refactor][std::string --> String] IR is updated with String (#5547)* [std::string --> String] GlobalTypeVar is updated with String* [std::string --> String] GlobalVar is updated with String* [std::string --> String][IR] ADT is updated with String* [std::string --> String][IR] OP is updated with String* [std::string --> String][IR] Attrs is updated with String input* [std::string --> String][IR] GlobalVar is updated with String* [std::string --> String][Test] Pyconverter is updated with String change	4
[Topi][Testing] Float16 unittests for dense, conv2d, depthwise conv2d (#8529)* [Topi][Testing] Minor cleanup for python reference implementations- Use input dtype for dilate/conv2d accumulate in python  impl. Previously, the python implementations of dilation and conv2d  would use numpy default dtype in some cases, rather than the input  data's dtype.- Added fallback for datatypes not supported by scipy.signal.convolve2d (e.g. float16).- Refactored to avoid duplication, use common get_pad_tuple functionality.* [Topi][UnitTests] Added float16 tests to test_topi_dense.py* [Topi][UnitTests] Added float16 to test_topi_conv2d_nchw.py* [Topi][Float16] Added float16 tests for depthwise conv2d.* [UnitTests] Explicitly set seed for float16 testsIntended to avoid flaky test failures later due to rounding errors.* [UnitTests] Fixed a few failing unit tests.- ref_data must be a test fixture, not acquired through  request.getfixturevalue, in order to have the random_seed be known.- dilate_python's return value didn't follow `out_dtype`.- The test_topi_conv3d tests had the reference results computed in  float64, due to dilate_python() not respecting the input data type.  With the correct dtype, the tolerances needed to be slightly widened.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[RUNTIME] Improve error messages for TypedPackedFunc (#7152)* [RUNTIME] Improve error messages for TypedPackedFunc- TypedPackedFunc now prints the function name when the incorrect number  of arguments is passed.- TypedPackedFunc now prints the function name and which argument when  an argument cannot be converted to the correct type.* check argument conversion by template deducing argument types* switch from template approach to TVMMovableArgValueWithContext* move passes back into cc files* remove error message prefixes* Remove TVM_ICHECK_TYPE_CODE. Rename name to optional_name.* revert changes to module pass for later PR* reverted too much* documentation* formatting* more docs* unify error message language. TypedPackedFunc contrustor that does not take a name* Update include/tvm/runtime/packed_func.hCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
remove unnecessary cast to int32 (#4573)	4
fix wrong log of tir pass VerifyMemory (#8445)	4
fix default out type of depthwise conv2d (#1134)	0
[Vulkan] Corrected typo in Vulkan capability error messages. (#8187)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Fix typo (#8197)	2
[Arith] Simplify cast (#7045)	5
Fix autotuning, broken in #7337 (#7566)* Fix autotuning, broken in #7337* retrigger CI, because I don't understand how it passed	4
Update tvm_runtime.h (#4278)fix the problem that android_rpc compilation failed	0
fix multiple transfer issue in loaduop (#4442)	0
[PASS] Add place device (#18)	1
[Torch] Clean up usage of try ... infer_value() ... except (#6504)* clean up infer value usage* try silence pylint* remove unused variable* make on_failuare optional* make on_success optional TrueCo-authored-by: masa <masa@pop-os.localdomain>	1
[MXNET]MaxPool3d and AvgPool3d Ops support added (#5614)	1
Add user-configurable backtrace limit (#10025)A spin off of #9872, this adds an env variable `TVM_BACKTRACE_LIMIT` which can be set to an integer to limit the frames printed out on errors. This can make it easier to run interactive TVM scripts with errors since the stack traces are often long (70+ frames).```bashexport TVM_BACKTRACE_LIMIT=5python some_code_with_an_error.py```cc @tkonoligeCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Vulkan] Implement sync for SyncThread("warp") (#8320)- Add sync if a SyncThread("warp") node is present.  The sync is done  at spv::ScopeSubgroup if supported (Vulkan 1.1+), and at  spv::ScopeWorkgroup otherwise.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Don't remove() TempDirectory in __del__ after atexit hook runs. (#5414)* Use atexit to remove TempDirectory before interpreter shutdown. * Can't rely on complex functions from __del__ anyway. * Fixes warning message on my box:       Exception ignored in: <function TempDirectory.__del__ at 0x12be10680>       Traceback (most recent call last):        File ".../tvm/python/tvm/contrib/util.py", line 55, in __del__        File ".../tvm/python/tvm/contrib/util.py", line 51, in remove        File "/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/shutil.py", line 509, in rmtree        AttributeError: 'NoneType' object has no attribute 'path'	0
[microNPU] re-enable network tests (#10565)This commit re-enables tests thathad failed due to a interrupted downloadingof the testing data.	5
[NODE][Serialization]fix serialization precision loss in float (#4503)* fix serialization precision loss in floatWhen we want to serialize a tvm.tensor object(like pickle)， we will get a precision loss cause by std::to_string()。For example, a2.value will be 0.0 while a.value=0.00000001 in the following:     import tvm    import pickle    a = tvm.const(0.00000001, 'float32')    a2 = pickle.loads(pickle.dumps(a))* remove line end spaces	4
[TOPI] Fix x86 conv2d template when tuning with unpacked layout (#5938)* fix x86 conv2d and conv2d_transpose template* address comments	1
[docker] Fall back to tlcpackstaging in bash.sh (#11976)This uses #11775 to make local builds work if they're run in the meantime before CI tags over a new image to tlcpackCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[VTA][Hotfix] Avoiding error when environment variable is not set (#3497)* avoid error when env var is not set* extra content	1
Add remaining targets to ci.py (#10425)* Add remaining targets to ci.pyThis adds build/test commands for all of the CI environments except ARM (that one will come in a follow up). Most of the invocations are similar and the scripts come straight from the Jenkinsfile. This improves the current situation by making it much easier to get CI environments locally. This also wraps pytest invocations in CI so that failures are parsed and a repro command is reported at the end of the failing CI run step alongside other logs to increase the visibility into this tool.This isn't perfect yet so some work (such as ARM support and certain tests that require pytest flags like in `tests/scripts/task_python_microtvm.sh`) is left for a follow up.* remove reporting changes* Clean up common functionality* Address comments* CommentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Dynamic Tile Op (#5983)* first working dynamic tile passes first test* add dyn tile to dynamic_to_static* fix cpplintt* respond to review comments. Thanks @siju-samuel* make dynamic tile compatible with numpy API	1
[AutoScheduler] Fix alignment of note (#6181)* Fix alignment of note* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	0
[Community] @Lunderberg -> Reviewer (#8834)	3
[TOPI] deformable_conv2d in NHWC (#6999)* [TOPI] deformable_conv2d in NHWC* Update python/tvm/topi/generic/nn.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/topi/testing/deformable_conv2d_python.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* style* fix* styleCo-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[AutoScheduler] Use a smaller iteration number for GA to acclerate the search (#6994)* [AutoScheduler] Use a smaller GA iteration number* fix* fix* add a new argument to control the search policy from task scheduler	1
[Frontend, pytorch] Vc/pytorch lstm (#8447)* lstm layer conversion to relay from pytorch model (TorchScript) was supported* bidirectional LSTM layer was supported for pytorch API* lstm tests were implemented. fixes in pytorch lstm* fix pytorch bidirectional lstm. update test comment* black format and some small fixes* LSTM with projection was supported for pytorch frontend. test was updated by new combination of LSTM types* lint fixes* add bias switcher for LSTM types test. fix LSTM implementation in pytorch frontend for case without biases. exception in the test for conversion LSTM with projection from pytorch to ONNX* transfer test_lstms to pytest format* onnx model saving was implemented through io.BytesIO. creating/removing tmp dir was removed. remove unneccessary comments* gpu target was added to the testCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
[RUNTIME] Introduce RValue reference(move) support to TypedPackedFunc (#5271)* [RUNTIME] Introduce RValue reference(move) support to TypedPackedFuncThis PR introduces RValue reference support the PackedFunc calling convention to address the above issue.Specifically, when an argument is a r-value reference, we will use a assign a different type code(`kObjectRValueRefArg`),and pass `Object**`  (the address to the Object pointer) instead through the values array.The callee can choose to move out this Object pointer and set the original Object pointer from the caller side to be nullptr.We also add an experimental move support to the python side(marked as _move so to indicate the dev nature).This enhancement will enable copy on write optimizations through out the TVM stack.* Address review comments* fix compilation	0
fix js test load module example (#3556)	3
[skip ci] Use Stanford Cars mirror to fix CI docs build (#11812)	2
[DOC] Add tip on mitigation for symbol conflict with PyTorch (#9433)	5
[TIR] Fix printing enum in TransformLayout::AsPython (#11211)After this PR, `as_python` can handle `transform_layout` correctly. The result will be like```pythonsch.transform_layout(..., buffer_index_type="read", ...)```Previously `buffer_index_type` was printed unquoted.	0
Remove unintentional pytest dependency. Fix #6398 (#6399)	0
[LLVM] Treat scalars as single-lane vectors in CreateVecConcat (#9264)LLVM differentiates between `<1 x ty>` and `ty`, while TVM does not.Make sure that a bunch of TVM scalars can be concatenated into avector when generating LLVM IR.	1
[BugFix][TensorIR] Non-positive constant input factors for `split` (#9805)* Update docs of GetProducers/GetConsumers* Fix split for non-positive factors	0
[microNPU] Refactor section name test to remove relay_ir_builder (#9658)Since we're in the process of removing relay_ir_builder, I'm refactoring the test_ethosu_section_name test case to remove it from there.	4
Arm(R) Cortex(R)-M55 CPU and Arm(R) Ethos(TM)-U55 NPU Demo App (#8922)This commit adds a demo application that uses TVM to run a model on bare metal Arm® Cortex®-M55 CPU and Arm® Ethos™-U55 NPU. The demo demonstrates running Mobilenet_v1 TFLite model on the Fixed Virtual Platform (FVP) of the Arm(R) Corstone(TM)-300 reference package.	0
[Format] Convert all Python code w/o CI (#6448)* Add black setup* Tweak pyproject.toml* Fix syntax issues* Fix* Tweak* Black all Python code	0
[RELAY] Basic block normal form (#6152)* initial commit* refactor utils* add util* revert anf test* update test* fix logging* fix scope bug* complete tests* remove logging* revert refactoring* add one more test case* fix missing var binding* fix test* fix lint* fix lint* fix clang-format* fix lint* fix lint* commit missing code* add analysis api* fix lint* fix lint* lint* add test for func* address CR* fix typo* fix return type* fix lint* refactor classes* fix lint* remove prints* address commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>	1
Fix some typos (#8101)* fix bugs in the auto scheduler record:* reformat the code* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code* fix some typos* fix some typos* fix some typos* fix the port number typo	2
[Relay][Frontend][Bugfix] Fix bug in converting slice_axis when axis is negative (#2739)* bug fix* trigger ci	0
[BYOC][TVMC] bugfix: disabled_pass -> disable_pass (#7850)Change-Id: I22d24a2e219103485a6a1519ce6256a104103ebb	4
[Bugfix] [VTA] VTA DRAM Have A Logic Issue May Cause GEMM Output Wrong. (#3278)* [Bugfix] [VTA] VTA DRAM Have A Logic Issue May Cause GEMM Output Wrong.Symptom:after change “LOG_BLOCK_IN” and “LOG_BLOCK_OUT” from vta_config.jsoninto 7, run vta "Simple Matrix Multiply" in "simulator", the vtacalculate result for GEMM is wrong.Sometime VTA crash with error “Check failed: phy_addr != 0 (0 vs. 0) :trying to get address that is nullptr”Analysis:Simulator hardcode kPageSize into 1<<12 and physical address calculatebased on this size, when doing “insn->dram_base” calculation , becauseGetElemBytes(dst_memory_type) larger than page size, different physcialaddress may get same dram_base, than caused logic issue and finallytrigger GEMM out put is wrong.Solution:add logic to check if PAGE SIZE larger then "GetElemBytes" return value.* address review comments.	1
[TUTORIAL][ANSOR] Using the template-free auto-scheduler on CPU (#6488)* add tutorial* add tutorial* update* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address comments* fix bugs* add the exmple for resuming the search* fix lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>	0
Align the naming rule for OpAttributeUnImplemented (#3695)	5
[CI][Caffe Frontend] add caffe environment (#6023)* [CI][Caffe Frontend] add caffe environment* [CI][Caffe Frontend] change the caffe deps into BVLC distribution.* [CI][Caffe Fronted] simplify configuration while installing tzdata for precompiled caffe.* [CI][Caffe Frontend] add more information about tzdata.* [CI][CaffeFrontend]remove the ci for gpu env and change to pip3 envCo-authored-by: fernchen <zifeng.cf@alibaba-inc.com>	4
Move FlattenAtrousConv before AlterOpLayout in the default opt pipeline. (#11706)Co-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>Co-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>	4
fix apt install command text in check_arm_qemu() (#11153)	0
[Ansor][AutoTVM v2.0] Phase 2: Update heavy operations with parallel_for (#6348)* Update auto_scheduler with parallel_for* Update* Update* Update* Update inferbound	5
add kDynamicStorageID to plan mem (#127)	1
[relay][refactor] Cache Op::Get in passes to reduce lookup overhead (#4594)* Refactor to use IsOp utility* retrigger CI	1
[Relay][Keras] Permute, Softmax support (#3618)	1
[TVMScript] support float inf, -inf and nan in TVMScript parser and printer (#12618)* support float inf, -inf and nan in TVMScript parser and printer* address comment and fix lint* use type_extensions.Literal* address comments* fix win build* remove template	4
Auto TensorCore CodeGen (#4234)* Add Auto TensorCore TensorCore Unit Test* Rebase to tvm master branch & Add auto tensor core* Code Refine* Add tensor core switch by pragma* Add pragma in tensor core example code* Get real tile size to replace hard coded 16* support more than 2 dimensions (e.g. batchmatmul) for buffer bind scope* support batch matmul* Move cuda env check to tensor_core.cc* Coderefine for tensor_core.cc* Refine comments* Some refinements of code and comment* Update TensorCore UT to pass the CPU test* remove redundant code* matmul's storage align for different layout* Add support for differenct position of type cast* Add formal tutorial for auto tensorcore codegen* move tensorcore check up to tutorial code* code and doc refine* comment out tune_and_evaluate in tutorial* fix cpplint error	0
[NNPack] Support for threadpool (#334)* [NNPack] Support for threadpool* fix lint* fix lint* Use static class function	1
[VM] check DLManagedTensor for conditions to construct NDArray (#11504)* check DLManagedTensor for contiguous and alignment to construct correct NDArray* correction from the reviewer* update error description for incontiguous DLTensors* small updateCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	5
[TIR] Add unroll_loop_with_partition_hint_no_interval attr in LoopPartitionConfig     to unroll loop (#12631)[TIR] Add unroll_loop_with_partition_hint_no_interval attr in LoopPartitionConfigto unroll loop	5
[RPC] Expose module handle (#459)* [RPC] Expose module handle* not include handle	0
Solve custom model of prelu (#4326)	5
[TVMScript][Fix] Add type hints for more uncovered cases (#9505)* add support for prevously uncovered cases* remove PrimExpr import* add exp test and mypy ignore* disable ling too long* resolve long line* nit* add dtype to unary ops	1
[VTA] Speedup TSIM by Multi-threading (#4491)This PR tries to increase TSIM performance by introducing multi-threading support.	1
Add Binary Intrinsic ops to TIR Ops in C++ (#5900)* Add Binary Intrinsic ops to TIR Ops in C++* clang-format	1
Add support for aten::__lshift__, aten::__rshift__, aten::__ior__ (#10631)* add support for aten::__lshift__, aten::__rshift__, aten::__ior__* lint* undo lint* lintCo-authored-by: Masahiro Masuda <masahi129@gmail.com>	1
[RPC] Fix tracker fault handling (#1109)	0
[Relay] Invoke tvm::build from relay compile_engine and interpreter (#4723)	5
[ONNX] Add MatMulInteger16 contrib op (#9186)* [ONNX] Add MatMulInteger16 contrib op* Fix formatting errors* Remove a code comment and do not set default value of nd* Move flatten_to_nd function outside matmul to be used across multiple functions* Add function docstring and describe the tests* Use max/min value of int16 as high/low while generating input vectors* Converge MatMul and MatMulInteger16 ops into a single op using output dtype* Fix indentation issues* Formatting changes* Fix CUDA batchmatmul strategy to allow mixed precision* Add test_matmulinteger to unsupported_onnx_tests	3
Add some missing operators (#1524)	1
Fix a word typo and add spaces. (#8278)Co-authored-by: kueitang <kueitang@qti.qualcomm.com>	1
[µTVM] Raise a better error when project_dir does not exist (#7165)	0
[microTVM] Update nrfjprog on reference virtual machine (#7723)* update nrfjprog and integration test* merge* Revert "merge"This reverts commit 58d5d9187448e6580b6b780821eb2ea42ec34e8e.* fix comments* fix clang* revert format* new line* format	1
[CUDA][THRUST] Enforce -libs=thrust to allow thrust offload (#7468)* add contrib/thrust.py* update cuda strategy* remove is_thrust_available, update nms, scan, sort and tests* remove unused import* trigger CI* update* add note on how to enable thrust in ssd tutorial* add warning* Revert "update"This reverts commit c1629b39e5277003a82cbf31fe4da493537bc05f.Co-authored-by: masa <masa@pop-os.localdomain>	4
[skip ci][ci] Mark more ethosu tests with xfail (#12560)See #12511 for context. Since more parameterizations are popping up asfailed, this disables whole tests rather than specific combinations ofparameters.	2
[VTA] Throw exception on mis-formatted files and avoid overwrite Scala code (#4555)	2
[REFACTOR] Get rid of packed_func_ext. (#4735)Move the conversion extensions to the specific class definitionsso that we longer need to include packed_func_ext.	5
add default value for leaky relu alpha (#7259)	1
PackedFunction to return params from the .so module, show warning when no params are set (#9811)* PackedFunction to return params from the .so module, show warning when no params are set* Linter checkup* Autoset of params, tests for get_graph_params* Linter checkup* Check that inputs were set before run* Return the original implementation if Run* Fix RPC behavior	0
[crt] fix heap corruption from bad allocation (#7735)The type of runtime->storage_pool was changed at some point from TVMNDArray to TVMGraphRuntimeStorageEntry. This change was not reflected in the call to the allocation for its buffer. If this unclaimed space is allocated to something else, data corruption will happen.	5
[TEST][TENSORFLOW] clean up code (#3342)	4
Improve Docker cache reuse by pointing to the current version of the image, (#5466)on top of another image to be used as reference.	1
[KERAS]Upsample3d & ZeroPadding3d op (#5125)* [KERAS]upsampling3d and zeropadding3d op* [KERAS]upsampling3d and zeropadding3d test case* Review comments updated	5
[NNVM][TENSORFLOW] bugfix. (#2444)	0
[TOPI] add take (#1158)	1
[Fix][microTVM] QEMU RPC issue (#8021)* add test* fix test* add parameter to test* cleanup* format* address comments* address comments* direct read/write from/to ring buffer* merge fix* add comment	1
[BYOC] Support Tuple Output in C/DNNL Codegen (#5701)* Support tuple output runtime* fix unit test	3
[LLVM] Update creation of llvm::DebugLoc, remove TVM_LLVM_VERSION < 70 (#12069)* [LLVM] Update creation of llvm::DebugLoc, remove TVM_LLVM_VERSION < 70* Properly deal with "handle" type* Emit correct subroutine flags* Fix llvm testcase to account for presence of debug metadata	5
[MetaSchedule][Test] Add unittests for CBR (#12252)	3
Fix mixed precision output type to original type (#11142)	0
[PASS] Add gradient pass (#28)	4
[NNVM][TENSORFLOW] LSTM operator and PTB word prediction frontend (#1389)	1
Upsampling op support (#298)* add nnvm upsampling symbol* add upsampling mxnet frontend* add doc for upsampling op* cleanup upsampling test* minor fix* use schedule_injective for upsampling* upgrade tvm	1
get_top_results works on a copy of output (#7327)	1
[CI] Update GPU image to add DNNL (#11786)Requested by https://github.com/apache/tvm/issues/11774Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/260/	2
[docs] Update installation instructions (#12269)This cleans up the installation instructions and adds some info about`apache-tvm` on PyPi.	5
[UNROLL] New unroll option (#647)	1
[Torch] Fix dtype handling for modules with integer parameters (#6311)* return the correct type for GetAttr node* keep _get_pytorch_value_type intact* add test and handle quantized param	2
[VERILOG] Generalize Buffer and Tests (#76)* adding tvm_buffer and fifo testbench* minor edits* line buffer test bench* adding double buffer tests for the tvm_buffer* making variable consistent with  python style	1
[Relay] Fix VM compiler for while loop with free vars  (#4889)* add additional switch to handle nested call node* Fix VM compiler for while loop with free var	0
[TVMScript] Represent ramp as index slice (#11308)* support represent ramp as index slice in tvmscript* fix testcase's comment, check slice lanes instead of extent	3
[VTA][Chisel] run all unittests by default (#3766)* [VTA][Chisel] run all unittests by default* better naming* add generated unittest folder to clean rule	4
[PASS] Memory barrier detection, storage access lower. (#317)	4
Fix x86 depthwise conv2d alter_op_layout (#3264)* Fix x86 depthwise conv2d alter_op_layout* Small fix* Add test case* Fix test* Assert kernel layout* Minor fix* Add get_shape function* Minor change	4
[VTA] YoloV3 Support (#4887)* [VTA] YoloV3 SupportIssue:YoloV3 use some operator and logic that not get good support byexisting vta logic, like nn.pad, upsample, and 255 output channel.Solution:add related logic to let darknet YoloV3 can running on VTA* Fix small(0, or 1 heigh/width) detect frame issue.* add yolov3-tiny turtorial* add os import* address review comments.* rename tutorial file with a short name.* rename deploy_vision_on_vta.py into deploy_classification.py.* address review comment, fix plint eror in deploy_detection.py	0
[CI] Update ci docker to add autodocsumm (#4903)	2
[Target][Codegen] Use target class in all codegens (#6347)* [Target][Codegen] Make all code generator use Target class instead of target string* Remove dep to TargetNode::str() in LLVM module* Allow  for llvm nvptx codegen* ...* Address comments from Cody* Rename UpdateTargetConfig => UpdateTargetConfigKeyValueEntry	5
[Bug] Fix x86 dense schedule extern ops (#8420)* [Bug] Fix x86 dense schedule extern ops* more* lint	0
[VTA] Infinite recursive device_api.ext_dev call fix. (#3843)Issuewhen try vta on fpga board, would see a Infinite recursivedevice_api.ext_dev issue that cause stack overflow and vtafailed.Analysis:device_api.ext_dev function in rpc_server.py is use to loadvta library, once vta library get load, device_api.ext_dev wouldget replaced with vta function by vta library, vta device_api.ccdid such work, but because a logic issue in VTA.cmake, the said filenot get compiled, then vta would keep failing on rpc_server.py.Solution:fix the logic issue in VTA.cmake.	1
[FQ2I] Add attrs to adaptive_avg_pool1d (#12290)* Add attrs to FQ2I adaptive_avg_pool1d* Fix failing test and add param	2
Allow install-dev to include all necessary header files (#338)	2
[Hexagon] Add HexagonThreadManager (#11653)* Adding initial threadmanager class* Fixed compile errors* Moving constant defines inside class* Updating qurt includes* use default scope for hexagon buffers* Updating buffer allocations* Fixed bug where array of pointers treated as array of structs* - Updated HexgonDeviceAPI to use HexagonThreadManager- Updated HexagonThreadManager interface to use TVMStreams- Added second `Dispatch` interfce in thread manager to use PackedFuncs- Updated thread manager to use vector for dynamic semaphore allocation- Added "#if defined(__hexagon__)" in several places to prevent compilation errors* Bug fixes + interface addition + basic thread tests - Fixed GetStreams not returning the streams properly - Added missing semaphore cleanup to prevent qurt kernel resource leakage - new interface functions:   - Start() : now all worker threads are blocked on initialization until ThreadManager->Start() is called   - WaitOnThreads() : blocking call which waits until all worker thread queues are empty - added extra debug logging - Two new basic thread tests working* Adding initial ThreadManager tests* HexagonThreadManager tests and refactor* remove stack / pipe size member vars* init pointers in the header file* move all mem allocs to SpawnThreads* start_semaphore as object instead of pointer* fix bug with WaitOnThreads deadlock + Wait/Signal off by one error* add / refactor Signal / Wait tests* add SyncFromTo test cases* add Dispatch test cases* add pipe fill and overflow cases* Updating dispatch to return bool and fix pipe overflow problem* change around min / max values for stack / pipe* integrate pipe fill / overflow tests back into HTM test suite* use HexagonBuffer* assert if stack / pipe sizes fall below min* Changed semaphore vector to store pointers, not structs (fixes vector capacity adjustment invaliding in-use addresses).* add producer consumer, thread order test cases* change to unordered_map for semaphores and remove PreallocateSyncs* tests running on device* code cleanup for compile warnings* remove #if defined(__hexagon__) guards* copyright, format, lint* add hexagon buffer map class* remove redundant thread manager tests* revert Hex Dev API changes for threading* add comments; remove untested code to dispatch / wrap a packed func* pass pipe address and not HTM pointer to thread context* rename to HexagonBufferManager* cleanup ahead of PR* use DLOG(INFO)* refactor GetStreamHandles to return a vector by value* adjust HexagonBufferManager methods; use thread_manager file names* style guidelines and debug prints* reinterpret cast for TVMStreamHandle* end member variables with underscoreCo-authored-by: Joseph McMahan <jmcmahan@octoml.ai>	5
[ONNX] Fix a bug with reshape imports when an initialized target shape is used more than once (#7109)* Fix a bug with reshape imports when an initialized target shape is used more than once* run autoformat	1
[BYOC][ACL] Enable remote device via environment variables (#6279)* [BYOC][ACL] Enable remote device via environment variablesImproves the ACL remote testing infrastructure by allowing a remote device to be specified via environment variables. This means external scripts can be used to enable the runtime tests. By default an RPC server will not be used and the runtime tests will be skipped.Change-Id: I8fc0b88106683ac6f1cbff44c8954726325cda21* Use json file as configuration for testsChange-Id: Iadce931d91056ed3a2d57a49f14af1ce771ae14b* Do not load the test config during class creationChange-Id: If718b5d163e399711111830f878db325db9c5f84* Add check for existence of fileChange-Id: I2568bca7f4c3ad22ee8f9d065a9486ee3114f35c	4
Tweak debugger result (#4426)	0
[RUNTIME][RPC] Update RPC runtime to allow remote module as arg (#4462)	1
Fix compilation when Arm FP16 extensions are enabled (#7386)Fixes incorrect number of template parameters in call to sort()Signed-off-by: Matthew Bentham <matthew.bentham@arm.com>	2
[TOPI] Minor fix in the LSTM recipe (#2131)	0
[BACKEND] Allow nvptx to pass ll ir to CUDAModule (#404)	4
[TIR][Arith] Add more strict checking in imm construction and folding. (#12515)* Add more strict check in tir imm construction and folding.* fix bool-compare compile error* fix some illegal imm construction in testcases* do not test i64 overflow behaviour because it is not consistent on cython and ctypes* fix float32 testcase* auto-inferred dtype should be int64 when value exceeds int32 range* add floatimm range check for fp16 and fp32* add more folding testcases and fix store fp32 folding result to double* fix i386 fp16 cases	0
save (#3033)savesavesaveupstreamlintremove bad changesfix buildsavesaveplease the ci godUpdate src/relay/pass/partial_eval.ccCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>savefix testci is ANGRYfix rebase problemfix rebaseadd testsavesavecomment	3
Initial support for enabling MyPy in CI  (#8302)	0
[TVMScript] fix print target's host (#10598)A followup fix for https://github.com/apache/tvm/pull/9594	0
[BUG_FIX] Fixes #6608: CHECK(data != nullptr) causes type checking to fail (#6610)	0
[MetaSchedule][Testing] Add unittests for C1D search space (#12036)	3
[CI] Fix the CI after image update. (#8164)- ci-gpu needs env var update to support all GPUs.- Update git-black to fix the recent encoding error due to latest black dep.	3
[REFACTOR][BOYC] Non recursive partitioning (#5493)* non recursive partitioning* refactor maps* rebase upstream* refactor shared output* address commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[CODEGEN] add callback post proc for opencl (#692)	1
[µTVM] Add platform timer and RPCTimeEvaluator to enable AutoTVM (#6964)* Add platform timer to microTVM.* Address liangfu comments* cppformat* clang-formatCo-authored-by: Liangfu Chen <liangfu@apache.org>	1
Import hybrid module in __init__.py (#1323)	5
[BYOC][TRT] Support batch norm for all ranks <=5, and all axes (#7026)* [TRT] Support batch norm for all ranks <=5, and all axis* Add return false* Fix TRT < 6 build	0
[Relay/TOPI][TFLite] Implemented MATRIX_SET_DIAG Operator for Relay/TOPI and TFLite Frontend. (#6303)* Corrected docstring error.* Minor changes.* Changed MATRIX_SET_DIAG registration from broadcast to injective.	1
[MODULE/RUNTIME] Remove Precompile, simplify module (#174)	4
[Relay, Op] Add conv2d generic layout op strategy when meta schedule is enabled (#12104)	0
QnnBinaryLayout bugfix + unit test (#6513)An attempt to fix an issue which appeared when ConverLayout passruns on quantized binary operations like qnn.add:def before():    x = relay.var("x", shape=(2, 2))    y = relay.var("y", shape=(1, 2))    return relay.Function(        [x, y],        relay.qnn.op.add(            x,            y,            lhs_scale=relay.const(0.0156863, "float32"),            lhs_zero_point=relay.const(127, "int32"),            rhs_scale=relay.const(0.0117647, "float32"),            rhs_zero_point=relay.const(85, "int32"),            output_scale=relay.const(0.0235294, "float32"),            output_zero_point=relay.const(128, "int32"),        ),    )The issue manifested itself as  [bt] (2) ./src/tvm/build/libtvm.so(tvm::relay::qnn::QnnBinaryBroadcastLayout(tvm::Attrs const&, tvm::runtime::Array<tvm::tir::Layout, void> const&, tvm::runtime::Array<tvm::tir::Layout, void> const&, tvm::runtime::Array<tvm::Type, void> const&)+0xa9) [0x7fadc080d949]  [bt] (1) ./src/tvm/build/libtvm.so(tvm::runtime::Array<tvm::tir::Layout, void>::operator[](long) const+0xb6) [0x7fadc0382996]  [bt] (0) ./src/tvm/build/libtvm.so(+0xb50c12) [0x7fadc037cc12]  File "/workspace/include/tvm/runtime/container.h", line 681IndexError: Check failed: 0 <= i && i < p->size_: indexing 1 on an array of size 1	0
add c backend to CreateTarget (#2256)	1
[TIR] Moved PrimExpr operator overload from op.h to expr.h (#11973)* [TIR] Moved PrimExpr operator overload from op.h to expr.hIf a compilation unit includes `<tvm/ir/expr.h>`, but does not include`<tvm/tir/op.h>`, the operator overloads for `ObjectRef` are declared,but the operator overloads for `PrimExpr` are not.  In this case, anyuse of `expr_a == expr_b` would use `ObjectRef`'s implementation andcompare reference equality of the two expressions, rather thanreturning a `PrimExpr` that represents the comparison.  By having theoperator overloads in the `<tvm/ir/expr.h>` header file, directlyadjacent to the `PrimExpr` declaration, the correct overload must beavailable whenever the `PrimExpr` can be used.Even though this would only impact `operator==`, `operator!=`, and`operator<`, the three operators defined for `ObjectRef`, this PRmoves all operator overloads to `expr.h` for consistency.The named version of the operators (e.g. `tvm::add`) do not haveoverloaded variants, and so they are intentionally kept in`<tvm/tir/op.h>`.* Explicitly convert TVMRetValue to bool in target.ccNeeded to avoid ambiguity between `TVMRetValue -> bool` conversion and`TVMRetValue -> int -> PrimExpr` conversion.* Used vector/unordered_set to track BufferInfoExtractor::call_order_Use of `std::set<Call>` had ambiguity between `operator<` by`PrimExpr` or by `ObjectRef`.The comment for `call_order_` implied that the previous usage of`std::set<Call>` was intended to have a de-duplicated list in theorder of occurrence.  However, the `std::set` was ordered by`ObjectRef::operator<`, not by insertion order.  Switching to using a`vector` for ordering and `unordered_set` for de-duplication resolvesthis issue, and also removes the use of `operator<`.* Remove C-style cast to fix lint error	0
[RPC] Call sync in remote cpu to gpu copies (#5512)	5
[relay][vm] Reuse allocated device memory (#4170)	1
Add Reference System to tvm.ci_qemu (#9853)This is added for upstreaming the CMSIS-NN demo from TVMCon which usesboth Zephyr and the reference system so I need an image with both to runit in CI.	1
Fixed onnx test failures when run on a cpu backend (#3764)* Fixed onnx test failures when run on a cpu backend* Updated check_torch_conversion function to include output comparison	1
[Relay, Topi, TF Frontend] Isfinite operator (#4981)* isfinite doc update* isfinit expr* isfinit expr* isfinite schedule reg* isfinite python binding* isfinite python binding* relay register isfinite* isfinite type relation* intrin isfinite* topi isfinite* testcase topi isfinite* tf frontend isfinite* tf frontend isfinite testcase* test case relay isfinite* small fixes* test forward tf isfinite* test cases injective for cuda* remove float16 test case* add support for isinf* remove unwanted import* fix conflict	5
[NVPTX] libdevice support, enable NVPTX backend in topi tests (#1365)	3
[SCHEDULE] Enhance cache_write to enable layout change. (#432)* [SCHEDULE] Enahance cache_write to enable layout change.* more tests	3
[Relay][Frontend] Add MXNet test example for relay (#2316)* Add MXNet test example for relay* Fix a bug in BiasAddSimplifier	1
[RELAY][DOCS] Port from_mxnet tutorial to relay (#2608)* check in* update build and run	1
[PASS] StorageRewrite Fold Inplace op storage when possible (#759)* [PASS] StorageRewrite Fold Inplace op storage when possible* update comment to fix typos	2
[microTVM] Replace static fixtures with parameterization (#12530)* Replace microTVM static fixtures with parameterization* [microTVM] Only perform parameterization when fixture is present* Reformat with black* Fix Cortex-M tests* Add docstring to pytest_generate_tests* Remove trailing space from docstring	2
[TVMC] 'tvmc tune' --rpc-tracker and --rpc-tracker fail due to argparse misconfiguration (#6822)Fix an error with `tvmc tune`, that causes --rpc-tracker and --rpc-key to be identified as a list of strings, rather than the expected string type.Removing the unnecessary nargs solves the issues.This is a follow-up of https://github.com/apache/incubator-tvm/pull/6762	0
Update dmlc-core, fix default ctors of NodeEntry  (#3017)	1
[AOT] Remove lookup parameter function in AOT (#7988)* AOT] Remove lookup parameter function in AOTThis PR aims at removing the function call to extract the parameterswithin the AOT main function by introducing a tir::lookup_param builtin.This has different benefits:- In AOT we now only use the v_handle field- We save cycles by not calling an intermediate function to extractlocal parameters- We reduce code size, since we don't need to pack a call to extractparameters and we don't need to produce the lookup_param functionanymore within the compilation unitChange-Id: I36c2f0724a79606424a4374f4f5cd669bb2a8a55* addressing commentsChange-Id: I83ba0189f559d310b5a80fe0bcc4d601b490d21a* retrigger CIChange-Id: I84ab4a526d1284ded41fe95636e94c15412f6b28	4
[IR] support general type annotation. (#1480)	1
Conda packages with cuda support (#2577)	1
[MetaSchedule] Enhance parsing in JSONDatabase (#11791)Originally, when failed with `std::stoi` and `std::stod`, the parserdisruptly stops and throw an incomprehensible error message, forexample, "stoi". This PR improves the user experience by detailing whichstring causes the parsing issue.A minor fix: out-of-range integers in parsing will now automatically fallback to floating point numbers.	0
[CI] Update ci-lint to v0.60 (#4850)	5
Add segment sum Op to relay and 7 corresponding TF Ops , fix scatter_add dynamic bug  (#7562)* Add segment sum Op* Remove unnecessary* Documentation* Black* Add GPU* Uncomment* Add documentation* Add dynamic tests* Add TF Op* Add Sparse Segment Sum* Add test coverage* PR Comments* Int64 tests* Add SparseSegmentSqrtN* Add SparseSegmentSqrtNOp* Deduplicate code* Add SparseSegmentMean* Parametrize Tests* Remove* Modularize* Black* Modularize Code* Pylint* PR Comments* Add scatter add tests* Remove TestCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>	3
[DYNAMIC] Add Dynamic reshape to a dynamic namespace and add DynamicToStatic Pass (#5826)* Dynamic reshape passing tests* Add Dynamic to Static Pass* rename test file to prevent pytest conflicts* fix clang build* add nested dynamic shape test* remove cuda tests until VM supports dynamic shapes* rename namespace from dynamic to dyn* fix lint* fix lint again* Remove incorrect doc strings* remove dynamic behavior from standard reshape* fix some tests* merge dynamic and static interfaces in python* fix missing import* missed a reference to relay.dyn.reshape* fix vta example* respond to review comments	0
[COMMUNITY] Junru's and Wuwei's PGP key for ASF release (#9488)* add PGP into KEYS* add wuwei's pgp as well	1
[MetaSchedule][Test] Add unittests for SFM (#12251)	3
Convert AOT to TECompiler (#8697)* Convert AOT to TECompilerThis removes the dependency on "compile_engine.h" from aot_executor_codegen.cc. This required a few changes to how AOT was operating:* AOT run_model is now based on the post lowering main_module* AOTOnDemandAllocator is ran twice to ensure SIDs are updated post-lowering* Moved to using tec::UpdateFunctionMetadataTests are passing, but would appreciate other validation :smile_cat:* Clarify reasoning behind replanning memory later* Use main_func_info rather than bespoke logic in AOTThis moves from using the bespoke AOT UpdateMainWorkspaceSize to theLoweredModule main_func_info property to unify with Graph executorcodegen.	5
[RELAY] Avoid unnecessarily reconstructing FunctionNode. (#3047)	1
support round-trip for T.Ptr in tvmscript (#11179)	1
Fixed issue #3069 by checking op tag (#3070)* Fixed issue #3069 by adding in_channels* Registerd group_conv2d_nchw as topi compute* Improved by checking tag value* Removed group_conv2d_nchw topi registration* Added test for relay group_conv2d_nchw* Added assertions to forbid small group size* Removed hard-coded oc_block_factor* Added explanatory comments to group_conv2d_nchw_cuda* Updated group_conv2d_nchw_cuda scheduleRemoved 'direct' CUDA tests* Reverted an accidental change in a conv2d test* Fixed indentation problems* Fixed a mis-commented line* Reverted change in group_conv2d_nchw tag* Removed commented int8 group_conv2d test* Fixed group size assertions in group_conv2d_nchw_cuda	3
Fix typos in comments (#7862)Fix two typos in comments.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	2
[ONNX]onnx gather bug fix (#1543)	0
[Bugfix] Shape inference of weight for grouped `nn.conv3d` (#11681)* Fix `nn.conv3d` weight shape inference.* Add test for conv3d type inference with groups.	5
remove exception handling of autotvm xgboost extract functions (#10948)	1
[µTVM] Avoid listing links when probing serial ports (#7265)SerialTransport.open() probes automatically the device name based upon agrep regex if a device name is not provided. The code expects to find onlya single device. Currently when it probes for the available serial ports itincludes in the list the device names that are also symbolic links.Since _find_openocd_serial_port() always returns a serial number for agiven serial port (not the device name path) the available device namesare always probed when the openocd flash runner is used.It's not uncommon that device drivers create symbolic links for certainkinds of serial devices, specially those that provide a serial port plusan additional endpoint to program the device attached, like a ST-Linkinterface, etc.As a consequence the current code fails to select the correct device namewhen symbolic links exist and the openocd flash runner is used.That commit changes the probe behavior to avoid listing symbolic links whenprobing the device name for the target serial port.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[Bugfix][AutoScheduler] Correctly resume status (#7614)	0
[Relay]Port eliminate_common_subexpr to non-recursive form (#6134)Co-authored-by: Zheng Jiang <zhejiang@amazon.com>	5
[BugFix][UMA] Fix order issue in uma_lower  (#12447)There was a flaw in uma_lower (see issue #12410) that lead in some case to a different argument ordering of the cached_func and the Relay function. This results in an incorrect lowering of the primfunc and eventually a wrong result of a run-time error, in some cases.This commit adds code to correct the described misbehavior and a unit test case to check this end-to-end functionality with a TFLITE model.	1
Make cc bot skip errors (#9988)* [skip ci] Make cc bot skip errorsThis adds some better logging and ignores errors (this job shouldn't ever show up as a PR failure) so we can diagnose things like https://github.com/apache/tvm/runs/4873810315?check_suite_focus=true* Submit cc'ed reviewers one at a timeCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Op] Add operators full and full_like (#1845)	1
[ci] Skip flaky CMSISNN test (#10749)* [ci] Skip flaky CMSISNN testSee #10748* Properly disable testCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[M3c][MetaScheduler] Add More Measure Callbacks. (#9780)* Add measure callbacks.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix comments.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[AMP] Disallow fp16 conversion for arange op (#8644)* [AMP] Do not allow fp16 cast on arange inputs* add test* Add comment explaining the issue with fp16 "end"	0
[BYOC] [ACL] ACL Runtime padding workaround (#6724)This workaround prevents execution of operations via ACL runtimein case if arguments or output tensor require memory padding.Workaround is applicable to all ACL versions prior forecoming ACL 20.11(which will not use data padding).	1
Set default value of p in LpPool as 2 (#8866)* Set default value of p in LpPool as 2* Update test_forward.pyFix bug in test.* Update test_forward.pyupdate with correct shape.* Update onnx.py* Update python/tvm/relay/frontend/onnx.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>Co-authored-by: luyaor <luyaor@luyaordeMacBook-Pro.local>Co-authored-by: Wuwei Lin <vincentl13x@gmail.com>	5
Bugfix StmtMutator IfThenElse (#4609)	0
[TIR] Disallow vectorization with strides in VerifyGPUCode (#12477)	1
[microNPU] Add the infrastructure for lookup table and TANH (#9547)Some activation functions like TANH and SIGMOID are implementedby calculating the values based on the QNN parameters andrecording the values into a lookup table (LUT).This patch adds the LUT functionality alongside with the TANHactivation function and the tests.	3
[BUGFIX] Fix schedule dataflow rewrite with multiple scan states (#126)	5
[PYTORCH]aten::norm support added (#5776)	1
Revert "[FRONTEND] [HYBRID] Augmented assign operator supported! (#1459)" (#1466)This reverts commit 33245b8c09009fa2c080e1f42e786ede745de963.	4
[TIR] Canonical simplify the intset before region cover proof (#9941)* canonical simplify the intset before region cover proof* add more rewrite rule for intimms to fix the issue after rebase* remove rewrite rules, there are existing rule can work after canonical simplify	1
[Topi] fix get_pad_tuple3d bug, the conv3d kernel layout should be DHW. (#9788)	0
[TOPI] Minor perf improvement for GPU scatter (#7233)* improve scatter 4d init* do not launch sorting based scatter for small input* do not use hard coded num threads* separate sort based implementation* register scatter as autotvm task* add missing import* fix strategy* add dedicated schedule and dummy flop* add test tuning script* try adding dummy knob* skip random_fill when a tuning workload is from scatterThis reverts commit 1fed88321e640b509fc46fac7da3b3cb79719552.* cleanup memcpy ir* remove scatter tuning script* make sure zero init arguments* add comment on why skip random init for scatter* restore ctx syncCo-authored-by: masa <masa@pop-os.localdomain>	5
[bugfix] Fix the behavior of TVMScript printer (#9974)* upd* lint	0
[WEB] update web runtime to latest emcc (#742)	3
[Hexagon] Detect link-params via IRModule instead of target attribute (#9695)Additionally, restore the construction of target attributes in theHexagon target. This was erroneously removed in PR#9352.That PR deprecated target attributes, but also added transferringthese attributes to the new mechanism. Removing the attributesfrom tvm.target.hexagon eliminated them completely. Restoring thetarget attributes allow time to transition TVM clients to the newmechanisms.	1
[CI] Cleanup stale logs for auto-tuning (#8160)	2
Gitignore updated as NNVM untracked files not detected	2
fix incorrect pos ids generation in EmbedLayerNormalization (#11149)	0
[PYTORCH]Dropouts And InstanceNorm support added (#5203)* [PYTORCH]Dropouts And InstanceNorm support added* Review comments fixed	0
[PASS] Simplify dependency of StorageRewrite (#291)	4
[TensorFlow][Frontend] Adding InversePermutation Op (#8277)* [TensorFlow][Frontend] Adding InversePermutation OpComputes the inverse permutation of a tensor. This Op is used by Mask R-CNNor other object detection models.* uncomment test_read_variable_op* restore several tests* fix lint error* fix python linting error* fix lint error* restore mistakenly deleted codes	4
Update tflite wheel version to 1.13.1 (#3435)	5
[relay][vm] remove throw in destructor (#3215)	4
checked split	5
Remove GTest cmake flag from install docs (#3953)	2
[DOC] Developer documentation for InferBound pass. (#3126)* Developer documentation for InferBound pass.	4
[MetaSchedule][Bugfix] Feature: Strides of buffer access (#12331)I fixed a bug in MetaSchedule per_store_feature.cc	0
[nvcc] enable multiple arch in one fatbin (#4377)	0
[Ansor][AutoTVM v2.0] Phase 2: Layout Rewrite in AutoScheduler (#6297)* enable layout rewrite for auto scheduler* refine* update* fix CI* fix CI* fix CI* resolve review comments* add ut* resolve comments* resolve comments* fix coding style	0
[Relay, TOPI] Support dynamic slicing on first few axes, keeping the rest static (#8068)* Supporting dynamic slice on first few axes* fix index normalization* update dynamic slice tests* pylint fix* fix loop index dtype* fix more dtype issue	0
[Frontend][MXNet] MXNet frontend support for AMP cast op (#5976)* amp_cast* fix test* more tests* test more ctxs* fix doc* fix typo* address CR comment* fix lint* revert doc change* Revert "revert doc change"This reverts commit a410dd5569730ac81af67ddb333c3afbe97eddd7.* fix doc* Update relay_pass_infra.rstCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>	5
[CUDA] Fix fp16 intrin, disable bad fp16 vecadd test for now (#4239)	3
[TOPI] add nn schedulers for HLS backends (#1663)* [TOPI] add nn schedulers for HLS backends* fix pylint* fix topi transform test	3
[COMMUNITY] New committer -- @mbrookhart (#6936)	1
[NODE] General serialzation of leaf objects into bytes. (#5299)This PR refactors the serialization mechanism to support generalserialization of leaf objects into bytes.The new feature superceded the original GetGlobalKey feature for singletons.Added serialization support for runtime::String.	1
[Relay][Op] Add compute, schedule, and tests for expand_dims and squeeze (#2133)	3
Simplify for cxx	5
[Relay][Pass] Support combine multiple dense op just into dense (#6062)* feat: Support combine multiple matmuls to flat matmul* fix: Change to_batch -> to_batch_matmul and enrich docstring* feat: Add wrapped batching ops pass for python	4
Remove LoweredModule (#8886)* Remove LoweredModule* Clean up some comments* QEMU flaky tests* Don't add external functions to the LoweredFunctions module* QEMU flaky test* Respond to feedback* flaky test	3
fix compute inline not to over write annotated opaque accesses (#9509)	0
[INTRINSIC] Add sqrt (#202)* [INTRINSIC] Add sqrt* [INTRINSIC] Expose on cpp	1
Add one extra space to improve diagnostic messages (#10268)	1
[Relay] Support resize in the ONNX conversion (#8455)* [Relay to Onnx]* Added support for resize2d op* Added unit test* [Relay to Onnx][Resize]* Fixed formatting errors* [Relay to Onnx][Resize]* Fixed issue in resize conversion: round maps to round_preferc_ceil* Updated resize unit test to test for coordinate transform mode andround* Known issue: Does not match for (NN, align_corners) and Cubic* * Fixed formatting errors* * Fixed some more formatting errors	0
[FFI][CYTHON] Release GIL when calling into long running functions (#11461)Unlike ctypes, Cython by default do not release GIL whencalling into C API functions. This causes problems when thefunction is long running. As the particular calling thread willblock other python threads by holding the GIL.This PR explicitly releases GIL when calling into possiblelong running functions. It fixes the timeout issue inPopenPool which previously relied on another python threadfor timeout.Added a regression test-case by changing sleep to sleepin FFI, which previously will indefinitely block the popen tests.	3
[tests] add utility to replace direct call to pytest.main (#11393)	3
[Dataflow]: nullptr check (#5176)	5
[NNVM] Bug fix Prevent fusing convolution with injective op  (#1608)	1
[BYOC] FTVMAnnotateTarget method signature update (#6786)Signature of FTVMAnnotateTarget changed to runtime::TypedPackedFunc<bool(const Expr& expr)>which allows to utilise extra information from passed expr argument.	4
[PASS][PRAGMA] Allow pragma debug_skip_region to skip region of computation (#318)	0
schedule_injective of arm_cpu should consider dtype itemsize (#9339)* schedule_injective of arm_cpu should consider dtype itemsize* trigger CI	5
[RUNTIME] Fault tolerant vulkan init error (#1107)	0
[AutoScheduler] Task scheduler callbacks (#6945)* [AutoScheduler] Task scheduler callbacks* docstring* address comments* Delete the explaination of callback in the tutorial* fixCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	0
Register PackedFuncObj with type registry. (#11039)* Fixes CHECK-fails when trying to use reflection dispatch with PackedFuncObj. * Triggered by trying to add PrettyPrint() to StructuralEqual checks.	1
[TVMC] add cl support in tvmc runner (#6831)* [TVMC] add cl support in tvmc runner* Cleanup comment and asssert device type in else case	4
Fix link formatting. (#10730)	2
[VTA] VTA Compilation Script for Intel FPGA (#3494)* initial compilation script for chisel-vta;* replace tabs with spaces;* compile script for de10-nano;* remove generated verilog source code;* remove `altsource_probe`, `debounce`, `edge_detect` ip;* replace quartus project files with a single tcl script;* Update install.md* improved makefile-based compilation script;* complete makefile-based compilation of chisel-vta for de10-nano;* install quartus;* conversion to .rbf file;* document chisel-vta compilation process for de10-nano;* rename generated bitstream file;* download and extract custom ip for de10-nano;* minor change* minor change* fix indentation;* bug fix;* improved robustness in makefile;* clean up;* add `.sdc .ipx .qsys` allowance in jenkins;* add ASF header;* add ASF header;* remove IntelShell.scala, update vta_hw.tcl, clean up Makefile & soc_system.qsys;* add ASF header;* keep sources compact;* keep sources compact;* it's not necessary now* AXI4LiteClient -> AXI3Client for IntelShell* remove connection to fpga_only_master;* a few important bug fix: wire reset pin, and set host_r_last to high* remove intel specific interface definition;* add NO_DSP option in Makefile;* AXI4Lite is not used in IntelShell;* minor fix: disable dsp and use logic instead;* quartus version change: 18.0 -> 18.1* remove altera related statement;* compose compile_design.tcl* initial tcl script for soc_system generation;* remove .qsys file;* remove unused;* .qsys can be generated by tcl script;* remove hps_io and shrink size of soc_system;* integrate into makefile;* version change: 18.0 -> 18.1* add sample config file for de10-nano;* parameterize DEVICE and PROJECT_NAME* remove extra lines;* brief description on flashing sd card image for de10-nano* docs on building additional components* parameterize DEVICE and DEVICE_FAMILY* parameterize DEVICE and DEVICE_FAMILY* parameterize DEVICE and DEVICE_FAMILY* de10-nano -> de10nano* minor change* add comment in code and document in order to address review comments;	1
[TOPI] Fix the filter width parameter in depthwise_conv2d (#6081)* [TOPI] Fix the filter width parameter in depthwise_conv2d* Retrigger buildCo-authored-by: Venkat Rasagna Reddy Komatireddy <quic_rasagna@quicinc.com>	2
[FRONTEND][TFLITE] get input tensor information from graph (#7400)* [FRONTEND][TFLITE] get input tensor information from graph* remove bare-except* fix lint* delete empty line* comment change* move some of the tflite frontend code from tvmc to tflite.py* update shape and dtype when user provided them* remove unused var. pass user provided shape_dict* remove duplicate code	4
[doc] fix typo (#4463)	2
[REFACTOR] Macro standardization, lint tests (#7)* code refactoring* code refactoring* code refactoring* code refactoring* fixing macro* refactoring, tests, makefile* style - making sure lint test pass* prefixed macros with VTA, fixed bugs	0
Arm(R) Ethos(TM)-U NPU TIR to CS for Conv2D (#8811)This commit introduces the TIR to Command Stream(CS)translation using Vela API calls for conv2D and copy operations.It will create Vela npu_op objects for each command.Change-Id: I906d2cb333652813142cc70fb39b8372ec498bd0	4
[LLVM,TIR] Print LLVM intrinsic names instead of ids (#9964)* [LLVM,TIR] Print LLVM intrinsic names instead of idsThis makes it much easy to understand what is happening with llvmintrinsics.* add test, version llvm	3
[Relay] CaptureIndexInSpans debugging pass (#11926)* [Relay] CaptureIndexInSpans debugging passThis pass will update (most) expression nodes to capture their post-dfsindexes. That makes it easy to connect pretty-printed fragments back tothe overall model, and is very handy for Collage which uses post-dfs indexesextensively.* - rename- add header decl	1
[BUILD] Add caching to CMake (#8373)* ccache* ccacheFix formattingAdd comment about nvccChange default to AUTOMore progressAdd auto as a modeDisable ccache in CIadd-cache-to-cmakeFix typo* Fix rebase* flaky test	3
[Relay][VM] Allow to config allocator type and refactor vm code structure (#6105)* [Relay][VM] Allow to config allocator type and refactor vm code structure* fix doc* fix* update* trigger ci* trigger ci* trigger ci* trigger ci* fix doc warning	2
Handle float16 in ConstantNode visitor in LowerToTECompute (#10902)Load the bit representation of Float16 as uint16, and convert it to thecorresponding float32 value.	0
Add FreeRTOS variant of NPU demo (#10004)* Add FreeRTOS variant of NPU demoThis adds an extra flag to the existing NPU demo that runs it using theFreeRTOS kernel task scheduling and queues.* Add FreeRTOS notes to tutorial* Update FreeRTOS demo to run in demo scriptAlso, minor text fixes.* Minor text fixes* Fix docs formatting	2
[CI] Update ci-cpu to bionic (#5555)	5
[SPIR-V] Add SPIR-V lowering for While node (#7574)* Add SPIR-V lowering for WhileNode* test vulkan in while loop tests	3
[Onnx] Round operator (#11446)* banker round op added based off tutorial* black'd onnx.py file* retriggering CI with empty commit due to autoscheduler test failure* removed youtube link in comments* retriggering CI due to test failure that passed locally	4
[Torch] Restore class-aware NMS for detection models by graph rewrite (#7154)* add a pattern to rewrite nms to batched nms* update object detection test to add rewrite* updated tutorial* add doc* fixed coord_start* test fixed by setting force_surpress=False* revert tutorial change* add some comment to explain the pattern* update NMS pattern following frontend change	4
[TensorIR][M2a] ComputeInline,ReverseComputeInline (#8170)This PR is part of the TensorIR upstreaming effort (#7527), which adds the first 2 schedule primitives:- compute-Inline- reverse-compute-inlineCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[Hot Fix][Jenkinsfile]Fix Hexagon Parameter (#10966)* Fix ci_hexagon parameter* fix template	0
Address issue #6415 using compiler-rt half-float function. (#6431)	1
Switch to CPU PyTorch (#10914)	5
Fold If when the condition is Constant (#7354)	5
[Torch, QNN] Remove FP32 piggy back and use QNN add/mul/concatenate (#5061)* use qnn add/mul/concatenate* remove logging	2
[Relay] [Op] zeros_like and ones_like (#1835)	5
[microNPU] Add support for SPLIT and SPLIT_V (#9621)Both, SPLIT and SPLIT_V get lowered to relay.split and in thelegalization the Relay split gets turned into strided slices. Thispatch adds the pattern and legalizer to enable offloading the TFLite'ssplits to the NPU.	0
[Frontend][ONNX] Fix softmax converter when input shape is dynamic (#11507)* [Frontend][ONNX] Fix softmax converter when input shape is dynamic* [Frontend][ONNX] mark dynamic softmax tests as xfailed with cuda	0
[TOPI] 1bit dense operator on x86_64 (#629)* add x86_64 target* add binary dense operator* rebase* improve schedule* remove x86 target* improve schedule	1
[AutoTVM] Suppress the warning messages when compile engine selects impls (#5821)	2
Better version handling for Arduino (#11043)* Fix bug allowing microTVM to be used with Arduino version v0.20 and   above (see changes to _parse_connected_boards) and adds relevant unit   tests.                                                                                                                                          * Only perform version check when calling build or flash (things that   actually require arduino-cli), and adds relevant unit tests.                                                                                    * Only raise a warning if the arduino-cli version present is below the  min version (previously any version other than v0.18 would cause an     error).                                                                                                                                         * Change version comparison to use version.check, like the rest of TVM	1
[SYMBOL] Add __iter__ and GetChildren for symbol (#268)* [SYMBOL] Add __iter__ and GetChildren for symbol* [SYMBOL] Fix lint	0
Add sccache to docker images (#9844)	2
[Frontend][TensorFlow] Improve Control Flow and TensorArray (#5699)* Improve TF parser control flow and tensor array* Fix tf tensor array scatter* Add ssd test* Add back static ta test* Minor fix for frontend and test_forward* SplitRel for dynamic shape* Fix test ssd* Fix loop var naming issue* Minor improve* Fix format* Fix clang format* Fix tensor array in pytorch frontend* Fix stack size issue for ssd test* Address comments* Fix slice size* Fix build* Rebase	0
[TFLite] Fix _test_tflite2_quantized_depthwise_convolution is unused (#12145)* Update test for tflite2_quantized_depthwise_convolution* fix input_node name error* Add entry for test_quantized_convolution* reformatted by black	3
[TVMC] run: Don't use static path to find model.tar (#9712)* [TVMC] run: Don't use static path to find model.tarCurrently 'tvmc run' when '--device micro' is specified looks for themodel in the project directory at <project_dir>/model.tar. That worksfor Zephyr but fails on Arduino because model.tar is actually located at<project_dir>/src/model/model.tar. As a consequence 'tvmc run' when usedto run a model on Arduino exists because model.tar is never found.This commit fixes it by using the MLF path returned by the Project APIinstead of using a static path.This commit also adds a project_dir attribute to TVMCPackage that can beset when a MLF archive is loaded/imported so the project dir can beconveniently found (similarly to package_path attribute).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] test: Add test for importing a MLF with project_dirAdd test for TVMCPackage when importing a MLF archive and setting aproject directory too. Setting a project dir is only supported when aMLF model is imported, so it must fail on Classic format.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	0
Fix infercorrect layout in Layoutrewrite and improve naming. (#12007)* Fix infercorrect layout in layoutrewrite.* Compatibility issue.* Fix lint.* Better naming and detailed comments.* Add unittest.	3
[RELAY][OPS]LRN and L2_Normalize (#1860)	5
[ci] Fix doc deploy folder (#10634)This was unpacking into `tvm-site/docs` instead of just `docs` at the top levelCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[microTVM] Add support for host-driven AoT Executor (#11044)* Generate AOT Metadata when targeting C runtime and packed API.* Also copy metadata.h and metadata_base.h to standalone_crt.* add support for get_input_index as well as setting up get_input_info as unsupported* add support for tvm.aot_executor.create in C runtime* changes in-progress to unit tests* Include get_c_metadata in emitted function list* make CRT error codes generic for graph or AoT executor, fix AoT lib link order* add AoT executor creation and initializaion, as well as support for get_input_index()* add allocation of inputs, outputs, and pools; add get_input(), but shape encoded in metadata appears to be incorrect;* add support to test_aot_executor for get_input()* fix numpy array shape so that get_input() works properly* implement run(), get_output(), get_num_inputs(), and get_num_outputs(); test_aot_executor() is now passing;* fix up some issues from rebase with main* clean up logging and test_graph_executor()* lint clean-up* more lint clean-up* fix i386 build errors* first set of changes addressing PR feedback* more PR feedback: device pass-by-value, docstring entries, return variable name* add mangling of get_c_metadata() name to avoid function name collisions* only mangle get_c_metadata() when using C runtime* add static specifier to all kTvmgenMetadata variables to avoid namespace collisions* use TVM_IS_CPP_RUNTIME preprocessor define to deteremine whether or not to include metadata.h c++ code* add TVM_IS_CPP_RUNTIME define for cpptest* add TVM_IS_CPP_RUNTIME to apps/bundle_deploy* add TVM_IS_CPP_RUNTIME web/Makefile* update number of expected generated C files for AoT source files* break out metadata data structures into separate metadata_types.h header to avoid c/c++ issues and remove the need for the TVM_IS_CPP_RUNTIME define* remove TVM_IS_CPP_RUNTIME from web makefile* fix metadata.h include-order lint issue* correct error mask bits* address PR feedback* trigger build* trigger build* trigger build* trigger build* add alternate name for test_graph_executor() too see if it runs in CI* fix lint* revert alternate test codeCo-authored-by: Andrew Reusch <areusch@gmail.com>	3
[CI] Install xgboost in Hexagon image (#12592)Needed for https://github.com/apache/tvm/pull/12587@mehrdadh cc @Mousius @areusch @driazati @gigiblender	2
update webgpu api (#6547)	5
[COMMUNITY] new committer -- gromero (#10911)	1
[Frontend][MXNet] add _npi_stack, issue #7186 (#7209)- https://github.com/apache/tvm/issues/7186- add MxNet stack, `_npi_stack`- https://mxnet.apache.org/versions/master/api/python/docs/api/np/generated/mxnet.np.stack.html?highlight=stack	2
[AutoScheduler] Fix distill record (#7439)* [AutoScheduler] Fix distill record* update comments	5
[Bugfix] Check file exists before removing it (#3178)	4
register depthconv, elemwise (#17)* register depthconv, elemwise* use global elemwise schedule for relu	1
Delete _ir_pass.pyi	4
[Relay, TOPI] Add searchsorted op (#9184)* Add relay definition* 1D cpu test working* multi dim working* gpu version working* check shape in type rel* support side* use target specfic max threads* add relay boilerplate* relay test working* cleanup topi test* fix test* add torch converter* handle other cases* more topi test* support torch bucketize* update doc* fix tests* fix lint* rebase fix* make the test case smaller* add tests for edge cases* replace "side" attribute with boolean "right"* add more descrition to binear_search IR gen params* return index from binary_search rather than update inplace* remove unused argument* format fix	0
Stop running some python testsuites twice (#7430)	3
[OP] Enable register via match tag (#57)* [OP] Enable register via match tag* more docs on usage	2
Use TVM log instead of hexagon_print (#11024)	2
[TUTORIAL]TFLite QNN Tutorial (#5595)* [TUTORIAL]TFLite QNN Tutorial* Review comments	5
[HEXAGON] Add op resize2d for hexagon (#11834)* [HEXAGON] Add op resize2d for hexagon* Reformat* Change to v69	4
fix typo (#9304)	2
[AUTOTVM] Fix local executor (#1651)The old queue size is too small. It will stall the executor due to race condition.	0
[ONNX] Add MatMulInteger importer (#10450)* implement matmulinteger* rm test* rm outdated comments* fix lint and review* wip* fixes* fix* alter tests* extra 4x4x4 step* comments	3
Update and rename start_rpc_server_to_tracker.py to start_rpc_server_to_tracker.sh (#4689)This is a shell file, not a Python file.	2
[ci] Switch to requiring `pr-head` for merges (#11081)This is necessary to fix the bug where PRs rebuild when editing the description.sCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Frontend][TFlite] Add parses support for unary elemwise ops (#4634)* [Relay][Frontend][Tflite] Add parses support for unary elemwise ops* Add generic method to convert unary functions: abs, exp, ceil, floor  log, sin, cos, sqrt, rsqrt, neg* Add relevant tests* Delete excessive underscores as requested in PR review* Change parameter name as suggested in PR review	2
[Arith] add SizeVar representing non-neg valued variable in a tensor shape (#4684)* [arith] add ShapeVar representing non-neg valued variable in a tensor shape* bounder remover; deal with div in int_set differently* fix bounder_remover* migrate unittest to use shape_var* use tvm.shape_var in integration & relay tests* add test case; fix Var register* fix lint* fix lint again* add default ShapeVar visitor in Relay* fix override* fix ShapeVar visit bug* revert IntervalSet for shape_var* remove bound_remover* remove is_var; use constructor for shapevar/var instead* ShapeVar -> SizeVar; add constructor comments* shape_var -> size_var in doc* tindex -> size	2
Prepare for switching VM to LowerTEPass. (#9550)This is a grab bag of fallout changes from switching the VM to use LoweTEPasswhich can be easily split out of the main #9483 PR.- AnnotateSpans can be used from C++ (though, unfortunately, it didn't help  me with debugging since spans are universally dropped in most passes).- Can get a human readable dump of the VM's PackedFunc names and indexes for  debugging.- If TVM_LOG_DEBUG defined then include types and ids of GlobalVars. I had  a lot of difficulty tracking down where duplicate GlobalVars for the same  name_hint were getting created and propagated.- GetCallLoweredProps follows same API as GetDeviceCopy and GetOnDevice  where will return 'null' properties if call/expr is not of call_lowered  form. Mildly more convenient, though switching all the above to ICHECK  and push 'if (op == the relevant op)' into all use sites would also be just  fine.- Misc VLOG improvements made while tracking down issues in #9483.	0
Fix rst formatting in documentation (#8303)Whitespace is required before ":sup:". Using backslash-escaped whitespaceprevents it from appearing in the processed document.	2
[Doc] Update ANTLR instruction (#4231)* [Doc] Update ANTLR instruction* Update docs/install/from_source.rst	2
[DOCS] Minimize necessary doc change (#5129)	4
Support rank-0 tensor (#687)* Support rank-0 tensor* fix lint	0
Add missing shape functions for relay.nn operations (#8489)* Update _nn.pyadd a few missing shape functions* Update _nn.pyUpdated conv_transpose shape function to accomodate conv1d_transpose* added tests for new functions* fixed a lint error* fixed shape func error* attempt fixing cuda error	0
[PASS] Improve storage rewrite(#846) (#847)* fix #802, create cache based on sugar tensor* [Pass] Improve storage rewrite* fix ci* fix comment* fix comment	0
Update image version tags in Dockerfile comments (#4631)* Fix typos on Docker image versions that we are currently running   as part of CI * Add version comment in the same pattern for ci_lint image	1
[Relay] add more descriptive error for checked_type (#2652)	0
[RELAY][BYOC] Register pattern tables from external codegens (#5262)* [RELAY][BYOC] Register pattern tables from external codegensThis adds utility functions to support registeringand retrieving pattern tables used by MergeComposite forexternal codegens.Change-Id: I5be165a321440e48b15ff6aff4970e0c67496aaa* Updated DNNL tests to use pattern table mechanism* Removed pattern table standalone test* Change reg to _op	4
[DOCS] TVM install addenda for M1 Macs (#8568)* instructiosn for m1 mac* typos* above to below* nits, link against python issue on github* correct link* more cleanup* correct source* address chrishoge suggestionsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[TVMSCRIPT] Make ceildiv available from tvmscript (#12096)	1
Patch replay trace. (#11621)	5
Add support for absolute opeartion (#1406)	1
[relay][heterogeneous] annotate using visitor (#3261)* annotate using visitor* retrigger CI	1
[DOC] Add Android Tutorial (#2977)* fix APP_STL for latest android ndk* add vulkan sdk for tutorial* add android tutorial* fix of invalid input layer name* update relay build opt_level 1 -> 3	5
HG: Commit message of changeset 6281661. (#5622)[Relay] Move compiler_begin/end_op to local static objects	4
[ci] Add @tvm-bot rerun (#11480)This adds a command to restart CI runs that have stopped (either from afailure, success, or abort) via GitHub comments addressed to tvm-bot:```@tvm-bot rerun```tvm-bot will then comment on the thread and send a request to Jenkins torestart CI. This does not restart GitHub Actions jobs though we may beable to add that in the future.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[FIX] Add CombineInternal<Mod> & Fix LoopPartition (#138)* Add CombineInternal<Mod> & Fix LoopPartition* Add check for path	1
[Hexagon] remove #if defined(__hexagon__) where it is no longer needed (#11708)* remove #if defined(__hexagon__) where it is no longer needed* format and lint	4
[Relay][Parser][Bugfix] Fix parsing hierarchical attibute names (#7976)	0
[Arith] Updated BufferDomainTouched to use IRVisitorWithAnalyzer (#11970)* [Arith] Allow binding of Var in IntSetAnalyzerThe other four subanalyzers in `arith::Analyzer` can each be providedwith variable bindings/constraints that are remembered internally.This adds the same capability to `IntSetAnalyzer`, rather thanrequiring users to independently track and maintain a `Map<Var,IntSet>` containing the domain of each variable, and appliesbindings/constraints alongside the other subanalyzers.* [Arith] Updated IRVisitorWithAnalyzer to mimic IRMutatorWithAnalyzerPreviously, `IRVisitorWithAnalyzer` did not allow subclassing, andcould only be used to collect bounds of variables along an entirestatement, and could not be used to perform scope-dependent analysis.This commit removes `final` from `IRVisitorWithAnalyzer` and providesthe same scope-based constraints/bindings during iteration as areprovided by `IRMutatorWithAnalyzer`.* [Arith] Moved IRVisitorWithAnalyzer to tvm::arith namespaceChanging for consistency, since `IRVisitorWithAnalyzer` it is part ofthe `src/arith` directory and the analogous `IRMutatorWithAnalyzer` isalready part of the `arith` namespace.* [Arith] Updated BufferDomainTouched to use IRVisitorWithAnalyzerThis used the earlier changes to allow subclasses of`IRVisitorWithAnalyzer`, and to expose binding/constraints to`IntSetAnalyzer`.* Avoid accidental Bind with dynamic Range* [Arith] Do not visit SelectNode in IRVisitorWithAnalyzerBecause both sides of a `Select` node are visited regardless of thecondition, the `SelectNode::condition` should not be treated as aknown value.* [Arith][IntSet] Track global and scope-dependent bounds separatelyResolves a bug that was found in CI, where an earlier scope-dependentconstraint was treated as a conflict by a later global bound.* [Arith] Recovery function for each subanalyzerThis way, if a subanalyzer throws an exception during`EnterConstraint`, the other subanalyzers are still appropriatelybacked out of the constraint.* [Arith][IntSet] Use CanProve instead of CanProveGreaterEqualThe `min_value - max_value` in the `CanProveGreaterEqual` argument canresult in an exception being thrown for unsigned integers wheresubtraction would wrap.* [Arith] Allow vector expressions in IntSet::operator(PrimExpr)Since these are tracked when lowering expressions, should allowpost-vectorization expressions.To maintain previous behavior, this only applies when using theautomatically tracked `Map<Var, IntSet> dom_map_`.  If an explicitdomain map is passed, the previous behavior of raising an error forvectorized expressions still occurs.* Avoid comparisons between integer and handle datatypes* [Arith] IntSet, Combine() extensionPreviously, the Combine() method didn't handle values without a knownlower bound, for boolean operators.* Added docstring* Naming consistency of `IntSetAnalyzer` methods.To be consistent with other subanalyzers, using "Update" whenproviding the analyzer with the same data structure as is usedinternally, and "Bind" used when providing it with something that mustbe converted to the internal data structure.	5
Allow RPCWrappedFunc to rewrite runtime::String as std::string (#5796)	1
[NNVM] Add symbol squeezenet (#1436)	1
support MXNet _minimum and _maximum (#2709)	5
[CODEGEN] fix & improments in codegen (#745)* [CODEGEN] update codegen for vector operation* update comment, fix for metal* fix some bugs in codegen* use 'restrict' in every argument* fix* fix	0
[TVMC] Fix wrong terminology in tvmc source (#10320)Renames variable from `runtime` with `executor` to betterreflect current terminology and reduce confusion.	5
[Hexagon] Disable `thread_local` on Hexagon (#9025)This is specific to running code on hardware: libc++abi can createTLS keys with destructors in the libc++abi library. Despite that,the library gets unloaded before the keys are destroyed, leadingto a crash. Turning off the use of `thread_local` is a workaroundfor this.	1
[SUBMODULE] switch to https (#341)	5
rocm: fix dense_rocblas in strategy, topi (#5191)	0
[ETHOSN] Add support for mean on Ethos-N78 (#10130)Adding the support of mean on Ethos-N78, which is based onan underlying pattern matching scheme.The operator is tested with 2 shapes: 4 and 3 dimensions.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>	3
[BYOC-DNNL] add post_sum pattern (#12151)* add post_sum pattern* add checkers for sum pattern* fix lint* fix error in test_pass_partition_graph* fix lint error	0
[FRONTEND][MXNET] Add expand_dims supoort (#1317)* [FRONTEND][MXNET] Add expand_dims supoort* fix lint	0
[Docs] Added documentation on pytest target parametrization. (#8638)* [Docs] Added documentation on pytest target parametrization.Follow-up from #8542, to document existing features.* [Docs] Updated pytest parametrization documentation following reviewCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Caffe Frontend] supporting group > 1 cases for Deconv op (#8260)* [Caffe Frontend] supporting group > 1 cases for Deconv op- Handling group > 1 cases, assuming group == output channels- Simply decomposed into Relay split, conv2d_transposed, and multi-leveled concatenate ops- Added some test casesSigned-off-by: zotanika <zotanika@gmail.com>* [Caffe Frontend] amending a test case for Deconv opSigned-off-by: zotanika <zotanika@gmail.com>* explicit importing tvm.testing* changing split axis to 0, according to PR #9336	4
[CI] Pin setuptools to v58.4.0 in CI to circumvent breaking change in v58.5 (#9446)* Pin setuptools to v58.4.0 in CI to circumvent breaking change in v58.5* Remove setuptools installation from ubuntu_install_vela.sh	1
[1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055	5
[Python dep] Add missing dep pkg for relay (#2568)	1
[relay][vm] move vm opt passes to pass manager (#3323)	4
[SYMBOLIC] Add symbolic API (#2)* [SYMBOLIC] Add symbolic API* Update Testcase to nnvm	3
Adjust Vulkan queue selection and creation logic (#6662)* Adjust Vulkan queue selection and creation logic- The queue selection logic rewrite addresses a bug in the oldimplementation where the code would pass an invalid queue index whenselecting any queues other than number 0.- The new implementation will attempt to use compute-only queues whichare common on AMD GPUs. It's not clear how much difference will thismake but hopefully it would lead to better scheduling.The queue changes were made as with the old configuration autotvm caused mysystem to hang, stutter or otherwise become unstable and crash. With the changeI'm able to run autotvm tuning inside a desktop environment.* Return multiple queue family indexes from GetComputeQueues* Only create one queue	1
[AutoScheduler] Fix the type inference for conv2d (#7501)* fix type inference for conv2d* fix	0
[RUNTIME] support limited save without cross compile (#659)	1
[Relay] fix target string (#3071)	1
[BYOC][Verilator] change runtime registry function name (#7351)* use lowercase for verilator runtime registry function* lint fix* update comment	5
[Relay][Pass] Avoid stack overflow when using PostOrderRewrite (#7588)* init* fix* fix	0
[MetaSchedule][Test] Add unittests for C2D (#12043)	3
[DOCS] Improve windows build instruction via conda (#6944)	1
[Relay] Keras frontend upsample and 1 channel conv2d fixes (#3937)* Fix upsample layout in keras frontend.* Fixed group conv being used instead of conv when channels=1* Add new conv2d test to catch bugs when channels=1.	0
vm external codegen (#4544)	5
[Pre-commit] Add pre-commit configuration to perform minimal checks locally (#8382)* [Pre-commit] Add pre-commit hook configuration file* [Pre-commit] Add header to configuratin file* [Pre-commit] Add basic configuration instructions* [Pre-commit] Extend pre-commit pipelines with C++ linting* [pre-commit] Add example usage comment for pre-commit hooks* [CI] Add in docker linting script mypy step* [CI] Use lint docker image for pre-commit checks* [CI][pre-commit] Minor cleanups on docker runners of pre-commit lints	1
[ONNX] Support depth_to_space op for FQ2I  (#8966)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Per channel fq2i (#8)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Fix bug in requantize dimension expansion.* Format.Co-authored-by: Josh Fromm <jwfromm@octoml.ai>* respond to review comments* start dtos* wip depth_to_space* dtos identCo-authored-by: Matthew <mbrookhart@octoml.ai>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>	5
Fix misprint (#2272)	0
[RUNTIME] Move Map into runtime (#7570)* [RUNTIME] Move Map into runtimeThis allows us to use Map to store parameters needed at runtime.* node.{Array|Map} -> runtime.{Array|Map}* missed some renames	1
[VM] Add get_input_index support. (#8661)	1
Improvements in conda recipe (#3791)	1
[Relay][Prelude] Add more dtypes to tensor_t (#4233)	1
[TIR]Show meaningful message when input shape size mismatch with expected size. (#9863)[Issue]When the transform function get a input data with incorrect shapesize, the error message for example "src_shape.size() == src_axis.size()(6 vs. 4)" will be printed out, but the said message not provide enoughinformation to help trouble shooting.[Solution]Add more meaningful error message.Co-authored-by: hua jiang <hua.jiang@xilinx.com>	0
[docs] Add CI contribution instructions (#12551)This PR documents the steps to introducing a new CI docker image, which we've been doing a lot lately.	2
[CMake] Add compile-time check that libtvm_runtime.so has no undefined symbols. (#8178)If libtvm_runtime.so erroneously requires definitions that are onlypresent in libtvm.so, the -Wl,--no-undefined flag forces them to becompile-time errors rather than runtime, and would be caught by theCI.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[NNVM] Move FTVMCompute registration of cast, greter, less to C++. (#1370)	4
[TIR] Add SHash and SEqual to IndexMap (#11798)	1
Add missing Slice layout fallback check of `stride=1`. (#10690)* Add missing Slice layout fallback check.* Fix lint* jostle ci	0
Support Deriving channels when it is not provided in AlterLayout. (#2972)	1
[microNPU][2b] Create CascaderGraphs from TE graphs (#9471)The first step in the cascader is to create aCascaderGraph from a TE graph. To do this, everyoperator in the TE graph must get 'matched' by aPart matcher. This converts TE operations intocascader Parts by augmenting them with Propagators.In this initial commit, we include basic Partmatchers for ethosu_conv2d and some transformoperators so that the graph creation can be tested.	3
[FRONTEND][TENSORFLOW] NCHW layout support (Resnet V1/V2). (#1743)	1
[APPS] Add How to deploy graph runtime example under new module factory (#6459)	1
[AUTOTVM] Support multiple targets load in tophub (#1803)	1
[microTVM] Project API Arduino support (#8708)* ProjectAPI Arduino support* Compile and run integration tests* Add support for other Arduino boards* Unit tests for project generation* AOT support* Arduino RPC server* Incorporate ProjectAPI changesAdd Arduino tests to CI* Copyright notices* Fix Andrew's PR comments* Additional PR commentsPR comments and Python 3.6 supportLinting fixRe-add test onnx fileTest Arduino cli bug workaroundSupport new hardware targetsTemporary fix for testsFormatting issueSpelling fixAdd test case for exact FQBN matching* Add unit tests from apps directory to task_python_microtvm.sh	3
Fix Jenkins skip for CPU/GPU (#9549)Each step should be checking the docs-only build and marking itself as skipped if necessary, these two were skipping the i386 step instead.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Bind iter domain in analyzer in CreatePrimFunc (#11187)* [TIR] Bind iter domain in analyzer in CreatePrimFunc* lint* fix test	3
trivial (#3954)	5
Update tune_relay_vta.py to support single board (#7100)- support single pynq board run, change is credited to 'https://github.com/i24361's change, https://github.com/i24361/incubator-tvm/blob/0472b1f347976229a29be8a6e60b626a0604c8df/vta/tutorials/autotvm/tune_relay_vta_with_one_board.py- fixes the save fail- issues and changes are discussed in https://discuss.tvm.apache.org/t/vta-workaround-for-autotuning-with-one-pynq-z1-board/8091/9	1
update depthwise_conv2d schedule and testing (#328)	3
[TVM] ref_counter -> ref_counter_ (#5184)	5
imp module is deprecated (#4275)	5
[TOP] Init dense	5
[TensorIR][PASS][M1c] PlanUpdateBufferAllocationLocation (#7873)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	1
[AutoScheduler] Do not return naive schedule in tracing mode (#7226)* [AutoScheduler] Do not return naive schedule in tracing mode* lint* fix	0
[Topi] add x86 schedule for batch_norm (#12321)* format black* format black* docstring* typo	2
[Target] Add a few AWS C5 instances in target tag system (#11869)	5
Fix QNN type inference (#7074)* check for incomplete types in QNN Relation functions* add regression test from #7067* respond to review comments	3
[CI][DOCKER] Update ci-gpu torch1.4 and onnx1.6 (#4826)	5
Remove comparison of unsigned expression < 0 warning (#6319)* fix: remove warning for if (unsigned < 0...)* change used types related to dlpack ndim to int32_t	1
Remove check_correctness in AutoTVM, which is busted (#7250)	4
@zhiics -> PPMC (#5692)	5
[ci] Fix action expressions for tvm-bot workflow (#11556)These weren't caught by `actionlint` for some reason but GitHub doesn't merge multiple `if`s, so this combines them into one.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[rpc] Implemented rpc logging (#11232)* Implemented rpc logging* fixing windows build issue* triggerCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
[NNVM] remove keepdims from expand_like arguments (#1517)	4
Rustify PackedFunc & Friends (#2969)	5
Retain qnn input kernel scales (#4292)* Add qnn conv2d attributes for input_tensor_scale andkernel_tensor_scale.The lowering in the tflite frontend loses the input_tensor_scaleand the kernel_tensor_scale by multiplying it and putting it intothe Requantize operation. This means that any graph partitioningpasses or other passes that need to access this information no longerhave it available in the qnn dialect.regardsRamana* Store input tensor scale and Weight tensor scale for Dense as wellAs for conv2d, the tflite frontend drops the input tensorscale and the weight tensor scale from the relay op. Storeit as separate fields in there.* Fix unintentional tab* Rename input_tensor_scale to input_scale and kernel_tensor_scaleto kernel_scale for conv2d.* input_tensor_scale -> input_scale weight_tensor_scale->weight_scale* Rework dense testcaseAnd use input_scale and kernel_scale* Be consistent in use of input_scale and kernel_scale values* Fixup qnn conv2d tests for input_scale and kernel_scale* Make pydoc identical between conv2d and dense for weight_tensor* Fix up conv2d parameters to be in the same order between C++ and python* Fix ordering of parameters for dense.* Add input_scale and output_scale to try and satisfy ci gods* Delete input_scale and kernel_scale.nn.conv2d does not contain input_scale and kernel_scale. We needto delete it when lowering it to nn.conv2d.* Add input_scale and kernel_scale for qnn.conv2d	1
[MetaSchedule][Test] Add unittests for GRP (#12246)	3
[VTA] Infinite recursive device_api.ext_dev call fix (#7985)#3843 fixed the infinite recursive call for the Xilinx boards, but didn't fix it for the intel boards. This fixes it for the DE10 (same missing symbol problem with same fix).	0
[ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op (#9017)* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op* Fix linter error for variable name and else after return* Separate quantized avg_pool impl and add TODO for global_avg_pool* Fix comment typo	2
Fixing issue #1395: Undefined symbol: cudaGetDevice (#1396)	2
Add Range op to ONNX, make tvm arange op support negative steps (#6647)	1
Add PaddlePaddle dependency in docker file (#8742)	2
Ensure google-mock is installed and setup (#9107)Google Mock is the mocking/helper framework that gets bundled with Google Test, it used to be separate but now isn't.  I ran into the issue of Google Mock not being configured fully in the i386 build of #9106, which uses the `HasSubtr` matcher. This PR aims to fully configure Google Mock for use, which is interesting in itself...The headers are installed as part of Ubuntu 18.04's `googletest` package:```shell$ dpkg -S /usr/include/gmock/googletest:amd64: /usr/include/gmock```But not the lib sources, that requires another package named `google-mock`:```shell$ dpkg -S /usr/src/gmockgoogle-mock:amd64: /usr/src/gmock```But in Ubuntu 16.04 the includes and lib sources are in the `google-mock` package:```shell$ dpkg -S /usr/include/gmockgoogle-mock:i386: /usr/include/gmock$ dpkg -S /usr/src/gmock/google-mock:i386: /usr/src/gmock```And excitingly, in Ubuntu 20.04 this will again be changed to `libgmock-dev` by thelooks of things, just to keep us on our toes.	4
[TVM] Update tvm and benchmark script (#196)	5
Hotfix pylint (#3615)	0
Fix Arm(R) Ethos(TM)-U55 NPU Demo app (#9323)* Change tvmc arguments to -executor=aot -interface-api=c -unpacked-api=1.* Enable the demo running on the CI.	1
[DOCKER][GOLANG] Fix golang compiler version to 0.10 (#1848)	0
Bugfix for path issues (#3038)	0
[MetaSchedule] JSONDatabase Utilities (#11680)This PR adds some utility to JSONDatabase to accelerate its loading/saving time.	5
[CODEGEN] Fix CPU compute attribute (#582)	0
[TOP][COMPILER] Add expand_dims, change graph_compare to not compare input optionally (#25)	4
Update mutate function (#23)	1
[Team] @merrymercy -> PMC (#2578)	5
Simplify TF get_output_names (#3025)	1
[Relay][OP] Support NMSv4 ingestion from TF. (#6085)	1
[DSL/TE] Scalar support for `te.extern`  (#6079)* fix make shape with scalar shapes* add test* add test* remove scalar shape assertion* fix the data type for overflow problems* add extra testsCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>	3
[Arith] Allow unused trivial iterators in bijective check (#11425)This PR extends `DetectIterMap` bijective check to allow unused zero-constant iterators (trivial iterators). For example, `i, j, k => i, j` are treated as bijective if `k \in [0, 1)`, even though `k` is not used in the mapping result.Previously, when trivial iterators are simplified, the above iter map can pass bijective check. When `simplify_trivial_iterators==False`, `k` will not be simplified and `DetectIterMap` will fail if `require_bijective` is set. This PR make the behavior of `DetectIterMap` consistent with different setting of `simplify_trivial_iteraotor`.Regression tests for `reverse_compute_inline` is also added.	1
[PYTORCH] [FRONTEND] torch.bool support for data type conversion (#11290)* [FRONTEND][PYTORCH] Support fo nn.SiLU added* torch.bool added to torch convert_torch_dtype_map	1
Update cuda softmax schedule for spatial inputs (#2338)	5
[PYTHON/FFI] Enable Cython FFI (#106)* [PYTHON/FFI] Enable Cython FFI* fix cython	0
[Relay][Quantization] Speed-aware quantization scheme improvement (#2723)* [Relay][Quantization] Speed-aware quantization scheme improvement* Add comment* Add use_stop_fusion to qconfig* Update comment	5
[Docs][UnitTest] Updated target parametrization documentation (#8724)* [Docs][UnitTest] Updated target parametrization documentationThe intended audience are developers writing unit tests, or debuggingunit tests that have failed.  Therefore, moving the recommended styleto the top of the section, and the implementation details to thebottom.* Documentation updates as recommended by tkonolige	5
Fix bug that disabled cuda integer dot product. (#12099)	0
[BYOC] Handle constants in IRModule-at-a-time external codegen (#11770)I tried to do to the TensorRT integration what #11631 did to the CUTLASS integration, viz: - Make sure all compilation options are passed in Target instances. This helps Collage. - Use a custom pass invoked via RelayToTIRTargetHooks instead of the relay.ext.$toolchain mechanism.   This helps use decouple external codegen from lowering.This PR collects the prep for that change: - TensorRT uses the JSONSerializer visitor to encode each partition function. Previously, when the   visitor encountered a Constant it simply generated and recorded a name for the constant. Then,   completely separately, and via a callback in TECompiler, the function is visited again in the   same order and with the same name generation convention by a ConstantUpdater to actually collect the   bindings, which are then encoded into a ConstLoaderModule to be made available at runtime.   However if all TensorRT compilation is to be done by a stand-alone pass there's no TECompiler callback   hackery available. So I've added a "const_name_to_ndarray" attribute to the IRModule of type   Map<String, runtime::NDArray> so that named constants can be accumulated throughout compilation by   any pass which needs to do so. Then the Graph, AOT and VM executors are all updated to merge those   constants into the final runtime artifact   (Compare with "Constants", the equivalent attribute for extracting TIR AllocateConsts.) - The TensorRT tests use the create_executor interface but it wasn't quite ready for the   new more general form of passing list-of-targets. - I want TensorRT compilation to work out of the box without the need for any special targets if   all the default options should apply. Go back and make the CUTLASS integration I did follow the   same convention. - To test this I also switched the 'demo' "ccompiler" external codegen target to IRModule-at-a-time   style. This means we can test most of external codegen machinery in one place without depending on   any target which may not be enabled in CI (eg TensorRT):     - Target instances are plumbed correctly so compile-time options are available.     - External modules are conveyed to the final export library.     - Constant bindings are conveyed to the metadata module.	5
[MetaSchedule] Added a cost model (#11961)In this PR, I added a cost model based on SegmentSum MLP, which can be used for pre-training or integration with TVM.	5
[TIR] More hygenic TVM_SREF macros (#12607)Previously, the `TVM_SREF_TO_BLOCK`, `TVM_SREF_TO_FOR`, and`TVM_TYPE_AS` macros required both the input and output variables.The input variable name is useful for improving the error messagereturned, but the output variable name isn't necessary for thisfunctionality, and prevents the macro from being used as part of anexpression.* Generate an immediately-invoked lambda expression to allow for an  independently-scoped `result` variable.* Use parentheses around the input argument, in case the sref is  the result of an expression.* Update all call sites to remove the macro argument providing the  first argument.	1
[CMSIS-NN] Conv2D with equal paddings can be mapped to CMSIS-NN target (#9801)	1
[Relay][Frontend][TF] Add tensor array ops (#3798)* [Relay][Frontend][TF] Add tensor array ops* rename* delete test* Move utility function* Refactor* fix tensor array ops* fix test* fix rebase* Fix serializer bug* Improve tf convert name lookup to use prelude api* Fix lint* Fix test	3
[AutoScheduler] Add layout rewrite support for dense and batch matmul on CPU (#7161)* [AutoScheduler] Add layout rewrite for dense and batch_matmul* Fix test & Address comments* Fix shape inference* fix test	3
[UnitTest] Removed vulkan from CI run of task_python_topi.sh (#9219)Vulkan unit tests were enabled withhttps://github.com/apache/tvm/pull/9093, but were only intended to runtests/python/unittest/test_target_codegen_vulkan.py.  Sincetask_python_topi.sh did not explicitly specify `TVM_TEST_TARGETS`, itdefaulted to `tvm.testing.utils.DEFAULT_TEST_TARGETS`, which includesthe vulkan runtime.This commit adds an explicit specification of `TVM_TEST_TARGETS` forthe topi unit tests, matching the targets enabled in the CI GPU build,but excluding vulkan.	0
[AutoTVM] Update XGBoost verbosity option (#5649)	5
[Relay] Improved `multiply_rewrite` for gluoncv_ssd quantization (#2848)* improved `multiply_rewrite` for gluoncv_ssd quantization* add a check to attach_simulated_quantize below to prevent quantizing lhs to INPUT if lhs_kind is INPUT;	1
[FIX] several bugs found when using NNVM (#391)	1
[BYORTL][Verilator] update ops and add MobileNet (#7972)* update* update vta submodule* cpp fmt* python fmt* skip if tflite is not available* fmt* change assertion* update comment	5
[TIR] Handle DeclBuffer in ToSSA (#12679)	0
[tvm4j] disable proxy test for now (#307)	3
[MetaSchedule] postproc: rewrite_cooperative_fetch (#10081)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
Vulkan TVM Android Support (#1571)	1
[TOPI][CUDA] Fix Winograd Kernel Size Support (#4276)* fix_winograd_cuda_kernel_size* add unit test	3
Remove pointer arithmetic in StorageObj::AllocNDArray (#7890)The data pointers returned by AllocDataSpace are intended to be opaquehandlers, where previous implementation assumed pointer arithmetic isvalid on them.  Updated to instead use the byte_offset field toindicate the offset in the allocated array.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[RUNTIME/CUDA] Allow save exit when driver unload (#122)	1
[COMMUNITY] @ajtulloch -> Reviewer (#2236)	3
[PASS] IRTransform to enable IR pass proptype in python (#401)	4
Enable StackVM in AutoTVM (#7897)	0
[WIP][Pylint] Making frontend tests pylint compliant (#12028)* [CI] Apply linting rules to tf tests* [CI] Apply linting rules to tflite tests* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to caffe2 tests* reformat by black* reformat by black* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to darknet tests* Update tests/python/frontend/coreml/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/tflite/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/coreml/test_forward.py* Update tests/python/frontend/tflite/test_forward.py* fix test errors* fix ci test errors* [CI] Apply linting rules to keras tests* [CI] Apply linting rules to oneflow tests* [CI] Apply linting rules to onnx tests* replace with tvm.testing.main()* fix conflict* Update as @areusch suggest* pylint pytorch/test_forward.py* Update as @areusch suggest* reformat by black* fix redefined-outer-name lint errors* remove rules* remove rules & fix pylint errors* Remove all unused_var and fix other pylint errors* reformatted by black* [CI] Apply linting rules to onnx tests* Disable unused-argument is for some unused-argument in function can not be removed* Fix ci errors* reformatted by black* Fix ci errors* Fix invalid-name/unused-variable,redefined-builtin* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to darknet tests* [CI] Apply linting rules to pytorch tests* [CI] Apply linting rules to tflite tests* reformat by black* [Pylint] Making frontend tests pylint compliant Part 1 of N* Fix ci errors* Fix invalid-name pylint errors* Fix invalid-name pylint errors* Disabale temporarily* Fix dangerous-default-value* Fix typo errors* Fix ci errors* [CI] Apply linting rules to tensorflow tests* Fix dangerous-default-value* Fix ungrouped-imports* [CI] Apply linting rules to tf tests* [CI] Apply linting rules to tflite tests* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to caffe2 tests* reformat by black* reformat by black* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to darknet tests* Update tests/python/frontend/coreml/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/tflite/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/coreml/test_forward.py* Update tests/python/frontend/tflite/test_forward.py* fix test errors* fix ci test errors* [CI] Apply linting rules to keras tests* [CI] Apply linting rules to oneflow tests* [CI] Apply linting rules to onnx tests* replace with tvm.testing.main()* fix conflict* Update as @areusch suggest* pylint pytorch/test_forward.py* Update as @areusch suggest* reformat by black* fix redefined-outer-name lint errors* remove rules* remove rules & fix pylint errors* Remove all unused_var and fix other pylint errors* reformatted by black* [CI] Apply linting rules to onnx tests* Disable unused-argument is for some unused-argument in function can not be removed* Fix ci errors* reformatted by black* Fix ci errors* Fix invalid-name/unused-variable,redefined-builtin* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to darknet tests* [CI] Apply linting rules to pytorch tests* [CI] Apply linting rules to tflite tests* reformat by black* [Pylint] Making frontend tests pylint compliant Part 1 of N* Fix ci errors* Fix invalid-name pylint errors* Fix invalid-name pylint errors* Disabale temporarily* Fix dangerous-default-value* Fix typo errors* Fix ci errors* [CI] Apply linting rules to tensorflow tests* Fix dangerous-default-value* Fix ungrouped-imports* Fix boolean value of Tensor with more than one value is ambiguous* Fix 'NoneType' object has no attribute 'shape'* Fix boolean value of Tensor with more than one value is ambiguous* reformatted by black* [CI] Apply linting rules to caffe tests* Fix dangerous-default-value* Fix conflict & fix pylint error* restoreCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>	0
[TOP][Example] register pool, global_pool; add mobilenet example (#32)* register pool, global_pool; add mobilenet example* tests of pool and global_pool* use new API of runtime module* small fix	0
[DOCS][FRONTEND] Modify from_mxnet to also return params, update docs (#36)	2
[QNN] Optimize lowering for requantize and FixedPointMultiply. (#4798)* [QNN] Optimize lowering for requantize and FixedPointMultiply.* Add check for requantize scale gt 1.* Added test case.	3
update dmlc-core for security reason (#1584)	5
[TVMC] Adds ethos-u-vela dependency in the "tvmc" set of dependencies. (#9590)The ethos-u-vela dependency is required for tvmc, given we support"ethos-u" as one part of our tvmc targets, therefore, we need toadd this package to the optional set of dependencies that usersexplicitly ask when using tvmc.There is no impact for other sets of dependencies.	1
[TVMScript] fix typo for block syntax (#11407)	2
[TOPI] add argmax, argmin (#515)* add argmax argmin* remove coder saver	4
[RUNTIME] Fix TypeKey2Index when for root Object (#8547)* [RUNTIME] Fix TypeKey2Index when for root Object* Temp skip tsim tests	3
[TOPI] Add generic batch norm (#9694)* Add topi batch norm and tests* Handle none values correctly* Return correct nun outputs for onnx* Use moving var/mean and update tests* Add a test for batch norm folding* Fix comment* Format with black* Re-order test args to match interface* Call fold constant manually	3
Add minimal forwarding RPC server for host driven python execution on Hexagon (#9526)* Minimal proxy RPC server for hexagon. Functions by routing fromAndroid to Hexagon via QTI FastRPC calls. Interim solution untilHexagon on-device RPC server is ready.* Apply clang-format.* Fix build to support building alongside Hexagon Launcher.* Add readme.* src/runtime/hexagon/rpc -> src/runtime/hexagon/proxy_rpc* Added small refactors to hexagon test_matmul.py.* Add skipif on additional env vars.* Fix IOS build.* Rename USE_HEXAGON_PROXY_RPC and add tvm_options entry.* Add NDArray::Container deleters.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[CI] Fix the hexagon string (#5304)	0
[TEST] Checkin test docker and scripts (#48)	2
[Runtime] Make ADTObject POD container type (#4346)	1
[Doc][AutoScheduler] Improve hyperlinks in tutorials (#7167)* [AutoScheduler] Improve tutorials* fix lint* address comments	1
[TIR] Expose: `call_packed_lowered`, `call_cpacked_lowered` (#12425)Added the following operations in TIR:- call_packed_lowered- call_cpacked_loweredCo-Authored-By: yongwww <yongcale@gmail.com>	1
[microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests	3
[LLVM] Fix generation of LLVM intrinsics (#5282)* [LLVM] Fix generation of LLVM intrinsicsThe type list in the call to llvm::Intrinsic::getDeclaration is notthe intrinsic's signature, it's the list of overloaded types. Withoutthis fix, the updated unit test would cause the following error:TVMError: LLVM module verification failed with the following errors:Intrinsic name not mangled correctly for type arguments! Should be:llvm.ctlz.i32i32 (i32, i1)* @llvm.ctlz.i32.i1Special handling for llvm.prefetch, sig matching for overloaded ints onlyThe prefetch intrinsic returns void in LLVM, while it returns i32 in TVM.This case needs to be handled specially, because rule-based intrinsictranslation would cause invalid LLVM type to be created.Do the signature matching only for overloaded intrinsics. It's not neededfor non-overloaded ones, so this can save a bit of compile-time.* Include intrinsic name in the error message* Fix number of arguments for llvm.fmuladd and llvm.pow	1
[MetaSchedule][Test] Add unittests for DEP (#12071)	3
[topi] fix sparse dense schedule on cuda (#5803)	0
[Relay][TOPI] Resize 1D (#8346)* rename resize to resize2d* refactor resize_2d* Add resize1d op, normalize attribute names across ops* normalize resize3d to match the API of 1D and 2D* fix lint* fix relay tests from API change* refactor topi tests, docs* fix method naming in framework frontendsfix more frontend issues* refactor resize tests to reuse components, add more coordinate tranform modes to tests* add cubic resize reference kernel and tests, add relay tests for resize1d* fix pylint* fix test typo	2
handle likely in IRMutatorWithAnalyzer (#5665)	0
[7/10] Code generation for Pooling and Fully Connected via CMSIS-NN (#9531)Support for code generation of Maxpool, AvgPool and Fully Connected layers via CMSIS-NN	1
[UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.	5
[RUNTIME] Fix memory leakage of TVMByteArray (#4856)	0
Change const to used dtype if it is passed in (#7285)* Add fix and unit test for const autoconvert dtype.* formatting* Address review comment, casting input value to int32* Fix failing test* Augment unit test	3
[DataType] Add bfloat16 (#5601)	1
[Relay] use transform instead of ir_pass for CPS (#3485)	4
[Support] Add `parallel_for_dynamic` with dynamic schedules (#9056)* [Support] Add parallel_for_dynamic with dynamic schedules* Update parallel_for.cc* Update parallel_for.cc	5
[Refactor][Relay] Refactor Relay Python to use new FFI (#5077)* refactor relay python* revert relay/ir/*.py to relay* Address comments* remove direct access to analysis and transform namespace	4
[Metal] Correct handle warning info (#1140)	5
[microNPU] Change weights and command stream section (#9523)Move microNPU weights and command stream to .rodata.tvm- Add unit test test_ethosu_section_name()	3
[CMAKE] Automatically detect newly added source files (#9611)* [CMAKE] Automatically detect newly added source filesBefore this commit, newly added or removed source files were notdetected by cmake. This manifested either as file not found errors fromthe compiler (when files were deleted) or packedfuncs not being found(when files were added). This commit uses the CONFIGURE_DEPENDS optionof cmake's `file(GLOB)` function to ask the build system to check fornew files on every rebuild. Checking for new files adds a slight butnegligible overhead to each build, but is better than unexpected errors.* remove unnessesary configure_depends	5
update qemu install (#8518)	5
Numpy compatible dtype inference for `tvm.convert` and `tvm.const` (#3861)* numpy compatible type inference* update* try to fix* fix* try to fix* fix lint* Update nn.h* cast to int32* try to fix* fix again* retrigger ci	0
Fix generating types like float44 and float88 (#5722)	0
[BUILD] correction in topi dependencies        Change at 'topi/include/topi/' not recognozed by make.	1
[AutoScheduler] Misc update to hardware parameter and task scheduler (#7020)* [AutoScheduler] Mics update to hardware parameter and task scheduler* update* update* update* update* fix* fix* update* improve warning message* update* lint* update* update* fix* Apply suggestions from code review* trigger CI	0
Create a new parameter --cache-from in tvm/docker/build.sh, so that we can point to an image to be used as cache, from an external (#5173)script. * Adjusts documentation to provide information about new optional   parameter "--cache-from" * Includes --cache-from in the underlying "docker build" command   triggered by build.sh, when required	1
[RUNTIME][CRT] use macro to replace hardcode number (#6365)Signed-off-by: windclarion <windclarion@gmail.com>	1
[Runtime][PipelineExecutor] Refactor PipelineExecutor.py and Add cross compile support for pipeline executor. (#11133)* [Runtime][PipelineExecutor] Refactor PipelineExecutor.py add crosscompile support for pipeline executor.Current pipeline_executor and pipeline_executor_build stay in samefile, this caused that the the running of pipeline_executor need supportfrom tvm and relay that is not available on edge device in which a runtimelibrary only can get build.Pipeline executor used PipelineExecutorFactory to store the pipelineconfiguration and export the pipeline executor library, but the currentexport not support the cross compile, add related logic.* fix ci issue.* use runtime to replace relay and leave the export_library inpipeline_executor.py.	1
[RELAY/OP] Gradient of relay level1 ops (#2633)	5
[Relay] Free Variables (#1786)	5
[RUNTIME]fix unused-value warning (#5140)	2
Ensure AOT passes all intermediary storages to function calls (#9064)* Ensure AOT passes all intermediary storages to function callsThis iterates over the return storage IDs rather than just using thefirst one to ensure all of them get passed to subsequent calls.Fixes #9036* Re-introduce multi sub graph AOT test	3
[PASS] Add GPU IR verifier (#1296)	1
[TIR] Properly initialize PRNG seed when copying schedule (#10806)* Make Schedule::Copy non-const, fork RND seed in Copy* fork seed in traced schedule copy toocommit eeb4a6d4b34909822ea5d56488afd11f254e53a9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Mar 29 06:39:38 2022 +0900    add more commentcommit 183b4cfe5d7938d5e440a9d77b7e8c3871544966Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Mar 28 10:04:12 2022 +0900    skip flaky vk testcommit c19ecc17afc8ee1b54aa2260bffb4e1d431ab429Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Mar 28 07:34:25 2022 +0900    move intrin decl for vector typecommit 3dd7f045f791b805012227ab4ee866995cc5297dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 09:40:29 2022 +0900    disable default post processor, tuning now works with compactness checkcommit 2f6fdae675975e2bd95a086dce8a88d9f267746dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 08:08:35 2022 +0900    more commentcommit c7ebfa904367885442f928c0cecf875190341930Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:42:46 2022 +0900    add commentcommit 78400bad77f5201b3cebfb8a7fee0642adead060Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:40:28 2022 +0900    disable tuning test for nowcommit a33243fbf91863f0a834505cd4936c0fff228603Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:30:03 2022 +0900    remove annotation check in ir comparatorcommit 105f98cc76081d46dddbda47dba2578a25cfadb2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:28:36 2022 +0900    clean upcommit 8aa16f209ee709375d90f2c3a5883a47df6ce104Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:15:24 2022 +0900    Add test* add test case that hangs without forkseed	3
[Relay][Doc] Correct bad formatting and typos in Relay operator addition doc (#1833)	2
[TOPI] Setting up AutoTVM template for Intel Int8 conv2D (#3955)	1
[TE] Correctly generate buffer binds with axis separators (#10819)In SchedulePostProcToPrimfunc, when the axis separator attribute ismoved to the buffer properties, it doesn't update buffers that are inthe buffer bind scope.  This occurs if `Stage.tensorize` is called fora stage whose layout transformation includes `te.AXIS_SEPARATOR`.	5
[CUDA] Improve local_response_norm schedule (#8946)* Improve cuda lrn schedule* fuse reduction and the next elemwise kernel* remove cpp schedule* fix* fixed unintended revertCo-authored-by: masa <masa@pop-os.localdomain>	4
[PYLINT] Disable consider-using-get (#2654)	1
Truncate function name (#2863)	1
[MAINTAINER] Add Nick Hynes to CodeOwner of SGX (#1128)	1
[PYTORCH]Minor bug fixes (#5683)* [PYTORCH]Minor bug fixes* Review comment fix, testcase added* Added testcase for bert model	3
[LLVM] Use llvm::Align with LLVM 11+ to avoid warnings (#5264)LLVM 11 is introducing a separate class to represent alignment.The functions in IRBuilder that create aligned loads and stores,and which accept the alignment as an unsigned value have beendeprecated (and now cause warnings to be emitted).	2
[refactor][relay pass] Separate analysis and transform passes (#5035)* [refactor][relay pass] Separate analysis and transform passes into different subfolders* remove pass folder	4
Fixes for tensorize in Windows build to expose TensorIntrin::make and search clang.exe (#1896)	1
Add two possible missing visit of let stmt in lowering (#11079)Refer to the issue in https://github.com/apache/tvm/issues/10831#issuecomment-1086287433	0
check attach_stage & group in schedule.copy() (#139)	5
Fix serialization issue (#2263)	0
Rename gpu to cuda, and bump dlpack to v0.5 (#8032)	5
[docs][tvmc] Fix ResNet50 model URL (#12040)Fix the ResNet50 Models in both tvmc tutorials so that the commandssuggested will work fine.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>Co-authored-by: Liam Sturge <Liam.Sturge@arm.com>	1
[Hexagon] Update hexagon API build instruction and cleanup hexagon_proxy_rpc (#10068)* Fix hexagon api build and Update Readme* Cleanup hexagon_proxy_rpc* Target Hack* Remove hack* address @cconvey comments* remove the rest of proxy rpc	4
[REFACTOR] Code base refactoring (#5)	4
[PASS] Allow compact checking when strides is available (#669)* [PASS] Allow compact checking when strides is available* remove assert compact	3
[Graph Executor Debugger] Fix parameter dump (#7903)* remove debug mode* reformat* format* address comments* add single call for all layers* fix test* revert* address comments* address comments* fix rerun node* fix error* format* raise error on array()* fix java* Revert "fix java"This reverts commit c4cf952dbc5c9c32d65ef0ca05d6ecbb5c06d5aa.* bring back for java api* fix error* cleanup* format* rm redundancy* add last execution track* trigger build* address comments* format* fix name overlap* Trigger Build* trigger build* trigger* trigger	0
[ThreadPool] Solve thread transitions issue (#4344)* [ThreadPool] Solve thread transitions issue* Use pthread_atfork to avoid master thread affinity be derived by child.* Code Format* comment of exclude_worker0_* set full cpu affinity* Redundant blank line* CPPLint* CPPLint namespace* CPPLint* Fix the wrong logic of bind master thread.	2
[Target][TVMC][UX] Avoid creation of dummy Target objects in TVMC (#9662)* [Target][TVMC][UX] Avoid creation of dummy Target objects in TVMC* nit fix	0
[PASS/OP/REFACTOR] IRDeepCompare, isolate computeop part, allow fuzzy bind (#218)	1
[hexagon][testing] nonrandom tests (#12164)Add support for populating unit-test input tensorswith values other than random.	3
Revert "[OpenCL] Fix type casting error (#11021)" (#11035)This reverts commit 8aafe5b1095b8c1024e826f6a8c2114606288182.	4
[MXNET] LRN support in MXNET frontend (#1520)	1
Conditions updated to cover better user scenarios[Re-raised] (#5043)* Conditions updated to cover better user scenarios* [1] New test case added* [2] New test case added* [3] Proper variable name used* [4] Review Comments handled* [5] Review comments handled* [6] Review comments handled	0
[Relay] Algebraic data types (#2442)* First pass on ADTs* Add doc string for tag field* Visit constructors in TypeVisitor for TypeData* Add to description of type call* Add type call to type solving and unification* Make type mutator for typecall consistent with others (only create new node if there's a change)* Ensure kindchecking can handle type calls and typedata* Fix bad nesting in module constructor* Correctly construct call in typecall test* Add call override for ordinary vars (do we want this?)* Remove generalization hack from type inference because it was breaking ADT constructors* Check that there are no free type vars in exprs after inferring type* Free var checks need module because of ADT constructors* Typecall test can't have unbound type var, make it global* Uncomment tmap test and remove comments about failing to infer ret type; those work now* Put in dummy visits for ADTs in graph runtime codegen to placate pylint* Fix Relay type infer test module constructor* Mark override for TypeCallNode in type solver* Ensure free vars check treats patern vars as bound* Run interpreter in more ADT test cases* Refactor kind check to return the kind, like typechecking* Fix invalid typecall in test* Add kind check to type inference, do not use nulls in func_type_annotation()!* Redundant whitespace* Make TypeData a separate kind* Make ADT handles a separate kind too, document calling convention better* Remove nats and tree from prelude, move to test, document prelude* Restore and document nat and tree to prelude, add more tree tests* Add alpha equality tests for match cases, fix variable binding bug* Add more kind check tests for ADTs* Add more tests for finding free or bound vars in match exprs* Add unification tests for type call* Update main() for alpha equality tests* Add simple type inference test cases for match exprs and ADT constructors* Add more ADT interpreter tests* Allow incomplete types when typechecking match cases* Type inference for pattern vars should use the type annotation if it's there* Two more specific test cases for ADT matching* Add option ADT to prelude* Fix broken reference to kind enum* Fix rebase snags* Do not attach checked types to constructors* More docstrings for module fields* Use proper wrapper for indexing into module type data* checked_type for constructors is not populated* Expand type call docstring* Rename PatternConstructor con field* Use error reporter for pattern constructor case* Condense error reporting in kind check, use error reporter* Expand docstrings and rename ADT fields* Rename 'option' ADT to 'optional' for consistency with Python* Add various list iterators and utility functions to prelude* Add smoke tests for new iterators in prelude* Add concat to prelude* Add smoke test for concat* Correct docstrings in prelude* Ensure that type defs are written in module initialization* Various requested renamings* Correct rebase snags* Add kind check tests for ref types* Update the main() for kind checking tests	3
[TUTORIAL] Fix Some Failed Tutorials of The Issue #6453 (#6534)	0
Improve CUDA conv2d_transpose_nchw (#4762)- combine pad and dilate;- fix for the issue https://discuss.tvm.ai/t/compile-error-for-cuda-target/4164- fix for the issue https://github.com/apache/incubator-tvm/pull/4472	0
Use ewise schedule for broadcasting (#460)	1
[RUNTIME][OPENCL] clFinish before releasing memory (#2737)	5
allow constant value let binding in script (#11115)	1
clarify NNVM’s LLVM requirement (#2117)	1
[MetaSchedule] Migrate MemoryDatabase to C++ (#12514)This PR migrates the existing MemoryDatabase, which is implemented inpython at the moment, to C++. The original intent of having an in-memorydatabase that does not persist on disk is merely for testing, but astimes go on, we found it useful in production workflow, and thus decidedto migrate it C++ for potentially better performance.	1
[BUILD/CODEGEN] Allow combine multiple functions in build stage. (#169)* [BUILD/CODEGEN] Allow combine multiple functions in build stage.* Enhance code module* fix compile	0
SSD support in NNVM (#1214)	1
[Bugfix][Frontend][TF] Fix incorrect calculations in tf SLICE (#4518)* fix formula for calculating end indices when size[i] == -1* add a test case for size[i] == -1* discard expanding dimension of begin_value & end_value since  it is needed only if you pass them as scalars not as tensors.* discard 'slice_tensor' variable so that implementation matches  the tf parser pattern	4
[Community] @Mousius -> Reviewer (#8764)* adding Mousius to reviewers,  name update for Siva Reddy* making Siva's name consistent	1
[Intrin] Adding a few missing math intrin  (#5011)* [intrin] exp2* [intrin] exp10* [intrin] log2/10* [intrins] exp10* [test] math intrin	3
Add Item list to community guide (#1264)	1
fix topi c++ conv2d_nchw lambda expr issue (#3570)	0
Option to specify alternate directory to output build to (#6016)This is useful when you would like to manage 2 separate builds in the same tvm tree. You can specify a build directory when using make by adding OUTDIR=alternate-build-dir.Change-Id: I3efed1135343f3903007115ce5dd683ef7bd9e8c	4
[tvm4j] Java runtime README (#391)	1
Fix wrong n_trial number in autotvm tutorials' progress bar (#4070)if n_trial is larger then config space.	5
[Hexagon] Add Hexagon-specifc timer to enable using `time_evaluator` (#10714)* Add stub* Hexagon profiler is called* add HAP call* move to hexagon_common.cc	4
[Docs] Update links and fix typos in docs and readme (#7965)	2
[QNN][TFLite] Parsing QNN Add op. Adding MobilenetV2. (#4142)	1
[BYOC-DNNL]rewrite downsize blocks for rensetv1 to get better performance (#11822)* rewrite downsize blocks for rensetv1 to get better performance* fix lint	0
[Reduction] Fix cross thread redunction (#5551)- The predictions were not correctly applied after transformation.  This leads to normal reduction itervar appearing outside of the loop,  which is undefined. See detailed comments.Signed-off-by: Wei Pan <weip@nvidia.com>	0
[Hexagon] Add top-level CMakeLists.txt for apps/hexagon_launcher (#11006)	5
Fix master CI due to stale push (#1943)	0
[COMMUNITY] @eqy -> Committer (#2311)* Add Eddie to committer* Fix order	0
Fix typo in Evaluate inference time cost code (#2542)	5
[RPC] LocalSession to provide RPCSession back by local env (#1102)	1
[TIR] Add preserve-unit-iters (#11585)	1
[Adreno] Add markup pass of relay tensors for static texture planning (#11878)* [Adreno] Add static texture markup relay passCo-authored-by: Chris Sullivan <csullivan@octoml.ai>* lint check* Remove hardcoded texture limit, check through target options* fix cpplint* Add winograd into annotation pass* fix clang* Remove extra call of PlanDevice in OptimizeImpl* Remove one more extra call of PlanDevice in OptimizeImpl* Fix/add scopes for static texture planning tests* Remove test_2conv2d as duplication of test_plan_device_issue* remove comments in test_residual_block* address review comments* fix black hits* Add textures test descriptions* Address PR commentsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>	5
[NNVM] Recover reduction behavir, fix CI (#1740)	0
[CMake] add support for find_package (#10097)* removed include header path, which are invalid* 1. added target tvm in a cmake export group2. added cmake package config file* added Threads as public dependency* changed temp config file name for better understanding	1
[Relay] Add printing for ADT Type (#3030)* Update pretty_printer.cc* Update pretty_printer.cc	5
A couple of fixes for GEN (#2593)	0
[TOPI][batch_matmul] Allow cblas batch_matmul implicit batch_size broadcast (#8250)* Allow cblas batch_matmul implicit bcast* Add cblas batch_matmul bcast when batch_a=1	1
[Relay][docs] Details on comp. graphs in Relay dev intro (#2324)	2
Arm(R) Ethos(TM)-U NPU BinaryElementwise operators support (#9442)This commit adds support for the binary elementwise primitive operators for the Arm(R) Ethos(TM)-U NPU and includes a few minor rewording changes.	4
[VTA] TSIM improvements and fixes (#3505)* add tsim init function* add sim device* test wait and resume* launch simulation thread from DPILoader* add VTASimDPI module to handle all simulation related stuff* test tsim init* move exit to simdpi module* update vta driver* add chisel DPI module* get back simshell* update vta to support dpi sim* update unittests* add tsim to integration-conv2d test* run resnet on tsim* remove max-cycles* match tsim counters with sim counters* use env in simulator to switch between sim and tsim* update unittest* rollback conv2d test* update resnet* add stats to matrix multiply* add stats* print stats after assert* update other tests* add stats to gemm* add return and remove unused libs* add missing arg* return lib* update comments for linter* add more comments to VTASimDPI module* remove trailing spaces* remove trailing spaces	4
[TIR][Arith] Avoid assigning range of possible values to integers (#11859)Previously, in `ConstIntBoundAnalyzer`, entering a conditional such as`if 2==0` could result in the expression `2` being treated as having aknown value of zero within the body of the conditional.  Evaluatingthe range of expressions using `2` in the body of the conditionalcould result in exceptions being thrown, such as evaluating `expr / 2`while setting `2` to its maximum value of zero.This issue was present for conditions with inequalities for some time,but was introduced for conditions with equalities inhttps://github.com/apache/tvm/pull/11524.  Both types are resolved inthis PR.	0
[FRONTEND][TENSORFLOW] Enhancements. (#1923)* [FRONTEND][TENSORFLOW] Enhancements.* Generalize the shape with explicite argument.* Supported entire range of mobilenet_v2 models.* Cast op updated to latest tensorflow.* Documentation updates.* CheckNumerics op handling without exception.* Test data from tensorflow official releases.* * CI error.* * self review* * Enhanced reshape handling.* * docs.* * tutorials* * review comments.* * review.	2
[Relay][Quantization] Extend FakeQuantizationToInteger to more ops (#8241)* support scalars in quantize and requantize* Add affine type support for ops with multipe output, use it in concat, move to header* support new ops, refactor tests* add more binary opsfix pylintfix blackblack broke pylintoops on black* fix a typo in a branch and add a test that hits it* improve comments	1
[Frontend] Onnx improvement (#165)* fix recently released layers* fix fc layers with partial infer_shape	5
[CMSIS-NN] Fix typo in EmitPool2D (#11702)Co-authored-by: Philipp v. K <phvankempen@gmail.com>	2
[Hexagon] Refactor test scripts (#11026)* Refactor hexagon test scripts* rever removing the script	4
[VTA][TSIM][Build] Towards TSIM CI testing (#3704)* building TSIM specific library along with fast simulator to quickly switch between dlls* cmake controlled TSIM libraries* always build tsim driver in either simulation modes* build DLLs based on CMAKE flags* updating the jenkinsfile* small restructuring* reducing the cmake flags* update instructions* reverting to 3 flags* update Jenkinsfile* adding new line* enabling TSIM unit and integration tests* fix description* temporarily disabling task_python_vta tests in CPU Build stage* move CPU tests in unit test stage* stage  reorg* better make* disabling TSIM tests for now* reverting some restructuring* fix	0
[Relay] Add new IR pass CombineParallelDense (#3862)* Refactor to create abstract ParallelOpCombiner* First draft of CombineParallelDense* Begin to work on tests* Test* Refactor to move out more common code* Clean up* Fix* Remove statics* fix wording* Start to add combine_parallel_op_batch* Resolve PR comments* Resolve PR comments* dummy change to retrigger CI* Change special case from bias_add to add* Revert special case change* Ignore units check* dummy change to retrigger CI* dummy change to re-trigger CI* Improve docs* Update docs* Update docs	2
[MetaSchedule] Allow Easy Logging Level Setting (#11305)This PR allowed users to set logging level without giving a logger config. Previous implementation hard-coded `logging.INFO` as the default logging level and requires a logger config to change it. Now the logging level and handlers can be inherited from the current `tvm.meta_schedule` logger setting.	1
[TensorIR][M1c] LCA detector (#7848)Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
Add dilation to MaxPool2DAttrs Rust bindings (#9215)Gets Rust bindings back in sync. Dilation must have been added to MaxPool2DAttrs on the C++ side, without it being added on the Rust side. There should probably be tests to catch this!	3
[AOT] Introduce checks for return values from operators (#10424)This matches the lowering of `call_cpacked` which checks only for anoperator return of `0` in the main flow:https://github.com/apache/tvm/blob/bd14a4d36e0d364ef9bd34b2ee96cc09ce64d4b3/src/target/source/codegen_c_host.cc#L207-L231This replaces:```c(void)tvmgen_default_fused_add(x_buffer_var, y_buffer_var, output_buffer_var);```with:```cif (tvmgen_default_fused_add(x_buffer_var, y_buffer_var, output_buffer_var) != 0 ) return -1;```when AOT generates the C output.	1
[CI] Enable ANTLR in CPU env (#2548)	0
[uTVM] Reset target and wait for runtime initialization on connect. (#5499)* This ensures a clean runtime environment for tuning and evaluating   tasks. * This patch assumes that the code flashed to target executes the ARM   BKPT or RISC-V EBREAK instructions after startup.	4
Add PReLU support to mxnet frontend (#1249)	1
[DOC] Add Build TVM Runtime on Device section (#372)* [DOC] Add Build TVM Runtime on Device sectionAdd Build TVM Runtime on Device section* Add echo USE_RPC=1>> config.mk in code-blockAdd echo USE_RPC=1>> config.mk in Build TVM Runtime on Device section* [DOC] Fix small problem	0
Fixed div by zero core dump. Fixed rounding intrinsics on int crash (#5026)	0
Some minor documentation issues rectified (#450)	0
[Hotfix][Testing] Wait for RPCServer to be established (#9150)	3
[Relay] Align strided slice shape functions (#10155)* fix static strided slice shape func for out-of-bounds negative stride slicing* Trigger CI* Trigger CI	0
[TIR][Pass] Remove-Weight-Layout-Rewrite-Block (#11870)	4
[Relay-TFLite] FP32 and Quantized Object Detection Model (#5479)* TFlite e2e FP32 Object detection model* Fix test* [Relay-TFLite] Quantized activations* Flexbuffer parsing* Lint* Relaxing checks.* Github reviews* commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-34-212.us-west-2.compute.internal>	3
[QNN] Conv2D with dilation support. (#4796)	1
Remove duplicated functions in relay_integration (#2370)	1
[Relay][FastMath] Relay pass to use fast exp/tanh (#4873)* [Relay][FastMath] Relay pass to use fast exp/tanh* Adding required_pass to the tests.* FastMath test changes.	4
[COMMUNITY] ASF mentors (#2906)	3
Note about CodeGenC's Range of Use (#866)	1
[CMSIS-NN] Moved all asserts in tests under a single utils function (#10148)	1
[MetaSchedule] exposed method: TuneContextNodeInitialize (#11576)I exposed the initialize() method for TuneContextNode on the C++ side and added a corresponding method to TuneContext class on the Python side, so that we do not need to call initialize_with_tune_context for every scheduling rule.	5
Revert "[Torch, QNN] Add support for quantized models via QNN (#4977)" (#5013)This reverts commit fc7f0783940c362bf48cd46817956381196201e2.	4
[TOPHUB] fix x86 backend after introducing dilation (#2129)	0
[Tutorial][Frontend] move from_keras tutorial to frontend (#2479)* [Tutorial][Frontend] move from_keras tutorial to frontend* remove tutorial/nnvm/from_keras.py	4
Fix use of fallback AutoTVM knobs in default scheduling (#8707)* Fix use of fallback AutoTVM knobsPreviously knob values depended on order of explicit cfg update and cfg.define_splitcalls in fallback mode* Add test for define_split with fallback defined values	3
[TIRScript] fix parse StringImm value in for loop annotations (#9755)* fix parse strimm value in for annotations* flatten buffer allow runtime.String attr value* remove unused import* rebase and ensure flattened attr order	2
[LANG/GPU] Cross Thread Reduction (#79)* [LANG/GPU] Cross Thread Reduction.* Fix doxygen error* Upgrade verilog testcase to new one	1
[TOPI][CUDA] minor change on assert statement in conv2d_NCHWc_int8.cuda (#8554)* [TOPI][CUDA] minor change on assert statement* [TOPI][CUDA] reformatting	3
[RPC] android process isolation/watchdog (#1387)	5
debug operator--() in include/tvm/node/container.h (#7461)	1
[RPC] More robust tracker protocol (#1085)* [RPC] More robust tracker protocol* fix normal rpc	0
[RUNTIME] Add device specific timers (#7472)	1
[RPC] Prefer IPv4 between IPv4 and IPv6 (#7013)This change fix problem with version of IP protocol on MacOS.  Previousthe `rpc_tracker` and `query_rpc_tracker` were not able connect to eachother with default hostnames.The root cause was in method `socket.getaddrinfo`. In `rpc_tracker` thedefault hostname is "0.0.0.0" and `getaddrinfo` returns IPv4 type. In`query_rpc_tracker` the default hastname is "localhost" and`getaddrinfo` on MacOS returns IPv6 type. Note: on Linux both have IPv4type.These tools worked by different protocols and this is why`query_rpc_tracker` wasn't able connect to `rpc_tracker`.Now we will prefer IPv4 type. And both `rpc_tracker` and`query_rpc_tracker` will use the same version of protocol.	1
[QNN] Register a bunch of unary elementwise ops (#10086)* 0;276;0cinitial commit* register a bunch of ops* unary ops* add a bunch of tests* 0;276;0crefactor tests* add tests to qnn* comments on macros* add back in log to pattern utils* update floating point func description* proper creating of calls to quantize and dequantize* fix lowering process for using dequantize and quantize ops	1
Add Operation class	1
[Fix][Relay] Remove schedule register for nonexisting log1p op (#4425)	2
[microTVM] Refactor uTVM to microTVM (#8283)* refactor* rename utvm_rpc_server.h* rename file* rename* rename file* rename* variables* directories* format* trigger* more refactor* one more* format	4
[DOCKER] Update ci-gpu to include NNPack (#2846)	5
[MetaSchedule] Arithmetic analysis (#10403)This PR changes the normal form of the affine detector and supports a single var predicate. It also enhances ModularSet detector to enable floor mod patterns.	0
[skip ci] Fix missing pack_lib in Jenkinsfile (#9924)This was inadvertently removed by #9554Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[GRAPH] Include default metadata description in graph. (#2770)	5
fix annotation of tir generic (#9119)	0
[Metal] Fix block launching parameter (#1219)	2
[ci][docs] Don't delete old versions when checking out docs (#11612)We don't have a good way to tell if a file was deleted or not in a docs update, so currently we delete the entire `docs/` folder and replace it from the build. However, this includes old version docs that aren't build in the normal docs build. This excludes them from the deletion so they stick around between updates. We'll have to revisit this list at each release but it should be a simple update.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
prevent casting handle to other types (#9114)	0
Fix x86 Conv tuning tutorial (#1932)	0
[COMMUNITY] @junrushao1994 -> Reviewer (#2463)	3
[microTVM] Zephyr: RISCV support for Zephyr QEMU RISCV-32/64 (#7804)* working on qemu* debugging* riscv hacks* config added* change target platforms* fix merge* debugging issue with zephyr 2.5* cleanup* working on qemu* debugging* riscv hacks* config added* change target platforms* fix merge* debugging issue with zephyr 2.5* cleanup* testing* pass riscv64* fix merge* small fix* update vm_name* add zephyr version* add comment for riscv32 issue* remove debug messages* cleanup* cleanup* change workspace* fix zephyr version* cleanup* change to symlink* fix flag* add comment* lint check* lint fix* fix format* rename debugger* rework argsCo-authored-by: Mehrdad Hessar <mhessar@octoml.local>Co-authored-by: Andrew Reusch <areusch@octoml.ai>	5
[Frontend][TensorFlow]Fix TF Dynamic input shape (#5825)* Fix TF Dynamic input shape* Remove warning* Add test	3
Add support for quantized multiply to Relay (#4141)This patch adds multiply operator for quantized tensors.The details of the quantized multiplication are outlinedin the code.This builds on pull request 3927 and includes the changesAnimesh mentions in the comments on that request.Change-Id: I555715b53d0266a91d5c03dc3dfe8fc31e7ce4e1	4
[OpenCL] Refactor cl_program generation (#7834)* Refactor OpenCL runtime module to build separate cl_programsfor each kernel. This can avoid pathological bugs in thevendor specific OpenCL compiler that may be triggeredwith large programs.* clang-format* Remove check on program size when deconstructing.* Refactor into SplitKernels method.* Limit number of loops for kernel parsing* Add return doc for SplitKernels per CR.	2
[ONNX] Fix more upstream tests (#7842)* fix unsqueeze test* fix dynamic strided slice with negative indices* add Shrink importer* fix selu defaults* Implement Hardmax* add a comment to the test* Fix typo	2
Add travis test scripts (#35)	3
[CI] Update deps for chisel (#4675)	5
fix rpc tutorial (#818)	0
[TensorIR][M2a] Parallel, Vectorize, Bind & Unroll (#8716)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[MetaSchedule] Schedule Rule: Parallelize-Vectorize-Unroll (#10033)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
adding Alexey (#12090)	1
[microtvm][RVM] Refactor Arduino/Zephyr into one RVM (#12023)	4
[DOCS] Update code review guideline (#8999)This PR updates the code review guidelineper community discussion.	5
[REFACTOR][RUNTIME] Update NDArray use the Unified Object System (#4581)* [REFACTOR][RUNTIME] Move NDArray to Object System.Previously NDArray has its own object reference counting mechanism.This PR migrates NDArray to the unified object protocol.The calling convention of NDArray remained intact.That means NDArray still has its own type_code andits handle is still DLTensor compatible.In order to do so, this PR added a few minimum runtime typedetection in TVMArgValue and RetValue only when the correspondingtype is a base type(ObjectRef) that could also refer to NDArray.This means that even if we return a base reference object ObjectRefwhich refers to the NDArray. The type_code will still be translatedcorrectly as kNDArrayContainer.If we assign a non-base type(say Expr) that we know is not compatiblewith NDArray during compile time, no runtime type detection will be performed.This PR also adopts the object protocol for NDArray sub-classing andremoved the legacy NDArray subclass protocol.Examples in apps/extension are now updated to reflect that.Making NDArray as an Object brings all the benefits of the object system.For example, we can now use the Array container to store NDArrays.* Address review comments	1
[TVMSCRIPT] Add type definition for preflattened_buffer (#10550)* [TVMSCRIPT] Add type definition for preflattened_buffer* argument should be buffer	5
add reviewer (#3755)	1
[Strategy] Add group_conv2d_nchw_int8 in cuda strategy (#8167)* add group_nchw_int8.cuda* fix* fix style* fix style* fix style* fix style* fix style* fix styleCo-authored-by: wangyucheng <wangyucheng@sensetime.com>	1
Change Schedule Array constructor to static make method (#170)* Change Schedule Array constructor to static make method* Add CreateSchedule* Add doc* Change CreateSchedule to create_schedule at cpp side	1
[TRT] Add check to support split op with TRT 5.1.5+ (#11154)	1
[PASS] Refactor build config, allow implicit unroll pragma (#167)	1
[TOPI]Add op argwhere (#3994)* Add op argwhere* Move shape func to _algorithm.py* Add lint rule* Raise exception if rank is not supportted* move argwhere to transform* Add argwhere example* Fix lint* Add 1-d support* cleanup* Add more dtype support* CR comment* Improve error message* Docs* raise exception	2
[NNVM] Fix dtype of output of mean. (#2334)dtype of count is the same as dtype of inputs[0] when created, but its type may  change when multiplied by inputs[0]->shape[i]. Which causes dtype of  output is not same as dtype of input.	1
Generate JUnitXML from pytest (#7407)* Generate JUnitXML from pytest.* address tkonolige comments	1
[Relay][Op] Remove reverse attribute from reshape and reverse_reshape operators. (#7086)	1
[ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs"* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[microTVM] Zephyr: Fix gdbserver_port option (#10678)Currently 'gdbserver_port' option is not working properly and if it'spassed to the API server it takes not effect, being ignored silently bythe server, hence no debug port for GDB is created when QEMU runs.This commit fixes it by correctly setting 'TVM_QEMU_GDBSERVER_PORT' envvariable so when Zephyr runs QEMU to create a virtualized board the GDBport is set correctly to the port passed to the Project API server.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	4
[Relay][TensorFlow Frontend] SoftPlus Sqrt (#3187)	5
[DOC][Relay]: Add API docs for Relay. (#1750)	2
[BUILD] Switch to CMake only Infra (#1254)	5
Fix misprint (#2223)	0
init the concat tensor with 1s and then slice them away (#7666)	5
[Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>	5
Fix several typos in pytest_target_parameterization.rst (#9447)	3
[IR] Try to improve nms and get_valid_count (#3282)* improve nms* add back get_valid_count syncs	1
[Rust] Add rust runtime to CI (#1851)	1
[RUNTIME] Better scalability for multi-thread parallelization of CPUs (#971)	1
[TOPI] update c++ pool and softmax (#905)* update c++ pool and softmax* clean up reduce axis	4
[FRONTEND][ONNX]LRN support for ONNX (#1518)* LRN support for ONNX* [ONNX] Updated lrn testcases	3
[runtime-hexagon-rpc] more Hexagon/Android logging (#10767)- Alter `android_bash.sh` to meet the runtime conditions needed  for FARF logging.  (Note that FARF logging is also governed  by certain preprocessor definitions.)- Alter `android_bash.sh` so that any stdout/stderr emitted  by `tvm_rpc_android_server` is saved to a log file  (`tvm_rpc_android.log`). Previously that output was simply  lost.	2
[APPS] Add logging to the bundle. (#8115)	2
[FIX,TOPI] Default to inlining fused operations for conv NCHWc int8 (#10682)Inlining fused operations used to be the default and performs better onx86.	1
[MetaSchedule] Bug Fix for Relay Integration (#10534)* Bug fix.* Fix tune relay script.* Remove debug info.* Retest CI.* Add regression test.* Remove comments.	4
fix bug: KeyError, can't find some parameter key (#12211)Co-authored-by: woobinw <Wubin.Wu@imgtec.com>	2
tvmrpc: Fix includes (#2825)	0
use an empty module for fold_constant (#8208)	1
[RELAY][FUSION] Enhance fusion rule that starts from elemwise and broadcast (#2932)* [relay][bugfix] fuse injective to elemwise and broadcast* enhance fusion for prarllel injectiveOD* check if tensor in schedule* fix codegen* fix lint* update* lint	5
[COMMUNITY] Mehrdad Hessar -> Reviewer (#9366)	3
[TUTORIAL] Introduce frontend folder (#2457)	5
[Relay][Frontend][TFLite] Add parser support for logical operators (#4642)* [Relay][Frontend][TFLite] Add parser support for logical operators* Add parser support for logical_and, logical_or* Add boolean dtype as a valid tensor type* BOOLEAN dtype is supported only from tf 1.15  so logical ops work only in that and newer versions* Logical_not is ommited since tflite can't convert it -->  throws errors for addv2* Add TFLite vesion check in tests for logical ops* Check is added because of boolean dtype lack of support	1
[RUNTIME] Hexagon driver for offloading kernels to simulator (#5492)* [RUNTIME] Hexagon driver for offloading kernels to simulator* Add sim_dev as external project when building with Hexagon/sim support* Change target CPU for sim_dev to v60	1
[DLPACK] Support the new python array api with DLPack (#7993)* [DLPACK] Support the new python array api with dlpack* Fix lint	0
Add map container and tests	3
[NODEREF] Introduce named attribute system. (#1618)	5
update nnvm.runtime to tvm.contrib.graph_runtime (#41)	1
[ADDON] Allow piggy back nvcc compiler and code (#35)	1
[Relay] Added Merge Composite pass (#4771)* [Relay] Added MergeComposite passThis pass allows for patterns to be wrappedin a function marked with 'Composite' and acomposite function name. This is intended to beused with the external codegen for the cases wherean external operator maps to multiple Relayoperators. In that case, the mapping can be expressedas a pattern and assigned a name.For more information on this pass and its motivation,see the RFC:https://discuss.tvm.ai/t/rfc-external-codegen-defining-composite-relay-operators/5470Change-Id: Icb1b803a9f0ac57c529143200228f3bb5793afc0* [Relay] Merge composite testsAdded tests for the merge_composite pass.Change-Id: I1728b4a05b0c1c36140a40f1afe028fde62185dd* Merge composite additional testChange-Id: I9bc7d6053c575e9468ac5abc31214c6ad8507e46* Support priority order in merge_compositeThe order in which the patterns are matchedwas currently random as an unordered_map wasused to store the pattern table. This usesarrays instead so that a distinct priorityorder of matching can be defined. Additionaltests have also been added to verify thisbehaviour.Change-Id: Ief347df4262639138d5d9d7c8cee7ef233af7b56* Improved merge composite docsChange-Id: Ie3a72045ecc3f13ad3c302fbdf192b7296a306a8* Removed unused variableChange-Id: I7814d5fde368ffaf1b3d6d806060c774c7720364* Remove unnecessary op checkChange-Id: I38e78d2acd5b86cb8e837be72ff9d72cd10bcf33* Improve styling on composite function creationChange-Id: I37add1c3134e0b5d5085fe1eb9daf8e06890fa8c* Comment rewordChange-Id: Ie05872dcbbe0c3e1190b0597083b9a64e6b66c66* Stylistic changes to avoid std::moveChange-Id: I43a93995bbf10530399900c992aa99dd4ae4575f* Relax a check in ExtractPatternChange-Id: I0faef77a66c55f83f09e6e47c561ffaea63dedfa* Remove new lineChange-Id: Ifdd02c12087a7e1a0a9b54825669bc0de8f13c3d* Removed MatchPattern from MergeCompositeThis is not necessary now that ExtractPatterncan fulfill the same purpose.Change-Id: I14dc020afa8e50f2df4c0a2efb88a011987f8196* Removed a new lineChange-Id: I8b50f0c9069aa1bcaccbe68eb421031f01a64842* Improved docs for merge compositeChange-Id: Ib1959a35c856e7ea5639de2e4ef314a54f44caf5* Fixed free vars in testChange-Id: I2b7f273db275964ec0e9820560663f0808adee79* Handle case where root arg might not be a callChange-Id: I4eeea3ce723d3ba337d110dcc690377daebe8626* Removed blank lineChange-Id: I07f5392c0e95cfe3cfa5c333703cc6f82d6034fb* Change to CHECK_EQChange-Id: I5c5d62d3cd57f72508b30b926f72091ae6f0d1cc* Revised a conditionalChange-Id: I23a7897ca15a7cd076db5039dc653a4b8c27e803* Improved doc stylingChange-Id: I377f0a1c1ac70f3b8d7584b0c49bddc8c6c134ef* Fail extraction if vars conflictChange-Id: I78e36d805e8ed6b55e61d490212a967c857554a4* Added further merge composite testsChange-Id: Ib1d800409fca4c1834c7fe0cab5a26ab99a26820Co-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>	1
[CUDA][CodeGen] Fix cuda codegen's fp16 inf literal (#12581)* Fix cuda codegen's fp16 inf literal* add relay testcase	3
[Relay][Frontend][TFLite] Add constant input support for elemwise ops (#4666)* [Relay][Frontend][TFLite] Add constant input support for elemwise ops* modify in tflite.py	1
Add driazati to triagers. (#11004)- This is to help with work on things like the merge bot and issue triage.	0
[microTVM][Zephyr] Enable RISCV Tests on QEMU CI (#9325)* add riscv32* add riscv64* fix url	0
[MetaSchedule] Extend tune_tir to support tuning of specific blocks. (#12342)* Added optional target blocks.* Checkpoint for debugging.* Building with packedfunc filter.* Extended tune_tir API to support named blocks.* Remove accidental import.* Improve integration test.* Change names for more consistency.* Update integration test.	3
Fix Schedule structure, refactor compute to all rely on iter var	4
[BYOC][NPU] Fix integration tests not running (#9415)As we were using `python` rather than `python3`, when ran in the docker containers it resulted in:```Exception: The minimal Python requirement is Python 3.6```This masked a further error of the wrong NPU configuration being usedfor the tests.	3
[TOPI] Add dp4a intrinsic to CUDA (#1707)	1
[ConvertLayout] Support QNN ops. (#5066)* [ConvertLayout] Support QNN ops.* Changing layouts to C.* Fixing dilation.* Empty commit.Co-authored-by: Ubuntu <ubuntu@ip-172-31-53-55.us-west-2.compute.internal>	0
Remove unnecessary memset in TVMMutableFuncRegistry initialization (#8818)Remove unnecessary memset() call in TVMMutableFuncRegistry_Create()when initializing a TVMMutableFuncRegistry struct. All struct members(registry.names, registry.funcs, and max_functions) are alreadyinitialized properly before returning, hence some CPU cycles might besaved (usually 12 bytes in a 32-bit platform and 24 bytes in a 64-bitplatform must be written with 0 by memset()).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[Bugfix][Build] Fix building with LLVM-10 on macOS (#5859)	0
Move static array initialization into a function go avoid link errors (#12678)* Move static array initialization into a function go avoid link errors* Fix line length	0
[CONTRIB] Patch nnvcc to generate error when build the empty result (#1049)	0
[CI] Switch to use prebuilt docker instead of build from scratch (#1442)	2
[ARITH] More recursive rewrite rule, cleanup simplify tests (#3502)	3
Fix TFLite RESHAPE assert (#4320)	3
[UX] highlight tvm script (#12197)* feat(ux): highlight tvm script* resolve dependency* refact(tvmscript): highlight fallback as plain text with warning; put ansi colors as default terminal style* refact(tvmscript): make terminal style close to the default notebook style* fix(pylint): disable=import-outside-toplevel* fix(ci-dep): Pygments>=2.4.0 to support ansicolors w/o #* refact: making Pygments versioning most robust and user-friendly* fix: pylint var naming	0
fix storage rewrite index remap (#8338)	0
Fix AttributeError when TEST_DATA_ROOT_PATH is set (#8047)Initiate a Path object from TEST_DATA_ROOT_PATH to fix the error:AttributeError: 'str' object has no attribute 'mkdir'	0
[REFACTOR] Streamline Function Attr interface. (#5045)* [REFACTOR] Streamline Function Attr interface.There has been quite a few recent changes that depends heavily onthe function attr interface. This PR streamlines that interface by introducingtwo APIs that covers most of the usages.- GetAttr which gets a typed object for a given key  - HasNonzeroAttr is a quick helper that calls GetAttr to quickly check an attribute- WithAttr that creates a new function object with the given attr  - The API comes with copy on write optimization to avoid multiple copies  - We deliberately pick the prefix With(instead of Set) to indicate this    function does not mutate the original input.On the python side:- We allow read access via func.attrs (which is a DictAttr)- func.with_attrs to create a new instance with updated attrs.We also get rid of the small wrapper functions and make sure the API centered aroundthe GetAttr and HasNonzeroAttr interface.This PR also changes the function construction to follow the new convention.* Address review comments* Address review comments* Fix doxygen path	0
Update amalgamation.py	5
[Relay][Pass] Fix CombineParallelConv2D (#2167)	0
[CI] Docs bot now edits previous comments (#11909)This PR improves the docs bot to edit a previous comment instead of making new comments.Fixes #11837	0
unify ssize_t definition (#11384)remove tvm_ssize_t type and unify the definition of ssize_t in Windows buildCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
[Onnx Operators] Celu (#8741)* complete celu op* forgot to add test* change order in convert_map, remove comment, delete import hiccupCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>	2
Strided_slice added in NNVM (#1318)	1
Fix for build (#117)	0
[REFACTOR][IR] Unified IR Primitive Op and Registry (#4687)This PR migrates relay's Op into the ir folder.Op and its registry provides an useful mechanism tostore any attribute meta-data of an operator includefunction signatures, lowering rules, side effect etc.These features are not only useful for Relay, but also needed in the low-level IR.At the current moment, intrinsic functions in the low-level IR are simplyrepresented by a string. This means we cannot type-check the low-level IRwhen the type does not meet the constraint, nor can we obtain furtherinformation such as side-effect and read write relation of these intrinsicswrt to arguments.Op will be used as the way to handle primitive ops(in DL terminology)(builtin intrinsics or in compiler terminology).We will perform follow-up refactors to make low-level CallNodetake Op as the function argument.	1
[MetaSchedule] Enable Task Filtering (#11512)This PR allows `relay.backend.MetaScheduleExtractTask` to take an extra argument `filter_func` which filters out tasks that don't need tuning. The counterpart of AutoScheduler is `traverse_to_get_io_tensors`.	1
[ci][docker] Remove nvidia ml repository before updating (#10828)`bash docker/build.sh ci_gpu` fails locally as well as in CI: https://ci.tlcpack.ai/blue/organizations/jenkins/docker-images-ci%2Fdaily-docker-image-rebuild/detail/daily-docker-image-rebuild/273/pipeline/57From [this post](https://forums.developer.nvidia.com/t/failed-to-fetch-https-developer-download-nvidia-com-compute-machine-learning-repos-ubuntu1804-x86-64-packages-gz/156287), so long as the build completes (meaning we don't use any images from this repo), it should be fineCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Tests] Ensure MyPy type checks pass (#9284)* [Tests] Ensure MyPy type checks passThere's a few errors that come up when type checking that aren't triggering any failures:```Checking MyPy Type defs in the meta schedule package.python/tvm/meta_schedule/utils.py:23:1: error: Cannot find implementation or library stub for module named "psutil"python/tvm/meta_schedule/utils.py:23:1: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-importspython/tvm/meta_schedule/search_strategy/search_strategy.py: note: In member "__init__" of class "MeasureCandidate":python/tvm/meta_schedule/search_strategy/search_strategy.py:59:13: error: Module has no attribute "MeasureCandidate"python/tvm/meta_schedule/search_strategy/search_strategy.py: note: In member "initialize_with_tune_context" of class "SearchStrategy":python/tvm/meta_schedule/search_strategy/search_strategy.py:83:9: error: Module has no attribute "SearchStrategyInitializeWithTuneContext"```To rectify this the `types-psutil` package adds type hints for `mypy` and `# type: ignore` stops `mypy` from trying to figure out types of `_ffi_api` resources.There's also a few places where variable type definitions are repeated even though they're only required once.Finally, I've ensured `task_mypy.sh` fails the build since it's stable right now, using `set -e`.* Add temporary # type : ignore for psutil	1
fix missing std::to_string during Android build, android rpc test script (#1279)	3
[Community] Krzysztof Parzyszek -> PMC (#10703)	3
[Perf] Add CublasLt extern support for better Igemm performance (#4550)* cublaslt added* fix lint* address comments* address more comments* Trigger CI* Trigger CI	1
Support `qnn.conv2d` in FoldExplicitPading (#10982)* wip support pad + qnn.conv2d folding* works* Added test but structural equality is failing* fixed structural equality test using map_free_vars=True	1
[Android][RPC] Fix Vulkan runtime support. (#8791)Update Android RPC app to reflect the newVulkan source code tree structure.	1
[RELEASE] Bump version to 0.7.0 (#6614)	5
[Relay] Support dynamic indices size in gather_nd and scatter_nd (#8105)* add gather_nd shape func* refactor gather_nd ref funcs* add dynamic gather_nd test* gather_dim -> num_indices_per_tuple* support dynamic scatter nd* minor fix* fix pylint* rename to index_rank and make it Optional* pylint, do not use -1 for default value	1
[Relay] Gather op dynamic input support (#9240)* support gather op dynamic input* fix shape func and add test* remove constness check* fix shape func output rank* restore checkCo-authored-by: masa <masa@pop-os.localdomain>	0
Add MicroTVM support for the STM32F746 Discovery board (#7225)* Add MicroTVM support for the STM32F746 Discovery boardSigned-off-by: Tom Gall <tom.gall@linaro.org>* Add reference to the discovery board in the docsSigned-off-by: Tom Gall <tom.gall@linaro.org>	2
[DOC] Document update (#329)	5
Rename build helper (#9297)	5
[FRONTEND][TFLITE] Add FULLY_CONNECTED op into tflite frontend, support Inception V4 (#3019)* Add FULLY_CONNECTED op into tflite frontend, support Inception V4* Fix comment style in TF Lite tests.	3
[CI] Add Arm Compute Library to Arm CI unit test pipeline (#8734)	3
[Community] @mdw-octoml -> Reviewer (#8868)	5
Pass that removes reshapes post LowerTE (#12215)Introduces a Pass for removing intermediate reshapes postLowerTE() in AOT compiler. This commit adds pass specifictests and updates usmp generated workspace pools due toreduction in number of allocations post reshape removals.Note: this pass at present does not support first reshapeappearing in the graph. If seen as a useful case, it can beadded in the future.	1
[BUGFIX] Check that virtual device is unchanged in WithFields (#9826)* Add default to serialization* revert changes in serialization.cc* update 0.6 converter* json updater working, except for cycles* clean up code* Fix tests* formatting* format:* Check that virtual id is unchanged in WithFields* Set virtual_device_ to fully unconstrained in ctor* visit virtual device in the attr visitorFix serialization tests* Fix tests after bad merge* Change virtual_device() getter method* lint* ci failed* ci was broken	0
[REFACTOR] Remove stale verilog generator (#2964)	2
Migrate flake8 from workflow to lint script (#9062)Saw https://github.com/apache/tvm/pull/9055 adds `flake8.sh` to the `docker/lint.sh` and remembered I started doing this in https://github.com/apache/tvm/pull/8652, but never updated it after the Docker images were actually updated.	5
Add `operator()` to `support::With` (#12418)	1
[DOCS] typo "@func myfunc" => "func @myfunc" (#2333)typo "@func myfunc" => "func @myfunc"	2
bump tophub rocm version (#5504)	5
[Target] Add __launch_bounds__ directive as part of the CUDA code generation (#8678)	1
[Relay] InferCorrectLayout for strided_slice & min_num_branches option in CombineParallelConv2D (#2961)* [Relay] InferCorrectLayout for strided_slice* Add min_num_branches option to CombineParallelConv2D* Return undef if original layout contains splitted axes	1
[M3c][MetaScheduler] Add EvolutionarySearch Search Strategy. (#9836)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Add ReplayFunc and EvolutionarySearch strategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix optional task name.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Remove extra files.* Fix things.* Add evolutionary search.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[TESTCASE] Add a mock test workflow of CUDA codegen (#19)	1
[CI] Install curl in the context of ubuntu_install_nodejs.sh (#8326)* Make sure that curl is installed, as this script is used on   ci_lint, which does not need all the packages installed by   ubuntu_install_core.sh	1
Fix Bug in Bilinear Interpolation and Add Deform Conv to PT FrontEnd (#7397)* Fix Bug in Bilinear Interpolation* Add NHWC Tests* clean* Fix Bug and Add Deformable Conv PyTorch for completeness* Add Tensor Utils* Remove stuff* Include vector* PR Comments* Empty Commit for CICo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>	4
[QNN] Add hardswish int8 impl using table lookup (#11700)* v1* [QNN] Add hardswish int8 impl using table lookup* format* format* fix* fix utest* fix ci error* jostle ci* triggle ci* remote nn* jostle ci* fix	0
[TVMScript] Script namespace changes (#9115)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Zihao Ye <zihaoye.cs@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	4
[Vulkan][Refactor] Move ownership of per-CPU-thread objects to VulkanDeviceAPI (#8196)* [Vulkan][Refactor] Moved VulkanStream ownership from VulkanThreadEntry to VulkanDevice- Implemented ThreadMap, a container for per-thread objects.  Unlike  dmlc::ThreadLocalStore, ThreadMap is intended for use as a  non-static thread-specific lookup.- Added ThreadMap<VulkanStream> as a member to VulkanDevice, updated  all uses.* [Vulkan][Refactor] Pulled VulkanBuffer allocation/deallocation into constructor/destructor.- VulkanBuffer owns the VkBuffer and VkDeviceMemory that it allocates,  and deallocates on destruction.- VulkanHostVisibleBuffer owns a VulkanBuffer, and additional calls  vkUnmapMemory on destruction.* [Vulkan][Refactor] Move the VulkanStagingBuffer to be owned by the VulkanDevice- Previously, was owned by VulkanThreadEntry, so any use required  looking up both the thread entry and the device.  Now,  thread-specific lookup is handled in the VulkanDevice class.* [Vulkan][Refactor] Move ownership of per-thread uniform buffer to VulkanDevice- Previously, VulkanUniformBuffer was owned by VulkanThreadEntry, so  any use required looking up both the thread entry and the device.  Now, thread-specific lookup is handled in the VulkanDevice class.* [Vulkan][Refactor] Moved ownership of per-thread workspace pool to VulkanDeviceAPI- Previously, the WorkspacePool was owned by VulkanThreadEntry, and  required a lookup from VulkanDeviceAPI::AllocWorkspace.  As a  result, non-global VulkanDeviceAPI would interact with each other.* [Vulkan][Refactor] Moved ownership of per-thread active device id to VulkanDeviceAPI- Previously, the active device was owned by VulkanThreadEntry, so  lookups to multiple global variables were required.  Now, everything  goes from the VulkanDeviceAPI.- Removed VulkanThreadEntry, as all functionality has been moved to  either VulkanDevice or VulkanDeviceAPI.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[BYOC] CUTLASS integration (#9261)* byoc cutlass* add cmake and fix build* test worked but accuracy is bad* fixed argument printing properly* moving files* moving contents of cutlass_profiler into python/tvm/contrib/cutlass* run black* remove irrelavant codegen code* clang format* tried replacing sm 75 with 80, didn't help improve accuracy* remove irrelavant code from generator* tried dense + bias fusion but generated cu file does not compile* dense + bias worked after adding Leyuan's patch, bias + relu worked too* tried adding sm80 generator but accuracy is still off* remove GemmUniversal generator* cleanup partition and build* moved partition, profile and build function out of test* turned out the result match's TVM non-cutlass result. Numpy fp16matmul is busted?* clean up test* LinearCombination can be reused for bias only epilogue* remove unsupported epilogues like gelu* removing deadcode* unify gemm templates for with or without beta scaling* supported gelu but accuracy is slightly off* gelu test passed with relaxed rtol* cleanup* remove unused stuff from library.py* move profiler template into its own file* removed gemm_profiler.py* move contents of compile_engine.py into gen_gemm.py* rename to profiler_template.cu to avoid CI issue* cleaning up trying to pass pylint* add missing asf header* run black* fixing many pylint issues except wildcard import* fixed wildcard warning* add missing CUTLASS.cmake file, restore gemm_profiler.py* pylint* minor fix* add license* start filling in TODO doc* rename GemmProfiler to GemmProfilerEmitter* more renaming and doc* add doc to the main compile API* refactored generator* run black* black fix* finish doc TODO* add test for 32 bit accum* fixed kernel generator to correctly handle fp32 accum* revise build-related API* add option to profile only one kernel* add option to enable parallel compilation* clean up gen_gemm* doc update* profile_cutlass_kernels -> tune_cutlass_kernelsCo-authored-by: leyuan.wang <leyuan.wang@bytedance.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>	2
Add an option to build with -pthread (ON by default) (#3671)	1
[microNPU] removing extra bytes for workspace (#9629)Given that microNPU codegen uses target hooksit undergoes the core compiler that updatesworkspace sizes. We dont need the additionalsizes anymore. This commit removes the sizes.Change-Id: Ifd2a01e5f9566ee0c37778f32392d6d1f8c7c091	4
Fix test_autotune to support schedules with no tuning space (#12484)	1
[LICENSE] add 3rdparty licenses (#4402)* [LICENSE] add 3rdparty licenses* rename license files to .txt	5
[AMP] Add default op attribute registration to __init__.py (#8460)* add attribute registration to init* blackify* remove unused improt* jostle ci* avoid circular import* change order to match orig* other thingsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
fix lint (#29)	0
[RUNTIME] Enable extension type to PackedFunc. (#447)* [RUNTIME] Enable extension type to PackedFunc.* More comments	0
[DOCS] include a tarball of docs, add a security faq (#5119)* [DOCS] include a tarball of docs during deployment* [DOCS] Add a short security faq	1
[microTVM][RVM] Fix base-box-tool command in README.md (#8613)This commit fixes the platform argument order for base-box-tool.py'test' command in the documentation about the RVM. Currently the examplein documentation places <platform> before option[--test-device-serial=<serial>], whilst the correct order is after allthe options, so trying to use the 'test' command arguments in the orderas suggested by the documentation will not work.This commit also fixes a typo (inovke -> invoke).Finally it tweaks a bit the text format: lines with maximum 80 columns,a better diagram format for the dir structure, and a better format forthe bash commands. A link is added too for easy access to the"microTVM Reference VM tutorial" found in tutorials/micro directory. Acouple of command examples were also added to the documentation.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	2
[CMSIS-NN] Pad fusion with QNN Conv2D (#12353)Pass that fuses nn.pad and qnn.conv2d for CMSIS-NN target.	1
[CUBLAS] Fix cublas batch matmul strategy plevel (#10351)	0
[TVM Basic] Extend generic func with get_packed_func() interface (#9784)add test_target_temp_strategy unittest.Co-authored-by: sqing <qing.siqi@intellif.com>	3
[TIR] Create a StringImm reference type (#4806)This is motivated by the want to send anarray of strings across the python/C++boundary. Arrays only support ObjectRef typesand so can't carry StringImmNodes. This createsa string reference type, StringImm, which canbe used with tvm::Arrays.Change-Id: I598a44536c156b97dbfe3e9518e0a1f705da850c	5
[topi, x86] for 1d loop, make outer loop parallel after split (#6455)Co-authored-by: masa <masa@pop-os.localdomain>	1
[Torch] Fix ELU conversion (#8699)	0
[skip ci][ci][docs] Add CI infra docs (#11403)* [skip ci][ci][docs] Add CI infra docsThis adds some documentation around CI infra and pointers to the guides to run a deploy.* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[BUG] Disable second PlanDevices pass (#11662)Though started with the best of intentions, the secondPlanDevices pass to account for memory scope's introducedby lowering is buggy and not ready for prime time. Ithas caused an ICHECK fail since for some reason the newconstraints are not flowing into device_copies.	1
[TVMC] Allow output module name to be passed as a command line argument (#10962)* Allows module-name as a command line argument to tvmc * Updates microNPU graph partitioner to pass module name to PartitionGraph() * Updates CMSIS-NN graph partitioner to pass module name to PartitionGraph()Change-Id: I12a4a2eef2ddc7e3c4a6c0dd8fdcab009c975bac	4
add helpful message to topi test (#592)	3
up (#1940)	5
[Relay]Allow dynamic batch for arm conv2d (#6509)* Allow dynamic batch for arm conv2d* Add TODO	2
[TensorIR][M2a] Structural Error Reporting (#8121)This PR is part of the TensorIR upstreaming effort (#7527), stage M2a.In this PR, we implemented ScheduleError, an error reporting mechanism for schedule primitives to report user-face error messages, with the functionality of rendering the TIR out in the TVM script syntax.This set of APIs allows future improvement of error location rendering, e.g. more colorful rendering mechanisms like synr does.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	1
[OpStrategy] Support MetaSchedule Layout (#11848)	1
[BYOC][TRT] Fix small bug preventing TRT runtime compilation for versions < 6 (#7372)* Fix small bug preventing TRT runtime compilation for versions < 6* Trigger ci	1
[AMP] Register some new ops (#9849)* add new mixed precision ops* remove space* Update python/tvm/relay/transform/mixed_precision.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
Add support for the quantized TANH operator to relay TFLite frontend (#8024)Change-Id: I70df765e1562fa586ed0ffd0e07b8858f7fbb831	4
[TVMScript] Doc Base Class & DocPrinter Scaffolding (#11971)This PR addes:- Doc base class- DocPrinter base class- PythonDocPrinter- LiteralDoc and its support in DocPrinterTracking issue: #11912	0
Actually add Compute Library tests to the Jenkins File (#8394)	2
[TOPI] fix bug of leaky_relu type issue (#1076)Signed-off-by: Janboe Ye <yeyuanbo@xiaomi.com>	0
[microNPU] Add support for hard swish (#12120)Adds support for hard swish by populating a LUT similar to Vela'simplementation.Change-Id: I7ca15a3e21bc91c1b41cdd4547fabaa00de96e90	4
Improve type handling in PyTorch frontend (#5834)* Improve type handling in PyTorch frontend- Use type information from graph for inputs if available. Check  against shape information from graph if available.- Allow user to set default dtype (default to float32 for sanity and  compatibility).- Implement type promotion to follow PyTorch mechanism. This includes  fixing the handling of many "Scalar" overloads in PyTorch binary ops.- Fix arange/linspace type semantics.- Added support for traced functions. (Because it really is about the  "self" input handling.)Aside from adding an optional default_dtype keyword argument, this does notchange the signature/requirements of from_pytorch.* Fix scalar detection using numpy.isscalarand address other review comments. Thank you @siju-samuel* refine test criteron on qnn_test::test_serialized_modules, fix bool conversion of const	0
Fix call mkl gemm in mkldnn.py (#7007)Co-authored-by: zhangfucheng <zhangfucheng.jason@bytedance.com>	0
[microNPU] Update Arm(R) Ethos(TM)-U55 NPU demo README (#9725)- Update README.md to show config.cmake options when building from sourceChange-Id: Iecc36d8bc889541b8b890de37d58052a46928a83	4
[BUGFIX/PASS] Fix Vectorize with If condition (#135)	0
[CONTRIB] windows compatiblity (#1009)	5
[Ansor][AutoTVM v2.0] Phase 1: Add annotation/compute_at/compute_root/compute_inline steps (#6073)* Add annotation step* Add compute_at/compute_root/compute_inline* Doc update* Update* Update* Update measure record UT* Update* Update* Update* Move state implementation to step* Move measure_record implementation to step* Order update & API update* Update the order of state api* Update	5
[microTVM][Arduino] Fix Arduino Versions in RVM Build (#8938)* fix arduino cli version* fix release directory name* fix spresense release version* Revert "fix release directory name"This reverts commit bba693212519f04005def55714b8b896f22dd9a7.	4
[RELAY] [OP] [MXNet Frontend] Add sequence_mask (#3437)* Add sequence_maskuse exactly the same arguments as mxnetfix* fix lint* fix lint* add mxnet conversion + relay* update* update doc* fix pylint* fix doc* address comment* try to address comments* try to enable shape check for valid_length* fix* try to fix* fix bug* try to fix* address comment* address comment	1
[TIR] Add tir::builtin::assume (#12267)* [RemoveAssume] Implemented T.assume in TVMScript, RemoveAssume* [UnitTest] RemoveAssume, initial functionality tests	3
[DOCKER] Upgrade ci-cpu to latest v0.50 (#2901)	3
C++ API work with python	1
ApplyPass -> ApplyPasses; Refactored infer pass; (#43)* ApplyPass -> ApplyPasses; Refactored infer pass;* lint fix	0
[Vulkan] Prioritize discrete GPUs as device_id=0. (#8588)- Added device_type to the device-queried information.- Sort the vulkan devices by the device_type.  Priority is discrete >  integrated > virtual > cpu > other.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[BUGFIX/REGRESSION] Complex inline call, regression test on lstm cell (#128)	3
[Relay][Text Format] Pretty Printer Smart Inlining (#2881)	5
Load platform specific lib for tvmdsoop instead of only so (#5542)	2
[DOCS] Fix sphinx precheck (#4967)* [DOCS] Fix sphinx precheck* ignore keras warnings* Remove more warnings	2
lldb pretty printers for relay (#4453)* lldb pretty printers for relayA set of lldb debugger pretty printers that use the relayPrettyPrinter functionality to display data structures inthe lldb debugger.* lldb pretty printers for relayA set of lldb debugger pretty printers that use the relayPrettyPrinter functionality to display data structures inthe lldb debugger.- Put the dot.lldbinit file in your home directory as .lldbinit.- Update the file to point to the pretty printer script tvm.py- Restart lldb	5
[RELAY][PYTORCH]GroupNorm op support added (#5358)	1
[Relay][Frontend][TFlite] Add parses support for SLICE (#4502)* [Relay][Frontend][TFlite] Add parses support for SLICE* TFlite 1.13: convertor gives nonsense output when size[i]==-1* TF parser: SLICE need fixing for size[i]==-1 -> gives wrong output  bcs of indices* Set end[i] = input_tensor_shape[i] as suggested in PR review* Add another test to cover size=-1 case	3
[Refactor] Remove AttrStmt with storage_scope key (#8516)* Remove all attr::storage_scope usage* pyformat* fixed VTA tests* Update TIR text printer to print storage_scope on allocate* print storage scope in AllocateNode ReprPrinter* Fixed accidently removed scope tag check* remove unused functionCo-authored-by: masa <masa@pop-os.localdomain>	1
fix build with llvm trunk (#4386)	1
[Driver] Remove duplicate PreProcessModuleForBuild (#10530)* [Driver] Remove duplicate PreProcessModuleForBuild`PreProcessModuleForBuild` was nearly identical to the `build()`function in the same file.* Name change, `tvm::TIRToRuntime`.	1
[BYOC] Prevent duplicate outputs in subgraph Tuple (#5320)* Fix duplicate output in partitiongraph* Add test case* Fix test_annotated_regions with duplicate compiler_end outputs* Revert "Fix duplicate output in partitiongraph"This reverts commit e1f8ef3f4ca5b2aaa31ace6fa968bb50e5e4d1fa.* Prevent duplicate outputs in Tuple in PartitionGraph* Fix lint* Add another test case for when regions are merged, and when TupleGetItem was duplicated* Pull GetFunctionOutput out of branch, improve description of GetFunctionOutput* Use std::move for GetFunctionOutput. Fix typo with testcase name* Use tvm.transform.Sequential	1
Switch to HalideIR, with C API compile	5
[BYOC][TensorRT] Reuse TRT engines based on max_batch_size for dynamic batching, improve device buffer allocation (#8172)* Reuse TRT engines based on max_batch_size for dynamic batching. Improve how device buffers are allocated* Fix python formatting* Allow user to configure engine building mode using TVM_TENSORRT_MULTI_ENGINE* Update doc* Typo	2
[MetaSchedule] TuningRecord Optional Arguments (#11598)In some situations, such as before measuring the candidates, the arguments `run_secs`, `target`, and `args_info` in `TuningRecord` are not required. Per this request, the new `TuningRecord` API now accepts arguments in the order of `trace, workload, run_secs, target, args_info` with the last three being optional. Note that some tests might fail due to the change of argument order, so they might need to be adjusted accordingly.	4
Only warn when unable to find a graph input (#1052)	5
Fix gelu in PyTorch frontend, tighten numerical checks (#5763)Previously, the PyTorch frontend approximated gelu with fastgelu.To provide a more faithful conversion, we implement gelu instead.We also tighten the numerical comparisons between PyTorch andTVM-from-PyTorch to 1e-5. The object detection models need anincreased tolerance of 1e-4 to pass.I had to throw in a few fixes for missing conversions(probably due to working with very new PyTorch).I must admit the GoogLeNet/NasNet test didn't run on my machine,probably due to problems at my end.	0
fix py files (#8194)	2
fix some typo in conv2d.py (#12067)	2
add glu (#11865)	1
[VTA] Search for libvta_fsim.so in $TVM_LIBRARY_PATH (#10278)This adds `$TVM_LIBRARY_PATH` to the search directory of`vta.libinfo.find_libvta`, matching the behavior of`tvm._ffi.libinfo.find_lib_path`.	5
Fix json serialization for NDArray (#11303)When `NDArray` is being stored as `ObjectRef`, the serializer won't trigger the right path for storage. Under the new serialization mode, we need to be able to leverage the `repr_bytes` mechanism to save `NDArray`.This change is backward compatible -- ndarray saved in previous format will continue to work. And fixes the problem of serialization when `NDArray` is involved as part of `ObjectRef`. In the future, we can consider consolidate the `NDArray` save into the `repr_bytes` and remove the specialization as we evolve to newer versions	1
[EZ] [ONNX] Remove unnecessary converters for greater and lesser (#8967)* simplify by removing unneeded conversions* remove uneeded* remove testCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[MetaSchedule] Introduce ArgInfo::FromEntryFunc (#11866)	5
[TOPI, x86] Adapt AVX schedules for SSE target (#1504)* adapt avx schedule for sse* fixed output type when mixed precision mode	0
[Collage] SubGraphs (#11981)* [Collage] SubGraphsSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.Collage works in units of 'sub-graphs', which are potential partitions of theoverall Relay model. This PR introduces SubGraph (an arbitrary partitioning, withoutany implication about how it is to be represented), it's companion SubSubGraph(implying a representation as a function), and some supporting odds 'n ends.* - make Integer <-> size_t conversion explicit- make 'Compiler' name explicit* - fix namespace ambiguity* - review comments	0
[VTA][TSIM] Use Module instead of RawModule for testbench by creating an empty bundle for the IO (#3242)* use Module instead of RawModule for testbench by creating an empty bundle for the IO* change default back to verilog	2
try to fix test (#784)try to fixfix	0
Fix a bug in Symbol::Compose when using subgraphs as input (#1314)	1
[Relay] Feature Detection (#3238)* initinitlintrenamecifixaddadd some docsaveadd some testadd some testlintlintlint* fix build	0
add level2 ops (#6)	1
[Legalize][QNN] Pass out_types to Legalize. Update QNN requantize to read from out_types. (#3782)	5
[PYTHON] Add buffer name when creating tensor bindings (#5670)	1
[DOC] fix code-block error in debuggging TVM part (#12597)The code block in part Debuggging TVM is not showing up. Just fix it.	0
[docs] Getting Started with TVM: Auto Tuning with Python (#7767)* [docs] Getting Started with TVM: Auto Tuning with PythonFollows up on the TVMC tutorial, shows how to accomplish the same tasks using the python api* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Fix some rendering errors, add descriptions of tuning parametersCo-authored-by: Cody Yu <comaniac0422@gmail.com>	2
[AutoTVM][AutoScheduler] Add workaround to alter op layout bug in task extraction. (#8143)* Add workaround to alter op layout bug in task extraction.* Only copy mod.	4
revert v1 (#53)	4
[USMP] Adding support for U4 usecase (#10785)* [USMP] Adding support for U4 usecaseThis commit adds support for placing I/Otensors within the workspace buffer.This is enabled using PassConfig optiontir.usmp.use_workspace_io. Once it is enabled,it will remove the I/O tensors from the TIRmain PrimFunc and replace them with Allocatenodes that is annotated to contain Input andOutput tensors.The USMP will plan memory for them accordingly.(i.e. it will re-use space used by them forintermediaries depending on the liveness).This will only be supported with C Interface API.Thus, this commit produces two functions to themetadata sources to obtain input and output structsthat points to location inside the workspace struct.Change-Id: I4c7e750ead9a880ba900602c17f53a125f97dbf9* fixup! [USMP] Adding support for U4 usecaseChange-Id: I78f03d36b12b4a5e8eae8d11701f51019489defc* fixup! [USMP] Adding support for U4 usecaseChange-Id: I857f3d0ba7bc192d56d750c44b232998b2876e7a	4
[ci] Disable flaky ethosu tests (#10488)See #10487Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[ONNX]Pool3d & upsample3d op support (#5135)* [ONNX]Pool3d and Upsample3d op updated* Pool3d and Upsample3d testcase* Review comments fixed* Review comments	0
[PYTHON] Improve equality wrapper (#567)use `object.__eq__`(default object identity comparison) as defaultimplementation of same_as. This should be OK since `EqualOp` and`NotEqualOp` are pure Python object, `object.__eq__` is sufficient.	1
Add conv1d support in BYOC TRT by converting conv1d to conv2d (#9324)Co-authored-by: ziyu.guo <ziyu.guo@bytedance.com>	1
FoldScaleAxis became non-recursive (#8325)* FoldScaleAxis became non-recursiveFoldScaleAxis moved from ExprVisitor and ExprMutatorto non-recursive MixedModeVisitor and MixedModeMutator.The specific transforming part itself is still recursive,however the underlying traversal machinery is non-recursive.Change-Id: I8bf40bd1f3f039ef0705c665a34a4624067048a1* Added extra empty lines as requestedChange-Id: I242ec95f92b3dfc7fa3dd89385f56ab07c6e72a8	4
[Relay] fix incorrect binding of Lets in ANF conversion (#10078)* fix incorrect binding of lets in ANF conversion* add test case* remove really weird auto-import from debugging* address comments	1
[INTRIN] enable popcount on cuda, opencl, metal (#774)	0
Strict gradient boundary check (#44)	5
[MetaSchedule] Handle deserializing empty string RVs in trace (#12481)* trace.cc* add tests* remove assert* add proper test* lint* lint	3
[TFLite] Implemented MATRIX_DIAG Operator for TFLite. (#6397)* Added implementation for MATRIX_DIAG Operator.* Added tests for MATRIX_DIAG Operator.	1
fix matmul broadcast (#11242)	0
[Relay][PyTorch] Add aten::lerp (#12167)	1
[CI][ACL] Switched to ACL 20.11 (#7106)	5
Fix GetQmin and GetQmax from relay.qnn (#9427)	1
[ONNX] Skip ADD inside Gemm op when vector is zero (#5697)	1
[TFLITE]Nit: Function names made consitent (#5515)	1
[Bugfix][TIR] Narrow-Datatype for thread axis (#11725)This PR fixes a bug in the pass Narrow-Datatype in TIR, where dtype ofcertain IterVar and loop variables are adjusted to narrower ones.The bug occurs when the dtype of thread axis is int32, while its extentis int64, where the original behavior will not narrow the extent toint32, which causes an assertion thrown in IterVar's constructor. Analternative approach is to re-dtype IterVar to int64, however, thesubsequent passes do not actually respect int64 thread axes, which leadsto even more issues in lowering.This bug prevents AutoTIR in tuning Huggingface DistilBERT.	0
[TVM] Eagerer const folding for logic ops (#1907)	2
[Onnx] Add Adam (#9002)* add adam op* lint* remove tests* lint	3
adjust pylint output (#3973)adjust pylint output to show file location to make it possible to locate errors	0
[Runtime][ThreadPool]Refactor affinity function and support CPU affinity list setting. (#9802)* [Runtime][ThreadPool] Refactor affinity function and support CPU affinity list setting.Issue:1. There are multiple affinity function using "LINUX" and "ANDROID" macrocheck and the multiple check make the logic maintain and change becomecomplex.2. Current logic of tvm [Runtime][ThreadPool] assume all of the cpu resources are available fora single backend runtime to do the data flow computation. But such assumption may nottrue when user running multiple task on the system and not want tvm taskexhaust all of the cpu resource, or when user going to run multiple backendruntime of tvm on the system, each backend runtime of tvm should use different cpuaffinity settings to achieve best performance.Solution:1.Refactor the affinity functions to move the "LINUX" and "ANDROID" checkinto one function.2.In this solution, we introduce a new "CPU AffinityMode type" named "kSpecify", by using"kSpecify" and the function named "tvm::runtime::threading ::Configure" user can specifythe cpu list for the cpu affinity of a backend runtime.This solution reused the existing per thread thread pool logic of [Runtime][Threadpool] thatcreated a worker thread pool for current thread which can running a particular runtime. for a multipleruntime use case, user can first launch multiple threads, then call "tvm::runtime::threading ::Configure"with cpu list to create tvm data flow worker thread pool, after doing this the execution of the multipleruntime on the multiple threads will use different cpu resource list.* fix windows build issue.* fix build issue.* fix build issue.* fix windows build issue.* fix plint issue* polish comments.* address review comments.* address reivew comments.* address review comments.* address review comments.Co-authored-by: hua jiang <hua.jiang@xilinx.com>	1
Setup Travis CI (#7)* Setup Travis CI* add source* fix source	0
Delete `from __future__ import annotations` since it requires Python 3.7+ (#11889)	1
[Relay][Op] fix conv transpose weight dtype inference (#8962)Fixes type inference error when data and weight have different dtype	5
Change 'delete's in Relay VM Instruction dtor to 'delete[]'s (#5735)	4
[Relay]Improve Shape Func handling for Tuple inputs (#5467)* Improve Shape Func handling for Tuple inputs* Fix lint* Improve* Fix build	0
[FRONTEND][KERAS]Max_pool3d and Averagepool3d operator support  (#5085)* [KERAS]Pool3d support added* Keras pool3d testcase added	1
CPP implementation of L2Norm and LRN ops (#1157)	5
[FRONTEND][KERAS]GaussianDropout/Noise parsing support (#4928)GaussianDropout & GaussianNoise are active only during training time. This can be skipped during inference.	5
Update overview.md (#103)misstype fix	0
Fix broadcast InferCorrectLayout (#10156)* Move function body to .cc file.* fix broadcast infer layout* add unittest* backward-compat: optimize for scalar layout* fix lint* fix lint and warning* Add newlines; Use std::vector* fix lint* jostle ci	0
Update SimplifyInference documentation (#6853)	2
[Hexagon] Include Utils.cmake for tvm_file_glob used in HexagonSDK.cmake (#9903)Don't include it from HexagonSDK.cmake because the Hexagon SDK file canbe included from different directories.	2
[LLVM] Remove use of deprecated PointerType::getPointerElementType() (#11984)With LLVM switching to opaque (typeless) pointer types, some functionsrelated to handling typed pointers are being deprecated (and will beremoved).The DWARF debug information does express pointee type. When constructingthis information from opaque pointers in LLVM IR, the pointee type needsto be obtained from somewhere else (not the pointer).Change the debug info generation to use the original PrimFunc to obtainthe necessary type information. This will work with older versions ofLLVM as well.	1
[TOPI] Improve get_valid_count and nms performance for CUDA (#5339)* get_valid_count updated to have correct results* speedup nms* update nms* revert back nms* recover one test for get_valid_count	1
Expose FTVMInferCorrectLayout Python interface (#8755)Co-authored-by: kueitang <kueitang@qti.qualcomm.com>	5
Change Vivado install instructions to version 2018.3 (#4003)	4
[DOC] Include TOPI in doxygen (#321)* [DOC] Include TOPI in doxygen* update	5
[Hexagon] Don't use {} initialization with FastRPC structures (#9033)The data members in FastRPC structures aren't guaranteed to remainin the same order. Replace aggregate initialization with direct,member-by-member initialization.	5
[CI] Update GoogleTest in ci_wasm (#11207)Looks like I missed this one in #11162 due to missing the cpptest callin the Jenkinsfile	2
[ETHOSN] Update to 20.08 version of the ethosn-driver. (#6606)- Updated ethosn relay backend to account for 20.08 api changes around cascading and quantization.   - Note: 20.05 compatibility is maintained for now to avoid compilation and test failures while the docker image still uses 20.05. The version switch can be removed along with associated compatibility measures when no longer necessary.	4
[Relay][VM] Port VM, VM compiler, and Object into python (#3391)* tmp* Port vm and object to python* clean up* update vm build module* update* x* tweak* cleanup* update* fix rebase* Rename to VMCompiler* fix	0
[Vulkan][Runtime] Added dummy implementations for TVMStreamHandle operations (#7969)rpc_runner_run interacts with stream handlers following PR #7819.Vulkan currently executes adds everything into a single command bufferper CPU thread, so there isn't a corresponding concept of streams.Therefore, added no-op implementations for these DeviceAPI methods.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TensorFlow] Disable failing tests on AArch64 (#12257)* [TensorFlow] Disable failing tests on AArch64Change-Id: I7a53ce2d0dfff682b2953a26cace53fc5cc5d388* Fix lintChange-Id: If60d6e6ad24230f3c7f17c7ec55a7febcbba90fd	4
Add More Shape Functions (#4179)* Add shape functions* Fix get_const_tuple* Fix cpplint* Fix pylint* Fix pylint* rebase and fix* Check Any for infer type* Fix expand_dim shape func for zero rank input* Fix pooling infer type* Address comment* Register layout transform attr	1
[BugFix][TIR] Fix cross-thread reduction when single reduction loop with predicate (#10016)	0
CUDA: broaden path detection (#6444)Debian/Ubuntu repackaged CUDA has slightly different pathsAlso, add CUDA versions 10.1, 10.2.	1
[REFACTOR] Automatically deduce function type signature in Registry.set_body_typed (#4623)Previously we support a limited case of function type deduction and in many placeswe have to supply the type twice during set_body_typed (one in the template parameter, another in the lambda signature).This PR improves the deduce function by enablng automatic function signature deduction.```TVM_REGISTER_GLOBAL("sub").set_body_typed([](int x, int y) -> int { return x - y; });```Unfortunately, because of template conflict, we can not support the original casewhere both type signature and lambda are supplied through set_body_typed.This PR refactors the existing regsitration to the new style.	1
[RUNTIME] if a param not in input, we still consume it's data (#5990)so the read pointer of stream can move forwardSigned-off-by: windclarion <windclarion@gmail.com>	4
[PatternLang] Support any index matching for TupleGetItem (#5909)* support any index matching* update doc	2
[TOPI][Hexagon] Implement quantized elementwise for hexagon (#12606)* [TOPI][Hexagon] Add test and schedule for uint8 resize2d* Fix correctness issue* Reformat* [TOPI][Hexagon] Implement quantized elementwise* Reformat* Address review comments* Reformat* Revert* Address review comments	1
[ACL] Adjust mobilenet test for Keras 2.9 (#12541)In Keras 2.7, one "reshape" operator was removed fromthe Mobilenet model, making our test which verifies thenumber of operators to be incorrect.This patch adjusts the operator count so that it is in linewith the changes in Keras. For reference, the change inkeras repo was done in hash b6abfaed132 "Remove unnecessaryreshape layer in MobileNet architecture".	4
tensorflow frontend supports user given outputs (#1913)	1
switch to more portable bash pipeline syntax (#7274)	2
[microTVM][Zephyr] Disable test_armv7m_intrinsic since it's broken (#12620)add xfail	0
Refactor diagnostic to avoid circular dependencies (#6692)	4
[ONNX] Update Slice op conversion to take strides into account, clean up tests (#6467)Co-authored-by: masa <masa@pop-os.localdomain>	3
Add AMD codeGen unit tests (#4509)	3
Additional fix for PR#2972 (#3044)	0
[RENAME] nvcc_compiler->nvcc, cc_compiler->cc, metal_compiler->xcode (#248)	5
[Runtime][PipelineExecutor] Pipeline Executor Sequential execution (#10082)* [Runtime][PipelineExecutor] Pipeline Executor Sequential executionIn the first, adding the "get output" logic. Secondly, adding the the sequential executinglogic of pipeline executor. In the last, testing the pipeline executor interface andchecking the output data.* Address review comments.Co-authored-by: Cody Yu <comaniac0422@gmail.com>* trigger build.Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
conv2d perf improved for conv2d_56_64_128, super resolution workloads added (#643)* conv2d perf improved for conv2d_56_64_128, test name added to differentiate workloads* fix lint error	0
[µTVM] Clone Zephyr 2.5.0 from maintenance branch (#7891)* [µTVM] Clone Zephyr 2.5.0 from maintenance branchClone Zephyr 2.5.0 from maintenance branch 'v2.5-branch' instead of fromrelease tag 'v2.5.0'. The maintenance branch is stable and includes allthe most recent fixes backported to Zephyr 2.5.0.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Retrigger CIRetrigger CI since the error reported for python3:i386 in test./tests/scripts/task_python_integration.sh seems to be a CI glitch, notrelated to the change here proposed, plus it was not possible toreproduce it locally, where task_python_integration.sh passes.	4
[NNVM][FRONTEND] Tensorflow frontend support (#1188)	1
[FIX] Fix depthwise conv2d on non-cuda GPU platforms (#8379)The depthwise_conv2d schedule had a bad check to make sure the iterationaxis was not larger than max_num_threads. Now the iteration axis isbounded by its size or max_num_threads, whichever is smaller.	1
[DOC] fix installation doc (#2290)	2
[build][hexagon] Respect x86 C/C++ compiler choice (#11312)- Fix issue where `CMAKE_C[XX]_COMPILER` isn't propagated  into the build configuration for `x86_tvm_runtime_rpc`.	1
Onnx squeeze enabled with auto axis handling. (#10742)* fix squeeze when axis is absent* lint* tidy code* fix argument	0
Fix Python syntax error AGAIN in start_rpc_server_to_tracker.py (#4685)#4682 Tried to fix a Python syntax error but did not go far enough because there are _three sets_ of embedded quotes.This PR solves the syntax error by using Python's triple quoted strings on the outside and then double quotes in the middle and then single quotes on the inside.	1
[RFC][Formatting] Add scripts for applying Black to the Python code. (#6437)	1
[AutoTVM] Fix typos (#3014)Signed-off-by: Ce Gao <gaoce@caicloud.io>	2
[MetaSchedule] Enhance Conv2d NCHW Winograd Schedule Rules (#12127)* Update winograd schedule rules.* Remove extra part for setting local storage scope.* Fix bgemm schedule.* Add winograd tile size to annotation.* Finish winograd schedule rules.* Process add relu.* Modify to nchw rules.* Add missing nchw output rules.* Add winograd conv2d nchw search space test.* Fix lint.* Leave consumer of output to autoinline.* Remove bgemm rules.* Remove bgemm schedule rule annotation.* Update unit test.* Fix test case.	3
[DOCS] Migrate HLS documents from md to rst (#5419)	2
[Relay] Fix invalid shape function for "copy" operator (#9749)The 'script' for of the shape function was ill-formed,resulting in a TIR shape function which did not assignto it's output, which in turn caused either OOM orassert fails as uninitialized dimensions worked theirway downstream. That fix is in python/tvm/relay/op/tensor.py.Everything else is for testing and debugging as I trackedthis down.Special thanks to Lily for helping me with the scalar vstensor switch in the copy shape function.[This is CORE-112 in OctoML JIRA.]	5
Support dynamic shape searchsorted (#9348)	1
Enable more warnings when compiling with clang 10.0 or greater (#6456)	2
Fix stride default value None in torch.nn.functional.avg_pool (#4984)* fix unordered dictionary problem for python version 3.5* modify style* default value of stride in torch.nn.functional.avg_pool is None* delete prev modifications* add testcase for nn.functional.avg_pool2d	1
[COMMUNITY] @cconvey -> Reviewer (#12598)	3
[Arith] Handle bitwise_and with power of 2 in modular set (#12272)	1
[Fix] Fix blas cmake for mac os (#3898)* fix cmake for mac os* rename	1
support returned function in relay.build (#10502)* fix InferType bug* fix InferType related bug* support returned function in relay.build* support returned function in relay.build* support returned function in relay.build* support returned function in relay.build* add warning about function returning function	1
fix tanh gradient and update tests to use downstream gradient (#7340)	1
[UTILS, DOC] Use TVM file downloading utility, conv2d tutorial (#48)	2
[Rust] Update Rust bindings (#9808)* Update Rust bindings* fmtCo-authored-by: AD1024 <dh63@cs.washington.edu>	5
[PTX] `ldmatrix` builtin to accelerate copying data from shared memory to warp memory (#10855)We already have PTX mma and mma.sp builtin support in #9909  and #10339 . However, we have not supported corresponding data movement builtins for these mma instructions, so the data movement would not be as fast as wmma.This PR brings the `ldmatrix` builtin, which is a native PTX warp-level instruction (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix), and we can use it to load several (1/2/4) 8x8 matrices from shared memory to warp memory.	1
[Relay][Doc] Docs for new op code (#3522)	1
Revert "[Relay] Expand type unification and other utilities" (#2434)	4
[BYOC-DNNL] support more post-ops (#12002)* support post-op swish* support post-op clip* enhance get_shape and get_dtype in dnnl.py to support efficientnet* add checks for with_eltwise whether in supported list* fix lint* fix test	3
[microTVM][Zephyr] Add 'config_main_stack_size' option to API server (#9026)	5
[TOPI] negation->negative to be consistent with numpy (#470)	5
[API/Refactor] Unified PackedFunc for API and Generated Functions (#26)	1
[RUNTIME] RPC runtime that support run testing on remote device. (#147)* [RUNTIME] RPC runtime that support run testing on remote device.* Fix ctypes in OSX.* fix lint	0
add take frontend (#1307)	1
[ci] Invoke tensorflow tests individually (#10198)* [ci] Invoke tensorflow tests individuallyThis is another (simpler) attempt at #10151 to avoid CUDA issues with tensorflow tests. This should work by cleaning up any reserved GPU memory by tearing down the whole process that has imported tensorflow each time a test is run.* [ci] Invoke tensorflow tests individuallyCopy of #10197 but using a pytest plugin instead of manually grepping through test filesCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix 8093, Enhance Buffer Index Simplify (#8204)	0
[CI] Use correct tag in Docker --cache-from (#9234)When I didn't have `:latest` available I saw that my image wasn't beingre-used between runs.	1
[Dyn] Use SizeVar instead of Var in the GetShape function (#9650)	1
[Hexagon] Pytestify Hexagon unit test (#8955)* [Hexagon] Pytestify Hexagon unit test* Fix formatting* Convert linker registration into pytest fixture* Use yield in pytest fixture* Restart CI	0
[DOC] topi intro tutorial (#1217)	2
Install nnvm lib and haders. Offer choice to build static or shared lib. (#146)	2
[Target] Add Target Parser for Arm(R) Cortex(R) M-Profile CPUs (#12319)This implements an initial Target Parser which uses the same logic asthe CMSIS-NN compiler flags to update the features and keys of the `c`and `llvm` `Target`s.Refactoring of the CMSIS-NN logic will be in a separate patch.	2
[TIR] Fix int32 vs int64 mismatch in For construct. (#10595)* Respect dtype in Scalarize.* Add unittest.* Fix lint.* Promote dtype of IntImm to match loop_var in For.* Fix dtype mismatches.* Lint* Lint.* jostle ci* Match dtype in hybrid parser.	0
[LANG] Change Schedule->Stage, Use Schedule for global schedule (#8)* [LANG] Change Schedule->Stage, Use Schedule for global schedule* add numpy as dep* add numpy installation, temporary disable osx	1
[TIR] fix crash when comparing IntImm to None (#12034)* [TIR] fix crash when comparing IntImm to None* [TIR] raise ValueError when comparing IntImm to None* fix: add test for non-pytest run	1
[CI Image] support CSI-NN2 in ci_qemu (#11689)* [CI Image] support CSI-NN2 in ci_qemu* build CSI-NN2, download related toolchain and qemu* using fixed csinn2 branch	0
Update README.md typo (#2132)	2
[RELAY] reorg testcase, make checked_type property, fix constructor error handling (#1850)	0
[RPC] Update RPC module to enable remote linking. (#6462)Remote linking is useful when the linker is not availableon the host environment.	2
[BugFix] SSD fully supported on GPUs, updated deploy_ssd tutorial (#2510)* nms fixed for gpu, tested on cuda and opencl devices, ssd now can run fully on the gpu* sort updated to use virtual thread* typo fixed* fix lint* fix lint* add support when batch_size > 1* intel graphics conv2d bugs fixed for inception_v3* intel conv2d api updated, nn input size 4 condition added* review addressed* move conv_tags to attributes* opencl ctx fixed* nms_ir index simplified	0
[CI] make sure graphviz is on both ci-cpu and ci-gpu images (#6645)	1
fix (#9412)	0
Improve NNVM to Relay conversion (#2734)* Improve NNVM to Relay conversion* fix pylint* support __lshift_scalar__, abs, ceil, floor, and trunc to pass CI	4
[TVMSCRIPT] Add synr dependency in preparation for tvmscript diagnostics overhaul. (#6795)	1
Unify name mangling in TVM (#12066)* Add NameSupply and GlobalVarSupply* Build GlobalVarSupply from IRModules instead of having it attached to an IRModule.* Pass GlobalVarSupply when lowering shape funcs* Partially replace instantiations of GlobalVar with GlobalVarSupply* Construct GlobalVarSupply from IRModule* Add tests for supply* Add documentation for NameSupply and GlobalVarSupplyCo-authored-by: Florin-Gabriel Blanaru <fgb@system76-pc.localdomain>	5
Fix inconsistent python/cpp API behavior for if_then_else, power (#3829)* fix inconsistent python/cpp APIs for if_then_else* fix error message* fix power consistency* fix* fix bug* add test	3
Fix global var in prelude (#3405)	0
[TVMC] Allow manual shape specification in tvmc (#7366)* add ability to optionally overide tvm shapes* add help documentation for --shapes* improve documentation* reformat test_compiler using black* Incorporate feedback from ekalda for better pytorch support and testing.* address feedback* switch input shape syntax to be more pythonic* add commentary* reformat common.py* fix lint issue* format common.py with black* torch/pytorch test hiccup* add -s to setup-pytest-env.sh for clearer error msgsCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>	0
[VTA] Enable streamlined GEMM execution (#4392)* disable pipelined adder and enable streamlined gemm execution* pipeline first layer of adder* explain difference between pipeadder and adder* add comment for explaining the hard-coded latency	5
Fix _get_yolo_detections (#8477)	1
[COMPILE] More debug message when compile error (#66)	0
[RUNTIME] Api to get number of runtime threads (#10896)* [RUNTIME] Api to get number of runtime threadsAdd `tvm::runtime::threading::NumThreads` and `tvm.runtime.num_threads`as a way to get the number of threads in use by the TVM runtime.* check if equal to hardware threads or hardware threads/2	1
fix missing span arg (#9318)	0
Fix Plan memory when multiple inplace option is available (#67)	0
[Relay] Fix Relay ARM CPU depthwise spatial pack schedule alter op layout issue. (#2861)* Fix Relay ARM CPU spatial pack depthwise alter op layout issue.* Update tune_relay_arm.py	5
[DOCS] Cleanup docs before rebuild (#5127)* [DOCS] Cleanup docs before rebuild* Ask doxygen to generate svg to minimize the file size	2
[TEST] include coreml tools in docker (#71)* [TEST] include coreml tools in docker* upgrade tvm	2
[Keras] Add l2_normalize support (#9383)	1
Set TOpPattern=kOpaque for scatter_nd (#7464)	1
[AutoSchedule] Compatibility improvement with XGBoost v1.3.0 (#7069)* [AutoSchedule] Compatibility improvement with XGBoost v1.3.0* lint	1
[Relay] Remove DeviceMap from LowerTE (#8788)* [Relay] Switch the graph, VM and AOT executors to use the mergeddevice_planner.cc from #9038, and finally remove DeviceMap from theLowerTE Pass.- We retire analysis/context_analysis.cc and  transforms/device_annotation.cc (and their tests). That  includes the CollectDeviceInfo, CollectDeviceAnnotationOps and  ContextAnalysis entry points. These are all subsumed by the  PlanDevices pass and the device aware visitors.- The following passes now use the new 'Device Aware' visitors to  recover the device for every Relay sub-expression:     - backend/aot_executor_codegen.cc (AOTOnDemandAllocator)     - backend/graph_plan_memory.cc (StorageAllocaBaseVisitor etc)     - backend/te_compiler.cc (LowerTensorExprMutator)     - transforms/memory_alloc.cc (DialectRewriter)     - backend/vm/compiler.cc (VMFunctionCompiler)- The following passes/utils must maintain the device information  encoded by the device planner within "on_device" annotations and  "param_device_types"/"result_device_type" function attributes:     - backend/vm/lambda_lift.cc (LambdaLifter)     - transforms/to_a_normal_form.cc (Fill)     - ir/expr_functior.cc (Bind)- Remove a lot ad-hoc 'homogeneous' vs 'hetrogeneous' conditionals  in favor of just asking for the device. Also removed a lot of ad-doc  encodings of the 'default' device.- We no longer need to run device-planning twice (before and after  lowering). Device planning is also decoupled from memory planning.- The LowerTE Pass no longer needs an expression-to-device side table  (which was the problem which kicked this series of PRs off in the first place).* [checkpoint] Revert unnecessary changes- Started down multi-target handling in interpreter but didn't finish- Some one-off debug stuff* [checkpoint] TODO's for default device logic	2
Misc. improvements to documentation/build setup for first-time builds. (#7840)- Makefile  - Target "crttest" ignored OUTPUTDIR variable- .gitignore  - Added ignores for download test data/models.- docs/README.txt  - Missing quotes on sphinx dep, needs pinned autodocsumm version- docs/contribute/pull_request.rst  - Use "ci_lint" docker image  - Updated C++ test instructions to refer to the from_source installation for gtest.  - Updated python test instructions with synr package dependency- docs/langref/relay_expr.rst  - Updated reference for example usage of TempExpr. `src/relay/pass/alter_op_layout.cc`    no longer exists, and `src/relay/transforms/alter_op_layout.cc` doesn't use TempExpr.    Picked a different use case as example.- tests/scripts/task_cpp_unittest.sh  - Updated "make crttest" to run only if "USE_MICRO" is enabled.  While USE_MICRO is always enabled    in the CI builds, task_cpp_unittest.sh is also recommended for use in    docs/install/from_source.rst, which does not mandate USE_MICRO.- docs/install/from_source.rst  - Added -DMAKE_SHARED_LIBS=ON to the google test cmake config.  By default, only static libs are    generated for gtest, while TVM's build preferentially selects the shared libs.- tutorials/get_started/auto_tuning_with_python.py  - Changed norm_img_data to avoid loop, improve readability- tutorials/get_started/relay_quick_start.py  - Previous version used different input data passed to the initial and deployed module, then    asserts that the results should be the same.  Modified so that the same input data are passed in    both cases.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
maximum relay op V2 (#1838)	5
[onnx] Relax tolerance for qlinearleakyrelu test (#11042)This has failed on `main`: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/3068/testsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[TVMScript] Add syntax sugar for T.handle and T.match_buffer (#9492)	0
Convert BuildModule to use TVM node system (#879)* Make python BuildConfig serializable/deserializable to/from string* Make C++ BuildConfig serializable/deserializable to/from string* Revert "Make python BuildConfig serializable/deserializable to/from string"This reverts commit a5e1fb3ff63a161cc0d63475d2a32816cc4c3666.* Revert "Make C++ BuildConfig serializable/deserializable to/from string"This reverts commit ec0c2c54543050fe6f264d06eebff33dee70370b.* Converted BuildConfig to use TVM node system* Fix lint* Fix lint* Added code to set node attributes through the C API* Fixed bug in build_config()* Fix lint* Fix lint* Fix test errors* Reduced scope of node __setattr__ to apply only to BuildConfig* Fix lint* Fix lint* Changed python BuildConfig to be immutable, with values set once on construction.* Fix lint* Fix C++ test* Fixed BuildConfig setting python-side args* Fix lint* Removed dependency on reflection.cc to construct BuildConfig (allow use in runtime library)* Fix lint* Revert "Fix lint"This reverts commit 16ed6d7a1ca5e551b035bad46e8361ea487cd45b.* Revert "Removed dependency on reflection.cc to construct BuildConfig (allow use in runtime library)"This reverts commit 43817c97a2ee045791e0c031d962fa97636ce8f6.* Avoid accessing BuildConfig when using runtime lib* Fix missing import* Fix error running under cython (root cause: node handle is not valid until after __init__ has returned, so cannot call __dir__ during __init__* Fix error where BuildConfig._node_defaults was not copied in build_config()* Fix lint* Fix lint* Fix lint* Fix lint* Add comments to python BuildConfig	5
[REFACTOR][NODE][RUNTIME] Move Node to the new Object protocol. (#4161)* [REFACTOR][NODE][RUNTIME] Move Node to the new Object protocol.This PR removes the original node system, and make node as a subclass of Object.This is a major refactor towards a better unified runtime object system.List of changes in the refactor:- We now hide data_ field, use Downcast explicitly to get a sub-class object.- Removed the node system FFI in python.- Removed the node C API, instead use PackedFunc for list and get attrs.- Change relay::Op::set_attr_type_key(attr_key_name) to relay::Op::set_attr_type<AttrType>().  - This change was necessary because of the new Object registration mechanism.  - Subsequent changes to the op registrations  - The change revealed a few previous problems that is now fixed.- Patched up a few missing node type registration.  - Now we will raise an error if we register object that is not registered.- The original node.h and container.h are kept in the same location.- Calling convention: kObjectHandle now equals the old kNodeHandle, kNodeHandle is removed.- IRFunctor now dispatches on ObjectRef.- Update to the new type checking API: is_type, derived_from are replaced by IsInstance.- Removed .hash member function, instead use C++ convention hasher functors.* Address review comments	1
Support negative pad values (#7375)* Support negative pad values* Update test_op_level2.py* Update pad.cc* Update test_op_level2.py* PR Comments* Update pad.cc* Address PR Comments* CI Error* CI Error* CI ErrorCo-authored-by: Ubuntu <ubuntu@ip-172-31-28-115.us-east-2.compute.internal>	0
[RUNTIME] Resolve constexpr issue in debug mode. (#5651)static constexpr is a bit weird before c++17.They are not inlined by default and does not have symbols after compilation.It usually isn't a problem when they are inlined(in c++17 they are inlined by default).But will create compilation error when passed to functions that take (const)references.This PR fixes the problem so that we can compile on debugmode.	0
Fix relative include path (#10402)	0
[microTVM][Zephyr] Fix: Test fails on hardware because of short timeout (#8677)* add timeout* rename timeout and change timeout to a reasonable value* fix tests after project api merge* retrigger because of flaktest	3
update (#11838)	5
[AUTOTVM][RELAY][DOCS] relay ports of `tune_nnvm_*` autotvm tutorials (#2594)	2
[ci] Add onnx model to S3 (#12716)Addresses this CI failure on `main`:https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/4235/pipeline/Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[RELAY][FRONTEND] Tensorflow frontend. (#2216)* [RELAY][FRONTEND] Tensorflow frontend support.* * LSTM removed for a while.* * basic ops are good.* * nn wip* * wip* * python2.7 corrections.* * NN ops are good.* * e2e models working good* * all good except LSTM* * rebase, tutorials and CI trigger.* * CI errors.* * enable opt_level=3* * Docstrings cleanup. testing.tf utils moved to relay from nnvm.* * tutorials update.* * LSTM work good now.* * Rebase* * CI error* * enable PTB.* * rebase.* * tutorials* Update python/tvm/relay/frontend/tensorflow.pyCo-Authored-By: srkreddy1238 <sivar.b@huawei.com>* * review comments.* CI fix.* * review comments.	0
[BENCH] Add Benchmark for Rasp (#68)* [BENCH] Add Benchmark for Rasp* [BENCH] Add arg opt-level* [BENCH] Add model choices* [BENCH] Improve	1
[TEST] Move llvm import test away from minimum test (#9171)* [TEST] Move llvm import test away from minimum testThe llvm import relies on the same system clang and llvm version andmay be tricky to get right on all platforms.Given this is an advanced feature, and there has been some problemsin windows(could relates to clang version update).This PR moves away from minimum tests.* Update test_minimal_target_codegen_llvm.py* Update test_target_codegen_llvm.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>	3
TF frontend: add expm1 op (#6783)* TF frontend: add expm1 op* TF frontend: add description for expm1* TF frontend: use overload operator - instead of subtract* TF frontend: Limits the range of input data in the Expm1 testCo-authored-by: xup <xp224797@alibaba-inc.com>	3
Add support for the quantized RESIZE_BILINEAR operator to relay TFLite frontend (#7866)Change-Id: I46008e5b7edc49d32847acd6d166374a8d85058g	4
[TIR] Allow string/buffer arguments to Schedule cache_read/write (#12661)Previously, the argument needed to be an integer specifying the indexinto the read/write regions of a block.  Now, the argument can be astring specifying the name of the buffer, or the Buffer object itself.This is a follow-up from https://github.com/apache/tvm/pull/11624.	1
[Fix] Remove unused variable in GraphExecutorCodegen (#8465)	1
[BUGFIX][CRT] Fix Compilation Error in CRT (#5713)	0
[AutoTVM, Relay] Clear compile engine after task extraction (#5724)	4
change to match by both op and name (#109)* change to match by both op and name* add zero prop for non differentiable ops* Update infer_shape_type.cc	5
[RUNTIME] Enable ext_dev type for quick plugin of device (#542)* [RUNTIME] Enable ext_dev type for quick plugin of device* [TEST] Update testcase to cover all computation	3
[TIR][REFACTOR] Remove te::Tensor dependencies from TIR passes. (#5372)* [TIR][REFACTOR] Remove te::Tensor dependencies from TIR passes.te::Tensor is an useful object for tensor expression, but bringsun-necessary reverse dependency in TIR nodes such as Provide and Realize.This PR is a first step to remove this dependency. We will use Buffer in all the placeswhere the te::Tensor was used. The rough correspondence are:- Provide -> BufferStore- Realize -> BufferRealize- HalideCall -> BufferLoad.After this change, we can not use IRModule of PrimFuncs cleanly to represent TIRat any point of the optimizations. Buffer will serve as the abstraction for the TIR datamodels to represent the intermediate storages and their constraints.We still keep Realize/HalideCall and Provide as TIR nodes for now to make the change minimum.Right after ScheduleOps, we call SchedulePostProcToPrimFunc to canonicalize the temporary IRgenerated by TE(which contains these nodes) to the TIR.The TIR optimizations are now mostly migrated to to the pass manager.Followup PRs are needed to migrate the remaining few passes.* Fix dev tutorial	0
[TVMScript] Syntax sugar for reads & writes (#9634)* add test file* add syntax sugar support* add comments* cleanup* update stub* remove failed tests* update stub with overload* address comments	1
[TIR] Enforce buffer pointer var type to be consistent with dtype. (#6317)Now that we have type_annotation in tir::Var.We should make sure that the type annotation to be consistent with the dtypein Buffer declaration and Allocation.This change allows future passes to directly use the content type information via type_annotation.This PR turns on the enforcement on Buffer and also fixed a few cases for Allocate.A follow up PR need to fix a few more cases in the hybrid script parsingbefore everything can be made consistent.	0
[QNN] Requantize operator (#3531)* [Relay] [Quantization] WIP - Common files for the qauntization work.* [Relay] [Quantization] WIP - Prototyping requantize op.* Requantize operator implementation.Requantize converts one quantized tensor representation to another quantizedrepresentation. The PR has following implementation features- Requantize operator defined in qnn namespace - relay.qnn.requantize- Lowering of the requantize to exisiting Relay operators- Integer fixed point implementation of requantize    - Two rounding modes - FE_UPWARDS (round towards infinity) and    FE_AWAY_FROM_ZERO (std::round behavior)- Floating point implementation as well, that can act as reference or can beused for devices when FP32 computation is not used.- Unit test casesRelevant Issue - https://github.com/dmlc/tvm/issues/2351Credit to TFLite and GemmLowp to provide reference implementations.* Typo and lint fixes.* Doc fix.* Uncommenting the lint script (fixing mistake).* Modifying the unit tests.* Moving C++ files into src/relay/qnn* Moving python files to python/tvm/relay/qnn. Some minor fixes.* Moving the attrs.h inside the include directory.* Pushing files that I forgot earlier. Changing util location.* Incorporating comments. API change. Lint fixes.* Modifying the GetFixedPointMultiplierShift API as per comments.* Forgot the dialect change.* Changing rewrite to qnn_lower.* Renaming Quantize to Qnn for clarity.* Remove use_int_domain.* Incorportaing review comments.* Adding API doc for QNN dialect.* Move the qnn_lower pass to transform namespace.* Moving from expr to module. Adding namespace in C++.* Minor sentence rewrites. Added qnn namespace.* Added the API doc.* Chanding default out_dtype to int8. Adding a test with in/out_dtype as uint8.* Style fixes. Better error messages.* Adding documentation.* More documentation fixes.* Adding out dtype check for requantize.* Adding corner case for FP32 to fixed point conversion.* Adding extra line.* Documentation fix.* Adding static inline.* Incorporating jackwish comment. Removed idtype from requantize lowering.* Removing Quantize/Dequantize code. Restricting Requantize to (u)int8/int32.* Style fixes.* Fix the docs.* Move to Legalize API.	4
[AutoTVM][Testing] Add `tune_relay` scripts (#12685)Example:```bashpython -m tvm.autotvm.testing.tune_relay  \       --workload bert_base               \       --input-shape '[1,64]'             \       --target "llvm"                    \       --num-trials 800                   \       --rpc-host 192.168.6.66            \       --rpc-port 4445                    \       --rpc-key 3090ti                   \       --work-dir /logs/autotvm-bert_base \       --cache-dir /cache-workloads       \       --graph-tuner True                 \       --cpu-flush True                   \       --backend graph```	1
[FRONTEND][DARKNET]LSTM and GRU support (#1576)	1
[LLVM] Retrieve entire target string from LLVMModule (#11802)The blob-embedding code creates a new LLVM module for which is needs moreinformation than just the target triple. The `_get_target_triple` functionin LLVMModule returned the triple with additional options appended to thestring. Instead of piggy-backing those extra options on top of the triple,replace `_get_target_triple` with `_get_target_string`, which will returnthe entire target string.	1
[Docs] Update Dev. Doc. on how to add a new relay operator (#7893)* first draft of add op* first pass editting doc* make main title visible again* address masa's comments	1
[Hybrid][Fix] Fix hybrid script to support array of tensors (#4494)* [Fix][Hybrid] Fix hybrid script to support array of tensors* add test case* clean up* trigger ci	4
[Relay][VM] Add AllocTensor instruction and better instruction printer (#3306)* Update vm print & add AllocTensor instruction* patch* fix invoke packed* update cmake* tweak move* update invoke_closure* lint* add doc* tweak	2
Fixed strided_slice size (#7659)Co-authored-by: Akira Maruoka <akira.maruoka@fixstars.com>	0
Add test case of argmax for detecting out of bound access (#2234)	3
[tvmc] Fix command line argument variable name (#6574)* This is a variable that we changed names during code review,   and it was incorrectly left behind with the previous name.	4
[Relay][Frontend][Onnx] Add support for onnx sequence operators. (#11894)This PR adds support for Onnx sequence operators introduced in opset 11. Specifically I've added converters for `SequenceConstruct`, `SequenceInsert`, and `ConcatFromSequence`, which we found sometimes show up in models exported from Pytorch. For simplicity, I handle these cases by just using Tuples. We may want to consider using the TensorArray ADT eventually instead.	1
[Relay] add max_pool3d in relay and TF converter (#4551)* [Relay] add max_pool3d in relay and TF converter* fix comments	0
[TE] Support varargs in te.compute (#9796)* [TE] Support varargs in te.computeSupport varargs (`lambda x, *args: ...`) in te.compute. The varargs takeindices into the remaining dimensions of the outputs shape. Thisrequires using inspect.getfullargspec instead of `fcompute.__code__`.Also add checks that there are no keyword arguments.* implicitly broadcast to remaining dimensions	5
resolve issue #10107 by setting eps larger (#10176)* resolve issue #10107 by setting eps larger* use numpy.testing.assert_allclose	3
[Tutorial] tutorial for tensorize (#1774)	5
[Codegen] Fix assertion errors in llvm backend when using llvm debug build (#7959)	0
add missing equal sign (#7531)	1
allow fallback path to non imagenet workloads (#886)	1
[VTA][Relay] Relay Compilation + AutoTVM compatible operator libraries for VTA (#3135)	1
[Refactor] Avoid Override Generic Op Strategy in "hls.py" (#8614)* [Refactor] Avoid Override Generic Op Strategy in "hls.py"* Fix The Broken CI Test Cases	3
Changed TVMCTVMContext to TVMContext (#6306)	4
[WIN] Fix a bug in find_llvm when specify llvm-config (#2758)	5
[TVMScript] Printer entry point (#12462)This PR:- Adds an entry point for the TVMScript Unified Printer- Adds a helper object class `RootNodeContainer` to provide an injection point for the actual printer implementation to add specialized logic on the root node to print.Tracking issue: https://github.com/apache/tvm/issues/11912	0
[tvmc] Add a --config option to `tvmc compile` (#8253)* Allow to send some configurations to the PassContext via command line * Add various validations to the new option with appropriate error messages * Add unit testing	3
Upgrade TVM to latest version	3
[Compatiblity] backward compatible to mxnet json (#49)	5
[Vulkan][Codegen] Fixed SPIR-V scoping bug with threadIdx (#8281)* [Vulkan][Codegen] Fixed SPIR-V scoping bug with threadIdxUnlike in Cuda, where the threadIdx.x/y/z are separate built-invariables, in vulkan the built-in thread index consists of an arraythat must be dereferenced.  If `te.thread_axis("threadIdx.x")` isfirst declared inside a scope, then that same python object is passedas the IterVar of a separate scope, this breaks the spirv scopingrules, and may result in an undefined variable.  This only applies tothreadIdx/blockIdx variables, as all other variable declarations obeytir's scoping rules.To resolve, the threadIdx and blockIdx variables are declared at thetop of the function declaration.  This makes the generated spirv codefollow the same semantics as tir/cuda.* [Vulkan][Codegen] Refactor of GetLocalID and GetWorkgroupIDShared behavior separated out into GetBuiltInValue.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
removed non-determinism from CanonicalSimplify (#704)* 1) removed non-determinism from CanonicalSimplify2) added couple of testcases for CanonicalSimplify* Use IRDeepCompare instead of comparison of string representation* Give a warning (instead of fatal error) when two "ComExprEntry"s are equal	1
Consistent result of DetectLinearEquation() when an empy vars is passed (#2860)	4
[Metaschedule] Support tuning on rocm and vulkan target (#11017)	1
[AutoScheduler][Relay] Control compile engine cache via PassContext (#7220)* [AutoScheduler][Relay] Control compile engine cache via PassContext* lint* lint	4
[DOCS] Add prerequisites about zlib1g-dev (#446)* [DOCS] Add prerequisites about zlib1g-devinAdd prerequisites about zlib1g-dev. It occurs `/usr/bin/ld: cannot find -lz` without zlib1g-dev.* Add prerequisites about python-setuptools Add prerequisites about python-setuptools. Otherwise, it will fail when executing `python setup install --user` command.* [DOCS] Add prerequisites about python-devAdd installation prerequisites about python-dev. Otherwise, it will fail with `SystemError: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel.` when executing `python setup install --user`.	1
[TOPI] Fix group_conv2d unit test (#3113)	3
[Relay] Implement `SoftmaxRel` for softmax operators. (#11728)* Implement `SoftmaxRel` for softmax operators.* Print better error message for wrong axis.	0
[Pass] Simplify consecutive casts in Relay (#10133)* initial commit* initial commit* update test* jostle	3
check in (#2484)	5
[TEST] Disable flaky TF combined NMS test (#8364)Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>	3
[Relay][Op] Adaptive pooling (#3085)* Add topi adaptive_pool* Use adaptive_pool to compute global_pool* Add relay adaptive pool2d* Fix lint* Fix typo* Minor change* Change support level to 10* Add contrib* Remove global pool schedule* Add contrib module* Fix lint* Update doc* Update doc	2
allow customize mkldnn library location (#4814)	1
fix icelake target for avx512 and vnni (#9928)	1
[UnitTests][Contrib] Enable contrib tensorrt/coreml unit tests (#8902)* [UnitTests][CoreML] Marked test_annotate as a known failure.The unit tests in `test_coreml_codegen.py` haven't run in the CIlately, so this test wasn't caught before.  (See tracking issue- Added `pytest.mark.xfail` mark to `test_annotate`.- Added `tvm.testing.requires_package` decorator, which can mark tests  as requiring a specific python package to be available.  Switched  from `pytest.importorskip('coremltools')` to  `requires_package('coremltools')` in `test_coreml_codegen.py` so  that all tests would explicitly show up as skipped in the report.- Added `uses_gpu` tag to all tests in `test_coreml_codegen.py`, since  only ci_gpu has coremltools installed.  In the future, if the ci_cpu  image has coremltools installed, this mark can be removed.* [Pytest][TensorRT] Mark the TensorRT tests with tvm.testing.requires_cudaPreviously, the tests had an early bailout if tensorrt was disabled,or if there was no cuda device present.  However, the tests were notmarked with `pytest.mark.gpu` and so they didn't run during`task_python_integration_gpuonly.sh`.  This commit adds the`requires_cuda` mark, and maintains the same behavior of testing thetensorrt compilation steps if compilation is enabled, and running theresults if tensorrt is enabled.In addition, some of the tests result in failures when run.  Thesehave been marked with `pytest.mark.xfail`, and are being tracked inissue #8901.	5
speed up reference resize kernel (#8592)	5
[dyn.nn.pad] cast pad value to input dtype (#10818)	5
[Relay][Topi] Fix GPU NMS when return_indices is True (#7005)* Add rearrange_indices* Fix output type* Clean test* Fix pylint* Fix CPU nms multi-batch* Diable test* Minor fix* Minor fix	0
[microTVM][AutoTVM] Fix autotvm bug and tests (#9003)* debuggging* cleanup and fix tutorial, zephyr and crt test* fix crt test* address comments	1
[PROFILING] Add json output to profiling reports (#8503)* [PROFILING] Add json output to profiling reports* format json in comments	5
[RELAY] IR builder stablize refactor, clean pass (#1934)	4
[PASS] Update coproc sync (#634)	5
[HEXAGON] QCOM hexagon library (qhl) (#12149)* qcom hexagon library (qhl)* fix lint errors* fix lint errorsCo-authored-by: aakaverm <aakaverm@qti.qualcomm.com>	0
[RELAY][PYTORCH]isNan, isinf, isfinite, ceil, clamp, round ops (#5316)* [RELAY][PYTORCH]isNan, isinf, isfinite, ceil, clamp, round ops* Review comments	5
[QNN] Enable constant folding for QNN operations. (#11228)* [QNN] Enable constant folding for QNN operations.This commit enables constant folding for QNN operations.This functionalty is disabled by default, use fold_qnn=True to enable.Co-authored-by: Alexander Peskov <peskovnn@gmail.com>* [NFC] Fixed comments* Added more unit tests for QNN opers in constant folding pass.* Address PR feedbacksCo-authored-by: Alexander Peskov <peskovnn@gmail.com>	5
[MAINTAINER] add srkreddy1238 as reviewer (#1305)	1
Fixed bugs for SSD sorting and multbox detection (#1578)	0
clean up conv2d type rels (#10236)	4
fix copro_sync.cc errors of ctx (#1274)	0
[CMAKE] Update cmake to support OSX/Linux (#228)	1
fix error when memory_id is VTA_MEM_ID_OUT (#4330)	0
Add test to confirm that we forbid allocate statement referencing undefined variable (#1899)	5
[AutoScheduler] Improve warning messages (#6935)* [AutoScheduler] Improve warning messages* fix lint	0
[FRONTEND][TFLITE]Logical not op support (#5475)	1
[CI] Hotfix CI (see #7010) (#7025)	0
[AutoScheduler] Fix conv3d's op strategy for auto-scheduler (#7328)	0
[Relay] Fix compiler warning in ExtractOperators (#9075)* [Relay] Fix compiler warning in ExtractOperatorsFix clang warning:  'OperatorExtractorWrapper::VisitExpr_' hides overloaded virtual functions* Restart CI	1
[Torch] Remove unused conversion (#8397)* fix weight shape in torch.mm conversion* Revert "fix weight shape in torch.mm conversion"This reverts commit a1a8fd313c999060db675848f8b3de3e1c78e468.* [Torch] remove unused conversion	1
[TOPI]fix scatterND large shape problem (#12200)* fix scatterND large shape problem* fix thread pool alloca* add scatternd unit test* update with comment* EmptyCo-authored-by: wrongtest <wrongtest0@gmail.com>	3
[TensorIR][M2a] Verification of cached flags (#8114)* [TensorIR][M2a] Verification of cached flagsCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Address comments* Update src/tir/schedule/analysis/verify.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[FIX] Detect like cores by looking at scaling_max_freq instead of (#8370)cpuinfo_max_freq	5
[microTVM] Update support for ARMv7m intrinsic (#8990)* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Formatting fixed.* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Issue 8717 Add schedule for depthwise_conv2d_nhwc* Resolve merge conflict.* Resolve merge conflicts.* Fixed formatting.* From Issue 8717//Fixed micro model library test. Checking size reduced to 16 bytes from 2466816.* From Issue 8717.Removed changes.* From Issue 8717. Fixed typo.* Fixed import.* Fixed import and method call.* Added QEMU testing comment.* Fixed ZEPHYR_BOARD usage.* Fixed tests. Removed issue 8717 changes.* Formatting fixed.* Removed test call from base_box_test.sh	3
RFC: initial stab at TorchScript fallback (#7401)* initial stab at TorchScript fallback* make types more flexible* Easy review bits. Thank you @masahi	1
fix parameter name in UnrollLoop (#679)In unroll_loop.cc the parameter name is "auto_max_depth", but in ir_pass.h the parameter name is "auto_min_depth"	2
[Relay/TOPI][OP] Add clip and wrap mode support in take (#2858)* Update take* Add special case for canonical simplify and fix test cases* Use lower case for wrap and clip* remove unnecssary lower* Fix mxnet converter for take* fix	0
[TEST/PYTHON] Add unittest folder, add a build pipeline. Rename Buffer.ptr to Buffer.data to be consistent with Array. (#29)	5
checkin dfs visit from min	5
Add missing check when deciding conv op and injective op are in the same group (#1622)	1
[NNVM] Elu support added in darknet frontend (#1199)	1
[DOCKER] Add demo-gpu image (#1407)	1
Fix prelu bug in pytorch frontend (#8192)* Fix prelu bug in pytorch frontend* Fix lint error* fix lint error* Fix lint error* Try to fix lint error* Fix lint errorCo-authored-by: huangyuheng <32429436+hyhzxhy@users.noreply.github.com>	1
Fix incorrect AOT Memory Planning (#8926)This change introduces a second memory planning phase in the AOT codegenerator once the storage rewrite pass has been completed, fixingincorrectly sized workspaces for a variety of models.It comes with accompanying tests so we can safely refactor this later.Also corrected a typo in the TE compiler regards the memory alignmentargument :smile_cat:Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>	1
[Target][BugFix] Convert dict and str to TVM object (#9807)* [Target][BugFix] Convert dict and str to TVM object* Add tests	3
[RUNTIME] Add interface header of runtime (#15)* [RUNTIME] Add interface header of runtime* fix mac build	0
Enable Alias, refactor C API to reflect Op Semantics (#41)* Enable Alias, refactor C API to reflect Op Semantics* add alias example	1
[RELAY] IR Wellform Checker (#1748)	5
[Relay][Prelude] Remove Peano nats from the prelude (#3045)	4
[RELAY][IR] Introduce IdNode to preserve var id across rewriting (#2178)	5
Fix USMP parallel to serial loop transform test (#9254)Caused by https://github.com/apache/tvm/pull/8469 being stale on merge when https://github.com/apache/tvm/pull/9115 had changed the namespace for `tvm.script`.	4
[COMMUNITY] Siyuan Feng -> reviewer (#7836)	3
Tutorial: update Building a Graph Convolutional Network tutorial (#4060)* update build_gcn.py tutorialupdates* support bias in GCN layer* download pretrained gcn model* verify model accuracy* use time_evaluator to measure runtime* fix adding bias in gcn layer* remove printing output* fix small bug* add DGL-PyTorch comparison into the build_gcn tutorial* add accuracy testing* adjust import order* handle different dgl versions* update number for dgl version checking	5
[WIP] C++ topi contributions (#312)* [WIP] C++ topi contributionsSummary:This diff implements C++ topi contributions for:  - relu with parametrix threshold  - pad with generic padBefore / padAfter specification  - matmult with transposes  - conv2d_nchw, conv2d_hwcn with runtime constant padding and strides  - depthwise_conv2d_nchw with runtime constant padding and strides  - group_conv2d_ngchw with runtime constant padding and strides  - broadcast_to a broadcastable shape  - broadcast_bop where bop is an usual binary op (+ - * / %)Convolution padding is implemented using the pad operation.To avoid extra memory consumption, it is generally recommended to inline the padding with the autoinliner.Unfortunately in its current form the elemwise checks are too restrictive to allow inlining.So this diff also proposes an extension to LHS injective (i.e. no reduction axis in the current IR design)Test Plan:Tested in C++ testsuite in a separate repository, I am looking for suggestions to quickly spin up some tests for tvm.Reviewers: tqchenSubscribers:Tasks:Tags:Blame Revision:* Review + Lint + GSG C++	3
Update ubuntu_install_paddle.sh (#9082)	1
[SCHEDULE] Detect useless bound early (#264)* [SCHEDULE] Detect useless bound early* fix	0
[Docs] Corrected typo in googletest build instructions. (#8459)Incorrectly specified "MAKE_SHARED_LIBS" option instead of"BUILD_SHARED_LIBS".  In future, perhaps googletest should be pulledinto 3rdparty submodules to minimize the setup of a dev environment.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TIR][Schedule] fix tensorize example (#12146)	0
[BYOC] [ACL] Update ACL to 21.08 (#9396)This PR changes ACL version to v21.08*ACL stands for "Compute Library for the Arm® Architecture"	4
[NVCC] Bugfix nvcc command tool that relies on the compile time env (#7964)* [NVCC] Bugfix nvcc command tool that relies on the compile time env* Update python/tvm/contrib/nvcc.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
PyTorch frontend: fix handling of duplicate use of a model weight (#5897)This happens e.g. in shared input/output embeddings in BERTor siamese networks.Thank you @siju-samuel for reporting.	1
[DOCS] Add intro tutorial (#45)	1
[ONNX][TOPI][RELAY] Resize refactor (#7883)* adds rounding mode for nearest neighbor, passing onnx unit tests for nearest neighbor* passing all linear test. passing all nearest tests except crop and resize, which needs a dynamic implementation of crop and resize* most of the bicubic tests are working* fix exclude outside* remove dead code* fix lint* fix defaults to match old implementation* fix lint* fix gpu tests* fix lint again* change order of operations to prevent GPU rounding errors	0
[OpenCL] Change of OpenCL profiling logic (#11180)* Enable profiling only when it is used explicitly* Change logic of clCommandQueue create/destroy* Update comments* Linter fix* Refactor queue create* Move queue recreation logic to function* Replace profiling flag by the queue info request* Enhance readability* Fix linter errors	0
[Hexagon] Use nullptr instead of 0 in hexagon_device_sim.cc (#6718)Passing 0 produces compilation warnings.	2
Rev ci-qemu container to v0.04. (#7946)	5
[Minor][MetaSchedule] Suppress warning for using `None` (#11868)	1
[TF] Add DenseBincount support (#12728)	1
[AutoSchedule] Extract tasks via compile engine (#6903)* make use TOPI schedule optional* extract auto_schedule task* format* add extract mode* silent autotvm* fallback to TOPI* use PassContext* lint* surpass fallback warnings* nit* fix test* address comments* address comments* doc* address comments* lint* skip unsupported tasks* reigger CI	1
[Frontend][TFlite] Add parser support for relu6, leaky_relu, relu_n1_to_1, log_softmax (#4805)* [Frontend][TFLite]Add support for relu6, leaky_relu, relu_n1_to_1, log_softmax* add implementation in parser* add qnn tests for each operator* Implement clip operation for quantized relu6, relu1* add 'clip' as in the quantized fused operations* remove redundant assertions and imports* Fix floating value quantization for RELU6 and RELU1	0
[BugFix] IndexMap.Inverse for unit iters (#11841)	0
[COMMUNITY] @masahi -> Committer (#2252)	3
[Frontend][Relay] Keras softmax and prelu fix (#6278) (#6278)* prelu and softmax with NHWC layout consideration* fix lint* fix lintCo-authored-by: Dongming Yang <dongming.yang@streamcomputing.com>	0
[DOCS] Fix figure links (#7268)	2
[PYTHON] Make IntImm more like an integer (#5232)	1
[CONTAINER] Add default python iterator for Map. (#8061)* [CONTAINER] Add default python iterator for Map.* formatting* add keys(), values()	1
[REFACTOR][IR] Add Node suffix to low-level IR nodes (#4649)* [REFACTOR][IR] Variable -> VarNode* [REFACTOR][IR] Add/Sub/Mul/Div -> AddNode/SubNode etc.* [REFACTOR][IR] Min/Max/FloorDiv/FloorMod -> MinNode/MaxNode etc.* [REFACTOR][IR] EQ/NE/LT/LE/GT/GE/Select -> EQNode/NENode etc.* [REFACTOR][IR] Add Node suffix to Select/Call/Load/Ramp/Shuffle/Let* [REFACTOR][IR] Add node suffix to IntImm/UIntImm/FloatImm/StringImm* [REFACTOR][IR] Add Node suffix to Any, AttrStmt, AssertStmt* [REFACTOR][IR] Add Node suffix to Store/Provide/Allocate/Free* [REFACTOR][IR] Add Node suffix to ProducerConsumer* Fix lint* style updates, test fixes	0
[TVMC][MicroTVM] Fix tvmc micro `project_dir` arg relative path (#9663)* Add fix for project dir path* address @gromero comments	1
[CMSIS-NN] Fixed error in finding input's dtype in maxpool (#11701)	0
[CMSIS-NN] Fixed return data type from pattern callback function (#9682)This commit fixes the following issue: MergeComposite callback function from pattern table returns non boolean data type	5
[Target] Remove deprecated parameters from target (#12416)* remove depricated parameters in target* lint* fix cpp testsfix* remove more configs in test files* address comments* fix error* fix hexagon* fix micro tutorial* fix integration tests* fix hexagon* lint* fix unittest* fix readme* fix assert executor in target* address comments* fix tutorials* fix hexagon target* fix tutorial* fix for tutorials* hexagon	0
[Rust] Some rust cleanups (#6116)* Some rust cleanups* Turn off default features for bindgen* Upgrade some deps for smaller total dep tree* Switch (/complete switch) to thiserror* Remove unnecessary transmutes* Fix null pointer assert* Update wasm32 test	3
[TOPI][x86] Introduce schedule_injective_from_existing and unify external schedules for all targets (#3983)* Fix extern schedule for x86* Register x86::schedule_extern* Fix* Fix* Replace extern.py with extern.h* Introduce new generic function schedule_injective_from_existing* Fix* Fix* Add back to C++* Fix style* Injective schedule calls local schedule_injective_from_existing* Fix* Remove target arg from schedule_injective_from_existing* Fix docs* Try to fix unit test* Fix test* Fix other tests* Fix bug	0
[ci][1/2] Shard `frontend: GPU` job into 2 jobs (#10413)This is the longest individual CI job by about an hour, meaning everything else is usually done and waiting on this job for a while before the entire build completes. This PR breaks it up into two roughly equal jobs (based on timings in https://ci.tlcpack.ai/job/tvm/job/main/2623/testReport/, both should take about 90 minutes). If capacity is available, this means CI jobs could potentially take 1 hour less. If not available, besides an insignificant queueing delay this PR has no effect.This is a two part PR since the Jenkinsfile changes cannot be bundled in this PR, so they will need to be in a follow up.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Fix] Fix some typos (#11503)Fix some typos in src/.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
fix java checkstyle version (#2998)	0
[TVMScript] support kTarget func attr in tir script (#9594)* support kTarget func attr in tir script* fix variable redefine lint error* use Target::Export()* fix cr issues	0
[TENSORFLOW]StatefulPartitionedCall/PartitionedCall Ops support added  (#5617)* Implemented functionInvocation Unit Test for StatefulPartitionedCall operator(working) and initial changes for placeholder(not working as of now)* Placeholder exercises with tvm* placeholder interim* SPOP Test cases structure* New test cases for spop* miscellaneous test cases for spop* Placeholder samples..working with shapes explicitly passed* Variables test case. Works with the same fix of shape_dict* SPOP Positive test cases first iteration* support output tensors as function args, multiple functions* Corrected Indentation* filewritter is only for debug purpose* support variables in function args* First working iteration of positive spop test cases* Removed commented code, simplified code* Code Reorganization- First working iteration of positive spop test cases* corrected variable name after refactor* Code Reorganization- First working iteration of positive spop test cases* move code inside mapped operator function* Removed extra line* support variables in function args* Removed commented code, simplified code* move code inside mapped operator function* Code Reorganization- First working iteration of positive spop test cases# Conflicts:#tests/python/frontend/tensorflow/test_forward.py* Code Reorganization- First working iteration of positive spop test cases* Function invocation more test cases* Simplified & Merged different Function Invocation Test cases* support invocation of nested callablesno need to explicitly handle paratitioned andstatefulPartitioned condition in convert_operator function* Simplified and Uniform testcases* support invocation of nested callablesno need to explicitly handle paratitioned andstatefulPartitioned condition in convert_operator function* Simplified and Uniform testcases* removed duplicate and renamed testcase* Negative scenario added for testing operator statefulness. Only Exception to stateful operators are Partitioned & StatefulPartitionedOp which have capability to execute even stateless operators within them* Miscellaneous reorganization changes for spop scenarios* Miscellaneous reorganization changes for spop scenarios* Corrected import of tensorflow modules safely using try except and other code reorganization* Negative scenario for resource variables handled* Documentation update for code* SPOP change in function handling* handle nested subgraph* refactor* get op def compatible with tf 1x & 2x* Fixed liniting issues* added doctsring and few nits* Merged changes for positive test cases and negative test cases* Moved StatefulPartitionedCall test case to the end of the TC list* Fixed some typos and semantics* dmlc-core* dmlc-core* fixes* Addressing Review comments in the PR for SPOP support* Fixed pylint errors* Corrected tensorflow import syntax* Placed the op_def_registry module import outside of for loop* Removed new stateful operators list and combined these operators with missing operators to display as single list. Also removed throwing seperate exception for stateful opsCo-authored-by: Prashant Sail <psail4444@gmail.com>Co-authored-by: maheshambule <mahesh_ambule@persistent.com>	4
[Relay] Non-recursive dependency graph (#9528)* fix recur* fix ci	0
[TOPI] add reshape, concatenate, split (#481)* [TOPI]add reshape* fix problems* fix lint* try to add concatenate* fix lint and error* fix doc* fix error* try to add split* fix lint* fix error* fix lint	0
[BYOC][JSON] Support input nodes with multiple entries (#6368)* Support input nodes with multiple data entries* Rename input_var_idx_ to input_var_eid_	5
[TVMC] compile: Check if FILE exists (#10608)Currently when a non-existing FILE is passed to 'tvmc compile' it throwsa traceback because a FileNotFoundError exception is not handled. Sincethere is no need for such abrupt exit, and the trace can also confuseusers, this commit fixes it by checking if FILE indeed exists, informingthe user about the non-existing FILE before exiting.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	2
register conv2d_transpose as generic function (#1137)	1
[TEST] Jenkinsfile (#389)* [TEST] Jenkinsfile* Fix wheel setup	1
Parametrize ONNX Unit tests (#8621)	3
Improve gemm tutorial (#800)	1
[microNPU] Some housekeeping in the test_ethosu folder (#10824)* [microNPU] Some housekeeping in the test_ethosu folder* Move the utility functions from test_codegen.py into infra.py for  wider accessibility* Remove some unused code* Make the conv2d codegen tests more general* Update test_identity_optimizer.py* Update test_lut_optimizer.py	3
[Fix] Fix recursive let for well formed check (#5780)	0
[Docker] Fix ordering of tf and tflite installs in ci_qemu (#8315)Similar to #8312, the recently merged #8306 required tflite to beinstalled after tf.  However, #8306 did not correct the ordering inthe dockerfiles.  After this and #8312, all dockerfiles should be upto date with the correct ordering.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Relay][QNN] Simulated Quantize and Dequantize (#7613)* Add initial implementation of flexible simulated qnn ops.* Added proper topi testing and fixed qnn axis bug.* Add injective schedule wrapping.* Stuck on typerel problem.* Relay integration fully working.* Simulated quantize totally finished.* Change dtype to be a scalar rather than tensor.* Undo change to quantize.* formatting.* Fix attritubes.* Fix negative axis dequantize bug.* Add topi simulated dequantize.* Add simulated_dequantize op to topi and relay.* Formatting.* Test negative axis perchannel dequantization.* Lint formatting.* Change import order to make lint happy.* Fix pytest.* Directly return make call.* Clarify disable mode for simulated qnn ops and fix typos.* Line too long oops.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>	2
Add TVM application extension with WASM runtime (#5892)* Refactor wasm runtime module and resovle conflict errorsSigned-off-by: leonwanghui <wanghui71leon@gmail.com>* Fix some cargo clippy warningsSigned-off-by: leonwanghui <wanghui71leon@gmail.com>	2
Fix conda package builds (#1445)	0
[LLVM] Clarify the status of pointers to llvm::Module in LLVMModule (#12123)LLVMModule has two members that refer to llvm::Module:1. llvm::Module* mptr_, and2. std::unique_ptr<llvm::Module> module_.The mptr_ member is always valid and it points to the llvm::Module.The unique pointer takes care of the ownership of the llvm::Module, anddeletes it when no longer needed. However, once JIT is initialized, ittakes over the ownership of the llvm::Module, and the module_ memberbecomes null.To avoid checks for null, use the raw pointer whenever accessing thellvm::Module, and only use the unique_ptr as the owner/deleter.The member names are changes to reflect their roles:  module_ -> module_owning_ptr_  mptr_   -> module_	4
[nnvm] fix nnvm compiler build module error (#3378)	0
[FRONTEND]onnx, mxnet, pytorch mathops added (#5561)	1
allow missing FCorrectLayout (#457)* allow missing FCorrectLayout* misunderstood OpMap[], fix	0
[Target] Replace IsaAnalyzer with Target Features (#12322)This is clean up to use the new `target.features` instead of `IsaAnalyzer`.	1
[microNPU] Force compute_cycles_hint to be interpreted as an int64 value (#12558)`compute_cycles` can be the size of an int64 value, however it seemsthat when that value is attached to the IR as a pragma from Python,it is interpreted as an `int`, rather than `int64_t`. This commit addsan explicit cast to ensure the value is interpreted correctly.The reason these values started appearing very large and randomly isstill yet to be solved, although the hope is that this fix will unblockCI.Change-Id: Idcdd7d37af1acd665590c87624446a025b50eb3d	4
Fix auto scheduler crash when set with consumers is empty (#7708)Set with consumers is empty during preparing auto scheduler sketchesfor Metal device. Added check on the size of the set.In case when the set with consumers is empty we just skip this rule.	1
[VTA] Support network which have no unique operator as start/stop name for graph pack. (#4703)* [VTA] Support network which have no unique operator as start/stop namefor graph pack.[Issue]  Current vta use 'start' and 'stop' name to define the pack start point  and end point, but this method not work for these network which have  no 2 unique operator as  start point and stop point.[Solution]  In this solution we give 2 addtional parameters start_name_indx and  stop_name_indx to make vta pack logic work with the said network,  for exampl for following networks which have no unique operator,  %0 = nn.add  %1 = nn.conv2d  %2 = nn.batch_norm  %3 = nn.leaky_relu  %4 = nn.add  %5 = nn.conv2d  %6 = nn.batch_norm  %7 = nn.leaky_relu  %8 = nn.add  with this solution we can use following parameter format to make  vta work on it.  relay_prog = graph_pack(                //....                start_name="nn.add",                stop_name="nn.add",                start_name_idx=0,                stop_name_idx=4)  to apply on new network, by printing the network we can get index information like following.  print(mod.astext(show_meta_data=False))  relay_prog = graph_pack(mod                          ...                          start_name="nn.add",                          stop_name="nn.add",                          start_name_idx=0,                          stop_name_idx=4)* address review comments and fix index count bugissue:when do print(mod), the output not only the Call is also have other typelike Var, need add logic to count all except meta.solution:add related logic* address review comments.* address review comments* add more detail comments.	1
[µTVM] Add ST STM32F746 disco board to tflite tutorial script (#7254)Currently tutorial script 'micro_tflite.py' assumes that all boards withtarget STM32F746 are Nucleo boards. As a consequence once that target isselected the script automatically defaults to the Nucleo board. However,the STM32F746 is also used on Discovery Kit boards (aka disco) which arequite similar but have some differences, so Nucleo config and final imagedon't work on the disco boards.That commit adds a way to select a different dev board and adds commentsaccordingly, informing how to use the script with STM32F746 disco boards.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[FIX,PYLINT] Fix pylint errors on MacOS with Python 3.8 (#6746)These errors do not seem to show up in CI, but they show up locally.	0
[Team] Jian Weng -> Committer (#3359)	5
[ci] Use r5.large nodes for hexagon build and some tests (#11120)* PR #11314 - [ci][docker] Update images to include sccache changes* [ci] Use r5.large nodes for less-intensive jobsThis uses the `CPU-SMALL` label for certain jobs in CI, which is backed by r5.large instances in EC2 rather than c4.4xlarge instances which are much more expensiveCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Bugfix] Fix sort changing original input data issue (#3212)* sort bugfix for not rearranging input data* separate sort schedule* fix lint* use identity op instead* fix lint* remove redundent code	4
[CI] better deletion script for pycache (#4635)	4
Add end-to-end SGX ResNet inference example (#388)	5
[CUTLASS] Fix hardcoded include path and logic for profile_all = False case (#9399)* fixed hardcoded cutlass include path* fixed profile_all = False case* add cutlass cmake option* check if cutlass path exists* improve err msg when cutlass is not found	1
[ONNX] Skip multiply with 1.0f constant for GEMM import (#5800)* [ONNX] Skip ADD inside Gemm op when vector is zero* [ONNX] Skip multiply with 1.0f constant for GEMM import	2
[MetaSchedule] Filter vector_load_lens based on buffer dtype (#12408)This makes the same config generic to work across workloads with different types.	1
[RELAY][DYN] Implementation of the dynamic pad operator (#6284)	1
Adding Hua Jiang as reviewer. (#4993)	1
[QNN] Add per-channel quantization to add/subtract/multiply (#10718)* Add per-channel quantization to QNN add/subtract/multiply* Add feedback* Add feedback - round 2* Fix for arm test* Add params to the test* Try again* Try int* Move lhs_axis and rhs_axis* Add as an attribute* Add quotes	1
WithFields for Tuples (#9533)	5
Fix str decoding error on non-English Windows (#2158)	0
Add compile library tutorial (#277)* Add compile library tutorial* Clean output* Refactor with sphinx gallery* Refactor* Change title and other minor fixes	0
Support PyTorch grid_sample  (#10184)* [relay] Fix stack overflow in device_planner observed on windows due to recursive function calls.* Revert "[relay] Fix stack overflow in device_planner observed on windows due to recursive function calls."This reverts commit 70581364771e2415b37b202a9fa6a937f275cfc6.* [PyTorch] Add grid_sample with zeros and border padding mode for PyTorch.	1
add split infer shape with convert op layout pass (#11825)	4
[LLVM] Fix build breaks from StringRef changes (#4923)- llvm::StringRef to std::string conversion is explicit now.Signed-off-by: Wei Pan <wpan11nv@nvidia.com>	4
[TIR] Tuple Reduction Support in CreatePrimFunc (#10671)* [CreatePrimFunc] Support multi-source ReduceNode (#64)* initial* assert structural equal test* Enhancement and tests* Fix dtype* DocsCo-authored-by: Andrew Liu <andrewlliu@gmail.com>	2
[ETHOSN] Allow Ethos(TM)-N testing without hardware (#9702)If no hardware is available, simulated tests allows executionof the Ethos(TM)-N - TVM integration. This patch allows thisfor the 21.08 release of the Ethos(TM)-N driver stack.	1
[AutoScheduler] Improve doc string (#6176)	2
[TE] Light refactoring of TE -> TIR paths. (#9263)* [TE] Light refactoring of TE -> TIR paths.- Added ScheduleToPrimFunc, extracting out common behavior in  ScheduleToModule and auto_scheduler's feature extraction.- Added `tvm.driver.build_module.schedule_to_module`, to avoid needing  to 4-line boilerplate needed to do so.  Also makes deviations from  the usual path (e.g. `debug_keep_trivial_loop`) much more explicit.* Removed schedule_to_primfunc, replaced usage with schedule_to_module.* Returned C++ function ScheduleToPrimfunc to be inside ScheduleToModule.	1
[microNPU] Fixing imports in the entry point (#9624)This commit fixes errornous reporting that Velais missing if other import errors.Change-Id: I8db97be10018726cf5d9483508321a176c212516	5
[CI] Update ci-wasm to latest (#6772)	3
[Runtime][ThreadPool] Remove a cout log output. (#10560)There is a debug std::cout logic left in thread_pool.cc, remove it.	4
[TOPI] Rewrite GPU argwhere using exclusive scan (#7314)* use ex scan to write argwhere* add doc	2
[REFACTOR][TIR] Provide->ProducerStore, Realize->ProducerRealize. (#5750)This PR finishes up the final step for DSL/TIR de-coupling to refactorProvide/Realize to use the DataProducer.As in the case of ProducerLoad, ProducerStore/Realize are not supposedto appear in a vaid TIR function ans are only used by high-level DSLsas intermediate structures.	1
[Torch] Fix PyTorch NMS conversion for negative scores (#7137)* Fix pytorch nms conversion for negative scores* updated mask rcnn test to verify outputs and also run cuda target* set rpn_post_nms_top_n_test to 200* fix parameter name* dump output box information* simplifying	5
[TOPI][RELAY][OP] add op crop_and_resize (#4417)* [TOPI][RELAY][OP] add op crop_and_resize* fix pylint* incorporate comments* fix ci	0
fix linting command instruction (#1919)	0
[FIX] Pad feature vectors to the same size in xgboost cost model (#11479)* [FIX] Pad feature vectors to the same size in xgboost cost model* add test* more test* explaination* formatting	3
Making CMSIS-NN tests pylint compliant (#11625)	3
[VTA] RPC path update. (#3924)Issue:RPC path get changed into "vta_rpc" from "pynq_rpc", but relateddocument still use old informaiton.Solution:Update RPC path information.	5
[DOCS] Add background context for stack allocation in builtin lowering (#10632)	1
[TOPI] VNNI support for int8 dense (#10230)* wip* revert for now* simplify blocking* add bench script* update type rel* refactor tests* end to end compilation working* paralleize outer loop* add shape check* fused schedule first cut* restore original test* black* add vnni check* add relay test* skip on ci* check dtype* lint* make it tunable* minor cleanup	4
Add onnx opset v13 support for softmax, logsoftmax (#8625)* add more support for softmax ops* noop* noop	1
Add count_include_pad support (#498)* update tvm submodule* Add count_include_pad support to avg_pool	1
Fix dense (#4728)	0
[DOC] Add schedule_computaion (#92)* [DOC] Add schedule_computaion* Finetune the doc* Finetune the doc* Finetune the doc* Set max_unroll_step=0 by default	1
Fix the meaning of conv{1,2}d_transpose output_padding parameter. (#5758)* Add output_padding to generic* Add output_padding to the reference impl* Add output_padding to arm_cpu* Add output_padding to the test* Add output_padding for cuda* Add output_padding for x86* Make use of the new output_padding argument in Relay* Adjust conv2d_transpose Relay test* Fix lint errors* Fix the VTA declaration of conv2d_transpose* support for output padding in conv2d transpose* some output padding will break IR pass* Fix new conv2d_transpose test* Update tophub* Fix conv1d output_padding too.* Fix the conv1d_transpose reference function.* Fix the cuda impl* fix the topi test for conv1d* format* Add tests for conv1d_transpose output_padding and some check that the values are valid.* Add check in the implementations* Add checks to the implementations of conv2d* Make use of the output_padding argument from topi in relay.* Fix relay tests asking for invalid output_padding* Fix line length* Fix vta tests* Update tophub references* Trigger CICo-authored-by: Thierry Moreau <tmoreau@octoml.ai>	5
[Torch] Add aten::roll support for Swin Transformer (#9371)* add test* first impl* basic example working* all test cases working* support adaptive avg and max pool* cleanup* axes transpose logic fixed for roll* pylint* fixed roll dim indexing	0
[REFACTOR][RPC][PROCOTOL-CHANGE] Modularize the RPC infra (#5484)* Update dmlc-core which was mistakenly overriden* [REFACTOR][RPC][PROCOTOL-CHANGE] Modularize the RPC infra.This PR refactors the RPC protocol to make it more modularized.- RPCSession: represent a set of features that need to be implemented- RPCEndPont: End point that forwards the RPCSession requests over a communication channel.- RPCModule: Exposes an RPCSession as an rpc device in the TVM Runtime API.In the new design, the local machine is presented as a special case of RPCSession.The remote is just another client session that calls into RPCEndPoint.The RPC communication path is as follows.```client -> ClientSession -> EndPoint[client@n0]-> networking[between n0 <=> n1]-> EndPoint[server@n1] -> LocalSession[@n1]```Because of the new modular design, we can now chain more sessions together.For example, we can now run the following proxy setup (testcase in test_runtime_rpc.test_session_constructor).```client -> ClientSession -> Endpoint[client@n0]-> networking[between n0 <=> n1]-> Endpoint[server@n1] -> ClientSession -> Endpoint[client@n1]-> networking[between n1 <=> n2]-> Endpoint[server@n2] -> LocalSession[@n2]```We can also implement other types of Sessions.As an example, We introduced a PopenSession that communicates withthe another process via a pipe.We also add more comments about the internal of the RPC.The communication protocol is simplfied using a similar convention as PackedFunc.This allows us to further reduce the amount of special remote syscalls.Due to the major improvement and simplification, we are making a non-compatible update to the RPC protocol.It means that the client and server needs to be upgraded to together in order for it to function correctly.This PR also introduces a versioning mechanism to the current RPC procotol,so that future upgrade will be produce more user friendly with error messages.* Address review comments* Remove ld library path	4
[Relay][Pass] Add inline pass (#4927)* add inline pass* IsInline -> IsMarkedInlined* fix comment	0
[Hexagon] Allow undefined symbols in libtvm_runtime.so on Hexagon (#9024)The shared library libtvm_runtime.so (or any other shared library builtfor Hexagon) will not contain definitions of symbols from libc. To avoidundefined symbol errors, turn that check off when building shared libsfor Hexagon.	0
[TESTS] Fix running tests without MICRO (#9867)	3
[ci] Move non-task CI scripts into ci/ folder (#12609)[CI] Update Hexagon image to install boost (#12613)The new image has xgboost installed, which I need for https://github.com/apache/tvm/pull/12587Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/279/pipelineCo-authored-by: masahi <masahi129@gmail.com>	2
Propagate ssh-agent authentication socket (#9926)In case that SSH_AUTH_SOCK is defined, two items will be added to thedocker run command:1) A propageted ssh authentication socket value , to support   an underlying ssh calls inside the running contianer.2) A mounted volume for ssh channel.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>	1
[TVMC] 'tvmc run' --rpc-tracker and --rpc-tracker fail due to argparse misconfiguration (#6762)to be identified as a list of strings, rathat than the expectedstring type.Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>	5
[Bugfix][TF] reset graph after getting tag of savedmodel (#4055)@zhiics @icemelon9	1
[DEBUG] get_node_output : To retrieve out put of any node - for debug purpose. (#820)	0
[REFACTOR][TYPE] Finish move all types to IR. (#4746)* [REFACTOR][TYPE] Finish move all types to IR.- Move definition of Ref and TensorType to ir- Move type_functor.h to public header.- Rename RefType -> RelayRefType for clarity.* Add atol	1
[CI] Update ci-lint to use the latest image that contains clang-format (#5568)	3
Name all the lock guards. (#938)	5
[QNN] Add - Refactoring to C++ (#3736)	4
[MicroTVM] add heap-size to project options (#12390)* heap-size is added to project options* change stm32l4r5zi recommended heap size* change stm32l4r5zi recommended heap size* addressing comments* addressing comments* addressing commentsCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
explicitly use new to avoid exit-time destruction of global state for VM (#6938)	1
[FIX,METASCHEDULER] Fix tune_te (#11676)`tune_te` was broken because it passed a primfunc to `tune_tir`. Now itis wrapped in an IRModule. Also the test is re-enabled.	0
[ETHOSN] Fix tests pylint errors (#12649)This pr fixes pylint errors in tests/python/contrib/test_ethosn as reported in issue #11414.	0
Add Conv3D bindings (#11381)	1
[BUILD] Windows build with cuDNN support (#999)	1
[Relay][Frontend] Simplify parameter handling in Tensorflow frontend (#2993)	2
[RELAY] Port winograd ops to relay (#2356)	5
Exposed lowered func to c++ API. (#4012)So that you can use: `build_mod_.GetFunction("get_lowered_funcs", false);`to get lowered_funcs.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:	3
[REFACTOR][TE][TIR] Call::Halide => ProducerLoad, DSL/TIR decouple. (#5743)In the HalideIR's design, DSL components and IR are mixed together.For example, Call::Halide can containa reference to a function which isconstructed in the tensor expression language.While this coupled design simplifies certain aspect of the DSL construction,it prevents the TIR to evolve as a clean standalone IR:- The additional tensor expression provided in the function is opaque to the IR  and may become obsolete as we transform them.- The duplication of the information in the DSL tensor and IR makes it hard to  design a stand-alone text format (when there are elements shared in the tensor  expression and normal statements).This PR aims to clearly de-couple the TIR from high-level DSL structures(tensor expression),while still provide clear extensions to build DSLs on top of the TIR.We introduce a DataProducer as a base class for high level tensor expressions objectsthat produce data. We then introduce ProducerLoad to replace the Call::Halide usage,so that the Call node can always be self contained and used for low-level calls.The high-level tensor expression DSL can still generate a PrimExpr that contains a ProducerLoad.These PrimExprs contains fragments of information that can be combined together togenerate a low-level TIR PrimFunc.We also state clearly that DataProducer **should not** appear in any TIR PrimFunc.Instead, the high-level DSL layer should lowered DataProducers to Buffers and TIR statementsthat produces these buffers. We can further provide verifications to validate such invariance.Changes:- Introduce DataProducer to serve as a base class for Tensor in tensor expressions.- Migrate use of Call::Halide to ProducerLoad- Migrate the other usages of Calls.We will also create follow-up PRs to migrate the remaining two DSL related IR nodes(Realize/Provide)to use the DataProducer.	5
Improve Rust bindings: Map, Array, String, various IR nodes (#6339)* Fix datatype* Add initialize macro* Add some TIR nodes* Better downcasting* Improve Array and add Map* Convert to new string API* Clean up some warnings* Add ConstIntBound type* Run cargo fmt* Remove debug prints* Add some more ops* Fix some string codeCo-authored-by: Jared Roesch <jroesch@octoml.ai>	5
[BYOC] Bind constant tuples in graph partitioner (#5476)* Bind constant tuples in the graph partitionerChange-Id: I815b32b5445a536c1837369b04f67dbbb0aed900* Add partitioning testChange-Id: I3a492ec8d1beab4830214e3bc8da2a7c80771ca4* Rename test targetChange-Id: Ie32f37c1395ff597c0047ad3a93ed04ce3f3125d	4
improved empty set definition (#346)fix similar problem like https://github.com/apache/incubator-mxnet/pull/4589/files .	2
[TIR][Printer] text format printer considering future parsing use (#5483)	1
[CONTRIB] TFLite Runtime (#4439)	1
[TVM][Bugfix] Fix missing runtime:: (#2966)	1
[FQ2I] Add log op to FQ2I (#10924)* unary op for resize2d and test* renamed test* added log in quantized form* black'd some files* changed suggested commentary	4
Improve docker/bash.sh to handle git worktrees (#5970)* improve error code when git ls-files fails* fix docker/bash to handle git worktrees	1
Remove uses of std::iterator, NFC (#12461)std::iterator is deprecated in C++17. It only defined some member types,and the new recommended solution is to define these members directly.	1
[ONNX] Fix test to disable default ONNX frontend constant folding (#12532)In TVM ONNX frontend, constants are folded by default, which makes `test_load_model__onnx` to fail because it is looking for "params" that were already converted into constants.This patch fixes the test to disable constant folding so that we can assert that "params" in the model are present as expected.	2
[team] add reviewer kparzysz-quic (#5482)	1
[PASS] Enhance LayoutTransform pass (#293)* [PASS] Enhance LayoutTransform pass* Fix* Fix Compilation* Refactor* Refactor* doc* fix* add file	2
[TVMC] Refactoring to document the --target regex and simplify test cases (#7654)* Adds comments to document the regex being used to parse the   --target=value string * Concatenate test cases without reducing the number of asserts   or number of actual tests	3
[Vulkan] Added conversion from bool to float. (#3513)* Added bool to float conversion support to spirv ir builder.* Added unittest for vulkan bool conversion.* Typo fix.	0
Trivial uTVM -> microTVM "spelling" fix to align with branding. (#8905)* Embarrassingly trivial fix remove use of uTVM and replace with the proper microTVM naming convention.	1
Add an option to FQ2I to fail soft or hard (#9660)add testfix lint, rework hard fail conditionalsapparently clang-format didn't take last time I triedI can haz overflowing comments	0
fix python lint warnings (#3145)	2
[METAL] Fix issue with GPU fails (#7819)* [METAL] Fix issue with GPU failsAdded first run to auto scheduler. This run is necessary for checkingthat the generated kernel is correct. When we just run time evaluatorwith incorrect kernel then it is possible that our application on iOSdevice will be added to ignore list because of big number of committedincorrect kernels. One run before running auto scheduling helps us toavoid this problem.Added complete handlers to all command buffers in Metal runtime. Ithelps to handle GPU errors and report about this error to the hostapplication.In case when error happened, we have to create a new stream. Addedmechanism for error handling and streams creating from python interface.* Try to fix QEMU build* Apply comment* Apply comments and fix build* Apply comments and fix lint* Fix CI	0
[RELAY]Testing Inception, Squeezenet, VGG port (#2013)	3
[Torch] Support bincount and scatter_add ops (#6740)	1
[ci][docker] Fix deploy to tlcpackstaging on Docker Hub (#12282)This was previously broken since it wouldn't pick up the new images names and there was an errant `'` floating around	1
[Tuning] Allow multiprocessing spawn to work (on macOS llvm at least) (#8363)* go to callable class* add some documentation and naming* extend comment* manually do logic to avoid bug with pointer comparison* revert changes to light change, correct comment'* more principled change, but also kind of hacky* test other tuning methods* remove check;* jostle CI	4
[CI] Deduplicate and clean XML test reports (#12332)This PR does the following:- When running on a shard,  removes the tests that will not run on the current shard from the suite. We'll no longer get the message `Test running on shard X of Y` in the XML reports. This will result in cleaner test reports.- Adds the shard index or `no-shard` to the generated XML report file name. Currently, the reports generated when sharding is present are overwritten in S3 by the last shard to finish in the CI pipeline. This change is needed as part of #11670.- Since the same tests might run on different configurations (CPU, GPU), it uploads the result of each configuration in a subdirectory in S3 (e.g `/pytest-results/frontend_aarch64`).	3
[microNPU] Add support for unary elementwise CLZ (#9577)Add support for the CLZ (count leading zeros) operatorand the codegen test.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>	3
[RUNTIME][PASS] Allow declare vector type array (#302)* [RUNTIME][PASS] Allow declare vector type array* fix bcast* [BUFFER] Enable vload/store function in buffer* ok	1
[CODEGEN] Enable cross compile of AMDGPU without rocm, update rpc  (#1154)	5
[WIP] Fixing an Infinite Loop case in UnmatchedChecker. (#4881)* save* save* remove* remove cerr	4
[ci][docker] Fall back to tlcpackstaging if images don't exist (#11775)See #11768. This adds a script to check if Docker images exist in `tlcpack` and switch to `tlcpackstaging` if not (the tags must match though). There is also a feature flag for this in jenkins in the `DETERMINE_DOCKER_IMAGES` env variable (which must be set to `yes` for this change to work)	1
Optimize x86 conv3d_ndhwc  using data packing approach. (#4866)Add tuneable conv3d_ndhwc schedule	1
[RPC] graduate tvm.contrib.rpc -> tvm.rpc (#1410)	5
a tiny typo (#4452)	2
[microNPU] Optimize separate padding operation for conv2d (#11468)Optimizes a case where padding appears as a separate nn.pad operation followed by a qnn.conv2d. If possible, the nn.pad will be partitioned and offloaded together with the qnn.conv2d operation, as opposed to separately. As a fallback, both operations will be considered separately.cc Mousius NicolaLancellotti ekalda manupa-arm	1
add bc for gfx1010 (#3984)	1
Fix missspelling (#4166)FIX "After connecting he usb" with "After connecting the usb"	0
[Frontend][TFLite] fix #9078 (#9099)Co-authored-by: sunway <wei.sun@hexintek.com>	0
[INTRIN] Enable pow (#471)* [INTRIN] Enable pow* rename pow->power* fix	0
[Frontend, Tensorflow2] Added support for TensorList ops (#8454)	1
[PyTorch] Add `aten::square` operator (#10766)	1
Fix relative import in x86 conv2d (#2149)	2
[IO] Support cross-endian	1
[PYTHON/FFI] Search PATH for DLLs (#3888)* Search PATH for DLLs* Fix lint issue	0
Fix Windows build (#3429)	0
Update virtual_machine.rst (#9222)Basic cleanup of language to replace "TODO," which makes this section look very incomplete.	1
bump sphinx-addon version (#8360)	1
[Relay][VM] Relay VM serialization (#3647)* relay vm serialization* fix lint* load params, fix stream* lint* fix typo	2
TF frontend: add softsign op (#6799)	1
[microNPU] Allow constants to be given as input to an operator (#9515)* [microNPU] Allow constants to be given as input to an operatorCurrently the expectation is that all constants need to be encoded,however, this is not always the case for scalar inputs. This PRensures that constants that don't need encoding are not treatedlike encoded constants by the EncodeConstants pass.Change-Id: I79cf4aa10d01c4ae9ce9cdafb6f21ebb2d028126* address commentsChange-Id: I67b61a2d2f67de25c47d2ace0e3a22c59ba8ea15	4
fix ctypes bug for mac (#9)* fix c_api bug for mac* update travis	5
[CI] Fix shell script exit codes (#3329)The exist code of a posix compilant shell is 0..255.  Attempting toreturn -1 will error in some shells and implicitly cast to 255 inothers.  Fix it by returning a legal return value.	0
[AutoScheduler]Simplify the code (#8351)	5
fix onnx conv2d_transpose loading (#245)	0
relax rtol/atol checks on some onnx tests (#2403)relax the error constraints on these tests due to likelyFP compuation accuracy issues.	0
[Relay] Add DefuseOps pass (#6946)Co-authored-by: minminsun <minmin.smm@alibaba-inc.com>Co-authored-by: minminsun <minmin.smm@alibaba-inc.com>	4
[TVMC] compile/tune: Check if FILE exists (#10865)Currently when a non-existing FILE is passed to 'tvmc tune' it throwsa traceback because a FileNotFoundError exception is not handled. Sincethere is no need for such abrupt exit, and the trace can also confuseusers, this commit fixes it by checking if FILE indeed exists, kindlyinforming the user about the non-existing FILE before exiting.Add test for verifying if 'tvmc compile' and 'tvmc tune' commands handlecorrectly the FILE option when it is invalid (e.g. missing, a dir, or abroken link).A TVMCException will be generated by test_tune_rpc_tracker_parsing testbecause FILE will be set by pytest to a mock object, which is not avalid input. Since FILE argument is irrelevant for the test in question,circumvent the Mock hijack of FILE argument by setting it before usingmock.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[FIX] Fix from_mxnet for multiple outputs symbol (#247)	0
fix cpp deploy (#468)	0
[Bugfix][TIR] Removed passing of IterMapExpr into PrettyPrint (#11412)Follow-up from https://github.com/apache/tvm/pull/11235, all errormessages should be based on expressions that are not IterMapExpr.	0
[CI] Cancel previous build if new commit has been pushed to a PR (#6518)	1
[REFACTOR][TIR][API-Change] Range/IntSet API style consistency. (#5953)- Range::make_by_min_extent -> Range::FromMinExtent- Update the APIs in IntSet to use CamelCase	1
[CUDA][PASS]Legalize tensorcore (#7147)* add pad_to_tensorcore & legalize for dense/bmm/conv2d* fix pad & slice* fix comments* fix comments* resolve conflict* resolve conflict* support only fp16* add tests/python/relay/test_pass_legalize_tensorcore.py* add tests for legalize tensorcore* fix pylint* fix pylint* code format* use_gpu test only; fix conv2d_alter_op* fix tests params* revert transform fix	0
fix to skip node not in graph. (#5238)fix to skip node not in graph because some network cannot be hybridized with some var unused.	1
[NNVM] Add argmax and argmin operations from topi (#1462)	1
Update PULL_REQUEST_TEMPLATE.md	5
[Docker] Add external directory mount (#8144)* add mount option* comment* cleanup* trigger* trigger* address comments* address comments* trigger* fix without --mount option* address comment* hopefuly last commit :D	1
[Relay] Add pass for getting calibration data from a relay module (#5997)* add simple pass to extract outputs* complete pass that collects all function inputs/outputs* add analysis pass for collecting outputs* reorganize the files* add the first test* update test with tuples* clean up Python code* merge with upstream* clean up transform.py* add comments for cpp files* fix lint issues* update submodules* modify files according to the review* fix style and typo* fix lint error* add checks for repeated function calls* fix lint error* merge review comments* small simplification* revise the code according to the review comments* add username in TODO* use IRModule directly* use better APIs according to the review* apply comments from the reviewer* retrigger ci	1
[Relay][Training] Add gradient for max. (#3915)* save* save	1
#7058 [Tutorial] Import errors in deploy_detection.py and deploy_classification.py (#7059)* Update deploy_classification.pyChanged import paths* Update deploy_detection.pychanged import paths and stop layer id* Update deploy_classification.pySet working directory to TVM base.* Update deploy_detection.pySet working directory to TVM base. Changed layer id in comment to 186.* reverse changes in deploy_classification.pyReversed changes in import path. Imports working as expected.* Reverse change in import pathsImports working as expected.* Update deploy_detection.pyDuplicated graph_runtime import removed.	4
[FUSION] add 'void AutoFuseEwise(Schedule sch)' (#36)* [FUSION] add Fusion(Schedule)* [FUSION] rename to AutoFuseEwise, detect whether the stage has been scheduled* [FUSION] change to visitor pattern* [FUSION] rename filename* [FUSION] fine-tune the interface* [FUSION] typo* move elem_wise to schedule* rename test function	1
fix typos: 4x "disbale" to "disable" (#2865)	2
[TVMSCRIPT] Support tir.abs node in tvm script (#8488)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[tvm4j] register user-defined function (#251)* [tvm4j] register user-defined function* [tvm4j] define java function (pushArgToStack) to convert arguments to C TVMValue* [tvm4j] make Module & Function extends TVMValue* [tvm4j] make registered cb function return Object* [tvm4j] add cb finalizer; add TVMValueBytes* [tvm4j] support NDArrayBase cb arg* [tvm4j] register cb function unit tests* [tvm4j] pass Function.Callback to resource_handle* [tvm4j] fix type cast	0
[Frontend][Tensorflow] Support SAME padding for dynamic h, w when stride == 1 (#7885)* Support SAME padding for dynamic workloads when stride == 1* Fix lint* Fix lint	0
fix vm doc	2
[Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.	0
[VTA] Fix VTA compile issue (#5481)* [VTA] Fix Pynq driver build issue.Issue:When doing vta compile in xilinx FPGA, the pynqdriver.cc report cannot find <vta/driver.h>Solution:add related path.* Fix libvta load fail issue.issue:run vta on pynq board libvta.so load failed.solution:fixed VTA.make logic issue	0
[BYOC][ACL] Improve installation tutorial (#6170)* [BYOC][ACL] Improve installation tutorialImproves installation script so that ACL can be built natively and improves tutorial to give clearer information on how ACL can be installed using two different methods.Change-Id: I6cec98b4b0a7dc2b151b36583d3d28f2b85f8702* Address commentsChange-Id: I88db6d9d539a8f06e2dfe1b9a0a3ac7a4b46cece	5
[JVM] Update the runtime PackedFunc for module	1
add paddlepaddle to python/gen_requirements.py (#9098)	1
[TE] Add LegalizeInvalidAttach to legalize the compute_at location after split or fuse (#5917)* Add LegalizeInvalidAttach* lint & typo* lint & typo* address comment* fix lint	0
[Darknet] softmax temperature in frontend (#1429)	5
[ Relay ][ Frontend ][ Tensorflow ]add op add_n to relay/frontend/tensorflow.py (#4181)	1
Add gradient for log-softmax (#4069)	2
[Ansor][AutoTVM v2.0] Phase 1: feature extraction for cost models (#6190)* [AutoScheduler] add feature extraction* fix lint* fix gpu test* address comments* improve flop estimation* rebase* refactor with group* fix* Apply suggestions from code review	0
dynamic conv2d for cuda (#6598)	5
[SCAN/Refactor] Refactor scan interface, enable fix point analysis. (#47)	0
[Vulkan] Rewrote PointerValueTypeRewrite transform (#8528)* [Vulkan] Rewrote PointerValueTypeRewrite transformIn C-style codegen, pointer types can be freely cast between scalarand vectorized types (e.g. `float16x4* <-> float16*`).  In SPIR-V,these are separate types, and no such casting is allowed.  This waspreviously handled by having a special-case for `Ramp(base, stride=1,lanes)` in the codegen.  That method didn't cover all possible cases,including Broadcast nodes used as indices.PointerValueTypeRewrite previously re-wrote the AllocateNode andparameter pointer types, but didn't update the Load/Store node.  Thischange tracks which variables can be updated to a vectorized type, andthen updates all references to those.  This includes removing the`RampNode`, as the vectorization is then included as part of thevariable type.* [StorageRewrite] Updates as recommended in review.- Added explicit TODO(Lunderberg) for follow-ups- Pass `checker.info_map_` instead of `checker` to  `VectorTypeRewriter`* [Vulkan] Allow for pointer rewrites that change base type.A single memory allocation may have more than one type of data storedwithin it.  This allows the PointerTypeRewrite pass to recognize if afunction only uses the pointer as a particular base type.  This wasn'tan issue in C-based codegen, but is required for Vulkan.  Since Vulkanshaders do not permit type-casting, the cast must be done when passingthe pointer argument into the shader.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Build] Update C++ standard to C++17 for AOT, iOS, VTA (#12712)Follow-up from https://github.com/apache/tvm/pull/12337 andhttps://github.com/apache/tvm/pull/12693, updating a few additionallocations that specified C++14.	1
[VTA] Recover rpc server support (#8604)	1
[ARITH] migrate indexdiv/mod to floordiv/mod (#4008)	5
[Docker] Refactor/clean-up of docker/bash.sh (#8670)* [Docker] Refactor/clean-up of docker/bash.sh- Added detailed help message, displayed using `-h` or `--help`.- Optional flags handled using `getopt`, can now occur in any order.- `--mount` flag may occur more than once.- Switched from short arguments to docker-run to long arguments  (e.g. `--volume` instead of `-v`).  Short arguments are good  shortcuts for interactive work, but can be more difficult to read in  longer scripts.- Mount the `.tvm_test_data` folder, to avoid re-downloading test data  already available in the host environment.* [Docker] docker/bash.sh CI fixDash-prefixed arguments as part of the command now require prefixing with-- to separate them from arguments intended for docker/bash.sh* [Docker] docker/bash.sh, consistent quoting* [Docker] Added --repo-mount-point for docker/bash.sh* [Docker] Updated command-line parsing of docker/bash.sh- Maintained previous behavior, any unrecognized flags after the  docker/bash.sh are part of the command, no -- is  needed. (e.g. docker/bash.sh ci_gpu make -j2)- Reverted changes to Jenskinsfile to add a --, no longer needed.* [Docker] Fixed multi-argument commands* [Docker] docker/bash.sh check permissions before mounting ~/.tvm_test_data* [Docker] Consistent workplace directory in docker/bash.sh for JenkinsSome locations in the CI perform build commands outside of the buildsteps (e.g. tests/scripts/task_ci_setup.sh#L38), and cmake doesn'tlike it if the build directory changes.  These should probably bemoved into the build steps of the CI, and be packed in tvm_multilib inthe Jenkinsfile, but for the meantime maintaining a consistent/workspace directory on all CI nodes allows cmake to run.* [Docker] Updated bash.sh for MacOS compatibilityMacOS has an older version of bash that handles arrays slightlydifferently.  All instances of array expansion `"${ARRAY[@]}"` shouldinstead be written as `${ARRAY[@]+"${ARRAY[@]}"}`.  Otherwise, `set -u`will erroneously complain about an undefined variable. Seehttps://stackoverflow.com/a/61551944 for details.Even though this is an older version of bash (observed in version3.2.57), this is the last major version available under GPLv2 and istherefore the default version on MacOSX.  At some point, the`docker/bash.sh` could be migrated to python for ease ofmaintenance/testing.	3
[RPC] callback option rpc server starts (#1092)	5
[Relay][Frontend] Fix tensorflow frontend lstm forget bias adding order (#3410)	1
[SYMBOL] Enable get index from symbol (#286)	1
[TIR] Revert #11428 and move loop dependent alloc extent check after region union (#12019)	4
Improve tensor mismatch ICHECK message (#7335)* Improve tensor mismatch assert message	3
[ONNX] Add more dynamism to Eyelike (#11615)* add dynamism-okness to eyelike onnx importer* add dynamism to eyelike* add more dynamism robustness to eyelike onnx importer* noop	2
gitignore tags (#277)Signed-off-by: Edward Z. Yang <ezyang@fb.com>	5
[BUG FIX] Add _type_has_method_sequal_reduce to Span and SourceNode (#8248)* add field _type_has_method_sequal_reduce to Span and SourceName* retrigger CI	1
[MetaSchedule] Add software pipeline in CUDA tensor core auto tensorization (#12544)cc @Hzfengsy @junrushao @junrushao1994 @masahi @spectrometerHBH	1
Fix stack overflow when partially-__init__ Node raises exception. (#7481)* Fix stack overflow when partially-__init__ Node raises exception. * If a Node subclass raises an exception and ctypes is in use before   __init_handle_by_constructor__ is called (or self.handle is   otherwise set), a Python stack overflow could result. This is   because the unset handle slot causes self.handle accesses to   fallback on the getattr(self, 'handle') method, invoking   NodeGetAttr. * Then I believe this causes an infinite loop. * The fix is to make Node.__getattr__ raise AttributeError for all   attributes in __slots__, then make __del__ tolerant to missing   self.handle. * I don't believe cython is affected because it implements a   descriptor to access its underlying chandle and that shouldn't be unset.* black format* actually use handle instead of self.handle	0
[BYOC][Contrib] Arm Compute Library integration (#5915)* [BYOC][Contrib] Arm Compute Library integrationArm Compute Library (ACL) integration using the BYOC infrastructure. This will enable offloading select operators from a relaygraph to ACL so we can achieve faster inference times on Arm CPU's due to hand crafted optimized routines. The PR adds initialsupport for offloading FP32 conv2d, maxpool2d and reshape to ACL. ACL codegen is used to generate a JSON representation of anoperator or 'ACL layer', the ACL runtime then uses this representation to construct a layer, cache it and create a packedfunction to for the graph runtime to call into.RFC here: https://discuss.tvm.ai/t/rfc-byoc-arm-compute-library-integration/7082Change-Id: If756dcea787ea346b1508e9a191b7eed7bd02b7f* Refactor ACL integration to support JSON runtime* Now uses JSON runtime* Addresses tutorial comments* Rename acl to arm_compute_lib in user facing apiChange-Id: I3b5ef80607f713e898363e82ab4398fbc2cf267a* Address commentsChange-Id: I041fda14f3bf9975f3518ba8a4e3ab43ba98403d* Address comments* correct mistakes in tutorial* reshuffle runtime to use fewer macro blocks* preprocess module using "optimize" functionality* use new module apiChange-Id: I219488e617e5767edd7489b43b8bfce876cd24b8* Enable ACL codegen tests in CI* Skips runtime tests as these are not supported on x86.Change-Id: I6843c003a2604afe95cfdccf2323d2a336b56fe5* Fix check for runtimeChange-Id: I3f9eec15c599f01b1105d624fb053b73bfb6ed41* Address comments* Add warning to ACL engine creation* Correct runtime check so it doesn't fail when codegen not present* Improve testing to check acl partitions is what is expected* Check results of multiple runs testChange-Id: I9522950930805b9b601dad03269adcf8ed3138cc* Address comments* Multiple style improvements* Use base class for creating json node for single op* Move GetSource to base class* Improve annotation checksChange-Id: I8219659c4b99e86df887cd914720157cb94c61a0* Improve tutorialChange-Id: I8f610bd37af1e3740fd48c2d502bcc4727d9d712* Initialize conv with nullptrChange-Id: I6c37f0d75a064001c74e171ff83b9f7a7c3f1918	4
[skip ci][ci] Fix broken test skips (#11456)	3
[TEST] Cache test data (#2921)	5
IntSet Evaluation, skeleton finish	5
[Relay][Frontend][Keras] batch_norm op params not handling well (#4310)* Relay Keras frontent batch_norm op params not handeling well* add unit test for Relay Frontend Keras batch_norm	3
[testing][hexagon] Better subproc errors (#11853)When a subprocess completes with a non-zero exit code, includeits stdout and stderr text in the Python exception's error message.	0
Fix 1d-softmax schedule. (#11719)	0
[TAG] Fix signature of decorated function (#228)* [TAG] Fix signature of decorated function* Add dep	1
[TOOLS] JSON upgrader to upgrade serialized json. (#4730)During Unified IR refactor we will change the structure of IRs.This will cause certain historical modules stored via json no longerable to be loaded by the current version.This PR introduces a backward compatible layer to try its best effortto upgrade json from previous version(this case 0.6) to the current version.We mainly aim to support update of high-level ir(relay).	5
Docker: Install Python packages 'requests' and 'Pillow' (#3495)Needed for:- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/tutorials/frontend/deploy_model_on_android.py#L30- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/tutorials/frontend/deploy_model_on_android.py#L37- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/python/tvm/contrib/download.py#L58	5
[Bugfix] Fix usages of logging-related macros (#7748)	2
[microTVM][Zephyr] Fix board names (#8998)* fix board names* fix base_box test* fix tutorial	0
[TOPI][Bugfix] Make semantics of empty `axis` in `squeeze` consistent with Relay (#12596)* Fix empty axis of `squeeze` in TOPI.* Add test case for `squeeze` with empty `axis`.* Add LLVM target for `test_squeeze`.	3
[BUILD] Windows support of DLL exports (#522)	1
[Op] Do not override specified layout in pooling (2nd PR) (#9328)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR)	5
[MetaSchedule] Generate MetaSchedule Dataset (#11641)In order to build a dataset for improving the cost model for MetaSchedule, I added several filesincluding importing models to TVM, extracting tuning tasks, and sampling measure candidates.Meanwhile, I exposed some methods in C++ to the Python side to assist the process.	5
[Fix] Explicitly retain `__hash__` of `StringImm` (#8449)* Fix missing `__hash__` of `StringImm`* Make __hash__ a methodCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
Add thread_warp_size for Metal device in default target attributes (#8202)	1
[TOPI][AutoTVM] NHWC conv2d templates for ARM (#3859)* [AutoTVM][TOPI] NHWC conv2d templates (spatial pack) for ARMAs some frontends (tflite for example) are using NHWC as the defaultlayout, we are enabling NHWC schedule templates in TOPI and AutoTVM.* some comments fix	0
Update CUDA key to fix #11168. (#11170)	0
[microTVM] Add QEMU build to RVM image (#8190)* num of cores* add target list* extension* qemu* fix* comments* add qemu to setup build* fix* add mps2 test* merge fix* add commit option* add log* fix* fix zephyr init* rename* fix zephyr init* uncomment* fixed qemu isntall* cleanup* version* add commit option* fixed qemu isntall* add docker import* cleanup* fix* cleanup* fix* fix zephyr path* fix* fix* address comments* fix test* fix* add wait* comments* changed test to script* add checks* fix zephyr* Revert "add wait"This reverts commit 70f3c7d840028d81823b99ae12ae6969b80d9a91.* address comments	1
[RELAY][PYTORCH]cosh,sinh,log2,log10,log1p op support (#5395)* [RELAY][PYTORCH]cosh,sinh,log2,log10,log1p op support* Review comment fixed* Gradient testcase added	1
[Hexagon] Add AoT capability to Hexagon launcher (#11214)* [Hexagon] Add AoT capability to Hexagon launcher	1
[CMSIS-NN] Aligned buffer sizes for Conv2D post CMSIS-NN SHA update (#11359)	5
[FIX] Avoid stack overflow in TargetHookVisitor with large modules (#11135)Use MixedModeVisitor to not recursively visit let nodes.	1
Finish schedule operation	5
[Relay][ONNX] 1-D global and adaptive pooling. (#7906)* 1D adaptive pooling added and tested.* Apply formatting.* Add onnx integration and tests.* Busted by lint.	3
[µTVM] Add VMWare to Reference VM instructions (#7221)* support vmware_desktop provider for microTVM reference VM* update tutorial* python format* try to fix sphinx warning* fix sphinx warning* retrigger CI	2
Use ctx instead of tvm.gpu(0) in nnvm_quick_start tutorial (#1801)	1
Implement tensorflow relational operators and related tests (#1714)	3
[Refactor] Remove scope attribute from Buffer class (#8463)Co-authored-by: masa <masa@pop-os.localdomain>	4
Upgrade XGBoost to latest (#5658)	3
[cmake][ANTLR] Support setting path to ANTLR jar (#4176)* Support setting path to ANTLR jar* Update comment	5
Replace std::result_of (deprecated in C++17) with std::invoke_result, NFC (#12562)	5
[TVMScript] TracedObject class that simplifies tracing ObjectPaths (#12299)Motivation: when printing a piece of TIR, we need to track an ObjectPath from the root TIR object to the currently printed object. This means that we need a convenient way to maintain an ObjectPath whenever we access a sub-object, e.g. via an attribute.Tracking issue: https://github.com/apache/tvm/issues/11912	0
[RUNTIME] Fix debug runtime i386 build (#1818)	1
Update dmlc-core to the latest commit (#3716)This includes changes to build TVM runtime for Hexagon.	1
support overlapped itersum (#12039)	1
[TENSORFLOW] Convert scalar Const into tvm.relay.const (#3885)* [TENSORFLOW] Convert scalar Const into tvm.relay.const* use _get_num_param() and _get_list_param()	2
[VERSION] Move version script to the project root (#2556)	4
[FIX,MICROTVM] Skip microtvm tests if microtvm is not built (#6693)	3
[FIX] Remove debugging print statement (#7072)Somehow a unnecessary print statement was left in the codebase.	0
[RELAY][BYOC] Add support for composite functions in BYOC (#5261)* [RELAY] Add 'check' functions to MergeCompositeCurrently, MergeComposite can only perform structuralmatches. This patch introduces the ability to specifya 'check' function alongside the pattern which can includecustom logic to determine whether an extracted patternshould be merged.For example, if you only want to merge 'NHWC' convolutions,you can specify a 'check' function which queries thedata_layout value of the extracted pattern (see the test).Change-Id: I9337ce39f10997051a286d888be38ed0d410d340* [RELAY] Reformat merge_composite.ccRun clang-format on merge_composite.ccChange-Id: I1736bff798cc6d93e57519b08ab3362869098779* [RELAY][BYOC] Support composite functions in AnnotateTargetThis patch introduces support to annotate composite functionsin the AnnotateTarget pass. In order for a composite functionto be annotated, you should name it according to the style:{codegen}.{name}eg. dnnl.add_reluChange-Id: I74d6c0b506153d866f6d1feb203b32dad59f2871	4
[TVMScript] Support TVMScript template meta-programming over variables (#11097)This PR supports a simple meta-programming paradigm for TVMScript, which allows users to get access to var definition in the Python environment.	5
[2/3][AOT][DeviceAPI] Add Hooks for Activate/Deactivate/Open/Close (#9500)* [AOT][DeviceAPI] Add Hooks for Activate/Deactivate/Open/CloseThis adds the relevant hooks into their starting places in the codegeneration. As per the [C Device APIRFC](https://github.com/apache/tvm-rfcs/blob/main/rfcs/0031-devices-api.md)* Standardise on `lowered_ir_mods` and correct device_hook variable name	1
Add Python representation for VirtualDevice (#9812)* Add Python representation for VirtualDeviceThis adds a Python class to represent the VirtualDevice so that thebehaviour for `device_type()` can be semi-replicated.These tests were actually not being ran and were broken so I've addedthem to the integration script.* Update other references to make_virtual_device	1
[MetaSchedule] Tuning Script Upgrade (#11797)* Support uint8.* Modify tuning functions.* Follow legacy setting, use int32 for uint8.* Add vm support.* Fix vm usage.* Use vm in rpc run module.* Fix lint & stuff.* Fix backend.* Fix ftimer.* Fix lint.* Limit backend choice.* Add try catch.* Display name in rpc try catch.* Support ahb from tune_relay.* Modify scripts.* Fix typo.* Minor fix.* Fix try catch & func name.* Fix utils.* Move utils to tune_utils.* Fix tune_utils.	0
[TOPI] depthwise-conv2d in NCHW[x]c layout for x86 (#2045)	5
[DOCS] VTA installation guide (#1428)	2
[Relay] Expose vm OptimizeModule to Python (#4800)* Expose VM OptimizeModule to python* added missing imports* fix import	2
Stop pylint complaining about unnecessary return statement. (#2684)Recent pylint introduced support for the useless-return diagnostic.This patch remove the useless returns.	1
[CI] Run frontend tests for aarch64 in CI (#10869)Using `task_python_frontend_cpu.sh` to begin with to match those usedin `ci_cpu` - can add more frontends after this initial set isfunctional.	1
[DOCKER] temporary revert cuda version to cuda8 (#2021)	4
[RUNTIME] Improve PackedFunc robustness (#5517)* [RUNTIME] Improve PackedFunc robustness- Add static assert to warn about unsupported type deduction.- Always inline template expansions for PackedFunc calls.* Fix style issue	0
Add cache flush for arm (#9170)* Add cache flush for arm* formattingCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	1
[VM][PooledAllocator] try reallocation once when OOM (#8285)	1
[microTVM][RVM] Always destroy the VM if all tests pass (#8739)Currently base-box-tool 'test' command will skip destroying the test VMif a single provider is specified (i.e. --provider virtualbox) even ifall tests pass. This is confusing (no warning is displayed to the user)and that will leave host resources (like USB devices necessary to runthe test) locked by the VM. So if the user tries to run a program thatuses the locked resource (e.g. openocd) cryptic failures might happen.Moreoever, even if all tests pass and more than one provider isspecified but the option '--skip-build' is set a VM will be left runningwithout notice.This commit changes that behavior by:1. Always destroying the VM if the release test pass2. Always keeping the VM up and running if a test fails1. guarantees no resource remains locked by the VM without necessity. Anew flag '--skip-destroy' is introduced in case the user still wants tokeep a VM up and running if the release tests pass. 2. guarantees the VMwhere the test failed is left running for further inspection of the testthat failed.Finally, for both 1. and 2. cases a proper message is displayed to theuser to inform if a VM was left running or not and about what actionsthe user can take next accordingly to the test result in the VM.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	3
[DOCKER] Pin torchvision==0.4.1 (#4140)The existing sequence of pip install commands fetches and installstorch==1.0.1.post2 then fetches an unpinned version of torchvision,recent torchvision packages hardwire the specific torch version theydepend on, the overall effect is that we install a pinned torchversion then replace it with whatever version the torchvision packagedepends on.The most recent torchvision==0.4.1 package results in some test casefailures.This patch pins torchvision back to 0.4.0, the most recent versionthat the test suite worked.  Removing the explicit torch installbecause it is implied and pinned as dependency of torchvision.Change-Id: Ib30bf6aed79ff130ea15ef5134fefb0508790574	4
[Relay] pytorch frontend support conv1d (#6203)* [Relay] pytorch frontend support conv1d* add tests for conv1dCo-authored-by: xutianming.xtm <xutianming.xtm@bytedance.com>	3
[PASS]LoopPartition (#56)* loop_partition draft* divide loop variable into constant domain and variable domain & consider multiple partitions* process doubt interval* fix and refactor, add relax_map arg in BoundDeduce* fix testcase and comment* rebase to zero, convert to SSA* change the logic of generating loop code & fix issues* add a testcase for relax map in deducebound && fix issues* clean code* const auto&* add test_multi_if	3
[microTVM][CI] Rename ci_qemu to ci_cortexm (#12281)* rename files* ci script* demo* RVM files* jenkins* more ci* Jenkins* fake name for ci_cortexm* merge with main* add cortexm config file	2
[Target][Legalization]Add Tir Level Legalization Function Registration And Update Intrinsic Lowering Pass (#7936)	4
[TIR] Fix dtype mismatch in UnifyThreadBinding (#11843)This PR fixed dtype mismatch in UnifyThreadBinding when multiple thread axes with the same thread tag have different dtype.	5
Get tvmc version from tvm (#7478)Change-Id: I6a6e78080f36e4e3e1689e03ea48e759fcd8e466	4
[PYLINT FIX]pylint issue fixes (#348)Before Fix************* ************* Module nnvm._baseR: 21, 0: Disallow trailing comma tuple (trailing-comma-tuple)R: 27, 0: Disallow trailing comma tuple (trailing-comma-tuple)-----------------------------------Your code has been rated at 9.99/After Fix************* siju@siju-DAEMON:~/D/Community/nnvm$ make lintpylint python/nnvm --rcfile=/media/siju/DATA/Community/nnvm/tests/lint/pylintrc-------------------------------------------------------------------Your code has been rated at 10.00/10 (previous run: 9.99/10, +0.01)	1
Add domain	1
Build crttest and cpptest separately. (#6057)* Build crttest and cpptest separately. * Try to fix random CI crashing, likely caused by concurrent cmake execution.* Revert to -j8	4
[microNPU] Integrate rolling buffers in Arm(R) Ethos(TM)-U (#10344)* [microNPU] Integrate rolling buffers in Arm(R) Ethos(TM)-UChange-Id: Iede5e68981a063f6eb1e118433cc2c92e175af52* Add documentation for create_tiles* Fix linter issues* Fix integration tests	3
Add packing for int8 1x1 convolution and support the int8 group convolution on X86 (#2991)* Support the 1x1 int8 conv with NHWC layout and weight packingfix linter* fix the memoize issue* fix the failed nhwc test* add the schedule for pack to unbreak other tests* skip avx512 compile* Support the 1x1 int8 conv with NHWC layout and weight packingfix linter* fix the memoize issue* fix the failed nhwc test* add the schedule for pack to unbreak other tests* skip avx512 compile* Unify the data_layout and kernel_layout relation* add asf header* fix the comment* retrigger the build/test	3
use caffe2.python.onnx.backend instead of onnx-caffe2 (#418)	1
[build] Use mold or lld if detected (#10683)This looks for the lld or mold executables when configuring CMake.If found, it sets them as the linker for gcc/clang via -fuse-ld=.Fixes #10679Co-authored-by: Lite Ye <yelite@users.noreply.github.com>	1
refactor Hexagon conv2d tests (#9333)* refactor hexagon conv2d tests* add nhwhwc schedule to logical filter tests* add gt_filter_block_shape function* turn on nhw8h8wc for logical filter and fix verification* cleanup* adjust command lines in the README	4
[Hexagon] Add support for linked-in model parameters (#8865)* [Hexagon] Add support for linked-in model parameters* Remove entry_func, since it's not used anywhere* Simplify linked-param codegen preparation a bit* Detect multiple linked-params functions* Add testcase to check for linked-param codegen* Empty commit to restart build	2
Add not operator for the frontend/onnx.py (#3836)	1
[iOS] Fix build issues on the latest XCode and iOS (#9298)1. Specify target for compiled dylib2. Specify Metal Shader Language version when we compile metal library.	1
[Relay][Frontend][ONNX] Add Sign and Equal operators to ONNX frontend (#3760)* [Relay][Frontend][ONNX] Add Sign and Equal operators to ONNX frontend* Dummy change to retrigger integration test	3
[Relay to onnx conversion][New ops] (#8436)* [Relay to Onnx conversion]* added support for Sigmoid op* added unit test* [Relay to Onnx conversion][Copy]* added support for Copy op* added unit test* [Relay to Onnx conversion][Round]* added support for Round op* added unit test* [Relay to Onnx conversion][Cast]* added support for Cast op* added unit test* [Relay to Onnx testing]* fixed formatting* * fixed formatting issues* * fixed formatting issue in onnx.py* [Relay to Onnx conversion][Conv2d Transpose]* Added support for conv2d transpose operator* Added unit test case. Unit test is similar to the conv2d unit test.* * Fixed formatting errors	0
[Relay] Add hd,tl,nth for list in Prelude (#2771)	1
[Collage] CombinerRule and CandidatePartition::EstimateCost (#12078)* [Collage] CombinerRule and CandidatePartition::EstimateCostSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.We complete the PartitionRule sub-class hierarchy with the addition ofCombinePartitionRule, which allows disjoint candidate partitions to beunioned based on simple rules. - By TOpPattern kind, eg a kOutElemwiseFusable and kBroadcast. - A tuple argument with injective fields. - The projection from an injective group (obviously of tuple type) - Combinations of the above.These let us mimic many common fusion strategies, including TVMs, so thatthe candidates explored during Collage search are as large as possible toexpose possible fusion opportunities but no larger.Also completes CandidatePartition with the EstimateCost method, which isused during search to construct a stand-alone IRModule for latency estimation.Finish units tests for PartitionRule and CandidatePartition.* - fix relay.collage ffi prefix.	0
Try fix cpp topi test (#871)* Try fix cpp topi test* move cpp test to another stage* update	5
[Relay][dismantler] Added handling of packed func (#8004)Added handling of CallNode objects created via packedfunctions invocation + test cases.Change-Id: I5374abc59a3b0f79f27364c45f1a5789536df940	4
[TVMC] A simplified TVMC API for python scripting (Part 1). (#7823)* Introduce new TVMC Python API.* Add simple testing model.* Split result utils into stand-alone file.	2
[TFLITE]Activation functions support (#4978)* [TFLITE]elu, leaky_relu, lrn, log_softmax activation functions* removed ops present in pr 4805* review_comments updated	5
intel graphics conv2d schedule fixed for input shapes (300*300) and (512 * 512) (#1709)	0
[PYTHON] Enable proper error message in python package (#7521)	0
[Bugfix][TIR] Handle bool tensor in FlattenBuffer (#11532)This PR fixes an existing bug in TIR lowering where the TIR below triggers an error:```python@T.prim_funcdef func(a: T.Buffer[10, "bool"], b: T.Buffer[10, "bool"]) -> None:    T.func_attr({"global_symbol": "main", "tir.noalias": True})    for i in T.serial(10):        with T.block("b"):            vi = T.axis.spatial(10, i)            b[vi] = a[vi]tvm.build(func, target="llvm")```The error message is:```  File "/root/Projects/tvm-dev/src/tir/transforms/flatten_buffer.cc", line 173TVMError:---------------------------------------------------------------An error occurred during the execution of TVM.For more information, please see: https://tvm.apache.org/docs/errors.html---------------------------------------------------------------Check failed: store->buffer->dtype == DataType::Int(8) (bool vs. int8) : Expected int8 backing arrayfor boolean tensor```This PR fixes this behavior.	0
[TOP] Initial Schedule of MobileNet on Rasp (#496)* [TOP] Initial Schedule of MobileNet on Rasp* Fix* Fix	0
Cleaning up Arm(R) Ethos(TM)-U codegen (#9147)This is a follow up commit to address thecomments of #8849Change-Id: I02d8de64f3bce0e7b544d652eee8737ec1ecbb80	4
[CI] Add python setup script (#6844)	1
[REFACTOR][FFI] Make more clear naming for C API Type codes. (#4715)This PR introduces more clear naming prefix for C API type codesto avoid conflict with other packages.We also removed TVMArray and TVMType to directly use DLTensor and DLDataType.	5
[COMMUNITY] @jcf94 -> Committer (#7141)	3
[USMP] bugfix workspace calculation (#10617)The workspace calculation should be doneafter memory is planned.	1
fix small bug about dense_grad (#5695)	0
TOPI C++ bugfixes (#864)* TOPI C++ bugfixes* Fix lint	0
Update ISSUE_TEMPLATE.md	0
Fix build error (#3552)* Fix build error* comments	0
Finish support for list-of-targets (#11382)* Finish support for list-of-targetsThis finishes the work started in https://github.com/apache/tvm/pull/11173 to support'external codegen' targets in the N build-like API surfaces. - It turns out it's ok if a build is given only a single 'external codegen' target, so remove that check   in CompilationConfig::Init. When Collage builds a 'candidate partition' it does so for a single target.   As far as Collage is concerned it does not care whether the target is regular (eg Target("cuda")), or   for a specific external codegen (eg Target("cutlass")), it just passes the target into the build. - Add CompilationConfig::FindPrimitiveTargetForKind which I'll later need to retrieve   the external codegen Target instance corresponding to a "Compiler" attribute value. - Target.update_target_host_consist was supporting three API styles:    - single target    - map from device type to target    - map from target to IRModule (for the ir_to_runtime API)   I replaced all those calls with a more specialized 'canonicalize' call:    - Target.canonicalize_target_and_host    - Target.canonicalize_multi_targets_and_host    - Target.canonicalize_target_map_and_host   In particular, all the tuning interfaces (task extraction, tuning, tuning records) all explicitly   *do not* support multiple targets since the underlying code just doesn't support that.* - Lints- Revert unintended changes* - more lints* - Fix model_library_format handling of target.- Improve comments in compilation_config.h* - Lints- Update target/target_host params documentation* - Fix micro library format tests- Rev micro library format from 5 to 6- Use Target.current() in a few places* - eta contract comprehension* - Woops, one more device: target map left- Handle host already being in Target* - lint* - lint* - Bug with append- Take device type from target* - Fix hexagon	0
fix restructured text (#5541)	0
Fix reshape usage in ARM Winograd (#5732)	0
Add standalone_crt/ to be part of the wheel package, when available. (#9005)* When using a packaged TVM such as tlcpack, it is impossible to run  `tvm.micro.get_standalone_crt_dir()`, because the subtree  `standalone_crt/` is not available.* This patch adds `standalone_crt/` as `data_files`, so that they  can be picked up by _ffi.libinfo.find_lib_path() and therefore  be found when `tvm.micro.get_standalone_crt_dir()` is invoked.	1
[Metal] Add pass for splitting kernel with huge number of args (#8313)* [Metal] Add pass for splitting kernel with huge number of argsThe Metal has some limitations on the number of input parameters. Moreinformation can be found here:https://developer.apple.com/documentation/metal/buffers/about_argument_buffers?language=objcIn this commit a new pass for splitting functions with big number ofarguments to smaller parts was added. In parameter `max_function_args`we can specify the maximum number of kernel arguments for specifictarget and then split kernel when the number of arguments exceeds thevalue of `max_function_args`. Currently this pass works only for concatlayer.* Add getting number of output parameters* Fix CI and apply comments	0
[RPC] Fix android rpc connection to tracker (#8327)* [RPC] Fix android rpc connection to trackerAfter commit 0bbaf0e, android_rpc wasn't able connect to rpc_tracker.Added addr field to cinfo.* Fix CI	0
[Arith] Remove diagnostic ctx argument from DetectIterMap (#10798)	4
[EXECUTOR] Split graph_executor to header file and (runtime) source file (#300)* [EXECUTOR] Split graph_executor to header file and (runtime) source file* Fix	0
[PatternLang][Bugfix] Ensure CallNode attrs are not undefined before checking (#7278)* Correct handling of call node attrs to handle non-operator calls (attrs may be undefined)* Linting fix	0
[TOPI] Fix GPU Dynamic Op Schedule (#7117)* Fix GPU dynamic op schedules* Fix dynamic shape nms* Fix* Fix test format	3
[BUGFIX] fix text printer when TVM_LOG_DEBUG is on (#10279)	2
[ci][docker] Add Jinja2 to images (#10741)This is needed for #10740Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[NNVM] Fix check in layout parsing (#1502)* [NNVM] Fix check in layout parsing* add one workload	1
Fix C runtime NDArray allocation bug (#6991)	0
[SPIRV] Minor update to TIR sort to make it work on VK/SPIR-V (#7607)* sort started to working* static size sort seems to be working* test sort on vulkan* add nvptx to sort test too	3
[METAL] use 32bit indexing for metal until we have a bound adapted pass (#462)* [METAL] use 32bit indexing for metal until we have a bound adapted pass* fix lint	0
[RUNTIME][PYTHON] More compatibility in ndarray (#463)	1
[Relay][Frontend] Support tf.where (#2936)* [Relay][Frontend] Support tf.where* fix comments	0
[Arith][BoundDeducer] Forbid non-supported expr type in bound deducer (#11323)	1
Link necessary libraries when building runtime for Android (#5496)- Link libgcc to enable use of thread-local storage (ThreadLocalStore  requires emutls).- Link liblog when building with Hexagon support.	1
[ONNX] Use relay softmax op to convert Softmax if posssible (#9892)	1
[COMMUNITY] Mark Shields -> Reviewer (#9369)	3
[TFLITE]Select op support for tflite frontend (#5486)* [TFLITE]Select/Where op support for tflite frontend* Review comment fixed* Review comment fixed	0
[CI] Rust CI Changes (#7773)* Tweak CI* WIP* CI Tweaks for Rust CI* Fix* Fix LLVM issue	0
[VTA][Chisel] add scalafmt and format existing scala codebase (#3880)* [VTA][Chisel] add scalafmt and format existing scala codebase* change column width to 100* add scalafmt conf file as a valid file type* add asf header to scalafmt conf file and rerun formatter	1
Update ethos-u-vela for demo app (#10129)Update from ethos-u-vela 2.1.1 -> 3.2.0	5
[TIR] Add TileWithTensorIntrin (#11075)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[skip ci] Add generated docs file to .gitignore (#10701)cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[RUNTIME] Simple NDArray container API in c++ (#1418)	1
[ci] Add a manual retry for conda setup (#12058)This makes it so the conda setup will re-run entirely in case of failures like in https://github.com/apache/tvm/runs/7287493088. [This issue](https://github.com/conda-incubator/setup-miniconda/issues/129) has some more context but there doesn't seem to be a better way to do a retry than re-running the whole thing since the settings in `conda/condarc` are picked up but they don't help for this particular issue.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TEST] Setup frontend stage (#2347)	1
[Doc] TVM_REGISTER_API -> TVM_REGISTER_GLOBAL (#4768)	2
add the link for how to setup rk3399 opencl driver (#827)	1
fix bug when converting constant nodes with types of int64 or float64 (#6159)Co-authored-by: yuweilong <yuweilong03@meituan.com>	0
[HEXAGON] Split huge 1D DMA Transfers into smaller transfers with legal sizes. (#10971)	5
[BYOC-OpenCLML] OpenCLML integration with TVM. (#10243)* [BYOC-OpenCLML] OpenCLML integration with TVM.* [BYOC-OpenCLML] Cleanup and review.	4
Consolidate RPC Context helper functions (#6915)	1
[Fix] int32/64 mismatch of buffer elem_offset at HandleBufferBindScope (#11755)Yet another int64/32 mismatch at TIR level. `ArgBinder::Bind_` requires `elem_offset` of arg & view to have the same dtype while `int64-broadcast-concat` produce int64 `elem_offset`	1
[TIR] Expose TVM Backend API-related Builtins and Misc (#12468)This PR exposes the following TIR operation in python:`tvm_thread_allreduce`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_type.py#L135)`type_annotation`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L718)`tvm_access_ptr`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L717)`tvm_throw_last_error`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L343)`TVMBackendAllocWorkspace`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L340)`TVMBackendAllocWorkspace`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L465)Co-Authored-By: yongwww <yongcale@gmail.com>	3
[Hexagon] Fix addressing TVMValue array (#9302)	1
[ARM_CPU] Conv2d int8 intrinsic for cortex-A72 (#10310)* [ARM_CPU] Conv2d int8 intrinsic for cortex-A72Add an intrinsic that performs a dot product of 8 4-element vectors atonce. Also conditionally inline fused operators into the mainconvolution loop depending on convolutions size. Small convolution = noinlining. Performance improves by ~20% on mobilenet on raspberry pi 4and ~30% improvement on performance for the individual convolutions.* ignore incorrect lints* fixup fstring* revert changes to conv2d_NCHWc (not int8)* remove error check, apparently tests rely on it* refactor alter op layout	4
[microTVM][RVM] Set the number of cores based on the VM sizing (#8624)Set the number of cores for scripts and builds that run inside the RVMbased on the specified number of cores for the VM.Currently Vagrant doesn't set env. variable TVM_CI_NUM_CORES with thenumber of cores available in the VM created by Vagrant, as a consequencethe scripts and builds (like the ones used to build TVM and QEMU) thatrun inside the VM after it is created will use the default number ofonly 2 CPUs, so not using the full CPU resources available in the VM,in case there are more than 2 cores available.This commit sets TVM_CI_NUM_CORES equal to the number of cores availablein the VM created by Vagrant so the builds (which use that environmentvariable to find out the number of CPUs that must be used for thebuilds) can use all the CPUs available, speeding up the builds.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
add FSetInputVariableAttrs (#92)* add FSetInputVariableAttrs* rename	1
llvm 14 and above move TargetRegistry into MC (#9305)TEST=build with latest llvm	3
[TARGET] ONNX codegen (#5052)* Relay to ONNX converter* Relay to ONNX op test cases* Relay to ONNX end to end model test cases* Add test cases to jenkins* CI CD fixes* ONNX codegen* ONNX codegen* ONNX codegen* onnx testcases* ONNX codegen* test onnx* ONNX codegen* shape calculation* move onnx codegen to contrib/target* review comments* ONNX target use visitor* onnx fixes* lint fixes* doc string changes* review comments* review comment fixes* review comment* pytest skip* rename type to node type* test* Fix for constantshpae, add exp, fix for metadatamodule* Fix cpplint* change error tol values	0
[microTVM] Zephyr: add mps3_an547 board support (#10479)* [microTVM] Zephyr: add mps3_an547 board supportAdd mps3_an547 board support to microTVM.On Zephyr this board is supported by two emulators: QEMU and FVP. Thiscommit only enables the support for running mps3_an547 on QEMU, sincecurrently there isn't a FVP transporter on microTVM. The main differencebetween these two emulators is that FVP is provided by Arm as a closedsource emulator and it supports the Ethos-U55 accelerator.The mps3_an547 is an Arm reference board. For more details, please see:https://developer.arm.com/tools-and-software/development-boards/fpga-prototyping-boards/mps3Since there are already specific tests enabled on the CI to testEthos using the AOT executor, for instance, this commit will only addsupport for mps3_an547 using QEMU for now, also enabling the board to betested on the CI.The FPU is disabled on this commit ("fpu": false, in boards.json). Thisis due to commit d4cc1c2196 ("target/arm: Enable MVE in Cortex-M55")being absent in QEMU v6.1.1, so it's not available in any zephyr-sdkrelease yet. That commit enables MVE (M-Profile Vector Extension) and sofully enables the instructions to run the code generated when FPU isenabled on Zephyr. It's available from QEMU v6.2.0.This commit also adds support for the QEMU_BIN_PATH env variable so itturns easy setting an alternative QEMU version other than the one thatis available via PATH, when running / testing any virtualized board,either by the CI in the future or locally by the users/devs.Finally this commit fixes two typos in comments.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* mehrdadh review: Add comment on why FPU is disabled if mps3_an547 supports it	1
[BYOC][TRT] Add DFPattern support for TRT backend (#10759)This PR adds DFPattern support for the TRT backend without removing the existing predicate registry.Adds and extends the following:In tensorrt.py: Add a pattern_table for all the supported ops and consumes the pre-existing op_registry checksAdds an additional pass as unmerge_composites.cc. This is required for the TRT backend as it expects a single primitive function to work with, while the MergeComposite and PartitionGraph will produce a single function for each Composite pattern.Adds test_inline_composites.py which tests the newly introduced pass.Both the pattern-based and predicate-based pass sequences produce syntactically equivalent IRModules.This is to ensure backwards compatibility."	4
[RELAY][PASS] Make FoldConst context and target invariant (#2114)	1
[Rust] Clean up conversions between TVM and Rust functions (#6114)* Replace ToBoxedFn with From* Compact and improve Typed and ToFunction impls- Clone one less time- Don't panic if number of args is wrong, return an error- Actually drop functions/closures on the rust side* Retry	1
Porting schedules (except convolutions) to C++ (#763)* Ported injective schedules to C++. Added some elementwise ops.* Fix lint errors* Added reduction ops and schedules* Fix lint errors* Fix lint errors* Fix lint errors* Added transform ops* Fix lint errors* Fix lint errors* Added softmax, log_softmax, leaky_relu and flatten ops.Fixed issue where TVM_DECLARE_INTRIN_UNARY used the PureExtern flaginstead of PureIntrinsic.Added softmax CUDA schedule.* Fix lint* Fix lint* Added binary_dense, batch_norm_inference, dense, dilate, scale_shift_*,global_pool and pool ops.Extended pad to allow specifying pad_value.Fixed issue where pad would throw if padding was zero in all dimensions.* Fix lint* Fix lint* Added CUDA schedules for dense, pool and global_pool* Added extern schedules for generic and CUDA* Fix lint* Added x86 binary schedules* Fix lint* Added rocm dense schedule. Added rocBLAS and cuBLAS support to dense ops* Added pow ops. Added x86 default and injective schedules* Fix lint* Fix lint* Fix lint* Fix lint* Fix lint* Fix indent* Removed schedules directory* Changed left_shift, right_shift to operators. Changed pad_value in pad() to remove pointer usage* Fixed usage of pad in nn/pooling.h. Fixed declaration of operator>>* Fixed comments for shift operators* Added comments to utility functions* Added TOPI C++ library, exporting broadcast_add op* Fix lint* Share libinfo.py with TVM* Fix lint* Add other broadcast ops* Fix lint* Fix imports in topi* Fix lib names* Fixed build issue where windows builds don't apply correct definitions* Removed TVM_EXPORTS from topi library* Attempted CI build fix* Add topi lib to tvm_multilib* Fix Jenkinsfile* Added TOPI build target to Makefile* Fix nn op namespaces.* Fix lint* Renamed TOPI lib to libtvm_topi* Removed _ffi/base.py* Remove _ffi from topi, now shared with tvm.* Make libtvm_topi loading optional* Fix compiler warnings* Fix lint* Fix lint* Fix lint* Fix build error by making new libs argument to Target optional* Added C++ Target type interop. Added registration of remaining C++ ops and schedules. Added test of broadcast ops* Fix lint* Fix lint* Fix compile error* Fix compiler warnings* Fix compiler warnings* Fixed int vector interop. Fixed argmin incorrectly invoking argmax. Fixed corner case in default schedules of attempting to fuse 0 length axes. Added tests for reduce ops.* Refactored reduce builders* Fixed typos in topi.cc. Added basic test.* Fixed padding size error. Added dense, dilate, pooling tests* Fixed issue where clip would output a different dtype to the input. Added split_sections op to cover the other mode of the python split op. Added tests.* Changed extension type numbers to avoid clash with NNVM* Fix lint* Fix compiler warnings* Removed use of std::vector from the public TOPI API* Fix lint* Add TOPI C++ tests to CI* Fixed detail namespacing. Improved comments.	1
[CI][Docker] Removes Dockerfile.ci_qemu as it was moved to Dockerfile.ci_cortexm (#12329)Removes the - now deprecated - Dockerfile.ci_qemu, as it was moved to be specialized in different environments such as Dockerfile.ci_cortexm, to support specific environments for tests in different platforms.	3
[microNPU] Fix incorrect comparison in schedulers (#9706)* [microNPU] Fix incorrect comparison in planning functionsAdded diamond graph test case to test_scheduler.py* Fixed copy luts test case	3
Refactor RewriteTensorize to prevent concurrent map updates (#11596)	5
Add optional mem_scope parameter to tvm.nd.array and tvm.nd.copyto (#11717)	2
Fix a typo for function registration (#927)	1
Add some docs on downstream consistency (#5742)https://github.com/apache/incubator-tvm/pull/5730#issuecomment-639567636	0
[RPC] Link in whole archive with BUILD_STATIC_RUNTIME (#10260)* [RPC] Link in whole archive with BUILD_STATIC_RUNTIME* Restart CI	1
[REFACTOR] Remove old Low-level Visitor/Mutator (#4612)	4
[AMP] Disallow converting layer norm to fp16 (#9782)* [AMP] Disallow converting layer norm to fp16* black	1
[Relay] VLOG for finer grained control of hyper-detailed logging (#9012)* [Relay] VLOG for finer grained control of hyper-detailed loggingI've been making very heavy use of DLOG and PrettyPrint to trace, understand anddebug Relay transforms. I've found myself deleting log statements to reduce theoutput verbosity, only to have to re-add them a few days later. Better would be tosupport leaving all those statements in place but have finer control over when theyare enabled.This PR introduces a 'VLOG(level)' macro to that end. The log is ignoredunless TVM_LOG_DEBUG is enabled (both as #define and an environment var), and thethe current verbosity level is >= level. The current verbosity level can be setglobally and/or overridden per source file (see 'VerboseLoggingEnabled').(Those familiar with the origin of the LOG and DLOG family will also recognize VLOG.)I also introduce a 'VLOG_CONTEXT' macro which pushes a string onto an internalper-thread stack. Each VLOG message includes that stack as its prefix, which is avery handy way to keep track of the (often recursive) program context in which eachVLOG is executed.I've rejigged some existing DLOGs to VLOGs to illustrate, but left most of themalone for now.  See the draft https://github.com/apache/tvm/pull/8788 for use in thewild.I noticed the DCHECK macros *disabled* instead enabled with TVM_LOG_DEBUG defined, sofixed that.I've also made changes to the Relay text printer to dump attributes in a humanreadable format rather than the indirect but machine readable 'meta' representation.This is gated by the show_meta_data_ flag, and I think this use is consistent with it'soriginal purpose.* [checkpoint] lints* [checkpoint] missing \n lintGotta get my docker setup going* [checkpoint] woops, we don't support gmock.h* [checkpoint] Address Hua Jiang's comments.* [checkpoint] strlen not avail on all toolchains?* [checkpoint] Rework TVM_LOG_DEBUG spec and parser* [checkpoint] woops, forgot the static modifier on map* [checkpoint] * -> DEFAULT for wildcard.Andrew pointed out *=9 suggests foo/*.cc=9 would work but it is not supported.* [checkpoint] constexpr length* [checkpoint] length is not constexpr on all targets, reverting* [checkpoint] minimize VLOG overhead when in legacy DLOG-only mode	2
CI trigger after repo move (#4252)	4
[METAL] Use CFBridgeRetain for retaining the allocated resource (#6393)	5
Prelu bug fix (#1358)	0
[CI] Remove `llvm -device=arm_cpu` and `cuda -libs=cudnn` from the default test target list (#10500)After recent improvement in GPU frontend tests, I found that `topi: GPU` has become a bottleneck. From the log https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10391/14/pipeline/319, it is clear that topi tests are running on target `llvm -device=arm_cpu` and `cuda -libs=cudnn`, which I claim is completely redundant since we already run on `llvm` and `cuda` targets. In https://github.com/apache/tvm/pull/9905, I've already removed them from `DEFAULT_TEST_TARGETS`, but `topi: GPU` uses its own list of targets which still includes `llvm -device=arm_cpu` and `cuda -libs=cudnn`. I propose to remove them from topi test targets, which hopefully will cut topi GPU tests time by half.	3
[FIX] Verify that tensor reshape is valid. (#6215)	0
[ci][build] Use ninja instead of Makefiles (#10934)* [ci][build] Use ninja instead of MakefilesThis switches the CI build to use Ninja which has slightly nicer output and faster behavior in the face of re-runs. This also adds a `--verbose` flag to `ci.py` to control build output accordingly.* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
change version number (#1175)	4
Fix tutorial to follow a change of elemwise_sum (#1374)	4
[Torch, QNN] Add support for quantized models via QNN (#4977)* qnn support initial import* fix upsampling num input* imagenet tests added* add qunatized module tests* quantized module tests working* imagenet test working* fix lint* remove top level torch import to fix ci error* disable lint warning on outside toplevel import* revert parse -> convert change* add comments to qnn translation* address comments, add sample outputs* add more comments* refactor bias add and requantize step	1
[FIX] Fix bug in MobileNetV2 quantization (#8243)* fix test* fix bug* fix pylint* fix pylintCo-authored-by: wangyucheng <wangyucheng@sensetime.com>	1
[Relay] Better shape inference in TensorFlow Frontend. (#3176)* Some bug fixes in tensorflow graph converter and added DepthToSpace operator.* Made DepthToSpace better comply with other function syntax.* Added better shape inference for unusual situations.* Lint fixes.* Added depthtospace test.* Added test cases for value inference and depthtospace.* Added fill testing.* Made comment changes and added BroadcastTo op and tests.* Fixed underlining and unneeded opt_level forcing.* Added _infer_value assertion that all values to infer are available in passed parameters.	2
[PYTORCH]where, addcdiv, addcmul op support (#5383)* [PYTORCH]Where, addcdiv, addcmul op support* Review comments fixed	0
fix minor misspelling (#8476)Co-authored-by: Masahiro Hiramori <mshr-h@users.noreply.github.com>	1
[FRONTEND] Composed operators (#175)* fix for composed symbol* fix* clean up* fix exception type	0
adding Manupa to reviewers (#8012)	1
Send list as argument to schedule_conv2d (#4358)When getting cuda schedule passing single tensor seem to work but after changing target to "llvm" causes assert.Sending list on other hand makes both cuda and llvm targets happy.See https://discuss.tvm.ai/t/solved-simple-example-error-attributeerror-tensorslice-object-has-no-attribute-op/2245/3	0
[IR] Update the type_keys to reflect the code-org (#5074)	5
[FIX] Save tensor size with alignment (#6487)	0
[BugFix] Linker: undefined reference to kTargetPoolReadWriteAccess (#10147)	1
[relay][frontend] Return Module from get_workload (#3483)* [relay][frontend] Return Module from get_workload* pass entry_func to autotvm* disable tune* add property to module* mod.entry_func to main* .main -> mod["main"]* fix	0
increase atol for float32 (#8712)	1
[runtime] Remove unused parameter. (#8580)* [runtime] Remove unused parameter.* fix build issue when TVM_CRT_DEBUG enabled	0
[AutoScheduler] Fix incorrectly array context device and hide info at the beginning (#7632)* [AutoScheduler] Fix incorrectly array context device and hide info at the beginning* Lint fix	0
[Target] Fix C5 Target Tag to Include CascadeLake Archs (#12385)>The C5 and C5d 12xlarge, 24xlarge, and metal instances feature custom 2nd generation [Intel](https://aws.amazon.com/intel/) Xeon Scalable Processors (Cascade Lake) with a sustained all-core turbo frequency of 3.6GHz and maximum single core turbo frequency of 3.9GHz. The other C5 and C5d instance sizes will either launch on the 2nd generation Intel Xeon Scalable Processor or the 1st generation Intel Xeon Platinum 8000 series (Skylake-SP) processor with a sustained all core Turbo frequency of up to 3.4GHz, and single core turbo up to 3.5 GHz using Intel Turbo Boost Technology.>The C5 and C5d 12xlarge, 24xlarge, and metal instance sizes enable Vector Neural Network Instructions (AVX-512 VNNI*) which will help speed up typical machine learning operations like convolution, and automatically improve inference performance over a wide range of deep learning workloads.According to [introduction to AWS EC2 C5 targets](https://aws.amazon.com/ec2/instance-types/c5/), the C5 12x and 24x machine are `cascade lake` architecture instead of `skylake`, and enable Vector Neural Network Instructions (AVX-512 VNNI*) which can be useful for AutoTensorization.This PR fixes the target definition of the 2 above mentioned C5 machine to support VNNI.	1
Don't requantize if bias or quantize scales are approximately equal (#9676)	5
[Frontend][TF] Fix Placeholder issue (#2834)* [Frontend][TF] Fix Placeholder issue* Add test cases	3
fix docker image when installing rust (#7004)	2
[TEST] Add dot (#61)	1
[TOPI][X86] Pool operator parallel support. (#4090)	1
[FIX][TIR] Remove unused code and fix typo in storage_align (#9583)	2
use pip3 for python3 (#3742)* use pip3 for python3* make python3 as default	1
[Relay][Pass] Update SimplifyTranspose to correctly simplify rank changing layout transforms (#7807)	4
[VTA] Update vta-hw dependency (#7874)	5
[FoldConstant] Create Interpreter for each constant subgraph (#6195)	1
add friendly tips when not found cl and link (#574)* add friendly tips when not found cl and link* fix lint	0
[ci] Add linter for PR title and body (#12367)* [skip ci][ci] Fix Jenkinsfile (#12387)This got out of date after merging #12178Co-authored-by: driazati <driazati@users.noreply.github.com>* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[CMSIS-NN] Align CMSIS-NN in TVM to TFLu SHA (#11273)	5
Fix zephyr/test_zephyr_armv7m (#9684)	3
missing header for GraphRuntimeFactory in android_rpc (#6648)	1
[MetaSchedule] Schedule Rule: Add RFactor (#9975)* add rfactor* format* fix ci	0
[TOPI] cuda reduction schedule (#7131)* complex reduce* fix* fix* fix	0
Add Adreno GPU target and topi supporting textures with dynamically allocated textures (#11161)* Add Adreno GPU target and topi supporting textures- There are 5 compute/schedules: conv2d for NCHW/NHWC, depthwise_conv2d  for NCHW/NHWC, average pooling- Fix of dynamically allocated textures caching- Add texture-nhwc scope- Fix issue with codegen of vars having non acceptable symbolsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>* Address comments* Add vectorization into some adreno pool flowCo-authored-by: Li <quic_lih@quicinc.com>* Fix adreno tests for running on the opencl host platform* remove unnecessary kDriverVersion in DeviceAttrKind* Move utils adreno functinos to separate shared file* fix black hitsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Li <quic_lih@quicinc.com>	5
[TIR] Tighten up invariance of CopyOnWrite in recursive stmt visitor (#8358)	5
fix cuda half math function is undefined: hpow, htanh (#6253)	1
[Relay] Introduce Executor and Runtime representations with associated registries (#9246)This is the underpinning Executor/Runtime objects for https://github.com/apache/tvm-rfcs/pull/29, this doesn't change the wiring as yet since that's a pretty big change in itself. Most of this patch is TVM boilerplate, which I've tried to minimize - there's maybe some future work to decide how to more easily define some of these things.	1
[microTVM] Remove Arduino aot code (#8869)* Fix Arduino DLDevice includes* microtvm_api_server fails if commands fail* Add regression test for microtvm_api_server not failing* Address PR commentsBreak error detection tests into separate fileAddress comments from MousiusRe-add necessary fixture	0
Release 0.3 (#1171)	5
Update rust contributors (#2500)	5
support t attr in onnx (#1300)	1
[ci] Add GitHub Actions bot to merge PRs on demand (#10833)This implements https://discuss.tvm.apache.org/t/rfc-allow-merging-via-pr-comments/12220. The bot can be invoked from a top-level review comment or via a regular PR comment. The text `@tvm-bot merge` anywhere in the body will trigger the bot. Right now it checks that the latest commit is reviewed and that all CI jobs that have run on that commit are successful. If it fails, it will leave a comment on the PR with the reason.This is just a start and some features are left for followups:* Various TODOs throughout the code* "Scheduled" merges that happen once CI finishes* Allowing committers to merge without getting a fresh review for changes after an approval	4
[community] @AndrewZhaoLuo -> Committer (#9896)* Update CONTRIBUTORS.md* Update CONTRIBUTORS.md	5
[RELAY][Fix] i64 indices (#5235)* fix* resolve comments	0
[runtime][hexagon] improved file-copy logic (#12194)- Add `tvm::runtime::CopyFile` function.- Change `HexagonModuleNode::SaveToFile` to use new function  instead of a shell `cp` invocation.  This fixes a problem where the `cp`-based implementation  couldn't handle certain valid filenames.  This also fixes a bug where `SaveToFile` simply skips the  file-copying step on Mac OSX.	2
[FRONTEND] A Python hybrid frontend (#1251)	5
[microTVM][RVM] Skip USB device attach if device is already attached (#8737)* [microTVM][RVM] Skip USB device attach if device is already attachedCurrently, when the VirtualBox provider is selected, if base-box-tool.py'test' command is used and a VM is already running with the USB devicenecessary to perform the tests already attached to it the command failsbecause it tries to blindly attach again the USB device without checkingif device is already attached.The failure can be reproduced by first running a VM for testing (thetests need to fail and leave the VM running):$ ./base-box-tool.py --provider virtualbox test --microtvm-board=stm32f746g_discothen one tries to re-run the tests without building the whole VM again:$ ./base-box-tool.py --provider virtualbox test --skip-build zephyr --microtvm-board=stm32f746g_discoThis commit fixes that error by checking and properly skipping the USBdevice attach if it's already attached to the VirtualBox VM.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* areusch review: Use --machinereadable for the outputUse 'showvminfo --machinereadable' output to parse for more robustnessto updates in VBoxManage.	5
[TOPI][Relay] New Op: MetaScheduleLayoutRewrite (#11826)	1
[RUNTIME][RPC] Change RPCServer to Event Driven Code (#243)* [RUNTIME][RPC] Change RPCServer to Event Driven Code* fix	0
[Relay] Partial Evaluator do concatenate, and has better termination checker for scalar. (#3703)* savelint somelintlintadd charrnnsavesavesaveremove debugremove debugremove spacerefactorsaverewrite dce* reset files* join -> meet* lint* address review comment* wordsmith	1
[CONTRIB] add peak test (#878)* add peak test* fix error for lanes=16* update doc* fix names* fix names	0
[Hexagon] Fix compilation errors in Hexagon launcher (#9189)A few instances of `runtime::Device` were missed when `Device` was movedfrom `tvm::runtime` to `tvm`.Co-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>	1
[TOPI] parallel schedule improve for x86 & layout_transform support (#1130)* add layout_transform. add schedule support for ndim>=5 for x86* fix lint	0
Add bot to ping reviewers after no activity (#9973)* Add bot to ping reviewers after no activity* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[RUNTIME][REFACTOR] Re-organize Containers into SubFolders (#8183)	4
[RELAY] Fix alter_op_layout (#2289)	0
[LLVM] Replace calls to Type::getVectorNumElements (#5398)This function has recently been removed from LLVM 11. Use alternativeway to obtain vector element count (VectorType::getNumElements) whichworks for all LLVM versions.	1
[tvmc] Introduce 'run' subcommand (part 4/4) (#6578)* [tvmc] Introduce 'run' subcommand (part 4/4) * Add 'tvmc run' subcommand to execute compiled modules * Include options to locally or remotelly using RPC * Include support to cpu and gpu devicesCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* adjust based on code review comments* make test fixture to safely skip environments without tflite* make --help option more clear* improve error message to show expected inputs* code-review adjusts* update doc-string to default zeros->randomCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>	2
[microNPU] adding more tests with USMP (#10362)Adding a few tests to confirm memory usagewith and without USMP.- Supporting the toggle to disable storage_rewrite.- There is a slight change to tir_to_cs_translator to   add index of Load nodes associated with NpuAddressRange objects	1
[TensorIR] TVMScript Parser/Printer (#7630)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[RELAY] Filter PlaceholderOp from schedule. (#2412)	5
[TE][Fix] Comparison of the output tensor (#9829)* [TE][Fix] Comparison of the output tensor* fix hybrid op issue* fix tensor replacement in schedule ops* fix compute inline	0
[IR] Fix a primitive check error (#5991)* fix primitive check error* assuming every Op has Type defined* CHECK_NE -> CHECKCo-authored-by: Liangfu Chen <liangfc@amazon.com>	0
[TIR] Simplify indices in layout transform (#11330)Co-authored-by: Yuanjing Shi <yuanjing@octoml.ai>Co-authored-by: Yuanjing Shi <yuanjing@octoml.ai>	5
Use version invariant rustfmt (#2886)	1
[Fix] Fix flaky test of #9952 (#9958)* fix to stablize the var orders when solve bounds in region analysis* change to std::find_if since num of vars is generally small	4
[Doc]refine the example description of max/min/sum/tag_scope (#4974)	2
[Relay][Keras] Dot (#3668)* [Relay][Keras] Dot* fix reshape* fix comments	0
Fix test_external_codegen, broken by #8591 (#8630)	3
[CUDA] auto detect compatibility when arch is not passed (#490)	4
[Contrib] CuDNN v7 Support (#311)* [Contrib] CuDNN v7 Support* Add test	3
[BITSTREAM SERVER] Bitstream server integration (#38)	5
[Caffe Frontend] Add support for Power layer (#9655)Co-authored-by: tangkun <kun.tang@hexintek.com>	1
[AOT] Rerun FVP test incase of first attempt failure (#10408)* [AOT] Rerun FVP test incase of first attempt failureIn light of the flaky failures seen when running these sorts of tests(issues: 10300 and 10314), adding a second attempt if the first attemptfails to try to help reduce the number of failures observed in CI.Change-Id: I25eb268555baab85fc7e70d2d70a37f3be49a54b* apply same logic to CMSISNN tests as wellChange-Id: I0afb54676e83af06fcd9ab2cc1fc2e87002031cd* rebase and print errors to stderr to capture in pytest logChange-Id: Ibedff4515050421d98462bfbd95d5cdf77b12412* reduce scope of retryReduces the scope of the retrial to only the portion that runs thetest module on the FVP.Change-Id: I4520f50aea5cc8e28c03b49da18612cc7f8b8045	4
[BugFix][Topi] Fix 'duplicated iterator names in the compute definition' bug of roi_align (#11322)	0
Fix function number datatype from char to uint16_t (#10014)rewrite the modified part to pass lint checkUse 2 bytes for func num in fun_registryFix errors in linterAdd the declaration of the helper functionsset 2 bytes for func num in func_registry test unitspass num_func by valueThis commit change the datatype of the number of the function from 1 Byte to 2 Bytes.Besides, I use some helper functions to access the number of function and the first function name.	1
file include/tvm/logging.h from AutoTensorize work (#1015)	1
[PASS] Add VerifyMemory pass and test cases (#410) (#993)	3
[WASM] Update support for latest emcc, add ffi test. (#6751)	3
[TUTORIAL] Cross Compilation and RPC (#184)* [TUTORIAL] Add tutorial for RPC* [TUTORIAL] Update tutorial* [TUTORIAL] Update tutorial* trigger update* [TUTORIAL] Improve build	1
[VMCompiler] Support shape func lowering for nested function call (#9405)* Support nested function call in shape func lowering* add test	3
[Docs] Bring Your Own Codegen Guide -- Part 2 (#4718)* BYOC Tutorial -- part 2* Fix comments* Address comments	1
[COREML]Unary ops support added in frontend (#6196)* [COREML]Unary ops support added in frontend* Used coreml enums	1
Update halideIR, add more device query for shared memory (#1087)	1
[COMMUNITY] @wweic -> Reviewer (#2789)	3
Remove unnecessary print (#5642)	4
[INTRIN] prefetch support (#246)* [INTRIN] prefetch support* lint* add buildin	1
[Docs] VTA install doc migration from md to rst (#5442)	2
[CI] Always run cpptest during build to ensure library correctness (#3147)	3
[runtime][refactor] Unify vm and interpreter objects (#4693)* unify vm and interpreter objects* move closure back vm* adt/closure back to vm.adt/vm.closure* closure base	4
[Relay to onnx conversion fixes][Pool, Pad] (#8435)* [Relay to Onnx conversion][Pool]* added missing ceil_mode in average pool and max pool conversion* [Relay to Onnx conversion][Pad]* Fixed issue in Pad conversion: changed pad_value to input instead of attrs* Refer to PR: https://github.com/apache/tvm/pull/7860* Updated unit test for Pad* Fixed some formatting errors	0
split test_forward_math_api function (#11537)	1
[TUTORIAL] Fix tedd tutorial after strategy change (#4947)* [TUTORIAL] Fix tedd tutorial after strategy change* Remove scale, remove link to external gdoc	2
Add dense schedules to __init__ for cpu (#2855)* Add dense schedules to __init__ for cpu* Add documentation for topi::shape* Add additional imports to topi CPU __init__.	5
Changes to cpp_rpc to make it work on Android (+ Hexagon offloading) (#5535)* Changes to cpp_rpc to make it work on Android (+ Hexagon offloading)- Implement getNextString to break up std::string into words. stringstream  just doesn't work on Android.- string::find_last_of doesn't look for the last substring, but the  last character from a given string.- Use SIGTERM to terminate processes (this isn't necessary, but using  SIGKILL is not a good practice).- Convert "./rpc" to a full path. When a module is uploaded and offloaded  to Hexagon, the dlopen on Hexagon needs an absolute path (or a path  without directories).* Only set the absolute patch on non-Windows platformsWindows has different macros for the maximum path length.	1
[VTA] Update docker for TSIM based simulation (#4674)	2
Canonicalize type annotation during construction of Var and SizeVar (#11443)* Canonicalize type annotation during construction of Var and SizeVar* Update tests/cpp/expr_test.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* lint* fixCo-authored-by: Junru Shao <junrushao1994@gmail.com>	0
Reveal hidden code snippets by inserting newline (#3892)	1
[PASS] enhance storage_rewrite to support different dtypes for unified buffer (#805)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op* repare address offset for different kinds of dtype* bc* aaa* aaaaa* repare address for different dtypes* remove nonsense files* add whitespace of line 581* use base alloc elem_type* enhance the testcast of basic buffer is 64bits,32bits,16bits,8bits* use extends[0]->type() as dtype of offset* clear program writes	1
[Arith] Improve floordiv / floormod rewrite simplifing rules (#10591)	1
Disable Hexagon TestConv2dPackedFilter test (#9344)	3
[AutoScheduler] Add tips on resuming the search from a log file (#7039)* [AutoScheduler] Add tips on resuming the search from a log file* Trigger CI	2
[CI] Ensure rat ignores rust cargo lock files [CI] Ensure rat ignores emacs backup files [CI] Ensure rat ignores .egg-info (#3314)	5
add relay and autotvm in readme (#2312)	1
[CI] Added message if test is running on another shard (#11331)	1
Fix clock type in rpc_session timer (#2362)* Fix clock type in rpc_session timer* Fix lint error	0
fix typo in backend interpreter (#2752)	2
[Relay] Fix memory leak when accessing NDArray (#5413)	0
[DOC] Doc redirection (#119)* [DOC] Doc redirection* fix setup url	1
Add .hsaco save/load for ROCm target (#3852)fix lld	0
Enable optional dependency memory and attr hint (#79)* Enable optional dependency memory and attr hint* fix travis	0
[DOCS] Jenkins deployment of docs, add FAQ (#157)	1
[AutoScheduler] Make RecordReader error-free (#8066)* fix bugs in the auto scheduler record:* reformat the code* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code	2
More friendly error msg; Fix Android Demo LLVM ver (#3962)	0
[TOPI] Access topi::matmul from Python (#1744)	5
[MetaSchedule] update misc parts (#10444)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
[CI] Temporary increase ci timeout (#7403)	1
[TOPI][RELAY] Add op Size (#3094)	1
[OpenCL] Enable OpenCL for GPU tests (#12490)* Add opencl target in test build script* Fix fp16 test and compile test for opencl* fix lint* Fix relay OpenCL texture tests* Fix lint* Enable relay OpenCL tests* Fix opencl relay texture tests* fix lint* Remove OpenCL gtest variable* Fix unbound variable* Skip tests that are not supported in CI* fix lint* Add path for opencl gtest directory* Fix opencl gtests include directory* Enable OpenCL googletest. Fix bug in opencl timer test* testing fix for build cpp tests* update googletest git version for opencl tests build* update cmakelist* Update CMakeList* Update CMakeList* Disable opencl googletests* update Opecnl.cmake* fix Opecnl.cmake* Apply comments. Remove xfail decerator for opencl tests. Now specific tests are skipped in the environment script* minor code changes* apply comments* apply comment* skip test in ci by decorator* fix pytest skipif warnings* Fix skipif for opencl gtests	3
[CI] Update ci_arm and ci_lint (#10146)This includes sccache, and these are the less troublesome images.Also see #10120 for update issue.	0
Fix incorrect function signature in header (#6172)	1
[REFACTOR][TE] Inline -> te/schedule/operation_inline.h (#5386)Rationale: inline is a transformation used in te torewrite its internal expressions. It is not a formal IRModule->IRModule transform pass.Also removed the python test as the test is covered by stage.compute_inline.	3
[Rust] Static syslib (#3274)	5
[TENSORFLOW]reduce ops updated (#5180)	5
[Relay] Restore kind checking (#1758)	5
[MetaSchedule][Minor] Update CPU Flush ArgParse Type (#11792)Previously `cpu-flush` option existed as a boolean or integer argument, which is a bit counter-intuitive because for argparse, any non-empty string such as `False` will be parsed to `True` when using as a boolean and integer a little bit vague here IMHO. This PR used a function from `distutils` to directly parse input string to boolean, which makes the usage more stragiht-forward like `--cpu-flush True` or `--cpu-flush False`. Meanwhile it still supports usage of `0/1` and made sure the argument is always required.	1
Add FP requantize flow. Set float32 flow by default for llvm x86 targets with (#9637)sse4.1 support	1
Generalize the use of booleans to support all cmake boolean values. (#6515)* Generalize the use of booleans to support all cmake boolean valuues.* Update to use a more simple method to detect if variable is a false value* Fix some errors* Debug CI issue* Fix logic error in cmake changes* Use new IS_TRUE_PATTERN to make intent clear	1
[NNVM][ONNX] Slice, Floor, Ceil, Clip and MatMul support for frontend  #1297 (#1371)	1
[TVM] Upgrade TVM Support	1
[TOPI] add 3D upsampling Op. (#4584)* [TOPI] add 3D upsampling Op.* fix lint issues* change align_corners to coordinate_transformation_mode* fix resize3d half_pixel* make a simple function and clean up trilinear_resize3d_python* fix doc	2
[Parser] Fix tokenizing inf (#7370)* fix tokenizing inf* use ParseNumber to parse inf, handle -inf* fix neg handling* fixed multi negation* refactor* use while loop* simplyfing* fix lint* simpler implementation per altan's suggestion* disable flaky test	3
Add PT OD tutorial (#6500)	1
[Fix] CI QEMU Install libpython3.8 (#8020)* add python lib* fix	0
[SCHEDULE] Further fix of reduce inline with multiple outputs (#508)	0
fix android packed runtime (#1430)	1
[ONNX] Wrap 'If' if it has multiple outputs (#8385)* [ONNX] Wrap 'If' if it has multiple outputsWithout this wrapper, an assertion in from_onnx() will fail with theerror message showing ""Number of output mismatch"* [ONNX] Test If nodes with multiple output tensors* Fix formatting issues	0
[FRONTEND][ONNX]HardSigmoid, min, max, mean ops support (#1645)	1
[LINT] Fix -Wextra (#4804)* [LINT] Fix -Wextra* Fix virtual-dtor	0
[BUILD] enhance build script for optional vta dep (#6497)	5
[microTVM] Project API infrastructure (#8380)* Initial commit of API server impl.* initial commit of api client* Add TVM-side glue code to use Project API* Change tvm.micro.Session to use Project API* Rework how crt_config.h is used on the host. * use template crt_config.h for host test runtime; delete   src/runtime/crt/host/crt_config.h so that it doesn't diverge from   the template * bring template crt_config.h inline with the one actually in use  * rename to MAX_STRLEN_DLTYPE * Create a dedicated TVM-side host crt_config.h in src/runtime/micro* Modify Transport infrastructure to work with Project API* Add host microTVM API server* Zephyr implementation of microTVM API server * move all zephyr projects to apps/microtvm/zephyr/template_project* consolidate CcompilerAnnotator* Allow model library format with c backend, add test.* Update unit tests* fix incorrect doc* Delete old Zephyr build infrastructure* Delete old build abstractions* Delete old Transport implementations and simplify module* lint* ASF header* address gromero comments* final fixes?* fix is_shutdown* fix user-facing API* fix TempDirectory / operator* Update micro_tflite tutorial* lint* fix test_crt and test_link_params* undo global micro import, hopefully fix fixture* lint* fix more tests* Address tmoreau89 comments and mehrdadh comments * fix random number generator prj.conf for physical hw * uncomment proper aot option	5
[TEST] Refactor RPC test to isolate runs into a sub-function (#8656)We kill the rpc server in the del function. When a serverco-exist with remote resources in the same function scope,the destruction order is not determined.This can cause server to be destructed before the actual remote array.As a side effect, it can cause sometime test to timeout due towaiting on the socket.	3
[TOPI] Add roi align (#2350)* [TOPI] Add roi align* Refactor bilinear in image resize* Rename to roi_align_nchw* Fix	0
Dedup BindParamByName function in VM compiler (#4793)	1
[CI] Disable flaky tests (#6841)* [CI] Disable flaky tests* format	3
Add attrs package (#2025)	1
Fix build status (#145)	0
Switch CRC-CCITT libraries (#6499)	5
[Frontend][ONNX] Support ONNX Scan operator (#9438)* [Frontend][ONNX] Support ONNX Scan operator* fix lint* remove test_scan_sum in unsupported_onnx_tests* support scan opset 8* fix lint* fix negative axes bug* fix lintCo-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>	5
[Relay tests] AlterOpLayout - Temporary attr update (#4357)	5
[Relay] Add foldr1 (#2928)	1
[Refactor] Enforce attaching storage scope to PointerType (#8366)* Add storage scope to ProducerRealize, always create a buffer with scope* update schedule_ops.cc* update schedule_postproc_to_primfunc.cc* restore more realize_scopeThis reverts commit b66c3baa54feeb8e34016713a1be21802b3296bf.* make the default scope be "" instead of None in ir builder* restore realize_scope visit in storage_flatten.cc* update storage_access.cc* make sure buffer var is of PointerType in ir builderThis reverts commit e650b6c24cabd52a073064e51c2e4fee816e88fd.* enforce default storage scope of global* added remap pass but does not work yet* fixed all reduce issueThis reverts commit 8e20003c5325085ed22ee57180aca18644b3b5ab.* simplify* trying mitigation for aot test* merge remaining changes from initial branch* remove use of attr::storage_scope from codegen* restore a visit to AttrStmt with attr::storage_scope in storage_rewrite* disable check* lint fix* revert default scope to ""* format* fix volatile access to shared mem in lower all reduce* fixed gpu coorporative load/store test* pass storage scope to PointerType in tvm script parserThis reverts commit 99cfb9d18781dcfdea169d920450f9063ab18b6b.* fixed tvmscript roundtrip test* fixed tir flatten buffer test* fixed test_tir_transform_hoist_if.py* use storage scope global by default in aot_executor_codegen.cc* add missing default storage scope in create_primfunc.cc* restore StorageInfo struct in llvm backend* UpdateStorageScope -> WithStorageScope* fixed lower warp memory test* GetStorageScope -> GetPtrStorageScope* Enable storage scope invariant check in AttrStmt constructor* remove GetPtrStorageScope and WithStorageScope from public header* move RemapStorageScope to its own file* add more method to RemapStorageScope* update lower_thread_allreduce to use RemapStorageScope* RemapStorageScope -> UpdatePointerStorageScope* remove realize_scope from hybrid script* removed realize_scope in schedule_ops* remove realize_scope from schedule_postproc_to_primfunc* remove remaining realize_scope usage from schedule_ops.cc* remove realize_scope usage from storage_flatten.cc* fixed test_tir_transform_lower_warp_memory.py following realize_scope removal* Add storage scope to ProducerRealize, always create a buffer with scope* update schedule_ops.cc* update schedule_postproc_to_primfunc.cc* restore more realize_scopeThis reverts commit b66c3baa54feeb8e34016713a1be21802b3296bf.* make the default scope be "" instead of None in ir builder* restore realize_scope visit in storage_flatten.cc* update storage_access.cc* make sure buffer var is of PointerType in ir builderThis reverts commit e650b6c24cabd52a073064e51c2e4fee816e88fd.* enforce default storage scope of global* added remap pass but does not work yet* fixed all reduce issueThis reverts commit 8e20003c5325085ed22ee57180aca18644b3b5ab.* simplify* trying mitigation for aot test* merge remaining changes from initial branch* remove use of attr::storage_scope from codegen* restore a visit to AttrStmt with attr::storage_scope in storage_rewrite* disable check* lint fix* revert default scope to ""* format* fix volatile access to shared mem in lower all reduce* fixed gpu coorporative load/store test* pass storage scope to PointerType in tvm script parserThis reverts commit 99cfb9d18781dcfdea169d920450f9063ab18b6b.* fixed tvmscript roundtrip test* fixed tir flatten buffer test* fixed test_tir_transform_hoist_if.py* use storage scope global by default in aot_executor_codegen.cc* add missing default storage scope in create_primfunc.cc* restore StorageInfo struct in llvm backend* UpdateStorageScope -> WithStorageScope* fixed lower warp memory test* GetStorageScope -> GetPtrStorageScope* Enable storage scope invariant check in AttrStmt constructor* remove GetPtrStorageScope and WithStorageScope from public header* move RemapStorageScope to its own file* add more method to RemapStorageScope* update lower_thread_allreduce to use RemapStorageScope* RemapStorageScope -> UpdatePointerStorageScope* remove realize_scope from hybrid script* removed realize_scope in schedule_ops* remove realize_scope from schedule_postproc_to_primfunc* remove remaining realize_scope usage from schedule_ops.cc* remove realize_scope usage from storage_flatten.cc* fixed test_tir_transform_lower_warp_memory.py following realize_scope removal* Address comments* Remove blank line diffCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>Co-authored-by: masa <masa@pop-os.localdomain>	4
Add topi.nn.fifo_buffer to TVM doc (#4343)	2
[NNVM] Fix dtype of output of pad. (#2331)Dtype of output of pad should follows input, but if dtype of input is not float,  output will still be float becase pad_value is float.	0
Fix docstring in topi.nn.fifo_buffer (#4349)	2
[TVMScript] Allow `val = buf[index]` without type annotation (#11060)* [TVMScript] Allow `val = buf[index]` without type annotationOther instances of `var = expr` were previously allowed withoutrequiring a type annotation, by using the dtype of the expression asthe dtype of `var`.  This behavior didn't work for `buf[index]`expressions, which are internally represented as `BufferSlice` pythonobjects, and only converted to `BufferLoad` primexprs when used as anexpression.This commit adds a `dtype` property to `BufferSlice`, allowing`buf[index]` to be used in a let statement without a type annotation.* Reverted a wider changeAutomatically adding a type annotation to Var if it could bedetermined from the dtype let the unit test directly compare theannotated and unannotated versions of buffer load.  Unfortunately, italso broke 54 unrelated tests, so that change is removed from this PR.	4
Fix a testcase name in test_codegen_cuda (#1526)	3
[Docker] Re-enabled automatic --tty flag when running bash. (#8861)PR8382 split apart the --interactive and --tty flags, but only--interactive was set if the user opens a bash session.  This commitrestores the previous behavior of running `docker/bash.sh IMAGE_NAME`of opening a bash session with both --interactive and --tty.	2
[RELAY][PASS] Enable decorating python class as Pass (#3364)	4
[CI] Remove mps3_an547 from the CI (#10621)Remove for now the mps3_an547 board from the CI.Both mps2_an512 and mps3_an547 boards hit a hard to debug issue whentests run against them in the CI environment.The failure seems not tied to any specific test and usually consists inthe board stopping to respond the host that will show a generic tracedue to a timeout when trying to call a function on the device, like:tvm._ffi.base.TVMError: MicroSessionTimeoutError: failed to read reply message after timeout 10sThe issue is very hard to reproduce locally and its root cause can bein one or more of the stack components being stressed, like the QEMUemulation and its interaction with the particular CI environment,Zephyr's serial driver (the boards share the same driver), microTVM codeusing Zephyr's bufffer and serial APIs, and TVM RPC protocol over theserial line when and with a serial FIFO too small (1 byte in the case ofbooth board UARTs), or even a "hiccup" when running QEMU on the CIinside a container.Hence the most reasonable thing to do is to remove the boards from theCI until that issue is solved.This commit also removes the commented out lines for running themps2_an512 board in the CI so the script file don't get polluted. Themps2_an512 is currently disabled.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[LINT] Expected, hence disable for a while. (#1212)************* Module nnvm.top.nnE:141, 8: Assigning to function call which doesn't return (assignment-from-no-return)	1
Fix missing sigmoid intrinsic in C++ (#2231)	0
[Target] Only append default keys if target doesn't have any yet (#12474)* [Target] Only append default keys if target doesn't have any yetThis allows target parsers to provide their own target keys. Without thischange, the default keys would always be appended, which may or may notbe desirable.* Add "cpu" to ARM CPU keys* Add "cpu" to the keys in the mprofile target parser* Restore the mprofile cpptest, since the "cpu" key is back* So the -device attribute is actually needed...	3
[BYOC] add multi functions support in partition pass (#8464)* add support for multi function* address commits and fix lint* fix testcases and using a set to avoid duplicate func name* fix lint	0
Add atol=1e-5 to test_topi_matmul.test_matmul (#1791)	3
Support quantized ABS operator in TFLite frontend (#9411)	1
add cmake with windows (#40)	1
Add docs for analysis namespace (#3985)	2
[QUANTIZE] Add config switch for nn.dense layer type. (#5801)	5
disable signal capture in unit test of paddle frontend (#9809)* disable signal capture in unit test of paddle frontend* code format	1
[TOPI] Bugfix for topi.prod (#8416)	0
[AutoScheduler] Guarantee init population sampling outputs a valid set (#6713)	1
[microTVM] Zephyr: Fix option name in PROJECT_OPTIONS (#8884)Fix option name for using an extra tarball when creating a new projectdirectory: it should read 'extra_files_tar', not 'extra_files'.This commit also removes the implicit C-style string concatenation inthe help strings for options 'nrfjprog_snr' and 'openocd_serial'. Italso adds a period at the end of some help strings, because some aremissing it plus help strings in TVMC use that form.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[License] move cma_api to 3rdparty. separate BSD 2-clause and 3-clause (#4410)* [License] move cma_api to 3rdparty. separate BSD 2-clause and 3-clause* add zlib license for blockingconcurrentqueue.h	1
[PatternLang] Don't rewrite expressions used outside of the pattern (#5930)* Don't rewrite expressions used outside of the pattern* add comments	1
[tvm][codegen] Make buffer auto broadcast independent to the order of input args (#3956)* [tvm][codegen] Make buffer auto broadcast independent to the order of the input arg* fix indent	0
[microNPU] Tweak a layout transform matrix (#10763)* [microNPU] Fix layout transform matrixOne of the layout transforms currently causes the cascader to stripeacross B16 axis (which is not allowed), so change that and deal withthe implications to the get_valid_block_configs.Change-Id: I04199f9f35fcc31618581567483cfb80d3b5aad2* Reduce the duplication of layout transfrom matrices* Change the nhcwb16_to_nhwc matrix for binary and unary elementwise  such that it matches the other NPU ops* Reduce the number of places where the same layout transform matrices are  defined* Add documentation to the layout transform matrices	2
ROCm changed name of library and removed old one in ROCm 3.7 release. (#6345)	4
tvm/contrib/rocm: improve finding of ld.lld (#3664)This refines the detection of ld.lld matching the neighbouring clangfile. This is particularly helpful on Ubuntu/Debian when either thedefault ld.lld is not installed or the versioned one is preferable forconsistency.@tqchen I think you last touched the clang equivalent in #3590 .	2
[TE][TIR] Implement layout transformations, non-flat memory buffers (#9727)* [TIR] Added BufferLoadNode::LegalizeDtypeWhen modifying a BufferLoad object, the return dtype must also beupdated.  This exposes the legalization function, so that passes thatuse `BufferLoad::CopyOnWrite` to modify the buffer/indices don't needto repeat the logic to update the dtype returned.* Replacing Store/Load in Stmt/Expr Visitor/Mutator* Removing Store/Load from optimization passes- UpdatePointerStorageScope- UnrollLoop- ThreadSync- LinearAccessPatternFinder- StoragePlanRewriter- VectorTypeRewriter- VectorTypeAccessChecker- NarrowDataType- IRConvertSSA- CompactBufferRegion* Removing Store/Load from examples- ConvertAddToSubtract* Replacing Store/Load in StorageFlattenNow, outputs BufferLoad/BufferStore with a flattened buffer object.temp commit, replacing Store/Load, BufferBindUnwrappertemp commit, replacing Store/Load, StorageFlattener* Replacing Store/Load in utility passes.- StmtSimplifier- IRSubstitute- BaseInliner- FeatureVisitor* Replacing Store/Load in analysis functions- StorageAccessVisitor- VarTouchedAnalysis- MemoryAccessVerifier- InplaceOpVerifier- GPUCodeVerifier- VarTouchVisitor- LCADetector- BlockReadWriteDetector- InstrumentBoundCheckers* Replacing Store/Load in lowering/legalization passes.- MakeCrossThreadReduction- CacheReadRewriter/CacheWriteRewriter- InjectVirtualThread- InjectDoubleBuffer- InjectCopyIntrin- LowerWarpMemory- LowerThreadAllreduce- LowerThreadAllreduce- LowerCustomDatatypes- LowerTVMBuiltin- CoProcSync- MergeDynamicSharedMemAllocations- VectorizeLoop- BF16Legalize* Replacing Load/Store in codegens.- Device code generators  - CodegenC  - CodegenLLVM  - CodeGenOpenCL- Utilities used during codegen  - ArgBinder  - MakePackedAPI  - ReturnRewriter  - SplitHostDevice- Execution environments  - CodeGenStackVM  - CodeGenHybrid  - AOTExecutorCodegen* [UnitTest] Add unit tests to test physical layout remapping.* Updated tvm::address_of() to hold BufferLoad instead of Load.* [TIR] Added IndexMap class.Holds a set of variables representing the input indices andexpressions in terms of those input indices.TODO:- Add validation, the index mapping should be invertible.- Add helper function, apply mapping to a set of indices.- Add helper function, apply mapping to bounds of input indices.* Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.StorageFlatten/FlattenBuffer passes updated to modify thebuffer/indices directly, rather than using vload/vstore.- Primary purpose of vstore/vload is to allow IR written in python to  define vectorized load/store.  This usage is maintained by returning  a BufferLoad/BufferStore node whose index is a Ramp.- Previously, vstore/vload was also used to compute the 1-d physical  index of a location within a N-d tensor.  This usage will no longer  be allowed, as it would not allow layout transformations to be  performed after a schedule definition, but any uses of the buffer  are flattened.* [TE] Added Stage::transform_layout to the C++ TE implementation.Adds an `Array<IndexMap>` in the stage to define the transformationsto be applied on the tensor's layout.  As of this commit, this mappingisn't propagated into the TIR graph yet.* Replace Store/Load with BufferStore/BufferLoad in ir_builder* [TE] Added Stage.transform_layout to the Python TE interface.Allows users to specify `s[A].transform_layout(mapping)`, andpropagate into the TE definitions.* Added pre_flattened_shape/pre_flattened_stride fields to Buffer.The shape and stride checks performed in ArgBinder::BindDLTensor(called from MakePackedAPI) require the tensor shape/strides prior toindex flattening.  Therefore, though it is no longer used by thelow-level code generators, we must maintain that information for usein MakePackedAPI.* [UnitTest] Test N-d indices exposed to low-level codegenWhen using te.AXIS_SEPARATOR in the call to .transform_layout, thisshould define groups of axes, each of which is flattened to a singleaxis, then exposed to the low-level codegen.* [TIR] Added PrimFunc attribute "layout_transform_map", filled from TE.Propagated the TE definition of the physical layout into the TIRgraph.* Added pre_flattened_type.If a boolean tensor is backed by an int8 buffer, the check on theargument buffer's type should be against the boolean type.When rebasing this PR, should be placed after the addition ofpre_flatten_shape/pre_flatten_strides.* [UnitTest] Added tests for loop iteration order.After transformation, the iteration order should follow the newtransformed axes.  In addition, the loop iteration variables should beexposed through the TE interface for further manipulation.* [TIR] Added BufferNode::axis_separators- Add axis_separators to represent divisions between groups  of tensor axes, where each group is flattened into a single  output axis, to be exposed to the low-level code generators.- Expose axis_separators to the python interface.- Update existing C++ calls to the Buffer() constructor.* [TIR] Added ApplyLayoutTransforms as part of StorageFlatten.For any buffers that have layout transforms defined in the"layout_transform_map" attribute of a PrimFunc, rewrite access intothe buffer such that they use the updated ordering.* Update usage of ir_builder where necessary.* [TE] Implement te::TransformSimilar to Fuse and Split, this represents a modification to theexisting loop iterations.* [TE] Added Stage::set_axis_separators.In C++, this is implemented as an `Array<IntImm>`, specifyingpre-flatteneing axes after which a new post-flattening should bestarted.  The python interface uses a sentinel value`te.AXIS_SEPARATOR` in the call to `transform_layout`, which is thenused to define the array of axis separators.* [TIR] Expose tir.transform.ApplyLayoutTransforms for testing* [TE] Rewrite loop iteration orderAfter .transform_layout, rewrite leaf_iter_vars to follow the updatedorder.  Use the te::Transform iter_var relationship to track use ofthe transformed variable.* [TE] Fill BufferNode::axis_separators from StageNodeDuring ScheduleOps and SchedulePostprocToPrimfunc, the axis separatorsdefined in the stage must be passed through to the TIR BufferNode.* [TE] Return transformed iteration variables* Moved Buffer's pre-flatten information to PrimFunc.Since the pre-flatten information is only used for validating userinputs, it makes much more sense to store it alongside the buffer_map.* Updated ethos-u C++ unit tests to remove use of Load/Store.* Bugfix, layout transformation.Error occured during conversion from TE to IRModule, when layouttransforms were applied to a reader of a `cache_read`.* In test directory, replacing all instances of T.load.* Return buffer object from tvm.tir.script.scope_handler.AllocateNow that the load/store require buffer objects, allocation should alsoreturn a buffer object to be used.* Added .astype to tvm.script.tir.node.BufferSliceSince `buf[i]` returns a `BufferSlice`, this lets the TIR examplesthat use `buf[i].astype('out_dtype')` continue functioning.* Replacing all T.store TIR calls.* Added LOG(FATAL) in constructor of Store/Load nodes.* Updated tvmscript parser to report error for Store/Load nodes.* [TVMScript] Added T.preflattened_buffer stmtUsed to specify `PrimFunc::preflattened_buffer_map`. Takes an argumentof the postflattened buffer, so that it will work for both simpledeclarations and `T.match_buffer` statements without needing tointroduce a param handle.  All other arguments are identical to`T.match_buffer.`* [TVMScript] Updated TVMscript for BufferLoad/BufferStore- Use `T.preflattened_buffer` calls in TVMScript to represent  `PrimFunc::preflattened_buffer_map`.- Remove `T.buffer_decl` for return value of `T.allocate`, now that  `T.allocate` returns a buffer.- For buffer access as a different type, make a `T.buffer_decl` for  those accesses.* Updated test_tvmscript_roundtrip.py for BufferLoad/BufferStore.* Updated TIR reference in USMP pool allocation unit tests.Using let var handles as the data pointer in buffers, rather than justas `T.load`/`T.store` arguments, requires annotation as`T.Ptr[T.primtype]`, rather than as `T.handle`.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! In test directory, replacing all instances of T.load.* tir.ComputeInline, correct variable count.Previously, this metaschedule primitive relied on `tir::UndefinedVars`ignoring the data pointer of BufferLoad/BufferStore nodes.  When`tir::UndefinedVars` was updated to visit the data pointer, similar tothe previous behavior when visiting Load/Store nodes, this caused thecount of undefined variables to be unexpectedly high.* fixup! Replacing all T.store TIR calls.* fixup! Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.* fixup! In test directory, replacing all instances of T.load.* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* Expose Buffer index flattening function to Python.* Updated test_tir_buffer.py offset tests.Replacing calls to `Buffer.vload` with `Buffer.offset_of`, whentesting the index calculations.* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.* fixup! Replacing Store/Load in lowering/legalization passes.* fixup! Replacing all T.store TIR calls.* fixup! Updated ethos-u C++ unit tests to remove use of Load/Store.* fixup! Replacing Store/Load in lowering/legalization passes.Fix linting for inject_double_buffer.cc* fixup! Updated ethos-u C++ unit tests to remove use of Load/Store.* fixup! Added .astype to tvm.script.tir.node.BufferSlice* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* fixup! Replacing Store/Load in lowering/legalization passes.* [UnitTests] Added T.preflattened_buffer in expected result* fixup! In test directory, replacing all instances of T.load.* [UnitTests] Bound checker update, compare against N-d buffer bounds.* Fixup, bound checker vectorize test.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTest] Fixed breakage in InjectRollingBuffer test.Needed a bit more re-writing than usual, because the test wasexplicitly calling lowering passes, then calling `tvm.build`.  Fixedby using the standard lowering flow, with preprocessing stepsinserting with `tir.add_lower_pass`.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTest] Fixed breakage in flatten buffer unit tests.- Updated pass to allow BufferStore/BufferLoad nodes to be visited  before the block's alloc buffer.- Added `T.preflattened_buffer` annotations.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTests] Fixed breakage in test_tir_buffer.py- Updated vload test for new behavior.- Added test for offset_of, testing behavior no longer in vload.- Added null check for buffer visitor.* fixup! Replacing Load/Store in codegens.* [UnitTest] ComputeInline, opaque access test updates* [UnitTest] Fixup, allow unit test to use `ib.pointer()[0]`.* fixup! Replacing Load/Store in codegens.The updated CodegenLLVM should use the BufferStore/BufferLoadconvention of indexing by `sizeof(dtype)`, rather than`sizeof(dtype.element_of())`.* fixup! Replacing Store/Load in lowering/legalization passes.BF16Legalize should also update the preflattened_buffer_map, since itis overwriting the `BufferNode::data` stored in the buffer_map.* fixup! Replacing all T.store TIR calls.* Fixed failing codegen c host unit tests.- Generated functions were making `uint8_t*` parameter arguments for  array handle for return value, rather than the earlier `void*`.- New parameter type was due to using  `PointerType(PrimType(DataType::UInt(8)))` as the type annotation, to  be usable as `BufferNode::data`.- Changing to `PointerType(PrimType(DataType::Void()))` still allows  usage as buffer, more appropriately expresses semantics.- Updated C codegens to allow `void*` types to be generated from  variables with type annotation, in addition to the previous behavior  of `DataType::Handle()` variables without type annotation.* Fixup, StorageFlatten when applied to post-StorageRewrite functions.Identified in a test that applied `tvm.lower`, then `tvm.build` on theresult.  If the result of an allocate node is used as the backingbuffer for multiple buffers, such as the output of the StorageRewritepass, then StorageFlatten would erroneously think that the secondoccurrence was an usage without earlier definition.* fixup, StorageFlattenWhen flattening a boolean buffer, the backing buffer should have typeint8, not the preflattened buffer.* Bugfix, correctly represent void* in LLVM IR.* Update, replace tir.Load with tir.BufferLoad* Added TVMScript error check for matching buffer/index dimensionalityNeeded for tests/python/unittest/test_tvmscript_error_report.py::test_high_dim_store* Bugfix, correct return type when lowering custom datatype.* Bugfix, removed unused primfunc from test_tvmscript_complete.py* Updated test_meta_schedule_postproc_verify_gpu_code.py TIRReplaced Load/Store with BufferLoad/BufferStore.* Allowed ramp nodes with buffer use analysis.* Updated tests in test_meta_schedule_postproc_verify_gpu_code.pyNeeded dummy writes to prevent buffer resizing, in order to triggerthe verification failure due to memory limits.* Updated TIR examples to be compatible with buffer dimension check.* Corrected section header in docstring.* Corrected indices size check in CogeGenC.* Fixed breakage in LowerThreadAllreduce.Since the AllocateNode is rewritten, any buffers that refer to thosevariables must also be rewritten.* [UnitTests] Replaced Store/Load in CUDA codegen tests.* Resolved breakage in C-based codegen for vectorized store/load.Needed to update to new convention of using the buffer's element typeas the stride.* Bugfix, incorrect LCA for buffer access in root scope.This had been present before the BufferLoad/BufferStore changes, buthadn't triggered on tests using Load/Store nodes.* Added docstrings for TransformNode member variables.* Added TODO for future removal of preflattened_buffer_map.* Fixup, transform layout + cache write tests.The correct sequence is to first apply any caching as needed, then toapply layout transformations, and finally to apply thread binds forthe computation step.* Bugfix, correct element type for scalarized access.* Bugfix, cuda buffer indexing when declared as different type.* Cuda codegen, update reference.* Bugfix, lower allreduceLoads of the output of the reduction should be replaced for allbuffers sharing a buffer pointer, not just for the buffer objectitself.* Removed obsolete comment.* Changed PrimFunc constructor preflattened_buffer_map to Optional* Removed flatten_buffer argument from T.match_buffer.* Correct call to VarUseDefAnalysis::VisitBuffer* Reverted unintentional testing change, lanes=2.* Updated lower_cross_thread_reduction to use buffer in allreduce* Updated transform_layout test to disable CSE* Updated CSE unit tests to use BufferStore* Replaced Store/Load for vta.transform and unit tests.* Updated unit tests for lower_cross_thread_reduction.* Updated arange to use scalar tensors.The start/stop/step tensors are declared as 0-d scalar tensors, butwere accessed as 1-d tensors.* Fix breakage in ethosu constant encoding.Buffers generated by "ethosu_copy" should have their buffer objectsrewritten, but shouldn't have their size updated in ethosu-specificCall nodes.* Fix breakage in ethosu call argument checks.Need to pull out indices from BufferLoad holders, not Load.* Resolve breakage from mismatched shape/index dimensions* Split out encoded parameters from preflattened buffer map.* Updated buffer shape/index dimensions to match in more ethosu tests* Fixed lint error* Removed debug code* Moved arith::Analyzer local variable to class member* Fixed SSA conversion of allocations.Can occur if allocation is inside an unrolled loop.  Added unit testto catch this failure mode.* Ethos-u index/buffer dimension updates.* Updated ethosu passes to handle buffer load/store.* Resolved bug in tvmscript printing of duplicate buffers.* Fix breakage in ethos-u test_assign_addresses, encode constants* Apply same changes to T.allocate_const as to T.allocateReturn a buffer when used in TVMScript, allow for aliasing buffers.* Fix lint errors.* Further updates for ethos-u tests.* Updated ethos.u buffer sizes in test.* Updated tir.BindParams to use BufferLoad instead of Load.* Updated topi.cuda.scan implementation to follow buffer dimensions.* Resolved breakage when flattening AllocateConst nodes.* Resolved breakages from latest merge with main.* Corrected error in merge.* Use empty indices for rank-0 tensor.* Added ir_builder workaround for 1-d indexing.* Consistent buffer access type in LLVM codegen, to match C codegen* StorageRewrite, update indices of modified buffers.* Dynamic relay nodes, access 0-d tensors with 0-d indices.* BFloat16 legalization, update buffer type.* Updated meshgrid to use 0-d index for 0-d buffer.* Corrected boolean handling in Allocate nodes.* Added workaround to unpack 1-d Tensor indices into N-d buffer indices.* Resolved a few more failures in relay tests on cuda.* Resolve linting* CI bump* Updated renormalize_split_pattern tests to use BufferLoad/BufferStore* Fixed cuda codegen checks for BufferStore/Ramp.* Simplify indices further, needed to avoid cuda register limit.* fixed dyn onehot shape func accessing 1d buffer with ()* Fixed codegen indexing for int4 scalar types.* Temporary workaround for incorrect constant folding.Need to further investigate vectorized LLVM constants* s/find_allocate_usage/FindAllocateUsage/g* Added buffer type consistency TODO.* Improved comment on address_of Op.* Rename LegalizeDtype to LegalizeDType, made private.* fix format and lint errors* Disable vectorization of AllocateConst buffer in StorageRewrite.* Pass buffer_map through to the PrimFunc in cmsisnn* try disabling problematic winograd test case* try different way of buffer mapping in storage_rewrite* Removed unnecessary ramp node in ir_builder.* Updated LLVM codegen for buffer indexing.TVM data arrays are always densely packed.  If the LLVM typecorresponding to a vectorized TVM datatype contains padding foralignment, the array location should be computed based on theprimitive element type.Co-authored-by: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: adstraw <astraw@octoml.ai>	5
[ARITH] Improve div/mod in rewrite simplifier (#3149)* [ARITH] Improve div/mod in rewrite simplifier* Fix lint error* Fuller file name in src/arithmetic/modular_set.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>* Generalize some rules* Replace gcd factoring with specialized rules* Mark rules that don't work for non-truncated division* More tests	3
[NNPACK] Modernize test (#2868)	3
[Relay][Frontend][Onnx] GRU Layer Support (#6020)* GRU debugging and testing added to onnx frontend.* All tests working and code formatted.* Fix lint issues.* Add a test case and changed RNN argument parsing.* Small refactor.	4
[microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger	1
[Relay] Refactor Interpreter to treat lowering as IRModule->IRModule rewrite. (#8597)* This continues the work outlined in the RFC  https://discuss.tvm.apache.org/t/rfc-relay-tecompiler-rewrite-existing-compile-engine-to-match-updated-compiler-flow/9233This gets about halfway there for the Interpreter:* Remove direct access to TECompiler from interpreter, and instead call  tec::LowerTEExpr when 'preparing' a module and expression for evaluation.* Make clear there's no phase distinction between create_interpreter and  evaluate on the Python side -- both must be prepared together as a single IRModule.* But in return make sure the result of evaluate on the Python side is a packed func  ready to directly apply 'simple' arguments to an already interpreted closure.* The interpreter builds and caches primitive TIR functions (and their corresponding  dynamic shape functions) as packed funcs as they are encountered.* Cleanup uses of interpreter for constant folding on the C++ side.Future work:* Fold LoweredModule into IRModule so tec::LowerTEExpr is just another pass.* Get rid of the implicit caching of lowered functions in TECompiler.* Make calling convention from Relay to TIR explicit, and remove all the function  attribute hackery currently needed so the interpreter can correctly invoke lowered  functions as it encounters them.* Make TECompiler private. Though could do this now it will make migrating the VM and  AOT uses of CompilerEngine harder.Force a gc between sphinx-gallery items to reclaim GPU memory. (#8722)GPU memory is only released once the PackedFunc for evaling the model is gcedby Python. In CI we're noticing intermittent 'CUDA: Out of memory' failureswhile processing the tutorials, and tracing showed there was no gc happeningbetween items. Not confident this will solve the problem but worth a try.* Get rid of logs spam.	2
[Relay][Frontend][ONNX] Fix reshape precompute, and type error (#3230)	0
[Relay][Frontend] Tensorflow version support upgrade from 2.1.0 to 2.3.1 (#6706)	1
[INFRASTRUCTURE] Migrate to json based config. Move gemm test to integration. (#28)* Migrate to json based config. Move gemm test to integration.* temp checkin* checkin  example json	5
Make Autopad static when available (#7755)	1
Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.	4
[docs] Fix the error in install/from_source.rst file (#11796)#10755 changed the TVM_LOG_DEBUG separator from ';' to ','.  This PR changes installation guide file accordingly.	2
Bump pyxir version tp v0.3.5 to avoid bad cleanup error with pyxir and tensorflow 2.6 (#10858)	0
[Frontend][ONNX] LSTM Support (#4825)* Initial version working and passing tests.* WIP on supporting other activations.* add support for multiple activation functions in lstm* All tests working and code cleaned up.* Undo import swap to avoid conflict with masahi.* Added new tests and related bug fixes.Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>	5
Decoupling AOT from graph memory planner (#8096)* Fix an issue with storage-rewrite pass and packed functionsChange-Id: I13888471d4b8927a4012d6a8e749fb7a8935dd77* RebasingChange-Id: I7aa12e0217b8a2e1ff2a97a7c5fdda6b7597ae64* Addressing commentsChange-Id: If9f1ee190690f9a810fe41eb1933d736f1eb4ec3* Add a pass to legalize packed callsChange-Id: I8aa43d3a1b837b03a5cf3c6b32fc760bd78d3436* Add a unit test for the legalization passChange-Id: I5b0d75380ff660dd5a0acf5b14fa84bb992fbec4* rebasingChange-Id: I52ceab5cf6e9b54390cb36c18dbb8e22505d8e18* Use common StorageInfoChange-Id: Ia8b7de1373f167ca7d0d69a99846d417405bbe48	5
[Hexagon] Fix scripts to enable automated testing on hardware (#10491)* Fix test and scripts for HW* revert port forwarding* address comments* address comments	1
Enable ONNX tests that needed onnxruntime 1.7.0 (#8502)	1
Fix docstrings in tvm.relay.cast_like (#8262)* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos	1
[TUTORIAL] Onnx tutorial (#50)* add onnx* fix	0
Enable uTVM in Jenkinsfile (#3269)	2
[community] @electriclilies -> Reviewer (#8684)	3
[Frontend][TFlite] Cast MirrorPad paddings to int32 (#9468)* [Frontend][TFlite] Cast MirrorPad padding to int32 As an int64 paddings of MirrorPad would generate wrong TIR as result, try to cast to int32 for best compatibility.* [Frontend][TFlite] Add tests for MirrorPad with int64 paddings* Update test_forward.py	3
[Vulkan] Support uniform buffer object for passing many scalar arguments (Take 2) (#7833)	4
[Relay, TOPI] Make Softmax op fusible with elemwise ops (#8909)* Change softmax op pattern to OUT_ELEMWISE_FUSABLE* Softmax is fused but x86 schedule is suboptimal* fusion properly done* Updating GPU schedule for fusion* update softmax warp shuffle schedule* fix compute_at* Bug fix in lower_thread_all_reduce when reduction storage is reused by storage_rewrite* Temp disable softmax warp reduction schedule when softmax is fused* Revert "Bug fix in lower_thread_all_reduce when reduction storage is reused by storage_rewrite"This reverts commit 8aa340e23438a922905b1e247afa8f223f284a9c.* lint fix* try make diff smaller* fix tests* fixed another broken test* Fix flaky uTVM templating test* fix equality check on output opCo-authored-by: masa <masa@pop-os.localdomain>Co-authored-by: Gavin Uberti <gavin.uberti@gmail.com>	0
Fixed extra reshape parameter bug. (#4524)	0
[QNN] Fix per-channel broadcast with invalid axes (#10936)* [QNN] Fix broadcast for invalid axis* broadcast -> channel	0
Introduce Apple BNNS backend (#7299)* Introduce Apple BNNS backendThis is simple JSON based runtime which offload execution ofsome operation into Accelerate frameworks via BNNS api.Works only for: * macOS 11.0 and later * iOS 14.0 and laterSupported primitives: * conv2d and fusing with bias and relu * dense and fusing with bias and relu/gelu * batch_matmulSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add conv2d DW testAlso fix some pylint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix clang-format issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Refactoring. Add TView abstractionSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add several more onnx topologies into testsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Avoid redundant tensor allocationSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix conv_splitter issueSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix isse with bias {1,1,1,1}Signed-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Min. Rename fileSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix review comments. InitialSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] test refactoringSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix cpplint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix clang-format issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix python formatSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix pylint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix pylint. Second attemptSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add integration documentation* Check onnx import before useSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add instance normalization operator* Add fusing sigmoid activation after conv2d* min changesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Add pooling operations to BNNS runtimeSupports `nn.max_pool2d`, `nn.avg_pool2d`, `nn.global_max_pool2d` and`nn.global_avg_pool2d` operations* Fix lint* Fix lint* Apply comments* Fix documentation* Fix comment to refer to BNNSCo-authored-by: dlexplorer <elvin.nnov@gmail.com>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>	0
Additional mali target support (#794)* Add Mali target support to tvm.target.create* Add Mali target support in codegen	1
add docs (#75)	2
[ONNX] Dynamic Gather (#7787)* add regression test* fix regression* fix lint	0
[DOC]Update documentation (#2286)	2
Fix Zephyr flashing on physical hardware, busted in #7813 (#7853)	0
[ci] Use r5.large nodes for builds and lint (#11258)This uses `r5.large` for linting and build steps and splits lint into 2 to keep runtime down. This is a subset split off of #11120. Once `task_cpp_unittest.sh` is fixed so it picks up sccache we can enable these smaller nodes there as well.	0
[Tensorflow, NNVM, TOPI] Support for logical operators (#2453)	1
[TIR][Schedule] Analysis functions to check if compute_inline and com… (#9743)* [TIR][Schedule] Analysis functions to check if compute_inline and compute_inline is allowedCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>* Address commentsCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
[ARITH] Simplify casts of constants 0 and 1 (#3758)* [ARITH] Simplify casts of constants 0 and 1* [EXPR] is_const_value to check whether non-ints are consts* Revert "[EXPR] is_const_value to check whether non-ints are consts"This reverts commit 7e1b3462e3f74fd0afb1541d72978107cfa23c30.* Use tvm::cast	1
[LLVM] Register factory function for CodeGenCPU (#11852)* [LLVM] Register factory function for CodeGenCPUAny target that has its own subclass of `CodeGenLLVM` must registera factory function that constructs an object of that class. Thisfactory will then be looked up and used in `CodeGenLLVM::Create`,which is the generic interface to create an LLVM code generator.However, there is no factory for `CodeGenCPU`, and so the creationof a `CodeGenCPU` object is done inside of `CodeGenLLVM::Create`.To make this happen, codegen_llvm.cc includes codegen_cpu.h, whichmakes the base class implementation depend on the derived class.This backwards dependency can be resolved by registering a factoryfor `CodeGenCPU`.* Add missing factory functions for other targets* Add cpp tests for codegen factories	3
[Ansor][AutoTVM v2.0] Phase 1: Add follow_split and follow_fused_split steps (#6142)* Add cache_read/cache_write step* Update* Add follow split and follow fused splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>Conflicts:src/auto_scheduler/compute_dag.ccsrc/auto_scheduler/transform_step.ccsrc/auto_scheduler/transform_step.htests/python/unittest/test_auto_scheduler_loop_state.py* add loop_state.pySigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update* Update* Update state->current_compute_dag to Optional* Add some doc strings for Follow_Split and Follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Check code using c-lintSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add more doc strings and change the order for follow split.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_split and follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_fused_split.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add test record for follow_fused_split1. delete a comment2. add "fuse" between follow_split and follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add doc strings for some functions and variablesSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Fix the code format in src/auto_scheduler/transform_step.hSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update* Update doc* Update* Update* Fix follow_split and follow_fused_split record test.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Doc update* Update some doc stringsSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Fix code style and some function definitions.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add comments on parameters.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add more doc strings and fix some.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>Co-authored-by: chengfan.jcf <chengfan.jcf@alibaba-inc.com>Co-authored-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>	5
[GOLANG] Some fixes for golang latest version compiler. #3119 (#3182)	3
[CodeGen][OpenCL] Limit OpenCL built-in vector lanes to 2, 3, 4, 8, 16. (#7777)	5
[LLVM] Explicit llvm::StringRef to std::string conversion (#4859)	5
[DOC] CoreML frontend tutorial (#2667)* [DOC] CoreML frontend tutorial* Update tutorials/frontend/from_coreml.pyCo-Authored-By: kazum <morita.kazutaka@lab.ntt.co.jp>* Update tutorials/frontend/from_coreml.pyCo-Authored-By: kazum <morita.kazutaka@lab.ntt.co.jp>* Addressed comments and added the original author	1
[DOCKER] Fix vulkansdk in the ci-gpu (#5566)	0
[Relay] Ensure nested higher-order functions are treated correctly (#2676)	1
Update test_rpc_exec.py	3
[Relay][Quantization] Fix Bug Which Cause Negative Left Shift Op (#7432)	1
[ARITH] Revamp IntSet (#3272)	1
[Relay, Topi] [TF, MXNet] Unravel Index operator (#5082)* first cut unravel_index* merge fixes* change rates to dilations* unravel_index op relay, topi, mxnet, tf* doc changes* small changes* remove empty unravel and argwhere attrs* remove empty unravel and argwhere attrs	4
[BYOC-DNNL] add partition test on sum pattern (#12357)* add partition test on sum pattern* fix lint	0
update relay python api doc (#2766)	2
[AUTOTVM] Decouple build and run in measurement (#1661)	1
[TOPI] Sparse Add Op added (#7435)* [TOPI] Sparse Add Op added* lint resolved* TF frontend support added* Test case added* [1] Review comment handled* [2] Review comment handled* [3] Review comment handled* [4] Review comment handled* [5] Review comment handled	0
[Relay] Move TOpPattern registration for nn.* to C++ (#12072)* [Relay] Move TOpPattern registration for nn.* to C++Some of the Collage machinery is best tested from C++, butrequires Relay ops to have their "TOpPattern" registered.However since the nn.* ops register on the Python side testscan't rely on those ops.The easy fix is to just move the registration to theRELAY_REGISTER_OP block. However since kOpaque is thedefault I did not preserve those registrations.There's still a few dozen more exotic ops still registeredon the Python side. I've left them be.* - D'oh! Even kOpaque ops must be registered.	4
Minor improve to assertion (#3295)	3
Free TensorRT engine and context (#7702)	5
[Relay] Handle float16 constants & fix BatchNorm (#3260)	0
Run C++ TOPI tests in Jenkins build (#870)* Added +x permission to task_cpp_topi.sh. Added C++ topi tests to Jenkinsfile* Fixed test_topi_math.py* Minor style fix	0
bump dockerfile (#6632)	2
[CI][MicroTVM] Disable autotune log check since microtvm autotune has erros  (#9639)* commented tune check* trigger	2
[RUNTIME] keep opencl runtime deps free from node (#1349)	1
Change CodeGenCPU::GetPackedFuncHandle to generate global variable with InternalLinkage. (#901)Emscripten seems to not have done initialization properly.	5
Fix plint complain for some files. (#10433)	2
[TIR][Schedule] Add get-child-blocks primitive (#9434)* get child blocks* fix* lint* fix	0
Couple patches to docker/bash.sh after #11976. (#11988)* Use python3 to run determine_docker_images.py * Properly detect presence of CI env var with + expansion.	2
[Relay/topi] Support scalar inputs in where op (#6383)* support where with scalars* add test for where with scalar* add comment	1
[microTVM]Fix dense_dsp schedule in autotuning (#12271)* dense layer used in autotune* format	1
[Vulkan] Added debug saving of Vulkan shaders, environment variable documentation. (#8333)Frequently, looking at the shaders generated by the Vulkan codegen isuseful for debugging.  While this can be done by checking the`mod.imported_modules[0].get_source()`, that requires the shader tofirst pass validation.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[CUDNN] Refactor descriptor initialization, remove `cudnn.conv.output_shape_from_cudnn` (#9948)* Introduce SetConvdescriptors to refactor cudnn/conv_forward.cc* more refactor* remove cudnn get output* cpplint	1
Fix an OrderDict initilization bug. (#2862)The dict which is used to initilize OrderDict is not ordered, so  metadata may not be at the end.	5
Increase depfiles lookup. (#509)Not able link build/src/top/tensor/*.d.       Hence don't compile nnvm for a change in tvm/topi headers.	4
[BugFix][TIR] Fix primitive `Bind` for init-inside blocks (#9359)* [BugFix][TIR] Fix primitive `Bind` for init-inside blocks* fix python black error	0
Fix vcvtph2ps codegen (#2925)	0
Fix the build to generate .d file (#99)	2
[TOPI] Fix adding dilation arguments (#2047)	1
Add ignore storage_order attribute to onnx pooling parser. (#5781)	1
[OP] Experimental assign op (#389)	5
removing nnvm dep from VTA sources (#4419)	4
[rust] convert error msg to string for panic macro (#8289)	0
[TOPI] Fix compiler warning in topi cpp (#837)	2
[FFI] Renamed __VisitAttrs__ and __fvisit__ to non-reserved names (#11392)All names beginning with two underscores are reserved for thecompiler, even if they occur inside a class or namespace.	2
[Relay][Training] Add and fix gradients (#4126)* add and fix gradients* fix linter issues	0
[TVMSCRIPT] Improve tvmscript type hints (#11654)* [TVMSCRIPT] Improve tvmscript type hints- Change numeric types to classes so they work as function arguments.- Add var as a class.- Add floordiv, index, and mod to PrimExpr.* use Union	1
[Relay][Training][Pass] Factor out first-order AD to a module pass (#7677)	4
[APPS] add an external dll call example (#2156)	1
Add black to lint docker image (#6451)	2
[AutoScheduler] Supported CSE (variable definitions) in feature extraction (#10686)Add supported for LetStmts in feature extraction. A stack of variable definitions is maintained and added to the arithmetic analyzer at the appropriate points. The buffer access analysis now creates a new arithmetic analysis context per set of loops to avoid redefining variables which is unsafe in the presence of let statements.	5
[DOCS] Add logo and tracker to docs (#1064)	2
Minor bug: causes undefined symbol error in libtvm from NNPACK (#1368)	0
[TVMC] Allow optional arguments to be passed to importers (#7674)* add support for optional args for frontends tvmc* remove unnecessary comments* Add changes suggested by Matt W. via PRCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>	4
[Relay][heterogeneous] Fix tuple annotation (#3311)* [Relay][heterogeneous] Fix TupleGetItem* retrigger ci* retrigger ci	1
Create a ctypes cython optional compatible package (#11)	1
support string option when create cuda/rocm/... target (#1071)	1
Add lift_if_then_else pass (#3865)* Add LiftIfThenElse pass* Add more comments* Rename and refactor* Add description for internal data structure* Rename a test* Minor change* Address comments* Improve update_for	5
[Bugfix] Visit each input param of the function in ExprVisitor visit_function (#8521)	1
[microTVM][Zephyr] Add skip for AOT test (#8628)* add hex indicator to message* add pytest skip* trigger* trigger	3
add missing narrow down of index within conditions (#11942)	1
[TVMC] micro, run: Disable micro support when USE_MICRO=OFF (#9632)Do not generate 'tvmc micro' subcommand and disable micro device optionin 'tvmc run' when TVM is built without support for microTVM, i.e. withset(USE_MICRO OFF).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
checkin domain	5
ROCm: Add SaveToFile and LoadFile (#3665)...and add rocm module_save to the tests.	3
[TOPI][Relay][TensorFlow] Add OneHot operator (#3781)* Add one-hot to Relay* topi implementation* Working* add topi test* Add TF test* Fix check* fix linting issues* fix documentation* Fix documentation* Add support for on_value, off_value, axis, dtype* Add full support for axis* Fix compute and update test_forward* Move on_value and off_value to inputs* Add topi test* Update tests* Update docs* Fix style* re-enable tests* Add one_hot to mxnet converter	1
Fix Annotate Target to support freevars(relay.zeros, relay.ones etc) of any size (including zero)  (#6826)* Fix Annotate Target* Add Test Cases* Formatting* Comments C++* Remove Unnecesssary test cases* typo* annotate_targetCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>	1
Remove unnecessary wasmtime from ci_lint (#10653)	4
[TOPI][CUDA] int8 group conv2d  (#2075)	5
[TIR][PASS] Remove legacy HoistIfThenElse (#5944)This pass has not been migrated to the new transform API,and contains potential bugs per https://github.com/apache/incubator-tvm/issues/5559.Given that it is not being actively used, this PR remove this passfrom the collection.Followup PRs are more than welcomed to land a better version thatconforms with the new transform API.	1
[ETHOSN] Remove support for older versions of the driver stack (#12347)Removes support for driver stack versions older than 22.05(semantic 3.0.1). Additionally, changes the integration to makeversion checks using semantic versioning rather than the previousyear.month versioning method.	1
[µTVM] apps: Fix Zephyr code example for STM32F746 boards (#7772)Commit c39a6e25d "Clean up uTVM demo runtime, add ONNX model test andtutorial (#7557)" changed the location of the Zephyr code example toapps/ so this commit updates the tutorial examples under tutorials/microto reflect the new location where src/main.c resides.Since commit c39a6e25d also split Zephyr configuration per boards,under project's boards/, this commit also adds a proper config for theSTM32F746 Discovery board and fixes a nit in the comment inboards/nucleo_f746zg.conf. It also removes CONFIG_MAIN_STACK_SIZE=50 fordisco and nucleo boards since the MCUs for both boads are Cortex-m7based, not Contex-m33.For the new boards/stm32f746g_disco.conf CONFIG_TEST_RANDOM_GENERATOR=yis set, otherwise when CONFIG_ENTROPY_GENERATOR=y linking will failwith the following error:rand32.h:33: undefined reference to `z_impl_sys_rand32_get'Finally, the size of 'uart_data' temporary buffer is reduced a bit (to8 bytes) to free some additional bytes in SRAM, since most MCUs have a1-byte FIFO, like it happens with Cortex-M-based MCUs.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
Support mxnet _contrib_SyncBatchNorm (#6245)	1
Rollback changes to SSA begin/end scope for Store in (#7073)C codegen. Instead, scope binary operator codegen inCUDA to fix the issue originally addressed by 5f4b9a9.	1
[ONNX] Unique op should always return int64 indices (#9490)	5
Remove deplicate line (#6017)	4
fix: select narrow dtype (#10519)	0
[BugFix][TVMScript] Fix printer for dependent loops (#9506)	0
[NODE][IR] Introduce StructuralHash for the Unified IR. (#5160)* [NODE][IR] Introduce StructuralHash for the Unified IR.This PR introduces a new way to handle structural hash for the unified IR.- Each object can now register an optional SEqualHash function, which  describes how to reduce its structural equality to sequence of hash values.- Optionally, the object can choose to allow labeling of vars(e.g. function parameters)  by calling DefHash- We implemented a non-recursive structural hasher that maintains its own stack  to traverse te IR.This PR also improves the hash value property from the previous relay's hash utility.In particular, the graph node mode hashs a DAG differently from a treeby attaching an unique occurence index to each graph node.In all of the test cases so far, structural_hash is consistent with structural_equal.- if structrual(x, y) then structural_hash(x) == structural_hash(y)- if structural_hash(x) == structural_hash(y) then highly likely structural_equal(x, y)  - hash no collison is found in our testcases.Ideally we should work on automatically generating these functions in the future.* Fix cases for EnvFunc and Array dims* fix testcase* Update src/node/structural_hash.ccCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>Co-authored-by: 雾雨魔理沙 <lolisa@marisa.moe>	5
Clear warnings when building with MSVC. (#10059)* Fix warning "unsafe mix of type 'const int64_t' and type 'bool' inoperation" occurring in tvm::tir::HasAnn* Suppress warning "destructor never returns, potential memory leak"occurring in tvm::runtime::detail::LogFatal::~LogFatal	2
[Docs] Moved the generated tutorials folders into a _staging folder. (#8735)* [Docs] Moved the generated tutorials folders into a _staging folder.Previously, reorganization or renaming of tutorials could causedocumentation tests to fail in CI.  The CI checks out the version tobe tested, which may still have generated documents in`docs/tutorials` and `docs/vta/tutorials`.  If a PR moves these todifferent folders, then they show up as duplicate `*.rst` files,resulting in sphinx warnings.This commit makes a `docs/_staging` folder in which sphinx is run.All tutorials are generated within this folder, and the entire foldercan be deleted with `make clean`.  As a result, it is safe toreorganize the tutorial without impacting CI.* Updates based on reviews.* Changed graph_runtime references in deploy_classification.py to graph_executor* Removed unnecessary graph_runtime import from tune_alu_vta.py	2
Add FreeRTOS dependencies to ci_qemu (#9854)This enables us to build demo applications which use FreeRTOS	1
Sort VM stats by time (#4601)	5
[CYTHON] Correct backtrace print for python3 (#989)	5
[Relay/TOPI][Op] Add batch_matmul in relay and TOPI (#2561)* Add batch_dot and cpu schedule* Add relay support for batch_dot* Rename batch_dot to batch_matmul* nits* Add missing file* Put batch_matmul and dense x86 schedule in separate files* Fix pylint* Remove unused import* Add cuda schedule for batch_matmul* Add test case with larger batch size* Add batch_matmul in api doc* Fix quantize pass rounding error* Fix pylint and minor change* bug fix	0
[TVMScript] IRBuilder, IRBuilderFrame base class (#12482)* [TVMScript] IRBuilder, IRBuilderFrame base classThis PR introduces basic data structures of the generic IRBuilderacross the codebase.IRBuilder is a general-purpose IRBuilder that can be used in TIR, Relaxand any other vendor-specific dialects; IRBuilderFrame is where contexualinformation as stored in the IRBuilder.* fix linter* Update include/tvm/script/ir_builder/base.hCo-authored-by: Junru Shao <junrushao1994@gmail.com>	5
Revert #5238 (#6680)	4
[TEST][TEDD] improve TEDD tests to also run on CPU Docker image. (#6643)* Amend regular expressions to match with what is being reported   by CPU Docker image Graphviz * Fix typo on dependency checking function * Organise imports	2
Fix missing <cassert> header, caused compilation failure. (#7740)	0
[DOC] Fix c++ doc build (#240)	2
[CMAKE] Compatible for ROCm before 3.7 (#6359)	1
Metal reinterpret fix (#3706)	0
Demote session traffic logs to DEBUG log level (#6989)	2
[DOCS] Update main website to tvm.apache.org (#4429)* [DOCS] Update main website to tvm.apache.org* Update jvm pom repo loc* Change the org to asf* Update ci addr to new one	1
[NNVM] TF: Add Pack operation (#1570)	1
Pin rust version used to compile tools (#2852)	1
Optimize Linux shared library modules (*.so files) (#2445)	2
Move CombineParallelConv2D to opt level 4 (#3248)	4
[ETHOSN] Per-tensor support for int8 operations (#10018)* Per-axis quantization to follow	1
Use bridge network and expose port on macOS when launch docker image (#3086)	2
[COMMUNITY] comaniac -> reviewer (#4841)	3
Use std::make_unique instead of std::unique_ptr(new ...), NFC (#12459)	1
[microNPU] Support different constant datatypes (#9626)Currently only uint8 datatype is supported for constants, as this isall that was necessary until now. This PR allows different datatypesto be used for constants, including different datatypes within thesame graph.A workaround was previously added for Mean legalization, this hasalso been removed and replaced with the expected datatype of theconstant.	5
Add batch_matmul convertion to FQ2I pass (#8635)	4
[Relay] Fix a corner case of fused identity (#11217)* fix a corner case for fused identity in te compiler* make fallback code work with gpu	1
yolo reorg op for relay (#1941)	2
[ONNX][Converter] Add dynamic nodes support (#9380)* [ONNX][Converter] Add support for dynamic nodes* fix lint	0
[FIX,PROFILING] Add extra precision to numbers when serializing to json (#10392)Numbers were serialized with too little precision when serializingprofiling reports to json. Deserialization can then sometimes round thenumber differently than if the full precision was available.Fixes #10382.	0
[Relay] Add dynamic SparseToDense (#6892)* [Relay] Add dynamic SparseToDense* Fix comments	0
[AutoScheduler] Support string processing to records (#7144)* [AutoScheduler] Support string processing to records* doc* remove log	2
[BYOC] Refine AnnotateTarget and MergeCompilerRegion Passes (#5277)* add target to region* refactor annotate_target* Make all unit test working* quick fix* enable BN, unit test failed* Fix vm test, unit test. Refactor annotate_target a bit.* quick fix fusion* revert fusion change* style fix* Refactor merge region pass* format* minor fix* Skip e2e test* lint* support AnnotateTarget multiple runs* Add HasAttr and revert DNNL codegen* address commentCo-authored-by: Zhi Chen <chzhi@amazon.com>	1
[TUTORIAL] Fix downloaded file path (#2590)	2
fix get layout in to_relay (#2610)	1
[microNPU] Fix offloading incompatible average pool (#11469)Fixes offloading a few corner cases of average pooling. Specificallynot offloading nn.avg_pool2d when:* The attribute count_include_pad=True* Padding exceeds the dimensions [3, 3, 4, 4]* The pool size is greater than [8, 8] when the pool uses paddingChange-Id: I7be546e28ebe1f17482f3ed3cee56996a71bfcd1	4
add output format to ndk build func (#2999)	1
[MetaSchedule] Introduce `ScheduleFnDatabase` (#12626)Following #12520, this PR introduces `ScheduleFnDatabase`, a mockeddatabase to allow injecting handcrafted schedules provided by a schedulefunction.The schedule function comes with the following signature:```pythondef schedule_fn(  sch: tir.Schedule,) -> bool:  task_name = sch.mod.attrs["task_name"]  # ^^^ provides an optional name of the task queried  ...```This mocked database helps incorporate the existing testing utility`apply_fixed_schedule` more formally into the MetaSchedule-Relay buildpipeline, and allows further extension to Relax with the same interface.Next as another follow-up, we will introduce ConcatDatabase that allowsmixing multiple databases, including the mocked and ones from JSONfiles.	5
change id (#9431)	4
[ONNX] Add Compress Support (#9067)* compress impl* unit tests* docstringCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[Relay] Reference (#2489)* movefix testfix lintfix testadd more codefix lintbetter type infer ability* fix build* address comment	1
Sanitize names of input tensors in interface header (#8720)* Sanitize names of input tensors in interface headerChange-Id: I7f02a993887bf84316262cd2586a734a9079c338* Update tensor name sanitizer tests to parameterize them.Change-Id: I157d8d8d607de2904285e403893f146e97b510d5* Only test unpacked, C interface API, AOT caseChange-Id: I9082ae32079a1a3924c06c7f26c757aafa46dec2	4
update test (#48)	3
[TVMC] rename composite target "acl" to "compute-library" (#7508)* Renames the "acl" composite target to point to the specific   library it represents	1
[PROFILING] Profiling over RPC (#8885)* [PROFILING] Profiling over RPCAllow for profiling over RPC by serializing the returned report beforesending it. Also remove collectors argument when profiling over rpcbecause it cannot be serialized.* lint* fixes* add comments	1
[VERSION] Update to 0.6.dev (#2736)	5
[FRONTEND][TF] Add conv3d (#4604)* [FRONTEND][TF] Add conv3d* fix high rtol	0
Allow Array/Map store objects that are not NodeRef (#4430)	1
[API] Added remove_global_func to the Python API (#6787)This is useful for unregistering functions after a test.Change-Id: Ic39499aa8f36bfe5470bc1f058ad3b96cf52b49c	3
[microNPU] Re-enable LayoutOptimizer pass (#9793)* [microNPU] Re-enable LayoutOptimizer passIt looks like in #9597 the LayoutOptimizer pass was accidentally removed.Probably due to a race condition in PR's. Re-enabling this feature.Change-Id: I4fc16a440f90277c5fcd887715166332af052c6b* change pass orderingChange-Id: I6e7a22f46660029bbf4be3deb2be929cecf5d365Co-authored-by: lukhut01 (generated by with_the_same_user script) <lukhut01@e127400.cambridge.arm.com>	1
[CI] fix ci_gpu dockerfile (#11644)	2
Fix pytorch frontend prim::Constant issue (#6051)	0
[CI][Fix] Remove additional qnn.op.transpose_conv2d from docs (#10083)Fixes CI after #10077, and replaces misuse elsewhere.Change-Id: I095fc8ea2b8d268b09538832cba1f5482a73a9d9	4
[Fix] relay onnx frontend bug when [A, B, M, N] * [1, B, N, K] (#9911)* [Fix] relay onnx frontend bug when [A, B, M, N] * [1, B, N, K]* fix lineCo-authored-by: tomoyazhang <tomoyazhang@tencent.com>	0
[Fix] Fix some errors in unittests (#12170)* unittests fix 0* fix unittests* fix unittests* fix unittest* fix unittest* fix unittest* Revert "fix unittest"This reverts commit 09b6b410bc51e91ca256e888d380e5648739d257.* fix unittests* fix	0
PIL is depreciated and should be replaced with pillow (a fork of PIL) (#4533)Change-Id: If2075df5475505f2da87dae7145af5a7ab83d8a4	4
Remove deprecated opengl files (#5711)	2
Add most of IR constructors	1
[TENSORRT] Improvements and fixes for TensorRT (#11203)A number of small fixes and refactors to improve the robustness ofthe TensorRT integration.Co-authored-by: Mark Shields <mbs@octoml.ai>Co-authored-by: Mark Shields <mbs@octoml.ai>	5
[TOPI] add injective scheduler for HLS backends (#1553)* [TOPI] add injective scheduler for HLS backends* Introduced PrintBinaryExpr	1
[Relay] Add `conv2d_backward_weight` op (without topi) (#9954)* python plumbing* add cpp def* legalize worked* clean up* layout conversion doesnt work* extract wgrad body* fix convert layout* black* fix kernel size* revert irrelevant change* add doc, clarify the meanings of parameters* update layout convert* test passed* fixed layout conversion* update convert layout* remove print* remove layout convert for now* minor fix* removed unused import* add wgrad python reference* add test stub* add doc* test other stride and pad* tweak* more pylint filter* fix typo in doc* swap arg order (data, grad) to be consistent with conv2d_transpose(dgrad)	5
[Relay][Runtime] Add memory manager for NDArray (#3121)* Add support for custom NDArray memory managementCredit to @icemelon9 and @wweic* Fix copy-paste issue* Fix naive allocator.h* Remove buffer field* Apply Wei's suggestions.Co-Authored-By: jroesch <roeschinc@gmail.com>* Fix Wei's suggestion* Fix go rts* Break MM dependency* Add docs and clean up diff* Add more docs* Move to VM folder* Fix lint* Remove Go dep.* Rename to Empty* Address Haichen's comments	1
[bugfix] remove duplicate resize (#3902)	4
[TVMC] Only load extra targets when there are workspace pools (#12253)After #11427, `tvmc compile` wouldn't work for external codegens thatdon't have a `Target` registered by `TVM_REGISTER_TARGET_KIND`. Suchexternal codegens can be expected to have no workspace pools and may notalways have a target associated as their implementation predates thismechanism. While it is likely a `Target` is specified for all externalcodegens in the future, we should still support external codegenswithout an associated `Target` until this is enforced.Co-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>	1
Fix vulkan build with homebrew install of vulkan-sdk (#1802)	0
[ci] Disable flaky microTVM tests (#10313)See #10312cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[CODEGEN/RUNTIME] Metal support, runtime improvement. (#111)* [CODEGEN/RUNTIME] Metal support, runtime improvement.* Fix case when no device is available	0
[TOPI] Fix `nn.pool*d` issue with 'vectorize' function and add unit tests (#8541)* Fix issue in 'vectorize' function for 1D and 3D tensors* Add pooling tests for channel last layouts* Add support for more general layouts in "poolnd" implementation* Reformat with 'black'* Fix lint issues	0
[TAG] Add tvm.tag module for tagging operator (#217)* [TAG] Add op_tag module for tagging operator* Fix accroading to comments* Add example* Add into doc* Add --fix-missing for docker	2
[Fix][VM] Fix copy constructor (#5237)	0
[MetaSchedule][M4a] Mutator: Mutate-Tile-Size (#10092)* [MetaSchedule][M4a] Mutator: Mutate-Tile-SizeCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Python 3.8 has no `math.prod`Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
init (#3571)quickfix	0
[microNPU] Adding rounding mode attribute to operators (#9514)* [microNPU] Adding rounding mode attribute to operatorsAllows rounding mode to be specified for each supported operator.By default "TFL" is used, which matches that of the behavior of TFLite.Other rounding mode options include "NATURAL" which rounds to thenearest value and "TRUNCATE" which rounds towards zero.	1
[TVM] Reduction simplification improvements (#2284)	1
[TOPI] Move ewise.h -> elemwise.h (#327)* [TOPI] Move ewise.h -> elemwise.h* fix test	3
[Refactor] Introduce target generic dispatch system (#556)* [TVM] Introduce target generic dispatch system* fix target warning	2
[TUTORIAL] Update tvm.make.Select to tvm.select (#177)	1
[Relay][Runtime] Add support for virtual machine Objects (#3120)	1
Update ci-cpu to v0.78. (#9199)* Close #9158	5
[TIR] TIR Schedule Misc Update (#10341)* tir schedule misc update* Trigger Build	5
Update rpc_module.cc (#10881)Sometimes our locally built module is wrapped as a submodule of the top module. In such cases, we want to fetch the symbol from the imported modules, other than only searching in the top level.	2
[AutoTVM][BugFix] Fix variable name conflict with OpenCL keyword (#6048)Co-authored-by: Yanming Wang <yanmwang@amazon.com>	5
[Tensorize] Support conds depend on outer loop vars inside tensorize scope (#7497)* [Tensorize] Support conds depend on outer loop vars inside tensorize scope* Reformat	1
[Strategy] Support for Int8 schedules - CUDA/x86 (#5031)* [CUDA] Op strategy changes for Int8 schedules.* Applying Haichen's suggestions.* Make 4D output work for task extraction.* Make x86 work.* Fix lint.* Lint fixes.* Tests, comments, out channel a multiple of 4.* Topi test.Co-authored-by: Ubuntu <ubuntu@ip-172-31-38-96.us-west-2.compute.internal>	3
Introduce centralised name transformation functions (#9088)* Introduce centralised name transformation functionsTo address some of the concerns raised in https://github.com/apache/tvm/pull/8280 and https://github.com/apache/tvm/pull/8720 I've put together a series of functions to combine together names to re-use between these areas. These are meant to be a starting point to fix up the name generation to use the TVM C conventions and port the interface API header to C++.These functions will also be used for constructing the names in the C Device API (https://github.com/apache/tvm-rfcs/pull/31).* Improve error handling and error messages* Sanitize sanitise to sanitizeThis patch aims to sanitize uses of sanitise to the form of sanitize to be consistent with the overall codebases use of American English.	1
[VTA] add support to event counters (#3347)* add support to event counters in VTA* fix comment* fix event-counter interface parameter* no longer needed* add sim back* add docs to event counters* fix docs* add more details about event counting* make dpi-module docs more accurate	2
[USMP] HillClimb stability patch (#10547)This patch increases stability of the hill climb allocation algorithmChange-Id: I56414ae661fa856baeddce00f4717a9f5a9e2954	4
[Hexagon] Add unit tests for Hexagon Device API (#11319)* [Hexagon] Add unit tests for Hexagon Device API* add scalar alloc for Hexagon + cleanup	4
[AOT] Use python temporary directory for AOT tests (#10518)* [AOT] Use python temporary directory for AOT testsUses a python temporary directory with a context manager in an effort tosolve the flaky FVP tests raised inhttps://github.com/apache/tvm/issues/10300 andhttps://github.com/apache/tvm/issues/10314. Now that CI is becomingmore and more parallelized, the thinking is that the python temporarydirectory implementation might be more stable than `utils.tempdir`.Removing the XFail markings off the affected tests, but keeping thework around implemented in https://github.com/apache/tvm/pull/10408while we monitor with the above change.Change-Id: Id07869b51cd2278ec4885ef964bc1b23892ba235* alter context manager to make more readableChange-Id: Iba0644db14e50648f6dc99a4ed0f455641c31912	5
[PYTHON] Support DLTensor compatible API (#136)* [PYTHON] Support DLTensor compatible API* optimize for common path	1
add onnx resize v10 and unit test (#6726)	3
[Codegen][cuda-fp16] fallback to fp32 simulation when cuda arch < sm53 (#4268)	5
Onnx opset support (#416)	1
[metal] support int64 (#7105)	1
[microTVM] Refactor pytest fixtures (#12207)* Refactor micro test fixtures* fix error* fix scope* address @guberti comments* fix help message* rename tvm_debug and added .gitignore* fix bug* fix bug	0
[BYOC-DNNL] Bug Fix (#12314)* add bias_add checker, check op's order in catched pattern* fix wrong return in legalize_pad_avg_pool* add check for pooling, ceil_mode=True has not been supported by onednn currently.* fix lint* fix test error	0
min/max support (#9918)	1
[RELAY] fix error message (#4945)	0
Pin hand landmark network to version 0.7.4. (#5813)* Versions above 0.7.4 are broken due to changes in the   quantization operations in the model, which are current   not supported by TVM.Fixes #5774.	0
[CI] Add TVM_INTEGRATION_I386_ONLY for Integration Test on i386 (#9388)* add script* address comment	1
Fix compilation errors with clang 11 (#7783)- Replace llvm::MaybeAlign::MaybeAlign() with llvm::MaybeAlign().- Use ICHECK instead of assert in hexagon_module.cc.	3
enhance shape inference. allow in complete shape (#94)	1
add coherent, length, and user bits option to Shell Config (#3593)	5
Issue8717 x86 dws conv2d schedule (#9092)* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Issue 8717 Add schedule for depthwise_conv2d_nhwc* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Formatting fixed.* Add default DepthwiseConv2D schedule in NHWC layout for arm cpu* Fixed micro model library test. Checking size reduced to 16 bytes from 2466816.* Revert "Merge branch 'update-arm-simd-intrinsic' of https://github.com/sergey-grovety/tvm into issue8717-x86-DwsConv2d-schedule"This reverts commit e927567058403bcc9e4fdc3d24828b3dcd6a661b, reversingchanges made to 0ccb5a01495d02f521eea2af9efa6a3153c4f72b.* Revert "fix test_export_model_library_format_workspace"This reverts commit 32ede712ada81242f435693403a78d98adf9afeb.fix formatmove schedule_depthwise_conv2d_nhwc to generic conv2d, add test for schedule_depthwise_conv2d_nhwcfix test_export_model_library_format_workspaceuse x86 depthwise_conv2d_nhwc schedule for arm_cpuAdd x86 schedule for depthwise_conv2d_nhwc# Conflicts:#python/tvm/relay/op/strategy/arm_cpu.py* move schedule_depthwise_conv2d_nhwc to generic conv2d, add test for schedule_depthwise_conv2d_nhwcfix formatRevert "fix test_export_model_library_format_workspace"added a missing comma* Revert wrong merge changes* empty commit to force pipeline restart* Add condition to use compute_at for generic schedule_depthwise_conv2d_nhwcCo-authored-by: Sergey Smirnov <Sergey.Smirnov@mir.dev>Co-authored-by: Alex-grovety <Alexey.Yazev@mir.dev>	1
Improve schedule load, add slice_like (#1299)	1
[ci][tvmbot] Fix authorization filtering (#12310)There was a level of unwrapping missing in the check for who is allowed to trigger re-runs causing it to always fail. This also uses a different actions API endpoint to re-run only failed GitHub jobs. This also fixes the text fixtures to match the GitHub API response, also tested live in driazati#34.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CMSIS_NN] Align CMSIS-NN in TVM to TFLu SHA (#12030)* [CMSIS_NN] Align CMSIS-NN in TVM to TFLu SHAChange-Id: I7bb3b92196ad9f1a22eee87d704545e72b79ca0b* Updated CMSIS SHA to CMSIS TOTChange-Id: I0fec18e823478da991d49aa782f58f1c2f6212ba	4
[docker] fixed ci-gpu docker environment path typo. (#7648)	2
do hint insertion after aggregation (#81)	2
[Relay][Frontend][Pytorch] Fixed ConvTranspose2D parsing (#5157)* Fixed conv transpose parsing.* small format change.* Chage test module names.* Simplified test syntax.	3
[Support] Linear Congruential Random Engine (#8642)* Add linear congruential engine.* Fix typo.* Minor fix.* Fix comments and intros.* Change to unsigned.* Minor comment fix.* Fix unsigned rand state to signed.	0
[CUTLASS] Conv2d dgrad (#10110)* add conv2d transpose nhwc cudnn test* support conv2d transpose nhwc direct offload to cudnn* add cutlass dgrad support* remove unused arg* allow target none* fix beta initiaization condition* disable dynamic dense fp16 test since it fails on cuda 11.6	0
[Hexagon]Disable hexagon gtest (#11236)* Disable hexagon gtest build	3
[ETHOSN] Cleanup of trademarks and registered trademarks (#9516)Simple cleanup of comments and documentation, mainly for Arm(R) Ethos(TM)-N NPU related code.	2
[RUNTIME] Fast path for single thread run to allow app level threading (#7454)* Fast path for single thread run to allow app level threading* add sync counter to avoid error in one of tests	3
Check in a experimental cython API (#10)	5
[TOPI-ARM] Do not alter layout if layout is NHWC (#5350)* [TOPI-ARM] Do not alter layout if layout is NHWC* Add test.	3
Add new functor	1
[BYOC, MergeComposite] Add additional check before re-using the cached match (#5552)* Add additional check before re-using the cached match in merge composite* clean up ExtractPattern calls	4
Fix the issue #1033 (#1037)* Fix the issue #1033fix the issue #1033 "converting to ‘const std::unordered_set<std::basic_string<char> >’from initializer"* Fix the issue #1033fix the issue #1033 "converting to ‘const std::unordered_set<std::basic_string >’from initializer".	5
fix handling a tuple node in op fusion (#2433)	0
[TEST] Remove script that references previously removed content. (#2596)	4
CUDA device API & VerifyGPUCode pass update (#5898)* Add kMaxRegistersPerBlock device api for cuda* Add vectorize check to verify_gpu_code* Lint fix* Cast fix	0
Fix compile warnings. (#6204)	2
Conv3D ONNX support and conv3D_ncdhw x86 schedules (#4949)* Support 3d Convolution with the ONNX frontend* add unit tests for conv3d in onnx frontendrespond to PR formatting requestsadd x86 schedules to conv3d ncdhw testfix a doc string format issuerefactor for changed upsream API* first attempt at conv3d autotuningadd default schedule for conv3d_ncdhwfill in autotvm integrationadd a fallback for invalid schedulesfix fallbackfix reduction order to get simd working correctly	1
[MetaSchedule][M4a] User-API: Tune-TE/TIR/Relay (#10079)* Add tuning scripts for tir, te & relay.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Minor fix.Nits.Add back tests.* slightly improve tune.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>	1
1) Add EQ op to the deduce_bound and add unittests for the same (#3775)2) Add EQ support in the loop partition and add test for the same3) Change typo truc to trunc	1
Clean up LowerTEPass and pass IRModule Attrs through passes (#8914)	4
[AutoScheduler] Fix the type inference for conv3d (#7475)	5
[Refactor] Rename asnumpy -> numpy in NDArray (#8083)	4
[Torch] Miscellaneous fix, enable some VM GPU tests (#6418)* fix strides conversion* enable gpu target for some vm tests* fix pooling stride None caseCo-authored-by: masa <masa@pop-os.localdomain>	0
[Relay] Support i16, f16 scalars in Relay text (#11224)While testing fp16 models for Collage discovered the Relay textformat did not support f16. While adding that cleaned up scalar handlingin general. However I left two inlined tests for 'is simple const'in place (fuse_ops.cc and memory_alloc.cc) since it's not clear whetherthey should remain specific to just {i,f}{32,64} or whether they canbe replaced with the support::IsSimpleScalar central predicate.	1
Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
[DOC] Update VTA readme files to avoid stale information (#1484)* updated readme files to avoid stale instructions* bsp generation turned off by default	2
[TEST] Use equal for equality comparison expression (#543)also improve comment and unit test	3
[TF] Support symbolic inputs of Fill (#5762)* [TF] Support symbolic inputs of Fill* Rebase and simplify. Value has been converted to constant if it istf.Constant	1
support equal and not_equal in topi (#1373)	1
GetChar() in base64.h should return int, not char (#2255)	1
[RELAY][RUNTIME] Add Relay interpreter and compiler for TVM runtime system. (#1954)	5
[TOPI][Relay] max_pool2d & avg_pool2d gradient (#3601)	5
Print llvm source by default in ROCMModuleNode::GetSource (#3662)	1
[Relay, Torch] Fix stack op axis check, support torch::stack conversion for a static list  (#6433)* fix torch::stack conversion, add dynamic stack test* add test to relay stack* add comment* add more comment* uncomment relay op tests* check for List ADT properly* improve assertionCo-authored-by: masa <masa@pop-os.localdomain>	3
Switch Windows CI to build Release instead of Debug (#6427)	0
[TensorIR][Minor] Allow Tuple/Array in TE lowering (#8916)	1
[Relay] Add a unit test for structural equality (#9745)This is CORE-135 from the forums, which suggested structural equalitywas deeply broken. But unable to repro. No harm including unit test.(attempt 3)	3
[CI][docker] Add comment (#11953)* add comment* address comments	1
[AutoScheduler] Separate shapes from DAG hash and enable schedule sharing (#7317)* [AutoScheduler] Separate shapes from DAG hash and enable schedule sharing* Update CI logs* lint* fix registry* add message; fix layout rewrite mismatch* update message* support other formats	1
Revert compile_cmd kwarg name change (#3746)* Revert compile_cmd kwarg name change* Fix binutil tests	3
Docs: pip dependencies for testing (#2728)	3
[CI] Remove Vela from ci_cpu (#12533)While the dependencies for microNPU and CMSIS-NN moved into ci_cortexm,Vela is still installed in ci_cpu. As a result, we have some of the microNPU tests outside oftest_ethosu folder failing since they use precence of Vela to decide whether to skip thetest.This change will* remove Vela from ci_cpu* remove unnecessary PATH update	5
Improve the lowering of Qnn Dense (#4213)* [QNN] Improving Dense lowering.* - Moving get_shape method to util- Finalizing the test cases and the code structure for optimized dense computation.* - Fixing cpplint.* - Addressing review comments.* - Renaming the variables correctly.* - Renaming the variables correctly.	1
[TEST][Keras] use pretrained model to avoid small error caused by random weights (#1701)	1
Add dot product support for quantized convolution. (#6445)* Add dot product support for quantized convolution.We added two new intrinsics in: topi/arm_cpu/tensor_intrin.py, namely- mmla4x4: compute a matrix multiplication between tile A(4,4) and tile  B(4,4)- mmla16x4: compute a matrix multiplication between tile A(rows,4) and tile  B(4,16)Then we used those intrinsics in two separate strategies. We added thestrategies in topi/arm_cpu/conv2d_int8.py and implemented the schedulesin topi/arm_cpu/conv2d_gemm.py. In particular:- schedule_conv2d_gemm, when accelerated, packs matrix A, compute GEMM,  and unpack the resulting matrix. This uses the mmla4x4 intrinsic- schedule_conv2d_gemm_hybrid doesn't do any packing on A and C which  are in native form.  This uses the mmla16x4 intrinsicPlease note that for the limitations of `tensorize` we need to padmatrix A in both cases (when dimensions are not multiple of the tilingshape)Change-Id: Id0d818d84ffc458c6dad7983fd350a0f3d5db395* Add back nhwc_spatial_pack strategy as defaultChange-Id: I8b1826a7ae1d742956296e8d157da19955a4942c* Fix linting through BlackChange-Id: Ic74ef5461a90bca9f4d4980a214137e384d5f923* Fix python lintingChange-Id: I5fb8a2ae4467a87bd3470f6b3753c074f9b7cc78* Addressing review commentsChange-Id: I284b1f2c121051e672f548d6c6ee2a3267854e31* Fix black linting issuesChange-Id: I1813b0226b536aedee0dce9eeeba27aa2d95518b* Fixing failing test and adding tests for dot-product compilationChange-Id: Ic040722abd5538fccb85af4de922394c939e7000* Fixing linting and review commentsChange-Id: If09e3baa514c85dc78d3c27c2ac2fa2e01773d89* Fixing black linting and address commentsChange-Id: I857b28b6f9b23307d8c1eebc509de6ad2783c756* Address review commentsChange-Id: I63d1a639d4a72abeb33148fd2868cd356ef84122	4
[Relay] Replace compile engine with TE compiler in the VM (#8501)* [VM] Add imports to new TE in VM compiler* [VM] Add comments to compile engine usages* [VM] Replace depreceated CachedFunc of compile_engine with TE_compiler* [VM] rm compiler engine compiler.cc* [VM] Replace compile engine with TECompiler in memory allocator* [VM] Add relay interface to te_compiler* [Relay] Fix linting errors* Move TEcompiler to VMCompilerContext; add global func into IRmodule when lowering in TEcompiler* add back the check* skip the check for ext func in tecompiler* skip tvm::build for external functions* trigger ci* retrigger ci* retrigger ci* remove the unnecessary loop in tecompilerCo-authored-by: YuchenJin <yuchenj@cs.washington.edu>	4
[RUNTIME][CRT] init TVMPackedFunc's name (#6044)or else src/runtime/crt/graph_runtime/graph_runtime.c TVMGraphRuntime_RunLine 639 will show messy code.Signed-off-by: windclarion <windclarion@gmail.com>	1
[Relay][Topi] Add max mode to ROI align (#7440)* ROI align with max on cpu passes* onnx test file was not running gpu testsgit status!* all passing* fix lint* lint again* lint* lint* typo* remove import* fix import* add inf, -inf to hybridscript and respond to comments* shorten code* make atol lower	1
[Test][Relay][Pass] Add test case for lambda lift (#4317)	3
[AutoTVM][Ansor] Enable random fill and CPU cache flush for AutoTVM and Ansor (#6391)* [AutoTVM][Ansor] Enable random fill and CPU cache flush for AutoTVM and Ansor* Trigger CI* add assert check of random fill function* remove duplicate tvm.get_global_func* Add check random_fill exists on remote devices* solve pylint	1
[Relay/TOPI][Op] Add shape op in Relay and TOPI (#2749)* Add shapeof op in topi* Add relay shape_of op* Add constant folding for shape_of* Allow shape op to specify dtype* Add mxnet converter for shape_array* lint* lint* Add doc	2
[microNPU] enable striping for network tests. (#11883)This commit enables the striping for network tests.Currently it requires, storage_rewrite to be run ifstriping is enabled to produce correct results.Change-Id: I12b976bb77d339771f8b5a554817d192e7c99723	4
[Object] Restore the StrMap behavior in JSON/SHash/SEqual (#5719)	5
[BYOC][MergeComposite] if root->args[i] isn't a CallNode, then Donwcast<Call> will check fail (#5623)we needn't execute L131 "call_map->Set(arg, new_arg)", because when argis CallNode and root->args[i] is not CallNode, new_arg will be a nullpointer. There is no point in caching null pointer.Signed-off-by: windclarion <windclarion@gmail.com>	1
[ONNX] Make the ONNX Importer More Static (#7429)* Construct static Ops if inputs are Constant* Expose FoldConstant as a function in addition to the pass* refactor onnx importer to do more static imports by constant foldingfix pylint* fix test regressions* fix style, two bugs* pipe freeze_params through sub_graphs when importing loops and control flow	2
[EXECUTOR] Add GraphHandle (#285)* [GRAPH] Add GraphHandle* Move to apps/graph_executor	4
[Relay] Unifier hotfix (#2437)	0
[Relay][Frontend][TFLite] transpose implementation for tflite.py (#3705)* transpose implementation for tflite.py* add TRANSPOSE to convert_map* Fix Unexpected keyword argument 'axis' in function call* add test for transpose oprator* Add the parameter 'axes' handling* add test for transpose oprator* solve conflict within CONTRIBUTORS.md* Improve the if condition for empty tuple* Add one unit test to cover empty tuple* solve conflict within CONTRIBUTORS.md	5
Update deploy.md (#223)* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdUpdate dlpack device and type enumerations.	5
[API/JIT] Enable registerable global function, introduce StackVM intepreter (#25)	1
[BUG] Fix incorrect libcuda.so found by cmake when multiple versions of CUDA exist (#1788)	1
[TIR] Added PrettyPrint of ProducerStore/ProducerRealize nodes (#9259)	1
[Relay] C++ GraphRuntimeCodegen, Deprecate Python2 (#2986)* [Relay] C++ GraphRuntimeCodegen* [Test] Deprecate Python2* [Python3] Add Py2 check* Update _pyversion.py* [Python3] Update test	3
Update tvm, fix lint due to pylint update (#423)	5
python3 compatibility	5
[BUG][ConvertLayout] Fix qnn.conv2d layout conversion too many values to unpack (#6442)This patch follows a previous bugfix in #6419. I made a very simple oversight for qnn.conv2d in that tinfos also contains qnn parameters. Therefore, we need to extract data_info and weight_info differently.Change-Id: Ib0ad01f427543371380d0bb604a77b5e0ec1103d	4
[AOT] BugFix of workspace calculation (#10337)Following an investigation from #10022,it turns out, currently the workspacecalculation assumes there would be a singlelowered PrimFunc could be produced perprimitive Relay Function.However, the exception turned out tobe the CMSIS-NN codegen that producesmultiple calls/PrimFuncs in the placeof a single call to single relay PrimFunc.This commit adds changes to workspacecalculation to be done on lowered IRModule.Additionally, changes the test utils tonot to generate any stack allocator codewhen USMP is used to make the tests morestrict.This change also removes the confusing"run_model" which has semantics identiticalto "__tvm_main__" in TIR.	5
add oneflow dependency in docker file (#9881)* add oneflow ci depend* fix comment	0
bugfix: "verify" call parameter name changed (#6382)	4
[TOPI] [Hexagon] Batch flatten slice op initial version (#11522)* [TOPI] [Hexagon] Batch flatten slice op initial version* Fix lint errors* Fix more lint errors* Fix lint warnings* Fix review comments* Update tests to use util functions* Update __init__.py* Fix review comments	0
[Relay][Quantization] Per-Channel FQ2I (#8883)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Per channel fq2i (#8)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Fix bug in requantize dimension expansion.* Format.Co-authored-by: Josh Fromm <jwfromm@octoml.ai>* respond to review commentsrespond to review commentsCo-authored-by: Josh Fromm <jwfromm@octoml.ai>	5
[Hexagon] Add random string to workspace name (#11593)	1
[BYOC][NNAPI]: Add testing package to ci_cpu image (#8088)This commit adds Android SDK to the ci_cpu image for supporting tests of Android NNAPI BYOC.	3
Fix build break in android_rpc (#8252)	4
[TEAM] New reviewer: kazum (#1417)	1
[Tests] Replace the Relay interpreter with the VM in the op tests (#11386)	3
[PASS] Layout transform pass (#233)* [PASS] Layout transform pass* Fix according to comment* Fix	0
Fix submodule URLs. (#10888)- Add `.git` suffix.- Remove incubator prefix.	0
[PY][FFI] Introduce PyNativeObject, enable runtime.String to subclass str (#5426)To make runtime.String to work as naturally as possible in the python side,we make it sub-class the python's str object. Note that however, we cannotsub-class Object at the same time due to python's type layout constraint.We introduce a PyNativeObject class to handle this kind of object sub-classingand updated the FFI to handle PyNativeObject classes.	0
add back supported tests (#10116)	3
Implementation of uTVM (#3227)* uTVM interfaces (#14)* some minor interface changes* implemented HostLowLevelDevice* added MicroDeviceAPI* implemented micro_common and added Python interfaces* current status, semi implemented micro session* added micro_common implementation and python interfaces (#18)* added micro_common implementation and python interfaces (#18)* current status, semi implemented* host test working* updated interfaces for MicroSession arguments allocation* make somewhat lint compatible* fix based on comments* added rounding macro* fix minor bug* improvements based on comments* Clean up `binutil.py` and make Python-3-compatible* Change argument allocation design* Address feedback and lint errors* Improve binutil tests* Simplify allocator (per @tqchen's suggestions)* Doc/style fixes* farts* mcgee* rodata section werks(and so does `test_runtime_micro_workspace.py`)* simple graph runtime werk* TEMP* ResNet works, yo* First round of cleanup* More cleanup* runs a dyson over the code* Another pass* Fix `make lint` issues* ready to pr... probably* final* Undo change* Fix rebase resolution* Minor fixes* Undo changes to C codegen tests* Add `obj_path` in `create_micro_lib`* TEMP* Address feedback* Add missing TODO* Partially address feedback* Fix headers* Switch to enum class for `SectionKind`* Add missing ASF header* Fix lint* Fix lint again* Fix lint* Kill lint warnings* Address feedback* Change Python interface to MicroTVMAll interaction with the device is now through `Session` objects, whichare used through Python's `with` blocks.* Reorder LowLevelDevice interface* Store shared ptr to session in all alloced objects* Move helper functions out of `tvm.micro`* Switch static char arr to vector* Improve general infra and code qualityDoes not yet address all of tqchen's feedback* Forgot a rename* Fix lint* Add ASF header* Fix lint* Partially address MarisaKirisame's feedback* Lint* Expose `MicroSession` as a node to Python* Revert to using `Session` constructor* Fix compiler error* (Maybe) fix CI error* Debugging* Remove* Quell lint* Switch to stack-based session contexts* Make uTVM less intrusive to host codegenAnd use SSA for operands of generated ternary operators* Inline UTVMArgs into UTVMTask struct* Remove `HostLowLevelDevice` header* Remove `BaseAddr` class* Address feedback* Add "utvm" prefix to global vars in runtime* Fix lint* Fix CI* Fix `test_binutil.py`* Fix submodules* Remove ResNet tests* Make `test_binutil.py` work with nose* Fix CI* I swear this actually fixes the binutil tests* lint* lint* Add fcompile-compatible cross-compile func* Add docs for uTVM runtime files* Move pointer patching into `MicroSession`* Fix lint* First attempt at unifying cross-compile APIs* Fix lint* Rename `cross_compile` back to `cc`* Address feedback* Remove commented code* Lint* Figure out failing function* Remove debugging code* Change "micro_dev" target to "micro"* Add checks in tests for whether uTVM is enabled* Add TODO for 32-bit support* Rename more "micro_dev" to "micro"* Undo renameWe already have `tvm.micro` as a namespace.  Can't have it as a methodas well.* Fix failing CIThanks to @tqchen for finding this bug.  Emitting ternary operators for`min` and `max` causes concurrency bugs in CUDA, so we're moving theternary op emissions from `CodeGenC` to `CodeGenCHost`.* Address feedback* Fix lint	0
[RELAY] Pass infra cleanup (#3336)	4
[DOC] move comments to file header (#91)	2
[CI] Fix pip cache config bug (#9933)* Update Dockerfile.ci_arm* Update Dockerfile.ci_cpu* Update Dockerfile.ci_gpu* Update Dockerfile.ci_i386* Update Dockerfile.ci_lint* Update Dockerfile.ci_qemu* Update Dockerfile.ci_wasm	2
[TVMScript] Add source_paths to Doc (#12324)This PR:- Add the source_paths attribute to Doc base class.- Add the corresponding Python binding for it.This PR is depended by multiple tasks, including the diagnostic output in DocPrinter, VarTable and IRDocisifer.Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>	5
[Relay][Op] MetaSchedule layout in TypeRel (#11819)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
promote C API lib to root, pass basic_test	3
Improve op_type missing message (#7384)	1
[Node] fix typos in include/tvm/node/functor.h	2
[ONNX] QLinearConv Support (#8007)* Add QLinearConv for onnx frontend* Reformat* Squeeze 1D tensor for weight_scale & weight_zero_point* Doing dequatize -> quantize if y_scale is not constant	1
[CI] Set default value for CI_NUM_EXECUTORS (#10642)* [CI] Set default value for CI_NUM_EXECUTORS`task_cpp_unittest.sh` relies on `CI_NUM_EXECUTORS` being set as anenvironment variable, which causes the script to fail if the value isnot set. To prevent this, setting the default as 1.Change-Id: Ie543998bc97c60cb2fb76a92831acc830e2805f5* move CI_NUM_EXECUTORS logic into task_build.pyChange-Id: I0c1a85032a688058c90d15964c0accf2b2510c36	4
[FoldScaleAxis] Support dense and bias_add op in fold scale axis (#9838)* [FoldScaleAxis] Support dense and bias_add op in fold scale axis* fix lint	0
[TVMC] Support compiling and running with VM  (#10722)* introduce vm compile path* support vm in tvmc* cleanup + lint* add profiler + simplify vm case in tvmcpackage* address comments + parametrize testsCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
[COMMUNITY] @sxjscience -> Reviewer (#2807)	3
hardware compilation flow, and driver tests	3
change install version of gpu to cpu (#9922)	4
[Runtime]Considering DLTensor's byte_offset in ZeroCopy function (#11340)	1
hotfix the ci (#4199)	0
Change the all #pragma once to ifdef include guard (#7264)	4
AlterOpLayout with tvm.target (#463)* AlterOpLayout with tvm.target* fix test	3
Update faq.md (#4893)various minor editorial updates - style, grammar, typos.	2
add onnx reverse sequence op (#7771)Co-authored-by: xp224797 <xp224797@alibaba-inc.com>	1
[CI] Golang unit test trigger for Jenkins (#2266)	3
Add op_name in error message for Pool (#7243)* add op_name in error message for Pool* fix tiny issue for arguments* fix tiny issue for LpPoolCo-authored-by: luyaor <luyaor@luyaordeMacBook-Pro.local>	0
[Bugfix, CuDNN] fix segfault when cudnnDestroy called with destroyed cuda context (#8267)* fix: cudnnDestroy called after cuda context is over* refact: rename global var with `g_`* clang-format* refact: let cudnn handlers leak	0
add range to plan memory (#147)	1
[VTA] [TSIM] Improve tsim example (#3206)	1
Retrigger build. (#6304)	5
fix bug in dense_nopack if dynamic input shape (#8166)	0
Pin xgboost dependency version to 0.90 (#4965)* Sets xgboost dependency to be 0.90, preventing   segfaults during TVM python unit tests execution * This is discussed in issue #4953	0
[Relay] Add Let list, a helper datastructure to relay (#1827)	5
[FIX] Add braces to if-else statements (#11493)Some if-else statements were missing braces. They have been added as perour style guide.	1
Make topi cuda nms_gpu method signature similar to non_max_suppression (#2780)	1
[DOC] VTA installation & basic tutorials (#47)	2
[CRT]Compilation warnings fixed for 32bit and 64bit compilation (#5349)	0
fix relay.build to not change the module argument in place (#5822)	4
[CI] Move gpu docker binary to cuda10 (#4229)* [CI] Move gpu docker binary to cuda10* Fix the gcn tutorial	0
1) fixed a functional bug in loop partitioning algorithm that is exposed when double splitting with indivisible factors 2) added a testcase (#2956)	3
fix opengl runtime	1
[Frontend][TFLite] ADD_N operator  (#5474)	1
Add conda package (#231)* Make sure to install the nnvm_compiler library.* Add conda packge.	1
[RUST][CI] Add rust frontend tests in Jenkins (#2375)	3
Reverse shape dims of weight type (#2155)	5
add `multiply` and remove `subtract` for dnnl json runtime (#9120)	1
Dynamic Batch Support for TRT  (#6955)* add_annotate_fn* Reshape_ann_fn* Prune Subgraph* Dynamic Shape* Make PT Mask RCNN Work* Cleanup* Remove comments* Remove COmments* GetBatchSizeFix* Fix Remove Droupout* Fix Remove Droupout* TRT Runtime* Add MaskrCNN R50* New Testing code* Fix black* Test Maskrcnn r50 done* Test MR50* Space typo* Change Log to Dlog* Move test to tensorrt.py* Remove imports* Remove function* Add it to trt* import error* Imports* Add torch to CI* trt_test* Check test* Revert Pytorch install* Fix* test dynamic batch* TRT* Resolve PR comments* Zero batch size addCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>	1
[TIR][ANALYSIS] Refine side effect analysis. (#5954)	5
[NNPACK] temporary disable nnpack test (#2115)	3
1) fixed constant folding for mod operation in CanonicalSimplify 2) added a unit test (#2487)	3
[TUTORIAL] Move mobilenet to tutorial, fix precompute_prune (#35)* [TUTORIAL] Move mobilenet to tutorial, fix precompute_prune* Some language improvements	1
[BUILD] Disable utvm standalone runtime by default (#4240)	1
[Pylint] Pylint integration_tests folder (#11672)* add folder to pylint* add init py* lint test_arm_mrpofile_dsp.py* one more change to tests/python/integratoin/test_arm_mprofile_dsp.py* add test_dot* test_ewise_fpga.py* test_ewise.py* test gemm* test_lower.py* test_meta_schedule_auto_tensorize.py* test_reduce.py pt1* test_reduce.py pt2* test_scan.py* test_tuning.py* test_winograd_nnpack.py* final test pass* comments* clean up test_lower more	3
[AOT] Return module list from AotExecutorFactory (#11191)* [AOT] Return module list from AotExecutorFactoryAdd a function "module_list" to AotExecutorFactory that returnsArray<String> containing names of all modules. At the moment therewill be only one entry in that list, most commonly "default".* Address review comments- Change assert in unit test to a simpler one.- Add custom name to the generated module in unit test.- Rename "module_list" to "list_module_names".* Remove obsolete comment	4
add msvc in cc (#531)	1
[VTA] [Chisel] Bug fix for VME Shell (#3737)* fix* fixes	0
[Frontend][TENSORFLOW] Add support for unpack with dim 0 after tensorlist stack (#8558)* enable testcase when tensorlist stack follows by a unpack for dim 0* address reviews and improve the docstring	2
[TOPI] support dilated conv2d and depthwise_conv2d (#1129)* support dilation in conv2d and depthwise_conv2d* handle dilated conv in extern libs (cudnn, miopen)	0
support aten::type_as in the pytorch frontend (#5787)* support aten::type_as in the pytorch frontend* use _convert_data_type to convert torch type to tvm type and add more types in the type_as test	3
Update ci_build.sh	5
[microTVM] Refactor `platform` used as board name in microTVM (#8940)	1
[RUNTIME] Minimum graph runtime (#484)* [RUNTIME] Minimum graph runtime* update docs	2
[COMMUNITY] David Riazati -> Reviewer (#10812)	3
[SUBMODULE] upgrade dmlc-core (#461)	5
just a typo fixed (#10442)* minor typo fixed* to trigger CI* to trigger CI* fixed formatting issues* black formatted file	2
[TIR][Schedule] simpilfy compute_at static bound (#10307)	5
[torch] Use try_infer_value for clamp min/max (#7712)	5
[TVMC] Add `--config` argument for config files (#11012)* [TVMC] Add `--config` argument for config filesCollecting common configurations for users of TVM and exposing them gracefully in tvmc using a `--config` optionas defined in https://github.com/apache/tvm-rfcs/blob/main/rfcs/0030-tvmc-comand-line-configuration-files.mdCo-authored-by: Shai Maor <shai.maor@arm.com>* Add correct test guardsCo-authored-by: Shai Maor <shai.maor@arm.com>	3
[runtime] Improved log information with function signature (#10326)This PR introduces a function signature printer in the `TypedPackedFunc` part, so that the log information in `detail::unpack_call` will be more complete. This PR allows users to obatin the original function signature when the `detail::unpack_call` fails.	0
[Relay][Frontend][Onnx] Refactor where importer to support dynamic shapes. (#7394)* Refactor where importer to support dynamic shapes.* Add a test for dynamic where.	3
[Relay] compute & schedule for relu, softmax (#2127)	5
[Relay] PlanDevices supports 'free' on_device annotations (#9693)* [Relay] PlanDevices supports 'free' on_device annotationsThis is in support of #9613, which allows PlanDevices to be runafter lowering so as to flow memory constraints in andout of PrimFuncs. That requires a way to insert device_copieswhen the memory scopes chosen during separate lowering of fusedprimitive functions clashes, but otherwise avoid device_copies whenscopes can be chosen so as to avoid them.We support that by generalizing the "on_device" annotation toallow the device constraint to be independently controlled forits 'body' and 'result'.# Standard user annotation: body is constrained to Son_device(body, S)# Used by PlanDevices to 'fix' expression to S# (was is_fixed=True)on_device(body, S, constrain_result=True)# Used by PlanDevices to indicate a device_copy can be# inserted if necessary.on_device(body, S, constrain_body=False)# Supported, but currently has no use.on_device(body, S, constrain_result=True, constrain_body=False)A few extra odd's 'n ends collected along the way: - Some CallLowered cleanup which I found useful. - The usual extra debugging output needed as I debugged.   In return I removed some particularly verbose logging I'd   added while tracking down unexpected object copies. - Cleanup warnings from clang-12 as I touch files.* [checkpoint] unused var	1
[REFACTOR][IR] kExternalSymbol -> kGlobalSymbol (#5211)* expose runtime::String to Python* kExternalSymbol -> kGlobalSymbol	1
[ONNX] [Relay] Resize Opset 13 (#9265)* Fix handling of optional inputs.* Missed one test in the ignore list.* split 11 and 13* removed comments, adjusted for git reviewCo-authored-by: Josh Fromm <jwfromm@uw.edu>Co-authored-by: Matthew <mbrookhart@octoml.ai>Co-authored-by: CircleSpin <jocelyn@pop-os.localdomain>	5
Add ability to have multiple copies of same input to onnx_inputs. (#5389)	1
prevent aggressive unrolling in vthread (#983)	5
[RUST] Remove empty ty.rs (#2958)	4
fix doc (#510)	2
add tvm.select (#148)	1
[Torch] Add support for max_pool1d (#5142)* [Torch] Add support for max_pool1d* add test* fix line-too-long* remove wrapper class	4
[Frontend][TFLite] Added broadcasting to prelu alpha. (#10435)* Update prelu test cases* Add broadcasting to prelu alpha	1
add amalgamation support (#91)* add amalgamation* update target name	1
[microNPU] Adding a option to enable striping (#11263)This commit adds a cascader option to enablestriping explicitly.When doing so fixed a bug that is associatedwith block config selection, that will betriggered when striping is disabled.Co-authored-by: Elen Kalda <elen.kalda@arm.com>	5
Add metadata information to the listing of PassContext configuration listing function (#8226)* Rename PassContext::ListConfigNames() to PassContext::ListConfigs() and its   Python counterpart tvm.ir.transform.PassContext.list_config_names -> list_configs() * Adjust PassContext::ListConfigs() to include also metadata (currently only including the data type) * Adjust unit tests	3
[microNPU] Calculate memory pressure for microNPU external functions (#11209)* [microNPU] Calculate memory pressure for microNPU external functionsDuring the microNPU compilation stage, the "used_memory" annotations onexternal microNPU functions are read to determine a memory pressurevalue. This value is passed to the cascader to better approximate thememory available for the optimization.Change-Id: I11a311b0005e785637014cb451f4aed96edcda26* fix get size from memory regionChange-Id: I41acfc83f05b2204075edb99f86a0eecaba00f71* add test case for full offloadChange-Id: If3e672d402ab237fa82e34761bb972d2e9483ba9	4
[TensorIR][M2a] Decompose-Reduction (#9041)This PR is part of the TensorIR upstreaming effort (#7527),which adds the `decompose-reduction` scheduling primitive.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[DOC/LICENSE] Make doc and license consistent, opensource repo when we get approval (#134)	1
[Metal] Fix run metal model when non first device is selected (#8261)In case when we select non first Metal device, we got problem instream, due to we used wrong device_id in CopyDataFromTo.	5
[CODEGEN] Skip unrolled hint, export symbol on win32 (#547)	5
[Relay][Params] Add APIs for storing and retrieving parameters from individual functions. (#4194)* Add support for attaching params* Fix types* Fix test	3
Minor doc fixes (#1458)	0
[ONNX]MaxRoiPool, Mod & Xor op support added (#5729)	1
[AutoScheduler] Python based measure callbacks (#7143)* add* make it work* format* add poilcy* comment* move test* format* fix ci* Delete useless old codeCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>	1
[Relay][PatternLang] Bug fix of rewrite func attr (#7358)When using pattern with attr of functions, such attrsmostly does not exist for op node. Therefore, hasattrcheck has to be done for op nodes.Change-Id: Ia313ab34be95ccc793c32fd8e5e5ef566b78685b	4
fix typo (#3611)	2
[COMMUNITY] Xiyou Zhou -> Committer (#11206)Please join us to welcome @zxybazh as a new committer to TVM. The contributor has contributed to Meta-schedule a lot.- [Commits History](https://github.com/apache/tvm/commits?author=zxybazh)- [Code Review](https://github.com/apache/tvm/pulls?q=reviewed-by%3Azxybazh+)- [Community Forum Summary](https://discuss.tvm.apache.org/u/zxybazh/summary)	1
enhance pragma to support single point copy  (#863)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op* repare address offset for different kinds of dtype* bc* aaa* aaaaa* repare address for different dtypes* remove nonsense files* add whitespace of line 581* use base alloc elem_type* enhance the testcast of basic buffer is 64bits,32bits,16bits,8bits* use extends[0]->type() as dtype of offset* clear program writes* enhance inject_copy_intin to support of pragma stmt with no loops* fix cpplint errors* fix cpplint error of !* enhance detectLinearEquation to support with no loop vars* fix cpplint errors	0
change Hexagon docker version (#10981)	2
[RUNTIME][CONTRIB] CoreML Runtime (#5283)* [RUNTIME][CONTRIB] CoreML Runtime* fix lint* fix CI* use xcrun to compile coreml model	1
[CI] Move golang tests to the end (#4164)	3
[VTA] [APPS] [TSIM] small naming fix  (#3293)* make off lowercase* update README	5
[MetaSchedule] Include te/tensor.h instead of forward declaring te::Tensor (#11731)ApplyHistoryBestNode declares an Array of Tensor. There are type traitsused in Array that require that the element type is complete at the timeof the declaration. With only a forward declaration compilation fails(clang 14.0.3, libc++).	0
[ETHOSN] Roll CI forward to Ethos(TM)-N release 21.11 (#11099)The updated code is already in the repo; this merely switches the CI over.	5
[RELAY][PASS] detect depthwise conv2d in mac_count pass (#3083)* check in* use groups* CHECK_EQ* trigger CI* Update mac_count.cc* trigger CI* trigger CI	5
[ARITH] Add floordiv for the deduce bound (#4025)Use fdiv in the tests for the deduce_bound	3
[intrin]support fmod for cuda (#1964)	1
[TEAM] Add alex-weaver as reviewer (#1377)	1
[TVMScript] Printer IRDocsifier (#12396)This PR:- Adds IRDocsifierThis PR is in draft state because it's branched off from a pending PR #12336Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>	5
[CUDA] Fix dense tensorcore legalize type error when units is specified (#9030)* Fix dense tensorcore legalize type error when units is specified* revert black change due to different version from CI	4
[ARITH] More robust int set checking (#487)	1
[Relay] Add shape check for ConcatenateRel and StackRel (#3699)* [Relay] add shape check for concat* [Relay] add shape check for stack* add test case for shape mismatch* [typo] add the missing assert* fix lint errors.* replace int with size_t.* statically cast param->axis to size_t.* switch to run_infer_type.* fix checking for negative index* add static_cast for param->axis* merge to latest tvm* fix lint error* Fix an error with negative index.* Update transform.h* Update transform.cc	5
[RELAY][OP] Add relay minimum op (#1840)	5
[Relay][Frontend][TFLite] frontend operator support: batch_to_space_nd, space_to_batch_nd (#3850)* Fix unittest* Fix pylint error: Line 915 too long* Fix the conflicting files* frontend operator support: space_to_batch_nd* add test case for frontend operator support: space_to_batch_nd* add test case for frontend operator support: space_to_batch_nd* frontend operator support: space_to_batch_nd* Fix ValueError: don't know how to convert type <class 'numpy.ndarray'> to node	0
[TIR][TVMScript] Update printer / parser to make T.allocate return buffer var (#12412)* Updated TVMScript syntax of `T.allocate` to return buffer var.* Added syntax sugar for `T.decl_buffer`. When `data` field is not  specified, `data` will be implicitly created via `Allocate` stmt.  * Updated the existing test cases. Most test cases can be updated by  changing `T.allocate` to `T.decl_buffer`. `T.allocate` in some tests  are updated to `T.allocate` + `T.buffer_decl`, to maintain the  legacy behavior of allocation and implicit buffer declaration (will  be followed up in future PR to adopt `T.decl_buffer`).	5
[Relay] Make check stricter: disallow inserting function with free vars into module. (#6313)* savelintlintfix testfix test* fix	0
[ROCM] View llvm ir and gcn asm with module.get_source(...) (#590)* view llvm ir and gcn asm with module.get_source(...)* fix lint	0
Fix TFLite 2.9 tests (#12130)This pr fixes the tests that will be broken when we will update TFLite tothe 2.9 version.We will update TensorFlow and TFLite versions to 2.9 so that we canbenefit from improvements in packaging to support multiple platformsand Operating Systems.	5
[TOPI] Add transpose_a/b & dynamic shape support for batch matmul (#8527)* Add basic support for batch matmul transpose* Update* Lint fix & add tf convert support* UpdateLint fix* Bug fix for qnn.batch_matmul* Bug fix for tensorflow test* Add grad support for batch_matmul* Lint fixRe-triggle CIBug fixRe-triggle CIRe-triggle CIRe-triggle CI	0
[TensorIR][Pass][M1c] FlattenBuffer (#7962)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	1
modified schedule_dataflow_rewrite.cc to fix Stale Tensor during Dataflow Rewrite #738 (#747)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op	1
[REFACTOR/PASS] Formalize argument bind and match util (#214)* [REFACTOR/PASS] Formalize argument bind and match util* grammar	4
Fix shape func for Reshape (#10721)	0
[CI] Add autodocsum as dep (#4902)	2
[TensorIR][M2a] Reduction Factoring (RFactor) (#8544)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[DOCS] Sphinx -- Introduce alias detection. (#4954)* [DOCS] Sphinx -- Introduce alias detection.Background: some of our namespaces import function from anothernamespace. For example tvm.te imports most of the operators from tvm.tir.Previously we manually exclude these aliases from the doc.However that means we can not link them by the alias name.This PR adds a sphinx callback plugin to detect such aliases, and create a rubric blockon the button of its current docstring `Alias of the original class`.It is done in a way so that we can refer to the generated docs.We also fixed a few docs errors.* Fix most of the issues	0
[TFLite] Cast operator adapted for MLIR-based convertor (#7639)* [TFLite] Cast operator adapted for MLIR-based convertorCast operator now can be executed in MLIR-based version.Unit test updatedChange-Id: I30e5c1c9d69355116b560af8f6d0582b2d593538* Comment addedChange-Id: I3e2d29ef201283de337168d0b82679b63ca2fcf4	4
[TIR] Make conversion from Integer to int64_t explicit (#12010)* [TIR] Make conversion from Integer to int64_t explicit* Fix compilation errors* Fix compilation issues in cpptest* Fix SPIRV compilation errors	0
[CI][arm] Fix tensorflow-aarch64 repository URL (#11829)Update the custom repository URL used to pull TensorFlow-aarch64.The mechanism of installation is also changed to a file based, as atemporary workaround before we update to a newer version that canbe pulled from the official PyPI repository.Change-Id: Ic55abc9a9cd373c1db6b0322e7323dffbf2c12c8	5
add git diff filter (#6484)	1
[Relay][Topi] Disable conv NHWC pack int8. (#4038)	5
Allow override gtest library search path (#1867)	3
[TOPI] Use cblas for dense and batch_matmul when "cblas" is in the target libraries (#3787)* Support cblas library in dense* start to add support for generic batch_matmul compute* Add x86 override for batch_matmul* Fix linting* reset file* Fix typos* dummy change to re-trigger CI	4
[Team] Jared Roesch -> PPMC (#4488)	5
[TIR] Enhance VerifyGPUCode (#6194)	5
[Frontend][ONNX] Update softmax calculation method when dimension > 2 (#11123)* update Softmax with uniform operator* add testcases for softmax	3
[ROCm][TVMC] Add ROCm to the TVMC driver (#8896)* Add ROCm to list of RPC clients.* Add ROCm to list of TVMC devices.* Enable ROCm by adding session call.	1
Add nix to gitignore (#3418)	1
[Lang] Layout in TVM node system (#2509)* move layout.h & layout.cc from relay to tvm* change ConvertLayout in relay to bijectiveLayout->Forward/backward* add first test case* add LayoutAxis* add LayoutAxis struct and compiles* simplify BijectiveLayout rule consturct* polish func name for Layout, move impl to .cc, remove Layout::defined(), add defined() checker* partially add layout py support* add layout test cases* add doc for tvm.layout & tvm.bijective_layout* fix lint* fix lint* fix layout name generation bug* fix layout typo* address comments and add topi.layout_transform* layout.h->data_layout.h, test_lang_layout.py->test_lang_data_layout.py	5
[Relay][QNN] Add unit test for int8 (#4159)* [bugfix][codegen] fix casting bug in llvm codegen* update example* retrigger ci* check llvm version	5
Fix trt Test (#7016)* Fix trt Test* Fixed stuff* Done* fix 0* Trigger BuildCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>	0
[TIR][BUILD] Remove buffer params from pass config. (#5652)Buffer configurations can be passed during constructionand does not need to be part of the build config.This is a refactor step to simplify the BuildConfig for the PassContext migration.	4
[NNVM][FRONTEND][Keras] Support for reusing layers (#1192)	1
[TOPI, CUDA] Improve conv2d_transpose schedule template (#3796)	1
[PROFILER] Add configuration information to profiler (#11530)Configuration is a place to store extra information related to thespecific profiler run. Right now it is just the executor used and thenumber of threads. The roofline analysis also adds peak flops and peakbandwidth.	1
[BugFix][TIR] Fix rfactor when RF block becomes spatial (#11031)Should fix #10899	0
[PyLint] Minor updates to pass pylint locally. (#8424)With either the ci_lint docker image, or the matched version ofpylint==2.4.4, I got two lint errors running locally that didn't showup in the CI.  Fixing them.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Relay][Frontend] Fix typo names in frontend (#3685)Fix typo names in caffe2 and onnx frontend:* sotrage_order -> storage_order* OpNotInplemented -> OpNotImplemented	2
[Relay] Refactor inline composites transformation (#10995)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
[docs] Add lightweight docs image (#11045)* [docs] Add lightweight docs imageThis image includes everything necessary to build the docs without any tutorials and is just about 1.5 GB which is significantly less than the CPU/GPU images.* remove ci.py docs --cpu flag, imply it via a lack of --tutorials/--full so it is the defaultCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[ci] Move pip dependencies to docker images, add ninja / shellcheck (#10257)Following on from #10246, this moves the `pip install`-at-runtime deps to the docker image install so they are baked in.	2
[BUG] Shape Func of Split Op Error (#8887)* [BUG] Shape Func of Split Op Error* Convert Tab to Space* Add Test Case	3
small fixes on docs (#769)* small fixs on docs* add IR output after parallelization	1
Add dockerfiles for the conda package builds (#3344)* First shot* Add dockerfile for CPU too* Finish the build infrastructure* Remove extra file* Comment out the Jenkinsfile section since it is not ready* Add missing license headers* Update to newer cudnn that anaconda packaged* Bump the build numbers for the newer cudnn* Bring back the toolchain option with a tweak for cuda* Cache some large packages in the docker and update to llvm 7.0.0* Merge all the python packages together* First fix for the conda cuda builds (again)* Use the tarball version of cudnn since tvm has trouble detecting the other one* Use llvm 8.0 from the numba packages* Also use llvm 8.0 for the cpu builds* Don't use the anaconda compiler for OS X* Enable Metal on OS X builds* Make sure to detect undefined variables in scripts* Fix build when not using cuda	1
[TVMScript] IRBuilder methods for `IRModule` (#12694)* IRBuilder methods for `IRModule`This PR introduces IRBuilder methods for `IRModule`.Co-authored-by: yongwww <yongcale@gmail.com>* apply code review suggestionCo-authored-by: yongwww <yongcale@gmail.com>	5
Save PyTorch frontend state in object (#7023)While the functional approach is pretty neat, we ended up havingglobal state (default frontend, dtype) and it'll be more soon(caching of inferred types, see #6900). To not have to pass aroundthe state, this moves the op conversion into a class with instanceshaving the state.	4
[Hexagon] Add missing #include <iterator> (#9968)This fixes compilation error with libstdc++.	0
Reshape with dynamic shape arg (#6208)Reshape operation updated to take shape from second operand.In case if shape is provided using second operand itcan be a tensor now.	1
Update README.md	2
[TIR][Schedule] Support for specific consumer block targeting in cache_read (#12505)* Add optional consumer blocks to cache_read.* remove comments* Fully functional* Add test for consumer targetting.* Formatting.* Add missing parameter comment.* Fix comments* Simplify type of consumer_blocks in python.* Change how consumer_blocks is printed in python.	4
[Pylint] Making hexagon tests pylint compliant Part 2 of N (#12176)Second set of **hexagon tests** modified to be pylint compliant as part of #11414 tracking issue. The files supported in this patch are:* [X] tests/python/contrib/test_hexagon/test_autotvm.py* [X] tests/python/contrib/test_hexagon/test_cache_read_write.py* [X] tests/python/contrib/test_hexagon/test_launcher.py* [X] tests/python/contrib/test_hexagon/test_maxpool2d_blocked.py* [X] tests/python/contrib/test_hexagon/test_models.py* [X] tests/python/contrib/test_hexagon/test_run_unit_tests.py* [X] tests/python/contrib/test_hexagon/test_thread_pool.py* [X] tests/python/contrib/test_hexagon/test_usmp.py	3
[Frontend][TFLite] Implement fake quant (#8780)* [Frontend][TFLite] Implement fake quant* remove unused variable* fix linting errors* add more tests* use pytest parametrize instead of a separate function	1
[Relay] Add Elemwise operator Sub, Divide, Power, Max, Min to tflite frontend. (#3357)	1
Use new SBT Debian Repo before bintray is shutdown (#7926)* Use new SBT Debian Repo before bintray is shutdownWe started seeing issues with building the TVM Docker images, and they stemmed from the SBT (Scala Build Tool) installation which was using bintray instead of a later SBT Debian URL. Thankfully bintray were just running some brown outs before they turn the service off on May 1st:https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/So I took the Scala repo URL from here:https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Linux.htmlWhich is the suggested SBT URL as they may change the backend again in future:https://github.com/sbt/sbt/issues/6294* Future-proof sbt repo setup	1
[RUNTIME] Refactor object python FFI to new protocol. (#4128)* [RUNTIME] Refactor object python FFI to new protocol.This is a pre-req to bring the Node system under object protocol.Most of the code reflects the current code in the Node system.- Use new instead of init so subclass can define their own constructors- Allow register via name, besides type idnex- Introduce necessary runtime C API functions- Refactored Tensor and Datatype to directly use constructor.* address review comments	1
[Auto-Schedule][Fix] Fix hang while tune model through rpc (#9032)* [Auto-Schedule][Fix] Fix hang while tune model through rpc* Fix problem with hang by using deep copy* Fix with local args* Update python/tvm/auto_scheduler/measure.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>	5
[ci] Disable dependabot PRs (#11072)A bunch of these just got created (e.g. https://github.com/apache/tvm/pull/11070) and are clogging up CI with 2x normal number of builds since they push to a branch and make a PR.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Frontend] Add ops in mxnet converter (#2844)* Add ops in mxnet converter* trigger ci	1
[Bugfix] Fix reshape (#5739)* Fix reshape* fix doc warning* fix ci* address comments	1
Fix cmake search for vulkan on Windows (#1343)	1
[FRONTEND][TENSORFLOW] Fix gather_nd indices (#5279)* [FRONTEND][TENSORFLOW] Fix gather_nd indices* retrigger CI	0
Fix missing header inclusion. (#7097)	0
[Hybrid Script] allow const_range allocation; allow const_range lazy compilation (#2423)	1
remove async tst (#10160)	4
[TOPI] Fix CUDA Library Tuning (#6132)	0
[NODE][REFACTOR] Refactor reflection system in node. (#4189)* [NODE][REFACTOR] Refactor reflection system in node.- Removed the old Node, Node is now just an alias of runtime::Object- Introduce ReflectionVTable, a new columnar dispatcher to support reflection  - This allows us to remove vtable from most node objects  - The VisitAttrs are registered via TVM_RESGITER_NODE_TYPE,    they are no longer virtual.- Consolidated serialization and reflection features into node.* Explicit type qualification when calling destructor.* Fix SPIRV, more comments	0
[AutoScheduler] Fix a bug in thread binding (#6683)* fix for lstm use case* update	5
Fix typo in module.py line 90 (#1947)	2
Switch from CompileEngine to TECompiler in Interpreter (#8486)This continues on:https://discuss.tvm.apache.org/t/rfc-relay-tecompiler-rewrite-existing-compile-engine-to-match-updated-compiler-flow/9233and #751, this time just replacing CompileEngine with TECompiler in the Interpreter,using the JIT helper added to help the transition.Some whitespace improvements while there.	1
[TOPI] Fix reduction (#6250)	0
[TOPI][Hexagon] Implement Argmax Slice Op (#11847)* [TOPI][Hexagon] Implement Argmax Slice Op* run through black* Address initial review comments* Fix variable names in tests* Fix lint issueCo-authored-by: arangasa (generated by with_the_same_user script) <arangasa@hu-arangasa-hyd.qualcomm.com>	1
Fix a case of linking to wrong OpenCL library (#11215)if explicit path to OpenCL SDK was pointed in USE_OPENCL option	1
[UnitTests] Apply correct requires_gpu() pytest marks for parametrized target (#8542)* [Onnx][UnitTests] Excluded additional onnx tests- The onnx tests `test_basic_convinteger`, `test_convinteger_with_padding`, `test_range_float_type_positive_delta_expanded`, and `test_range_int32_type_positive_delta_expanded` don't run correctly on CUDA targets, so they are added to the exclusion.- Parametrized over the relative directory name, rather than the full directory name.  This improves readability of the pytest output, and keeps the same parametrized test name across different python version.- Changed the target-specific skips to check the target kind, rather than the full target string.* [UnitTests] Apply correct requires_gpu() pytest marks for parametrized targetPrevoiusly, the addition of tvm.testing._target_to_requirement pytest markswas handled by the parametrize_targets function.  The_auto_parametrize_target function assumed that a unit test that was alreadyparametrized had all markings needed.  If a unit test was explicitlyparametrized using @pytest.mark.parametrize, these marks would be missing.In most cases, this explicit use of @pytest.mark.parametrize('target', ...)should be avoided, but has value in the case of marking with multipleparameters with @pytest.mark.parametrize('target,other', ...).  This usecase isn't yet supported by the tvm.testing.parameters function.  Therefore,if this occurs, detect it and add the appropriate marks.* [UnitTest] Bugfix, applying requires_* markers to parametrized targets.Initial implementation did work correctly with@tvm.testing.parametrize_targets.Also, went through all cases where "target" is used to parametrize onsomething other than a target string, and renamed.* [Onnx] Switched from using pytest.skip to tvm.testing.known_failing_targetsAfter merging of the `tvm.testing.parametrize_targets` and`tvm.testing._auto_parametrize_target` code paths,`known_failing_targets` can be used in both cases.* [Testing] Enable `Target` object as argument to _target_to_requirementPreviously, tvm.testing._target_to_requirement required the argumentto be a string.  This commit allows it to be either a string or a`tvm.target.Target`.* [Testing] Auto-target parametrization, handle pytest ParameterSetIf the unit test has already been parametrized with pytest.params toadd parameter-specific marks, respect those existing marks.This can happen in some cases in the CI, uncertain yet what is causingthem.  Maybe pytest-xdist related, but there's some difficulty inreproducing it locally.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[PatternLang] Add ConstantPattern (#5689)* Add ConstantPattern* update doc	2
removing deprecated script (#3667)	4
schedule over operation	5
[COMMUNITY] @vinx13 -> committer (#3100)	3
[Relay] Register abs gradient: grad * (select(x < 0, -1, 1)) (#3447)	5
[BYOC][bugfix] Handle empty tuples in annotation pass (#7288)	4
fixed some typos (#3112)	2
[CI][1/2] Update the Python version of pyxir (#10406)Currently the CMake file for pyxir is looking for things in Python3.6,so it needs to be upgraded to use 3.7 now that we have moved to use 3.7.Otherwise the build fails when the docker images are updated since the3.6 can't find the pyxir packages which have moved to 3.7.Additionally, there seems to be a problem with the newer version ofsetuptools installing the pyxir libraries, so reverting these versionsto the previous versions as a workaraound.Note that this has to be done in two patches for the changes to gothrough the current CI, this patch downgrades the pip and setuptoolsversions.	1
Fix TorchScript fallback build (#10556)This was missing a header `libtorch_runtime.h`. The test in `test_libtorch_ops.py` is also currently being skipped in CI since `torch` isn't available but that's left for a follow upcc @t-vi @masahicommit-id:f8998762Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Training] Make AutoDiff thread through global function. (#6336)* save* lint* lint* fix warning* fix test* save	3
Update SGX example (#1933)	5
[GraphRuntime] Debug graph runtime (#3232)	1
[AutoScheduler] Enable schedule sharing in dispatch context (#7344)* [AutoScheduler] Enable schedule sharing in dispatch context* Update python/tvm/auto_scheduler/dispatcher.py	5
Changed topi cc resize to python implementation with new features. (#3788)	1
Rename axis parameter in onnx squeeze (#1683)* Rename axis parameter in onnx squeeze* Add test	3
[relay][frontend] TensorFlow saved model support (#2586)* [relay][frontend] TensorFlow saved model support* Add Examples section* keep one copy of tensorflow_parser in relay	1
[Frontend][TFLite] Densify Op added (#7048)* [Frontend][TFLite] Densify Op added* [1] Review comments handled* TODO added for sparse_to_dense Op usage* stale comments removed	4
Implement Keras Conv1D (#7035)	5
Tensor API	5
[CMPL] Add Support for Other Data Types (#252)* [CMPL] Add Support for Other Data Types* [CMPL] Add test* [CMPL] Fix	0
Rust Refactor Stage 4: Rewrite Rust graph runtime to use new APIs (#5830)* Port graph-runtime to new API* --amend* Fix file lint* Remove old travis file* Add @kazum's patch* Update rust/tvm-sys/src/datatype.rsCo-authored-by: Andrew <amcharg@gmail.com>Co-authored-by: Andrew <amcharg@gmail.com>	5
GitHub Action lint Python code for syntax errors (#4688)* GitHub Action lint Python code for syntax errorshttps://flake8.pycqa.org/en/latest/user/error-codes.htmlOn the flake8 test selection, this PR does _not_ focus on "_style violations_" (the majority of flake8 error codes that [__psf/black__](https://github.com/psf/black) can autocorrect).  Instead these tests are focus on runtime safety and correctness:* E9 tests are about Python syntax errors usually raised because flake8 can not build an Abstract Syntax Tree (AST).  Often these issues are a sign of unused code or code that has not been ported to Python 3.  These would be compile-time errors in a compiled language but in a dynamic language like Python they result in the script halting/crashing on the user.* F63 tests are usually about the confusion between identity and equality in Python.  Use ==/!= to compare str, bytes, and int literals is the classic case.  These are areas where __a == b__ is True but __a is b__ is False (or vice versa).  Python >= 3.8 will raise SyntaxWarnings on these instances.* F7 tests logic errors and syntax errors in type hints* F82 tests are almost always _undefined names_ which are usually a sign of a typo, missing imports, or code that has not been ported to Python 3.  These also would be compile-time errors in a compiled language but in Python a __NameError__ is raised which will halt/crash the script on the user.* Force a retest* Rename start_rpc_server_to_tracker.py to start_rpc_server_to_tracker.shThis is a bash file, not a Python file.	2
[Relay] Legalize pass (#3672)* [Relay] Rewrite pass.This pass transforms an expression to other expression.This pass has many usecases * Replace a expr to another expr, if the other expr has faster performance. * For ASICs, we might want to modify the inputs to adapt to the HW support. * Alter op layout can work in conjunction with this pass.The supporting usecase is the Intel i8 x i8 conv. Intel HW supports u8 x i8 convin HW. Using this pass, we can replace an i8 x i8 conv to a sequence ofoperators where one of the operators is now u8 x i8 conv. This will also helpautomatic quantizaion performance.* Better API name.* Removing the conv2d legalization for x86. Will send a separate PR.* Test name changes.* Registering one funtion to register FTVMLegalize.* Better comments.	1
Revert "Added tesnorizeation for avx2 based gemm. (#3982)" (#4007)This reverts commit 23727eb49ea71609fc29963b996a68a14fddf79c.	4
WebGL end-to-end example (#369)	5
[microTVM][Zephyr] Add missing CMSIS-NN source files to cmake file (#12642)This PR adds missing CMSIS-NN source files to Zephyr cmake template file for models like keyword spotting, anomaly detection, VWW and image classification.	2
[ARITH][BOUND] Fix bound inference to avoid allocating too much (#3526)* [TVM] Fix bound inference to avoid allocating too much* [ARITH][BOUND] Pass analyzer to PropBoundToInputs	4
[CUDNN] Add partitioning support for conv2d and log_softmax (#10961)	2
[Makefile] Updates to top-level makefile. (#8317)* [Makefile] Minor cleanups to up top-level makefile- Renamed OUTDIR variable to TVM_BUILD_PATH- Delegate emcc calls to the makefile in "web" directory- Separated out build rules by type (e.g. C++, java, web, formating/linting)* [Makefile] Allow TVM_BUILD_PATH to include a list of directoriesIntended for development purposes, where building both debug andrelease versions at once may be useful.* [Makefile] Preserved behavior of using root tvm/config.cmake if it exists.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Don't use std::move in WithFields (#10009)* Don't use std::move in WithFields* lint	4
[TIR] Add type hint for TIR  (#9432)* add init* get rid of span* afs header* update scope_handler* rm tir/__init__.pyi* fix linting* fix lint* new test case* add axis module* address comments* redefine ty types* lint* address comments* address comments* fix ci* add test cases* fix CI* address comments* add types* mypy --strict* comments* update test comments* linting fix* address comments* add pylint for tir type check* address comments* move doc string* comments* getter setter* add PrimExpr, IterVar and Var* add sequence* change for handle	0
[TensorRT][BYOC] Minor refactoring to handle constants in pattern-based ops for TRT (#10994)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
Fix setting up hints for getaddrinfo (#2872)	5
hot fix (#10464)	0
Modify debug output (#10372)1. Modify debug output to make it more readable3. Replace magic number with a variable `error_ct_threshold`3. Add function to set error counter threshold externally for debug purposes	0
[Graph Tuner] Fix benchmark layout in graph tuner (#3926)* Fix graph tuner benchmarking layout transform* Add test	3
[Relay]collapse_sum and broadcast_to compute & schedule (#2180)	5
simplify makefile	2
AutoTVM x86 (#1772)* AutoTVM for x86 conv2d* Add ApplyGraphBest dispatch context* Fix tutorial* Fix conv2d* Improve tutorial* Fix default schedule* Fix 1x1 default schedule loading* Fix workload type* Change gridsearch to random* Add reference to autotvm arm* Merge conv2d common and 1x1 decl* Fix lint* Minor fix	0
[PASS] Revert the change of intel gpu warp index (#1127)	4
[Bugfix] Fix Python debugger segfaults with TVM built with LLVM (#5685)* Import readline before loading libtvm* make lint happy	1
[PASS] Improve SSA conversion, add forbid list in loop-par (#142)	1
add aten::masked_fill_ in pytorch frontend (#8403)Co-authored-by: Jackson Hsieh <chengpi@amazon.com>	1
[Hexagon] Skip test_avg_pool2d_slice because of segfault on hardware (#11929)	1
keras frontend tutorial (#278)* keras frontend tutorial* fix	0
[rpc] use callback func to do send & recv (#4147)* [rpc] use callback func to do send & recv. don't get fd from sock as it is deprecated in java* fix java build* fix min/max macro define in windows* keep the old rpc setup for py* add doc for CallbackChannel	2
Fix typo in a comment (#8129)Fix typo in a comment about AOT executor.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	2
Remove SGX demo private key from repo (#1237)	4
Remove SGX toolchain installation from CI Dockerfile (#4948)	2
[Blocksparse] Pipeline for lowering dense model to sparse-dense (#5377)	5
[PASS] Improve loop partition to remove un-necessary warning. (#766)* [PASS] Improve loop partition to remove un-necessary warning.* fix comment	0
[TVMScript] Improve printer for TIR syntax sugar (#9680)	1
l2normalization operator support for tensorflow (#1528)	1
[VTA] Make vta graph_pack compatible with latest TVM, and bring back object detection tutorials. (#8731)* [VTA] Make vta graph_pack compatible with latest TVM, and bring backobject detection tutorials.* remove deploy_detection.py.* move out deploy_detection.py from legacy folder.* fix build error.	0
[TensorIR] add TIRTextPrinter support for Block and BlockRealize (#7716)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[MetaSchedule] Developer Ergonomics Enhancement (#11622)Per discussion with @Kathryn-cat- [x] Move `initialize_with_tune_context` as private API `_initialize_with_tune_context`, andencourage using `TuneContext.initialize`- [x] Instead of using bunch of import statements, encourage using `ms.xxx` as the prefix(e.g. `ms.database.MemoryDatabase`) to organize things better- [x] Move `DefaultLLVM`, `DefaultCUDA` to a separate file and make them more discoverable- [x] Move `DummyDatabase` to `tvm.meta_schedule.database.MemoryDatabase` given it's actually useful- [x] Delegate class members' methods in `TuneContext`, for example, having`TuneContext.generste_design_space` from `TuneContext.space_generator.generste_design_space`Next PR:- Allow using a string `"default"` in `TuneContext` as well as `tune_relay/tir/te` to quicklyspecify a set of target-specific rules- Add `TuneContext.tune` to allow directly tuning without task scheduler.- Enhance detection of `ScheduleFn` in `TuneContext` to make it easier for users to quickly try outtemplate-driven scheduling on TIR.Co-Authored-By: Kathryn (Jinqi) Chen <65606304+Kathryn-cat@users.noreply.github.com>	1
[ARITH] Bugfix div subtract rewrite rule (#3504)	0
[relay] Small refactor for context (#4091)	4
[RUNTIME] Refactor to enable stackvm in runtime. (#1588)	1
[Runtime][Pipeline Executor] multiple threads management and the data forwarding notification mechanism. (#10234)* [Runtime][Pipeline Executor] multiple threads management and thedata forwarding notification mechanism.In this patch we create working threads for each runtime of pipeline.the threads would be terminated once the runtime class gets destroyed.We also add a notification mechanism derived from the 'binding configuration'of the runtime to forward the data notification.* address review comments.* address review comments.* fix typo.* fix typo.* trigger build.* address review comments.* address review comments.* address review comments.* address review comments.	1
More fixes and tweaks to the cuda conda packages (#3281)	0
[Relay][ONNX] fix bug in from_onnx (#2430)	0
[SCHEDULE] Fix the scan schedule with rewriting (#80)	0
[TF] Support TupleWrapper as direct ancestor of control flow ops (#5639)	1
ffi (Object): make class dict visible in instances (#5843)	1
Change license file to be detectable by github (#72)	2
[Hexagon] Do not auto-build apps when building TVM (#9970)* [Hexagon] Do not auto-build apps when building TVMThe Hexagon cmakes have recently become unwieldy due to a complexnetwork of dependencies between various automatically built components.This was in large part because of trying to automatically build someapps, which then tried to build TVM runtimes again, but with theirown configurations.This patch removes the ability to automatically build any Hexagon--related apps from the main TVM build. The following cmake optionsare now deprecated:  - `USE_HEXAGON_LAUNCHER`  - `USE_HEXAGON_PROXY_RPC`In order to build the binaries needed for HexagonLauncher fromtvm.contrib.hexagon:  - Build TVM+runtime for x86, with codegen for Hexagon enabled.    This can be done via `USE_HEXAGON_DEVICE=sim` or `target`.  - Build Android runtime and tvm_rpc with `-DUSE_RPC=ON`,    `-DUSE_CPP_RPC=ON`, and `-DUSE_HEXAGON_RPC=ON`.  - Build Hexagon runtime with `-DUSE_HEXAGON_RPC=ON`, and    `-DBUILD_STATIC_RUNTIME=ON`.* Add README.md* Restart CI* Add optional variable to set output directory	1
[DOCS] Add TensorFlow frontend docs (#4154)* Start to update TF frontend docs* Add rst* Remove markdown* Update wording* Resolve comments	0
Make tests multi-process friendly. (#3683)This side effect at module import time has a race condition between the "exists" check and the "mkdir" call.  The safer thing is to just call mkdir and catch the "already exists" error which is what makedirs does.	1
[REFACTOR][PY][API-CHANGE] establish tvm.ir, migrate corresponding files (#4862)* [REFACTOR][PY][API-CHANGE] establish tvm.ir, migrate corresponding relay files.This PR establishes tvm.ir and migrates the corresponding relayfiles into the new folder.API Change:- relay.Module -> tvm.IRModule* Update with ADT* Migrate transform* address comments* Migrate module* Migrate json_compact* Migrate attrs* Move LoweredFunc to stmt temporarily* temp migrate container* Finish migrate container	5
[Flaky] TFLite quantized conv test (#6084)	3
[ONNX]LpPool Support added (#5696)	1
[TEST][FLAKY] fix for #3099 (#3101)	0
[Relay][Frontend][ONNX] Add support for op Where (#4184)* Add support for op Where* Update impl version	5
[CMSIS-NN] Convert CMSIS-NN to use Target Hooks (#9397)* [CMSIS-NN] Convert CMSIS-NN to use Target HooksThis migrates CMSIS-NN to use Target Hooks instead of fully BYOC, whichmeans it will now go through any central passes the Driver API.* Mutated PrimFunc arguments in LowerTE so all functions are correctly lowered* Made Target `cmsis-nn` to match external code generator `cmsis-nn` to connect the Target with the external code generator* Modified Partition Graph to sanitise compiler names to generate them properly in C* Port tvmc fixes for hybrid targets* Update NPU tests with new sanitisation	1
Add `make docs` and doc building instructions (#9534)* Update doc building instructions and pin dependenciesThis pins the dependencies for the docs and adds `pytest` as adependency which was missing when I built. Tested out therequirements.txt with a fresh `ubuntu:focal` Docker image to verify thatthe required depedencies work.* Address comments, add Makefile for docs and add to instructions* Use Python for running scripts* Fix lint, add lint command* Add option for cpu to only run the precheck, address comments* Fix 'make doc' usage, add some -x's* Fix bad condition on --cpu, add defaults for envs* Fix another 'make doc'* Fix for running on MacOSCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Bump ONNX version (#3286)	5
Complete pytorch grid_sample (#10504)Pytorch's grid_sample() supports various interpolation options:(1) data dimension: 2D / 3D(2) interpolation method: nearest / bilinear / bicubic(3) padding_mode: zeros / border / reflection(4) align_corners: True / FalseHowever, TVM only supports a part of above options:(1) data dimension: 2D(2) interpolation method: bilinear(3) padding_mode: zeros / border(4) align_corners: TrueThis commit completes the options not supported by TVM, and keeps existinggrid_sample of onnx/pytorch uninfluenced.Co-authored-by: shukun.net	5
[typo] sin ==> in (#2238)sin ==> in	2
[microTVM][ARM] Enable tests that were skipped unintentionally (#12223)* fix bug to enable tests* fix import issue	0
[microNPU] Upgrade to 21.11 version of Arm(R) Ethos(TM)-U55 NPU driver (#9777)Change-Id: Ide4d2a33a215b4f1367667ad0fa4a66913cf9ad4	4
fix compilation error with cuda 11 (#6213)	0
[PASS] Avoid recursion in FoldScaleAxis (#2299)* [PASS] Avoid recursion in FoldScaleAxis* remove GetForwardScale	1
[RUNTIME] Enable OpenCL (#17)	0
[REFACTOR] Move Node always bebind NodeRef, expose ->	4
[FIX,TUTORIALS] Import tvm.testing in tutorials that use it (#7248)	1
[Target] Allow spaces in target attributes (#8587)* [Target] Allow for spaces in target attributes.Some target parameters, such as the device_name on vulkan, have spacesin them.  This prevented round-trips between string and Targetobjects, which can occur in some cases.* [Vulkan] Fixed "device_name" property querying.* [Target] Switched from escaped spaces to quoted spaces.Instead of -attr=value\ with\ spaces, will instead be written as-attr='value with spaces'.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[CMSIS-NN] Support for passing cpu flags to Arm(R) Corstone(TM)-300 software (#12132)Change-Id: I12312dc0fb27ac991f1a25544f226cd00b5f9281	4
[BYOC][ACL] removed ACL 20.05 limitations (#7251)Removed checks for padding in according with changes in ACL 20.11*ACL stands for "Compute Library for the Arm® Architecture"	4
[TVMScript] Fixing T.buffer with typed positional arguments other than int32 (#10892)* workaround for T.buffer with typed positional arguments* address comments* fix linting	0
Add a FunctionPattern, remove unused attributes in CallPattern (#7151)* Add a FunctionPattern, remove unused attributes in CallPattern* update docs	2
[ONNX] Support importing Conv with missing attributes (#7899)* [ONNX] Support importing Conv with missing attributes* fix removal of attributes ONLY when they are default and for autopad* move comment to the right place	4
AVX schedule for conv_NCHW[x]c (#1143)* add conv2d_NCHWc compute template* add conv_NCHWc compute decl and schedules* allow default avx schedule* fix lint* remove unused schedule* remove import nnvm.reg* pass schedule object to compute and schedule	4
[ci] Add retries to S3 uploads/downloads (#12221)`aws s3 cp` sometimes segfaults, so retry it when the command fails	0
[TIR] Avoid unnecessary dtype escalation in loop splitting (#12035)This PR introduces a type check to cast loop split decisions (sometimes given as `int64`) back to a smaller datatype when the loop variable's data type is smaller. This issue usually happens during reloading a trace from disk using JSON database and causes the failure of `CompactBufferAllocation` pass.	4
[Conv2DTransposed] Fix wrong shape check and add new TOPI module to support groups (#9465)* f wrong type check in conv2d_transpose* add test case for conv2d transpose* add groups support for conv2d_transpose* add  naive implementation and schedule for conv2d with groups* enable tests for cpu and arm_cpu, raise error for cuda platform* revert the cuda and generic strategy* revert back the x86 strategy* revert back the arm_cpu strategy* revert back the arm_cpu strategy* revert back the arm_cpu strategy* fix EOF of x86* clang lint updated c++ code* update topi implementation* Revert test* Revert test* add generic/x86/arm specialization for conv2d_transpose with groups > 1* remove commentted codes* fix lint* fix lint* fix c++ lint* fix lint* fix python lint* remove comments and reformat* lint file* lint code* fix lint* update logging information in convolution.hCo-authored-by: Alicja Kwasniewska <alicja.kwasniewska@sima.ai>	5
[Hexagon] Add doc on TVM - Hexagon RPC flow (#10507)* [Hexagon] Add doc on TVM - Hexagon RPC flow* updated for the latest code* add TODO on removing rpc_local_session.cc	4
[MetaSchedule][BugFix] Fix broken integration tests (#10885)	3
[Relay][AlterOpLayout] Fix strided slice type change. (#8022)* Fixed strided_slice alteroplayout bug.* add test for non standard int8 conv2d padding.* Add test for large index slices.* Us same dtype as input in strided slice.	3
µtvm debug improvements (#5648)* Forever loop in UTVMDone to aid debugging* Use parameter and callback function as a micro debug hook. * Previously, users had to uncomment a region of code in   micro_session.cc and recompile to debug. Now they can pass in a   key in the micro.Session config:       config = tvm.micro.device....generate_config()       config['debug_func'] = _python_launch_gdb       with micro.Session(config) as sess:         ....* clang-format* Only forever loop on device (on host this blocks unittests)	3
[LLVM/CPU] Add comments with origins of various runtime/backend types, NFC (#9177)LLVM codegen defines a bunch of LLVM IR types that correspond to variousruntime entities. Add comments showing what entity a given LLVM IR typecorresponds to wherever applicable. Since there is no direct connection(e.g. using names, etc.) that could be extracted automatically (via ctagsor other mechanism), this can make the LLVM codegen a bit easier to read.	1
[FIX,CMAKE] Only set Clang flags for C++ files (#7424)Clang flags were set for all file types, causing nvcc to error out.	0
[AutoScheduler] Fix layout rewrite for axis with extent=1 (#7279)	0
Do not mutate GlobalVar's checked_type field. (#2026)	5
[TVMC] Re-enable PyTorch test (#9441)This test was originally disabled due to the issue documented in #7455affecting CI. I believe this has since been resolved by #9362.Note: This patch should not be merged until the changes inhttps: //github.com/tlc-pack/tlcpack/pull/81 are reflected in CI.Change-Id: Ib918595a1d9149e3c858ca761861304450cbfe13	4
[FIX,AUTOTVM] Add backtraces to tuning errors (#9901)* [FIX,AUTOTVM] Add backtraces to tuning errorsCollects tracebacks in LocalBuilder and LocalRunner and adds them to theerror messages.* formatting* correctly unpack traceback and exception* add assert* fix?* one remaining measureresult* formatting* fixed	0
[RELAY] Add missing arg in vgg (#2329)	1
[Relay] Register layout conversion function to more reduce ops (#9048)* Register layout conversion function to more reduce ops* bug fix for exclude=True case, the original code compute wrong axes* properly handle variance op, which has two inputs* update test expected output	3
fix extern naming (#1238)	0
ignore model option in target (#889)	1
[Relay][Frontend][Onnx] Robustify Loop Importer (#7353)* Add test for array loop.* Fixed scalar issue.* Formatting.* Fix injective schedule for dynamic shapes.	0
[FIX][AUTOTVM] Make autotvm work with spawn (#6790)Like #6671 this PR fixes autotvm when using the spawn start method formultiprocessing. I've added some tests to make sure that things workwith spawn in the CI.	1
[BYOC][Verilator] Skip mobilenet test if Verilator is not available (#8094)* skip mobilenet test when verilator is not available* add skipped to pytest* add pytest	3
[AutoScheduler] Refactor task interface for tuning single operators (#7028)* [AutoScheduler] Refactor task interface* updae tutorials and tests* update* fix lint* fix lint* update* fix test	3
[RPC] Report correct port from C++ RPC to tracker. (#9642)	5
[FIX] Fix using num_workers in omp (#7078)Co-authored-by: zhangfucheng <zhangfucheng.jason@bytedance.com>	1
[Runtime] add necessary const qualifier for NDArray container of parameters (#4590)	2
[Hexagon] Move aot/graph_executor interactions into launcher (#10907)* [Hexagon] Move aot/graph_executor interactions into launcherFollow-up from https://github.com/apache/tvm/pull/10581, applyingsimilar changes to the AOT and graph executor interactions.  Thismoves the file management and upload/download from the unit tests intothe launcher.* Added Session.test_executor to avoid duplication in graph/aot test.* Resolve lint errors* Moved link flags workaround out of session, into create_aot_shared* Separated Session.get_*_executor and Session.get_executor_from_factory* Updated to resolve lint error	0
Update required cmake version in docs. (#9484)* The version used in CI is the minimum version. * docker/bash.sh ci_gpu cmake --version prints       cmake version 3.10.2 * Found in #9239	1
[COMMUNITY] @Laurawly => PMC (#7307)	3
[MODULE] support load back of .ll file into llvm module (#183)	2
Assume /Tools is part of the HEXAGON_TOOLCHAIN env variable (#9609)	1
[TF parser] Handle int64 dtype in range (#6918)	0
[REFACTOR] Replace TensorObj and TensorValue with NDArray (#4643)* replace TensorObj and TensorValue with NDArray* NodeBase to Object in Python* rebase	4
[NNVM][TEST] Numgrad: fix nan and multioutput (#1754)	0
[FIX] Fix threadpool reset by killing threads before destroying their shared queue (#8658)	1
[TVMScript] Support roundtrip of LetNode (#11742)Just a missing support for `tir.LetNode`	1
[DOC] Fix mistyped word (#6362)* Fix the doc mistyped word in `tvm.te.hybrid.build` functionCo-authored-by: gigo <gigo_liao@qbitsemi.com>	1
[PYTHON] Check in a symbolic construction interface in python, start … (#4)* [PYTHON] Check in a symbolic construction interface in python, start add graph API* Graph API	1
Fix split's last factor issue (#4044)	0
[BYOC][VitisAI] Fix issue in Vitis AI codegen out tensor names matching & update docs and docker (#7350)* Fix bug in vitis ai codegen out tensor names matching & update docs & update docker* Update vitis_ai.rst* Move gpg-agent package installation to vitis ai core script* Refactor install_vitis_ai_core script* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update vitis-ai docs pynq/edge setup & adjustements for comments* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Reorg Vitis AI dockerfile to make sure gpg-agent is installed before llvmCo-authored-by: Jorn Tuyls <jornt.tuyls@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[TE] Support schedulable TIR compute definitions in TOPI (#11589)This PR adds `te.extern_primfunc` which provides the interface around TE ExternOp that allows a TVMScript defined schedulable TIR PrimFunc to be inlined into a TE compute graph. The result is that TIR can be used for compute definitions in Relay OpStrategies and, paired with meta-scheduler support in relay as introduced in #10578, these compute definitions can be scheduled and tuned as demonstrated in the attached tests.  Prior to this, compute definitions were limited to those definable in TE only. As a consequence of this patch and ongoing improvements to TVMScript meta-programming (#11097), TOPI can be extended to include compute and scheduling functions targeting schedulable TIR uniformly.	1
[Contrib] Fix error message at callback_get_section_size() (#4221)* [Contrib] Fix error message at callback_get_section_size()* Trigger notification	1
Improve error messages in graph tuner, graph runtime, and module loader. (#6148)* Raise error if no operators are found in GraphTuner* Raise error if key cannot be found in graph runtime inputs* Detailed error message when module loader is not found	0
[TVMC] Allow options on --target to contain dots. (#7651)* Allow tvmc compile --target options to accept dots * Adds testing for dot separator in quoted and unquoted   values * Add an "unquoting" conditional so that quoted and   unquoted strings look the same when parsed	1
[Frontend] [Torch] [ONNX] GRU layer (#8781)* GRU cell was implemented in common.py. GRU was supported on pytorch frontend side* update GRU in common.py and onnx frontend* fix issue related to GRU accuracy in pytorch and ONNX frontend* small fixes and remove excess* common GRU was additionaly updated. tuned pytorch GRU was strongly accelerated* GRU cell in ONNX frontend was used from common.py. previous implementation was removed* small fixes in comments* fixes after review. GRU test was implemented for pytorch frontend* tests for RNN layers was unified for pytorch frontendCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
[Fix] Fix a few bugs when dtype is fp16 (#4088)* Fix layer norm for fp16* [Fix] Fix arange for fp16* [Fix] Fix mxnet frontend for fp16* [Fix] Fix arange for fp16* remove comments* x* fix nnvm	0
[RPC] Replace timestamp with counter (#7389)	5
[Relay][Transform] merge PassContext and BuildConfig (#3234)	5
[OpenCL] Fix type casting (#11038)* [OpenCL] Fix type castingThe previous PR apache/tvm#11021 was reverted in apache/tvm#11035 dueto it affected performance of generated OpenCL code.This PR fixed the same issue but doesn't lead to performancedegradation. Tested on Resnet50_v2 network.* Implement using select built-in	1
add PaddlePaddle tutorial (#9124)* add paddle tutorial* fix some format issues* fix code style* clean* fix format* fix title underline* PaddlePaddle>=2.1.3	1
Outdated renaming for flatten in ONNX converter (#2843)	5
[MATH][TOPI][NNVM] introduce trunc, round (#1310)	1
[FQ2I] Add topk to FQ2I (#10170)* Add topk/dyn.topk to FQ2I* Remove dyn.topk* Add uint8 to sort	1
[RUNTIME] Remove header dependency on private header (#163)	4
[GRADIENT] Register more gradient operators (#300)* Add conv2d max_pool backward op* Added tests* Fix testing* Address comments* Change dot to matmul* Address comments* Break down indicator function* Make greater, less numpy compatible	1
[TOP] complete level2 (#8)* [TOP] complete level2* [TOP] add split	1
[Relay] Change some passes to mix mode (#6695)	4
[TFLite] Implemented EXPAND_DIMS Operator for TFLite. (#6243)	1
enable AlterOpLayout to keep OP unchanged (#471)	4
[PTX-MMA] Add full PTX MMA code generation support (#9909)	1
[Topi, ARM] Disbale Winograd for quantized tensors. (#5363)* [Topi, ARM] Disbale Winograd for quantized tensors.* Relaxing float	5
disable cuda int8 schedule for non-cuda gpu target (#9014)	1
Add test for issue #327 (#328)* Added test for adding down-/up-sampled layers* Enabled test for adding down-/up-sampled layers* Normalize whitespace	1
[Hexagon] Codegen for 2d Load/Store (#10586)* Added unit tests for codegen of 2d physical buffers in Hexagon.* Update IndexMap when buffers are updated.* Extended CodeGenLLVM::BufferAccessHelper to support N-dThis way, a subclass can override GetBufferPtr, without needing toreimplement all of the other indexing logic forBufferLoad/BufferStore.* Updated CodeGenHexagon to treat 2-d physical buffers as T*** Moved indices size check earlier.Previous location in `CodeGenLLVM::BufferAccessHelper` occurred afterpossible integer wrapping in `indices.size()-1` loop bounds.* Updated to use `llvm::ArrayRef` instead of `std::vector`.* Resolve lint error.* CI fix, contextlib.nullcontext not available on python3.6	0
Fix non-zero extent of access_ptr out of range (#1937) (#1939)	0
[AUTOTVM] Allow fallback for template & Fix bugs in tuners (#1615)* support fallback & fix bugs in tuners & clean topi test* update task extraction* update task extraction* fix arm tutorial* Update tune_nnvm_arm.py	5
[CMSIS-NN] Re-enabled skipped tests (#10928)	3
Add support for aten::dot (#9893)* Add support for aten::dotThis implements dot product as a composite of of multiply + sum* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay] Check match expressions for completeness (#3203)	5
adding Leandro to committers (#7999)	1
[USMP] Adding support for U1 usecase for constant pools (#10189)* [TIR.Constant] U1 usecaseConstants are now aggregated into one struct and initialized in default_lib0.cfileChange-Id: I34d61f8139c8a92c06944fe990ba892a660476fdUnit test fixedChange-Id: I436e7b6d6b3064b3f8bbfbb048d4296b63a6b69c* RefactoredAddressed:* PoolInfo splitted to WorkspacePoolInfo and ConstantPoolInfo* workspace_byte_alignment moved to ExecutorCodegenMetadata* getModuleAlignment -> GetModuleAlignment* GenerateInternalWorkspaceBuffers refactored* reverted format change of src/tir/transforms/legalize_packed_calls.cc* addressed comments for src/tir/usmp/analysis/extract_buffer_info.cc* removed commented code from include/tvm/tir/usmp/utils.hChange-Id: I7d1b32884b0e5992e2e00c7838c85e425d9c25fd* more unit test fixesChange-Id: I573a05fa1cb4037ae83691f7dff2c2724b1d7700* More refactoring and unit test fixesAdded ConstantMemoryPoolsChange-Id: If1e391c631575980564bca790ba33748c82d907f* bugfixChange-Id: Iacc7a9d734a505dfa0d8d32d23ea3f57e6de8582* refactoring. added constant_alignmentadded constant_alignmentunit tests updatedChange-Id: I378193cb9e675e352c61d96ff4e09655090053e1* unit-test bugixChange-Id: Ia4411d59c4a376c01326fed366cdb196a432899e* unit test fixChange-Id: Ia2077bdeb1d2c6c9827eeef90ab410ae31b8c4a4* Added support for c++ runtime* refactored* renamed pools and constsrenamed pools and consts to workspace_pools and constant_pools* addressed upstream comments* addressed upstream comments-2* addressed upstream comments-3	1
[ci] Restructure Jenkinsfile (#11380)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[ETHOSN] Drop back to Ethos(TM)-N release 21.08 (#10157)There is some clash with the int8 support that went in at the same time.	1
[ROCM] Fix undefined symbols by adding library (#8446)* Add libhsa-runtime64 reference.* Remove lib in library definition.	5
conv2d_56_64_128 mark==1 bug fixed (#624)	0
[microTVM] Fix RVM onnx dependency and Zephyr document update (#7774)* fixing poetry* fix onnx issue* add zephyr README* Update README.md* clean up* moved onnx* replace with poetry* add tflie	1
[ONNX] Add Einsum converter (#8985)* einsum* address review* move files around* use generic topi op* TODO comment* jostle ci* jostle ci	2
[skip ci] Fix scipy intersphinx link (#10181)Scipy changed their docs recently to be pinned to each release, so we need to update the URL we pass to intersphinx accordingly. Skipping CI on this to unblock other developers but local testing with```bashpython tests/scripts/ci.py docs```both showed the same error as CI before this change and no error after.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR][VM] Revert a change to lower_tvm_builtin.cc from #6126 (#8274)* revert a change to lower_tvm_builtin.cc from #6126* disable sim target on VTA tutorialfix bad refactortry again	4
[AutoScheduler] Fix task scheduler restoring (#6934)* [AutoScheduler] Fix task scheduler restore* miner fix	0
[TOPI] Layout Rewriting in TE (#11844)	5
[RELAY][TF] Support symbolic newshape for Reshape (#5429)* [RELAY][TF] Support symbolic newshape for Reshape* Only need to pass data* Use MakeReshape() in Reshape()* Change newshape to Expr* Create a template for Array<T>* Fuse reshape when newshape is constant* Make newshape Optional* Use bool() of OptionalCo-authored-by: Li Xiaoquan <xiaoquan.li@denglin.ai>	1
Revert "Implemented rpc logging (#10967)" (#11227)This reverts commit aa3bcd9d3374878c5e958b842f51bfd82f0ebd9e, because itfails on Windows CI as reported in issue #11220. PR #11223 tries to addressit but is is failing in the regular CI with testing issue on Hexagon.	0
[CI] Update ci-cpu to the latest (#6031)	3
[API] Change attr to explicit name set_attr (#46)	1
[RELAY]missing schedules updated (#2196)	5
Fix code comment and typos. (#3063)	2
[Hexagon] Fix missing pytest import (#12565)* Add pytest* lint	3
[TOPI] Fix uint8 resize_bilinear issue. (#2490)Signed-off-by: Zhebin Jin <zhebin.jzb@alibaba-inc.com>	0
[microNPU] Fix stride bug in strided slice legalization (#10286)* [microNPU] Fix stride bug in strided slice legalizationTFLite slice is legalized to a strided slice operation with`strides=[1]`, but a similar TFLite strided slice operation islegalized with `strides=[1, 1, 1, 1]` which fails during compilation.Since we only support strided slice with no stride, hard-coding thisvalue to `[1]` during legalization.Change-Id: Ia34183c6984f3c4f7e88063bf8b17fc44f1eb7f9* add legalize strided slice testChange-Id: Icb9480f3c1ed8ce0a328fc7c079c492b33a62e79	3
[contrib][nnpack] remove training-optimized ops (#2224)	5
Fix a small timer bug. (#10875)	0
[TIR][Transform] relax LoopPartition restriction that the intersection of all conditions can not be none. (#10340)Co-authored-by: sqing <qing.siqi@intellif.com>	5
Use `/usr/bin/env bash` in shebang for all scripts under tests (#10277)* Use /usr/bin/env bash in shebangThis makes scripts executable on system without /bin/bash (NixOS)* Use `set -e` in script instead of `bash -e` in shebang	1
[CI] Update DGL in gpu image (#10111)* validating ci_gpu:20220128-070420-fa317edf7* remove gcn tutorial workaround* update ci-gpu image to v0.81	5
[PASS] copy intrin (#536)* [PASS] copy intrin* update comment thanks to derisavi	5
[Relay][Topi][CPU] Dense with weight transform (#7404)* Add CPU dense weight transform* Fix format* Fix python format* Fix pylint* Minor fix* Add test* Do not need to infer layout for dense* Fix test* Rename dense_pack* Fix test* Fix lint* Fix dynamic shape dense* Fix lint* Fix autotvm task extraction test* Disable AlterOpLayout in micro_tflite.py tutorial	3
[MetaSchedule] Check auto tensorization applicability in MultiLevelTilingWithIntrin (#12263)* Check auto tensorization applicability in MultiLeveltilingwithintrin* fix qbert loader* add MultiLevelTiling rule in integartion test* unused import* fix cpp format* add more test* Check for tiling failure	0
[CodeGen] Cleanup generated code (#5424)- remove unnecessary white spaces from storage kind- do not start a new scope for vectorization as temporary  variables are alll uniquely generated.The above two changes make vectorized code much cleaner.Signed-off-by: Wei Pan <weip@nvidia.com>	4
[TEST][FLAKY] topi/tests/python/test_topi_sort.py::test_argsort (#4891)* [TEST][FLAKY] topi/tests/python/test_topi_sort.py::test_argsort* upadate test function of argsort like topk* Shuffle index and get data from shuffled index* Replace the random.uniform with np.arange	5
[CODEGEN] Change default max_auto_unroll from 256 to 32 (#164)	4
[TFLite] Convert TFLite NCHW to NHWC (#3141)* Convert TFLite NCHW to NHWC* Minor comment fix	0
[Hexagon] Implement avg_pool2d slice op (#11417)* Implement avg_pool2d slice op* Address review comments and fix the STIR schedule* Fix formatting issues* Address pylint errors* Additional formatting issues* more pylint fixes* Changed arch version to v68 for now* Changing arch version back to v69* Move the test to tests/python/contrib/test_hexagon/topi	3
[TIR][Schedule] Update compact_dataflow constraint (#10158)	5
[ManifestAlloc] Handle TupleType inputs in CheckReshapeOnly (#6776)* Changes in CheckReshapeOnly to support TupleTypes as inputThis arises insed ManifestAllocPass inside relay.vm.compile* [ManifestAlloc] Handle TupleType inputs in CheckReshapeOnly	0
[NODE][RELAY] Move most of the reference related code to node (#1747)	4
[TOPI] Allow batch matmul to be fused into injective ops (#4537)	1
[TIR][USMP] Greedy memory planning algorithm (#9214)This commit implements a greedy memory planning algorithmsusing the proposed USMP design.There are two greedy algorithmsintroduced here which use the size and number of conflicts as the criteria.- Adds few test cases checks for fan-out and linear structures.- Added a test case for ResNet sub-structure- Added a test case for MobileNet sub-structure- This includes a slight fix for buffer info  extraction where non-linear network buffers  owned by the main function should not show  sporadic liveness.	1
[DOCKER][FRONTEND] Run DarkNet tests (#2673)* [DOCKER][FRONTEND] Run DarkNet tests* update tests to pass CI	4
[Doc][Fix] Fix qnn op parameters hint order (#9622)As the following parameters are both Expr.```zero_point : tvm.relay.Exprscale : tvm.relay.Expr```It's really get me confused when I followed the python arg hints to create a relay and without success.	1
[DOCS] Fix topi tutorial (#1222)	0
[CI][AArch64] Skip libgomp failures in integration tests (#12554)Some integration tests are failing when running in CI machines thathave torch installed (validated only in AARch64 for now), with anerror message related to libgomp, similar to the one above:OSError: /.../dist-packages/torch/lib/libgomp-d22c30c5.so.1: cannotallocate memory in static TLS blockAs part of enabling the integration tests in AArch64, I'm marking thistests as skipped, so that tests can start executing and don't regresswhile we take time to investigate these specific failures.	0
Fix error during running on nvptx with cuda9 (#1162)	1
[TOPI] Fix compiing batch_matmul and dense when two args are the same tensor (#9207)* Add explicit copy stage for batch_matmul(x, x) case* do copy in relay strategy to avoid dup* add copy to dense op and schedules* black* add batch_matmul test* add dense test* fix cuda int8 dense test* remove need_copy flag* do not use tag to decide if tensors are same* rename to copy_if_identical and add comment* black* one more fix missed* add length check on input tensors* one more length check* fix variable name	0
correct error (#9193)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[Relay][Docs] Relay Language Reference (#2232)	2
[TOPI] Custom schedule for standalone transpose in cuda (#8030)* [TOPI] Custom schedule for standalone transpose in cuda* check if input is not Any* fix vta test* check input shape* fix injective* move transpose out of sparse.py* update comments, use warp size* missspelled transform* formatting* rename test* comment* fix tests	3
[NODE] Enable EnvFunc to serialize global function as node (#1721)	1
Update README.txt	5
[CI] Add back the tests after timeout adjusted (#7408)	3
Update index.rst	5
[cmake] update vulkan rules (#5777)	5
[RUNTIME] Introduce MetadataModule to separate code compilation/interpretation and weight initialization (#5770)	5
[RELAY/PASS] Simplify inference. (#2033)	5
Change Rust version to stable in Docker (#5138)	2
Update Docker image with tag 20220404-055909-fcdf4636d (#10889)Updates docker image with tlcpackstaging 20220404-055909-fcdf4636dto update TensorFlow to 2.6 and solve Vitis-AI build fixes caused byan allocation error from TensorFlow 2.6.Also updates Keras, h5py and pyxir.	5
create symbol from nodeattr (#116)	1
Create closure object for GlobalVar (#3411)	1
[uTVM] fix crt building and running error (#6231)Signed-off-by: windclarion <windclarion@gmail.com>	0
[Rust] Make TVM Rust bindings installable via Cargo.  (#7503)* Make TVM Cargo installableRewrite the Rust Module API and change some imports causing crashes.This commit also updates the docs to remove outdated information.Fixes for version bumpUpdate build.rs to use new tvm-build versionTweak build.rs to use release version of tvm-buildAdd docsAdd Readme for tvm-sys crate.Fix Cargo verisions for pre-releaseAdd READMEMove generated code to OUT_DIRFix pathAdd descp for tvm-sysTweak versions for publishingTweak versions for publishingAdd README for tvm-graph-rtConform to Apache branding guidelinesFix capsAdd headerRemove warningFormatClean up buildTurn docs back onTweak CIWIPRemove CI changes* Disable docs* Fix	0
[topi] enable fp16 sort for arm (#4084)	0
[ARITH] Upgrade CanonicalSimplify to Simplify Mod (#676)	5
Generate requirements.txt from Python spec (#7289)* Generate requirements.txt from Python spec.* add tests, collect actual requirements (first cut).* add tornado and cloudpickle* add xgboost* add xgboost version restriction* cleanup and prepare for merge* black format* add type annotations and docstrings* remove example requirements.txt* fix setup.py extras_require* use typing. classes for type annotations, python 2 compatible :)* fix python2 typing.Pattern* retrigger CI* address comaniac comments* retrigger ci	1
[hexagon][testing] Better pytest ID strings (#12154)- Add utility functions to allow more human-readable pytest test IDs.  Helpful when ID strings become too large for humans to easily read.- Update the `test_avg_pool2d_slice.py` unit test to use this mechanism.	1
Remove old AOT Executor code (#8758)* Remove old AOT Executor codeThis removes the old AOT execution functions that relied on the model descriptor which was removed in https://github.com/apache/tvm/pull/8280.* Remove rogue tvm_model_t from demo app* Remove aot_executor from demo CRT libs	4
Keep up with changes of NodeRef	4
[Pytorch] Add `aten::fmod` and `aten::remainder` (#10613)* [Pytorch] Add `aten::fmod` and `aten::remainder`* update tests	3
[AUTOTVM] API change (#1583)	4
[COMPILER][BUG] Fix out of bound access. (#1723)* [COMPILER][BUG] Fix out of bound access.* * Review comments.	0
[VirtualMachine] Zero copy in set_input when input is DLTensor (#11003)* method of creating of NDArray from external DLTensor was implemented* set input without copying for DLTensor source* code clean up* update description and comments after reviewCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	5
Make duplicated function name checker working (#705)	1
[Relay][TensorFlow] Support tf.math.reduce_prod (#3166)	1
[FIX] Correctly link to PAPI (#8691)	2
[BugFix] Generate unique names for reduction blocks (#10726)	0
[COMMUNITY] ajtulloch -> committer (#4043)	3
Add default for split op (#9489)* split fix* add default split test case	3
Label Pattern Partitions (#5627)* Label Pattern Partitions with a default label to prevent nested partitions and an optional user supplied-label* Add node names in topological order to Partitioned attribute* respond to review comments* move partition tag into const in attr namespace	4
[REFACTOR] top->te (#4759)Bring up namespace te -- Tensor expression language DSL.	4
Fix UnboundLocalError: local variable 'tensor' referenced before assignment (#3074)	0
[TUTORIAL] Fix vta tutorial after relay function refactor (#5095)	4
Fix Relay docs formatting and grammar (#2471)	2
Make Google Test usage configurable in CMake files (#3628)* Add USE_GTEST as a CMake variable* Add GTest section in installation docs* Incorporate feedback	5
[Relay][bugfix][error reporting] BiasAddRel does not check for a negative index being out of bounds (#7554)	1
Checkin Schedule and split construction in front-end	5
Add missing sgx includes (#2878)	1
[NODE] Macro to define NodeRef methods, constructor style example (#3224)	5
Split GHA into 2 workflows (#9578)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
OpenCL debug runtime timer handler added. (#10140)	1
[frontend][ONNX]support ConvTranspose explicitly specified output_shape (#11076)* support ConvTranspose explicitly specified output_shape* fix unit test case* fix lint test* retest* fix code error* fix lint test* update test* retest* fix test onnx official tests	3
[WIN] export void Configure(...) symbol for Windows (#12091)Extends TVM API for clients of Windows. It helps configure thread pool on native side on Windows OS.Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>	5
[Relay] add Tuple pattern (#3596)* implement tuple pattern* add tuple pattern* lint;* lint* lint* fix error* fix* add test	3
[PROFILING] Catch any errors while setting locale for printing (#11860)Change profiling::Report printing to catch any errors when setting thelocale (used to add separators to large numbers). This avoids issuesaround misconfigured locale.	5
Let remote RPCModule get function recursively (#11053)Same reason as we changed the get_function to recursive searching. This is useful when we treat the local module as a data segment and wrap it in another empty LLVM module to reuse the export_lib API. After de-serialization in remote, the data segment will be translated as an imported module. Thus we need to fetch the function recursively.	1
[PASS] Check memory info bound to guard failure (#409)	0
Documentation issues (#1702)	0
[Doc] update frontend tutorials to new model based runtime (#6063)	1
[topi][relay] new PR to re-add tan to TVM (#5025)* Add relay operation relay.op.tan.* Update tan implementation in TVM.* Update tests.* Add shape function for tan.* Add missing main test to python/frontend/tensorflow/test_forward.* Revert, back to sin/cos.* Revert "Revert, back to sin/cos."This reverts commit 4da5b503b921585ba9d80944b29136142b575c40.* Fix implementation of tan in cuda. Do not support tan for float16.Simplify topi/tests/python/test_topi_math. Add testing for tan with float32 and float64.Finally implement tan as sin/cos in llvm.	3
[Relay] Add support for tuple node in operator fusion (#2187)	1
[CUTLASS] Initial support for conv2d wgrad (#10177)* [CUTLASS] Add wgrad support (without split-k)* run black* wgrad tests now work under pytest* dw conv2d properly supported for wgrad* all tests work* fixed for sm75* cpplint* fix conv2d grad test	3
[MetaSchedule][Testing] Test search space of conv1d (#12032)* [MetaSchedule][Testing] Test search space of conv1d* Add checks for trace roundtripping	1
add metal to list of choices (#8282)	1
Remove pytest dependency in arm_compute_lib.py (#7556)* Add OpAttrContext class which allows to temporarily change an attribute of an operatorChange-Id: I19b809a105ea8769e56bd89e028e090959a08728* Replace TempOpAttr with OpAttrContext in arm_compute_lib.pyChange-Id: I1c42dd6a29e765b06ce28192397016efeea2e82a	4
[tvm4j] RPC Server (#268)* [tvm4j] RPC Server* [tvm4j] fix recursively function calling; connect to proxy server; osx rename .so to .dylib* [tvm4j] test case for proxy connection; thread pool for serving	3
[Frontend][Keras] Support nested layers recursively in keras frontend (#7949)* Support nested layers recursively in keras frontend* Fix lint* Fix issue* Fix formatting* Fix unit test	3
Fix builtin_fp16.h path according to: https://discuss.tvm.apache.org/… (#8705)	0
[Hexagon] Generalized HexagonBuffer::CopyTo/CopyFrom (#10878)* [Hexagon] Generalized HexagonBuffer::CopyTo/CopyFromThis change operates on the allocation regions in a `HexagonBuffer`,rather than referencing the managed allocation owned by a buffer,handling copies between two sets of possibly discontiguous regions.This will be necessary to handle discontiguous buffers that cannot bestatically planned at compile-time, such as user-initiatedallocations, within a shared memory pool.Contiguous regions of memory are recognized and result in a single DMAcall.	5
[LLVM] Support CodeGenBlob for large >2GB models on x86 (#10882)	1
Extend TensorComputeOp to allow scalar inputs (#2606). (#3300)	1
[Relay] Fix reduce axis bug (#3422)* fix relay reduce axis bug* add tests for reduce bug	0
Disable OpenGL test temporary (#801)	3
[Refactor] Rename .asnumpy() to .numpy() (#8659)	4
[QNN] Use sigmoid Lookup Table method instead of fallback to fp32 (#12038)	1
[Frontend][MXNet] Add support for MXNet GroupNorm (#7409)* Add support for MXNet GroupNorm* Fix python lint* Fix lint	0
[RELAY][DYN] Dynamic UpSampling3D Op (#6353)* frontend and start of cpp impl* upsampling3d typerel and makefunc* impl upsampling3d dynamic to static pass* passes test_dyn_upsampling3d_infer_type_const* fix bugs and improve doc for resize and upsampling* code cleanup* make tests more complex* code cleanup, fix test_dyn_upsampling3d_run* fix typo* ci not working	1
Don't run non-tvm_op GraphRuntime nodes in Debug Runtime over RPC. (#7512)* Don't run non-tvm_op GraphRuntime nodes in Debug Runtime over RPC. * These are filtered out in SetupOpExecs for normal debug runtime operation.* retrigger CI* retrigger CI* address tkonolige comment	1
[TFLite][CI] Update TensorFlow dependency to 2.9.1 (#12131)This updates the TF version to be used in TVM CI to 2.9.1,which brings improvements so that more platforms are supported byofficial packages.When building TFLite, an update to CMake was also required,which is updated now to 3.18.4.ethos-u-vela dependency is also updated, from version 3.2.0 to 3.4.0so that it is closer to the TensorFlow version being proposed here.This PR updates the Docker images scripting to install TF and TFLite.Change-Id: I290085f0c018ad57606f1295494c19ff6e1af2dd	4
[PROFILING] Combine USE_VM_PROFILER and USE_GRAPH_RUNTIME_DEBUG into a single flag USE_PROFILER (#7637)	2
[RUNTIME] Move module export to the function level. (#4405)	1
Overestimate binary size for microTVM compiled binaries. (#5590)* Overestimate binary size for microTVM compiled binaries. * Currently uTVM binary section sizes are computed by summing the   sizes of all symbols in the section. * This method produces errors because it presumes the linker works in   a particular way, rather than analyzing the linked output. * As we intend to move away from linking inside TVM (RFC   forthcoming), just using this stopgap to make forward progress   until then.* address weberlo comments* fix regression (use 64 bit word size)	1
[NNVM][TENSORFLOW] bug fix on bilinear and resize op integration in frontend. (#1440)	0
[COMMUNITY] @jwfromm -> Committer (#7316)* [COMMUNITY] @jwfromm -> Committer* add areas	1
[TOPI][OP] cuda for argwhere (#6868)* argwhere* cuda schedule* sort argwhere result* Use single block and thrust to fix flaky behavior* format* used dynamic strided_slice* Fix dynamic strided_slice* try new strided_slice* Improve dynamic strided slice to bind data depedent shape var.* all tests pass* remove print* use new strided_slice* cleanCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>	4
[USMP] Change internal workspace section (#11246)This commit changes the internal workspace generationto be under .bss.noinit.* with NOLOAD behaviour as itdoes not need any form initialization.	5
[BUG] DataType Bug In SplitRel (#8899)* [BUG] DataType Bug In SplitRel* Add Test Case	3
[TOPI] add dilation operators (#316)* add dilation operators* fix pylint* dilate testcases success* n-D tensor dilation* support arbitrary dimension	1
[Relay] Continuation Passing Style (#3456)* saveaddme find type checker problemsavesavelintdolintreset tiadd some docadd failed test caseadd recursion for cpsadd recursion for cpsfix pytestlintsavefix test errorlintsavefix error* fix rebase* fix* fix test* lint* lint* restore rewriteannotationops* do	3
[PY] GraphRuntime: Update the tutorials to the module-based interface (#6482)* [PY] GraphRuntime: Update the tutorials to the module-based interface.Also added document about the encouraged usage.In particular, we encourage the following usage.lib = relay.build(...)gmod = graph_runtime.GraphModule(lib["default"](ctx))I have changed most of the tutorials and apps.Some follow up PRs are needed to update some of the tests code.* Fix VTA tutorials	0
Fix warning showed with GCC10 (#7336)catching polymorphic type 'struct dmlc::Error' by value	0
Allow inplace memory optimization for different data type (#1696)	5
add softmaxoutput (#207)	1
[Relay] GNF (#2492)	5
fix gemm tutorial for env that may not have right instruction (#810)	0
[RELAY][OP] strided_slice (#2094)	5
Support match pvar with dtype constraint (#9016)	1
[Target] Fix device mask issue and typos (#9768)* [Target] Fix device mask issue and typos* Skip target hook	1
[LANG] Improve serializer (#1658)	1
[LANG] Generalize compute to tensor region (#1476)	5
[Relay][Training] Add gradient for Crossentropy (#3925)* savesaveredo max testsaveaddress commentfix* address comment* increase rtol* address review comment	1
Add all parameters to from_tensorflow docs (#3321)	2
[ARITH] CanonicalSimplifier, better folding, eliminate store. (#3464)	1
Issue fix for tiny yolo (#436)* Issue fix for tiny yoloThere was an issue in maxpool frontend for tiny-yolo. It is fixed in PR. https://github.com/dmlc/nnvm/issues/431* updated review comment, python 3 compatabilty* Update darknet.py* Updated review comment	5
[Arith] Fix floormod rewrite simplify rule (#10626)	0
[MetaSchedule][M4a] Schedule Rule: Multi-Level-Tiling (#10043)* multi level tiling* remove tensor core related code* pylint* fixCo-authored-by: Junru Shao <junrushao1994@gmail.com>	0
[Relay] add ShapeFunc for tanh (#6898)* add ShapeFunc for tanh* _schedule_dense_small_batch turn autotvm off when dense's inner dim is unknown* fix CI pylint	0
Improvements to the conda build (#2742)	1
Add tile operation	1
[µTVM] Add documentation (#7164)	2
Fix typos in runtime comments (#9726)Fix typos in comments about signal handling and entries in the runtimecode.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[Texture support][Part 0] Device API and runtime support (#7711)* Add TVMBackendAllocTexture and support in OpenCL device API.* Add runtime optimized caching allocator.This should be replaced with AOT memory planningwhen the relay/tir/compile engine refactor lands.* Few bug fixes for runtime texture allocator.* Add OpenCL device api support for image2d<float16> textures.* Update OpenCL DeviceAPI to support Image2D data spaceallocations and copying to/from host/image2d directly.Allocation employs a lowering convention to 2d imagesfor activations and weights.* Fix to follow OpenCL spec. for indexing.* Rename texture_pool.h -> texture.h* Move Nd to 2d lowering convention code into runtime textureutilities that can be shared by codegen and the runtime.* Update texture lowering utilities* Add TODO comment about pitch support* Remove FreeTexture* Fix ICHECK comment* Partial cherry pick from @ZihengJianggit@github.com:ZihengJiang/tvm.git:52822c5bd[RUNTIME] OpenCL texture memory.* Remove runtime and device texture APIs.* Add OpenCL packed functions for texture workspace (de)allocations.* Add OpenCLBuffer structure to trackmemory layout through OpenCL Device API.* Rebase: TVMContext -> Device* Implement DLTensor* overload of CopyDataToFrom in OpenCL DeviceAPI.* Implement OpenCL CopyDataFromTo(DLTensor*...)overload and tensor shapes to calculate image extentwhen copying date directly to or from texture cache.* Update format (cpp-lint)* Update format (clang)* Buffer descriptor name change and formatting.* Add texture pool documentation.* Update runtime to use new global.texture scope.* Move texture_pool.cc into opencl impl.* Add test coverage for copying in and outof storage allocs of texture scope.* Documented APIs and structures, renamed buffer descriptor layout tags.Co-authored-by: ZihengJiang <ziheng@apache.org>	2
Quick fix of VTA FPGA Toolchain Installation documentation (#3196)	2
[CODEGEN/LLVM] Initial support for codegen LLVM. (#49)* [LLVM] Initial support for codegen LLVM.* Fix the naming issue of codegen	0
[microTVM] Arduino: Fix MLF archive filename in generated project dir (#9320)* [microTVM] Arduino: Fix MLF archive filename in generated project dirCurrently generate_project API method is copying the input MLF archivefilename without renaming it to "model.tar" - hence not in accordancewith the specification. As a consequence when the server looks for thatfile to determine if it's a project dir or a template dir it alwaysdetermines it is a template dir since "model.tar" can never be found, soa TemplateProjectError() exception is thrown when instantiating aGeneratedProject class.This commit fixes that by correctly copying the input MLF archive tothe newly generated project dir as "model.tar" so the server can findit.It also takes the chance to change the MLF path returned byserver_info_query method: only if it's not a template dir the MLF pathis returned, otherwise an empty string is returned (it doesn't makesense to return a MLF path when it's a template dir because there isn'tany model associated to a template dir).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Retrigger CI	1
[TEAM] New reviewer: kevinthesun (#1606)	1
[Fix] Fix a typo in include/tvm/ir/function.h (#8617)	1
[Python] Dist wheel tools (#348)	1
[TESTS] Import script robustness (set -u) (#2896)Adopt the "set -u" idiom from the docker scripts as a mechanism toimprove future robustness.	1
allows constant param in op construct (#2257)	2
[TensorIR][M2a] Storage Align (#8693)This PR is part of the TensorIR upstreaming effort (#7527), which adds the oneschedule primitive storage_align.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[TFLITE]Quantize & Dequantize op (#5394)* [TFLITE]Quantize & Dequantize op* Testcases added* Review comment fixed	0
[Relay, TOPI]  Deformable conv2d (#2908)* [Relay, TOPI] Add deformable conv2d* Moved to op level2* Fix lint* Moved to level2 & bug fix* Update comments* Disabled flaky test of conv2d	3
[Relay] Fix Partial Evaluator, Add stricter checking for CheckWellFormed (#3749)* aot* save* save* fix test* remove vta changes* lint	4
[BYOC][TensorRT] Make TRT runtime robust to empty or weird subgraphs (#7581)* Prevent TRT runtime crash for duplicate inputs and outputs* Add empty subgraph unit test	3
[IR] Update HalideIR (#2582)	5
Add DictAttrs to IRModule and refactor DictAttrs utility functions (#8750)* Add DictAttrs to IRModuleNodeMove GetAttrs to be a member of DictAttrsGeneralize WithAttrs to work with IRModule and move to attrs.hChange func->GetAttr to func->attrs.GetAttr* lint* Fix documentation* fix typo* Another typo!* Revert GetAttrs to ->attrs.GetAttrs change* Didn't mean to revert these* Revert a few more things* Add GetAttrs to IRModuleNode	1
Extract channels from weight shape for conv2d. (#6805)	4
[IOS] Improve the iOS RPC with exclusive filesys lock (#981)	5
[COMMUNITY] @FrozenGene -> committer (#4719)	3
adding Liangfu Chen as reviewer (#1926)	1
[MetaSchedule] Handle 'warp_execution' implied extend of threadIdx.x in VerifyGpuCode (#11949)	0
Add MUL operator to relay tflite frontend (#3304)	1
[FIX] skip_conv_layers will affect quantization of nn.dense (#7795)* [FIX] `skip_conv_layers` will affect quantization of `nn.dense`* [ add ] quantization test case for dense & conv2d* [ fix ] reformat* [ reformat ] test file	2
[COMMUNITY] Bohan Hou -> reviewer (#7837)	3
[CUDNN] Add cuDNN as a Relay partitioning target (BYOC) (#10871)* [CUDNN] Add cuDNN as a Relay partitioning target (BYOC)This adds infrastructure to support offloading of Relaypatterns to cuDNN. In this initial commit, only softmaxis supported.* Refactor common TE BYOC code into separate file* Add test guard	3
typo (#5008)	2
[REFACTOR][PY] relay.op.Op -> tvm.ir.Op (#5705)* [REFACTOR][PY] relay.op.Op -> tvm.ir.Op* Improve the error check	0
[TUTORIAL] Onnx tutorial (#60)* update* back to color	5
[DOCS] Add save_param_dict, readme (#42)	2
[Hybrid Script] Support logical and/or; support 0 < a < 5 clause (#2264)	1
[TVM] Fixed SPIR-V codegen incorrectly not declaring the interface for the entry point (#1400)	1
[runtime] fix: remove anoymous namespace and rename BooleanToTranspose (#6465)	4
[FRONTEND][Keras] fix reshape (#493)	0
Fix three typos (#5620)Co-authored-by: Zeng Liyong <liyong.zeng@streamcomputing.com>	2
Expose tvm.ndarray.empty in doc. (#1125)Expose tvm.ndarray.empty which has already been implemented, just not yet documented.	2
[Vulkan][Docs] Minor updates following Vulkan target query. (#8151)- Better error messages, specifying which capability is needed.- Documentation section outlining the different target capabilities,  which vulkan properties they correspond to, and which spir-v  capabilities they are required by.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[RUNTIME][PYTHON] Switch to use __new__ for constructing node. (#1644)	1
[Arith] Simplify the output of InverseAffineIterMap (#11167)This PR simplifies the result of `InverseAffineIterMap` by assuming the `output` param has the same range as the output range of the affine transformation. For example, for iter map `i, j => i * 16 + j, i \in [0, 8), j \in [0, 16)`, after this PR, the inverse will be `m => m // 16, m % 16, m \in [0, 128)` instead of `m => (m // 16) % 8, m % 16`	2
Use the best tuner possible (#4397)* Use the best tuner possible* Add comment denoting availability of better tuners* Fix typos and wording	2
Add more cases to keras _convert_reshape (#3846)	1
[Relay][VM] Add ReshapeTensor instruction in the VM to replace the reshape op (#6089)* [VM] Add reshape tensor instruction* update* lint* fix* fix	0
[VTA][HotFix] Relay->VTA quantization fix (#4433)* relay -> vta fix* setting optlevel to 3 for quantization to fold batchnorm	1
[CI][VitisAI] Update CI Vitis AI PyXIR version (#7575)* Update Vitis AI CI PyXIR version to v0.1.6* Add --depth 1 to PyXIR clone command	1
[OpenCL] Add vectorization to cuda conv2d_nhwc schedule (#8636)* Add vectorization to cuda conv2d_nhwc scheduleAdding vectorization significantly improved performance. About 6-7xboost.* Apply comment* Move schedule to topi/gpu dir* Add vectorization to inner loop* Update values of vectorization factor	5
[CODEGEN][LLVM] Cache packed func ptr, lift alloca (#2070)	5
[Hexagon] Implement fixed_point_multiply op through intrinsics. (#12659)This commit adds high-performance implementation of fixed_point_multiplyoperation based on Hexagon intrinsics for vmpye/vmpyo instructions.Benchmarking of 'fixed_point_multiply' op with (1,8,56,56,32) inputtensor on Qualcomm SM8350:  * default implementation: 10.06 ms  * optimized implementation: 1.42 ms  * speedup: 7x times (!!!)Please note that this is introducing a small round-up error for somecorner cases with negative shift argument (The same as for ARM CPU, seePR#5980). This is because we are rounding twice instead than only once:  * original q_multiply_shift: round(x*y*2^-s)  * hexagon q_multiply_shift: round(round(x*y)*2^-s)	1
[Target] Use TargetNode::attrs for Target serialization (#5993)	1
Improve the frontend tflite _test_rsqrt test to support tflite 2.6 (#9888)Updated the test quantized graph creation to support tflite 2.6	1
[BASE] Make macro namespace angostic (#480)	1
Fix inceptionv3 (#1446)	0
[ConvertLayout] Keep span in ConvertLayout (#7895)	5
[codegen] Add multiple operands and function support when using fp16 compilation (#4056)* overload half operators for cuda codegen* add float16 te test_op_level1* fix test_op_level1.py* fix lint* disable fp16 test if gpu does not support* disable fp16 test if gpu does not support* bypass float16 test if gpu does not support float16	1
[Arith] Allow constant values in InverseAffineIterMap (#12026)	1
[RUNTIME][BUGFIX] Fix DSO module problem when its parent get destructed. (#7918)* [RUNTIME][BUGFIX] Fix DSO module problem when its parent get destructed.Previouslyw we set the context of the DSO module to be the root module.This can cause problem when the root module is not the dso module andget destructued early (but we still need the dso module).This PR makes the following change to fix the problem.- Merge multiple DSO modules to one during export.- Set the context to be the (only one) dso module.- Updated testcase to cover the problem.The enhancement creates some restrictions on the dso import hierachy(all dso modules needs to be merged without a cycle). This is the casefor our current use scenario. The merged logic is also more consistentas the library itself is merged.* Address review comments.	1
fix build issue for MSVC 2017 15.8.0 and above (#1928)	0
[NNVM]Tensorflow and Onnx basic ops (#1666)	5
[Graph memory plan] Support nested tuples (#6809)* add test* test working* uncomment other tests* remove redundant visit* test double nesting* support nested tuple in CallNode's return type* Revert "support nested tuple in CallNode's return type"This reverts commit 66225eda33f37647cfc11ceb8caa2125dfe88d0d.	4
[microTVM] Zephyr Test Refactor (#8713)* refactor host to qemu* remove unused variables* remove skip-build arg* fix microtvm test script	3
[microNPU][5] Convert Proposals to te.Schedules (#10062)* [microNPU][5] Convert Proposals to te.SchedulesChange-Id: I6771578f1007b8fea02e2dec7d0c797a6ef6aa5e* FixesChange-Id: Id062ca7793656be4e870ac48ba41a34aa83276d2* Fix testChange-Id: Ib0fd55b99459c26425e1805df19d12367244e1b0	3
MXNet pre-quantized BERT (#6039)* MXNet pre-quantized BERT* Comments.* Trigger.* Retrigger CI* Retrigger CI* Retrigger CI* Retrigger	5
[VTA][Chisel] End-to-end Inference with Chisel VTA (#4574)* [VTA][Chisel] End-to-end Inference with Chisel VTA* Update TensorAlu.scala	5
[Relay] Add space_to_batch_nd and batch_to_space_nd operators (#6477)* [Relay] Add space_to_batch_nd and batch_to_space_nd operators* Correct python-format errors* correct lint errors* tflite frontend to use batch_to_space and space_to_batch operators* Add new pad_value parameter with default value is 0 for space_to_batch_nd and correct variable names* Fix cppdocs - add documentation for pad_value	2
fix pytest (#12483)	3
[TE] Bugfix for reduction that involves multi-outs with where cond (#7692)	0
[CI] Amend docs bot comment (#11836)This PR fixes the docs bot comment message.	2
[Relay][Testing] Relay-to-Python compilation (#3156)* First pass at Relay-to-Python converter testing utility* Indicate astor as a dependency* Add astor dep to host as well* Typos and small bugs* Handle ADTs and matching in Python conversion* Remove any dependency on ast.parse* Eliminate unnecessary type var field in Python version of ConstructorValue (already gone on C++ side)* Update constructor value, fix syntax errors* Don't forget keywords arg on Call nodes* Fix some incorrect calls to ast nodes* Fix more calls, a little more cleaning up* Missing cases in attr conversion* Lower op calls instead of running them through interpreter, as in @MarisaKirisame's AoT compiler* We do still need the module* Remove changes to op attrs: Will PR separately* Smoke test and corrections* More tests and fixes* Ensure imports are properly global in generated Python code* Add unit tests for refs* Add unit test for tuple indexing* Add unit test for if expression* Remove astor dependency* Remove astor from meta.yaml too* Fix if test and add basic local function test* Add global function test, refactor earlier tests* Correct 'clause' field in ADT so Python and C++ field names match* More fixes and tests for matching and constructors* Dramatically simplify matching: no need for a thunk* Improve ref writing test* Ensure local recursion works* cleanup* Add test for global recursion* Add test for higher-order calls* Get ops working, add basic tests* Remove accidentally duplicated test* More docstrings to appease pylint* Forgot to fix a test using constructor values* Reduce optimization level in fusion and fix tuple input to operators* Test op with tuple output, fix tuple output code* Add unit test for batch norm* Add a couple more tricky test cases* Correct nat constructor to drop unnecessary field* Fix the op attrs file (accidentally reduced it)* Address review comments* Adapt to new ConstructorValue representation (no more runtime dep on module)* Use pass manager and updated interfaces. Extend module.from_expr to accommodate necessary demands* Use sequential return value* Lift out nested conditionals* Replace triple single quotes with triple double quotes* Use main variable instead of entry_func	1
Add Tensor, cleanup test, all present tests pass	4
Interpreter call in FoldConstant now always uses graph executor with link-params=0 (#10465)Addressed issue https://github.com/apache/tvm/issues/10390Change-Id: I1a6b2dd27845f9292f1e07f9da1b9be722481f46	4
[TIR] Check dynamic shared memory in VerifyGPUCode (#10923)	5
[Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. "NVIDIA", "radv", "AMD open-source driver") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `"unknown_driver_name"` if the driver does not supportquerying the driver name.	1
[PyTorch] Add aten::numpy_T (#12179)* add numpy_T* add warning* fix linting	0
Refactor AOT Test Utils parameters into object (#8650)* Refactor AOT Test Utils parameters into object`compile_and_run` was getting quite complicated to understand as well as being mostly duplicated by `comile_and_run_multiple_models`.This patch pulls out some common parameters into a data class `AOTTestNetwork` which makes it clearer what each parameter is doing and provides documentation.* Rename Network -> Model and sizebytes -> size_bytes	1
[Relay] fix doc in ty.py (#1949)	2
[Bugfix] fskip of EliminateCommonSubexpr cannot always return false (#4620)* 'fskip' will not always return falsefskip returns false at the end of PackedFunc, discards return true in 'cast' case* Update build_module.cc	5
[TOPI] Fix declaration for different dtypes (#546)	0
[DOCKER] torch install depends on future package (#4098)The torch package depends on the future package but the torch wheeldoes not expose that dependency resulting in an inconsitent install.Ideally the wheel should declare all of its dependencies, I'm not surewhy the packagers have choosen not to do this, for now the simple workaround is to explicitly install the future package.Change-Id: Ic9f0f4bb4c78ab65706fc1b20c1b4fd287856a9e	4
[CMake] use wrong flag name (#7341)Signed-off-by: windclarion <windclarion@gmail.com>	0
[microNPU] Set output tolerance of codegen and network tests to 0 (#10675)After the recent upgrade of Tensorflow to 2.6, we are now able to usereference kernels in order to verify the output. Thus, removing thetolerances previously added.Additionally, the network tests have been altered to use TFLite as areference, rather than TVM.	1
[RELAY][BYOC] Preserve type information in Merge Composite (#5640)Keep the type information when extracting patternsso that it can be used as part of 'check' functions.Change-Id: I16cc70c3d013a794d2ceefb5bec815129c7b8825	4
[CI][AArch64] Mark tests to be skipped due to torch crash (#12730)Some integration tests are not being run on CI due to theconfiguration of the machine with onnx and torch not callingthe integration tests script.This patch skips two more tests failing with the error messagebelow:```"OSError: /.../torch/lib/libgomp-d22c30c5.so.1:cannot allocate memory in static TLS block"```	0
Generate predicates for non-root iteration variables as well (#2258)	5
[MetaSchedule][UX] Make `Database` with-able (#12520)`ApplyHistoryBest` right now plays a role as the database adaptor to query inside the database.In fact, the logic could be simplified and users only have to deal with `Database` instead of thisextra object.- [x] Add `EnterWithScope`/`ExitWithScope`/`Current` to Database- [x] Migrate `te_filter_func` => "tir_filter" in Relay's pass context- [x] Migrate `f_take_tuning_record` => "Database.query_tuning_record"- [x] Migrate `TECompiler` to use `Database`- [x] Remove apply-history-bestNext PR:- Migrate `f_direct_dispatch` (potentially unify with `apply_fixed_schedule`?)	0
[Docker] Enable NNPACK for ci_gpu (#2856)	0
Docker build script robustness (#2710)* [DOCKER] Make all install .sh scripts directly executable.* [DOCKER] Use curl -L consistently.Make the use of the curl -L option in docker build scripts consistent.* [DOCKER] Drop use of --force-yesThe --force-yes option is generally not recommend, it can leavesystems in an undefined state.  The use of --allow-* options ispreferred.  In this particular case the --force-yes option appears toserve no purpose.  Dropping it.* [DOCKER] Drop superflous repeated apt-get update.The "apt-get update && apt-get install" idiom is necessary andspecific to Dockerfile.  In shell the repeated apt-get update issuperflous.  Drop the duplicates.* [DOCKER] Robustness -e -u -o pipefailThe install scripts used to construct docker environments do not, ingeneral, propagate errors.  Some of the scripts use adhoc &&directives to chain together short sequences of commands but there arenumerous failure modes which are silently ignored.  This patch puts inplace some consistent, basic, shell error trapping across all of theinstall scripts.Note this is a step forward towards more robust scripts but it is nota complete solution.* [DOCKER] Shallow clone.Use shallow clone to reduce bandwidth requirements of repeated docker(re)-builds.* [DOCKER] Use clone --branch rather than clone then checkoutUse the git clone --branch idiom rather than git clone && gitcheckout.  This paves the way for using --depth=1	1
[RELAY][MXNET][FRONTEND] add support for MXNET numpy operators (#6054)* [RELAY][MXNET][FRONTEND] add supports for OPs in numpy from mxnet* Update test_forward.py* Update mxnet.py* Update mxnet.py* Update test_forward.py* update and bugfix* test for multiple dtypes* Update test_forward.py* add data type and optimize coding style* replace pytest.skip with @pytest.mark.skipif* Update test_forward.py* update pytest style* Update test_forward.py* Update test_forward.py* Update test_forward.py* Update test_forward.pyCo-authored-by: Ubuntu <ubuntu@ip-172-31-39-169.ap-northeast-1.compute.internal>	3
[iOS] Better RPC guide and bug fix (#357)	0
Realize the function op during forward rewrite (#10410)	1
[ci][docker gpu] Install dnnl in docker GPU. (#11744)BYOC related tutorial may use dnnl  and such tutorial run at docker gpuwhich need to install dnnl to prepare the environment.	2
[TFLite] Support quantized EQUAL op in TFLite frontend (#11520)* [TFLite] Support quantized EQUAL op in TFLite frontendSupport EQUAL quantization operation conversion as part of issue #9187* [TFLite] Support quantized EQUAL op in TFLite frontendUpdate elementwise quantized test for EQUAL opChange-Id: I3897d1ac07051ebfc10356ad45397117b592f878	4
Two small fixes to AMDCPU codegen for LLVM 10+ and ROCm 3.5+ (#5920)- For LLVM 10+ we need to avoid calling Align with 0, or else  we get a crash.- For ROCm 3.5+ we need to use code object 3 (the default in LLVM 9+)  but for ROCm < 3.5 we want the code object 2.- As we want to separate codegen from the API, we need to add  a device api query for the version.  But every one else wants now one, too. (But I only filled it  in for CUDA for now.)- I'm throwing in an addition of kMaxRegistersPerBlock for ROCm.  This was introduced for CUDA in #5898.	1
[TVMC][microTVM] Add new micro context (#9229)* [microTVM] zephyr: Make platform options comply with RFC-0020Make Zephyr platform options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] arduino: Make platform options comply with RFC-0020Make Arduino platform options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] crt: Make crt options comply with RFC-0020Make crt project options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM][Unittest] Adapt test to RFC-0020Adapt test to new metadata fields accordingly to RFC-0020 specification.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] Add info() method to GeneratedProject classAdd info() method to GeneratedProject class so one can use the ProjectAPI to query options for project dirs instead of only for templateprojects.This commit also adds for the sake of convenience a setter and a getterfor 'options' in case it's necessary to set or get 'options' after aGeneratedProject class is instantiated without initializing 'options'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] Fix typo in python/tvm/micro/session.pyFix typo in comment.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Allow multiple runs on micro targetsCurrently there is a limitation on microTVM / TVM which doesn't allowrunning a model multiple times in sequence without previously flashingthe model to the device.Root cause is that RPCModuleNode class destructor is called once a runfinishes. The destructor sends a RPCCode::kFreeHandle packet withtype_code = kTVMModuleHandle to the device which wipes entries incrt/src/runtime/crt/common/crt_runtime_api.c:147:static const TVMModule*registered_modules[TVM_CRT_MAX_REGISTERED_MODULES] when TVMFreeMod() iscalled when the target receives a kFreeHandle packet.Hence when one tries to re-run a model registered_modules[0] == NULLcauses a backtrace on the host side. Probably never before a model onmicroTVM was run without being flashed just before the run, so tvmc runimplementation for micro targets exposed the issue.This commit fixes it by not calling TVMFreeMod() for system_lib_handleon the target side when a session terminates so the pointer to thesystem_lib_handle is not flushed from 'registered_modules', allowingmultiple runs on micro targets.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] Pass main parser when calling add_*_parser functionsCurrently when a add_*_parser functions are called in main.py to buildand add the various subparsers to the main parser only a subparser ispassed to the functions. However if one of these functions need to builda dynamic parser it needs also to call the main parser at least once toparse once the command line and get the arguments necessary to finallybuild the complete parser.This commit fixes that limitation by passing also the main parser whencalling the subparser builders so it can be used to build the dynamicsubparses.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] micro: Add new micro contextThis commit introduces support for micro targets (targets supported bymicroTVM). It creates a new micro context under the new TVMC command'tvmc micro'. Moreover, three new subcommands are made available in thenew context under 'tvmc micro': 'create-project', 'build', and 'flash'.The new support relies on the Project API to query all the optionsavailable for a selected platform (like Zephyr and Arduino) and alsofrom any adhoc platform template directory which provides a customProject API server.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] run: Add support for micro devicesAdd support for micro devices using the Project API to query all optionsavailable for a given platform and open a session with an specifiedmicro device. Use of 'tvmc run' with micro device is enabled via the'--device micro' option in addition to the project directory.Once the project directory is specified 'tvmc run' will make all optionsspecific to the platform found in the project dir available as optionsin 'tvmc run'. They can be listed by '--list-options' and passed via'--options'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	4
[SCHEDULE] Add store_predicate (#131)	1
[TIR] Bugfix in StorageFlatten, index flattening in PrefetchNode (#10657)This resolves a bug introduced inhttps://github.com/apache/tvm/pull/9727, and adds a test to catch thisfailure mode.  This bug occurred because StorageFlatten's visitor forPrefetchNode inserted additional pre-flattened `BufferLoad` nodesafter visiting the body of the Prefetch, preventing those `BufferLoad`nodes from being flattened.  Moving this visit to after the insertionof the `BufferLoad` nodes allows the usual buffer flattening to applyto the newly inserted nodes.	1
[CUTLASS] Add conv2d profiler (#9737)* Add cutlass conv2d profilercommit 1c0bbb297da43dab75ad995afbcacd59e9fe4c87Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:29:03 2021 +0900    fix lintcommit 463574ce087ca0444b23d4f47baf8066b8fbd3dfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:28:38 2021 +0900    fixed conv2d checkcommit 588c5abe15abbf0339a7972d81b8235bc7460620Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 15:05:27 2021 +0900    update testcommit a447b57ade7c99da4efd38e1bdfc213a64a80fd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 14:54:52 2021 +0900    speed up profiling by removing initializationcommit 93cd039ba04dd80e887ec1a71f358cd86a1e5221Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:26:29 2021 +0900    fixed nhwc cudnn depthwise convcommit 6db71727f553ee2009c9d3feb2b019c24459f4d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:39:05 2021 +0900    add cachecommit f7d17a116acd80c57dfa04aa86577fa30898908dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:05:38 2021 +0900    removed im2col profiling for conv2dcommit b724f446d07030e45f30474a1e3c124f1541b496Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:57:54 2021 +0900    blackcommit fe4687b9f6d41eff66742cdde694680538a3d5e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:49:13 2021 +0900    fixed cmd arguementcommit ab114f5c3e1a3086255caccd6c0f5df09d7c3755Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:22:19 2021 +0900    conv2d profiler workingcommit 49ee61f5583f73f0d9b2c18c1861c90fa028f64aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 20:26:15 2021 +0900    add conv2d profilercommit 49e2c8918a1a2632f60c700675f12f9d83adef24Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:03:36 2021 +0900    do not offload depthwise conv2dcommit cd8367768229720020d81d597d751bfa95ec014eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:20:01 2021 +0900    lint fixcommit 870823c6d54114896aa9db91eb72c132cdef762fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:54:38 2021 +0900    add comment on IC == 3 casecommit 6b780db7f8059a9a58bfc257fbb9cf7cb60b72a2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:48:33 2021 +0900    check align on N dimcommit 308c4dac39f761ac157f032b9fe7028820980294Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:34:42 2021 +0900    fixed check functions for fused cases, run infer type before mergecompositecommit 8d6a1bfee26e9dfdeb1ab001aa89b43b9cd33d74Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:59 2021 +0900    test IC=3 convolutioncommit ffce47de724398222f672b220156b19c8f48a700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:16 2021 +0900    use align1 kernel for unusual channel cases (IC = 3 etc)commit 6cdf205a3451b5fa7ea0cbff0228ae1da775573aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:06:56 2021 +0900    add dtype and layout check in parttern matchcommit 7743cc6dde442b45d66b4be320268ed24f352422Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:53 2021 +0900    add sm75 kernels to sm80 profilingscommit efceccb994ec71bc4ba847c791fc4f696eb7f242Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:42 2021 +0900    skip legalize when batch size is dynamiccommit 65fbc0a0813cb4e980e53240f34f3ba18754ab99Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:36:36 2021 +0900    bug fix in im2col encoding* minor fix* lint fix* allow autotvm NCHW depthwise conv2d schedule even if -libs=cudnn* Update python/tvm/contrib/cutlass/gen_conv2d.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* simplify processing profiler outputs* more simplify* fix runtime checkCo-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[ARITH] Fix the rule y < x && x <= y (#4220)	0
Enable use json for graph attr exchange (#5)	4
[VERSION] Version for v0.8 cycle (#6615)	5
codegen llvm: move nvptx-specific intrinsic handling into codegen_nvptx (#5726)See discussion in #5600.I'm also throwing in a pointer lifetime fix for the context held byNVPTX because otherwise topi/tests/python/test_topi_softmax.pywould sefault for me. With the test, I can also run resnet-18 onthe nvptx target in gpu_imagenet_bench.py.	1
Update ci_i386 to v0.74. (#9211)* Close #9158.	5
Add dtype option to verify_mxnet_frontend_impl (#1908)	1
[MetaSchedule] Fix Cyclic Dependency in PyClass Family (#10368)Following the design of module_pass, we developed a mechanism, a decorator named derived_obj, to systematically allow derivation from TVM objects in pure Python and being passed into any language, without cyclic dependency. This PR introduces the new mechanism to all PyClasses in meta schedule.	1
[BUILD] Fix osx compilation (#271)	0
Fix an issue with Upsampling and update one test to hit the broken usecase (#5530)	1
[BugFix][TOPI] Fix the integer overflow problem of the scatter_nd op. (#8415)* Fix the integer overflow problem of the scatter_nd op.* Fix scatter_nd's crash problem:1. Existing scatter_nd cuda implementation has a very large bound,   which could overflow int32 range when input tensor shape is   large enough;2. The overflow could cause the if statement always evaluate to   true, thus conducts invalid memory accesses;3. We fix this problem in this commit by reducing the bound, the   original large bound is not only unnecessary, but also degrading   the performance; With this fix, scatter_op's performance improves   100x on some cases.Co-authored-by: wenxizhu <wenxizhu@tencent.com>	1
[DOCS] Migrate some markdowns to rst, fix sphinx3 warnings (#5416)* [DOCS] Migrate some markdowns to rst, fix sphinx3 warnings* Add note block	1
[COREML]Reduceops support added to frontend (#6252)	1
[TVM] upgrade to generic schedule (#173)	5
[ci] Generate Jenkinsfile from a template (#10740)* [ci] Generate Jenkinsfile from a templateThis uses `jinja2` to generate the Jenkinsfile. This is useful since it lets us both keep common functionality easy to define (i.e. iterate over all images and do something) while keeping the output easy to debug (you can look at the `Jenkinsfile` directly instead of trying to imagine what the Groovy interpreter will do). This will become more useful as we start to make CI more configurable, such as adding dynamic test sharding.This mostly introduces the infrastructure and makes some token changes to demonstrate the generation process, but already its use is shown since the parameters was missing an entry for the `ci_hexagon` image.* Address comments, fix CI with temporary workaroundCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Frontend]Make onnx gemm tensor C optional (#7489)* Make onnx gemm tensor C optional* fix codestyle* add tests* fix codestyle	0
[DOCS] Fix scipy docs inv (#8619)	2
[Runtime] Add 'static_library' runtime::Module (#11442)(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).This adds a new 'DSO exportable' runtime module representing the contents of a .o file. Itallows external codegen toolchains to yield a result which: - Like CSource modules, can be conveyed directly to the final export_library compilation   step for linking into the final .so and saved to a know location without risk the   underlying code artifact will be lost. - Like DSOLibrary modules, are self contained so that no additional compile-time arguments   need be conveyed from the CSource module to the final export_library command lineSince this is the third flavor of 'DSO exportable' module, add a Module::IsDSOExportable.Since adding the above, can't resist also adding a Module::ImplementsFunction virtual andcalling it from TEComplier to check if an external codegen function actually provided theimplementation it promised.Note: - I've left the existing implementation of runtime.load_module alone which   relinks .o files to .so files. - Though also contained in the .o metadata, I require static libraries to always   carry their list of exported function names.This is all pretty stop gap pending a good rework of TVM to supoprt the notion of artifactsand, perhaps, build rules.	1
temp checkin	5
[TEST] Remove `llvm -device=arm_cpu` and `cuda -libs=cudnn` from (#9905)default list	4
[Relay][Parser] Support slash in identifier. (#8352)* [Relay][Parser] Support slash in identifier.Variables from tensorflow may contains '/' in name (x/y/z).* Check identifier name after parsing.	1
Add link to the reviewers	2
JNI Crash fix (#1357)	0
save (#2015)	5
Add check to ensure input file was successfully opened in NNVM deploy code demo (#4315)	2
QNN quantize and dequantize operators. (#3745)* QNN quantize and dequantize operators.* addressing review comments.* addressing review comments.* Adding new line at the end of the file.* Adhering to styling guidelines.* Adding name to contributors.* Fixing lint issue.* Fixing file name.* Removing unnecessary code.	4
[ir] use DataType instead of Type for readability because Type has been deprecated (#4513)	1
[Relay]where compute and schedule (#2179)	5
Add a check Callback to the Pattern Paritioner (#5646)* add a check callback to the paritioner* fix doc string* fix unit test spelling* add a test with types	3
[RELAY][OP] Faster-RCNN Proposal OP (#2725)* [RELAY][OP] Proposal* Fix* Fix test	3
[FRONTEND][MXNET] Use leaky by default for LeakyReLU (#5192)	1
Fix an issue with dynamic functions overwritting call arg types (#7295)* Fix an issue with dynamic functions overwritting call arg types* fix a bug for un-annotated inputs* normalize names in TypeSolver::Unifier* fix name normalization	0
Fixed issue #483, removing enum dependancy (#485)	4
[Frontend][MXNet] Change mxnet graph traversal from recursion to iteration (#2007)	4
[Profiling,VM] Profiling interface for VM and Graph Runtime (#7624)* [Profiling,VM] Profiling interface for VM and Graph Runtime* lint* fix test* make profiling test actually run* Try to better match the graph runtime function names to vm* formatting* DurationNode.value -> microseconds; PercentNode.value -> percent; make frame sorting optional.* renaming for the tvmcontext -> device change* formatting* remove old vm profiler get_stat api* fix tests	3
Fix a bug in batch_matmul that te.max should be used (#7111)* Fix a bug in batch_matmul that te.max should be used* Additional fix to batch_matmul- add to this PR, https://github.com/apache/tvm/pull/7111* Remove previous change for numpy test batch_matmul* Add test to dynamic batch matmul* Fix what clang-format flagged* Skip dynamic batch matmul test on cuda- error is found during test as shown below``` File "/home/jojo6174/tvm-installation/tvm/src/tir/analysis/verify_memory.cc", line 202RuntimeError: Memory verification failed with the following errors:```* Skip dynamic batch matmul test on nvptxCo-authored-by: Insop Song <insop.song@gmail.com>	3
[microNPU][2c] Add performance modelling to cascader (#9778)* [microNPU][2c] Initial Performance Model* Added the pre-computed performance modelling per block.* Added the aggregation of cycles given a stripe config.* Implemented the op-specific performance code for conv2d.* Created a DeviceConfig class to hold constant performance related datathat is dependent on the accelerator configuration* Added generation of all valid block configs. This is pre-computed andgiven as an argument when constructing EthosuParts.* Implemented selection of the block config that gives the least amountof data read given a StripeConfig.* Add test guards* Extended block config testing	3
refine error (#5929)	0
[tvmc] Fix inconsistent usage of host_name -> hostname (#8324)* This prevents a python error when running tuning via   and RPC tracker on tvmc. * Add test case	3
[Hexagon] Account for objects being smaller than the allocated space (#9769)* [Hexagon] Account for objects being smaller than the allocated spaceIn particular, graph executor will reuse allocated buffers for varioustensors. These tensors may be of various sizes as long as the bufferis large enough to hold them.* Trigger CI* Add #include <algortihm> to hexagon_buffer.cc* Update cpptest, plus fix one CHECK in hexagon_buffer* Rename `offset` to `copied` for consistency	1
add missing inline (#910)	1
Fix map assign issue in CI test (#5854)	3
Fix direct and broken links (#9314)Updates links to use references instead of direct links, fixingbroken links and making all internal docs links more durable torefactoring	4
proper device query through rocm api (#4305)	5
[Relay] Parser CI dependencies and build rules (#1965)	5
fix cuda half math function is undefined: hpow, htanh (#6225)	1
[microTVM][ARM]Add tests for arm schedules (#11472)* add more tests for arm_cpu schedulesconv1d_ncw, conv1d_nwc, conv2d_NCHWc, depthwise_conv2d_NCHWc, dense_dsp, avg_ pool and max_pool tests are added.Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
[TEAM] vinx13 -> Reviewer (#2083)	5
[MODULE/REFACTOR] Introduce Module for AOT and runtime linking. (#51)	2
Added a regression test for #696 (#720)	3
Fix int8x4 broadcast value codegen in cuda (#1959)	0
[REFACTOR]  top - namespace for Tensor Operation DSL (#4727)* [REFACTOR] introduce top - Tensor Operation DSL.Historically we put Tensor, Schedule and compute under the root tvm namespace.This is no longer a good idea as the project's scope grows largerthan the tensor operation DSL.This PR introduces top -- a namespace for tensor operationalDSL concepts such as schedule, tensor, compute.We moved the related files to the new top subfolder.* Move relevant files into include/tvm/top and src/top	2
[TEST/CI] 32bit compatibility and CI. (#159)	3
[BUGFIX][ARITH] Fix FloorMod Simplifier (#10336)* fix canonical simplifier* improve comments	1
[CI] Updated argument parsing of optional arguments in ci.py (#10906)* [CI] Updated argument parsing of optional arguments in ci.pyPreviously, optional arguments were identified by comparing the string`"typing.Optional"`.  This misses some cases, as `Optional[int]`expands to `Union[int, NoneType]`.  This commit updates the check toidentify `typing.Union` annotations where one of the types is`NoneType`.* Bugfix, correctly handle type annotations outside of `typing.*`	0
[Arith] Simplification of ceil, log2, and left_shift (#11646)* [TIR] Simplify expressions using tir.ceil and tir.log2These expressions are introduced in `topi.math.ceil_log2`, and canotherwise be propagated through to the generated kernel.* [Arith] Added left shift handling to ConstIntBoundsAnalyzerPreviously, only right shift was handled.  These left shifts areused in the `cuda.sort` implementation.* Update to avoid left shift of negative numbers* Updated rewriting of log2(x) to only occur in ceil(log2(x))Per @wrongtest's request, to avoid rounding differences betweendifferent devices.* Avoid assumptions made of negative arguments to left-shift* Recognize bounds of int(ceil(log2(arg)))	2
[Coreml] Fix Coreml Input Shape Handling (#8562)* convert ot python list like expected* test example* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[Ansor][AutoTVM v2.0] Phase 1: The base class for cost models (#6187)* add the base class for cost models* address comments* Update tests/python/unittest/test_auto_scheduler_cost_model.pyDisable test if user doesn't have llvmCo-authored-by: Zhao Wu <zhaowu@apache.org>	1
AutoTVM: selecting tuning templates when extracting task (#4338)* AutoTVM: selecting tuning templates when extracting taskMake the procedure of trying new templates easier.Test: tests/python/relay/test_autotvm_task_extraction.py* Use dict to match key for topi ops* fix lint issue* be more pythonic :)	0
[Relay][Topi][TensorFlow][ONNX][Lang] Add support for Any op (#4205)* Add support for Any op* Support ONNX frontend* Add doc* Add to relay docs* Dummy change to retrigger CI	4
[DOCKER] fix sphinx install versions (#8316)	0
Rename dim_var to axis, update testcases	3
[DOC] Using External Libraries in Relay (#2694)* added relay quick start* added relay/using_external_lib.py* update using_external_lib* Update using_external_lib.py* update tvm/make/config.mk -> cmake/config.cmake* Fixed: result mismatched when lowering relay with cudnn support at opt level 2* setting opt_level=2 and out_channels=16 for consistency of original tutorial* Fixed some typos	2
Add the equivalence of graph_runtime.py in tvm_runtime.js (#950)	1
Fix dynamic batching when use_implicit_batch=False (#8461)	1
[ONNX]fix datatype on Reciprocal op (#7519)* fix datatype on Reciprocal op* clean up test case	3
Update TVM to latest (#432)* Update TVM to latest* remove darknet from testing due to cffi	3
fix deploy_model_on_rasp.py spell error. (#2491)* Update deploy_model_on_rasp.pyspelling fix.* Update deploy_model_on_rasp.pytrigger CI	5
[frontend][tflite] float16 quant support (#7736)* [frontend][tflite] float16 quant support* remove skip conditions in tests	3
[TIR] Tir constants integration into compilation pipeline (#8509)* [TIR] Introduce tir.allocate_const to TIRThis PR is adding non-scalar constant representation in TIR. This is used toexpress constants (i.e., parameters) in the TIR instead of bypassing theTIR as it's done until now.Change-Id: Id3afc4d7197260cb43ecde60f05ccbce3fc42430Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: Id4a09a637c9c1fd7d49989c6c10f474a78569e18* [TIR] Integrate tir constant nodes in compilation pipelineThis PR integrates tir.allocate_const to the compilation pipeline to support --link-params.Change-Id: Ic8d0cb75d596299fcae7078b304598afbf0c5494Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: Id98cc682bbfacfe75c4d8b260fd41658f1f196b2* [TIR] tir.const extractionThis commit tries to implement an amendment to tir.constant RFCwith centralized storage of constant data within the IRModulePlease note that data and irmod_storage_idx are not mutual exclisivefurther more the irmod_storage_idx is valid only immediatly afterprim func addition to the mod or after update within the mod.If prim func is out of the the module scope then the index becomemeangless. irmod_storage_idx also is not used in calculation of hashfunction of the tir.constant node.Change-Id: I40742ed580468b0252ea3fec02184cba65e20871* unit test fixedChange-Id: Ied2186554d4cbad44b2346216c8be92449e55732* cmsis-nn codegen fixNow handled case when params of the functions came as constantsChange-Id: I5874e182e34ef94e23048eaf3c61b01a56d91131* Fixes for unittestsChange-Id: I5b82ee3f80337155706b5470973f494a301b5d90* Rebasing tests fixesChange-Id: I94ac87907081bab53c1dd1ab2db106ae057b4b19* Linter: added method param descriptionChange-Id: I2f8c4c8d244b74c794abaa6079c46cc593ffcbdb* Printing removal fixThis patch removes forgotten print in fuse_opsChange-Id: I4bb5934f3b4cd5fde19d36a8e3319aae136bce8a* BugfixFixed concurrent map update bug hereChange-Id: Ifec3bf5030086d9079b9e493096f17dfd82297ec* Reworked logic for not to introduce empty constant list to modue attrsChange-Id: I082c85b3b4b70c218f0d714f5613ef6e178bd020* Added support for tir builtin::tvm_access_ptrThis fixed unit tests for tests/python/integration/test_arm_mprofile_dsp.pyChange-Id: I10919f301ef9ddc3fd87f0e1a8414e9a52fc7938* Unit test fixFixes unit tests in torch frontendChange-Id: I6c179834f93dd202605d1ce5a7f07d987b9dc469* Addressed requested changesAddressed changes requested upstreamChange-Id: I741e52b89eb285732c23b1ac7ff277e757a088c3* Namespace usage changed to conform earlier C++ standardChange-Id: I1b29238cfe2a6bedb525f4f823a3a540f631d836* BugfixChange-Id: I57a44b714b307278a243817ec2864e53ad31366b* updated IRModuleNode::ExtractPrimFuncConstantsUpdated IRModuleNode::ExtractPrimFuncConstants as perrequest upstream.Change-Id: I35db0145fb5827efd0445ce665d0c99465274016* Minor changestypo fixdrenamed ExtractPrimFuncConstants to ExtractConstantsremoved getters/setters from FuseMutator and added parametrizedconstructorChange-Id: Ib2326805781779b88c963a8642ff683c8755956e* Moved LinkedParam/LinkedParamNodeMoved LinkedParam/LinkedParamNode from tvm::tir namespace to tvmnamespaceChange-Id: Ie3f0303bd4f7890c6d680268c91f2051977bc7f4* Addressed upstream commentsChanged BindParams argument to Array<NDArray>Removed 'name' argument from te.constSwitched to in-depth comparision of NDArrays in constant de-duplicationRemoved extra final comma from NDArrayToTIRChanged return type of ConstantAllocationSize to int64_tMade link_param a tvm.testing.parameter for test_fuse_take and test_fuse_gather_ndChange-Id: I4285099cc63756aa5ebe91a5bd207d4135499b41* Removed unnecessary forward declaration+linterChange-Id: I2a6c0d1f97773aeb1ae3f458da252a22079ccdb1* Constant extractor now is a separate passChange-Id: Ia4adca9d3315b26fbdc006ef7c115900c081e303* Added forgotten file + unit test fixChange-Id: Ice305f4fefd13fe95e97574e6d63ffeb664621df* Changed to IRModule passRefactored ExtractPrimFuncConstants to IRModule pass.deDup -> DeDupRefactored logic of Applicator supplementary classChange-Id: I6c120d175eb6790ba90f176c4f856bde8f0c7c94* bugfix after rebasingChange-Id: Ie3ee6ea2479476a30f486baef74f20070f117942* -v -> -vv to have more debug informationChange-Id: I12c63731663b9c9ea574b9ed5cb17311ba3cf701Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>	5
[TIMER] Enhance time evaluator to create multiple results (#830)	1
[Codegen] Swap out analyzer when outlining (#9117)Problem: the `analyzer_` in `CodeGenLLVM` and derived classescan generate invalid code for outlined functions.Consider code like this:  let x = y in    // attr compute_scope    blah = xThen it gets outlined in codegen_cpu (for example):  let x = y in    call foo(x)  foo(x) {    blah = x  }Now, if `analyzer_->Simplify` was run on the body of `foo`, itwould produce:  foo(x) {    blah = y  }Because the `analyzer_` knows that `x` is same as `y` (becauseof the `Let` statemement), but doesn't know that `y` is no longeravailable in the outlined function `foo`.Seehttps://discuss.tvm.apache.org/t/compute-scope-issue-with-analyzer-invalid-simplification/11111	0
[CI] Introduce all platform test for windows/mac/linux. (#6756)This PR introduces a minimal set of test cases thatare supposed to run in all platforms during CI.The set of testcases are supposed to help onplatform dependent regression.See tests/python/all-platform-minimal-test/README.md for guidelines.- Enable windows mac LLVM build via conda with cython support.- Test on all platform test cases.- Update implementation to improve MSVC support.	1
add var binding for expr	1
[TIR] Asynchronous stage in software pipeline (#12171)* [TIR] Support asynchronous stages in software pipeline transform* Support interleaved async producers separated by a consumer* clean up* adding doc* adding doc* simplifying* make wait count computation a two pass process* commit_stage -> commit_queue, wait_stage -> wait_queue* make async_commit_queue special scope stmt* codegen async_commit_queue in cuda* clean up* clean up* Move block predicate outside of commit_queue* updating test* test updated* changed async_wait to an annotation* update doc* update meaning of software_pipeline_async_stages* update test* fixing codegen* more fix* remove one of tests that have async and sync ops in the same stage* format* lint and other fix* Define attr::software_pipeline_async_stages* populate wait count in a separate function* fold variabel consumed into AsyncStateLocal* introduce CompletePipelineLoopStatements function for further refactor	4
[ARITH] More aggressive CSE during canonical simplify (#166)	5
checkin basic cpp test	3
[Ansor][FLAKY] Bug fix for compute at mutation error (#6557)	0
[LANG][ATTRS] Enable deep equality comparison and hash of Attrs (#1903)	0
[Frontend][PaddlePaddle] Enhance paddlepaddle frontend with more operators (#9724)* add operators for paddle frontend* add operators for paddle frontend* retrigger ci* retrigger ci* retrigger ciCo-authored-by: wjj19950828 <wjjisloser@163.com>	1
[CI.Lint.Black] Use "en_US.UTF-8" for Red Hat 6&7 Compatibility (#9537)	1
Lazy import XGBoost (#6939)	2
[Diagnostics] Add environment variable for controlling top-level printing and fix issue with pretty printing/parsing roundtrip. (#6874)* Update Parser in order to handle the NMS code* Add support for displaying traces optionally* WIP* Fix* Fix error reporting in parser and clean up __init__.py due to CR* Format* Quick fix for If* Fix format* Fix lint	0
[LLVM] Avoid warnings when compiling getNumElements with LLVM12+ (#6738)* [LLVM] Avoid warnings when compiling getNumElements with LLVM12+Extract the element-count code into GetVectorNumElements and make itcompile cleanly with all LLVM versions.* Trigger another build	4
[AlterLayout] Respect input layout for dense op if explicitly specified (#9535)	5
Conv2d updated  (#435)* improved conv2d for last group of workloads* conv2d_nchw improved on 14_256_256 and 56_64_128	1
[APP] improve parameter pack (#645)	2
Support more dtypes for TVMDSOOp (#5694)	1
Support FoldScaleAxis for depthwise convolution (#1664)	1
Update code_review.rst	5
[CMSIS-NN] Re-use CPU Target Parser (#12320)Previously `CMSISNNFlags` was derived using logic specific to the external code generator, this converts the external code generator options into a `Target`.	1
[DOCS] Fix the QNN TFLite tutorial build (#5641)* [TUTORIAL] Fix execution error of TFLite quantized tutorial* Assign TensorCore to docs build	2
Revert "fix cuda half math function is undefined: hpow, htanh (#6225)" (#6249)This reverts commit ed04cdd35f1990959ec788be0131b1388fd11d31.	4
[IR] Unify approach to Visitor/Mutator under Functor (#4606)IRMutator and IRVisitor were the main data structures for doing low level IR visiting.As the project evolves, we start to introduce more powerful variants such as StmtFunctor and ExprFunctor.This PR brings new classes that allows us to migrate the visitor mutator to be sub-class of these functors.List of changes:- Create separate class for ExprMutator and StmtMutator, following convention used in relay.- Introduce copy-on-write to StmtMutator that can later benefit the statement mutations  if we use move semantics and keep a single copy of stmt.- Move two generic visit mutate util to use the new classes.We will send followup PRs to migrate the existing passes that use the legacy visitorsto the new one.	1
[MetaSchedule] Enhance AutoInline for Spatial Task (#11996)Previously, Auto-Inline on CPU will only inline according to strictconditions, for example, ordered index mapping. This is generally goodpractice to do so, but on the other hand, there is no much benefit tostop inlining only due to some restrictive conditions for pure spatialsubgraphs. By doing so, we also save some search trials on pure spatialsubgraphs so that more can be allocated to more important ones.	2
Update to 0.10.0 (#12190)This updates the version numbers after the v0.9.0 release and adds a version selector option for the v0.9.0 docs.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
don't rely on cudnn for compilation (#10495)	5
update nnvm-fusion (#82)	5
[Torch] Reduce testing time of LSTM tests (#8583)* reduce testing time* lint issues were resolved. weights for test are always randomly generatedCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
Fix typo in err msg (#4251)	2
[CI] Cleanup logfile before tutorial runs (#4896)	1
[VTA] Improved RPC for VTA (#2043)* assign default port to 9091 as the documented* bug fix in printing RuntimeError and add an additional search path* pretty print rebuild runtime args* PRC => RPC* replace vta_config.json file path`build/vta_config.json` => `vta/config/vta_config.json`* undo the change in adding lib_search path* search vta_config.py file in vta/config* avoid exposing driver function calls, and use predefined `VTAMemGetPhyAddr` instead.* rename `tests/hardware/pynq` => `metal_test`* set config path back to `build` dir	5
[BUILD] add target_host to compiler.build (#240)	1
[ONNX][Converter] Fix when onnxoptimizer is unavailable (#9700)	0
[Torch] Add index_put operator (#7465)* [Torch] Add index_put operator* Skip test_frontends.py::test_load_model__pth	3
[Pytorch] add aten::rnn_tanh, aten::rnn_relu (#12017)* emptycommit 2nd try* dev* comments* format* formatCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>	1
[PASS] StorageRewrite, Memory optimization pass as in NNVM. (#104)* [PASS] StorageRewrite, reuse memory pass as in NNVM.* fix issue	0
[TOPI][Hexagon] Implement quantized avgpool (#12340)* [TOPI][Hexagon] Implement quantized avgpool* Fix pylint errors* Needed to adjust input padding for int8 buffer layout* Fix formatting issue* Add unit test for fixed-point conversion utility functionAlso, address review comments.* Remove pytest.skip for test_avg_pool2d_slice.py to enable on-target testing* Fix formatting issue* Update python/tvm/topi/hexagon/utils.pyCo-authored-by: Christian Convey <christian.convey@gmail.com>* Update comments and error messages* Address review comments* Import Tuple from typing* Address pylint errorCo-authored-by: Christian Convey <christian.convey@gmail.com>	0
[RUNTIME] Improve signal handling in python env. (#7919)* [RUNTIME] Improve signal handling in python env.Python execution environment handles the signal by cachingthe signal a state and invokes the handler when executiongoes into the python interpreter.This model can cause problem when runnning a long runningc++ function. As keyboard interrupt can only be caught in the end.Additionally, because python registered special signal handlers.Socket operations can return EINTR that needs to be explicitlyretried when the interrupt is not a KeyboardInterrupt.This PR adds the following changes to resolve these problems.- Allow execution env(python) to register CheckSignals function  to the TVM runtime.- Add runtime::EnvCheckSignals to check the signal error.- Add retry when EINTR is encountered in socket.- Register the python C API functions in cython mode.To testout the EnvCheckSignals, run the following code```pythonimport tvm.testingtvm.testing.run_check_signal(10)```Note that the C API functions are only registered in cython FFI modebecause ctypes have problems invoking these functions. This howeverwon't affect the correctness, but will defer the interrupt handlingto function return sites.Co-authored-by: Andrew Reusch <areusch@octoml.ai>Co-authored-by: Robert Kimball <bobkimball@gmail.com>* Address comments* Alternative implementation that preserves python exception.* Address comments* Update check signalsCo-authored-by: Andrew Reusch <areusch@octoml.ai>Co-authored-by: Robert Kimball <bobkimball@gmail.com>	5
[ci] Clean up mergebot commit messages (#11437)* [ci] Clean up mergebot commit messagesAdds both bullets and closes #11433* Fix error from pr #11442Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CI][DOCKER] Install blocklint for identifying non-inclusive language (#11128)Installs blocklint in the ci_lint Dockerfile	2
Improve AArch64 depthwise convolution through smlal/smlal2 intrinsic (#6711)* Improve depthwise convolution through smlal/smlal2 intrinsic- Added an intrinsic to load a single int16x8 vector and produce two  int32x4 output vectors through smlal/smlal2 instructions- Changed the NHWC depthwise schedule to accomodate the aforementioned  intrinsicChange-Id: I347c3bf98fa8dd87057304dcda0d78e558424c57* Address review comments* Rebasing - 2* Rebasing - 3* Rebasing - 3* Fix linting	0
[TOPI][CUDA] Add reorder option in int8 conv2d (#2327)	1
[Rust] Restore the Rust CI testing after Docker image update (#8657)* Fix Rust CI* Turn Rust CI back on	0
Combine unit and integration test steps into one stage (#9733)This removes the barrier wait between test and integration tests in CI. This will increase capacity requirements and usage but, assuming we can meet that with autoscaler, should reduce CI times by an hour or two since we're doing all the testing in parallel.The slow path is CPU unit test -> GPU frontend tests, so kicking off the GPU frontend tests faster should help decrease CI runtime.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Remove duplicate as Checks and CHECK value (#2531)	4
[TVM] Fix negating undefined in DetectLinearEquation (#1816)	0
[QNN] Support input scale and zp of 1-element vector in qnn.conv2d_transpose (#10952)* Support input scale and zp of 1-element vector in qnn.conv2d_transpose* Lint	1
[TOPI][AUTOTVM] Improve style (#2034)* [TOPI] Improve the style of using autotvm* fix	0
[AutoScheduler] Fix the occasional crash caused by split memo (#6883)	1
Add SGX to docker (#1822)	2
[TE] Fix MakeLoopNest for warp memory (#5382)	1
[FRONTEND][TFLITE]Gather, StridedSlice op support added (#4788)* [FRONTEND][TFLITE]Gather, StridedSlice op added* Review comments fixed	0
[TIR] Remove ProducerConsumer and AllocateNode::new_expr (#5333)* [TIR] Remove ProducerConsumer and AllocateNode::new_exprThis PR removes two legacy IR parts in TIR that are deprecated.ProducerConsumer node only serves as a hint markup and may no longer beinformative after extensive transformations in the pass.If necessary, we can add related info via AttrStmt.The new_expr field in the AllocateNode is deprecated since it can just bereplaced by a LetStmt.- Remove dependencies of passes on ProducerConsumer.- Remove ProducerConsumer from the IR.- Remove the deprecated fields (new_expr, free_function) from AllocateNode.* Fix additional testcases	3
[TIR] Avoid `import *` in TIR tensor intrinsic registration (#12424)	2
Add AVX512VNNI support for TVM (#3388)	1
[Relay, TOPI] Add negative log likelihood loss (nll_loss) op (#8056)* add nll_loss* enrich the doc and rename parameters* update upon review* add tests* update based on reviews* update upon reviews* update upon reviews	5
[apps/bundle_deploy] Link demo_* targets with LDFLAGS and also with -lm. (#6636)	1
Qnn fully connected (#3910)* Qnn Dense layer.* Reformatting code.* Reformatting code and making the test case more readable.* Fixing lint issues.* Fixing test method names to pass the nose related configurations.* Aligning the code for code style.	5
Add normal distribution to random engines (#1352)	1
[Frontend][Tensorflow] SelectV2 and BroadcastArgs op support for tf2 models (#7901)	1
[MetaSchedule] Distributed Measurement (#11683)This PR includes the distributed measurement of tuning candidates using builder and async runner, as well as some auxiliary functions. It enables multiple builders and multiple runners with a tracker connecting in between. The hierarchy of files in the database can be further compacted to make the database more concise.	5
[TIR][REFACTOR] ForNode introduce thread binding and remove legacy field (#7306)[TIR][REFACTOR] ForNode update- Remove deprecated device_api.- Add ThreadBinding for_type.- Add additional annotations.More style consistency refactor to make the ForNodeto be consistent with rest of the codebase.- ForType => ForKind- Add constant prefix k to enum consts per Google C style- Introduce ForKind to the python side.	0
[RUNTIME] [OPENCL] Fix access modifiers (#1643)	0
[Relay] DQN Port (#2009)	5
[CPP] Refactor remove tvm/tvm.h (#3523)	4
Syntax error String::fromwe() should be String::from() (#6846)Co-authored-by: Mikael Sevenier <mikael.sevenier@sima.ai>	0
[CODEGEN] fix vector conversion for opencl (#783)* support more argument type in depthwise_conv2d* mark all pointer as 'restrict' & fix vector conversion for opencl	0
[RUNTIME] Better error message in cuda launch (#513)	0
[TOPI][Relay][OP] Add a strided_set operation. (#4303)	1
[ARM] Fix NCHWc int8 dot product schedule lowering (#10773)* [ARM] Fix NCHWc int8 dot product schedule lowering* fix arm task extraction test not running* skip test on i386	3
[ARM CPU] Fix infer shape error of depthwise (#4384)* [ARM CPU] Fix contrib_spatial_pack error* PyLint error fix* diable no-else-return as other files* Change the test case split OC not be 1 to cover 5D weight layout	3
[MetaSchedule] bug fix ApplyHistoryBest. Previously, ApplyHistoryBest returned the incoming module without applying the tuning history. (#10183)	0
[TensorIR] Renormalize split pattern (#10401)	5
Oneflow fronted support more model and fix bug (#11321)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* restruct oneflow fronted* support vision-transformer* black format* update black version and reformat* fix ci error* fix doc error* fix gpu fronted test failedCo-authored-by: hhhfccz <hjk1938927583@163.com>	0
Add order to functions in C Codegen (#10590)* Add function ordering to C Codegen* trigger* fix comment* address comments* add test* add unorder check* fix test* address comments	1
Factor out import of common tflite.Operator in tflite frontend. (#5355)* Restructure imports in tflite frontend.These python modules are needed for every tflite file parsed.Factorize out imports of the common most ones.Now that the import of operator is common, asserts can be commonized.Loses 473 lines of duplication.* Only restrict to tflite.Operator	1
[Hexagon] RPC server/client for simulator (#10361)This is the C++ code for running Hexagon code on simulator via theRPC mechanism. It is intended to be integrated into the currentHexagonLauncher, although the integration will require further changesto the launcher python code.The final goal is to be able to run the same file.py on eitherhardware or simulator without needing to edit the python file, butsimply by changing the configuration of the execution platform(i.e. something like --exectute-on=simulator as a command line orin an environment variable). The exact details are still to bedetermined.	5
[SPARSE] Improve sparse performance on ROCM (#7935)* [SPARSE] Improve sparse performance on ROCMThe current sparse dense gpu kernel uses warp level storage to handlingcaching of data. Warp level storage uses shuffle intrinsics, which areslow on rocm (because they actually read and write to shared memory).Rocm does provide intrinsics to do the correct memory management, butthey are not available through tvm. Instead this PR switches to usingshared memory on rocm devices. Performance is about 2x faster.* default to shared mem* formatting* formatting	1
Update ISSUE_TEMPLATE.md	0
Fix Metal accuracy problem caused by <dtype>3 vectors usage (#7830)On example of float3 datatype:Using of float3 data type for loading of data cuncurrently into dense array sharedbetween all threads in Metal threading group can lead to data race between threads.float3 datatype has size and and alignment eq to 16 bytes while kernel assumes tocopy 12 bytes in arbitrary not aligned places.Using of packed_float3 datatypes solves the issue	0
Deploy the Pretrained Model on Jetson Nano  (#11037)* Create deploy_model_on_nano.pyadd deploy_model_on_nano.py* Update deploy_model_on_nano.py* fix doc build bug* Update deploy_model_on_nano.py* fix ci error* Update deploy_model_on_nano.py	5
[AutoScheduler] Improve the rule of mutating parallel granularity (#6568)* fix mutate parallel* fix comments* fix lint* fix tutorials* update* fix tests* address comments	1
[Relay, ONNX] Support gather_nd batch_dims attribute for TF/ONNX (#8084)* Add GatherND batch_dim support* adding tests* test working* improved reference code* refactor ref func* batch dim 2 tests from tf all passed* batch_dim -> batch_dims* add example* minor change* add onnx test* fix onnx version* fix lint* remove move on batch_dims* fix pylint* fix compiler warning* add shape constraint for batch_dim and update doc* make the output shape doc clearer	2
[skip ci] Disable flaky test `test_empty_like` (#11968)See #11967Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[FRONTEND][Keras] Add test for MobileNet (#513)	3
[PatternLang] Simplify Pattern API Implementations (#5703)* Add syntatic sugar; include pattern to API docs* fix doc warnings	2
[Torch] Fix cast to long (#6301)* [Torch] fix cast to long* retrigger	0
[BYOC] Support control flow in annotate_target (#6641)* Change annotate target* Annotate_target* Revert namespace changes* Add tests for if-else node* Add while_let testcase* No merging in ifelse* Remove scope builder* Add ops* Replace < with less* Linter* Pass Tests* Change back to static const* Cpplinter* address PR comments'* PR Comments* Clang-format check* PR Comments* PR Comments* Change back to Insert Ann in AnnotateARgsCo-authored-by: Ritwik Das <dasritwi@3c22fb14d7c6.ant.amazon.com>Co-authored-by: Ubuntu <ubuntu@ip-172-31-3-223.us-west-2.compute.internal>	4
Conditional Loop Partitioning - Extending to remove if conditions (#1797)	4
[CI] Update MxNet to 1.6.0 with MKL (#5240)	5
properly extract error type from windows error message (#4780)Co-authored-by: Jon Soifer <jonso@microsoft.com>	0
[TIR] add support for multi-blocking layout and their transformation (#9996)* add ceildiv and shapediv* add boundary checking in layout_transform* support multi-blocking and shape padding* refine the log for shape transform* add test for multi-blocking layout transform* delete unwanted comments* remove workaround* fix lint errors	0
[vulkan] Add integer dot product (4xint8, 4xuint8) tensorization for the vulkan SPIR-V target. (#10391)* [Vulkan] Add cmake change for spirv dot product* add spirv and vk runtime change* add back conv2d int8 related change* add back dense and group conv2d change* add back test_topi_conv2d_int8.py change (but don't test on vk)* check dot prod availablity in batch matmul schedule* do not run uint8 tensorization on arm* add vulkan target to conv2d int8 test but comment out on CI* do not run vk batch matmul test* Fix performance regression due to missing of constant folding in the index expression.Co-authored-by: Masahiro Masuda <masahi129@gmail.com>	0
[FIX,AUTO_SCHEDULER] Handle manually unrolled loops in auto scheduler features (#11166)For multiple statements in a loop, add flops for each statement insteadof only using the last statement.	1
[Relay][Topi]Add Sort Op to Relay (#6978)* Add sort op to relay* fix lint* fix sort docstring* fix docs* add TODO, shape_func, cleanup* add dynamic tests for sort and argsort	3
[DLPACK] Enable cython support (#1589)	1
[ci][docker] Remove Docker image upload prefix (#11769)These should go to the same tag as in `tlcpack` so the only difference is the user	1
Update CONTRIBUTORS.md (#9804)	5
[WIP] Add OpenGL topi. (#836)[TOPI][GL] OpenGL topi.	1
[Relay] Add Python type functor and tests (#4209)* Add Python type functor and tests* Lint roller	3
[RELAY][TypeSystem] Add support for populating type args (#1962)	1
[TVM] Fix llvm codegen (div by power of 2) (#2204)	0
[Support] Fix StartsWith when the string is equal to the prefix (#9393)	0
[COMMUNITY] @cchung100m -> reviewer (#4557)	3
[ETHOSN] Add support for non-default Ethos(TM)-N78 configurations (#9386)- Updated tvmc with new Ethos-N78 composite target.- Added additional Ethos-N78 specific configuration options.Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>	5
[Relay][Quantize] Integrate data-aware calibration into quantization (#4295)* [Relay][Quantize] Integrate data-aware calibration into quantization* Update _calibrate.py* trigger ci* Address comments* address comments	1
[UTILS] Move target to tvm; rename convolution as conv2d (#492)* Move target to tvm; rename convolution as conv2d* Fix* Fix	0
[Docs] Added developer documentation for DeviceAPI and Target. (#8082)* [Docs] Added developer documentation for DeviceAPI and Target.* [Docs] Update on the DeviceAPI/Target documentation.- Clarified wording based on suggestions from @csullivan- Fixed incorrect links to `c_runtime_api.h`* [Docs] Update on the DeviceAPI/Target documentation.- Switched from argument style example of `tvm.target.Target` to a  JSON-formatted string, based on @zxybazh's suggestion.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[DOC, EXAMPLE] Updated READMEs, tests, etc. (#41)* bug fix for new drivers in new PYNQ image v2.1* updating instructions for resnet inference* updated the instructions for starting the RPC server* deriving host/port from env for unit tests	3
Fix warnings from Logging, enable Plan memory to take external memory (#93)* Fix warnings from Logging, enable Plan memory to take external memory* fix external memory id* fix graph	0
[TIR] Fix buffer scope in structural equal (#8768)* fix buffer scope in structual equal* make global equal to empty	1
Fix executor for different compilers (#8006)* Fix executor for different compilersAt the moment compiling this file throws multiple errors with C++ compilers, this change proposes to fix them.1. `tvm_model_t->run_func` of type `TVMBackedPackedFunc` returns an int at the moment which is different from the signature of this function `tvm_runtime_run`, implicit casting is not favorable in many compile chains and throws errors.2. The index of iterators were of type `int` while that of `model->num_input_tensors` and `model->num_output_tensors` were of type `uint32_t`, this type difference again throws errors in many toolchains, and can potentially cause incorrect calculations.3. C Style struct initialization of tensors with `(DLTensor){...}` is not supported in many C++ toolchains and throws “non-trivial designated initializers not supported” error. Explicitly setting values should work in all cases even though it looks a little less nice.* changing type to size_t* fix format for clang	0
[relay][vm] Separate VM runtime with executable (#4100)* [relay][vm] Separate VM runtime with executable* Address comments* move ctx back to vm* make only vm related fields and methods protected* integrate seriliaztion/deserialization to executable* create stream	1
[Runtime] Add graph_executor get_input_index API. (#8633)* [Runtime] Add graph_executor get_input_index API.In graph_executor use case, user can use set_input withinput index to set input parameter, but there is no straightforward way to get correct index number with input name, hereprovide get_input_index API to do such work.* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update src/runtime/graph_executor/graph_executor.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
[AOT] Calculate used memory at the callsite of primitive functions (#11208)* [AOT] Calculate used memory at the callsite of primitive functionsIntroduces a new pass in the AOT executor called "AnnotateUsedMemory"which applies liveness analysis to the callsite of each primitivefunction in order to calculate the total size of the live tensors atthis point of execution. The result is provided as a function annotationcalled "used_memory", which can be consumed by later stages of thecompiler (e.g. external codegens) to provide more information about thecurrent memory consumption. This can be useful for some optimizations.Change-Id: I8d6b7447498f19260358bbefe34029ddd86b9c89* small fix to file descriptionChange-Id: I0e460f6cf43f9b12ffa5fc66fcb68e55304daeb2* Various improvements addressing commentsIn addition, a new "io_used_memory" annotation is added to the mainfunction which refers to the total size of the IO tensors in theprovided module, enabling these to be discounted from memory pressurecalculations where necessary.Change-Id: Iafe9c85d7fc69c77a2115ed4efe7645160387c86* addressing commentsChange-Id: I00f5ba80d5e004076e4c27d39bec143178b3b1dd* add note for dynamic shapesChange-Id: If6409e2953addfc880bcc6d95083b78bdf5a23d0	1
[DOCS][NNVM] Delete duplicated tensor operators from list (#1669)	1
[HEXAGON] [TOPI] Dequantize (#12677)dequantize op hexagon	5
fix proxy registration with tracker (#1283)	0
fix #5686: remove a overstrict assert in MakeAllreduce (#5686) (#5785)	1
[LANG] Change namespace convention to dot (#100)	4
Use std::optional instead of dmlc::optional, NFC (#12443)* Use std::optional instead of dmlc::optional, NFC* Fix linter* Set deployment target to macOS 10.13Otherwise std::optional<T>::value() is "unavailable"...* Fix linter again* Update Hexagon apps to use C++17 as the C++ standard	1
[docs] Various content corrections (#11517)* [docs] Various content corrections* Fix underline title	0
[Bugfix][Frontend][TFlite] Fix wrong function call in TANH tests (#4517)* Replace sigmoid() with tanh() in tests for TANH	3
[CI] Update ci-cpu to the latest (#6283)	3
[CODEGEN] Fix Metal codegen when storage scope is needed (#469)	0
[NDArray] Expose NDArray::CreateView to python (#10712)Modifying the array view is needed for Hexagon targets, in order tofirst call `tvm.nd.array` with the physical dimensions, then update theshape to contain the logical dimensions.	2
Add `SkipVectorize` pass (#3222)	4
fixing nnvm tutorial typo (#2188)	2
[RUNTIME] Enable return NDArray in RPC (#1610)	0
[CI] Bump Python version from 3.6 to 3.7 in VitisAI.cmake (#10656)This is required because we migrated the default version in whichdependencies are installed, due to Python 3.6 coming to EOL.Co-authored-by: Elen Kalda <Elen.Kalda@arm.com>	1
[PASS][ConvertLayout] Fixes AttributeError during ConvertLayout to NHWC (#6419)Fixes an issue described in #6410. In order to retrieve the shape a tensor `checked_type` should be used.Change-Id: I991d194d9cc15ee20464ff2e239fd05c035000c8	4
Simplify expressions early on (#702)* Simplify expressions early on* fixed lint errors	0
[Docs] Bring Your Own Codegen Guide -- Part 1 (#4602)* BYOC tutorial: codegen C* Address comments* Address comments* Add build option* Address comments* Use TVM_DLL_EXPORT_TYPED_FUNC	1
Fix incorrect device name in TVMC. (#8181)* Fix incorrect device name in TVMC.* Rename gpu -> cuda.* Bump CI.	0
Arm(R) Ethos(TM)-U NPU Pooling operators support (#9384)This commit adds support for the max and average pooling primitive operators for the Arm(R) Ethos(TM)-U NPU and includes a few minor rewording changes.	4
Add cuda target check to dense tensorcore schedule. (#5376)	1
[Bugfix][MetaSchedule] Auto-bind when there are no spatial loops (#11570)	0
[ARITH] Use explicit div mode in python. (#4014)	1
Fix doc of strided_slice (#2103)	2
Adds cpp test for Halide IR cse pass.	4
Explicitly set HardwareParams in test_auto_scheduler_sketch_generation. (#8018)* This test depended on the number of CPU cores available, and failed   when cores < 4.	0
[Jenkinsfile] Build NNPACK and run tests in `ci-cpu` (#2095)	3
Hexagon conv2d full output slice  (#9198)* split h axis by constant factor 2; no cache write* enable cache_write, but not yet able to compute_at* cache_write with compute_at* cleanup, make loop split semantics more clear* parameterize height loop split* nhwhwc wiggling (needs cleanup)* added input channel splits for crouton depth* cleanup variable names and magic numbers* comments* add README* added 3x3 conv2d (no padding) case* add ASF header and RFC link* cleanup README	4
[CMAKE,HEXAGON] Only enable Hexagon custom logging when building for Hexagon (#10587)Move custom logging flags behind `#ifdef defined(__hexagon)`.	2
[microTVM] Bump versions in reference vm (#11067)* Update spresense sdk version to make hack unnecessary* Bump arduino SDK version* Fix Arduino RPC server test* Fix versions for all board libraries	0
[Relay][PRNG] Support generating data of any shape in threefry_generate (#8085)	5
[COMMUNITY] @grwlf -> Reviewer (#2190)	3
[CUTLASS] Refactor GEMM generator in preparation for conv2d (#9571)* split non-gemm specific generator code to gen_tensor_op.pycommit 250f915652e72e0012e9aa6ce0b6ef337d3da845Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:44:52 2021 +0900    remove conv2d stuffcommit 1a6b27c438472f13acd4a0f466d78f293415e076Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:41:31 2021 +0900    remove unused importcommit f7c3b5a191b8c73e8b178c32f6d3182fb0f697d6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:37:07 2021 +0900    add profiler boilarplate for conv2dcommit ca1ae274fb8f96a1dcde688deaf15339fe5604fbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:22:06 2021 +0900    introduce gen_tensor_op.pycommit 37bb918e0873f04457c29479eb21a530b7052217Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 05:45:41 2021 +0900    more conv2d codecommit 5c00398892c99cb2a03be51f75878992663432ddAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 05:13:30 2021 +0900    Begin conv2d support* fix* use functools.partial* remove unused import	2
update compiler version in docs (#5281)	2
[onnx] fix onnx where broadcast (#10106)* fix onnx where bcast* jostle ci* jostle ci* jostle ci	0
[WEB] Setup lint, doc, test (#5556)	3
[Torch] chunk and unsafe chunk (#8718)* alternative chunk op was implemented in pytorch frontend. aten::unsafe_chunk was added to op map in pytorch frontend* chunk was replaced by new one in pytorch frontend. it is faster in 2.5 timesCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>	1
Update Application.mk (#1483)Fix application crash problems on armv7a architectures	0
Check in basic schedule container	5
[Vulkan] Support passing 64 bit scalar  (#7572)Co-authored-by: Wuwei Lin <wuwei@apache.org>	4
[DOCKER] Revert git shallow clone change. (#2841)This patch reverts one of my earlier patches (squashed in #2710) toreduce bandwidth requirements of git clone, in this particular case weare checking out a specific hash rather than a tag or branch name. The--branch option to git clone permits tags or branches but does notpermit a specific hash.	1
[Tutorial] External Tensor Op (#137)	5
Set total memory of emcc module to 1GB (#906)	1
Move dense compute back to python (#364)	4
[ANSOR] Auto-scheduler tutorial for GPU and necessary refactor/fix (#6512)* add gpu tutorial* refactor mutation in evolutionary search* update* update double matmul* fix lint* add double matmul test* fix mutate compute location* fix sketch search policy* fix lint* update* address comments* fix PruneInvalidStates	1
[microNPU] Improve cycles estimates for memory transfers (#10508)Change-Id: Idadc5f354dce42c8dbcdcbe281d324adddb41ba3	5
[Hybrid script] Backend support (#2477)* a preliminary version is done?* we no longer need the redundant hybrid/api.py* support assert stmt* cast supported* intrin -> runtime; util is mainly in charge of compilation time* assert statement* fix python lint* fix cpp lint* on the way to module* rollback .cc* fix typo, no direct expose then* @vinx13 ceil is added i guess?* wip...* temp commit* fix import* i preliminary version is done?* on the way to build hybrid module* nearly fixed...* dumped python are equiv as original python* on the way to bootstrap* cpu bootstrap done* bootstrap!* fix lint* fix doc* resolve some review concerns* support load/save* fix lint* thanks to xqdan fixed my typo* fix build, make dump non-optional* add vthread* jesus why i added this	1
Include \0 terminating character in strncpy (#9775)I'm fairly certain that this bugfix is correct, but please check my code. I'm not sure how this hasn't caused more problems for people! Perhaps the malloced memory is usually filled with 0s? Either way, on my system, the malloced memory was NOT all 0s, and so when we would copy the `key.size()` valid characters of `key` into `*out_type_key`, it was likely that the next character was not 0, and thus, because no terminator was copied over, the string wouldn't be terminated correctly. This fix ensures the terminator gets copied over, by copying `key.size() + 1` characters.	1
[CONTRIB] Allow customized initializer in PopenPool (#8789)	5
[TVMScript] Text underlining in DocPrinter based on Doc's source_paths (#12344)This adds an ability to print a "diagnostic marker" based on a given ObjectPath. For example, say we are printing a fragment of TIR like```for i in T.serial(10):    a[i] = 5```and we would like bring the user's attention to the bound of the loop:```for i in T.serial(10):                  ^^    a[i] = 5```In this case we would give the doc printer an object path that represents this loop bound, i.e. something like `path_to_underline=ObjectPath.root().attr("extent")`Tracking issue: https://github.com/apache/tvm/issues/11912	0
Fix rust rt link (#8631)* Fix support for linking to only libtvm_runtimealso ensures that the ResNet example uses the new support.* Fix build.rs to rebuild if the Python script changesCo-authored-by: Jared Roesch <roeschinc@gmail.com>	4
[Bugfix][AutoScheduler] Strictly select impl using plevel (#6956)* [Bugfix][AutoScheduler] Strictly select impl using plevel* lint	1
[Hexagon][Runtime] Better support for 2-tier memory (#12574)- Introduce 'global.ddr' memory scope:  - Like 'global', this allocates memory from the Hexagon SoC's    DDR memory.  - Like 'global.vtcm', the specified tensor shape must be 1d    or 2d, where 2d indicates Hexagon's "indirect tensor"    (i.e., discontiguous) allocation scheme.- Change memory-alignment strategy to always be 2048-byte aligned  on Hexagon.  (This can be refined in the future, but for now it  ensures all allocations meet the strictest alignment requirements  for any Hexagon operations.)	1
[SOURCE] Add ASF header to __init__.py files (#4359)	2
[Relay] Serialization round-trip tests (#1968)	3
[ONNX] Support NMS Center Box (#7900)* [ONNX] Support NMS Center Box* fix silly mistake in contional	0
[Runtime][PipelineExecutor] Add graph manually splitting logic into the unit test. (#11334)* [Runtime][PipelineExecutor] Add graph manually splitting example intothe unit test.Current unit test create 3 seperate module then re-connect them torun the pipeline executor. And this is not a real use case for pipelineexecutor.Adding a manually graph splitting logic which split a full network into 3subgraph then run the pipeline executor and verify the result tosimulate the real use case.* address review comments* trigger build.* address review comments* address review comments* rebase and trigger build.	1
A clone of test/python/unittest/test_runtime_micro.py, however (#5546)modified to run specifically on ARM cortex-M hardware, whichcurrently is just the STM32F746 discovery board.Signed-off-by: Tom Gall <tom.gall@linaro.org>	1
Remove prints in `generic_op_impl.py` (#3616)	4
[Relay][VM] Fix compilation of If-Elses (#5040)	0
[TVM][CUDA] NVIDIA GPU Int8 Support (#1503)	1
check in (#2627)	5
[MetaSchedule] Allow optional params to be None (#11188)	2
[BUILD] Simplify build process (#326)	5
[TIR] Additional Stmt/Expr simplication rules (#11373)* [TIR] Additional Stmt/Expr simplication rules- Enabled simplification of `A[i] = A[i] + 0` into no-op.  This was a  bug introduced in https://github.com/apache/tvm/pull/9727, which  applied this rewrite only to `A[i] = A[i]`, and not to statements  which simplify to `A[i] = A[i]`.  Regression test added to prevent  reoccurrence of this bug.- Enabled simplification of `x - x` to zero for floating point types.  Previously, this simplification was applied only for data types that  could be used as buffer indices.* Updated to maintain separate int/float simplification paths* Updated to use tvm.testing.main* Remove duplicate rewrite rules	4
[MXNET]conv3d and conv3d_transpose addedx (#5814)	1
[COMMUNITY] @yzh119 -> Reviewer (#10993)	3
Make first order gradient graphs more efficient (#5959)Previously, nodes are visited as often as they are used and each time aderivative is computed. Only at the leaves were the contributions ofeverything added. This patch changes this to add at any node that isused several times.	1
[Hexagon] Fix getting/setting DMA state (#10288)* [Hexagon] Fix getting/setting DMA stateThe bits [3:0] of the first word of the descriptor (both 16- and 32-byte)is the DMA state. It must be set to 0 before starting a DMA transaction.* Restart CI	1
[Bugfix] Fix broadcast type func with incomplete type (#8438)* [Bug] Fix broadcast type func with incomplete type* fix	0
[Onnx] Support Negative Log Loss (#8872)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[FIX] resolve int64/32 for AttrStmtNode (#10983)* resolve int64/32 for AttrStmtNode* rm debug header* refine* add test case* lint	3
[µTVM] Remove binutils module, no longer needed after microTVM refactor. (#6947)	4
Add bing to reviewer (#3214)	1
[AutoScheduler] Fix policy for zero-rank output (#7180)	0
[COMMUNITY] @liangfu -> committer (#5460)	3
[TOPI, Relay] Support roi_align NHWC layout (#7463)* begin nhwc roi align* integrate mode change from upstream* adding test* support nhwc shape func* update strategy* refactoring test* refactor test* refactoring* fix lint* update relay op tests	3
[CUDA] Improve adaptive and global pool schedule  (#8936)	1
[RELAY] Fix reshape header file (#7218)The header file definition of InferNewShape wasincorrect, this patch fixes it.Change-Id: Id24b8eccb52323692fe88bdda46cc49cba54588c	4
[Frontend] [MXNet] make_loss operator support (#4930)* make_loss test case* mxnet frontend make_loss support* added comment for make_loss* pylint fix* Update mxnet.py	5
[QNN] Add qnn op for abs to fix wrong scale on quantize (#12287)* [QNN] Add qnn op for abs to solve wrong scale on quantize* Fix for pylint to allow redefine absCo-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
Correction in documentation (#1810)	2
Optimize move semantics of NodeEntry reducing copies of shared_ptr which causes atomic contention (#2576)	1
[NDArray] Set shape_ in NDArray::FromDLPack (#5301)	1
[µTVM] Add virtual machine, test zephyr runtime on real hardware (#6703)* Split transport classes into transport package.* Introduce transport timeouts.* black format* Add metadata-only artifacts* Simplify utvm rpc server API and ease handling of short packets.* add zephyr test against qemu* Add qemu build config* fix typo* cleanup zephyr main* fix nonblocking piping on some linux kernels* don't double-open transport* validate FD are in non-blocking mode* gitignore test debug files* cleanup zephyr compiler* re-comment serial until added* remove logging* add zephyr exclusions to check_file_type* add asf header* lint* black format* more pylint* kill utvm rpc_server bindings, which don't work anymore and fail pylint* fix compiler warning* fixes related to pylint* clang-format again* more black format* add qemu regression* Fix paths for qemu/ dir* fix typo* fix SETFL logic* export SessionTerminatedError and update except after moving* fix test_micro_artifact* retrigger staging CI* fix jenkins syntax hopefully* one last syntax error* Add microTVM VM setup scripts* obliterate USE_ANTLR from cmake.config* add poetry deps to pyproject.toml - mainly taken from output of `pip freeze` in ci-gpu and ci-lint* initial attempt at setup.py + autodetect libtvm_runtime SO path* hack to hardcode in build* make pyproject lock* Add ci_qemu to Jenkinsfile* build in qemu* checkpoint* create diff for jared* add missing stuff* address liangfu comments* fix new bug with list passing* release v0.0.2* works on hardware* switch to pytest for zephyr tests* add missing import* fix option parsing* remove extraneous changes* lint* asf lint, somehow local pass didn't work* file type lint* black-format* try to fix ARMTargetParser.h #include in LLVM < 8.0* rm misspelled deamon lines* move to apps/microtvm-vm* fetch keys from kitware server* fix path exclusions in check_file_type* retrigger CI* reorganize vm, add tutorial* fixes for reorganization - enable vagrant ssh* update ssh instructions* rm commented code* standardize reference VM release process, add prerelease test* remove -mfpu from this change* fix exit code of test_zephyr* rm unneeded files, update check_file_type* add asf header* git-black* git-black against main* git-black with docker* fixes for virtualbox* black format* install python3.8, for zephyr gdb* timestamp zephyr vm name, permits launching multiple VMs* log warning when initial vagrant destroy fails* revert changes moved into #6789* address leandron@ comments* black format* black format* add --skip-build to test subcommand, detach device from other VMs* black format* address leandron@ comments* don't rm release test when building only 1 provider* revert pyproject.toml* remove need to copy pyproject.toml to root * this often contributes to erroneous changes to that file	2
[Relay] Add ResizeNearestNeighbor and CropAndResize in tf converter (#3393)	1
revert PR#2420 nms changes (#2747)	4
[skip ci][ci] Fix black version (#10893)See https://github.com/psf/black/issues/2964, this is broken in CI now after the update in c2744704bec3cc914fa96dc4f4c9b0ccfaa8ace4. This rolls back the Docker change and includes a fix for when we update to `ci_lint:v0.71`.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[MetaSchedule][Minor] Add Describe Function For Tuning Scripts (#11754)This PR is based on #11751 and adds `describe` function for `tune_relay` and `tune_onnx` script on both AutoScheduler and MetaSchedule. It prints out very useful information for reproducibility as follows:```Python Environment  TVM version    = 0.9.dev0  Python version = 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0] (64 bit)  os.uname()     = Linux 5.15.5-76051505-generic #202111250933~1638201579~21.04~09f1aa7-Ubuntu SMP Tue Nov 30 02: x86_64CMake Options:  {    "BUILD_STATIC_RUNTIME": "OFF",    "COMPILER_RT_PATH": "3rdparty/compiler-rt",    "CUDA_VERSION": "NOT-FOUND",    "DLPACK_PATH": "3rdparty/dlpack/include",    "DMLC_PATH": "3rdparty/dmlc-core/include",    "GIT_COMMIT_HASH": "3b872a0adae07b0cd60248346fd31b158cba630c",    "GIT_COMMIT_TIME": "2022-06-15 11:27:59 -0700",    "HIDE_PRIVATE_SYMBOLS": "OFF",    "INDEX_DEFAULT_I64": "ON",    "INSTALL_DEV": "OFF",    "LLVM_VERSION": "11.0.1",    "PICOJSON_PATH": "3rdparty/picojson",    "RANG_PATH": "3rdparty/rang/include",    "ROCM_PATH": "/opt/rocm",    "SUMMARIZE": "OFF",    "TVM_CXX_COMPILER_PATH": "/usr/lib/ccache/c++",    "USE_ALTERNATIVE_LINKER": "AUTO",    "USE_AOT_EXECUTOR": "ON",    "USE_ARM_COMPUTE_LIB": "OFF",    "USE_ARM_COMPUTE_LIB_GRAPH_EXECUTOR": "OFF",    "USE_BLAS": "none",    "USE_BNNS": "OFF",    "USE_BYODT_POSIT": "OFF",    "USE_CLML": "OFF",    "USE_CLML_GRAPH_EXECUTOR": "OFF",    "USE_CMSISNN": "OFF",    "USE_COREML": "OFF",    "USE_CPP_RPC": "OFF",    "USE_CUBLAS": "OFF",    "USE_CUDA": "/usr/lib/cuda-11.2",    "USE_CUDNN": "OFF",    "USE_CUSTOM_LOGGING": "OFF",    "USE_CUTLASS": "OFF",    "USE_DNNL": "OFF",    "USE_ETHOSN": "OFF",    "USE_FALLBACK_STL_MAP": "OFF",    "USE_GRAPH_EXECUTOR": "ON",    "USE_GRAPH_EXECUTOR_CUDA_GRAPH": "OFF",    "USE_GTEST": "AUTO",    "USE_HEXAGON": "OFF",    "USE_HEXAGON_GTEST": "/path/to/hexagon/gtest",    "USE_HEXAGON_RPC": "OFF",    "USE_HEXAGON_SDK": "/path/to/sdk",    "USE_IOS_RPC": "OFF",    "USE_KHRONOS_SPIRV": "OFF",    "USE_LIBBACKTRACE": "ON",    "USE_LIBTORCH": "OFF",    "USE_LLVM": "llvm-config-11",    "USE_METAL": "OFF",    "USE_MICRO": "OFF",    "USE_MICRO_STANDALONE_RUNTIME": "OFF",    "USE_MIOPEN": "OFF",    "USE_MKL": "OFF",    "USE_MSVC_MT": "OFF",    "USE_NNPACK": "OFF",    "USE_OPENCL": "OFF",    "USE_OPENCL_GTEST": "/path/to/opencl/gtest",    "USE_OPENMP": "none",    "USE_PAPI": "OFF",    "USE_PROFILER": "ON",    "USE_PT_TVMDSOOP": "OFF",    "USE_RANDOM": "ON",    "USE_RELAY_DEBUG": "OFF",    "USE_ROCBLAS": "OFF",    "USE_ROCM": "OFF",    "USE_RPC": "ON",    "USE_RTTI": "ON",    "USE_RUST_EXT": "OFF",    "USE_SORT": "ON",    "USE_SPIRV_KHR_INTEGER_DOT_PRODUCT": "OFF",    "USE_STACKVM_RUNTIME": "OFF",    "USE_TARGET_ONNX": "OFF",    "USE_TENSORFLOW_PATH": "none",    "USE_TENSORRT_CODEGEN": "OFF",    "USE_TENSORRT_RUNTIME": "OFF",    "USE_TFLITE": "OFF",    "USE_TF_TVMDSOOP": "OFF",    "USE_THREADS": "ON",    "USE_THRUST": "OFF",    "USE_VITIS_AI": "OFF",    "USE_VULKAN": "OFF"  }```	1
macOS is now supported (#8396)Remove warning about macOS support from tutorial	1
Enable hipModuleGetGlobal() (#4321)	1
Add `is_floating_point()` test and better type support in `verify_model_vm()` (#7134)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* add test + supporting functions* Revert whitespace changes* Properly assign dtype to random integers* Reformat with black* Switched default dtype logic, removed extra line	4
[DOC] clarfiy explanation (#3340)	2
[Fix] Fix `dtype` in Cache-Read/Write (#12421)	0
[Bugfix] Bilinear resize bug fix from PR #2777 (#2857)* error fixed* rename* solve conlicts with master* more test added* fix error* remove test* comment addressed	1
Bump all Docker image versions to update Python to 3.7 and Tensorflow to 2.6. (#10654)These updates are required due to Python 3.6 coming to EOL and also to updateTensorFlow to a newer version.	1
[FRONTEND] CoreML (#63)* add coreml* fix bool* fix flatten in fullyconnected* fix duplicate flatten* fix syntax* add tutorial* fix mxnet flatten, fix tutorials* fix flatten issue* fix lint	0
[ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear (#9028)* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear	0
[BYOC][ACL] Support asymmetric per-layer quantized operators (#6109)* [BYOC][ACL] Support asymmetric per-layer quantizationAdds support for asymmetric per-layer quantization in the ACL runtime. This includes support for qnn.conv2d, nn.maxpool2d and reshape. Reflected these changes in codegen and runtime tests.Change-Id: I8f610bd37af1e3740fd48c2d502bcc4727d9d712* Address commentsChange-Id: I4f9e3e7dbf6053066927cf07c4c19ecc88572e9d* Fix tutorialChange-Id: I4371e9d97a120fb7776db40ffcde60f46927af4d* Improve test infrastructure* Doc-string for generate trials* Output params on errorChange-Id: Ib2e2b1fcdf05cdc77f7f4fb4b46395f28c129957	4
[MetaSchedule] Use Add-Unit-Loop in Auto-Bind (#11581)Following #11575, this PR allows CUDA thread binding for TIR programslike```python@T.prim_funcdef zero_dim_add(    A: T.Buffer[(), "float32"],    B: T.Buffer[(), "float32"],    C: T.Buffer[(), "float32"],) -> None:    with T.block("C"):        vi = T.axis.spatial(1, 0)        C[()] = A[()] + B[()]```where there is no loop available to be bound to threadIdx/blockIdx.	1
[TIR] HoistExpression, generalization of HoistIfThenElse (#11592)* [TIR][Arith] Use non-inlined bindings when proving conditional* [TIR][Arith] Recognize Var when used as a literal constraint* [TIR][Arith] Added simplification of constrained if_then_else opThis feels like it should definitely be part of RewriteSimplify, butthat will require making CanInlineLet be a virtual function.* [TIR] Implemented HoistExpression transformationThis is a generalized form of HoistIfThenElse, which can also hoistLet bindings, or portions of conditional expressions.  This will beused in upcoming changes to separate compute loops into a slow loopthat handles edge cases and a fast branchless loop.* [TIR] Expressed HoistIfThenElse as special case of HoistExpression* Lint fixes* Fixed breakage in tvmc unit test that relied on pass type* More accurate handling of kUsingBlockVarDidn't correctly reproduce previous behavior.  In addition topreventing hoisting of expressions that use a blockvariable (e.g. threadIdx.x), should also prevent hoisting ofexpressions across a "thread_extent" AttrStmt.* Updated comment for HoistExpression pass* Fix linting error	0
Add qnn batch_matmul operator (#8401)* Add qnn batch_matmul operator- add support of the different out type for x86 batch_matmul* Fix code style* Add out_dtype to generic batch_matmul* Restore fixe in batch_matmul for dynamic shapes* Fix documentation for qnn.batch_matmul* Remove debug code* Modify zero point for qnn batch_matmul test	3
[Hexagon] Adjust RPC read buffer size from python  (#11022)* added buffer size* remove default size	4
[HotFix] Op is not bound to any variables (#12401)* [HotFix] Op is not bound to any variablesAfter PR #12349 inference of some Adreno networks was broken. In theoutput it was the following error:```TVMError: Not all Vars are passed in api_args: 'compute' is not boundto any variables```	4
[Fix] Tensor core type issue for dense (#7187)* fix tc type issue for dense* fix lint* rm float 32Co-authored-by: Leyuan Wang <ziyu.guo@bytedance.com>	0
[Relay, TOPI] Refactor Adaptive pool and add 3d support (#5049)* add stub for nd impl* refactored indices compute* refactored divide step* remove unused variables, add doc* fix lint* add relay op def* add python registration* refactor topi test* update relay tests, but test result is weird* workaround for weird bug* add relay adaptive pool 3d test* add topi tests* update doc for 3d* typo fix* fix lint* add more tests including NDHWC	3
add sanity check to input shape type (#469)	1
[MetaSchedule] Refactor testing workloads (#10497)	1
[TOPI] Add conv2d int8 template (#1735)	1
fix first-order AD on tuple arguments (#6827)	0
Prevent simplifing unit IterVar in CreatePrimFunc (#11292)Simplifying unit iter vars in CreatePrimFunc changes semantics of the PrimFunc, which need different handling in analysis.This reverts commit 26cefab5df8f24af7dc43a3239dbfd0e858fd1a2.	5
[RELAY] Fix get_int_tuple for shape like '(1001,)' (#2691)tshape.strip('()[]').split(',') will make a list ['1001',''] but the empty one isn't needed.	1
[LLVM] Fix build errors in CodeGenCPU::AddDebugInformation (#12054)This code is guarded by TVM_LLVM_VERSION >= 50 and < 70, so the errorswere not detected in local tests or in CI.	3
[ETHOSN] Remove remaining support for the N77 variant (#11262)Specifically removes some TVMC tests that are no longer necessaryand some partitioning infrastructure.	5
Added a docstring to missing CMSIS-NN test (#11690)* Made CMSIS-NN tests pylint compliantChange-Id: I6bc536a80a24a1603e9f75f8ee9a26d0d88f10df* Removed comments that disabled pylint checksChange-Id: Iee513a4a5bef1db5b78e1d25a30ac7202f8b0e92* Fixed pylint issue in the generate_constants testChange-Id: Icd341cf524b331ced1fc7ef282b67296583b0fa4	3
[ETHOSN] Fix quantization parameters in test (#10178)The Ethos(TM)-N tests for addition had a poor choice of parameters forthe int8 case.	2
[TIR][Schedule] Blockize and Tensorize (#9871)* WIP* WIP* WIP* test cases* add examples* lint* Amend co-authors informationCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>* WIP* address comments and changed tensorized comparator* update* nit* fix example* lint* lint* lint* remove unused* trigger ci* clang-format* fix* rebaseCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
[Relay][Dynamic] Add Dynamic Resize Op (#6198)* WIP* optionally remove output shape inference from topi* fix resize* add resize to dynamic_to_static passadd resize to dynamic_to_static pass* fix clang-format* fix bad rebase* add argument to dynamic resize doc string* fix i386 test* fix lint	0
[DOC] Add test script starter command to document (#3993)	2
fix bmm quantization realize (#11586)	0
[microNPU] Add a pass to move allocate nodes to the outer scope (#10725)* [microNPU] Add a pass to move allocate nodes to the outer scopeAdds a pass called `HoistAllocates` to move allocate nodes to the topof the body of the main function. In doing so, it opens the door toother optimizations that need to swap the ordering of external calls.Pass illustration:(before)```allocate {    extern_call {        allocate {            extern_call {            }        }    }}```(after)```allocate {    allocate {        extern_call        extern_call    }}```Change-Id: Ibcfc3c75b15deebb5c6645a4923a6ddf683b37c4* address comments* uses prim func pass, rather than module pass.* adds error message informing user to run this pass with LowerToTIR()  pass for now.Change-Id: I57757b9dc5bff0208034a974a341c09cce0294bc* Support allocates when not followed by a sequence statementWith a test to back this case up.Change-Id: I670809f5ee53b583a15d9b783852dda3089756e9* Add new directory tir/contrib/ethosu to cmake buildChange-Id: I3e9f24adfe992ace4e03238a18a8378b03257e1a	4
[Torch] Support logsumexp, clean up unnecessary infer_shape usage (#6374)* clean up infer_shape usage, add logsumexp op* add more tests for logsumexp* remove commented code	4
[CI] Allow CI_PYTEST_ADD_OPTIONS to be unbound. (#5644)This patch allows the test script to execute normallywhen CI_PYTEST_ADD_OPTIONS is not available.	3
[IR] Change pragma  convention, enable pass unroll option via pragma (#1112)* [IR] Change pragma scope convention, enable pass unroll option via pragma* add coverage test* add explicit unroll as option	1
apps: microtvm: Disable `CONFIG_FPU ` for Zephyr runtime (#8055)`CONFIG_FPU` was being enabled by default for every platform,regardless of whether or not the platform using the sample app actuallyhad a HW FPU unit. As a result, FPU instructions may be included onplatforms that aren't able to support them, or in a best-case scenariowe will get a warning about the conflict during builds, which pollutesthe CI output, in a worst-case scenario a fault.This change removes the `CONFIG_FPU=y` setting from being set at theapplication level, since this flag should be set at the chip level forany platform that has an FPU.Signed-off-by: Kevin Townsend <kevin.townsend@linaro.org>	1
fix (#3550)	0
[schedule] Improve ceil_divide in tile/split (#3842)	1
[DOC] Capitalize TVM consistently (#3316)	2
[ci] Bump i386 shards (#11271)i386 is the longest running test step by a small margin:<graph incoming>This also only runs the C++ unittests on the first shard so we don't end up wasting compute minutes.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[NODE] Node base system refactor (#1739)	4
[Rust] Fix issue with CPP enums. (#4019)	0
[DOC] Grammar fix (#7824)A grammar fix for runtime document.Co-authored-by: Joey Tsai <chunit@qti.qualcomm.com>	2
[Relay] Fix an assertion exposed by loop vectorizer (#4916)- Allows uniform conditions for select expressions (the same as halide)  exposed by the loop vectorizer.Signed-off-by: Wei Pan <weip@nvidia.com>	1
update for using new functions (#11100)	1
[Relay, BYOC] Make constant binding in PartitionGraph optional (#9721)* make constant binding in PartitionGraph optional* add test* Doc string update* remove default argument from set_body_typed	1
[Relay] [Training] Add numerical gradient check. (#3630)* add check_grad* finish* what does the fox say?* lint lint lint lint lint lint lint lint lint	5
[Collage] CollagePartition pass (#12086)* [Collage] CollagePartition passSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.This adds the main CollagePartition pass, which: 1. Inspects all the targets in the CompilationConfig and builds    PartitionSpecs describing how to generate speculative CandidatePartitions    for them. 2. Runs the above rules on the model to collect all the candidates. 3. Eliminates candidates whose target contradicts any constraints already    imposed by, eg, device planning. 4. Eagerly estimates the cost of each candidate. 5. Performs a shortest path search to chose an 'optimal' set of candidate    partitions so as to minimize estimated model latency, such that every sub-expression    node is contained in exactly one candidate partition. 6. Coalesces adjacent optimal candidates which ended up on the same target. 7. Rewrites the model according to the chosen optimal partitioning.As for the existing partition_for_<external codegen name> methods, the result ofCollagePartition can then be built using regular TVM.Very special thanks to @mbaret for authoring test_pass_collage_partition.py.Logic to prune the candidates after step 3 will be in a follow up PR since itdeserves its own testing. A demonstration driver will also come as a follow up.* - lints* - more lints* - use the _ffi_api properly	1
[Relay][Passes] Iterative A-normal Traversals (#7374)* [WIP][Relay][Passes] non-recursive a-normal traversals* fix clang warning* Refactor ANormal Iterative traversal into a higher order function utility with lambdas* refactor missed pass* add explict use of  to lamdbas	5
Fix typo. (#9462)	2
[TE] Optimized version of concatenation layer (#11341)* [TE] Optimized version of concatenation layer     1. Concat implemented using extern_op     2. New tests added.     3. Workaround to allow inline extern_op-s with other layers.* *test fix* test_any.py fix.* test_forward.py from tensorflow fix.* lint fix.* Fixes after code review.* New comment added.* Lint fix.* Another lint fix.* Comments added.* rebase issue fix.* Restored previous state.* Update after code review.* After code review changes.* lint review.* Change strategy for cuda to fix tests.* Rebase to main* Comments changes after review.* Some more comments fixes.* One more error fix in comments.* restart build	0
[BYOC][TensorRT] Add TensorRT own int8 calibration support to TensorRT BYOC integration (#8808)* update trt* clean codes* tetsing running trt* clean data* clean codes?* remove env func* fix num_bings* add buildfromjson func* change condition* reset input and output func* re-config func* re-added trt version check* checking sanity* try to fix sanity issue* checking sainity* fixing sanity issue* fixing sainity issue* fixing sanity* clang format fixed* clang format fixing* clean trt cali* try to fix clang format* fixed some comments* remove double destroy engine codes* modify comments* add checking function* add trt int8 test* update trt int8 test file* Update test_tensorrt_int8_exp.py* update trt int8 fikle* change a little* upate trt int8 file* upate trt int8 file* fixing ci* fixing ci* fixing ci* fixing ci* fixing ci* fixing ci issue* fixing ci issue* fixing ci* fixing ci issue* fixing ci* fixing ci problem* fixing ci* upate trt python int8 test file* fixed ci* fixed ci* fix gpu build* fixed ci* update trt int8 test file* fix bug* fix bug* update trtint8 file* reformat* update trt int8 file* update* modify	5
[microNPU] increase workspace sizes for network tests (#11943)The network tests with striping were reported to be flaky.This commit increases the workspace size to be generous andalso repeats the test case to make sure its not flaky.Change-Id: I134f504250c8fa0bbbcf5f673acec7ffa2ec2f55	4
[CI] Identify non-inclusive language in commits (#11230)* Adds a script blocklint.sh that checks for non-inclusive words  * Updates the task_lint.sh script to call blocklint.sh  * Replaces the terms Master and Slave where possible  * Replaces the terms Blacklist and Whitelist	5
[TVMScript] Printer: add boolean operators to OperationDoc (#12518)This PR adds boolean operators to OperationDoc. This is needed by the TIR expression printing because it has `tir::And` and `tir::Or`.Tracking issue: #11912	0
Update nn.rs (#10063)	5
[ETHOSN] Add support for 20.11 Ethos-N driver stack release (#7506)- Updated ethosn relay backend to support 20.11 api changes. - Removed legacy support for 20.05. - Added a mechanism to specify the ethosn driver stack version.	1
fix device on HandleCopyFromRemote (#9616)	0
[RPC] Fix ios_rpc build (#8864)	0
[FIX] Simplify during create prim func (#9691)	1
[TIR] Improve Let/LetStmt support. (#5949)Let/LetStmt are useful primitives to create variable bindings.While let binding are harmful for simplification and integer analysis,they are useful for other cases:- C0: LetStmt is useful to represent a step that has side effect(e.g. call a PRNG)- C1: Let expression can be used to create deep nested expression for complicated functions.This PR improves the let support in the following ways:- Enable vectorization support for let- Change let simplification strategy to simplify the most trivial case  while ignore more complicated cases(to avoid deep nest explosion)- Enhance arith module to handle const bound and modular set for let.The overall recommendation is to only use Let in the cases when necessary(C0, C1).	1
[TIR][REFACTOR] Enforce allocate to use the correct var pointer hint. (#7216)* [TIR][REFACTOR] Enforce allocate to only accept buffer_var with correct PtrType.This is a refactoring step to cleanup legacy issue of opaque buffervar without ptr type information. Now all the allocation comes with the rightpointer data type. Places touched:- TVMScript Parser: add the right info to get the correct pointer type.- Cross thread all reduce: set the right pointer type.- Storage rewrite: setup the right pointer type.- Custom dtype: remap the variables with new pointer type.x* Address commentsCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	1
[Tests] Fix requires_gpu (#8050)	1
[RELAY]full, full_like compute and schedule (#2170)	5
Add in Array, fix most of IR	0
[MetaSchedule][Test] Add unittests for NRM (#12250)	3
[Torch] Add support for split (#5174)* [Torch] Add support for split* fix* fix test class	3
[Frontend][Tensorflow] Support explicit_paddings for TF 2.x (#7445)* Ignore some TF2.0 attributes* Support explicit padding for conv2d, max_pool, conv3d* Remove conv3d explicit padding test since TF API doesn't allow it	1
add platform to build directory (#8945)	1
fixing URL; adding () to print (#17)	1
[CUTLASS] Support batch_matmul (#9439)* Import batched gemm changecommit cfacfa296e2487a189e52d189567b140c675ccc2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Nov 1 15:57:49 2021 +0900    change is_constant pattern to wildcard in gelu patterncommit 84da94306ca81209a8ccc44fd7d606cbce047082Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Nov 1 05:41:11 2021 +0900    fixed batch stride Ccommit 66e5779ee69dc0cd3969f268608b551ec549d79bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 20:47:16 2021 +0900    refactoring codegencommit 561daeafa66cddf6a565b537072a5efce0b0dbf1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 20:05:20 2021 +0900    generated kernel compiled and result matchcommit a5740bcf5287097b64dff8adb50f0cddc2c41349Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 19:36:53 2021 +0900    partitioning looks goodcommit 59112fdf78a4541905fad9b899737600e0ed9391Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 19:01:47 2021 +0900    [WIP] cutlass batch matmul support* fixed test* refactoring* gelu test fixed* more refactor* batch_matmul fp32 accum working* dynamic batch matmul working* black* remove doc TODO	2
[DEBUG]Support a debug framework for TVM Runtime (#1378)	1
[FIX] Fix allocate size^2 in graph_executor (#207)	0
Fix the python intrin rule (#5895)	0
[Relay] Register compute and schedule for upsampling, with miscellaneous fixes (#2171)	0
[Relay][Frontend][ONNX] operator support: Tile (#3941)* [Relay][Frontend][ONNX] operator support: Tile* Trigger notification	1
Add support and testing for tf.assert (as no-op) and tf.no_op to TF Relay frontend. (#4172)	3
[RPC] Add Data & Time For RPC Tracker / Server Logging (#11950)	2
[UnitTests] Added cuDNN to default test targets (#8383)* [Target][UnitTests] Look up target requirements based on tvm.target.Target- Read target.kind.name instead of using string manipulation.- Target device query on a non-existent target is no longer an error.  This occurs if expanding `vulkan -from_device=0` on a non-GPU  machine.* [UnitTests] Added cuDNN target to default test targetsSome unit tests explicitly test cudnn in addition to`tvm.testing.enabled_targets()`.  This moved the cudnn checks into thesame framework as all other targets, and adds it to the default listof targets to be run.  Also, added `@tvm.testing.requires_cudnn` fortests specific to cudnn.* [UnitTests] pytest.xfail for CuDNN conv2d with asymmetric padding* [Topi][CuDNN] Added handling of dilation to conv2d_cudnn* [Topi] Skip dynamic batch matmul on cudnn, vulkan, openclPreviously, cuda/nvptx targets were excluded.  Changed it to look upby target.kind.name, and to also exclude vulkan/opencl, as the dynamiclookup currently doesn't work on those backends.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[DOC] fix :code: markup syntax (#3140)	0
Check common subdirs for vulkan/spirv headers (#1298)	5
[TEAM] Lianmin Zheng -> committer (#2142)	5
Fix error reporting for missing axis (#2835)* Fix error reporting for missing axis* Update data_layout.cc	5
update (#26)* updates (#1)* add scalars* change format* change inferattr interface* remove scalar* remove warning	2
Simple workaround for PyTorch symbol crash problem in meta schedule test (#10342)* Simple workaround for PyTorch symbol crash problem in meta schedule test* workaround for CI	1
[BYOC] Pattern Language MergeComposite (#5656)* Pattern Language MergeComposite* fix DNNL pattern* Use builtin binary operator syntax for demo* Improve unit test	3
[CMSIS-NN] Aligned scale computation with TFLM to fix numerical mismatch (#10817)Fixes numerical mismatch in Conv2D layers byaligning order of output scale computationwith TFLM. Correct output scale is neededto calculate quantization parameters neededby CMSIS-NN.	5
[TOPI][Tensor Core] Conv2d and Dense ops support on Tensor Core (#5099)* [TOPI][Tensor Core] Optimization of CNNs on Tensor Core #6004* update conv2d test* # pylint: dense_tensorcore.py* modify* modify conv2d* modify the unclear comment,add shape assertion in conv2d compute,combine general gemm intrinsic* add shape assertion in conv2d compute, combine general gemm intrinsicCo-authored-by: libaihong <libaihong@inspur.com>Co-authored-by: libaihong <61525430+libaihong@users.noreply.github.com>	1
[TensorIR][M2a] Compute-At (#8943)This PR is part of the TensorIR upstreaming effort (#7527), which adds the following schedule primitives:* `compute-at`* `reverse-compute-at`Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
DNNL-BYOC enhancement (#9797)* add unit test for byoc-dnnl* add byoc-dnnl pattern and their test cases	3
[CI] Bump black version to 22.3.0 (#10960)* Make all required adjusts in the code to comply with the new version* Upadte ci-lint to v0.71, based on tlcpackstaging/ci_lint:20220411-060305-45f3d4a52	1
Fix Softmax in onnx frontend (#1642)	0
[iOS][RPC] Enable tests for connection configuration: tracker via proxy (#9398)	5
Keep CODEOWNERS file up to date. (#8500)* Keep CODEOWNERS file up to date.The CODEOWNERS file was used as a mechanism to mark committers' areaof expertises and faciliate the review process. This PR attempts tobring its state to up to date. This is of course non-comprehensive,but can serve as a starting pt to help us to find the right personto shepherd the PRs.* Update .github/CODEOWNERSCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update CODEOWNERS* Update .github/CODEOWNERSCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update .github/CODEOWNERSCo-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Chenfan <jcf94@outlook.com>	5
[skip ci][Community] Wuwei Lin -> PMC (#12605)[Community] Wuwei Lin -> PMC	3
[AutoScheduler] Fix custom build func in PopenWorker (#8939)* [AutoScheduler] Fix custom build func in PopenWorker* Add assertion	3
Update cross_compilation_and_rpc.py (#816)	5
[RUNTIME] Refactor extension type handling, now it is header only (#924)* [RUNTIME] Refactor extension type handling, now it is header only	4
[Torch] fix torch version check (#10481)old code checkout "1.10.2" greater_than "1.5.0" if false, fix it	0
[RUNTIME][OPENCL] Make OpenCL runtime Compatible with OpenCL2.0 #2897 (#2950)There are many OpenCL platforms that do not yet support OpenCL 2.0,hence we use 1.2 APIs, some of which are now deprecated.  In orderto turn off the deprecation warnings (elevated to errors by-Werror) we explicitly disable the 1.2 deprecation warnings.At the point TVM supports minimum version 2.0, this commit can bereverted.	4
[microTVM] Include standalone_crt dependencies in MLF (#10095) * Adds runtime to AOTExecutorFactoryModule * Standalone CRT files are added to MLF tarball if runtime is crt * external_dependencies info added to metadata.json for crt runtime * microNPU demo Makefile references standalone crt files from MLF tarball	2
fix import (#1621)	2
[BugFix] Fix to allow zero-copy between numpy and TVM NDArrays (#9230)	1
Windows Support for cpp_rpc (#4857)* Windows Support for cpp_rpc* Add missing patches that fix crashes under Windows* On Windows, use python to untar vs wsl* remove some CMakeLists.txt stuff* more minor CMakeLists.txt changes* Remove items from CMakeLists.txt* Minor CMakeLists.txt changes* More minor CMakeLists.txt changes* Even more minor CMakeLists.txt changes* Modify readme	4
[Relay][VM][Interpreter] Enable first-class constructors in VM and interpreter via eta expansion (#4218)* Fix constructor pretty printing* Make Module::HasDef name consistent with API* Add VM constructor compilation via eta expansion* Lint* Fix CI* Fix failing test* Address comment* Retrigger CI* Retrigger CI	1
Call previous excepthook in tvm_excepthook. (#5675)* Call previous excepthook in tvm_excepthook.* Rename prev_excepthook.* Create a tvm_wrap_excepthook to wrap a given excepthook with tvm custom excepthook workand call it on system previous excepthook.* Add docstring.	2
[RUNTIME][ABI] Flat structure arguments (#232)	1
Initial commit	5
Address execution hot spots (#154)* Address execution hot spots* extend the reserve logic a bit* FIx more hot spots* Performance: use resize(0) instead of clear() in order to avoid a free/malloc	1
Added additional information to the from_onnx tutorial (#7127)	5
[microTVM][RVM] Improve base-box-tool 'build' command (#8738)Currently base-box-tool.py 'build' command will fail with a 'packer'error message on the second run if it's run twice and the box for aprovider built on the first run is not removed manually before thesecond run.This commit avoids that failure by checking for the existence of a boxfor each specified provider and if a box already exists it will refuseto overwrite the box (since building a box takes a quite amount of timeto be done), exiting and warning the user. A new option '--force' isadded to the 'build' command that allows the user to explicitly rebuildthe box in case one already exists.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[FQ2I] Add support for some unary operators (#10273)* initial commit* lint	5
[relay][frontend] Return module from frontend parsers (#3353)	5
[microTVM] Zephyr: implement 'west_cmd' server option (#8941)Currently Zephyr Project API server lists option 'west_cmd' as anoption available in Zephyr platform by advertising it in PROJECT_OPTIONSbut that option is not used by any API method.That commit adds that option to the server as a non-required option tothe build() interface method, allowing the user to specify analternative path to the west tool. If that option is not specified theZephyr build system takes care of searching for west as a module (sorelying on West being available on Python, i.e. relying on'python3 -m west').Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	5
Replace UseDefaultCompiler with GetAttr (#5088)	1
RelayViz Graphviz renderer (#10400)Following https://github.com/apache/tvm/pull/10085, this PR adds agraphviz backend. It requires python `graphviz` package and `dot`executable in the PATH, similar to `tedd.py`.This implementation is much like a porting of `visualize` function inhttps://tvm.apache.org/2020/07/14/bert-pytorch-tvm, except that`node_attr_dict` is replaced with a callback `get_node_attr`.`get_node_attr` can be somehow used to emphasize a set of nodes.It might be useful if we encounter problems in inferencesand want to find nodes with certain types and attributes.An example is provided inhttps://github.com/chiwwang/tvm/blob/graphviz_renderer_example/test_viz.pyIts outputs are (conv2d with NCHW layout is green-colored):https://github.com/chiwwang/tvm/blob/graphviz_renderer_example/mod_with_subgraph.pdfhttps://github.com/chiwwang/tvm/blob/graphviz_renderer_example/mod_wo_subgraph.pdf	3
Small refactors and bug fixes. (#2281)	0
compile engine dump tir and shape funcs (#7552)	5
[CI] Bump arm version (#7584)	5
fix batchnorm infer_value error, add regression test and unit test (#5845)	3
[microNPU] Fix flaky compute cycle annotation test (#11510)Fixes non-deterministic test by disabling striping when runningthe cascader.Change-Id: Ib44f299f21fa0b41be4bfac3deb61a9c16818c58	4
[Relay]Allow every runtime module to handle constants (#5885)* update source module* address comment	1
[Relay] Enhance relay.split(), allow splitted dim to be dynamic (#6289)* [Relay] Enhance relay.split(), allow splitted dim to be dynamic* Add assert in shape function* Fix CI	0
[TOPI] Rename output tensors for better readability (#3006)	1
fix squeeze to output (1,) if all axes are squeezed. E.g squeeze((1,1,1...), None) case (#498)	0
[QNN] Align output_scale/zero_point of sigmoid to Torch (#12624)* [QNN] Align output_scale/zero_point of sigmoid to Torch* [QNN] Align output_scale/zero_point of sigmoid to Torch	5
remove PEP498  f-string new feature for support  python3.5 (#4250)	1
Update installation doc with minor improvements (#6104)Make some minor improvements to the install from source docabout flags to enable, package managers, and virtual environments.	0
[BUILD] Fix reflection build for gcc-8 (#1304)	0
[ci] Don't diff Python files when checking formatting (#10895)	2
mix fix (#1079)	0
[COMMUNITY] @guberti -> Reviewer (#10976)	3
[TOPI][RELAY][TENSORFLOW]Math ops added (#5502)* [TOPI][RELAY][TENSORFLOW]Math ops added* Extra newline removed* CI fix* Review comments fixed* Review comments fixed	0
Cleanup of '-Wsign-compare' warnnigs. (#504)	4
[PASS][FIX] Fix LiftAttrScope with if (#309)* [PASS][FIX] Fix LiftAttrScope with if* [PASS] Fix on proc sync* fix	0
[Relay] Setting Legalize opt_level to 1. (#4198)	1
[Runtime] EdgeTPU runtime for Coral Boards (#4698)	1
[ci] Fix docker post-merge builds on main (#12210)* This patch addresses #12097 by referencing the branch from the `BRANCH_NAME` environment variable, which is also used in the Jenkinsfile* This avoids git rev-parse, which assumes the local repo contains a git branch that matches the name of the branch being merged. I think this is in spirit of what the script was trying for.	1
[RFC] Improve quantized convolution performance for armv8 architectures (#5754)* Improve quantized conv2d performance for armv8Signed-off-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: I3a3d29f5332dd9b3354e8e0dfb24677a521f9c8f* Add ASF header to conv2d_gemm.pyChange-Id: I33853279e39c849ae1b555a9c91d7557985a0a35* Run clang-format-10 on c++ filesChange-Id: Ieee22f032e595dabfc1616ab33466fcbf8d94365* Fix pylint errors/warningsChange-Id: I435d4d7bca7500db99547f4401fdc0d0995a1ff4* Fix pylint errors/warnings in topiChange-Id: I2fc1ad8453e9020072ab967c849df5390c2967b5* Fix legalizations tests for aarch64Change-Id: I0a67a49a7849f52ef7d57b9292ce9125bbb7cb2c* Reintroduce conv2d_nhwc_spatial_pack.arm_cpu and int16 castChange-Id: I91b67fabd475e90a9b75f2dd5ecfee851265e0bb* Switch type of legalization depending on the strategy usedChange-Id: I9a03040a8c40a6cd2658ed14c3751e05a8e19f2b* Revert last commitChange-Id: Ice34101e358e3ce8ebfb12c58f73e910ba5de8e8* Fix the auto-tuner by registering the correct schedulesChange-Id: Id9273688b2620e1ea849ab01b4c46af8fbf37fd0* Address review commentsChange-Id: Ia1755a0af7b6d159072d9f0c93c932c481101e48* Improve usability and readability of conv2d_gemm_weight_transformChange-Id: I3333186bbc2fe4054b58ce15d910e3be7b315482* Change variable name to weight in Conv2DGemmWeightTransformRelChange-Id: Ifb5f1f33af7512fe67c6b049b20a42a0bb2d26c9* Fix clang-10 linting errorsChange-Id: I25ccc844d9cee23766096e1daddb6180abc413a6* Trigger testsChange-Id: Id37706fb7cf77a87a3cc817ecf8046297d9ca95a	3
[ci][easy] Fix parameters for macros (#11377)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Fix runtime::String backward compatibility in JSON (#5725)	5
[TOPI] Simplify GPU NMS IR and optimize a bit (#7136)* remove get_valid_counts from pytorch nms* fix pytorch nms for negative score* merge reset by -1* move max_out_size handling to triangle loop* update torch nms test* fuse the last two kernels* parallelize the first kernel* merge first and last kernel* remove unnecessary cases* fix typo* revert pytorch frontend change* fuse rearrange step with triangle loop* fix max_output_size handling* check if already surpressed* fix topi vision test by wrapping tir const around int argument* fix for num anchors = 0 case* fix missing zero init of num valid boxes when the input is empty* add some comments and missing doc* typo fix* add a guard against zero dim grid / thread block inside ir_buidlder* typo fix* trigger CI	0
Enable miopen Group Convolution (#3987)* enable group conv through miopen* linter fix	0
Activations for coreml added (#1508)	1
Update Docker CI (#8193)* add failing onnx tets* point jenkins at new docker* support convtranspose opset 11 autopadding* Don't force output shape for conv transpose tests, add 1D and 3D cases* disable test until CI update complete* try updating docker images again* skip a test until update complete* next try at docker images* manage TF memory use in TF1 tests* support explicit padding for NCHW TF padding test* Update to tagged tlcpack imagesThanks, Andrew!Co-authored-by: Andrew Reusch <areusch@gmail.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>	5
[OPT] Improve PreComputePrune When Output Is Pruned (#178)	1
[tvm4j] add GraphRuntime (#1472)	1
[tvmc] command line driver 'compile' (part 2/4) (#6302)* [tvmc] command line driver 'compile' (part 2/4) * Add 'compile' subcommand into tvmc (tvm.driver.tvmc) * Add frontends: Keras, ONNX, TensorFlow, tflite, PyTorch * Add tests for the 'compile' subcommand * Enable command line driver tests as part of integration tests * Skip tests if the cross-compilation toolchain is not installedCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>* tvmc: adjust TODOs* tvmc: fix linting errors* Address code-review comments* Adjust pytest fixture to not break when there is no tensorflow* Fix frontend tests, to cope with different frameworks in different images* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Fix lint and code-review issues* Re-format with black.* tvmc: Move dependencies to extras_requiresCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
[CI] Upgrade Python dependencies as part of Docker image buildMake sure that Python package dependencies we install as part of the Docker image setup take precedence over previously Ubuntu installed packages that might be installed (e.g python3-***) via apt.	1
[cuDNN] Add support for log_softmax (#8369)* log_softmax strategy and cudnn impl* add log_softmax cudnn test* silence terrible pylint suggestion* fix typo	2
[Torch] Various updates for PyTorch frontend   (#7348)* add conversion for detr* remove explicit broadcast_to before batched matmul* use take with wrap mode* add test for transformer and negative indices* add sort and argsort* add logical_and* support masked_select* add gpu targets to masked_select test* improve sort conversion	1
[TOPI] sparse_dense Op sparse_data input added  (#6889)* [TOPI] sparse_dense op sparse_data input added* [1] clang issue resolved* [2] python format resolved* [3] lint error resolved* [4] Review comments handled* [5] Lint error resolved* [6] Review comments handled* [7] Review comments handled* [8] Review comments handled	0
[RPC] Enable shutdown hook (#308)	1
[Bugfix] Add a nullptr check to tir.Buffer to fix the illegal memory access (#8910)* fix wrong log of tir pass VerifyMemory* fix a typo of convention* add a nullptr check to tir buffer* add test case to trigger buffer nullptr bug* update the style to fix ci error	0
[Target] Allow 'true' and 'false' strings in conversions to integer (#8254)* [Target] Allow 'true' and 'false' strings in conversions to integerThis will allow Bool parameters to take true/false values insteadof 0 and 1 only.* Convert the string to lowercase.* Reserve memory for lowercase string* Add include <cctype>	1
[RUNTIME] More reliable thread enumeration (#1017)	1
Bugfix plan memory, fully support mxnet executor (#32)* [PASS] include knullop info in plan memory* Bugfix plan memory, fully support mxnet	1
Fix TVMC micro import error (#9688)	0
[Runtime] Introduce runtime::Array (#5585)* Introduce runtime::Array* Sync with dmlc-core* Tests added: size, capacity, empty, front, back, push_back, pop_back, insert * 2, erase * 2, resize, reserve, clear	1
[RELAY][PASS] Dead Code Elimination (#1776)	4
Add a the ability to trigger debugging in the interpreter without recompiling (#2219)	0
[LLVM] Auto-convert shuffle with single index to "extract element" (#6006)* [LLVM] Auto-convert shuffle with single index to "extract element"Data types with a single lane are treated as scalars in TVM. On theother hand, in LLVM there is a difference between a scalar type anda vector type with a single lane. Because of that, a shuffle witha single index is equivalent to extracting an element in TIR, butnot in the generated LLVM IR. This patch changes the LLVM codegenfor shuffle to auto-convert single-lane vectors to scalars.* Try another build	1
[Arith] Updated arith::DetectIterMap to keep extent=1 components (#10980)* [Arith] Updated arith::DetectIterMap to keep extent=1 componentsPreviously, arith::DetectIterMap simplified the output expression byreplacing iteration variables with extent==1 with their value.  Thisprevented the return value from being used inarith::InverseAffineIterMap to solve for the variable, as it no longerexisted in the returned expressions.This commit changes arith::DetectIterMap to keep the iterationvariable even if extent==1, and adds a motivating unit test thatrequires this updated behavior.* Updated to retain default behavior of DetectIterMapTo avoid breaking existing test cases, updated to maintain the samedefault behavior, but a flag to maintain trivial iterators in theresult.* Updated FFI and Python API for DetectIterMap	5
[COMMUNITY] csullivan -> Committer (#10364)	3
[CI][VitisAI] Update CI Vitis AI PyXIR version to v0.3.1 (#8814)* Update CI Vitis AI PyXIR version to v0.3.1* Add Vitis AI requirements to gen_requirements.py	1
[FIX,AUTOTVM] Add flop counts to cublas (#7297)	1
tvmc: solve a linting error on onnx command line driver frontend (#6536)* Updates onnx load function from "load" (a compatibility attribute)   to "load_model" (the actual function) * Add a pylint command, that we don't see currently on upstream CI,   but it reproduces when linting it locally.	1
[ci] Disable flaky tuning test (#10490)See #10489Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TVMSCRIPT] TVMScript Parser support BufferSlice indices (#8408)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[CoreML] Solve CoreML frontend issue of image scaler and padding so that Mobilenet mlmodel can work correctly. (#3800)	1
[CI] Add m6g instance (ARM64) to mainline CI (#6804)* [CI] Add m6g instance (ARM64) to CI (#6781)* [CI] Add m6g instance (ARM64) to CI* address commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-54-90.us-west-2.compute.internal>* [CI] fix cpp test (#6796)* Update tests/python/unittest/test_target_codegen_x86.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Ubuntu <ubuntu@ip-172-31-54-90.us-west-2.compute.internal>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[CI] Apply linting rules to AOT tests (#11657)This enables pylint against the AOT test cases.One issue I found was with the `tvm.testing.parameter` which breaks the naming convention rules in pylint (constants are upper case and function parameters are lower case). It may be worth a syntax similar to:```tvm.testing.parameter("enable_usmp", [True, False])```	3
[DLPACK] Upgrade to the latest version (#150)	3
[OpenCL] Use size_t instead of int64_t for OpenCL timer count (#12328)Resolves a few gcc warnings for comparing signed and unsignedintegers.	2
[BYOC][ACL] Depthwise convolution support (#7206)* [BYOC][ACL] Depthwise convolution supportAdded support for depthwise convolution. ACL only supports depth-wise convolution when kernel size is 3x3 and 5x5 and strides are (1, 1) or (2, 2), if this is not the case then fallback to TVM.Also rework tests to remove non-deterministic trials.*Compute Library for the Arm Architecture (ACL).*All credits to Luke Hutton @lhutton1Change-Id: Ida1f5802a65377b84325edf14a0149242c1af857* linter* CHECK -> ICHECKCo-authored-by: Luke Hutton <luke.hutton@arm.com>	4
[RELAY] Remove primitive attribute from composite function (#5014)* A composite function should not be primitive since we still may need to perform passes on it.Change-Id: If62d06d265234861a6ec0df7749dc1c339c1055c	4
[PyTorch][BugFix] PyTorch-TVM Bridge Build Scripts (#10527)	0
[FRONTEND][TENSORFLOW] Enable strided_slice with fix. (#2002)	0
[CodeGen] avoid crash if an exception is raised during llvm cpu codegen (#9786)* avoid crash if an exception is raised during llvm cpu codegen* use pytest.raises	3
[TOPI] dense API to remove redudant use_bias (#476)	1
Fix topi test for tensorcore (#5563)	3
[Relay] symbolic max_output_size  (#5844)* symbolic max_output_size* pylint* fix ci	0
fixed rocm runtime. set default gcn arch to be gfx803 (#544)	1
[Torch] Fix conv2d transpose with group (#10235)* [Torch] Fix conv2d transpose with group* lint* wrong issue number* do not run test on cuda	3
[Refactor][std::string --> String] IRModule is updated with String (#5523)* [std::string --> String] IRModule is updated with String* [1] Packedfunction updated* [2] Lint error fixed* [3] Remove std::string variant	4
[RELAY] Allow StructuralEqual/Hash via Var.vid (#6424)* [RELAY] Allow StructuralEqual/Hash via Var.vid* Reduce MSVC warning* Fix the data* Add atol of resize3d	1
Fix tf reshape (#4285)* Fix tf reshape* Fix test* Fix pylint* Fix pylint	0
[RPC] Take PageAllocator out of MinRPCServer, make it template parameter (#10219)* [RPC] Take PageAllocator out of MinRPCServer, make it template parameterThis way MinRPCServer can be instantiated with a custom PageAllocator.* Restart CI	2
[docs] Update tlcpack-sphinx-addon (#12188)This includes https://github.com/tlc-pack/tlcpack-sphinx-addon/commit/545450acaf0ee4e2932d8c5d9ab6e321d0bc86c8 which fixes the sphinx-gallery cards and closes #12156	0
[Torch] Experimental support for FX-quantized models (#10091)* works on resnet18 and deeplabv3* yolo5 conversion worked* fixed sigmoid* [Torch] Support clamp_min, clamp_max* fixed clamp_min* fixed quantize for 1 dim input* cleanup* improve inline_qparam impl* add clamp_min/max test* add fx quant test* cleanup* skip build in testing* black* improve clamp conversion* leave TODO on inf handling	5
[RUNTIME][ABI] Remove TVMValue as argument, use address (#236)	1
[DOC] Initial doc system (#88)* [DOC] Initial doc system* Migrate API* Update docs	2
[Tutorial] mxnet (#47)* [Tutorial] mxnetupdateadd from_gluonadd to __init__fix tutorial and from_gluonfix doc lintmerge from_mxnetfixfixfix tutorialfixfix header* fix tutorial* fix data* fix	0
[skip ci][hotfix] Fix broken lint (#10827)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CI] Update Docker images to bring TF 2.9 and integration tests (#12738)[CI] Update Docker images to tag 20220908-060034-62bdc91b1Updates all Docker images to tag 20220908-060034-62bdc91b1, toupdate TensorFlow/TFLite/Keras to 2.9, and cascaded dependenciessuch as numpy. Updates ethos-u-vela to 3.4.0.It also brings ONNX and PyTorch to ci_arm, to enable Integrationtests to be run in CI.Standadises the minimum CMake version required in CI to be 3.18.4,fixing apps/microtvm/zephyr_cmsisnn to require this version.Finally, adds a new import error in the tutorials documentationwhich doesn't affect the final result. The new warning added is'absl:Found untraced functions such as _jit_compiled_convolution_op'	1
Add tvm-bot to triage role. (#9675)* Based on discussion in https://discuss.tvm.apache.org/t/rfc-ci-add-a-skip-ci-tag-to-shortcut-builds-and-tests/11589/10 * We will probably continue to leverage this bot for other, similar PR review feedback.	5
[Relay] Legalize and AlterOpLayout for Int8 Intel. (#3961)	5
[PASS] Add save/load json (#1)	5
[TIR][CompactBufferAllocation] Improve upperbound estimation of buffer compaction (#12527)Hi, this change wants to add some minor updation to region estimator used by buffer compaction:- Add and clearify among `EstimateRegionStrictBound`, `EstimateRegionLowerBound` and `EstimateRegionUpperBound`     Originally we have `EstimateRegionLowerBound`, actually it implements strict bound estimation IMO. Now add `upper` and `strict` version for where we actually want them.- When estimating upperbounds (eg. in buffer compaction), try estimate each dimension independently when they are dependent accesses where `EstimateRegionLowerBound` is expected to fail.   Eg, `A[i, i], 3 < i < 16`  fails via `EstimateRegionLowerBound` who check indices be independent. But we can still try best to invoke strict bound analysis on each dimension individually.- If range->extent == 1 for `EvalSet(range, dom)`, invoke `EvalSet(range->min, dom)` instead.    Eg, `EvalSet([k*k, k*k+1), dom_k)` results to [-inf, +inf] due to current algorithm limitation but  `EvalSet(k*k, dom_k)` results to a range which makes more sense.	1
[RELAY][VM] Add shape_of instruction (#5855)	1
[COMMUNITY] New reviewer @leandron (#7112)	1
[PyTorch] [Relay] Add aten::pad (#11922)* add aten::pad* fix* fix CI	0
Use a uint64_t to serialize primitive_attrs in the Relay VM to fix 32bit RPC (#9169)	0
[CI] Pin NNPack pthreadtools version (#4152)	1
[TVMC] Allow direct numpy inputs to run_module (#7788)* progress, graph params need to figure out* black and lint* change np.load(inputs_file) to happen in drive_run* make inputs optionalCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>	1
Add support for passing arguments by args and kwargs when using executor (#2402)* Add support for passing arguments by args and kwargs when using executor* Fix linting* Update comment, and add arity checking* Small tweak to error message	0
Fix incorrect call to Unicode Win32 InetPton (#4306)* Fix incorrect call to Unicode Win32* Removed inet_pton call. Win32 already has it.	4
making quantization tweaks (#6731)	1
Add cuda imagenet inference benchmark script  (#73)* resnet example merged to imagenet* resnet example merged to imagenet* merge with master* merge with master* cuda imagenet benchmakr added* benchmark syntax fixed	0
[TOPI][SPIRV] Cast to float32 not float64 before log2 in sort/scan (#7669)* [TOPI] Cast to float32 before log2 in sort/scan* revert sort change since this seems unnecessary* only does cast to float32 on vk + dynamic input case* check against IntImm instead of Var* revert change* use clz for ceil_log2 when compiling for vk* add doc on ceil_log2* fix pylintCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	0
[CI] update oneDNN to v2.6 (#11140)* enable CI to get and build latest oneDNN release* remove the source code after installed* fix wget error and improve naming* refine the cmake/make commandsCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* pinned to v2.6 by default* simplify the logic and install to /usr/libCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>	1
MXNet NDArray bridge. (#930)* MXNet NDArray bridge.Support convert a tvm Function as MXNet's async NDArray function.* fix lint* update comment	5
Add nick to committer (#2143)	1
Add shuffle support to TVM (#3633)	1
[Relay][Frontend][ONNX] operator support NonZero (#5073)* [Relay][Frontend][ONNX] operator support: NonZero* update* Solve the build fail* solve the build fail* Replace ctx_list with tvm.cpu()	0
[Documentation]Fix example code in comment of tvm.build_module.build() (#4195)* Fix example code in comment of tvm.build_module.build()* Update build_module.py	5
[microTVM] Refactor zephyr installation + Update Zephyr RVM doc (#7915)* refactor* script* update* fix* different zephyr branch* trigger build	0
fake quantization to integer (#8228)	5
[Fix] avoid unexpected throw in AttrInitEntry (#6128)	5
[TIR] Specialize MutateArray in StmtFunctor. (#7486)StmtFunctor applies context dependent copy on write,which requires check over all the dependency chain.Such function is better suited as a special implementationto avoid misuse. This PR refactors the code to specializethe function.Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>	1
[Contrib] Add MKL DNN option (#4323)* [Contrib] Add MKL DNN* update* update	5
[QNN] Change default rouning to UPWARD. (#4131)	4
[Relay/TOPI][Op] Add variance and layer norm op (#3700)* Add LayerNorm op* update* fix* Add mean_std and mean_variance* add std and update doc* add license* x* lint* x* fix* fix doc	2
[Code Style] Changed code to match the tvm code style conventions. (#9040)* [Code Style] Changed code to match the tvm code style conventions.[Issue]While reviewing the tvm code, I noticed some naming convention issuesin the diag_ctx_ and current_func variables.Variable current_func should be current_func_ because it is a classvariableVariable diag_ctx_ should be diag_ctx , because it is a public variable[Solution]Changed the variables to match the tvm code style conventions* addressed comments* removed debug logic* fixed plint issue* fixed building issue* fixed whitespace issue* fixed linting error in type_solver.cc	0
fix compiling warning in simplify_expr.h (#7828)	2
[ONNX]Support Opset 13 split IFF the split is a constant (#9643)	1
[TOP] add conv2d_transpose (#217)* [TOP] add conv2d_transpose* update tvm* fix pylint	0
[Relay] [Parser] fix parser for cast. (#3873)* fix* lint	0
[External Codegen] Fix annotate pass static variable (#5023)'fannotate' in the annotate_target pass was designated asstatic. This meant that if you use the pass to annotatemore than one codegen, its value is not updated when thetarget changes resulting in incorrect annotation.Change-Id: Ib4f3af5cfbef44f29771818219755198ac313a0e	4
[TIR][Printer] Fix SelectNode TIRTextPrinter bracket mismatch (#7405)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	0
[TFLite] Implemented ONE_HOT Operator for TFLite (#6223)	1
[SCHEDULE]Improve bound deduce for loop partition (#743) (#755)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop* [IRbuild]Support automatically Name Loop Variable in IRBuilder (#719)    * add idx_num in class* using typical index [i, j, k] first, then i_suffix* keep inputs names* fix lint* improve comment of name* fix lint* [SCHEDULE]Improve bound deduce for loop partition (#743)    * add divided checking when deducing    * related testcase* fix* * transform LE and GE first* remove is_equal* modify testcase for edge cases checking* * fix comment* * fix lint* * apply transformation form LT -> LE, GT -> GE* * fix lint* simplify code and testcase* add negative co-efficient case* More complicated cases* add testcase* simplify testcase* comment case for now* fix testcase	3
Report JUnit test results for all TVM Python tests (#7450)* Enable JUnit parsing for Python tests* retrigger CI* prefix junit results with FFI type* remove - in junit prefix	0
[BUILD] Enable RTTI of most part of library, example extension pkg. (#161)	0
[3rdparty] sync submodules (#3229)	5
[CODEGEN] Enable inline llvm asm code (#1486)	0
[Runtime] Enable option to use OpenMP thread pool (#4089)	1
[ETHOSN] Support conversion of add to depthwise (#12531)In similar fashion to the conversion of mul to depthwise, this commitconverts add when one input is a constant of shape [1, ..., n] to adepthwise convolution. If neither input is a constant, the add isoffloaded naturally like before.The addition testing has been improved to use pytest features.	3
[CI] Add pre-check script to check sphinx doc build. (#4956)Introduce the check stage to the unittest stage for nowso we don't have to rebuild CI images.As we make additional CPU images to make use of the sphinx,consider move it to an earlier stage.	4
Run ONNX Node Tests on available targets (#8189)	1
[Relay] Expose FunctionGetAttr to Python (#4905)* [Relay] Expose FunctionGetAttr to Python* add testCo-authored-by: Jon Soifer <jonso@microsoft.com>	3
[DOCKER] Use clear name that is separate from ASF brand for cache (#6360)	1
[microTVM]Fix test util functions (#12641)* Fix test utils* Update python/tvm/micro/testing/utils.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>	1
[Runtime][Contrib] Support cudnn softmax (#5214)	1
Add Havisha to triagers and alphabetize. (#11005)- Havisha has offered to help with maintaining some of the roadmaps.	5
[TOP] GraphExecutor (#11)	5
[LLVM] Use ArrayRef<int> in calls to CreateShuffleVector (#5399)This switch was made in LLVM 11. Previously this function was expectingmask indices of type uint32_t. This variant is now deprecated.	1
[Relay] Expand type unification and other utilities (#2189)	5
[CODEGEN] Force not inline compute core for better debug (#557)* [CODEGEN] Force not inline compute core for better debug* also support llvm4	1
[Relay][Pass] Add pass to remove unused functions in relay module (#4334)* [Relay][Pass] Add pass to remove unused functions in relay module* Add tests* Fix lint* Fix visit order* Add pass argument* Fix	0
[FIX] Make master compile	1
[DOCS] Fix Sphinx Warning: the target found for cross-reference (#4925)* [DOCS] Fix Sphinx Warnings: the target found for cross-reference warnings* Fix the warning: undefined label	2
[HEXAGON] Change arch and do not disable assert (#11858)	3
[Arith][GPU]Rewrite simplify fix for Vectorized Cooperative Fetching (#5924)	0
[OP] Improve bitwise op type checks (#1415)	1
[LINT][PY] Fixes for pylint==2.4.4 (#4849)	0
[Rust] Update rust install in dockerfile (#1855)* Update rust docker* minor edit for consistency	2
Remove duplicated PackedFunc C++ test (#8812)I came across this file whilst looking at the C++ tests and realised it's aduplicate of the PackedFunc tests which doesn't get invoked.```$ diff -u tests/cpp/contrib/bnns.cc tests/cpp/packed_func_test.cc--- tests/cpp/contrib/bnns.cc   2021-07-30 12:59:33.830443830 +0000+++ tests/cpp/packed_func_test.cc       2021-08-23 12:47:43.193708421 +0000@@ -17,6 +17,13 @@  * under the License.  */+#include <dmlc/logging.h>+#include <gtest/gtest.h>+#include <tvm/runtime/packed_func.h>+#include <tvm/runtime/registry.h>+#include <tvm/tir/expr.h>+#include <tvm/tir/transform.h>+ TEST(PackedFunc, Basic) {   using namespace tvm;   using namespace tvm::tir;```	1
[LANG] Expose tvm.cast (#195)* [LANG] Expose tvm.cast* Update* Add unittest	3
Correctly build with -runtime=c without -system-lib (#7954)	5
[Relay] Change when int8 operations are converted to int16 on Arm (#12671)Currently, Relay QNN uses its `helper_no_fast_int8_hw_legalization` to convert most `int8` convolution and dense operations into `int16` ones on Arm. This currently occurs on ARM chips except for `v8.2a` chips with `dotprod` support.However, this behavior means that `int8` operations are replaced with `int16` ones on Cortex-M chips. On these chips `int16` is substantially slower, as while it saves a few sign extension operations, it doubles the amount of memory loads we need to perform. This PR changes when `helper_no_fast_int8_hw_legalization` is used on Arm, and instead makes **not** doing this replacement the standard. We will only do this replacement if we are on a chip with ASIMD support but without `v8.2a` and `dotprod`. This ensures that Cortex-M microcontrollers do not have `int8` operations turned into `int16` ones.I have also verified that this does, in fact, improve performance for some common models. For example, MobileNet_v1_0.25 on the Cortex-M4 saw a 10% performance improvement, compared to before this change. Accuracy does not seem to be affected.	4
delete dependency node (#121)	4
correct mistake in muladd function logic (#2269)Doesn't make sense to have %1 = mul(%x, %y) computed but never use the result %1	1
[TOPI] LRN & L2norm Operator (#1051)	1
Add test for the qnn_add operator (#4282)* Add test for the qnn_add operatorThe tests use fake quant approach so until the tf session tensors remain in float32.The test data has to be passed in uint8 because of how the tflite/tvm comparison works.Abs tolerance up to 1 is allowed for the qnn results. For now input_stats are hardcodedassuming the tests for the other qnn ops will pass the input data in the same range.* Separate qnn uint8 test function from the fp32 elemwise testsIsolate qnn uint8 elemwise testsRemove blank lines	3
[Quantization][RELAY] Add check against NCHWc ops in the quantization pass (#2646)* check in* fix typo* fix typo* change message* change message* typo* lint	2
[PASS] UnrollLoop, isolate arithmetic module. (#32)	4
add rocm schedules to topi C++ (#4507)This imports the CUDA schedules to rocm.	2
Add count_include_pad support to AvgPool (#1163)* Add count_include_pad support to AvgPool* Fix python_cpp/test_topi_pooling.py* Change auto to explicitly type, and fix format.	0
enable bmm (#12018)	0
added surpport for arg type of numeric float16 and testcase, fixed the (#10797)cierror	0
[Bugfix] Fix AutoTVM bug (#3462)* fix autotvm* fix bug when heap_items is empty	0
[CODEGEN] Concise typecast for threadIdx (#208)	5
[ci] Disable flaky cmsisnn tests (#10315)See #10314cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[FIX] Fix bug in resize2d unittest func name (#12498)	3
[TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests	3
[ETHOSN] Stricter data type conversion checks (#10271)The 21.11 update for the Ethos(TM)-N driver is slightly more strict inaccepting various operator attributes.	1
[Lang] Fix undef BijectiveLayout and add scalar layout support (#3105)	1
[Relay] Extract dataflow matcher data structure into header (#8774)* extract dataflow matcher data structure into a header file* lint* lint	2
check stmt in	5
[COMMUNITY] Mehrdad Hessar -> Committer (#10901)Please join us to welcome @mehrdadh as a new committer to TVM.Mehrdad has greatly contributed to the Hexagon backend, tvmc bug fixes as well as microTVM implementation. He has been also very active in the PR reviews, community meetings and forum discussion to share his ideas.- [Commits History](https://github.com/apache/tvm/commits?author=mehrdadh)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:mehrdadh)- [Community Forum Summary](https://discuss.tvm.apache.org/u/mehrdadh/summary)	0
improve antlr import error message (#4888)	0
Add resource_handle to both TVM_DLL_EXPORT_TYPED_FUNC and TVM_DLL_EXPORT_PACKED_FUNC macros in packed_func.h. This is a patch PR for #7388. (#7343)Co-authored-by: JC Li <jinli@nvidia.com>	0
[relay][op] Add shape func to tile (#4441)* [relay][op] Add shape func to tile* retrigger ci* check dynamic axes* retrigger ci	1
[TF] ignore Truncate in cast (#2022)	1
[Docker] Update onnxoptimizer to 0.2.7 (#12278)Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>	5
[RPC] Add explicit type cast to print. (#8524)	1
Add check to only cast opaque handles to cl::BufferDescriptor at runtime. (#8256)	1
[Community] Hao Lu -> Committer (#3789)	3
[NNVM][DOC] Update NNVM symbol documentation to latest. Ref. 1591 (#1599)	3
[OPT] Low-bit Quantization (#2116)* [QUANTIZE] Quantization implementation.* Update.* Update.* Update.* Update.	5
[REFACTOR] Migrate Low-level IR Passes into the New Stmt/Expr Mutator (#4607)* CombineContextCall* Migrate BoundChecker* Migrate CoprocSync* Migrate detect_device* Migrate loop_partition* Migrate infer_fragement* Migrate inject_copy_intrin* Migrate inject double buffer* Migrate lower_intrin and simplify* Migrate storage flatten* Migrate inject prefetch* Migrate inject_virtual_thread* migrate inline* Migrate lift attr scope* Migrate custom datatypes* migrate lower_thread_all_reduce* Migrate lower_tvm_builtin* migrate lower_warp memory* Migrate make_api.cc* Migrate remap_thread_axis* Migrate remove_no_op* migrate rewrite_unsafe_select* Migrate skip_assert simple_passes* Migrate split_host_device* Migrate ssa* Migrate storage_access* Migrate storage_rewrite* Migrate tensor_core* Migrate unroll_loop* Migrate vectorize* Migrate verify compact_buffer gpu_code* Migrate verify_memory* Migrate storage_sync* Remove unused refs to mutator* Migrate hybrid_op* Migrate tensorize* Migrate schedule ops* Migrate schedule_dataflow_rewrite* Migrate auto_inline_elemwise* Remove unecessary ref to visitor* remove unecessary ref* Migrate bound_deducer* Migrate domain_touched* Migrate autotvm feature touch extractor* Add annotations	1
explicit import testing (#956)* explicit import testing* Enable init api for extension modules	5
[BYOC] support arbitrary input dims for add/mul/relu of dnnl c_src codegen (#9127)* support arbitrary input dims for add/mul/relu of dnnl c_src codegen* fix lint* fixCo-authored-by: sunway <wei.sun@hexintek.com>	0
[ONNX] More Unit Tests! (#7956)* support same lower and maxpool in autopad* fix isinf tests* lower tolerance on roialign test becuase the onnx result is cropped to 4 decimal places* slow support for bottom-k* throw with nullptr in gathernd and scatternd, fix typo* fix lint* fix a copy typo	2
Feat(frontend-pytorch): Add input types argument and Support cast to float16. (#6546)1. Add input types argument for converting TorchScript file.2. Support casting float32 to float16 when converting to operation.	1
[PASS] More robust UnrollLoop configuratin (#576)	5
[TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	1
[Relay] Fixes to sum (#2439)	0
add layerNormal infer layout (#11784)	5
[CUTLASS] Residual connection fusion (#9820)* [CUTLASS] Support residual block fusion for conv2dcommit d4a78a3e13530974e852b4c0480b7c8d0f792e68Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:33:41 2021 +0900    fixed residual block check conditioncommit 6ee5a3913333e8ba2d5d0ed6842a58fe37baa547Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:25:04 2021 +0900    minor fixcommit 8af8b3078f11ee293d2e22d9e37e715c617ffb75Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:18:50 2021 +0900    remove SimplifyExpr passcommit 20ae2d874917c69fabc6fcf03a3d47aff98eee91Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:16:46 2021 +0900    fix bad mergecommit 17eed222c5e69e7863c95563b638e5390c634b1bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:13:53 2021 +0900    blackcommit fda151b524cb28581256befa74575bbfa23efa4cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:09:45 2021 +0900    Support residual block fusioncommit ce9d52fd629d6119abdd471b00ff6a79223d6752Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 15:56:32 2021 +0900    Remove SimplifyExpr pass from the pipeline (makes DETR result nan)commit d3b681d95977b6fc0965a0a3ec8af3f866bd9e91Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 15:47:07 2021 +0900    fix no_beta_scaling valuescommit 87b36dbbb11adb582ffb628fc6ad62668dcdee7eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 14:59:40 2021 +0900    fill in TODO doccommit fd67595831c7b8741f30577bc91488bcce34a76aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 14:31:06 2021 +0900    Refactor cutlass kernel generation and selection* do not try to support broadcast binary op* add comments* remove residual input shape check	4
[Unittest] Fixing unittest (#9180)	3
[relay][op] multibox_transform_loc (#2315)	5
[Codegen][LLVM] Add ability to turn on fast math flags (#9223)* flags to turn off and on* turn fast math on always* llvm more opts* move to default codegen opt* TODO* add fast math options to llvm target* move to using new target attributes* llvm fast math target opt code* add -O flags* fix todo lint* support llvm 4.0, 5.0* use same opt level as target machine* revert TargetOptions* fix thing* prevent regression in llvm* togglable opt-levelsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[DOC] minor grammatical improvements (#3341)	1
[EXECUTOR] Move tvm_op and Handler<DLTensor> to graph_executor.cc (#259)	0
Bug fix for debug builds in micro_session.cc (#6968)* If the build decides not to inline kReceiveBufferSizeBytes,  we will encounter a linking error.Change-Id: Ibbe5b20fdd63acb2b4652ca9896f5737eaf14b00	4
[TIR] Ignore Allocate/AllocateConst in BufferAllocationLocator (#10998)* [TIR] Ignore Allocate/AllocateConst in BufferAllocationLocatorPrior to this commit, the BufferAllocationLocator mutator used in thePlanAndUpdateBufferAllocationLocation pass would erroneously insert anentry to `BlockNode::alloc_buffers` for buffers allocated using`Allocate` or `AllocateConst` nodes.  This error was introduced inhttps://github.com/apache/tvm/pull/9727, which deprecated `Load` and`Store` nodes, replacing them with `BufferLoad` and `BufferStore`nodes.  As a result, BufferAllocationLocator identified these asbuffers whose allocations should be moved to inner loops, rather thanas unmanaged allocations that should be ignored.This commit restores the earlier behavior by only operating on bufferallocations in `BlockNode::alloc_buffers`, and explicitly ignoring anybuffers whose allocation is done with `Allocate` or `AllocateConst`.* Only inject opaque block if managed buffers exist.Previously, all buffers found were managed buffers, so this checkwasn't needed.	1
Add Logan to reviewer (#4390)	2
Remove run_infer_type duplicates (#4766)	5
[TOPI] Depth wise convolution backward methods for NHWC (#434)* rename the nchw and pass the unit test; going to do it for nhwc depthwise* bug with fusion* nchw works fine; nhwc float32 problem remains* still cannot bind them together* fusion works* syntax fix* all bugs fixed; test cases pass* minor fix on nn.h* back wrt input* backward wrt input nhwc; only test case in recipe* test case for depthwise back wrt input* test case for depthwise backward wrt weight* tags* minor fixes* pylint test; add arch=3.7* modify scheduler* better backward depthwise w.r.t weight scheduler* updated scheduler* test_topi_depthwise_conv2d_back_input.py and test_topi_depthwise_conv2d_back_weight.py success* all test cases wrt input pass* update* new test cases and scheduler* not working 1 and 2* good wrt weight, bad wrt input* test cases added* remove tf lines* minor fix* compute arch changed* remove compile hook* minor change* pylint* fix the float for python case* fix cases for python3 case* except for memoize* fix most; memoize still wrong* memoize added* unexpected layout cases added for scheduler* error message layout other than NHWC added* improve padding* fix as pr requests* remove dilate in backward wrt weight	4
[fix][pass] Save the function when it is used as a call arg (#4389)	1
[LLVM] Fix CodeGenLLVM::LinkParameters (#8213)- Generate valid LLVM IR.- Set proper alignment on the constant variables.	1
[TOPI] Add topi.target; Schedule for raspberry pi (#406)* CPU Schedule for raspberry pi* Update* Update* Add topi.target* Refactor* Update* Make python3 happy* Improve* Improve* Improve* Use get_const_int	1
Make CMakefile/config.cmake/install_tvm consistent (#6562)	5
[RELAY][OP]Reduction operator framework, argmax, argmin (#1865)	1
[AutoScheduler] Add function name in message (#7703)* [AutoScheduler] Add function name in message* fix	0
[USMP] Improve algorithm extensibility (#11880)* [USMP] Improve algorithm extensibility* [USMP] add option for custom_algorithm to avoid PackedFunc on default path* [USMP] add test for custom algorithm* [lint] fix wrong line length* [USMP][test] fix PoolInfo for latest tvm	3
[REDO AFTER GH BUG] Add support for quantized models via QNN (#5016)This reverts commit f346c60287b50950275e20db9e6d84b3fc568a00.	5
[LLVM] Create TBAA information based on the unrelying buffer type (#6046)Currently, the TBAA information is based on the access type, i.e.the data type from the load or store instruction. When the samememory area is accessed with different types, the correspondingload/store instruction may end up not being aliased to each other.This could lead to incorrect code being generated.An example of when such a situation can occur is when two differentbuffer_decl's are created for the same buffer:  ba = buffer_decl(... dtype = 'int16' ...)  bb = buffer_decl(data = ba.data, dtype = 'int32x32' ...)Then instructions  ba[x] = 0  ... = bb[x]may be reordered in the final code due to the alias info indicatingthat they are not aliased.	5
[LLVM] Fix a possible tbaa issue (#11181)* fix a possible tbaa issue* Correct tbaa index unit by underlying buffer elemtype* always use byte as index unit in tbaa	1
[PASS] RewriteUnsafeSelect lowers unsafe select to condition expr (#335)	4
[ONNX] Add index_put operator (#8894)* onnx:add index_put* reformat code* add parametrize_targets* change slice to onnx_index instance* modify test_forward	3
[Contrib] cblas batch_matmul (#3210)	5
Fix logging in autotvm record (#2195)	2
[Runtime][ThreadPool] Enhance CPU Affinity configuration for OpenMP case. (#11343)This commit allows to pin threads to cores when we use OMP. It enhances`tvm::runtime::threading::Configure` method to work with OMP and "kSpecify"affinity mode.	5
[TOPI] Fix the CPU op perf (#56)	0
[Relay/TOPI][Op] Add erf intrinsic and op (#3702)* add more ops* stop vectorization for erf* x* cleanup* fix* add whitelist for vectorizable intrin* add tf converter* fix dense* fix* add missing intrin* fix mxnet frontend* fix nvptx	0
Added support for tflite quantized maximum and minimum (#6018)* Added support for tflite quantized maximum and minimum* Unit test simplifiedBugfix in unit test. Unit test slightly simplified* re-trigger CI* renamed use_real_qnn to ignore_qnn_params	2
[Codegen][CUDA] Fix make_int4x cuda codegen vectorize (#8137)Co-authored-by: wangyucheng <wangyucheng@sensetime.com>	1
[TVM PyTorch Integration] libstdc++ CXX11 ABI Compatibility & boolean tensor support (#12232)* first commit* rename* cmake* deprecated* newline* config* config* typo* skip tvm_class* rename* delete ptr* delete ptr* save progress* boolean support* cmake file* polish code* compile config* improving the codes* format* doc&errormsg* zero-cost copy* one step* to ndarray* extra output* delete extra codes* update test* boolean support* strong test* decrease memory copy* polish* reformat* polish* remove redundant importCo-authored-by: juda <yzhou@octoml.ai>	5
[Ansor][AutoTVM v2.0] Phase 2: Evolutionary Search (#6310)* init commit* Add rest rules* refactor* address comments* improve test* address comments	1
[TVMScript] Base IRBuilder methods for `PrimFunc` (#12745)Base IRBuilder methods for `PrimFunc`This PR introduces base IRBuilder methods for `PrimFunc`.Co-authored-by: yongwww <yongcale@gmail.com>Co-authored-by: yongwww <yongcale@gmail.com>	5
Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a	5
[Docker] Update CI CPU and GPU images based on new Docker build files. (#6690)* Turn on Rust docs and MxNet based ResNet* Add deps needed for Rust examples and docs* Setup Rust path* Bump Jenkinsfile* Fix broken version setting, which instead redirects stdout and stderr* Update Jenkinsfile* Format* Disable Rust change for now* Completely disable ResNet* Reset test changes* Remove temporary labels* Remove patch needed for docs	2
Stop pylint complaining about useless import alias. (#2655)Recent pylint warngs about import renames with no effect.  Removethem.	4
[ci] Rebuild Docker images if necessary (#11329)This rebuilds Docker images and uses them in later stages in the same build. If the build is running on `main`, then the images are uploaded to Docker Hub automatically once the run is complete. Images are always rebuilt, but Docker Hub functions as a cache. If there have been no changes to `docker/` since the last available hash on Docker Hub, then the build will just use the images from Hub.	1
[CMake][Minor] Update CMake warning flags (#8152)	2
Fix auto-scheduling after 9c6658721 (#8478)	0
[CI] Pin mypy version (#8329)	5
[Relay][Frontend][Tensorflow] Fix type assignment for operator 'tf.range' (#4294)	1
[Runtime] Driver version + consistent clock speed units (#7867)* Added kDriverVersion to DeviceAttrKind, implemented for VulkanDeviceAPI.The vulkan backend has had inconsistencies that look correlated todrivers used.  This will help in collecting information fortroubleshooting.* Changed units for OpenCL's clock rate from MHz to kHz, to match Cuda/ROCm.* [Docs][Runtime] Additional documentation for tvm.runtime.Device, DeviceAPI feature matchingPrimarily documentation, with some changes to the OpenCL DeviceAPI tomatch available features in cuda/vulkan.* Added CL_TARGET_OPENCL_VERSION definition, for use with unified OpenCL headers.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[COMMUNITY] Add Ziheng's key for ASF release (#6552)	1
[Release] resolve license issues (#4408)	0
[CODEGEN] Enable closure with no argument (#635)	0
{QNN] Making scale/zero_points as expr instead of attrs. (#4611)	1
[CI] Update GPU image to use newer CMake (#11194)Requested in https://github.com/apache/tvm/pull/11156Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/257/pipeline	2
update ci-gpu to v0.78 (#9378)	5
[Relay] Support for PyTorch Non-Maximum Suppression (#6314)* [Relay] Support for PyTorch Non-Maximum Suppression* fix comment* add verify_model_vm	1
[LANG/BUFFER] Change buffer arguments to match DLPack order, add scope (#203)	1
[RUNTIME][DEBUG]Support remote debugging (#1866)	0
Fail early before running invalid dynamic graphs (#5856)* fail early before running invalid dynamic graphs* fix an issue with the VM comment	0
[DOCKER] Add docker demo image (#1404)	2
[NNVM][TENSORFLOW] Mobilenet support. (#1335)	1
[RUNTIME][uTVM] AutoTVM + uTVM for Cortex-M7 (#5417)* Prototype for micro TVM.* Cleanup and sync micro tvm prototype.* Use /std:c++14 with MSVC. * Per tqchen: project has already moved to C++14 * Presubmit failed for code that built locally on gcc.* fix ASF lint, and fix add_asf_header too* Compiles with USE_MICRO=OFF.* Cleanup TargetPtr and word size representations.* fix compile warning* address logan's comments* address logan and liangfu comments* address thierry's comments* address u99127, liangfu, tmoreau89 commentsCo-authored-by: Logan Weber <weberlo@cs.washington.edu>	2
[ONNX][Relay] Add dynamic unsqueeze / expand_dims op (#9039)* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* working tests* add test, remove unneeded file* fix things* more lint* more lint* pick things* disable opencl tests* unsqueeze tests* clean up* dyn stuff* add num_newaxisCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
Conda build recipe (#288)* Typofix.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Probe for nvrtc in lib directory as well.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Conda build recipe for TVM.Signed-off-by: Edward Z. Yang <ezyang@fb.com>	2
[Relay][Transform] quantize opt passes to pass manager (#3289)	4
fix mcpu on os x (#7276)	0
[DOCS] Initial docs (#4)* [DOCS] Initial docs* update instruction	5
Fix Jenkins pipeline (#835)	0
Update installation guide of windows (#364)* update installation guide of windows* update installation doc of windows	2
[CI] Pin h5py version to < 3.0 to workaround issues with TF/Keras (#6808)* Pin h5py to use the previous major release (2.x) and not   new version 3.0, due to incompatibilities with TF and Keras   that make TVMC and Frontend tests to fail	0
[microNPU] Enforce bias when pattern matching conv2d (#9244)Currently a conv2d pattern is matched when no bias is present.However, legalization expects a bias to be present, thereforecausing an error when this is not the case. For now, enforce abias when offloading conv2d to the NPU.Change-Id: I7f74b0f2c151f51ddc66ee1c5ebb77534238909b	4
clean standalone CRT files in microTVM VM rebuild script (#7095)	2
Sum operator for ONNX. (#386)	1
[TESTS] Triage the testcases to fit the the new namespaces (#5071)* [TESTS] Triage the testcases to fit the naming convention of the new namespaces* Remove multiple usage of system lib to avoid test problems	0
[DOCS] Update pass infra tutorial (#6193)* [DOCS] Update pass infra tutorial* update tutorial	5
[TIR][REFACTOR][API-Change] Migrate tir/stmt.h to use constructor. (#5778)This PR migrate tvm/tir/stmt.h to the new constructor style that isconsistent with the rest of the codebase and changes the affected files accordingly.	2
[ETHOSN] Improved handling of 5d reshapes (#10860)Resolves an issue with 5d reshapes in the Yolo network and added atest case. Refactored the reshape tests to use parametrization.	2
load empty config (#6100)	5
[VTA] VTA hardware/software codebase re-org (#5037)	2
Relaxing convolution infer checks. (#3511)- Weight dtype can be different than idtype. So, using the weight tensor to setthe dtype of weight.- For conv2d NCHWc operator, the weight can be of any dimension. For int8computation on Intel, it can be 7D. Relaxing the weight type checking.	1
Remove cython init messaging. (#1110)	5
Add creation of Hexagon device in RPC client (#6035)	1
[BYOC][Optimization] Run accelerator specific optimizations  (#6068)* register and invoke optimization pipeline for external codegen* add unit test	3
[COMMUNITY] Alexander Peskov -> Reviewers (#11648)* adding ramana to reviewers list* adding apeskov as reviewer* fix	0
[Frontend][Tensorflow] Support range like axis in tf.raw_ops.All for TF 2.x (#7502)* add TF2.x raw_ops.all axis range support* apply linting* fix range() func input	0
[Runtime][Object] Add Object::unique() (#7615)	1
[MetaSchedule] Complete NCHW Conv2D Winograd Kernel Scheduling (#12648)* Complete winograd scheduling.* Fix test.	3
[OpenCL] Fix OpenCL get_valid_counts errors due to intrinsic atomic_add (#5857)* [OpenCL] Fix atomic add used by get_valid_counts* Rename l -> load, add flag to enable atomics* Opencl doesn't do data rearrangement	5
[Relay][Frontend][TFlite] Add test for qnn_mul operator (#4395)* Add a function to set the qnn output range wrt each elemwise operation.* Add comments warning for nonsense clamped output in the tflite/tvm results comparison.	2
[Meta Schedule][M3b] Runner (#9111)This PR is part of the meta schedule project (#8473) that adds theasynchronous program runner interface, as well as a referenceimplementation of RPCRunner. LocalRunner will be implemented withPopenPool executor in a follow-up PR.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Address commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>fix lint	0
[DOCKER] Fix CI script (#1826)	0
[Refactor] Relay Node::make to constructor (#5128)* relay Node::make to constructor* patternwildcard* Address comments	1
Enable NPU and CMSIS in ci_qemu (#9957)These are required for running the demos under ci_qemu in combination with Zephyr	1
[SYMBOL] Change list_input->list_input_names, add list_input_variables (#59)* [SYMBOL] Change list_input->list_input_names, add list_input_variables* fix	0
[ci] Don't skip index-triggered builds (#11915)This code was there to stop Jenkins restarts from doing a repository scan and scheduling a ton of builds. However, I haven't noticed this happening during restarts lately, and repository scans are useful to patch up PRs that didn't get CI run properly (i.e. while Jenkins was down or something).For example in #11914 since this code is there all the messed up PRs needed their CI to be manually re-triggered even though they were detected during the scan.	1
Fix 2 spelling mistakes (#129)	0
[microNPU] Move the compilation to use Target Hooks. (#9597)* [microNPU] Move the compilation to use Target Hooks.This commits moves the current compilation flowto use target hooks, so that the generated TIRis provided to unified module to for unifiedoptimizations.Change-Id: Ib3239a04ab201748e7f1b1ffa503cfe2aa7ccb7b* [microNPU] Move the compilation to use Target Hooks.*Fixing unpacked API tests*Adding use_device_api target attr to example target hooksChange-Id: I72c51caa57e9a0c2a538f40eb73939e28d4f112f* [microNPU] Move the compilation to use Target Hooks.* Modifed CLZ test case to support target hooks* Modifed reference TIR for test to include allocate annotation* TIR to CS translation tests are modified to run MakeUnpackedAPIChange-Id: I3a3d28777a6995e7f2b8789e14c5cb0f280dc763* [microNPU] Move the compilation to use Target Hooks.* Added a missed documentation to changes in source module* Skipping device api test for packed API as microNPU does not  support it.Change-Id: I6da1adcf8fdd3f972ec9b37ff530ff673e93058c* [microNPU] Move the compilation to use Target Hooks.* fixed tvmc test use unpacked-api for microNPU compilationChange-Id: Ib722d91ca3b3e4c6d13075ee0873acb86f487247* [microNPU] Move the compilation to use Target Hooks.* adjust target name.Change-Id: I862957324440705fb6093939b97b1a00fa1d4b46* [microNPU] follow up on using target hooks* Fixed few typos and cleaned up as per suggestionsChange-Id: I2a744a4bc4015e1884dbef4165252aa13aa30b31* [microNPU] follow up on using target hooksFixing some typos and change params toconst_dict as it seems more clearerChange-Id: Ia36a4635a68f6490bcc3eeaa72eeeeaadb6aa7f6* [microNPU] Move the compilation to use Target Hooks.Fixing up lookup table tests to use new runtime moduleimport structure resulted from using target hooks.Change-Id: I250aedef7cc73edad3812bb7e9aab013ed8bed5b	4
[ANDROID][RPC] Remove binary distro jar (#677)* [RPC][JVM] Remove binary dist gradle from repo* fix header	0
Memory leak in the relay interpreter (#3448)	5
update rocm intrin rule (#4499)	5
fix mxnet model import (#449)	2
[TensorIR] Print TVMScript with prefix T instead of tir (#9422)	0
[TEST] Various CI fixes for the VTA and Relay  (#5181)* [VTA] Set the correct type for synchronize* Fix the legacy API* Temporary remove the structural equal	4
doc: fix description of stop_fusion annotation (#8095)	0
Fix array pointers releasing with `delete` operator (#11328)It may be safe to release POD-types array with `delete`operator, but `delete[]` is always better.	1
[Relay][PRNG] Add uniform distribution generator wrt threefry PRNG (#8041)* Add uniform distribution generator wrt threefry PRNG* fix lint* remove the redundant print* modifications based on review* update docs* update uniform algorithm to use bit operations only* add type restrictions* minor fix upon review* update test and error information	5
[TE][TIR] Enable CreatePrimFunc to properly handle 'layout_free_placeholder' (#11054)`layout_free_placeholder` is used to guide proper layout transformation on TE/TIR-level. However, previously it is not properly supported on upstream AutoTIR. This PR introduces legalization of this block annotation into function attributes.Note that this attribute is not useful on Relax end-to-end tuning, because in Relax @jinhongyii developed a set of more powerful mechanisms to handle these cases more effectively without introducing bugs like https://github.com/apache/tvm/issues/9476.	0
[microTVM][ARM] Keep microtvm testing only in QEMU Image (#11809)* Move scripts* Address comments* move ethosu tests* move cmsisnn tests to qemu	3
[TIR] Enhance software pipeline validation and fix predicate of epilogue (#11106)* Fix pipeline validation* fix predicate* Update test_tir_transform_inject_software_pipeline.py* Update inject_software_pipeline.cc	5
Support x86 dilation conv2d and improve multi-batch conv2d (#3308)* Support x86 dilation conv2d and improve multi-batch conv2d* Fix lint	0
[RUNTIME][FFI] Fix cython FFI compact with np.int64 (#6321)	0
Fix typo in include/tvm/runtime/crt/crt.h and NEWS.md (#7770)* Fix typo in include/tvm/runtime/crt/crt.h and NEWS.md	1
doc: fixes to dataflow_pattern (#8247)	5
avoid loop dependent allocation in buffer compaction (#11428)	5
[VTA] Fix VTA function Vivado Compile Error. (#3375)Issue:when using vivado compile vta.cc with top function 'vta', vivadoreport deadlock error like '...with default size is used in a non -dataflowregion, which may result in deadlock Please consider to resize thestream using the directive ‘set_directive_stream’ or the ‘HL S stream’pragma.'Solution:give the queue a default size as 8.	1
[Hexagon] Pass kDLHexagon device when allocating workspace pool on Hexagon (#10289)	1
[µTVM] Zephyr: Fix missing board-specific config file in build dir (#8230)Currently board-specific config files (boards/*.conf) are notcopied from Zephyr project dir to the destination build dir, soas a consequence the per board configs are not used when buildingthe runtime libraries, like libcommon. Hence, for instance, it'scurrently not possible to set CONFIG_FPU per board since it onlytakes effect when it's set in the generic 'prj.con' config file.This commit fixes it by copying to the build dir (to each libdir) the proper .conf for the selected target board. For example,if target 'qemu_x86' is selected 'qemu_x86.conf' is copied tothe boards/ dir inside the lib dirs, so Zephyr build system canfind it and combine it with configs found in the generic 'prj.conf'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	5
[Keras] fix convert_pooling with same pad (#322)	0
[DOCS] Fix markdown syntax error (#430)Fix markdown syntax error (code shifts out of markdown-code box).	0
[MyPy] Minimal type checking on TIR schedule (#8367)* [MyPy] Minimal type checking on TIR schedule* [MyPy] Remove set +e from runnning script	1
Remove device type dependency (#11198)	4
[microNPU][3] Plan generation for the cascader (#9890)* [microNPU][3] Plan generation for the cascaderThe cascader creates 'Plans' which describe howto schedule subgraphs. As part of the cascadingalgorithm, it's necessary to explore a largevariety of Plans which are Pareto optimal (interms of memory usage and performance). This isdone by the Plan generation algorithm.This commit adds the TensorConfig and Plan datastructures which hold information on how to schedulethe tensors/operators. Additionally, it includesfunctions to calculate Pareto frontiers which areused to cull sub-optimal Plans.Change-Id: Ia358b2a1b29bd810df4441027752ced75812ad4e* Fixes to lint/testChange-Id: If4e083a3c96af75a8ffa72510704818d21a477d9* Improve python docsChange-Id: I831137f8235665bc20ab4c060cc7049ffd48088a* Fix enum hashing issue with old gccChange-Id: Ifbe97eb33b1ef313710f24c687a8155421a3c195	4
[CI] separate out legacy as a stage (#3337)	5
extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>	5
[Relay][Frontend] Fix MxNet RNN without providing state initialization as input (#3326)	5
[COMMUNITY] @kevinthesun -> committer (#2760)	3
Support mean in NNVM to Relay converter. (#2680)	1
Options to create test directory and print commands in AOT Test Runner (#9638)* Options to create test directory and print commandsChange-Id: I381b4dfe870c9f6462681e77ebf0d3187749a535* Lint fixesChange-Id: I043d4e403df242ed304e10e98828fb1c982aac43* Addressed review comments: fixed incorrect docstringChange-Id: Ic0c7528986d87cb04ef5ccb03bf98ccf87750675	4
[EXECUTOR] Fix bug and improve (#252)* [EXECUTOR] Fix bug and improve* [EXECUTOR] Enhance test case	3
[Quantize] Skip for same input-output domain scale. (#2611)	5
[Onnx] Fix NLL Loss tests (#8971)* support negatibve indices in gather* move check to Tensor level indexing, gathernd* add test, update transform.h* remove unneeded gather* missing gather nd change* update tests* proper tensor comparison* blacking* lint* fix error* turn on test* missing test case* revert changes* add normalize_gather_indices* undo change* update* more removing diffs* more undoingCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[TOPI][AlterOpLayout][ARM] Enabling NHWC to NCHW layout transformation. (#4249)	0
[Relay] fix anf for reference and pattern matching (#2637)	0
Add cloudpickle dependency to docker images (#6701)	2
Add FlattenAtrousConv pass into the default optimize pipeline. (#11077)	4
[CONV] Reduce data size of asymmetric padding testcase (#4658)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>	1
[RELAY] Move frontend utils (#5345)* [RELAY] Move frontend utilsThe util file currently under frontend is used fromoutside of frontend (in qnn/op/legalizations). This suggeststhat the file should be pushed up to a higher level.The benefit from this change is that importing qnn no longeralso imports all the frontends.* Inline get_scalar_from_constantChange-Id: I1cc64e9ecb0eadb6ac0f7b62e6ea174644af4ad4* Remove util.py from RelayChange-Id: If9cd7cf3fc0bd1861a3a9b5604f338e084d8db96* Shorten functionsChange-Id: Ieb537d82e6ee52421ff05a90cd00a03679ffebf2* Line lengthChange-Id: I1d216b7e73a060c4f118f5da50ce58b18eba907f	4
Fix a spelling mistake (#136)	0
remove AttrsEqual and AttrsHash related code (#5169)	4
[Hexagon] Refactor Hexagon.cmake (#10227)This file is included every time TVM is build, regardless of whetherany support for Hexagon is enabled or not. This refactoring is meantto remove underlying assumptions about what features are enabled andwhat the compilation targets are. Now, when there is nothing neededfrom Hexagon, the script exits early (although it doesn't need to),and the rest of it is (and should remain) safe to execute regardlessof build configuration.Disable "runtime.module.loadfile_hexagon" from the offload runtime,since it conflicts with device_api.hexagon.v2.It was only used with offload on Android, which is being deprecated.	1
[tests][hexagon] Fix `allocate_hexagon_array` bug. (#11709)Fix bug where `allocate_hexagon_array` in`tests/python/contrib/test_hexagon/infrastructure.py` wasn'trespecting the caller-specified `memory_scope`.	5
[COMMUNITY] MichaelJKlaiber -> reviewer (#12501)	3
Add support for tflite arg_min and arg_max (#5992)* [Relay][Frontend][TFLite] Add parser support for arg_min_max* this implementation supports only the case when the axis is a scalar* tflite 1.13 removes all dims of size 1, Relay doesn't do this* WARNING: every newer version of tflite > 1.13 needs keepdims=TRUE* Migrated to tflite 2.1.0keepdims set to False and added some checksNote the unit tests emmitted following warning:/workspace/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(T_multiply_red_temp, 0x53f5050)* linter* Removed quantized argminRemoved quantized argmin due to inablility to provide proper test case* added negative ranges* re-trigger CICo-authored-by: Ina_Dobreva <Ina.Dobreva@arm.com>	1
Enable python debug runtime for exported network libraries (#8793)* Add get_json method to graph_eceutor factorySigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Update Debugger runtime documentation for exported libraries* Fix cpplint* Change module get_json to get_graph_json, add test* Fix get_graph_json test* Change verificatino of llvm support in tet to decorator* Fix sphinx warning in debugger.rstCo-authored-by: Alexander Peskov <peskovnn@gmail.com>	0
[Docker] Update tensorflow/tflite/xgboost versions (#8306)* [Docker] Updated tensorflow/tflite version to 2.4.2Tensorflow update required following update to cuda 11.0.  Based onhttps://www.tensorflow.org/install/source#gpu, the 2.4 branch oftensorflow should be used with cuda 11.0.- Removed pinned version of keras/h5py, no longer needed.  https://github.com/tensorflow/tensorflow/issues/44467#issuecomment-720631688- Updated tflite version to 2.4.2.  Also, tflite install script now  reads the installed version of tensorflow, to keep the version  matched in the future.* [Docker] Corrected version pinning of xgboostPreviously, due to missing quotes, installed most recent version ofxgboost, piping the results to a file named '=1.1.0'.  Now, installsxgboost at least at version 1.1.0.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
add onnx elemwise greater/less (#3186)	1
Add fallback for ApplyGraphBest (#2485)	1
[Relay] fix exponential blowup in interpreter (#3559)	0
[DOC][DEVGuide] Runtime system note (#467)	5
Update the tvmc tutorial with additional requirements (#8334)* Updates the tvmc tutorial with additional requirementsThe pre- and post-processing scripts supplied in this tutorialrequire pillow to be installed, and this tutorial also requiresthat onnx be installed. This patch indicates those are requirementsfor successful completion of this tutorial.* Fix typo in tvmc tutorial, update to new tvmc outputFixed a typo in a tutorial command, and updated the outputto reflect the current output of TVMC* Update tutorial to indicate tvmc operating system supportTVMC does not currently work on macOS or Windows.	1
Removing older Object detection TFlite test (#5477)	3
[MetaSchedule][Minor] Fix Integer Overflow in Tuning Statistics (#10935)* [MetaSchedule][Minor] Fix Integer Overflow in Tuning StatisticsThis PR fixes the integer overflow when the flop count of a given workload is larger than `MAX_INT` during tuning statistics printing.* Fix linting.* Support printing int64_t.	1
[Topi][Cuda]Optimizations of global_ave_pool for NHWC layout (#5450)* Optimizations of global_ave_pool for NHWC layout* Optimize the code format to pass inspection of pylintCo-authored-by: Shawn-Inspur <wushaohua@inspur.com>	4
[Frontend][PaddlePaddle] Fix bug for paddle frontend (#9236)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes* fix bug for paddlepaddle frontend	1
[skip ci] Fix scipy intersphinx link (#11399)Follow-up from https://github.com/apache/tvm/pull/10181, as the URLhas changed again in https://github.com/scipy/scipy/pull/16221.  From[thiscomment](https://github.com/scipy/scipy/issues/14267#issuecomment-1034196161),the `html-scipyorg` portion wasn't intended to be part of the URL.This should resolve the HTTP 404 occurring in `Docs: GPU`step (e.g. [here](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-11269/13/pipeline/405#step-975-log-73)),by accessing `https://docs.scipy.org/doc/scipy-1.8.0/objects.inv`instead of`https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/objects.inv`	2
[PASS] Allow allocation in parallel scope (#305)	1
[Fix] Fix get_valid_count flaky test for cuda (#4901)* get_valid_count accuracy issue fixed for individual tests but not for all tests running together* minor fix* initialize valid_count and PrefixSum buffers* test updated* udpate relay test as well* update document* fix lint* address comment* fix lint* correct atomicAdd identifier name	1
Expose llvm.nearbyint intrinsic. This is a faster alternate to rounding. (#4001)* Expose llvm.nearbyint intrinsic. This is a faster alternate to rounding.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Added python binding. Added test.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:	3
µTVM CRT modifications for on-device RPC server (#5921)* Reorganize CRT into parts, public API, and add standalone build. * Create a make-based build in src/runtime/crt. This is intended to   be built in build/standalone_crt (generated by running ninja   standalone_crt in build/). Its job is to build CRT without   depending on headers not explicitly allowed in CRT. * Create a "public-facing" CRT API targeted to firmware running   alongside CRT in include/tvm/runtime/crt. Developers who are   integrating the CRT are the target of this API. * Reorganize CRT internally into common/ and graph_runtime/   pieces. Build each pieces as a separate statically-linked library. * Slim down TVMGraphRuntime public-facing API to just the functions   that are used externally. * Updates to apps/bundle_deploy to make this work.* Add TVMFuncRegistry, CRT test infrastructure, and tests. * Also add error_codes.h, a file containing error codes returned by CRT.* Add TVMErrorf()* [API_CHANGE] Integrate func registry into CRT. * NOTE: This changes the default API for functions exposed under the   CRT by the TVMFuncCall API. `resource_handle` is now always given   as a new 6th parameter. * `resource_handle` is NULL when invoked on a global function and a   pointer to the module owning the function otherwise.* Generalize arena-based memory manager.* lint* Fix git-clang-format arg parsing* add apache header* add mutable func registry tests* git-clang-format* fix more lint* Move memory_test to crttests.* fix tests* checkpoint* checkpoint* bundle_deploy demo_static works* rm debug printf* git-clang-format* fix lint* add asf header* pylint* update build configs for jenkins* make regression compiler happy* fix build errors in regression GCC* address comments* git-clang-format* fix for 32-bit cpp regression* fix incorrect use of memcpy and tests for 32-bit* clang-format	3
Add alternate cublaslt library name. CUDA 11.0 uses cublasLt. (#6541)	1
[FIX,AUTOTVM] Fix printing of measure results (#10647)* [FIX,AUTOTVM] Fix printing of measure resultsThe check for if the error_no was valid was wrong.* switch logic	2
[Rust][Diagnostics] Add initial boilerplate for Rust diagnostic interface. (#6656)* Add initial boilerplate for Rust diagnostic interface.* Codespan example almost working* WIP* Hacking on Rust inside of TVM* Borrow code from Egg* Update CMake and delete old API* Fix Linux build* Clean up exporting to show off new diagnostics* Improve Rust bindings* Fix calling* Fix* Rust Diagnostics work* Remove type checker* Format and cleanup* Fix the extension code* More cleanup* Fix some CR* Add docs and address feedback* WIP more improvments* Update cmake/modules/RustExt.cmakeCo-authored-by: Robert Kimball <bobkimball@gmail.com>* Update rust/tvm/src/ir/diagnostics/mod.rsCo-authored-by: Robert Kimball <bobkimball@gmail.com>* Clean up PR* Format all* Remove dead comment* Code review comments  and apache  headers* Purge test file* Update cmake/modules/LLVM.cmakeCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Format Rust* Add TK's suggestion* More CR and cleanup* Fix tyck line* FormatCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	0
Mutate free variables in CommReducer in cache_write (#2354)	5
[RUST] Rust DSO module (#2976)	5
Tutorial enhancement to keep it clean on docs.tvm.ai (#1450)	2
[TIR, analysis] Add GetAutoTensorizeMappingInfo to generate transforms for auto tensorization (#11740)This PR added a utility function `GetAutoTensorizeMappingInfo` to propose mapping from workload block iters to the iters in the tensor intrin. An example usage is conv2d, where the computation block has more iters than the matmul tensor intrin.	1
[TEST] rfactor+ewise, cite rfactor paper (#474)* [TEST] rfactor+ewise, cite rfactor paper* include all authors via abbrv* [TOPI] Add transpose* fix lint	0
[TIR][Transform] HoistIfThenElse added (#6066)* [TIR][Transform] HoistIfThenElse added* lint error resolved* Pass position changed* pylint error resolved* CI issues resolved* Frontend tflite test case failure resolved* [1] Review comment handled* [2] Review comment handled* [3] Review comment handled* Lint error resolved	0
[RELAY][PASS] FoldScaleAxis Forward (#2020)* [RELAY][PASS] FoldScaleAxis Forward* Introduce helper function type_as* Update per review comment* Fix according to comments	0
Fix test_ir_type. (#5390)* The void return type is not None/nullptr, it's VoidType or   TupleType([]).	3
[Torch] Remove unnecessary reshapes for batch_matmul (#7675)* [Torch] Remove unnecessary reshapes for batch_matmul* lint* fix* reorder* lint	0
[BYOC][TENSORRT] Fix bug of Segmentation Fault  when loading engine file. (#10597)Co-authored-by: XuZhi <xuzhi.xu@alibaba-inc.com>	2
black format master (#6494)	5
[BUFFER] Smarter slice to detect compactness (#587)* [BUFFER] Smarter slice to detect compactness* move simplify of begins early	4
[VTA][Chisel] add ISA BitPat generation (#3891)	1
[RUNTIME] Remove parameter def from runtime (#486)	1
Fix format error in integrate.rst (#6677)	0
Remove linux from travis (#156)	4
[Hexagon][Docker] Update image version (#11332)	5
Add javadoc build into Jenkins workflow (#1909)	1
[MetaSchedule] Misc minor fix (#11904)	0
[CI] Prevent the complete Jenkins pipeline to run when files commited only to  `/docs` (#9031)* Add script to look for changed in doc dir* Modify Jenkinsfile* Minor changes in scripts* Working Jenkinsfile on selective stages on docs* Pass groovy formater on Jenkinsfile* Implementation of relay_to_tir target hook (#8423)This the first new hook proposed in the Additional Target Hooks RFC, longerterm the compilation should move to using `Target` proper but this unblocks our current work whilst illustrating the eventual interface via `Target` in `src/relay/backend/contrib/example_target_hooks/relay_to_tir.cc`Ideally the host target would be annotated onto the `IRModule` so as this `Pass` could use it instead of defaulting to C but this is fine for now.* [CUDA] Fix dense tensorcore legalize type error when units is specified (#9030)* Fix dense tensorcore legalize type error when units is specified* revert black change due to different version from CI* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op (#9017)* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op* Fix linter error for variable name and else after return* Separate quantized avg_pool impl and add TODO for global_avg_pool* Fix comment typo* Fix line break in `setup.py` (#9029)* [Onnx] Add SoftmaxCrossEntropyLoss (#8906)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger* [BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc* Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.* Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a* [microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger* [AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci* [Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to "* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build* [Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0* update gpu and cpu (#8853)* VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* [FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* move rust lint script (#8726)* [AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality* [TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests* extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>* Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.* Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* [Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>* [Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>* [Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.* [UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.* Remove AOT Executor header from Arduino project (#8857)* [Community] @mdw-octoml -> Reviewer (#8868)* [TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* [Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci* [Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials* [Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.* [TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue* [RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860)* [Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI* [RPC] Fix ios_rpc build (#8864)* [Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. "NVIDIA", "radv", "AMD open-source driver") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `"unknown_driver_name"` if the driver does not supportquerying the driver name.* [ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs"* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>* refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling* Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>* [TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>* [CI] make pre-commit hooks to run on every push instead of every commit (#8888)* [TVMScript] Fix printing ForNode annotations (#8891)* [1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055* [microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests* sce loss example* add comments, remove other tests* lint* lint* jostle* lint up* jostle* uncomment some tests* proper return* clean up* lint* minor merge errorsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Michalis Papadimitriou <mikepapadim@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>* [Hexagon] Don't use {} initialization with FastRPC structures (#9033)The data members in FastRPC structures aren't guaranteed to remainin the same order. Replace aggregate initialization with direct,member-by-member initialization.* Test* Minor checkstyle issue* Test* Test file* Revert changed in unit tests* Change script name* Test* Revert format on groovy file* Remove test file* Minor change in script* Minor formating changes* Revert logic in conditions for changed filesCo-authored-by: Christopher Sidebottom <christopher.sidebottom@arm.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Anirudh Sundar <quic_sanirudh@quicinc.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: AndrewZhaoLuo <andrew.zhao.luo@gmail.com>Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>	1
Update symbolic.cc	5
[METAL] Fix the rest memory leaks in Metal runtime (#8175)* [METAL] Fix the rest memory leaks in Metal runtimeWhen we throw exception from autoreleasepool, then the resources won'tbe released in proper way. In the documentation we can see that "Whenthe block is exited with an exception, the pool is not drained.".Link on the documentation:https://clang.llvm.org/docs/AutomaticReferenceCounting.html#autoreleasepoolImplemented a wrapper which handles all exceptions in autoreleasepoolblock and throw them after this block.* Apply comments* Add documentation comments to wrapper and macro	2
[PyTorch][Fix] Fix for numerically unstable logsigmoid (#12563)* Fix numerical instability for log sigmoidFix numerical instability for log sigmoid in pytorch frontend* update* add test for overflow check* merging two tests	3
[TECompiler] Decouple TE compute and schedule lowering in ScheduleBuilder (#10561)* Decouple TE compute and schedule lowering in ScheduleBuilder* fixed merge conflict* removed create_schedule stuff* add public, fix include path convention* Forgot visiting arg in ScheduleBuilder CallNode vsit* fixed anchor impl selection	0
Fixed bugs that occured when using bitwise operators on floating point type expressions. Further crash when using ops <<, >>, %. Finally added regression tests for both types of bug. (#4892)	0
[MetaSchedule] Schedule Rule: Auto Inline (#9943)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>	1
[VTA] [Chisel] fix tensor issue/commit in gemm (#3637)* fix tensor issue/commit in gemm* remove trailing spaces	4
Move jenkins/ dir into ci/jenkins and spread docs around. (#11927)	2
Use int for int8x4 due to performance overhead of char4 (#1569)* Use int for int8x4 due to performance overhead of char4* Add a comment about using int* Remove invalid test	3
[REFACTOR] Separate ArgTypeCode from DLDataTypeCode (#5730)We use a single enum(TypeCode) to represent ArgTypeCode and DLDataTypeCode.However, as we start to expand more data types, it is clear that argumenttype code(in the FFI convention) and data type code needs to evolve separately.So that we can add first class for data types without having changing the FFI ABI.This PR makes the distinction clear and refactored the code to separate the two.- [PY] Separate ArgTypeCode from DataTypeCode- [WEB] Separate ArgTypeCode from DataTypeCode- [JAVA] Separate ArgTypeCode from DataTypeCode	5
Fix typo in test script (#5635)	3
[RELAY][PASS] Memorize FoldScaleAxis backward transform result (#2214)	4
Change HalideIR back to most recent commit (#865)	4
[WEB][RUNTIME] TVM WebAssembly JS Runtime (#5506)* [WEB] Remove the old web runtime* [WEB][RUNTIME] TVM WebAssembly RuntimeThis PR introduces a brand new TVM web runtime based on the WASM standard API.Main highlights:- The new runtime is rewritten using the Typescript.- The new runtime now directly interfaces with WebAssembly's standard API,  instead of relying on emscripten's API.  This change will make the js runtime more portable to runtime variants.  For example, we could also try to make it interface with the tvm's rust runtime implementation.- System library can be provided through WASI  - We also build a hack to enable Emscripten to generate a WASI like    bundle for runtime environment on the Web.- The wasm generation now uses the mainlin LLVM.- Dynamic link(dlopen) is not used due to limitation of wasm,  instead we rely on the recent new RPC refactor to directly  restart a new session for each wasm binary sent to the RPC.* Address review comments* Skip tensorcore test	3
[Ansor] Parallel the InitPopulation (#6529)	5
Improve graph tuner dealing with Tuple (#3649)* Improve graph tuner dealing with Tuple* Add test case* Move some data out of _base.py* Fix lint	0
Fix storage_access not visiting else branch (#8525)* Fix storage_access not visiting else branch* fix conflict with #8516 in the test* update thread sync test following #8516 update	5
[DRIVER][RUNTIME] Make runtime fully device agnostic (#23)	1
Enable array, basic form of tensor	0
[Topi][Unittests] Parametrized tests in `test_topi_dense.py`, split out gpu-independent implementations (#8336)* [Topi][UnitTests] Parametrized tests in test_topi_dense.pyNow, tests run for multiple data types, can be extended withadditional datatypes.* [Topi] Separated generic-gpu nn.dense implementations into topi.gpu.denseAs a follow-up to the renaming of "gpu" to "cuda", separatingimplementations that require CUDA (e.g. dense_cublas.cuda) fromimplementations that require any GPU, but not necessarily a CUDA GPU(e.g. dense_small_batch.gpu).My intent is to pair this migration with the extension of unit teststo cover additional GPU runtimes, migrating only implementations thatrun correctly on non-CUDA GPU devices.* [Vulkan][Codegen] Updated storage sync to avoid incorrect matmul results on some GPUs- In ThreadAllreduceBuilder, separate out load/store so that they can  have a memory barrier in-between.- In Vulkan codegen, added Workgroup memory sync for subgroup thread  sync, since the different subgroup threads can still access  workgroup memory.  Longer-term, may need tir enhancements to  separate out sync of control/memory.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[skip ci] Revert "[skip ci][ci][docker] Prune all non-relevant images (#11491)" (#11496)	1
[Frontend][TensorFlow]Improve TensorFlow Static Shape Tensor Array (#5243)* Support TF Frontend Static TensorArray* Fix pylint* Fix lint* Move get_tensor_array_shape into prelude* Fix lint* Fix common	0
[ONNX] Make `freeze_param = True` and run `DynamicToStatic` by default (#10750)* [ONNX] make freeze_params=True and run DynamicToStatic by default* remove convert_to_static in onnx test* fixed qlinearconv conversion for freeze_params=True* fixed assert msg placement	3
[3/3][AOT][DeviceAPI] Wire up cpacked Device API context (#9501)* [AOT][DeviceAPI] Wire up cpacked Device API contextAdding the same functionality for the Device API to the cpacked calling convention. The MakePackedAPI pass now implicitly uses any variable named `kDeviceContextVar` as the `resource_handle` and this is then used in the `cpacked` calling convention which always expects some form of resource_handle to be passed.* Document calling conventions* Remove superfluous variable	4
[CI] Refactor of tvm.testing.requires_* annotations (#11313)* [CI] Improved skip messages when using @tvm.testing.requires_*Previously, the same message was given regardless of why a testcouldn't be run.  This has been split up into separate checks for TVMcmake options in `config.cmake`, enabled targets in `TVM_TEST_TARGETS`environment variable, and checks for available hardware.* Refactor to specify repeated feature marks, compile-only markers* Fixed lint errors* Import from contrib, not from a different import* Removed use of requires_llvm() as a list of marks* Corrected mark from requires_gpu to requires_cuda* Adding missing "not"* Added USE_CMSISNN as a requirement for corstone300.	1
[BYOC] InlineCompilerFunctions helper pass (#11923)* [BYOC] InlineCompilerFunctions helper passThe TensorRT BYOC integration needs to 'undo' partitionings in some situations. Add anInlineCompilerFunctions pass to make that robust. In particular, it must undo both the'partitioning' (ie separating out the "Compiler" function) and any 'compositing' (ie separatingout small sub-graphs as "Composite" functions).Fix misspelled nn.bias_add while there.Note that the current implementation is broken but untested in CI. I have all the testsfixed in a follow-up PR.* - Lints* - Only AOT compilation paths ensure "executor" is provided as a Target attribute.	1
[TIR] Update region min/extent in ReplaceBufferMutator (#12725)Prior to this commit, `ReplaceBufferMutator` only checks`BufferRegionNode::buffer` to determine if a `BufferRegion` needs tobe replaced, and doesn't check the `BufferRegionNode::region`.  As aresult, updating `T.reads(A[B[i]])` would fail to replace `B`.This commit checks `BufferRegionNode::region` for buffer usage toresolve this issue.	0
[TE] Fix Const Int bound analysis to handle uints for division (#10102)* case to handle uints* add unit test	3
[Relay][VM] Memory planner (part 1) (#5144)* Start on memory planningWIPMove to test_memory_passes.pyWork on memory planningPost-rebase and VM changesPlumb through the offsetsBasic tests all pass, fix offset to data buffer.Fix compile errorsFix wsApply suggestions from code reviewCo-Authored-By: Haichen Shen <shenhaichen@gmail.com>Address CRUpdate src/runtime/vm/vm.ccCo-Authored-By: Haichen Shen <shenhaichen@gmail.com>Fix another commentFix lintFixFixFixLint is done?FixMore fixTrying to debugNo clueFix lint* Fix docs* Disable aggressive constant eval* It works* Fix lint* Found issue with dynamic* Fix the pass, but runtime segfaults* fix scalar tensor, test_any_elemwise passes* Fix split pass* Fix 0-rank issues* Fix* debug* apply Haichen's patch and clean up* lintgit add .* fix serializer and test_tyck_alloc_tensor test* Fix the constant lift pass in presence of closures* Restore old finder* Fix rebase issues* Fix* Fix* Fix issue coercing the shapes incorrectly from i64 to i32* Fix linting* Fix clang format* Format memory.cc* Fix 0-rank case* Add fix for (0,) shape* Ignore shapes for now* Apply suggestions from code reviewCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>* Update src/runtime/vm/executable.ccCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>* Fix* lintCo-authored-by: Zhi Chen <chzhi@amazon.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>	1
[COMMUNITY] Egor Churaev -> reviewer (#8231)	3
[TVMC] run: Fix call to non-existing method (#9608)	0
[TE][TensorIR] fix tensor attr in create_prim_func (#9764)* [TE][TensorIR] fix tensor attr in create_prim_func* Update src/te/operation/create_primfunc.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[topi][relay] add operation tan to TVM (#4938)* Add relay operation relay.op.tan.* Update tan implementation in TVM.* Update tests.* Add shape function for tan.* Add missing main test to python/frontend/tensorflow/test_forward.* Revert, back to sin/cos.* Revert "Revert, back to sin/cos."This reverts commit 4da5b503b921585ba9d80944b29136142b575c40.* Fix implementation of tan in cuda. Do not support tan for float16.Simplify topi/tests/python/test_topi_math. Add testing for tan with float32 and float64.Try again to implement tan as sin/cos in llvm.	1
[PASS] More improvement of canonical (#314)	1
[Fix][Autoscheduler] Costmodel enhancement & bug fix for graph debug runtime (#7197)* Enhancement for autoscheduler cost model* Bug fix for graph_runtime_debug* Update* Lint fix* Update* Update* Add file exist check for cost model load* Update* Update* Lint fix* Update* Bug fix	0
[ci] Use S3 for artifacts (#11349)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
add simplify test (#12)	3
Fix the problem that android_rpc compilation failed. (#4244)Signed-off-by: qinqiuping <autumnqin@126.com>	0
Update autotvm_relay_x86.py (#9601)	5
[TOPI] Specify non-zero absolute tolerance in tests (#1925)	3
Make keras reshape less restrictive (#7446)	1
[FRONTEND][TENSORFLOW] Bugfix (#2267)	0
Remove extern C warpper for cuBLAS (#3877)	4
Unbreak CI image build (tensorflow 2.6.5, ci_gpu bugfix) (#11546)* Pin protobuf to 3.20.1 due to #11545.* Unpin and instead update to 2.6.5* attempt to fix gpu build* Revert to 2.6.3, pin protobuf for ci-arm.* escape bash char	4
[TVMSCRIPT] Add tir.min node in tvm script (#8219)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[DOCS] Try upgrade build (#1066)	1
[Relay][Frontend][Onnx] Compare against onnxruntime more consistently during testing (#7300)Co-authored-by: Josh Fromm <jwfromm@uw.edu>	3
Move WrapTimeEvaluator from RPC to profiling, NFC (#11172)	4
refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling	4
[RELAY][PASS] Bind, FoldConstant (#2100)	4
[µTVM] Fix two warnings when deprecated forms are used (#7269)* [µTVM] Specify loader for yaml.loadSpecify the loader to be used by yaml.load as the current form used withoutspecifying explicitly a loader is deprecated since PyYAML 5.1 and willthrow a noisy warning.For details, please see:https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-DeprecationSigned-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Avoid using tvm.target.createAvoid using tvm.target.create as it's deprecated and usetvm.target.Target directly instead.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
Bump cmake version for GPU build (#11156)The cmake version (3.10) in Ubuntu 18.04 does not cope well with themore advanced cmake use in libtorch surrounding the CUDA target.We switch to a self-built cmake 3.14 (already used by arm and i386 CI).The context for this is #10758 .	1
[Refactor] Replace std::tie with structured bindings (#12610)* [Refactor] Replace std::tie with structured bindingsWith C++17 enabled in https://github.com/apache/tvm/pull/12337, usingstructured bindings to replace cases where `std::tie` is used todefine local variables.* Added missing header for <optional>* Silenced unused variable warnings after structured bindingsThis is a bug in gcc version 7, resolved in gcc 8.  While gcc version7 is used for CI, we'll need to silence unused variable warningsresulting from using only part of a structured binding.More information: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81767	0
[Releay] Fix on_device call for explicit virtual_device (#12088)	0
[CUTLASS] Conv2d activation fusion, part 2: Sigmoid fp16, SiLU and HardSwish (#9795)* [Torch] do not pad if pad widths are all zero* silu fusion supported* adding hardswish support* support fast_math sigmoid op* fixed type inference for yolov5 + silu fusion* use include_non_call_ops=False in AnnotateTarget* update cutlass* revert change in build.py* simplify codegen* lint	4
Revert "[Runtime] Allow parameter sharing between modules (#3489)" (#3884)This reverts commit 224cc243b4e54a77d011644fe7d81bdee8e8116b.	4
[Test][TF][Relay] Fix argument preparation for vm test mode (#4296)	3
[Collage] PruneCandidates and demo_collage_partition.py (#12105)* [Collage] PruneCandidates and demo_collage_partition.pySee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.This completes our checkin of our Collage 'sketch' branch into main. Special thanksto Matthew Barrett for his help getting this over the line.The only C++ functionality added here is for 'pruning' candidates. This is a somewhatspeculative algorithm (and I've called that out in the comments) which tries toelide candidate partitions which will 'obviously' not contribute to the final optimalpartitioning. For largish models such as GPT2 this can significantly reduce the number ofcandidates we need to actually measure latency on. I beefed up the MockCostEstimator tomake it possible to assert pruning occured from within the test_pass_collage_partition.pyunit test.The rest of this PR adds the demo_collage_partition.py driver file we've been usingto test and measure perfomance differences against various baseline (though onlyfor the CUDA ecosystem). To eliminate loading time the models of interest are directlyexpressed in Relay text form in menangerie.py.* - lint	5
[Strategy][ARM CPU] Remove contrib spatial pack schedule of depthwise convolution (#5148)* [Strategy][ARM CPU] Low the plevel of contrib spatial pack of depthwise convolution* address comments	1
Add Reduce operators to TFLite (#3421)	1
[BYOC][ACL] Fix list is not supported as an input node (#10801)* [BYOC][ACL] Fix list is not supported as an input node* fix clang lint error* fix compile warnning* fix python module import error* rename concatenate test file* fix always MakeACLTensor with same eid 0* do not offload concat default* fix concattnate test failure* fix test failure* fix lint error* fix lint* remove global var offload_concat* support concatenate with pattern table mechanism* disable pylint dangerous-default-value warningCo-authored-by: XuZhi <xuzhi.xu@alibaba-inc.com>	2
[Relay] Add a non-recursive LetNode VisitExpr_ for LabelOps Pass to avoid stack overflow (#8917)* Add a non-recursive Let VisitExpr_ for LabelOps* fake commit to retrigger CI* fake commit to retrigger the CI* fix CI issue* fix CI issue	0
[BYOC][JSON] json_node.h should include data_type.h (#6224)Fixes compilation issue after #6214.Change-Id: I07e25356bbfe4a7bd0950f2672441ce1c338dc3f	4
check for dynamic rank before accessing value (#7414)	5
Fix optimize	0
[FIX] Fix RPC for the VM (#7810)* [FIX] Fix RPC for the VM	0
[QNN] Legalization for Intel x86 QNN Conv2D (#3896)* QNNLegalize for conv2d* [QNN] Legalization for Intel x86 QNN Conv2D	5
Fix Intel OpenCL SDK search path for Windows (#8301)Co-authored-by: Andrey Malyshev <andrey.malyshev@gmail.com>	0
[TIR][Schedule] Refactor Tensorize (#12070)* Refactor blockize* Refactor tensorize* Address review comments* typo* rename variables according to review	2
[Frontend][MXNet] ones zeros ones_like zeros_like ops support (#1814)	1
[CI][ETHOSN] Enable CI for Ethos-N (#6171)This introduces the necessary changes to docker tosupport building the Ethos-N driver stack. This isrequired for subsequent patches which introducethe Ethos-N integration into TVM.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com># Please enter the commit message for your changes. Lines starting# with '#' will be kept; you may remove them yourself if you want to.# An empty message aborts the commit.## Date:      Mon Jul 27 15:43:41 2020 +0100## On branch ethosn-ci# Changes to be committed:#modified:   docker/Dockerfile.ci_cpu#new file:   docker/install/ubuntu_install_ethosn_driver_stack.sh## Untracked files:#CombinedMemoryMap.hex#OutputModel.hex#config.txt#docker/install/ethosn_cap/#docker/install/ethosn_driver_dev-20.05-dbg-20200612-141030.tar.gz#ssd.npy#tests/python/integration/test_tir_gemm.py#tests/python/relay/test_pattern_annotate.py#tests/python/unittest/failure.py#	5
TF frontend: add rint op (#6818)* TF frontend: add rint op* Added negative numbers to the test	3
[MetaSchedule] Add Testing Script with ONNX Support (#11587)This PR introduces 2 tuning script for meta schedule and auto scheduler tuning support with onnx files. Now we can easily introduce onnx models benchmarking with command line scripts. Sample tuning call looks similar to the following scriptFor Meta Schedule ONNX tuning:```python3 -m tvm.meta_schedule.testing.tune_onnx_meta_schedule \    --model-name   "$MODEL_NAME"                             \    --onnx-path    "$ONNX_PATH"                              \    --input-shape  "$INPUT_SHAPE"                            \    --target       "$TARGET"                                 \    --num-trials   $NUM_TRIALS                               \    --rpc-host     $RPC_HOST                                 \    --rpc-port     $RPC_PORT                                 \    --rpc-key      $RPC_KEY                                  \    --rpc-workers  $RPC_WORKERS                              \    --work-dir     $WORK_DIR                                 \    |& tee         "$WORK_DIR/$MODEL_NAME.log"```For AutoScheduler ONNX tuning:```python3 -m tvm.meta_schedule.testing.tune_onnx_auto_scheduler \    --model-name   "$MODEL_NAME"                              \    --onnx-path    "$ONNX_PATH"                               \    --input-shape  "$INPUT_SHAPE"                             \    --target       "$TARGET"                                  \    --num-trials   $NUM_TRIALS                                \    --rpc-host     $RPC_HOST                                  \    --rpc-port     $RPC_PORT                                  \    --rpc-key      $RPC_KEY                                   \    --rpc-workers  $RPC_WORKERS                               \    --log-dir      $WORK_DIR                                  \    |& tee         "$WORK_DIR/$MODEL_NAME.log"```	2
[Relay][Pass]Improve memory_allocation pass to support multiple i/o dynamic kernels (#4595)* Add more shape funcs* Fix test* Enhance test_any_concat* Fix pylint* Minor fix test* Fix pylint* Minor refactor* Add test any for elemwise	3
Expose relay BindParamsByName to Python (#4751)* expose BindParamByName to python* fixed alpha equal test	3
[Runtime][PipelineExecutor]  Tutorial of using pipeline executor. (#11557)* [Runtime][PipelineExecutor]  Tutorial of using pipeline executor.Tutorial of using pipeline executor including the byoc use case.* fix ci issue* document change.* triger build* fix doc issue* fix ci issue* doc issue* fix ci issue* fix ci issue.* fix __file__ not found problem.this is a known issue of sphinx-galleryhttps://github.com/sphinx-gallery/sphinx-gallery/issues/211* fix byoc with dnnl issue* enable dnnl and pipeline executor* trigger build* trigger build* fix build issue* trigger build* oneflow cause crash, do test with change* add sphinx skip* plint* remove from_oneflow change test.* remove pipeline executor change for test* plint* enable DNNL and pipeline* disable DNNL* enable DNNL without pipeline* remove dnnl and add cutlass* use cutlass with byoc* change into cutlass* fix doc convention issue* remove duplicate variable* fix plint issue.* address review comments.* address review comments* fix bug.* polish the document* fix plint issue* address review comments.* address review comments* address review comments	1
[Tutorial] Demo showing how to run a pruned 🤗 model. (#5975)	1
Hotfix for issue #3641. (#3644)	0
[Hexagon][CI] Re-enable Hexagon tests in CI (#11613)* [Hexagon][CI] Re-enable Hexagon tests in CIThese were enabled in https://github.com/apache/tvm/pull/11294, thenerroneously disabled in https://github.com/apache/tvm/pull/11313.This applies the same fix as inhttps://github.com/apache/tvm/pull/11294, checking the`ANDROID_SERIAL_NUMBER` to determine if Hexagon tests can execute atruntime, but using the refactored `pytest.skipif` messages introducedin https://github.com/apache/tvm/pull/11313.* Fixed circular dependency, but feels somewhat ugly	0
[AutoTVM]Core functionality for Graph tuner (#2184)* Add graph tuning* Add tests* Fix tests* Fix pylint* Small fix for docstring* Minor fix* Support fetching workload from relay expr* Simplify benchmark layout transformation* Add relay support* Fix infer layout func name* Refactor internal data representation* Fix issues* Add PBQP solver* Fix layout transform check* Add PBQPTuner test* Fix lint* Update tutorial* Fix tutorial* Fix lint* Add relay test* Remove nnvm since nnvm graph can be converted to relay function* Modify benchmark layout wrt new layout_transform api* Fix lint* Update docstring for DP tuner* Refactor traverse graph* Support graph tuning for multiple target operators* Fix fetching workloads* Add x86 depthwise_conv2d infer_layout* Fix x86 depthwise_conv2d autotvm* Fix PBQP tuner* Fix DP tuner* Generate dummy layout transform record* Update tutorial* Modify layout records name* Add ASF header* Add ASF header for testing files* Fix test* Fix topi fetching* Some refactors* Fix lint* Fix tutorial* Rename test files* Fix doc typo* Add test case note link	2
Add xgboost version restriction (#12050)Co-authored-by: jiabeizhao <jiabeizhao@tencent.com>	1
[DOCKER] Update lint to reflect the latest state (#8330)Pins mypy version.	3
[ONNX] NMS in ONNX (#6839)* NMS partially working on CPU, fails on GPU* support dynamic iou_threshold* WIP NMS with while loops* working nms with dynamic shapes* add a test with dynamic score_threshold and pass it* Fix type checking in lambda lift* ONNX NMS working on GPU, had to remove threading from some kernelsfix lintfix lambda lift testsfix unit testsrespond to review commentsfix lint* better parallelize get_valid_counts* improve nms parallelization* respond to cuda/thrust enablement issueCo-authored-by: Jared Roesch <roeschinc@gmail.com>	0
[rust][ci] Disable rust nn tests (#11420)See #11419Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[microTVM] Fix platform name for qemu_x86 in Zephyr AOT tests (#8762)Currently two Zephyr AOT tests (test_tflite and test_qemu_make_fail) arenot running when qemu_x86 platform is selected because the platform nameis wrongly listed as 'host' in the match list for not skipping thesetests. This commit fixes it.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	0
Some Windows and MSVC fixes (#4569)* fix python exception creation in Windows* better string conversion for msvc* fix cpp style issue	0
[microTVM] Update Zephyr to 2.7 (#10094)This supports the reference system added in #9853	1
Faster sparse_dense on GPUs (#6580)* Faster sparse_dense on GPUs.This new sparse_dense requires a padded matrix, so a new op`sparse_dense_padded` has been added. AlterOpLayout should transform`sparse_dense` to `sparse_dense_padded` when possible on the gpu.* formatting* more formatting* Check that alteroplayout is definedbefore using it* check if FTVMAlterOpLayout exists before using it* formatting* restore message passing* Fix sparse_dense and sparse_dense_padded docs* Fix old sparse_dense, autotvm and sparse_dense dont play well together* Remove unused imports* clarify warp count in cuda_transpose* Document multidimensional access* Warn users not to use sparse_dense_padded* rename nn.sparse_dense_padded to nn.internal.sparse_dense_padded	1
move rust lint script (#8726)	4
Set split node's range to minimum of ext and split factor or split nparts, but only when PassDownDomain is called with allow_missing == false, i.e. by InferBound. Add a helper PassUpThreadBinding() to get a map telling whether an IterVar has at least one leaf IterVar deriving from it binding to a thread. Add two unit tests. (#5044)	3
[Autoscheduler] Task Extraction Raises Exception on Lowering (#9750)* forward messages* Update python/tvm/auto_scheduler/relay_integration.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* remove type annotation for consistency* lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>	4
[RELAY] Hotfix build_module creation (#3198)	1
[DOCKER] Add clang-format and nodejs to ci-lint (#5567)	1
[CI FIX] (Really) Skip test_conv2d in Hexagon (i386) (#10687)* [skip ci] Skip test_conv2d in Hexagon* properly skip hexagon test_conv2d on i386* skip entirely	3
[CUDA][TOPI] Fix CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES with NMS for certain GPUs (#7623)* Use less threads for certain GPUs to avoid register limit* Move util function to nvcc.py* Fix lint	0
Add cooldown interval logic for the profiling functional (#11465)* Add cooldown interval logic for the profiling functional.* Remove string serialize hack from RunIndividual functions* Update src/runtime/graph_executor/debug/graph_executor_debug.ccCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	0
[TVMC] Split common tvmc test file into more specific files (#9206)The `test_tvmc_common.py` file was becoming a bit of a mixed bag oftests and as we now want to extend the `Target` processing logic it madesense to split each out into its own file to make it clearer what eachdoes.`test_common.py` has also been renamed before we start using it for all thetests instead.	3
[APP] Android RPC (#359)* [APP] Android RPC first version* [APP] Android RPC build jni automatically* [APP] Android OpenCL RPC tested on real devices* [APP] optimize android app interface. add ndk compile tool* add ndk compile tool* [APP] fix android app thread crash; add android test script* [APP] android app - show alert dialog and disconnect when error occurs* fix ndk build script code lint* fix ndk build default argument* ndk script build remove shell=True. disable android app screen orientation	2
[TIR] Expose ScriptComplete in header (#12419)	5
[doc] fix typo, codege to codegen (#4383)	2
[EXAMPLE] Fix example for simulator (#40)	0
fix nvcc compile option to be compatible with older cuda (#7065)Co-authored-by: masa <masa@pop-os.localdomain>	0
Expose Missing TIR Builtins to Python (#12466)This PR exposes the following TIR operation in python:`address_of`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L3247)`lookup_param`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tir_usmp_analysis_extract_bufferinfo.py#L171)`infinity`: add new unittest`reinterpret`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L2991)`isnullptr`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L260)Co-Authored-By: yongwww <yongcale@gmail.com>	5
Add tutorial for convolution in CUDA (#343)	1
[Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials	0
Make TVMLogf platform-independent (#6916)* Make TVMLogf platform-independent. * Some platforms need to use an alternate printf() to support basic   things like %zu. Since %zu is platform-specific, we prefer to   use a printf() that supports it or allow the platform to fix it up   as needed.* git-clang-format	0
[submodule] update dlpack (#403)	5
register loadbinary_hip (#1151)	5
update dnnl version from v1.5 to v2.2 (#10266)	5
[Frontend][MXNet] add _npi_subtract_scalar (#7191)* [Frontend][MXNet] add _npi_subtract_scalar- add mxnet numpy operator, subtract- https://github.com/apache/tvm/issues/7186- https://mxnet.apache.org/versions/master/api/python/docs/api/np/generated/mxnet.np.subtract.html* Fix python style using black	1
[TUTORIAL] use resnet v2 (#51)* use resnet v2* fix	0
[Relay][Frontend][ONNX] Add Erf to ONNX frontend (#3988)* Add Erf to ONNX frontend* dummy change to retrigger CI	4
[Runtime] Fix TVM_DLL_EXPORT_TYPED_FUNC to work on Windows (#4955)* [Runtime] Fixed TVM_DLL_EXPORT_TYPED_FUNC to work on Windows* fix styleCo-authored-by: Jon Soifer <jonso@microsoft.com>	0
[CUDA] Swap block x and z dimension for conv2d NHWC schedule (#9087)	5
[BugFix] Fix a wrong use of `std::move()` in cross-thread reduction lowering (#9728)* [BugFix] Fix a wrong use of `std::move()` in cross-thread reduction lowering* Remove	4
[AOT] Enable A-Normal Form in the AOT executor (#11091)* [AOT] Enable A-Normal Form in the AOT executorThe sequence of calls produced by the AOT executor codegen is arbitrary,especially in the presence of 'branchy' networks. This makes itdifficult to analyze memory usage for each call. By running theToANormalForm pass to insert a series of let bindings before thelowering and codegen stages, we can establish an ordering for theevaluation of the external calls, thus allowing reliable analysis ofmemory usage.Change-Id: Ic320b68cde83c96b228a8d1d2829a0e8ac7b768f* Maintain GetStorage(var) == GetStorage(value) invariant for letsChange-Id: Id40b70f67a3e37f75b8331aa89f1819072e4d48e* Add check to ensure ANF runs in AOTChange-Id: I8de2bd19c7c17057e2bc89f6a68595780c2e9433* Avoid let block traversal and don't visit var in let visitationChange-Id: I74c080e2a09e84a75400db5c3395d508697d5d0f	5
[COMMUNITY] @wrongtest -> Committer (#11028)Please join us to welcome @wrongtest as a new committer to TVM. The contributor has contributed to TensorIR schedule primitives, arithmetic analysis and TVMScripts.- [Commits History](https://github.com/apache/tvm/commits?author=wrongtest)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:wrongtest)- [Community Forum Summary](https://discuss.tvm.apache.org/u/wrongtest/summary)	3
Add support for using the VM across the RPC boundary.  (#7746)* Get basic verison of VM RPC working* Test case passes* Clean up PR* Lint* Format* Address Andrew R and TK feedback* Add comment for Andrew* Address Zhi's comment* Format* Fix broken test	3
Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>	4
[COMMUNITY] tkonolige -> Reviewer (#7311)	3
[Relay][AlterOpLayout] NHWC to NCHWc pad operator. (#4103)* [Relay][AlterOpLayout] NHWC to NCHWc pad operator.* Fixing culprit.* Flaky test 1.* Flaky test 2.	3
[TF] Support TensorFlow < 1.13 for test_sparse_add (#8647)	3
[AUTOTVM][Bugfix] Fix history loader for heterogeneous execution	0
add more syncs (#3151)	1
[AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality	3
[LLVM] Make sure all functions have target-related attributes set (#11222)LLVM codegen create new function, e.g. the "_compute_" function fora compute_scope attribute, etc. These function did not have functionattributes defining the target properties, specifically "target-cpu"or "target-features". Make sure this information is present on allfunctions created in CodeGenLLVM.	1
[Fix] Minor modification in unittests (#12247)Update unittests to align with the expected behavior	3
[TOPI] Add out_dtype argument for conv2d; Add x86 schedules (#646)* [TOPI] Add out_dtype argument for conv2d; Add x86 schedules* Fix* Fix lint* Fix	0
[LLVM] Include Support/Host.h for declaration of getDefaultTargetTriple (#5268)In newer versions of LLVM, this header is no longer included by one ofthe already included headers in llvm_common.h, so include it explicitly.	1
[Relay][ONNX] Batch_matmul to dense optimization (#8440)* [ONNX]Add batch_matmul to dense optimization* Add extra check to avoid unnecessary reshapeCo-authored-by: Ubuntu <ubuntu@ip-172-31-14-16.us-west-2.compute.internal>	1
Fix compile error when AddRewrite gets additional args (#10669)	1
[Relay/TOPI][OP] Add meshgrid op in Relay, TOPI, Pytorch frontend (#5961)* Add meshgrid op with pytorch importer* Fix c++ lint* Fix pylint* Meshgrid: add scalar test for pytorch, add topi python wrapper* Add indexing mode attr.* Add MeshgridAttrs python binding* c++ lint	5
fix tvm.relay.build() docs (#6940)	2
Update tags with minor fix (#7448)	0
[ARITH] Analyzer CanonicalSimplifier (#2891)	5
[MetaSchedule][Test] Add unittests for T2D (#12249)	3
Amendments for gradients (#5941)* Amendments for gradients- We fix the dtype handling of consts in generated gradients.- We add a collapse_sum_to instruction mirroring the collapse_sum_like.  While for general definitions (potentially dynamic shapes),  collapse_sum_like is the first choice, when moving to static,  using collapse_sum_to will greatly simplify the graph.  (This simplification is not part of the PR.)* Fix Broadcast rel description in commentThank you, @MarisaKirisame	0
[Bugfix][Op] Fix shape inference of adv_index (#9717)* init* test* lint	3
[Pytorch] fix translation of transpose when axis argument is as a list (#5451)	0
[CI] Set workspace to be per executor (#4336)	1
[Metaschedule] Make custom schedule_rule registration optional (#10975)See the discussion in https://github.com/apache/tvm/pull/10793#discussion_r837626566 for the context.Now I'm doing auto-tensorization on VNNI, I do need to be able to switch on / off `schedule_rule` freely.	1
[RELAY]Frontend darknet (#2773)* [RELAY]Frontend darknet* CI test file updated & CI error fixed* avg_pool pad fix* Changed repo_url and doc formatting	2
Fix some typo errors in license header (#5956)Signed-off-by: leonwanghui <wanghui71leon@gmail.com>	0
fix lint (#2649)	0
Fix typo (#4144)	2
[DOC] minor gramatical improvements to tensor_expr_get_started (#3330)	1
[DOCS]Update debugger in docs.tvm.ai (#1924)Update debugger in index to reflect in docs.tvm.ai under the Design and Developer Guide	2
Fix incorrect stride in conv2d_nhwc_python (#1670)	0
another cmake fix (#5630)	0
[PYTORCH]floor_divide support for squeezenet (#5702)https://github.com/apache/incubator-tvm/issues/5133#issuecomment-636330705	0
Pre-allocate buffer for x86 roi_align (#3475)* Pre-allocate buffer for x86 roi_align* Fix typo	2
Adding support to check if an attribute is present or not without having to get the value (#3957)* Adding support to check if an attribute is present or not without having to get the value.* - Renaming the method to more appropriate name.	1
[PASS] Add order mutation (#7)* [PASS] Add order mutation* A few benchmarks on compose speed	1
[FRONTEND][TENSORFLOW] support multiply outputs (#4980)* [FRONTEND][TENSORFLOW] support multiply outputs* [TENSORFLOW][TEST] add tf_testing.AddShapesToGraphDef test* update frontend test* retrigger CI	3
fix wrong type declaration of float64 "log" in intrin_math.py (#1169)	2
[Relay][Op] Multinomial (#12284)* Add multinomial operator.* Implemented Pytorch integration with multinomial.* Fixed test paramatrization and added onnx integration.* Add statistical testing.* Make get_type more flexible.	1
[Relay/TOPI] Added 'offsets' and 'alignment' attributes to MATRIX_SET_DIAG. (#6429)* [Relay/TOPI] Added 'offsets' and 'alignment' attributes to MATRIX_SET_DIAG.* Added support for 'offsets' and 'alignment' attributes of MATRIX_SET_DIAG.  (Similar to MATRIX_SET_DIAG V3 of TF)* Added tests for 'offsets' and 'alignment' attributes of MATRIX_SET_DIAG.* Changes by black.* * Added offset check in Relay.* Minor changes.* Added more tests and some minor documentation changes.	4
[BugFix] Fix the race condition issue of packed func. (#7246). (#7619)Co-authored-by: wenxizhu <wenixzhu@tencent.com>	0
[ARITH] Simplify let (#3568)	5
[CMSIS-NN] Fix memory alignment bug in CMSIS-NN demo (#11221)* Updates convert_image.py to include memory alignment	5
Transpose core dump resolved (#1355)	0
[TFLITE]Hard Swish & MobilnetV3 model testing (#5239)* [TFLITE]Hard Swish & MobilnetV3 model testing* CI Failure addressed	1
[TOPI] Intel graphics conv2d autotvm template added (#3839)* update lint* lint fixed* lint updated* lint fixed* lint fixed* lint fixed* updates* add intel graphics as a package* remove print info* depthwise conv2d schedule added for intel graphics* asdf* fix lint* fix lint* fix ci* add channels	1
[TOPI] Basic x86 schedules (#775)* add basic x86 schedules* parallelize & vectorize batchnorm + relu* fuse conv into bn + relu* move rc loop to outer* add nhwc conv* change weight layout to hwcf* conv + bn + relu fusion for nhwc conv* fix conv_nhwc schedule when no fusion* clean up default parallel schedules* simplify elemwise parallel* fix elemwise parallel for batch == 1* update nhwc conv test* fix and add comment* fix lint* remove redundant import* remove default multithreading for some ops* remove default multithreading for global pool	4
Update tflite schema version to 1.13 (#3356)	5
[DOCS] update docs (#67)	2
Add build_create_shared_func to tvm/contrib/cc.py (#3840)	1
Fix division range estimation error in simplifier (#6244)Division a/b assumes maximum values when b is close to 0. Accountfor that when estimating the range for a/b when 0 belongs to theestimated range for b.Assume that a division by zero cannot happen in a valid program,so in such cases treat the range for b as a union  [b.min_value, -1] u [1, b.max_value]	0
[NNVM] Move FTVMCompute registration of the elementwise operator to c++ (#1351)	1
[TIR][Schedule] enhance compute_at and reverse_compute_at primitive to choose possible position (#12450)Current TIR "compute_at" primitive will compute at it's closest consumers. When a block has multiple producers, whoever compute at later who is behind. But for some special hardware, we usually hope keep the a certain order whatever it's compute at early or late.eg: block A and block B are producers of block C. block A compute at block C first and block B compute at block C later. We hope the result is block B->block A->block C under some loop var.	5
enable partition const loop with build flag (#732)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop	2
Support for sign (#2775)	1
[TVM][RUNTIME] A minimum example to generate external library wrappers for DSOModule (#4280)	5
dump lowered IR when debug logging (#292)* dump lowered ir when debug log* avoid calling tvm.lower() twice when not debug	0
[DYN][RELAY] Resize support for NCHW-convertible layouts (#6293)* fix lint* fix typo* remove channel_axis from resize shape func* fix lint	0
Fix relay.testing.darknet convert_image (#7667)	3
also save graph attributes (#78)	5
[TVMScript] Python Expression Precedence (#12148)This PR:- Handle expression (operator) precedence during Python code printing (`(* 1 (+ 2 3))` prints as`1 * (2 + 3)`)- Addresses remaining feedback from previous PR #12112- Reformats Python import with isortTracking issue: #11912	0
[TESTS] Improve script robustness (#2893)A number of test scripts use the '|| exit 1' idiom.  This has twoissues, first process exit codes are defined to be in the range 0-255.Second, more importantly, the idiom is fragile because it requiresthat every possible failure point be explicitly coded.  This patchremoves the idiom in favour of "set -e" as used in the docker scriptsas a more robust mechanism to ensure that script failures are alwayscaught and propagated by default.	0
[Relay][Keras] force const dtype to be float32 (#2376)* [Relay][Keras] force const dtype to be float32* fix pylint	0
Take zero extent loops as NoOp and remove it and add unittest for the same (#3724)	3
[HOTFIX][TARGET] Change LOG in compilation config to DLOG (#9486)CompilationConfig was merged in on the basis that it is an internal experimentalstructure that helps to group the target. Constructing the config should notemit messages for most cases. Change LOG(INFO) to DLOG(INFO)so users won't be overwhelmed by messages.There are a few warning cases that also changes to DLOG. Given CompilationConfigis still experimental, it would be better to respect the current default conventionand not trigger warnings that indicate an non-experimental suggestion.Warnings can be updated according with the convention.	5
[Keras] ReLU6 support (#481)	1
[M1b] Scaffolding ScheduleState data structure (#7765)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>	1
[Relay][Pass] ConcretizeLike and EliminateIdentity rewrites for SimplifyExpr (#7731)* factor out some common code for DF rewriting, add ConcretizeLike* slight refactoring, add EliminateIdentity pass* lint* merge ConcretizeLike and EliminateIdentity into SimplifyExpr* nits and lint* remove static stuff* document* definitely ran clang-format but ok* make ToScalar return optional, fix missing virtual destructor* lint* tweak scalar conversion API to maintain compatibility	0
[WIP] Onnx1.0 (#294)* add more op for onnx 1.0* fix syntax* fix lint* fix* update 1.0* fix* update model	5
Bumping up CMSIS-NN version to be in sync with TFLu (#9247)Change-Id: I51103632f6d41652d616857f987a846ea2b22a5c	4
[FIX] Fix bug and typo in rpc_server (#263)* [FIX] Fix bug and typo in rpc_server* [FIX] Remove unnecessary condition	4
[COMMUNITY] ekalda -> Committer (#12715)	3
[MetaSchedule] Random Feature Extractor (#9760)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[AutoScheduler] Use a smaller retry number (#6996)	1
[Metal] Fix bad stream after interrupted tuning session (#8244)* [Metal] Fix bad stream after interrupted tuning sessionAfter interrupted tuning session, we may face the problem that thestream object was released, but we didn't create a new one. In this caseit wasn't possible to run a new Metal task on the device withoutrestarting rpc application.Created a global function `metal.ResetGlobalState` which should becalled in RPC application when the connection was closed. In thisfunction, we reinitialize the streams of Metal devices. And itguarantees to us that the new RPC session will work with the correctstreams.* Refactor metal_device_api- Rename function GetStream -> CastStreamOrGetCurrent- Add several checks on device id- When we use `SetStream` with nullptr, then the default stream will be  associated with the device.	1
[Relay][doc] Update the description of returns in mxnet.py (#2309)	5
[Topi,x86] Split MKL from BLAS. (#6182)Make cblas and mkl seperate entities in cmake and topi, allowing usersto use both a BLAS library and MKL. In the future, MKL specificfunctions can be added easily. MKLDNN is also split off from MKL andBLAS for the same reasons.Other improvements:  - cblas and mkl strategies are now only applied when they are viable.  - compile_engine will log which implementation it has chosen and why.	2
[Relay] shape func for zeros, zeros_like, ones, ones_like (#4448)	5
[microTVM] Update Zephyr 2.5 (#7786)* update to zephyr 2.5* unbreak test_zephyr* fix stack size* always create packer.log* add qemu debugging* fix transport with debug false* size down ring buf, shouldn't need to be so large* update to zephyr 2.5* fix buffer size* cleanup* cleanup* remove debugger* nit* update ci script* remove debug mode* fix packer log* comment* update ci_qemu* change zephyr version on Vagrant* make it compatible to zephyr 2.4Co-authored-by: Andrew Reusch <areusch@octoml.ai>	5
[FRONTEND] DarkNet Yolo2 Frontend Support  (#377)	1
allow variable composition (#133)	1
[Layout] Unify dense op input layout (#8921)	5
[microNPU] add E2E tests with cascader wo striping (#11410)This commit adds end-to-end tests using the cascaderw/o striping. It needed few adjustments to the orderin which the arugments are provided to the entry pointfunction in AoT when both memory pools and devicesare present.Change-Id: I37e04afd635add895e317586f628a62cae75f3fa	1
remove fatal (#5888)	4
[CI] Temp disable rust docs build (#7743)	2
[MetaSchedule] Add logging of usage of tensor intrinsics (#12445)* [MetaSchedule] Add logging of usage of tensor intrinsics* fix	0
[Frontend] [Paddle] fix testing problem (#11259)* fix testing problem* remove clear_executor_cache	4
[skip ci][ci] Remove inplace flag from black script (#10918)This was erroneously added in #10895Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TOPI] Parallelize GPU NMS inner loop (#7172)* make NMS inner loop parallel* use one block two avoid global sync issue* temp disable write by only thread 0* leave a TODO on write by only one thread* add some comments, remove check the check on negative class id* minor improvement when topk is available* fix write by a single thread	0
[Ansor][AutoTVM v2.0] Phase 2: Basic GPU Sketch Search Policy (#6269)* Add PreloadMeasuredStates & Split search_policy.py* Add GPU sketch rule* Update* Bug fix for log record* Lint fix* Update tutorial* Update* UT fix* Remove tutorial* Update* Update* Update UT* Lint fix* Update* Update	5
[µTVM] Add support for mps2_an521 board (#7813)* [µTVM] Zephyr: Allow user inform if a board is emulatedSome boards supported by Zephyr that run emulated by default, i.e. their.yaml config file sets the field "simulation: qemu", don't have theprefix "qemu_" on their names, so µTVM can't currently recognize it asan emulated target to properly use the QEMU transporter (instead of theserial port) to open a session against it. Such a boards usually havereal physical (hardware) counterparts, being specific boards and notgeneric or "fake" ones simply tied to a CPU type of interest.That commit allows the µTVM user to explicitly inform that µTVM needsto use the QEMU transporter to open a session against a given board byadding the suffix "-qemu" to the board name. That is necessary becausefor boards that don't have the name prefixed by "qemu_" and even thoughrun emulated by default on Zephyr there is no easy way to detect them,since it's not possible to determine it by looking at any Cmakegenerated file or by using the `west` command to query that info.The case where the board is emulated by default but has the prefix"qemu_" in its board name is already handled by the current code.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Add new target mps2_an521This commit adds a new µTVM target to support the Arm reference boardMPS2-AN521, which is based upon a Cortex-m33 core.For more details about that board, please see:http://developer.arm.com/tools-and-software/development-boards/fpga-prototyping-boards/mps2Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Add an example for the mps2_an521 boardThis commit adds an example on how to run the Zephyr demo under apps/using as a target the Arm mps2_an521 board, which is emulated by defaulton Zephyr. The example is added to the tutorial script micro_tflite.py,where other examples for other targets exist.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix lintFix lint accordingly to the CI error.* Satisfy lintSatisfy lint about boolean expression format.* Address suggestion from AndrewAddress suggestion from Andrew in the review.Also updates the comment about suffix being trimmed off.Thanks,Gustavo	0
[ROCM] Working math function support for ROCm backend, a bug fix in LLVM based codegen (#570)* added math function support* bug fix extern func call in llvm based codegenlint fixfix buildbug fix extern func call in llvm based codegen* moved rocm bitcodes detection to python	4
std::string -> tvm::String for Conv1DAttrs (#9921)This is necessary to make the Rust bindings work.	1
do (#2883)	5
ONNX Opset 14 Support - HardSwish (#10735)* ONNX Opset 14 - HardSwishAdded hardswish support to TVM CI and fixed unit test.- Add class HardSwish and added its reference to convert_map in onnx.py;- Removed test_hardswish entry from test_forward.py;* ONNX Opset 14 Support - HardSwishFixing onnx.py format.* jostle ci	0
[CI][DOCKER] Add pytest-lazy-fixture to images (#10970)Install lazy-fixture pytest plugin. This is needed for PR #10865.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	3
Migrate simplifier to new infra. (#3368)	5
Fix example code (#6627)	0
Only allow 4d or 5d inputs to TRT nn.pad (#8073)	1
[PTX] Intrinsics for async copy from global to shared (SM80) (#11368)* registor ptx builtin for async copy* add basic codegen* add test* update codegen* wip* codegen bug fixed, test working* add commit group* add doc	2
[Golang][Doc] improve the samples and doc (#4385)* [Golang][Doc] improve the samples and doc* [Golang][Doc] add asf header* [Golang][Doc] Improve the end to end example* [Golang][Doc] Improve the end to end example	1
[AUTOTVM] Use range in AnnotateSpace to fix JSON serialization (#2278)	5
add a check for null function attributes (#5674)	1
[BUG_FIX] Fix resize test (#6298)* fix resize tests* add different scale to resize tests* fix dynamic to static resize test* fix error throwing in topi resize* fix topi and importer tests* fix lint* flakey test failed* make resize test less sensitive; had floating point rounding err on gpu* remove nearest_neighbor + half_pixel option from pytorch importer* remove nearest_neighbor + half_pixel in upsample3d	4
[OpenCL] Fix type casting error (#11021)Faced situation when generated OpenCL kernel contained the following ifcondition:```if (uint4(...) && (int4(...) == int4(...)))```In this case, got the following error:"can't convert between vector values of different size ('uint4' and 'int __attribute__((ext_vector_type(4)))')"Added casts for binary ops. But it was necessary to modify `CastFromTo`and add new method `CastTo`. Because with `CastFromTo` the followingcode was generated:```if (uint4(...) && (convert_uint4(int4(...)) == convert_uint4(int4(...))))```But the OpenCL compiler still generated the same error.This is why added new method `CastTo`. In this method we don't check thecurrent type of op and just add cast to a new type.Finally the following code will be generated:```if (uint4(...) && convert_uint4(convert_uint4(int4(...)) == convert_uint4(int4(...))))```	1
fix doc examples & easy install (#143)	2
[Fix][Frontend][TOPI] minor bugs (#8622)* fix* fix* lint	0
[Compilation Warning Fix] comparison between signed and unsigned integer expressions (#807)The compilation warning is fixed. src/runtime/graph/graph_runtime.cc:392:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   CHECK(data_byte_size == size)         ~~~~~~~~~~~~~~~^~~~/mnt/D_DRIVE/work/nnvm_22_Jan/nnvm_latest/tvm/dmlc-core/include/dmlc/logging.h:109:9: note: in definition of macro ‘CHECK’   if (!(x))                                                \         ^	5
Fix typographical error. (#6664)	0
[Frontend] Check LLVM enabled/installed (#8414)-Some ops(ex:view) call infer_value when converting a model into Relay IR.-If LLVM is not enabled, it leads to segementation fault.Co-authored-by: kueitang <kueitang@qti.qualcomm.com>	0
[DOCS] Fix sphinx warnings (#4917)* Fix Python docstrings* More fixes* Fix lint	0
[DOC] Reorganize docs (#397)	2
Rename shared_ptr<Node> to NodePtr (#8)	5
[Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0	4
match pytorch 1.6 googlenet pretrained model (#6201) (#6212)	5
[TIR] Expose Memory Copy-Related PTX Builtins (#12611)* Expose Memory Copy-Related PTX BuiltinsThis PR exposes the following TIR operation in python:`ptx_ldmatrix`: tested`ptx_cp_async`: tested`ptx_commit_group`: tested`ptx_wait_group`: testedCo-authored-by: yongwww <yongcale@gmail.com>* apply code review suggestionCo-authored-by: yongwww <yongcale@gmail.com>	3
Add needs-triage label to CI Problem template (#12386)These issues are reported as part of the CI monitoring rotation andusually reflect something contained and actionable. As such we shouldtrack and monitor these issues and make sure that a proper mitigation orfix is merged soon. This label helps us do this since we can easilyfilter by untriaged issues and make sure each one has an assignee.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[AutoScheduler] Improve hyperlinks in the tutorial (#6521)* improve auto-scheduler tutorials* improve tutorials* fix lint	0
[Relay] fix small typekey issue (#1992)It might cause TupleTypeNode to be printed incorrectly.it doesnt show in http://ci.tvm.ai:8080/blue/organizations/jenkins/tvm/detail/PR-1989/1/pipeline/141, but if you run it on local machine you will see what get compared being NodeBase and TupleType.Also as a side thought can we write a giant macro that make sure everything get did right (all field get visited, typekey match, declare_node_type_info match, etc?) I can do some macro metaprogramming, so I can take up the work.	1
Add TopK to ONNX Frontend (#5441)* Add TopK to ONNX Frontend* respond to review comments	1
[microNPU] Add support for TFLite concatenate (#9589)* Add legalization pass and is_valid checks for concatenate* Add TIR pass for removing concatenates and replacing them with direct  writes to the final buffer* Add testsCo-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>	3
Register SkipVectorize (#3228)	5
[LANG] Add all and any in the python API (#196)* [LANG] Add all and any in the python API* compatible with python3	1
refactor build module to take IRModule (#4988)	4
Add Winograd matrices computation. (#3553)	1
[Hybrid Script] Supporting scheduling hybrid script (#2416)* on the way to enable hybrid schedule* I think I am done with imperfect loop split?* copyright watermark* loop annotation* fix lint* fix lint 1* shit!* loop reorder supported* support bind to add some tests* fused tested* imperfect loop testcase* fix lint* add bind testcase* fix doc* fix online edit typo* resolve @mercymercy review* fix indent* i should convince myself it is not flaky test first* fix test hybrid* how many flaky test are you expecting; i ball ball u to let me pass* rebase halide...	4
Enable conv family fused with mish (#12228)	1
Fix comments while reading the codes. (#42)	0
[Hexagon] Minor changes/fixes in codegen_hexagon.cc (#12308)1. Change calls to inherited functions to use CodeGenCPU:: instead of   CodeGenLLVM::.2. Fix #if guards for #include's.3. Remove InitContextPtr and GetContextPtr, use the ones from CodeGenCPU.	1
Fix a Uninitialized Variable Warnings. (#10436)There is a 'Uninitialized Variable' Warning in building process, just fix it.	0
fix calibration pass to support multiple functions (#5768)Co-authored-by: Ubuntu <ubuntu@ip-172-31-43-142.us-east-2.compute.internal>	1
[RPC] Add the IPV6 support for server side auto tuning (#2462)* use IPV6 instead of IPV4* backward compatible* add error report* fix linter* more linter* fix the python2 api	0
Fix the shape check for vta dense strategy (#6983)	0
[REFACTOR] Establish tir (#4740)TIR is the new namespace for low-level IRfor tensor-level optimizations and loop transformations.This PR establishes the namespace and files.- lowered_func.h,buffer.h,data_layout.h -> tir/buffer.h,tir/data_layout.h,tir/lowered_func.h- ir.h -> tir/expr.h, tir/stmt.h- ir_functor_ext.h -> tir/expr_functor.h, tir/stmt_functor.h	5
Add `init` member to ReduceNode (#6138)- This patch adds a new member to ReduceNode called init which allows  initialization with a custom ProducerLoad or a Float/Int immediate.- This allows initialization of the output Tensor of a reduction with  another Tensor instead of the `identity_element` defined in the  CommReducer- One example use case for this node is to initialize the Output of a  convolution reduction with the Bias values thereby saving the  Bias-add computation.	1
[TARGET] each option of target str should only contain one '=' (#5988)src/target/target_id.cc ParseAttrsFromRawString L222:if ((pos = FindUniqueSubstr(s, "=")) != -1)require option contains only one '='Signed-off-by: windclarion <windclarion@gmail.com>	1
fix convert_pooling in caffe parser (#9828)	0
[TEST] use rpc.LocalSession for simple tests (#6294)To avoid flaky due to networking.	1
[microTVM][Zephyr] Add MIMXRT1050 board support (#9068)* add target support* fix ci issue	0
[TF] Fix some shape mismatches between TF and Relay (#6166)Make ndarray_size output scalar  Make gather_nd output scalar if needed	1
[Topi x86] Missing vectorize for depthwise conv2d. (#5196)	5
Edit onnx parser to infer values in post order (#5755)* edit onnx parser to infer values in post order to speed up onnx imports with many calls to infer_value* fix pylint	0
[DOCS] Move the api docs to the api subfolder (#5626)* [DOCS] Move the api docs to the api subfolder* Update numpydoc location* Ignore 403* make sure folder exists	1
[DOCS][COMMUNITY] Improve code review guideline on API designs (#2459)	1
[CUDA] Fix codegen for warp shuffle intrinsics (#5606)* fix shfl intrin* improve test_lower_warp_memory_cuda_half_a_warp	3
Tighten buffer bound for TensorComputeOp by improving EvalSet on ranges (#2565)	1
[microNPU] Expose compute cycle annotations to TIR lowering (#11288)* [microNPU] Expose compute cycle annotations to TIR loweringAdds an AttrSttmt "compute_cycles_hint" to each NPU operation for laterpasses to consume.Change-Id: I09779bdab6de6ef2094db610bb20d6e052e68ee3* compute_cycles->compute_cycles_hintChange-Id: Iebd71e699522e92a28fd321ffdb41ed7924db4e0* add test to check annotations in compilation flowChange-Id: Idcdcc8c8b5536c4732f297246b71aa8378a2732c* add compute cycles hints for copy operationsChange-Id: I007ba19732e16081fa2ea9baca40c64a653c93cf* fixing annotations for copies and improving test coverageChange-Id: Ib812c4151fab03f4c1adcc016b4e798003a22e5e* rebaseChange-Id: I653101908706096ae25ad1ebf08e7b6c4f1196c7	4
[Meta Schedule] Allow Non-strict Population Size in Evolutionary Search (#10163)	1
Hotfix Jenkinsfile (#9592)* [CI] Use correct variable for image name in Jenkinsfile* Hotfix jenkins* Update JenkinsfileCo-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>	2
fix test (#3525)	3
[Relay] Recursive destructor call replaced with non-recursive for Call nodes. (#7832)* [Relay] Recursive destructor call replaced with non-recursive for Call nodes.Recursive destructor call replaced with non-recursive (based onExpandDataflow) for Call nodes. This prevents OutOfStackexception during unwinding a chain of destructors for large-sizedsubtrees based on smart-pointers.Change-Id: Ib9da3ff8af3a0a41287b8ce9ab2bee2d0813d01cAddressed requested changesAddressed requested changes, simplified the codeadded unit testChange-Id: I7fdd44da3b6c366a555fd9157fa3630b6e789d64* removed inline befor Call destructorChange-Id: I6328e423670f185393d50ccd3d6fdc1326be3767	4
[CI] Split Integration tests out of first phase of pipeline (#9128)* [CI] Split Integration tests out of first phase of pipelineI took a look at the time taken by each stage in the Jenkins pipeline and what comprises the 6 hour CI build time. CPU Integration tests took `65` minutes of the `100` minutes of `Build: CPU`. By adding `python3: CPU` with just those Integration tests, it lines up with `python3: GPU` and `python3: i386` which both take a similar amount of time and takes roughly 60 minutes off the overall run time.Numbers copied from sample successful run (final time approx: 358 minutes):|Phase|ID                           |Job   |Minutes                                      |Start||-----|-----------------------------|------|---------------------------------------------|-----||0    |0                            |Sanity|3                                            |0    ||1    |0                            |BUILD: arm|2                                            |3    ||1    |1                            |BUILD: i386|33                                           |3    ||1    |2                            |BUILD: CPU|100                                          |3    ||1    |3                            |BUILD: GPU|25                                           |3    ||1    |4                            |BUILD: QEMU|6                                            |3    ||1    |5                            |BUILD: WASM|2                                            |3    ||2    |0                            |java: GPU|1                                            |103  ||2    |1                            |python3: GPU|66                                           |103  ||2    |2                            |python3: arm|22                                           |103  ||2    |3                            |python3: i386|70                                           |103  ||3    |0                            |docs: GPU|3                                            |173  ||3    |1                            |frontend: CPU|40                                           |173  ||3    |2                            |frontend: GPU|185                                          |173  ||3    |3                            |topi: GPU|110                                          |173  ||     |                             |      |                                             |     |Numbers predicted after change (final time approx: 293 minutes):|Phase|ID                           |Job   |Minutes                                      |Start||-----|-----------------------------|------|---------------------------------------------|-----||0    |0                            |Sanity|3                                            |0    ||1    |0                            |BUILD: arm|2                                            |3    ||1    |1                            |BUILD: i386|33                                           |3    ||1    |2                            |BUILD: CPU|35                                           |3    ||1    |3                            |BUILD: GPU|25                                           |3    ||1    |4                            |BUILD: QEMU|6                                            |3    ||1    |5                            |BUILD: WASM|2                                            |3    ||2    |0                            |java: GPU|1                                            |38   ||2    |1                            |python3: GPU|66                                           |38   ||2    |2                            |python3: arm|22                                           |38   ||2    |3                            |python3: i386|70                                           |38   ||2    |4                            |python3: CPU|60                                           |38   ||3    |0                            |docs: GPU|3                                            |108  ||3    |1                            |frontend: CPU|40                                           |108  ||3    |2                            |frontend: GPU|185                                          |108  ||3    |3                            |topi: GPU|110                                          |108  |* Fix typo in ci_cpu commands	2
[Fix] Add ConstantNode to IsAtomic (#5457)* add constantnode to atomic* Add ToANormalForm to FoldConstant	1
Fix typo in tutorial doc (#10974)Fix typo in the commented out code in TVMC Python tutorial.	2
[RUNTIME] Switch time evaluator to use device specific timing. (#7631)	1
[CUBLAS, CUDNN] Support dynamic batch size (#7194)* support cudnn and cublas on dynamic batch size* added test for cublas* add comment on algo choice	1
[TOPI][Relay] Add bitwise ops (#4815)* Add bitwise ops to topi* Add the bitwise ops to relay.	1
Revert "[Frontend] Add Span filling for frontends to Relay (#9723)" (#10072)Because of the failure of LSTM conversion from Pytorch	0
Make choice of archiver configurable (#288)At present, the build system assumes that the Unix archiv utility 'ar'will always be present in PATH, but this might not be true if a separatetoolchain is being used. This patch makes the build system a bit morecross-compile capable.	5
[Relay][TOPI] operator All (#3124)* [Relay][TOPI] operator All* Update tests/python/frontend/tensorflow/test_forward.pyCo-Authored-By: yongwww <55wuyong@163.com>* fix comments* change to level 4	4
[Relay] Fix Fuse (#3035)* save* fix* Update fuse_ops.cc	1
[CI][Hexagon] Add Hexagon Tests to pipeline (#10302)* Add hexagon tests to CI Hexagon* Fix CRT libs* cleanup and fix Jenkins* Address @areusch comments	1
[TOPI, Relay refactor] Move Dilation2d from nn to image namespace (#5110)	4
reminding message for TVM_REGISTER_NODE_TYPE (#4365)	5
change ci image version (#4313)	4
[CI] Enable llvm-11 and llvm-10 in build tests, recover webdocs. (#5579)This PR ties up the last loosen end of the recent CI update.	5
[OP] Conv2d and Depthwise Conv2d for Raspberry Pi (#49)* [TUTORIAL] ImageNet Inference on Raspberry Pi* Update tvm	5
[LLVM] More optimized option, allow emit assembly (#187)	1
[ARITH] Fix x||!x for comparisons in rewrite simplifier (#3029)	0
[FRONTEND]Darknet support batch size for yolo (#5688)Fix the issue reported in https://discuss.tvm.ai/t/yolov3-tiny-batch-input-test-failed/6796	3
[CI] Fix windows build, add azure pipeline (#3458)	1
[Torch] Support index_select (#6295)* support index select* minor fixCo-authored-by: masa <masa@pop-os.localdomain>	0
Add Pack operator to TFLite (#3521)	1
fix RemoveUnusedFunctions pass	4
[COMMUNITY] New committer -- trevor-m (#8141)	1
add files (#6986)	2
TVMC: Allow to overwrite TVM_CONFIGS_JSON_DIR via environment variables (#11623)If a non-default location for the build directory is used, e.g. set via TVM_LIBRARY_PATHwe need to provide the user a way to overwrite CONFIGS_JSON_DIR as well.	5
update docs for installation for CUDA (#3832)	2
Do not show meta-data when printing IRModule (#6881)	5
return mod from frontend for autotvm (#3401)	5
use auto source_group (#146)	1
[relay][op] add expand op (from ONNX) to relay frontend (#4483)* Add Expand to onnx.py* add test function for expand* Fix a onnx frontend test* Add tests for the value itself instead of shape only on test_expand* Cleaned up some unnecessary modifications.	4
Improve reduction schedule on arm CPUs (#6110)* Improve reduction schedule on arm CPUsChange-Id: I9cd85deac6a57666b82ff7250d827652a4000d82* Retrigger CIChange-Id: I5efd99e34268e6bb990904a4b98e1edf2174b26b	4
[Autoscheduler] Reduce task weight coercion overhead (#8995)Co-authored-by: Peter Salas <psalas@octoml.ai>	5
[Frontend][Keras] Fix Dense with 3d inputs (#7753)* Fix keras rnn dense* Fix unit test* Fix unit test	3
Migrate badge to new job (#3459)	1
[NNVM] Support argmax/argmin in tensorflow frontend (#1514)	1
[Frontend][Relay] Fix node indices attribute error for tensorflow 2.3 (#6288)* Fix errors caused due to node attributes* Add node_indices attr for old keras pkg support	1
[NNPACK] Add nnpack.convolution (#301)* [NNPACK] Add nnpack.convolution* Add instrinsic* Fix lint	0
[CI] Upgrade ONNX (#9965)* jenkinsfile and one test* formatting* swtich to proper repo for docker* fix missing - with _* jostle* upgrade to latest images* jenkinsfile and one test* formatting* swtich to proper repo for docker* fix missing - with _* upgrade to latest images* jostle ci* update with official images* jostle ci	5
[TE] Support negative indices  (#9023)* initial change* more explicit api* switch to select* add support for negative indices* reduce things further* lint* to CamelCase* unit testCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[Relay][Frontend][Onnx] Add support for Size op in Onnx frontend. (#7031)* Add support for Size op in Onnx frontend.* Simplify target testing.	3
[COMMUNITY] @optima2005 -> reviewer (#5004)	3
Add tvm::support::hexdump() debug utility (#6154)	0
[RELAY] Fix ops in packed layout (#2472)* [RELAY] Fix ops in packed layout* Fix style	0
[TOPI] Formalize the tag system (#473)	5
[autotvm] fix #2617 (#2619)	0
C-RNN layer support is added (#1492)	1
[TIR] Schedule Primitive: Add-Unit-Loop (#11575)In TE, a unit loop could be introduced by fusing an empty list of loops on a stage. This PR adds its counterpart in TIR, while being a bit more explicit with a new schedule primitive which adds a unit loop without impacting any existing functionalities.	1
[Ansor] Support multiple output ops and fix Python API printing (#6584)	0
[CI] Add alexnet and googlenet caffe model to request hook (#12510)This PR intends to move the alexnet and googlenet caffe models from the old link to s3, therefore getting rid of the flakiness in `caffe/test_forward.py` introduced by external url timeouts. Fixes #12465	0
[TIR][REFACTOR] Remove ir_pass in favor of analysis/transform. (#5415)This PR removes ir_pass(old style pass functions) in favorof analysis/transform(new style pass manager).	4
fix tf.compat.v1 issue for tf verison <=1.12 (#4593)	0
[TIR] Fix an index out of bound issue of compact buffer region (#11201)After https://github.com/apache/tvm/pull/10557, the region extent after compaction is ensured to not exceed original shape. Now when the inferred region min is negative, the index remap rule `idx -> (idx - region_min)` would introduce out of bound accesses, which would cause crashes at runtime.The two updated cases in UT:- padding block inlined to poolingCurrent version results to out of bound accesses in `cache` block, since the H/W extents are compacted to no more than 224 but accesses are shifted by `- (-1)`.```python@T.prim_funcdef func(X: T.Buffer[(224, 224), "float32"], Y: T.Buffer[(224, 224), "float32"]) -> None:    cache = T.alloc_buffer([224, 224], dtype="float32")    for h, w in T.grid(224, 224):        with T.block("cache"):            cache[h + 1, w + 1] = X[h, w]    for h, w, kh, kw in T.grid(224, 224, 3, 3):        with T.block("compute"):            Y[h, w] = T.max(Y[h, w], T.if_then_else(T.likely(1 <= h + kh, dtype="bool") and T.likely(h + kh < 225, dtype="bool") and T.likely(1 <= w + kw, dtype="bool") and T.likely(w + kw < 225, dtype="bool"), cache[h + kh, w + kw], T.float32(0), dtype="float32"))```-  sparse access`A_data_local[A_indptr[i] + k]` is rewritten to `A_data_local[T.min(A_indptr[i] + k, 0)]` instead of `A_data_local[0]`. Compared to current version, interestingly, it keeps the semantic that negative sparse index result to oob behavior.	5
[VTA] [Hardware] Chisel implementation (#3258)	5
[RUNTIME] rename allocator.make -> allocator.make_object for term consistency (#4416)	1
[CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN (#9979)* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NNChange-Id: I6a3ba9db3e3cb2bd7c10383ebd52f9a1cdad74d0* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN* Addressing commentsChange-Id: I98fcdf95bf408700968827e1abd084a916b3b21c* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN    * Addressing comments    * Remove build folder before running demo to address #10020Change-Id: Ifa7ad3ff431f427f8afb8b3c9f06711b3b59ad62* Correctly filter tvmc TargetsFixed logic to check for >2 TVM Target to be based on none-hybridTargets onlyCo-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>	1
[ci][tvmbot] Enable re-run for GitHub Actions (#12295)This adds the right permissions so anyone associated with the repo can trigger a re-run (GitHub hasn't flagged all committers as repo `COLLABORATORS` for some reason so it's difficult to determine from a username who has commit rights) and makes it so `@tvm-bot rerun` also re-runs all the Actions on a PR.	1
relax rtol atol for div/log operators (#2400)	1
[Relay][ConvertLayout] Support deformable conv2d (#7087)* add test case* fix* support* test case* fix* fix test* fix bug* add comment	1
Fix Tensorflow conv3d pad bug, add non-cubic data and kernel tests (#4772)	3
Fix a bug with Alter Op Layout (#6626)* Regression test for a Scalar type issue in Alter Op Layout* fix the regression test by avoiding the Scalar optimization if types aren't defined	3
[Relay] Bug fix when applying history using an iterator or records. (#11306)* Bug fix when applying history using an iterator or records.* I forgot strings are iterables.	1
[TIR] Fix perf regression of tir refactor (#5258)	4
[docs][microtvm] fix command path in microTVM Reference Virtual Machines Running Tests documentation (#11333)	2
Fix code comment of operators (#3830)	1
[Relay, TOPI] Add numpy style cumsum op (#7334)* Add cumsum relay/topi op* relay tests working* add torch frontend converter* fix for importing detr* fix bad merge* begin cuda cumsum* support non innermost axis* support rank higher than 3* making binop parameter* fix overflow issue in thrust scan* generic binop parameter working* relay test working* fixed for bool input* remove pytorch change* fix pylint* doc update* Update python/tvm/topi/cumsum.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Update tests/python/relay/test_op_level3.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* add example outputs* add supported input and output dtype in thrust log* adding more loop var names* fix cpplint* fix missing check for the cuda target in nms thrust sort* parallelize cpu cumsum* making binop argument tir function* update doc for binop* doc updateCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>	5
Raise an exception when extern function does not return Stmt (#5964)The function for tvm.te.extern should return either PrimExpr or Stmt,however there is no check if it actually does so. If it does not, theresult may be a segmentation fault later on. Catch this case early on,so an informative message can be shown.	5
fix the description of create_shared (#793)The type of parameter options should be a str list.	2
[microNPU] Removing constant args from PrimFunc (#9951)Before this commit, microNPU creates PrimFunc as ifit accepts constants from the callee. This commitchanges the PrimFunc to remove the constants as anargument to PrimFunc as they are not provided fromthe main function.	1
[ONNX] Update onnx shape op with slice index support (#10947)* support shape op slice indices* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
axis bounds checking in split (#1178)* Add kazum to contributers* Add axis bounds checking in split	1
Improve makefile (#165)* Improve makefile* Fix	0
[VM] Remove undesired arg to load_late_bound_consts (#9870)* Remove undesired arg to vm exec load_late_bound_consts* No-op for ci	4
Update CI badge location (#6517)	5
[Relay] add more function to prelude (#2660)	1
[frontend][keras] Add support for TimeDistributed (#7006)* First pass on modifying Keras importer to handle TimeDistributed* Use squeeze inside TimeDistributed, add tests* linter fixes* More linting* Even more linting* Fix unused argument annotations* Forgot one pylint annotation* Forgot to set up data layout in _convert_activation* Decouple data_layout from etab* Linting fix* Forgot to set data_layout argument* Missed an etab.data_format, also test_conv1d was not in the test file's main* Rebase fixes* Linting fix* _convert_lambda needs a data layout argument too* linting fix too* Lint the test file too* Redundant variables* Simplify further* Another simplificationCo-authored-by: Steven Lyubomirsky <slyubomirsky@octoml.ai>	5
adding giuseros to reviewers list (#7950)	1
[REFACTOR] Remainings of util => utils (#6778)	5
[Relay] Add support of conv2d with NHWC for Bifrost (#8430)Reuse generic Mali strategy for conv2d with NHWC inBifrost target.	1
[WIP] WebGL Backend (#672)Basic WebGL Backend	5
[microTVM][TVMC] Add TVMC test for Arduino and Zephyr (#9584)* Add TVMC test for Arduino and Zephyr* Address @gromero comments* address comments	1
[Relay] Support 'external codegen targets'. (#11173)* [Relay] Support 'external codegen targets'.(Part of Collage, https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md)This change prepares the VM and Relay target handling machinery to supportexternal codegen targets in addition to 'regular' targets. This allows usto configure the build with Collage as follows:```    host_target = tvm.target.Target("llvm")    targets = [tvm.target.Target("cuda", host_target),               tvm.target.Target("cutlass", host_target),               tvm.target.Target("cudnn", host_target)]    with tvm.transform.PassContext(...):        exe = tvm.relay.vm.compile(module, target=targets)```Four changes are required:1. I introduce four new target kinds for the external codegens currently supported   by Collage. Others can be added as they are vetted for use by Collage. These   are given a device type matching the external codegen's assumption (ie just CUDA   currently), and given a target kind attribute "is_external_codegen" of True. The   latter is needed by Collage to signal the target kind name represents and external   codegen 'compiler' name. See the RFC for specifics.2. I introduce the binary relation Target::IsExternalCodegenFor so that   external codegen targets can be related back to the 'underlying' targets   they are implicitly using in their codegen.3. I rework the VMCompiler and BuildModule interfaces to accept an Array<Target> of   'raw targets' instead of a Map<Integer, Target>. This more general representation   is needed because we may now have multiple targets of the same device type   active simultaneously. I add new static methods on the Python Target to   convert to this form in a way that mimics check_and_update_host_consist.4. I rework CompilationConfig to work from Array<Target> directly, to not depend   on the host_target argument (since dealt with on the Python side), and to   understand that if we have two targets for the same device type the non-external   codegen target takes precedence.The change to CompilationConfig seems neutral with respect to the recent discussionson compilation configuration representation and tvmc.I made a few attempts to remove Target.check_and_update_host_const entirely in favorof using CompilationConfig as the definitive target handling choke point but backedout once they became too large.* - Working on unit tests* - Fix two Debug-only failures* - Use Array<Target> in GraphExecutorCodegen/AOTExecutorCodegen ifaces instead  of CompilationConfig (don't want to bake it into any official APIs).- Started unit tests.* - Lints* - Moar Lints* - Fix some unit tests* - Fix last unit test failures* - whitespace* - Address Eric's comments.  CI likely to fail due to stricter FindPrimitiveTargetOrFail but let's see.* - Comment adjustments.- Unit test for new Target members.	1
[CI] make pre-commit hooks to run on every push instead of every commit (#8888)	1
[tests] Fix changed var name from 'target_str' to 'target_names', NFC (#11982)	1
codegen_spirv support Call::reinterpret (#3795)	1
[QNN] Concatenate operator (#3730)	1
[FIX] Fix build error: call to 'make_const' is ambiguous (#415)	1
[Relay] Allow converting keras.layers.Sequential (#2842)* Allow converting keras.layers.Sequential* Use existing new_var function* Only update expr when missing* Add test	3
[CMSIS-NN][Perf] Converted Relay Conv2D into CMSIS-NN Depthwise (#12006)	5
[REFACTOR][IR] alpha_equal to structural_equal (#5161)	4
Fix double compile of runtime sources for TRT, ACL (#7436)	1
[CI] Fix Flaky Test `test_task_scheduler_gradient` (#10360)* [CI] Fix Flaky Test `test_task_scheduler_gradient`A change to fix the issue of flaky test mentioned in #10356 by increase the `chain_rule` factor and avoid small gradient.* Retrigger CI.	1
Update `is_floating_point` to handle bfloat16 (#7133)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* Properly handle bfloat16 in is_floating_point* Revert test changes* revert whitespace changes	4
[CODEGEN/PASS] Improve callpacked lowering, allow pass array callback. (#110)* [CODEGEN/PASS] Improve callpacked lowering, allow pass array callback.* fix cython	0
[NNVM] Introduce const shift ops (#1325)	5
[BYOC][ACL] ACL migrated to v21.02 (#7649)This PR switches ACL* version from v20.11 to v21.02ACL stands for Compute Library for the Arm® Architecture.Change-Id: Id364b571d5611ca6eb6d2bde09448a65aae3f73b	4
disable one of rewrite in torch detection test (#7365)	3
[REFACTOR] examples->apps (#210)	4
[RPC] Fix the multihop cpu case (#5522)	0
Get list of unsupported ONNX operators (#2995)	1
fix Stage.fuse (#33)	1
Fix Vulkan Build, add tanh to llvm instrinsic, fix halideIR (#868)* Fix Vulkan Build, add tanh to llvm instrinsic, fix halideIR* fix llvm tanh	0
Bring back numbered lists to TVM docs. (#7290)* Upstream fix in https://github.com/tlc-pack/tlcpack-sphinx-addon/commit/995178d81e6e38eabbc28da2b285b68583c88769	1
[RELAY][RUNTIME] Add compute and schedule attributes for all ops in relay/op/tensor.py (#2050)	1
[ARITH]  Analyzer Infra, ConstIntBound, Modular (#2668)	5
[VTA] [Chisel] make dram offset configurable for uops different than 4-bytes (#3654)	5
[Hexagon] Provide empty weak definitions of two missing functions (#10847)	1
[Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>	1
cleanup Hexagon conv2d tests (#9473)* cleanup Hexagon conv2d tests* add back fixtures; skip tests only on i386* fix pylint error* fix maxpool failures* fix `skipif` statement and verify error + code review feedback* fix typo "physical_physical"* retrigger ci* determining i386 processor string from CI* hopefully the correct test disable	3
[CYTHON] Make speedup component minimum (#13)	5
[Relay][Frontend][ONNX] Add ConvInteger support. (#8456)* Add ConvInteger support and fix some ConvTranspose padding bugs.* Simplify pads check.* Fix style.* Remove changes to conv_transpose.	4
add rm xla attributes in tf docs (#5950)	2
Hide symbols from dependent libraries if HIDE_PRIVATE_SYMBOLS is ON. (#4041)In current implementation HIDE_PRIVATE_SYMBOLS hides symbols from TVMitself but not from its dependent libraries. This is problematic whenother third-party libraries with the same symbols are linked to thesame executable.One example is using TVM with Mesa OpenCL drivers: they depend on LLVMand load its shared libraries with RTLD_GLOBAL flag, which results inconflicts with LLVM symbols that TVM uses. Arguably this particularissue belongs to Mesa (here's their tracking bug:https://gitlab.freedesktop.org/mesa/mesa/issues/236) but in generalthat's the right thing to do regardless of this particular bug.Note that I'm not enabling this functionality for Darwin as in myearlier tests their linker didn't seem to understand "--exclude-libs"(but I don't have test platform ATM to double-check).	3
Enable groups argument for conv2d_transpose on the cudnn backend (#10396)* wip* reset conv2d_transpose topi conv_mode to 1* fix for 'Error: identifier “hfabs” is undefined'* address @masahi's comments in pytorch test_forwardCo-authored-by: Masahiro Masuda <masahi129@gmail.com>	3
[CI] Enable llvm in CPU test (#688)* [CI] Enable llvm in CPU test* fix llvm	0
checkin basic var	5
DeviceType enums match dlpack (#8407)* DeviceType enums match dlpack* intentionally left blank	5
Update CI images (#8031)* [CI] Updated docker images* fixes to build images* fix arm image version* update qemu* reset jenkinsfile	2
[Bug Fixed] Make query_rpc_tracker show the correct device server port and customized address (#8203)	1
[CodeGen][CUDA] Fix bugs (#5209)- Support vectorized casts- It is incorrect to extract elements from int8x4 with   0x000000ff & (x >> i * 8)  as this value is of type int in C/C++. If this expression  is used for sign extensions, the sign bit will be wrong.  Simply use C style casts instead and sign bits will just work.Signed-off-by: Wei Pan <weip@nvidia.com>	1
[ETHOSN] Improve inferring new shape of the Reshape operator (#12594)Fixes the case when reshape is > 4 dims. While this cannot be offloaded to the NPU, the check was previously producing an error preventing further compilation. The correct behavior is to ensure the check returns False and not offload the reshape.	0
[DOCKER] Golang CI recipe. (#1759)	2
Fix the gemm conversion in onnx frontend (#1241)	0
[Hexagon] Slice op relu (#11449)* Add support for relu slice op.* Format code* removing out_shape in relu def and lint issues* removing out_shape in relu def and lint issues* Changes as per the new formatCo-authored-by: Venkat Rasagna Komatireddy <89959097+rasagna-quic@users.noreply.github.com>Co-authored-by: Venkat Rasagna Reddy Komatireddy <rasagna@hu-rasagna-hyd.qualcomm.com>	1
[metal] update language version (#7116)* [metal] update language version* fix mps	0
Update fuse_ops.cc (#2102)	1
Correct runtime.load_module (#6161)	1
[DNNL][CBLAS][BYOC] Unifles all MKLDNN/DNNL  to DNNL (#11638)* unifies all MKLDNN/DNNL_CODEGEN to DNNL* translate -lib=mkldnn to -libs=dnnl in target* type check added before* rebase and update conv2d from mkldnn to dnnl	5
[TOPI] Add compute for more operators (#849)* [TOPI] Add compute for more operators* Remove device except llvm* Address comments* Remove matmul compute* Add outtype to boolean operator* Address coments	1
[Doc] minor fix for release doc (#5948)	2
[INIT] Allow proper throw in compiler (#39)	1
Implement C++ registry to back Python target.generic_func (#892)	1
[STORAGE][BUFFER] Support access ptr for clear access pattern. (#266)* [STORAGE][BUFFER] Support access ptr for clear access pattern.* fix lint	0
Add a conversion of individual operations in FQ2I pass. (#10239)* Add a conversion of individual operations in FQ2I pass.* apply review comments* apply review comments 2	4
maintenance (#5058)	5
[TEST] Comptiable with python3.5 (#3675)	3
[Relay][Frontend][Onnx] Loop Support (#6700)* Onnx loop almost working, checkpointing for safety.* Very close to working.* Last piece is fixing scan initialization.* snapshotting for debug.* Fix Josh's issue* Use subgraph proto class.* Loop with scan.* Simple loop test now working.* Scan outputs now working.* Added second loop test.* Removed unneeded helper functions.* Remove bad merge artifact.* Cleaned up scan output creation.* Cleaned up some style mistakes.* Add pylint skip for unused-argument.* Remove onnx dependency.* Remove now obsolete checks for 0 shaped tensors.Co-authored-by: Jared Roesch <jroesch@octoml.ai>	5
[ARITH] detect iter affine map with predicate (#7752)	5
fix lint error (#475)	0
[Hexagon] Initial support for meta schedule tuning (#12587)Enables AutoTVM-style, template-based tuning for Hexagon.To run compiled code on Hexagon, we need to use Hexagon `Session` object https://github.com/apache/tvm/blob/dc522a6ff65b68532cd1bba43827cd981114df2c/python/tvm/contrib/hexagon/session.py#L35 in the metaschedule `RPCRunner`. But for RPC "session", `RPCRunner` expects an instance of `RPCSession`, https://github.com/apache/tvm/blob/53fe5966823eee4e011d7228bceab3c82c1d9caa/python/tvm/rpc/client.py#L32,  to be created and used by various customizable functions. Since `RPCSession` and Hexagon `Session` have slightly different API, we cannot use `RPCRunner` with customizable functions directly. So I introduced an alternative implementation of `RPCRunner` for Hexagon.The test is disabled for simulator since `HexagonLauncherSimulator` is not pickle-able due to its `multiprocessing.Process` attribute: https://github.com/apache/tvm/blob/c97895e0ffb512e73c89de7cdee9846f052244fc/python/tvm/contrib/hexagon/build.py#L614Output log from tuning `vrmpy` dense (included in the test)``` ID | Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated--------------------------------------------------------------------------------------------------------------  0 | main | 150994944 |      1 |       380.3399 |     397.0000 |              397.0000 |     32 |--------------------------------------------------------------------------------------------------------------```	3
[VTA][TSIM] Introduce Virtual Memory for TSIM Driver (#3686)* initial virtual memory;* initial integration;* include the header file in cmake;* implement allocation with virtual to logical address mapping;* virtual memory for tsim_driver;* implement the missing memory release function;* readability improvement;* readability improvement;* address review comments;* improved robustness in virtual memory allocation;* remove VTA_TSIM_USE_VIRTUAL_MEMORY macro and use virtual memory for tsim by default;* link tvm against vta library;* merge with master* build virtual memory system without linking tvm against vta;* minor change;* reuse VTA_PAGE_BYTES;* using DRAM class from sim_driver as VirtualMemoryManager;* satisfy linter;* add comments in code;* undo changes to Makefile* undo changes to Makefile* retrigger ci;* retrigger ci;* directly call into VirtualMemoryManager::Global()	2
[TOPI] Fix reduction fusion with injective input (#475)	0
[TIR] Make lower_warp_memory support extent(threadIdx.x) < warp_size (#5307)* support extent(threadIdx.x) < warp_size in lower_warp_memory* more docs for lower_warp_memory	2
[Docker][Onnx] Upgrade ONNX to latest version (#9519)* initial commit* jostle ci	5
[NNVM] Add ONNX upsample converter (#1591)	1
Fix edge cases in const_int_bound and fold_scale_axis (#6911)	0
[IR] fix style in ir_mutator and ir_visitor (#4561)	0
[LLVM/RUNTIME] Support Parallel for on CPU (#54)	1
We observe multiple groups across a range of domains (ASR, NMT, LM, etc), (#3566)internally and externally, interested in replacing standard dense layers withblock-sparse matrix multiplication layers. The motivations are generally: higherperformance (due to reduction in FLOPs, memory bandwidth/cache footprint),enabling larger models (e.g. fitting more layers in a given memory budget).Some public work along these lines:* https://openai.com/blog/block-sparse-gpu-kernels/* https://openai.com/blog/sparse-transformer/* https://arxiv.org/abs/1802.08435* https://arxiv.org/abs/1711.02782Various groups have been able to successfully train models with reasonablelevels of sparsity (90%+) with marginal accuracy changes, which suggestssubstantial speedups are possible (as this implies a >10x reduction in FLOPs).It is fairly straightforward to realize these theoretical speedups, see e.g. TVMbenchmarks for Intel CPUs inhttps://gist.github.com/ajtulloch/e65f90487bceb8848128e8db582fe902, and CUDAresults in https://github.com/openai/blocksparse, etc.* https://github.com/openai/blocksparse (CUDA)* https://software.intel.com/en-us/mkl-developer-reference-c-mkl-bsrmm (MKL BSRM)* https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html (SCIPY BSR representation)This is extracted from an internal patch we've been using internally. There arevarious extensions possible (int8/fp16/bf16, CUDA/other GPU architectures), butthis is a reasonable starting point. This needs more thorough unit test coveragehowever.We follow the conventions established by scipy.sparse.bsr_matrix and otherlibraries, see the unit tests for details.For folks interested in experimenting with scheduling/AutoTVM etc,https://gist.github.com/ajtulloch/e65f90487bceb8848128e8db582fe902 is a usefulstarting point.	1
fix a few bugs with shape inference and types in the onnx importer (#5534)	2
[Relay] Move prelude to text format (#3939)* Fix parser* Doc fix* Add module utility functions necessary for prelude* Implement prelude in text format* Remove programmatically constructed prelude defs* Fix 0-arity type conses in pretty printer and test* Make prelude loading backwards-compatible* Fix patterns* Improve some prelude defs* Fix `ImportFromStd`It needs to also follow the "add unchecked, add checked" pattern* Lint roller* Woops* Address feedback* Fix `test_list_constructor` VM test* Fix `test_adt.py` failures	0
[BYOC-DNNL] enable conv3d->bn folding (#10837)* support conv3d bn folding* add test case for fold_scale_axis* modify lint* remove test cases* unify conv2d 3d impls, and add test cases.	3
Added CopyFromBytes and CopyToBytes convenience methods to NDArray.  Fixed typos. (#4970)* Added CopyFromBytes and CopyToBytes convenience methods.  Fixed typos.* Removed unneed argument check* Use TVMArrayCopyFrom/ToBytes methods* Moved CopyFrom/ToBytes to ndarray.cc* CopyToBytes impl was using CopyFromBytes.  Fixed* changed inline to TVM_DLL* Used impl from TVMArrayCopyTo/FromBytes into NDArray CopyTo/FromBytes* Move implementation of all CopyFrom/ToBytes into a common impls* make arg const* simplify method impl	1
[Runtime][PipelineExecutor] Polish the name and comments of variable. (#10395)Polish comments and variable name	1
[NODE][PASS] Introduce config to PassContext. (#5631)This PR introduces a new config field to the PassContextto allow it store arbitary config values.To make sure that the config is validated, we allow each passto register the config key they would expect and the corresponding types.We also introduce a CreateObject from Map<str, Object> to allow config creationfrom a json-nest(like in vscode) in python.We added an example of UnrollLoopConfig.Followup PR should migrate the passes to use the new config field.	5
Arm(R) Ethos(TM)-U NPU codegen integration (#8849)This commit integrates the codegen for Arm® Ethos™-U.* Adding Conv2D tests and a mobilenet_v1 conv2d offload test.Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Matthew Barret <matthew.barrett@arm.com>Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Matthew Barret <matthew.barrett@arm.com>	3
[HEXAGON] Initial clip operator for Hexagon (#11549)* [HEXAGON] Initial clip operator for Hexagon* Changes to utils and infra for pylint* Remove unused import* Use tvm.testing.main()* Address pylint error* Fix incorrect function call* Changes to calls to transform_numpy* Add newline at end of file* Add requires_hexagon and rename under topi* Whitespace fix and reduce input size* Remove te tensor arguments* Correct call to tvm.build* Run black formatting	1
[Docs] Fix for broken link in apps for wasm-standalone dir (#8045)* [fix] Broken link in apps for wasm-standalone* [fix] Broken link in apps for wasm-standalone* [CI] Manual trigger for CI	2
add MXNet converter for where operator for both NNVM and Relay (#2647)	1
[DOCKER] Fix git clone failure. (#2816)git clone --branch=xxx won't take a hash, switch from the hash to thetag that represents that hash.	0
[Frontend] Prevent tflite frontend from producing int64 shape/parameters (#7030)	2
support of multiple devices for tvm.build (#1773)	1
[ROOFLINE] Add CUDA support to roofline analysis (#12205)* [ROOFLINE] Add CUDA support to roofline analysisAdd functions to estimate peak flops and bandwidth for CUDA. Add a newregistration mechanism to the roofline analysis to support adding anytarget. This mechanism uses generic functions with overrides. Newtargets only need to add `estimate_peak_bandwidth` and`estimate_peak_flops` functions.Also fix cuda codegen and tensorcore_infer_fragment.cc to supportfilling matrix_a and matrix_b fragments.* formatiing* move statement back inside loops* print out report for debugging* default to avx2* review comments	0
[Relay][Frontend][ONNX] New Operators and Opsets to Support BERT (#4197)* Added slice v10* Added constantofshape operation and small refactor.* Finished one_hot implementation.* Reshape working across all bert layers.* Fixed constantofshape and removed code duplication.* onnx model fully ingested.* Working on improving onnx tests.* Changed onnx testing to use onnxruntime instead of caffe2, also formatted.* Add arbitrary output nodes to onnx frontend.* Added v6 tiling for bert squad 8 support.* Small syntax fixes* Reduced code duplication in split opset versions.* Added batch matmul test* Added unstack split testing.* Adde onehot test, needs a little cleanup probably.* Replaced deprecated constant fill with constantofshape and updated tests accordingly.* Added tests for new opset version of slice and tile.* lint clean up* Lint fixes* Changed onnx dependency* Went back to caffe2 runtime for CI integration.* Rebase and small typo/syntax changes.* Added hard casting of onehot attributes to int.	1
Add silent mode to rpc server and rpc tracker (#1268)	1
[SCHEDULE] Add factor_axis to rfactor (#895)	1
[TFLITE]DepthToSpace and SpaceToDepth support (#5041)* [TFLITE]DepthToSpace and SpaceToDepth op parser support* DepthToSpace and SpaceToDepth testcases* Review comments fixed	0
[CI] Fix clang-format error (#5577)	0
[TOPI][CUDA] Enable vectorization on fp16 type (#4867)- This allows to better utilize the memory bandwidth- Note that not all cases are vectorized for fp16 datatype. For  instance, when the size is not a multiple of 1024, the inner loop  may be an expression that cannot be vectorized. In this case, a  small inner loop is still benefical for latency hidding.Signed-off-by: Wei Pan <weip@nvidia.com>	5
[ETHOS-N] Re-enabled tests and updated module hashes (#8498)	5
[TIR] add loop partition hint pragma (#9121)* add loop partition hint pragma* fix unintialized var* fix to remove hint at last* use tir compare for loop partition testcase	3
[RELAY] Fix compilation under clang-4.0 (#1998)	0
Add back-to-back conv2d Hexagon test for stripe scheduling (#9390)* Add back-to-back conv2d Hexagon test for stripe scheduling* top level README with test specific links* test specific readmes* add ASF to README* add k_split and h_split case* add back deleted line from README* add 3x3 conv2d -> conv2d case* address code review feedback* skip conv2d -> conv2d test due to flakiness on i386	3
Remove qemu installation from Zephyr RVM (#8701)	4
[PYTORCH]Activations for pytorch (#5194)* [PYTORCH]Activations for pytorch* Review comments updated	5
[usmp] U3 use case (#11015)* U3Change-Id: Ibc088f19ad1dc9466fc368f8523baa30ee88b7d0* addressed upstream comments* Unit test addedAdded unit test for InterfaceCNode::EmitConstantPool method	3
Add function attribute for shape func for profiling (#8148)	1
[VM] Fix the shape function of conv nhwc (#8480)* Add dynamic support for conv2d nhwc	1
[CI] Fix Rust permissions for wasmtime and sccache (#10015)Previously this was ran as part of `ubuntu_install_rust.sh`, as we now have multiple scripts which write as the container build user we have to fix up each time to ensure future users don't break.	4
[Parser] Add support for parsing the any dimension.  (#6277)* Add case for any dimensions* Fix second test case	3
[Doc] Misc doc fix (#5672)	0
[CUDA] Support multiple TIR-level dynamic shared memory allocations (#8571)	1
[Lint] Ignore Hexagon generated files in CPP lint (#10609)* Ignore Hexagon generated files in CPP lint	2
[PYTHON] Make decorator optional for runtime (#1350)	1
explain the lowering process in nnvm.compiler.build (#339)	5
Fix conv2d_nchw for opencl intel graphics (#8201)	0
[BUILD] Enable path option for ROCM, CUDA, Vulkan, simplify optional build (#1270)	0
[Relay] [PyTorch] Add aten::broadcast_tensors (#11863)* add aten::broadcast_tensors* add entry* fix test	3
[Relay] Mix mode type inference (#6704)	5
[Doc] TFLite frontend tutorial (#2508)* TFLite frontend tutorial* Modify as suggestion	2
[BYOC][FIX] Infer types in MergeComposite (#5766)If InferType isn't run between partitioning passes,function calls are inserted which don't have a type.This can result in failures for patterns which wantto check types.This works around it simply by running InferType afterevery partitioning.Change-Id: Ie0887f0564a41eb0913bfe42a362e8effe9681b9	4
[CI] Move build configuration to shell scripts (#5164)* [BUILD] Fix VTA build in CI* [CI] Move build configuration to shell scripts	5
[PassManager] Implement pass manager tracing API (#4782)* Implement pass tracing API* Set is_before correctly* Add docs for trace function* Fix lint* Remove PDB* Ensure trace_func is set before calling* Fix conditional	0
[QNN] Renaming dense operator. (#4033)	1
[TOPI] Fix reduce fusion with more levels (#477)	0
[ci] Install GNU parallel on lint image (#10951)This will make it so we can lint in parallel on a single machine and still get good output.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Op] Add unbiased variance op and corresponding support in pytorch frontend (#6232)	1
Add NNPACK to CI (#2085)	1
[microTVM] Make Arduino API server obey timeout (#12074)* Make Arduino API server obey timeout* Pass arm_cpu as default option to micro testingSyntax fixIncrease Zephyr default stack size for create_aot_session* Set write_timeout when appropriate* Fix unit tests and lintingCheck whether arm-cpu flag is breaking testsUpdate tests for arm-cpu flag	3
[profiler] Skip i386 skip condition (#11280)See #10698 for some context, this test wasn't actually being skipped on i386Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[ci] Redirect sphinx-gallery URLs to S3 (#11839)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Allow Ctrl+C during docker/lint.sh. (#10291)	2
[docs] Update relay docs (#5112)* Update relay docs* any -> py:func* make clean	4
[ci][docker] Conditionally link sccache to clang (#11316)This was causing errors with #11314 since it was making it appear as if `clang` was available when it was only the sccache wrapper.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Get around limitation of g++-4.8 (#4626)	1
[AutoScheduler] Improve the GPU tutorial by deleting measure_ctx earlier (#6660)* del measurement process in the tutorial* fix* trigger CI	0
[PYTORCH]Reduce_ops support added (#5308)* [PYTORCH]Reduce_ops support added* Review comments updated* typo bug in qnn test	3
Update README.md	2
[DOC] Improve the order of tutorials within a subsection (#6880)	1
[NNVM/TOPI][OP] gather_nd (#2041)	5
[Doc] Minor improvements for auto-tuning tutorials (#6919)	1
[LLVM] Enable same target option in JITModule (#778)* [LLVM] Enable same target option in JITModule* not set mcpu explicitly	1
Fix HexagonSDK.cmake (#9914)	1
fix empty config caused KeyError (#4520)	0
[PASS] not vectorize if_then_else (#2389)	4
add nvcc support (#7668)	1
[VTA] Runtime refactor to allow for non-shared memory FPGAs (e.g. F1) (#3554)* updated runtime to support non-shared memory FPGAs for instruction and micro-op kernels* adding driver-defined memcpy function to handle F1 cases* refactor to include flush/invalidate in memcpy driver function* update tsim driver* bug fixes* cleanup* pre-allocate fpga readable buffers to improve perf* fix* remove instruction stream address rewrite pass for micro op kernels* fix:* white spaces* fix lint* avoid signed/unsigned compilation warning* avoid signed/unsigned compilation warning* fix* fix* addressing comments* whitespace* moving flush/invalidate out of memmove* clearnup* fix* cosmetic* rename API* comment fix	0
Refactor, refactor code structure, fix pynq rpc (#29)	0
Update schedule_dataflow_rewrite.cc (#2934)	5
[BYOC] Switch TensorRT BYOC integration to IRModule-at-a-time using RelayToTIR hook (#11979)* [BYOC] Switch TensorRT BYOC integration to IRModule-at-a-time using RelayToTIR hookThis does for the TensorRT integration what #11631 did for the CUTLASS integration.- All compilation options are captured within the attributes of a Target of  kind "tensorrt" (instead of the "relay.ext.tensorrt.options" attribute in  PassContext). This means all BYOC configurations options needed by Collage can  be captured uniformly by a list-of-Targets. It also means RPC boundaries (as used  internally at OctoML) only need to worry about maintaining the fidelity of the  Target instance(s) rather than reaching into the PassContext.- Compilation is switched from function-at-a-time (relying on the TECompiler) to  IRModule-at-a-time (using the RelayToTIR target-specific hook mechanism). Though  not strictly necessary for Collage I want to check the path is now clear to  deprecate the support for BYOC in TEComplier.- Get all the TensorRT tests going again, except for a few I've disabled with  x-link to a new issue #11765. CAUTION: The TensorRT runtime is not supported in  CI so many of these tests are cosmetic.- While trying to track down a 'free(): invalid pointer' error in test_tensorrt_int8_exp.py  made the TensorRT allocs/frees more robust, but turns out its also broken in main.  No harm leaving these changes in though.* - Lints* - Woops, fix test* - lints* - Use default tensorrt target if none given in targets list* - fix free error* - accidentally introduced 'transforms' namespace- can't use default Target("tensorrt") arg* - D'oh! Include ended up #if protected* - restore mark for test_dynamic_offload- handle missing runtime in versioning- turn test_maskrcnn_resnet50 back on now that we have the  import-torch-first workaround.* - wibble	1
[BugFix][VTA] Fix vta_conv2d crash issue after change vta_config.json configuration. (#3213)Issue:Once change LOG_BLOCK_IN or LOG_BLOCK_OUT into > 4 value, when run vta“Simple Matrix Multiply” or load vta, vta would crash at vta_conv2d.py.Analysis:This issue caused by resnet18 logic of vta_conv2d.py which havein_filter minmum size that is 16. > 4 value would cause such in_filtercheck failed then make xfer_size be empty and find_schedules functionreturn a empty list finally cause crash.Solution:add the empty list check.	1
Remove CMake string REPEAT (#9771)According to https://cmake.org/cmake/help/latest/command/string.html#repeat this was added in 3.15, we can revert this if we ever use a minimum cmake newer than that. Until then we should just remove the alternate path to reduce complexity.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[MetaSchedule] Mutator Rule: Mutate Unroll (#10045)* mutate-unroll* mutate-unroll	5
Restart popen pool. (#11074)Retrigger CI.Address issues.Retrigger CI.	0
[PYTHON][FFI] Speed Up get DataType (#9072)	5
[TIR][REFACTOR][API-CHANGE] Change Call.name to Call.op(RelayExpr) (#5863)* [TIR][REFACTOR][API-CHANGE] Change Call.name(string) to Call.op(tvm::Op/RelayExpr)This PR brings a major refactor to the tir::Call structure.The current Call structure uses a string field(name) to identify thefunction/intrinsic being called. This approach is limited as we startto expand TIR to be more structured. In particular, we are interested inthe following aspects:- Type a function and perform better compile time type checking so that we  can find errors early.- Register additional properties about an operator, such as:  - Whether an intrinsic can be vectorized  - What is the adjoint function of the intrinsic(for tensor expression AD)  - Whether the operator has side effect.- Perform specific codegen about an intrinsic if necessary.- Call into another function in the same module.The refactor changes the Call.name field to Call.op.The Call.op field has a RelayExpr type, and we can pass:- A tvm::Op which represents the corresponding intrinsic.- A tvm::GlobalVar for calling into another function in the IRModule.All the current intrinsics are migrated by registering an tvm::Op.Because the unified IR shares a single Op registry. We use the "tir"namespace for tir related intrinsics, for example bitwise and is now registeredunder `tir.bitwise_and`.To simplify upgrade, we introduce a `tir.call_extern` intrinsicthat allows us to call into arbitary external function without type checking.However, we should move towards more type checked variants in the system.Under the new op design. We should no longer try to pattern match all thespecific intrincis. Instead, we should rely on attr of each Op to do transformation.For example, the vectorization pass depends on the TVectorizable property of the op,which can be registered independently.In this way, we can still grow the number of intrinsics when necessarywithout having to change all the passes.The same rule applies for tensor expression AD. Currently we are performingAD by pattern match on operators like exp, sin, cos. We should insteadchange to the ajoint registeration mechanism like those in relay.Followup refactors need to be performed, including:- Fold the Call.call_type into operator's attribute.- Enrich the operator registry information- Refactor passes(e.g. AD, intrin lowering) to use the attribute based transformation* Fix nms* Fix remaining testcase* Address review comment	1
[TEST] Fix java compilation (#279)	0
[Golang] bugfix #2517 (#2558)	0
Update function.py	1
[µTVM] Print .elf statistics for a model runtime built with Zephyr (#7449)* [µTVM] Print .elf statistics for a model runtime built with ZephyrCurrently there isn't any statistics about the used resources by a modelruntime built with Zephyr, making it difficult to have any idea about, forinstance, the amount of memory taken by the operations necessary to run themodel.Since Zephyr's SDK already exposes the statistics about various memoryregions on linking by passing '--print-memory-usage' to the linker, it'spossible to use it to have an idea about the amount of memory used by themodel and how much memory is left on the device.That commit adds a simple method to extract the memory region informationout of the build output and then uses it to show memory usage statisticsfor various memory regions when Zephyr finishes building the image to beflashed to the target device.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* v2: Fixes accordingly to Andrew review- Catch StopIteration in case of a weird output or no additional lines  after the last memory region- Use of _LOG.info() instead of plain print() for better control over  the output by the main script- Set log level in micro_tflite.py script as an example on how to get  the new memory usage statistics and also because currently that's the  main script used to test microTVM + Zephyr's SDK- Improve statistics headerSigned-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix buildIt seems build system is using Python < 3.7, so 'text' argumentis not present as an alias for 'universal_newlines'. To satisfyit use old 'universal_newlines' argument which is available priorto Python 3.7.* Fix buildAvoid exception anti-pattern when catching StopIteration* Retrigger CI	0
[CPP-RPC] Fix command line argument capture (#11801)There are a couple of instances where command line options use a "-",however when capturing these values a "_" is used, meaning theydon't get captured and the default value is used instead. Fixingthis by renaming instances of "_" -> "-".Change-Id: I9e083e25c5cc273298cd15df85a5862ee5f6722c	4
[RUNTIME][OBJECT] Introduce static slots for common objects. (#5423)The _type_child_slots can be used to enable quick type checking optimizationby checking the whether the type index is within the bound.This PR enables these static slots:- Introduce a static assert to avoid the scenario when a developer forget to  _type_child_slots when the field is set for the type's parent.- Revamp and assign static type index to common runtime objects- Add a DumpTypeTable call to allow developer monitor the current situation  of type table and offers suggestions for the slots(ideally the slots equals  the number of children so there is no overflow.	1
- Adding support for Mxnet flavored dequantization for both default and using MKLDNN. User can choose between the two at runtime. (#3945)- Added tests for new methods added.	1
Added Dockerfile demonstrating OpenCL & OpenGL installation (#1770)	2
[REFACTOR][IR] attrs.h -> ir (#4709)This PR moves attrs.h into the ir folder as itcan serve as a common infra for building ir dats structures.We also moved common container(FloatImm) into ir/expr.h	4
temp checkin of schedule ops	5
BUG: Look through on_device annotations when looking for shape constants (#9345)https://github.com/apache/tvm/pull/8788 introduced a perf regressionsince a `shape.as<ConstantNode>` in `alloc_tensor` was always failingdue to the extra `on_device` annotation on the constant. Fixed that,and introduced some helpers to make this situation easier to deal with.(This is CORE-102 in OctoML JIRA).(Second try -- test_crp.py failure seems unrelated)	0
fix prelu importer and add tests: (#5521)	3
[EXECUTOR] Improve LayoutTransform pass (#273)* [EXECUTOR] Improve LayoutTransform pass* Remove offline params for now* Small fix	0
Fix json parsing behavior (#83)	5
[QUANTIZE] Memorizing the quantize node mapping (#3233)* [QUANTIZE] Support for clip operator* [QUANTIZE] Memorizing the quantize node mapping.* [QUANTIZE] Remove use_stop_fusion and skip_k_conv in qconfig* update* update* update* update	5
[DLPack] Upgrade dlpack to 0.2 (#609)	5
[crt] fix shift out of type bounds (#7733)* [crt] fix shift out of type bounds	0
[TEST] Remove nnvm related code in topi and test script (#4562)* [TEST] Remove nnvm related code in topi and test script* Remove docs dep	2
[TIR] Add PreOrderVisit and VisitPrimFuncs (#7627)* [TIR] Add PreOrderVisit and VisitPrimFuncs* Update stmt_functor.h* address comments* fix lint	0
[6/6] Arm(R) Ethos(TM)-U NPU codegen integration with `tvmc` (#8854)* Add Arm(R) Ethos(TM)-U codegen support on tvmc* Include `ethos-u` as a new target for tvmc* Adds testing for the new targetCo-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>* Add Arm(R) Ethos(TM)-U codegen support on tvmc* move partition_for_ethosu from tvm.relay.backend.contrib.ethosu  to tvm.relay.op.contrib.ethosu* lazy load ethos-u-vela dependencies and show an appropriate error  message in case the dependency is not present* Adjust test casesCo-authored-by: Leandro Nunes <Leandro.Nunes@arm.com>* Add Arm(R) Ethos(TM)-U codegen support on tvmc* add missing importChange-Id: Ieefa0ee6e86bdc09ff93fcc632ed003b5f3f3a99Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>	1
[SCHEDULE][RUNIME] Introduce pragma for additional extension hint, threadpool runtime. (#299)	1
[Relay] Fix typo in parser (#3785)	2
expose SaveToFile symbol on windows (#1685)	2
[COMMUNITY] @MarisaKirisame -> committer (#4645)	3
declare type name for optional<TShape> (#429)* declare type for optional tshape* add doc* move code to another place	4
Add TShape and Tuple to nngraph	1
Rename tvm_dso_op to libtvm_dso_op (#5714)	5
Add create_local_debug_runtime to micro exports (#7528)* Add create_local_debug_runtime to micro exports.* retrigger CI	0
Fix RPC (#1542)	0
[RUNTIME] Add cudnn conv3d (#4418)* [RUNTIME] Add cudnn conv3d* add output checking to test_cudnn.verify()* fix tests failure* revised per as review comments* unify conv_output_shape, conv_find_algo and conv_forward* convert python list to tvm.array in conv_forward* revise per as comments* 'pass as reference' for vector args* add back con2d/3d seperated implementation* remove unused included header* remove extra std::vectors* remove unused header	1
[Relay][TF] Support for Atan/Atan2 in Relay Tensorflow frontend converter. (#5104)* add Atan/Atan2 op* fix bug and testing	3
[microNPU] Add support for pack and unpack (#9960)* [microNPU] Add support for pack and unpackPack is represented by a series of `expand_dims` operationsfollowed by a `concatenate` in Relay. Unpack is representedby a `split` followed by a series of `squeeze` operations inRelay. This commit legalizes `expand_dims` and `squeeze` toreshape operations while making use of existing legalizationtechniques for `split` and `concatenate` so that pack andunpack can be offloaded to the NPU.Change-Id: I3fbebb4ece5ca04598f8e587b9e6c0ddf280266d* rebase and add tests for expand dims and squeezeChange-Id: Ic6a9fd77b61368720328bfe82032490bcc66152c	4
[TFLite] Fix detection of crop in convert_batch_to_space_nd (#6670)	0
[USMP] Add performance characteristics to PoolInfo (#10005)* [USMP] Add performance characteristics to PoolInfoScheduling algorithms that wish to optimize aroundmemory pools require further information about theperfomance characteristics of those pools. Thiscommit adds clock frequency, bandwidth, latency andburst length as optional fields to PoolInfo.Change-Id: I4cf3f35324d093fb38e874f0f2e587cb84d4ba1e* Remove unused importChange-Id: I1e2ef885425f4361b80c2bab9261ec129e61a756	4
[ETHOSN] Fix some more pylint issues (#12675)Fixing a few more pylint issues caught when using pylint==2.9.3.Change-Id: Ie7ca61e1a8083a40e0ffccf1418192966884707a	4
Expose clip to frontend mxnet (#512)	5
[Bug fix] Fix in arm_cpu/conv2d_alter_op for NHWC quantized (#6027)* Bug fix] Fix in arm_cpu/conv2d_alter_op for NHWC quantizedFew minor typos to be fixed in topi/arm_cpu/conv2d_alter_op.py for theNHWC quantized route:- Kernel shape was misread (CO, IC, KH, KW) -> (KH, KW, IC, OC)- Pad along the K dimension was misspelled: pad_k -> pad_K- Workload name was wrong: "conv2d_NHWC_int8_without_tranform.arm_cpu"  -> "conv2d_NHWC_quantized_without_transform.arm_cpu"This submission fixes those errors and add a further test for conv2d_alter_op.pyChange-Id: I0622df05f1d4d15311946f6e75f1840a34815a5b* Move -target to -mtripleChange-Id: Ieff80c774e8ab0fa7f48d83d50a79f3a62e8fe13* Retrigger testsChange-Id: I5541bed54eacc5063bf4a4fda725209cc23f621e	3
[ROOFLINE] Calculate roofline from existing TIR PrimFunc (#11238)Refactor roofline_analysis to use a pass instrument to save TIR codefrom compilation for feature extraction. This should support differentcompilation pipelines and avoids recompiling the module twice.	1
[Hexagon] Improved ergonomics of HexagonLauncher in unit tests. (#10581)* [Hexagon] Improved ergonomics of HexagonLauncher in unit tests.The goal of this commit is to reduce/eliminate common code requiredthrough unit tests that interact with Hexagon hardware.- New testing fixtures in `tests/python/contrib/test_hexagon`.  A test  running on hexagon hardware should only need to use the  `hexagon_session` fixture.  - `rpc_server_port`: Iterates through port numbers, selecting an    unused port for each unit test.  Avoids needing to explicitly    specify unique ports for each unit test.  - `tvm_tracker`: Starts a tracker on use, exits after test.  Avoids    needing to manually start a tracker prior to running the unit    test.  - `hexagon_launcher`: Starts a `HexagonLauncher` server on use,    stops server after test.  Avoids needing to call `start_server()`    and `stop_server()` in each test.  - `hexagon_session`: Starts a hexagon session using    `hexagon_laucnehr.start_session()`, exits after test.- Added `Session.upload` function, which delegates to  `HexagonLauncher.upload`.  Avoids needing to interact with both the  launcher and the session.- Allowed `tvm.IRModule` as argument passed to `Session.load_module`,  which will automatically save/upload the module, then load it.  Avoids needing to handle save/upload of temporary files in each unit  test.* Added default port for tracker if not already set.* Pass through None from hexagon_launcher to hexagon_session.* Updated launcher to use external tracker if specified.* Avoid setting up the local tracker unless required.* Declare previous_port as global, instead of list.* Corrected type hints.* Docstring updates	5
[TIR] Add conversion from FloatImm to float in Python (#9009)This method matches the IntImm method for converting from IntImm to int.	1
Use ==/!= to compare str, bytes, and int literals (#4686)Identity is not the same thing as equality in Python so use ==/!= to compare str, bytes, and int literals. In Python >= 3.8, these instances will raise __SyntaxWarnings__ so it is best to fix them now. https://docs.python.org/3.8/whatsnew/3.8.html#porting-to-python-3-8% __python__```>>> dtype = "float">>> dtype += "16">>> dtype == "float16"True>>> dtype is "float16"False>>> 0 == 0.0True>>> 0 is 0.0False```	2
[Hexagon] Add optimized schedule for nn.pad (#12714)Motivation:In case of quantized models nn.pad operation typically is not fused with QNN opsand lives as a standalone operation. In this case it uses default injectiveschedule for Hexagon target and it is not optimized very well (based onanalysis of real models like ResNet50 INT8).What was done:New schedule for Pad operation was implemented instead of default injective schedule.For Hexagon target injective schedule does fusion of all axis and vectorizationon 128/64/32 (depends on dtype). It works fine for Add, Sub, etc... but not for Pad.New optimized schedule does these steps (fusion+vectorization) only if last tensordimension is divisible by 128/64/32 (depends on dtype). It was done only for Hexagon,for other targets (x86, cuda, etc.) there is no changes and it uses default injectiveschedule.Benchmark results on Snapdragon 888:4d NHWC layout with ((0, 0), (1, 1), (1, 1), (0, 0)) padding, "uint8" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 112, 112, 32)  |         10,03        |           0.2          |    50.1x times    |(1, 56, 56, 128)   |         0,099        |          0,085         |  ~1x (no speedup) |---------------------------------------------------------------------------------------|4d NCHW layout with ((0, 0), (0, 0), (1, 1), (1, 1)) padding, "uint8" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 128, 56, 56)   |         10.96        |          1.38          |    7.9x times     |(1, 32, 126, 126)  |          1.66        |          1.58          |  ~1x (no speedup) |(1, 32, 128, 128)  |         13.98        |          2.66          |    5.25x times    |---------------------------------------------------------------------------------------|5d NCHWc layout with ((0, 0), (0, 0), (1, 1), (1, 1), (0, 0)) padding, "uint8" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 4, 56, 56, 32) |          6.39        |          0.29          |     22x times     |(1, 56, 56, 128)   |          0.15        |          0.15          |  ~1x (no speedup) |---------------------------------------------------------------------------------------|Summary:For some input tensors we get up to 50x times speedup, for other performance is the same.No performance degradations were detected.	5
[Relay][Frontend] SparseTensorDenseMatMul support for Tensorflow (#6685)* [Relay][Frontend] SparseTensorDenseMatMul support for Tensorflow* Lint error resolved* [1] Review comments handled* [2] Review comments handled	0
[DOCSTRING]missing function parameters updated (#5228)	5
Fix the issue #1036 (#1040)	0
Add flag to build static version of TVM runtime (#8059)* Add flag to build static version of TVM runtimeSetting BUILD_STATIC_RUNTIME to ON (default: OFF) will cause astatic libtvm_runtime.a to be built.This library will then need to be linked into other projects with--whole-archive linker option, or otherwise the linker may removefunctions that are not used at the time of linking, such as functionsregistered (in the TVM registry) via global constructors.* Add BUILD_STATIC_RUNTIME with a description to cmake/config.cmake* Explain the issues with posix_memalign on Hexagon* Empty commit to restart build	0
[TIR] Allow memory (aka storage) scopes to be retrieved/applied to PrimFuncs (#9689)* [TIR] Allow memory (aka storage) scopes to be retrieved/applied to PrimFuncsThis is in support of #9613 which allows memory scopes to flowout of already-lowered PrimFuncs into the rest of the Relayprogram. This means scope choices made during lowering canbe accounted for in the rest of the program, with device_copiesinserted as required.Somewhat more speculatively we also allow memory scopes to flowin to PrimFuncs. This is in preparation for when we can splitlowering into two phases: i) lower "primitive" fused Relayfunctions to TensorIR in a schedulable form roughly isomorphicto TE, and ii) actual scheduling down to traditional TIR. Oncethat split is made it will be possible to flow memory scopesout of one PrimFunc and into another so as to avoid unnecessarydevice_copies being necessary due to independently chosenmemory scopes.I also suspect we'll want to put our focus on layouts ratherthan memory scopes, but this at least sets up some of themachinery.* [checkpoint] Junru's comments.	1
[APP] Fix misprint in demo.cc during initializing of picture tensor data (#6566)	5
[BUGFIX] Define kTargetPoolReadWriteAccess globally (#10262)* Fix bug* Fix whitespace* lint* Move the other consts out of PoolInfo	5
[FIX][VM] Fix relay vm optimize (#6322)* [FIX][VM] Fix relay vm optimize* retrigger ci	0
[Hexagon] Add support to run on multiple devices (#12504)* working in parralel using worker* creating launchers per test and clean up* clean up* ci change to distrube tests* ci work with any number of devices* fix running on simulator* adding function docstring* fix android_serial_number to always return a list of string* lint issue* fix internal error when skipping tests while androideserial number is not set* lint issue	0
fix first-order AD tuple/projection expr duplication (#8318)	0
[Hexagon] Softmax slice op initial version (#11559)Resolve merge conflict in utils.py	5
[AutoScheduler] Propogate global autotvm state to PopenPool workers (#8913)* [AutoScheduler] Propogate global autotvm state to PopenPool workers* Fix* lint	0
Install gdb by default, sort packages (#10913)gdb is helpful to debug segfaults in CI problems and for local development. It's cheap, so I propose we just include it as standard.	0
[Frontend][MXNet] Fix mxnet converter for hybridblock and add div_sqrt_dim (#3701)* Fix mxnet converter for hybrid block* tweak* fix rebase* fix* add test	3
[Hexagon] Pass SDK information to launcher build for Android (#9902)	5
[BugFix] Fix NeedsMultiLevelTiling by skipping trivial block iterators (#10804)This PR fixes a bug of `NeedsMultiLevelTiling`, which didn't consider the effect of trivial block iterators (iterators whose domains are `[0, 1)`). Such iterators impacts the following analysis by overlargely counting the number of iterators that are not used to index the block read regions, and might lead to the application of multi-level tiling where the rule is supposed not to apply.To fix the problem, we simply skip such trivial block iterators.	0
[UX][TIR][Schedule] enhance function annotation for tir primitive (#12147)* [UX][TIR][Schedule] enhance function annotation for tir primitive* lint* fix mypy* fix pylint	0
[Rust] Add first stage of updating and rewriting Rust bindings. (#5526)* Add tvm-sys* Use as_mut_ptr* Address CR feedback* Update rust/tvm-sys/src/datatype.rsCo-authored-by: Nick Hynes <nhynes@berkeley.edu>* Final CR comments* Fix find and replace error in frontendCo-authored-by: Nick Hynes <nhynes@berkeley.edu>	0
[CodeGen] Disable -mfloat-abi hard option for LLVM < 6.0 (#4071)The -mfloat-abi hard option does not work for LLVM < 6.0 as it is ignored.This adds a fatal error when using unsupported LLVM versions so that the failureis not silent.	0
[DOC] Scan tutorial (#98)* [DOC] Scan tutorial* Fix according to ziheng's comment	0
Add safe up/downcasting to the Rust object system (#6384)* Revamp the rust object system with safe subtyping* Small nits	5
[build][hexagon] fix several compiler warnings (#11245)	2
[Frontend] Asymmetric padding of convolution support (#4803)	1
Fix ICHECK_NOTNULL in logging.g (#7193)	2
Fix the values for test_fmod since it fails way too often otherwise (#5723)	0
[Relay] Extract intermediate node by its expression ID (#12646)[Relay] Extract Intermediate Expr by relay expr ID for analysismodify doc commentsCo-authored-by: Bin Li <binli1@amd.com>	2
[Hexagon] Register strategy for concatenate (#11562)* [Hexagon] Register strategy for concatenate* Restart CI	5
[TIR] Add DeclBuffer IR node and functors (#12300)* [TIR] Add DeclBuffer node* [TIR] Add IR functors for DeclBuffer* [TVMScript] Add printer and parser for DeclBuffer* Update printer* Update printer* Add test case* lint* fix	0
[Relay][TOPI] Fix compute and schedule bugs for conv2d_winograd_nhwc on mali device. (#8091)1. add argument `auto_scheduler_rewritten_layout=""` in conv2d_winograd_nhwc_mali;2. add `need_auto_scheduler_layout=True` for conv2d_strategy_mali andconv2d_winograd_without_weight_transfrom_strategy_mali.Signed-off-by: haizhu.shao <haizhu.shao@gmail.com>	1
[AOT] Support LLVM backend with C++ runtime (#10753)* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm* add metadata serialization support to llvm codegen* Organize MetadataQueuer into a separate file.* Add DiscoverArraysVisitor to metadata_utils* Fill DLTensor metadata in LegalizePackedCalls.* Improve error message from Call asserts* Pass non-String device_context down to codegen. * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.* test fixes* Also fill preflattened_buffer_map (TODO, maybe don't do this)* Fix C codegen.* Set USMP elem_offset to 0.* Clarify calculation of byte_offset from elem_offset.* fix tests* Fix arm compile warning* Fix hexagon test. * previously I believe we required interface_api == "c", but   this really means to generate C API bindings, and we are generating   "packed" bindings. * I think "c" was chosen here because the distinction between   interface-api and use-unpacked-api is confusing. "c" interface-api   means to generate an entrypoint API for microcontrollers that   accepts bare data buffers. "packed" interface-api means to generate   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same   determination for the operator functions. * A further confusion here is that there are two ways to call   "packed" operator functions: tir.tvm_builtin_call_packed and   tir.tvm_builtin_call_cpacked. This distinction describes whether or   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT   only ever requires call_cpacked because target_host == target, and   for all suitable target_host, we expect a single DSO-exportable   runtime.Module. When we move away from this by introducing   heterogeneous target support to AOT, we can use this as a condition   to help us choose between call_cpacked and call_packed (and   possibly add a compile-time option to assert it is call_cpacked,   for situations where we really don't want call_packed).* Document T.preflattened_buffer* Fix test_aot_legalize_packed_calls* Address manupa comments* Fix convert_pool_allocations_to_offsets test.* lint* Fix T.preflattened_buffer* Add preflattened_buffer_map to TIRTextPrinter* Fix tests* Fix BYOC* Fix invoking C device API.* remove comments* Address Mousius comments* lint* lint* Fix GMock linking on new CMake* address masahi commentCo-authored-by: Masahiro Masuda <masahi129@gmail.com>	1
Improve the frontend tflite _test_abs test to support tflite 2.6 (#9783)Updated the test input node name	3
Fixed a typo (#3218)* Fixed a typo* Remove outdated url link.	2
[AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space (#7175)* [AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space* Update python/tvm/autotvm/task/space.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: ZhaoYanjie <roger.zhao@montage-tech.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
support double buffer to use in ir builder DSL(#1897) (#1898)	1
tensor_array split test (#4619)	3
[AUTOTVM][DOCS] Add a link to the defining network description of auto-tuning tutorial (#4023)* [AUTOTVM][DOCS] Add a link to autoTVM tutorial to direct the details of building NN with relay* [AUTOTVM][DOCS] Add a link to autoTVM tutorial to direct the details of building NN with relay	2
Fix PRC typo (#2939)	2
[HotFix] Skip the flaky MetaSchedule Auto-Unroll test (#9956)	3
fix grad for zeros and ones (#7357)	0
RelayViz interface and terminal ast-dump (#10085)* RelayViz interface and terminal ast-dump.This PR follows https://github.com/apache/tvm/pull/8668, with splittingout interfaces class and terminal ast-dump implementation.This visualizer is aimed for quick look-then-fix, so the interface issimple. Despite that, customization is still possbile throughimplementing interfaces defined in `interface.py` or overriding existentimplementations inside a renderer module, like `terminal.py`.A tutorial is also provided in this PR.A graphviz renderer will also be contributed after this PR.* lint and typo	2
[skip ci][ci][paddle] Disable flaky test_forward_group_norm (#11436)See #11435Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[PASS] Improve graph fusion (#286)* [PASS] Improve graph fusion* Change fusion center to segment head* Use 'master' to identity the schedule node* Make things compact* Fix	0
Fix metal backward compatibility (#1105)	0
[CI] add GH workflow to comment with link to docs (#11594)	2
Register Shape Func for Some Operators to Handle Dynamic Shapes (#5955)* Register Shape Func for Floor OperatorRegister the shape function for `floor` operator. Otherwise, a bug will happen when input of floor is any.* Register shape func for log* add shape function for crop_and_size* change import location* add mirror_pad shape function* add test cases for crop_and_resize and mirror_pad shape funcs* support different layout* fix pylint error* fix pylint error* add test for nchw layout* block nchw test* test for nchw* use tvm.testing.assert_allclose insteadCo-authored-by: lisiyuan <lisiyuan@nucflow>	3
[Frontend][MXNet] Support a few contrib ops in mxnet (#5819)* support for bert in mxnet1.6 and gluonnlp0.9* fix converter* Add test cases* add a todo	2
use SetAffinity when logical cores > physical cores (hyperthreading) (#1453)	2
[C++ RPC] Fix C++ RPC build problem on Linux (#5671)	0
[QNN][Relay] Fixed bug in quantized conv2d. (#6420)* Fixed bug in quantized conv2d where when kernel size = (1,1)  and strides != (1,1) it would raise size mismatch error.* Added test to check qnn.conv2d with kernel size = (1,1) and  strides != (1,1).	3
[BYOC] [ACL] include_non_call_ops = False (#7121)ACL codegen now uses AnnotateTarget pass with include_non_call_ops = Falseto prevent promoting non-call ops under the target of its arguments.Squeezenet unit test added.	1
Include required CMSIS headers in Cortex-M micro kernel. (#6988)* The existing kernels referenced CMSIS functions presuming that   those functions were defined by user code. This was the case with   the old blog post build flow. Add #include, since it's impossible   to compile the kernels without it. * TODO: port those functions to the micro kernels and remove external dependency	4
[Relay] Add RecoverVirtualDeviceMap helper (#12085)* [Relay] Add RecoverVirtualDeviceMap helperDevice planning is halfway through the transition to using the virtual_device_field on every expression node to capture device/target/etc info. In the meantimeit is necessary to derive from a 'device aware' visitor so as to track deviceinformation. In Collage this is not feasible, so as a stop gap allow the mapfrom expression nodes to virtual devices to be reconstructed as a stand alonemap.This code can be removed once expr->virtual_device() is the canonical representation.* - review comments	4
[LLVM] Protect ll when emit pass (#436)	4
[TIR] Add TIR While node (#7425)* add while node* update visitors* binary search lowering works* llvm codegen working* cuda codegen working* nms updated to use while loop* add missing upper bound check too* add mandelbrot test* add gpu mandelcommit ee2363bf8131830cf0fb112890befd6be6a03f36Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Jan 29 11:44:02 2021 +0900    enable extern lib offload for nvptx* rename test* run black* add doc* add collatz test* add while + vectorize test* simplify bin search* Add special case visit method to storage_access.cc* disallow while loop inside vectorized loop* disallow trivial condition since we do not have break* error out in CoprocSync for now* error out LiftAttrScope for now* add placeholder to inject_vpthread* refactor to use MakeAttach* handle WhileNode in InplaceOpVerifier* error out in InjectVirtualThread* try handle WhileNode in StoragePlanRewriter* remove WhileNode visitor from storage rewrite* add while loop storage rewrite test* update tests* move test_vectorize_while_fail to  test_tir_transform_vectorize.py	3
[Relay] A set of utilities that allows a model to be run efficiently on tensorcores. (#6748)	1
[BUILD][DOCS] Migrate VTA CI, test, build, docs	2
[API] Expose AutoInlineInjective (#368)	5
[uTVM][Runtime] Introduce Virtual Memory Allocator to CRT (#5124)* initial crt_memory and memory leak fix in graph_runtimeChange-Id: I0f79f909a04d1c677aabb80f202f0612c5ce7f2a* fix memory leakChange-Id: I37104c09e28112b1974fa2b064c809d0a8d686c3* clean upChange-Id: I039b12015a1d56c8f4120867cd5a5292da34f3e3* implement vreallocChange-Id: I35800470bcbfcf96652494f359711cb4c2d34398* allocate from stack memory for most of the variablesChange-Id: I72071289843fff4031c0df8796868a0b9fbc57ee* allocate from stack memory for all of the variablesChange-Id: I32dba85ac1660c77f51c2d0d8ab6436ed0c01c74* lintChange-Id: If12cd240685d7791fc60bc0cfb66389cdc186b73* lintChange-Id: I7c9d90c11b60b8edda2427ebd189ebe535af2100* facilitate the growth of TVM_CRT_MAX_NDIMChange-Id: I939fa43027a5c7529c5c7c6bd8d6e6beb91b7581* extend test coverage of vmallocChange-Id: Ie4ff6b64fdfe6810836cf8fd44dace82a20c4581* lintChange-Id: Ibf3c06619ef296df5c49f3945cb6428777781d69* move logging.h to src* fix an error in macOS* remove logging.h* use cflags for gcc* fix compilation error	0
[RELAY][OP] Operators.   pool2d, global_pool2d, batch_flatten, tanh, sigmoid, floor, ceil, trunc, abs, negative, multiply, mod, pow,  resize (#1813)	1
[LLVM] Rename t_tvm_context_ to t_tvm_device_, NFC (#9176)Follow the change from DLContext to DLDevice.	4
[BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm	1
Support LLVM trunk (#3907)* support LLVM trunk* guard with USE_LLVM in if condition for c++14* GREATER_EQUAL -> GREATER	1
[CI] Update GPU image for PyTorch 1.11 (#10849)* Update Jenkinsfile to point to the new GPU image* fix onnx test* fixed keras tutorial	0
[TensorFlow] Fix limitation that depth_mult can only be 1 for DepthwiseConv2dNative (#3676)* [TensorFlow] Fix limitation that depth_mult can only be 1 for DepthwiseConv2dNative* Improve code readability	1
Fix a tensorflow test bug. (#3165)Length of input_shape isn't always 4.	0
[QNN] Fix order of operations in qnn.quantize slightly to prevent undefined behavior (#9558)* switch order of quantizations* add back in round	1
[ci] Add -x to all CI scriptsFixes #10316cc @Mousius @areusch	0
Add labels to each Jenkins step (#9556)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Buffer logger assert removed (#6147)	4
[Hexagon] Treat floats as float32 when passing args to offloaded kernels (#9010)`TVMArg` can hold a floating point value, but it's stored as `double`. InHexagon ABI doubles are passed in a register pair, but if the offloadedfunction was using floats (i.e. float32), it will expect values beingpassed in single registers. Since floats are much more common on Hexagon,assume all scalar floating point values are floats. This is only an issuewith offloading, and can be treated as a limitation (we do somethinganalogous for integers already).	2
[TOPI] Add broadcast and reduce operators (#267)[TOPI] Add broadcast and reduce operators	1
[ci] Use available CPUs in builds (#10359)* [ci] Use sccache in builds* trigger ci* updateCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[ETHOSN] int8 support for the mean operator (#10463)- Updated the test; no other changes necessary- Parameterized the tests for easier failure analysis	0
[Bugfix][TVMScript] Convert BufferSlice to BufferLoad when used as range/loop start and end (#10370)A quick fix of the parser issue mentioned in #10327 .Ranges and loops require `start` and `stop` to be PrimExpr, however, `BufferSlice` is not always scalar so it's not a `PrimExpr`.This PR performs the transformation.	1
Fix caffe2 relay frontend (#2733)	0
[MetaSchedule][Test] Add unittests for C3D (#12046)	3
[skip-ci][COMMUNITY] leandron to PMC (#10448)	3
[TVM] [NNPACK] Modernize and improve NNPACK bindings (#2084)	1
Install xgboost>=1.1.0 in CI container (#6679)	2
Check in Tensor API on python	5
Fix misprint (#2243)	0
[Doc] Update release document (#6573)	2
[VTA] de10-nano driver (#3394)* rework;* `de10-nano` -> `de10nano`;* fix compilation error;* bug fix;* Update install.md* Update install.md* Update install.md* update with current runtime;* add debug messages;* bug fix in cma kernel module;	0
SGXify graph runtime (#937)	1
[ci][docker] Use RFC image tags only (#11938)This ignores image names like `123-123-abc-validated`Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Add mod supoort in relay.build (#3424)	1
Fix bug in ONNX importer (#3084)	2
[microNPU] Refactor codegen tests (#9623)* [microNPU] Refactor codegen testsChange-Id: I9c08520c9e03eb3fc32bd911b56c95981e851b4b* Fix paramsChange-Id: I8cea69ed3824c3a0417bb67abbabce460c17c4c6* Remove printsChange-Id: Iadf048e9590e724d73c2adac51bbe303de6f59a8* Address review commentsChange-Id: I56d647d86e3d495abe38b13cca349a71ec81cf4d	4
[RELAY][TOPI] `alter_op_layout` for x86 (#2602)* alter_op_layout for x86* cleanup* cleanup* fix lint* fix lint* fix lint* fix lint* change support level* change other support levels	1
[DOC]Clear javadoc directory everytime (#1917)	2
[TIR] Add software pipelining (#10066)* [TIR] Add software pipeliningCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* fix* fix* lint* fix* format* doc* remove print* lint* lint* doc* Apply suggestions from code reviewCo-authored-by: Junru Shao <junrushao1994@gmail.com>* address comments* address comments* refactor FragmentInfo::GetSize* remove unused* refactor* address commentsCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	1
make simplify inference iterative (#8246)	5
added support for rocm gpu autodetect (#549)* added support for rocm gpu autodetect* changed type casting from old style to static_cast* fixed code to generate gfx specific code object* fixed namespaces	0
[Relay] Make DeviceAnalyzer a mixed mode visitor (#10248)* hack to ExpandDataflow* add test from mei* Update DeviceAnalyzer to inherit from MixedModeVisitor* indent	5
[OpenGL] Let OpenGL texture always be 1024 x nrows. (#817)* OpenGL texture is always 1024 x nrows.* Address review comments.	1
Workaround to make conv2d_transpose compilation for CUDA work (#4472)	1
Onnx eyelike (#8191)* add ONNX EyeLike converter* need to implement k* test pass* eyelike tests all pass* Revert "test pass"This reverts commit 0aa7347aaf27493d53492c9f0be305bf8358760b.* removed comments, black'd, lint* changed == to is in onnx.pyCo-authored-by: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: Jocelyn <jocelyn@pop-os.localdomain>	4
[ARITH] Refactor to use explicit div/mod functions instead of operators. (#4000)* [ARITH] Use explicit div/mod functions instead of operators.* fix pooling case	0
[TEAM] siju-samuel -> Reviewer (#1745)	5
[CI] Update ci-gpu to v0.52 (#3374)* [CI] Update ci-gpu to v0.52* update nodejs	5
[TOPI] GPU scatter 1D via sorting based approach (#7056)* add thrust stable sort* rename* scatter via sort working* correctly handles negative indices* clean up, add some comments* add doc string* remove scatter benchmark stuff* add more doc* fix typo* lint fix* silence lint* fix py format* check for thrust availablity before testCo-authored-by: masa <masa@pop-os.localdomain>	3
[SCHEDULE] tensorize (#223)	5
[CI][Docker] set environment variables for UTF-8, to prevent errors when running `black` (#8089)* Sets environment shell encoding to UTF-8 * This prevents the black formatting tool to exit with the following error:   "RuntimeError: Click will abort further execution because Python was    configured to use ASCII as encoding for the environment"	1
Parameterize test_link_params. (#9276)	3
[PYTORCH]AvgPool3d, MaxPool3d and Squeeze op support (#5220)* [PYTORCH]AvgPool3d, MaxPool3d and Squeeze op support* Testcases added* review comments	1
[NEWS] add v0.6 release (#4558)* [NEWS] add v0.6 release* remove link prefix* fix issue number	0
[CODEGEN] ARM Popcount lowering rule and codegen updates (#1235)	5
Nested rfactor fix, update predicates as well as source. (#3382)* Nested rfactor fix, update predicates as well as source.* Linter* Syntax fix.	0
[TVMC] Add configuration json files to the Python package (#11063)Add the `configs` directory to be part of the installed version ofTVM in the setuptools configuration, and introduce a new functionto load the `configs` directory from the right paths both when TVMis locally installed for development, as well as, when it is installedas a package.	5
[DOC] Make range related function consistent (#249)	1
[TIR] Improved error message if tir.Schedule passed to lower/build (#11913)Previously, if a TIR Schedule is passed to `tvm.lower`, the errormessage is returned `ValueError: ('Expected input to be an IRModule,PrimFunc or Schedule, but got, ', <class'tvm.tir.schedule.schedule.Schedule'>)`.  This can cause userconfusion, as the expected class name in the error message does notdifferentiate between between a `tvm.te.Schedule` and a`tvm.tir.Schedule`.  Updated error message to explicitly state thatthis should be a `te.Schedule`.	0
Fix for dynamic batch size conv2d nhwc (#7598)	0
[Relay] parser/pretty printer roundtripping (#3536)	5
[BYOC][FIX] Fix typo in "default" (#5348)Default annotations were incorrectly being named 'defualt'which results in them not being removed in PartitionGraph.	4
[DOCS] Fix Sphinx Warnings (RST indent, cross-ref, and image scale) (#4920)* fix indents* Fix image scale and cross-ref	0
[PASS][RUNTIME] Support attr scope lift and runonce (#303)	1
[TVMScript] StmtDoc Definitions (#12111)This PR addes:- All StmtDoc subclasses- Python bindings for StmtDocTracking issue: https://github.com/apache/tvm/issues/11912	0
[Relay][VM] Fix constant folding issue in VM compiler (#4077)* [Relay][VM] Fix constant folding issue in VM compiler1. allow pass params when compile a module2. enhance profiler robustness* remove dead code* fix lint* add get_params* fix test* don't pass params back* remove get_params* docs* move compile function to api* compile clashes with builtin name* fix compilation error* remove dead code	4
pickle memoize no longer print message (#1111)	5
fix flaky TF test (#8431)	3
[METAL] Fix memory leaks in Metal runtime (#7714)* [METAL] Fix memory leaks in Metal runtime1. In case when we build runtime without ARC, we can have problems with   memory releasing. Due to some of Objective-C methods returns   autoreleased pointers, we should specify `autoreleasepool` blocks to   determine life cycle of these pointers.2. Added workaround for problem with work group size.   Sometimes auto scheduler generates parameters when work group size   is more than possible. And in this case we got assert from Metal   library. Added check for this situation and it helps to avoid   assert.3. Fixed memory leak problem when fill tensor by random data.   DLManagedTensor increases reference counter in NDArray but nobody   delete this DLManagedTensor in proper way. This is why memory which   was allocated by NDArray was never released.4. Removed unnecessary retains. It is not necessary use retain in some   places where they were used, due to we build metal runtime without   ARC.* Use const_cast instead of creation DLManagedTensor	1
Remove an obsolete comment (#2527)	4
[Adreno] Fix winograd tests and accuracy (#12202)* [Adreno] Fix winograd tests and accuracy* Fix lint* Fix test on cpu	3
TVM Vertical Integration with PyTorch (#11911)* optimize_torch & as_torch* split files* code formatting* optimizing optimized_torch* scrap your boilerplate* as_torch polished* configuration fixed* Apply suggestions from code reviewCo-authored-by: Lite Ye <liteye859@gmail.com>* more document* file deleter* optimize deleter* drop how-to guides* clang-format-10* formatter changes* reformat* reformat* reformat* reformatting* fixed* auto setting* fixed* split long string* tune_tir* upgrade as_torch* optimize as_torch* as_torch* fixed typoCo-authored-by: juda <yzhou@octoml.ai>Co-authored-by: Lite Ye <liteye859@gmail.com>	5
[TVMScript] StmtDoc Printing (#12112)This PR addes:- StmtDoc Printing in PythonDocPrinterTracking issue: https://github.com/apache/tvm/issues/11912	0
[FIX] Recover global state after test_util.py (#5824)In test_util.py, a program exit is simulated to testthat the error throwing behaviour is accurate.Unforunately, this also deletes necessary global stateand so all subsequent tests that run and use tempdirthrow the same error.This patch is a simple fix to restore the global stateat the end of the test.Change-Id: I62fef46167e47f6af43271e2ce1db30f54857647	5
[fix] quantize op consistent with python description (#11872)* move round op before `add expanded_output_zero_point`* consistent with python description `(round(input_tensor/output_scale) + output_zero_point`	1
[Relay][Pass] Add a relay pass to extract fake quantized ops (#10089)* add relay pass to collect fake quantized ops* add more tests* more tests* lint* lint* remove unused imports* update comment* lint* reuse SubgraphExtractor and update test assertions* remove print* lint* remove unneeded commentCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
[Relay] Add grads (#3857)* Add gradient implementations* Add docstrings to fix lint errors	0
Update Jenkinsfile	2
[AutoScheduler] Remove `max_registers_per_block` in HardwareParams (#7040)* [AutoScheduler] Fix hardware params* address comments	1
[LIBXSMM] Add libxsmm to tvm ci (#10179)* [LIBXSMM] add libxsmm to TVM CI.* Config "make" thread number in a more flexible way.Co-authored-by: Cody Yu <comaniac0422@gmail.com>* Empty commit to trigger github CI.* Update ubuntu_install_libxsmm.sh.* Trigger CI tasks.* Trigger CI tasks.Co-authored-by: wenxizhu <wenxizhu@tencent.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	5
[FIX] Add task_ci_python_setup.sh to the arm CI (#6850)	1
[PASS] FoldScaleAxis (#55)* [PASS] FoldScaleAxis* Move FoldAxis to O3* Set unroll to 0 when ready	1
[microNPU] Support binary elementwise with non-4D inputs (#9521)Reshapes non-4D inputs to become 4D, then reshapes the output back tothe non-4D input shape.	1
[FRONTEND][TENSORFLOW] bug fix for tensorflow official slim models. (#2864)* [FRONTEND][TENSORFLOW] bug fix for tensorflow official slim models.* * review comments	0
[Relay] Fix output dtype for conv2d wgrad when the original one is void (#10459)* [Relay] Fix output dtype for conv2d wgrad when the original one is void* fix cpplint* also add out dtype information to dgrad* also use out_dtype for wgrad* remove redundant import	2
[Relay][Op][Bug] Fix missing return in scatter_nd cuda strategy (#7447)* fix missing return in scatter_nd cuda strategy* add Relay test for scatter_nd, fix documentation	2
fix upsampling fuse pattern (#330)	1
[REFACTOR][ARITH] Unified IR, introduce arith subfolder. (#4722)Spread the arithmetic.h into several components and moveinto arith subfolder.The arith namespace will be used for arithmetic expressionpattern detections and simplifications.	1
[TIR][MetaSchedule] Estimate TIR FLOPs (#10782)	5
[Meta Schedule] Fix testing issues for models with more than one inputs (#11298)	0
[DOC, HARDWARE] Hardware developer guide, migrating to use Vivado 2018.2 (#1473)	1
Additional canonicalization added for AddNode (#5846)	1
Remove opengl runtime and cmake (#5712)	1
[RUNTIME] More reliable runtime only detection (#914)* [RUNTIME] More reliable runtime only detection* fix lint	0
Better error message handling for contrib (#946)* Better error message handling for contrib* fix lint* fix testcase* fix test	3
[TOPI,x86] Improve performance on int8 conv2d on x86 (#9966)Appended fused operations in cov2d for int8 were computed in a separateloop from the main conv2d computation:```for i in ... parallel  for j in ...    accumulator = 0    for k in ..      vectorized_multiply_add(accumulator, data, kernel)    out = accumulator  for k in ..    out = out + fused subsequent ops```This patch moves the fused ops one more loop nesting inwards to get```for i in ... parallel  for j in ...    accumulator = 0    for k in ..      vectorized_multiply_add(accumulator, data, kernel)    out = accumulator + fused subsequent ops```On quantized mobilenetv2, this results in approximately a 30% speedup.	1
Fixing a doc nit (#3123)URLs to the authors repo for these tutorials had an extra`https://`, this patch removes that.	4
Add support for Xilinx FPGA board with SDAccel (#1278)	1
[Relay][Op] Fix Reshape Compute (#6396)* Fix Reshape Compute* Fix test* Fix lint* Fix lint* Fix* Fix lint* Fix test* Rebase test	3
[GPU][TOPI] Fix cross thread reduction schedule (#414)	0
[PYTHON] Allow general types (#425)	1
[VTA] improved virtual memory mapping (#4545)* [VTA] improved virtual memory mapping* Update virtual_memory.cc	5
[microNPU] Added checks for out of range shifts (#9707)* [microNPU] Added checks for out of range shifts* Added testcase* Addressed comments	1
[TVMSCRIPT] Using diagnostics for TVM Script (#6797)* [TVMSCRIPT] Using diagnostics for TVM Script* fix lint* More documentation, improve some error messages* Apply suggestions from code reviewCo-authored-by: Leandro Nunes <leandron85@gmail.com>* Add synr to ci setup and setup.py* remove typed_ast dependencyCo-authored-by: Leandro Nunes <leandron85@gmail.com>	4
[LANG] Add reflection routine to construct node (#265)	1
[C++] Require c++14 by default (#5056)	1
fix corner case when relay return empty tuple (#10128)	0
[BUILD] Improve build instruction with llvm. (#422)	1
Fix Canonical Simplifier (#5505)	0
[docs] Getting Started: Introduction and Installation (#7638)* Getting Started: Introduction and InstallationThe first two sections of the "Getting Started with TVM" guide.A high level introduction to TVM, and a slight introductionon installation options.Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>	1
Attempt to prevent concurrent update in Map (#9842)* Attempt to prevent concurrent update in MapCalling Map::Set invalidates exising iterators to protect fromusing already deleted data due to re-hashingChange-Id: Ib6b580758e74c8b77ed560932d87b643bd6c9402* Migrated to using TVM_LOG_DEBUGNow uses TVM_LOG_DEBUGMap state_marker made atomicChange-Id: I090c4b33e6edaa977cccba11f8d1c6ff3fbca430* removed usage of atomicsChange-Id: I7bd930cb52d58ca10fd49a5fe8f5d48b3e955d0a	4
[Frontend][Torch] Fix up graph input handling (#5204)* [Frontend][Torch] Simplify operator input handling* [Frontend][Torch] Allow user supplied input names to override graph inputs* Fix pylint issues* Updates from code review feedback* Fix tutorial to use shape list input* Disable intermittent test failure in topi vision test	3
[Relay]Some backend improvements for PT OD models (#6464)* Some backend improve for PT od models* Fix clang* Fix pylint* Enable gpu tests* Minor fix	0
[Frontend][MxNet] Support bidirectional RNN layer (#3397)* Support bidirectional RNN layer* tweak* tweak	1
Move BuildConfig context stack to C++ (#1025)	5
[M3c][MetaScheduler] Add ScheduleRule class & PostOrderApply space generator. (#9761)* Add ScheduleRule class & PostOrderApply space generator.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix comments & docs.* Fix for mypy.* Retrigger CI.* remove get_hex_addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[ARITH] Tight bound for floormod (#6771)	5
[RUNTIME] Add clear() function in tvm::Map class (#7826)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[µTVM]: Zephyr: Add mps2_an521 board to the CI (#7914)Since Arm reference board mps2_an521 is now added as a test platform totest µTVM with Zephyr and that test platform runs by default emulated,plus Zephyr docker images were updated to use Zephyr v2.5-branch, addthe mps2_an521 board as a platform to be automatically used by the CI.That change will allow testing µTVM on top of Zephyr running on aCortex-m33 MCU. Currently only a x86 VM is used for that kind of test.Hence it will help ensure that there is no regression on Arm-based MCUs.That commit also adds explicitly the parameter --microtvm-platforms=hostto the current x86 test for ease of reading on which test platforms aretriggered in the CI ('host' is the default platform when that parameteris ommited, so nothing changes for tests on the x86 VM).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	3
[QNNParam] Refactor the implmentation of QNNParam (#11011)* The patch is to simplify the implmentation of QNNParam and make it more friendly to Python 2.x.* Empty-Commit* fix error about boolean value of Tensor with more than one value is ambiguous.	0
[ARM][Performance] Improve ARM CPU depthwise convolution performance (#2345)* Add sptialpack schedule for arm cpu depthwise convolution* Supply comments.	1
[EZ][Autoscheduler] Log exceptions in topi lowering (#9615)* fix things* linting* remove format string messing things up* Update python/tvm/auto_scheduler/relay_integration.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* move to tvm errorCo-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[TVMC] use target_host when it is set (#6855)* [TVMC] add cl support in tvmc runner* [TVMC] use target_host when it is set* Cleanup comment and asssert device type in else case* add a test for tvmc compiler* remove unused func	1
[LLVM] Remove `using llvm::BasicBlock`, NFC (#11850)There are a few places in CodeGenLLVM and CodeGenCPU that have thisdirective. There is no other `using` directive for any other LLVMtype anywhere. Remove it for consistency with the rest of the code.	4
[TensorFlow] Fix a bug output index is ignored (#3631)Enhance test to cover this case	3
[Topi] Tensorcore support for Conv3D (#5284)* one weird trick.* Added schedule knob for different workloads.* Initial conv3d tensorcore working.* Added conv3d tensorcore strategy.* Added layout conversion to tensorcore friendly format for conv2d and conv3d.* Add target name check.* Fixed bad names and depthwise check.* Removed duplicated attribute assignment.	4
[torch] Add narrow operator (#7535)	1
Make microtvm_template_projects available in tutorials. (#11164)	1
[Frontend][TensorFlow]TensorFlow Parser Control Flow Enhancement (#5020)* Improve TF control flow major logic* Pass mod into operator convert function* Fix LoopBound* Add more control flow tests* Add two test cases for stridedslice* Fix docstring* Fix lint* Fix import* Fix test assert* Minor fix conv3d* Add more comments* Fix for dilation2d* Change newly added atan* Change newly added unravel	1
[TensorIR] Fix parser autocompletion mode (#7737)Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	0
[METAL] Split kernels and compile them separately (#7980)	5
[Hexagon] Delete offload runtime, move files to right places (#11090)Within src/runtime/hexagon- delete directory android,- move files from hexagon to ., delete hexagon,- merge host/hexagon_module.cc with hexagon_module.cc, delete host.Rename HexagonHostModuleNode to HexagonModuleNode.	4
update dependency (#1495)	5
fix print attr of null node (#11959)	0
[microNPU] Add various options to the cascader  (#10509)* [microNPU] Added options to Cascader* Added option to toggle multi-dimensional striping, it is disabled by  default because it has a very high computational cost. Single  dimension striping shares most of the benefit with greatly reduced  cost.* Added multiple developer/debugging options prefixed with 'dev_'  Also added these options to tvmc.* Added cascader logging, if enabled it will dump information about the  cascader proposals to a 'cascader_log.json' file.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Change-Id: I2ec59ae0bd84b73b2cc4bc56d39e3831b0aeec27* Updated memory_reduction testcasesAlso added enable_striping to plan_generator.hChange-Id: I496b30ed6af6f0730087329cd81a69c5040a5e4dCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>	4
[Doc][AutoTVM] Fix bugs that override n_trials (#4842)	0
[AutoScheduler] Fix typos in feature extraction and cost model (#7280)	4
[skip ci] [CI] Re-generate Jenkinsfile (#12360)Timing of merges resulted in the Jenkinsfile being out of sync	2
Change array to copy on write semnatics	4
Add member object accessors to With<> (#12100)* Add member object accessors to With<>Currently the With<> template constructs an object, but gives no accessto it, so it's only applicable to situations where we rely on the side-effects of creating the object.* Restart CI	1
[CI] Set test dependency on "transformers" package with pytest.importorskip (#12528)`test_meta_schedule_integration_extract_from_bert_base` depends on the `transformers` package, which is not currently installed in our Docker images.When running this test currently, it fails with an ImportError. This patch makes this dependency explicit and will make the test to be skipped when the dependency is not installed.`test_meta_schedule_integration_extract_from_bert_base` is part of the integration tests, which is currently only running on AArch64 and CPU image (both not at the moment with torch installed in the live CI system), so this is another issue to be understood/fixed.	0
[Relay][Training] Add more gradients (#7323)* add more gradients* add documentation	2
[PASS] Prepare storage rewrite for unified buffer (#885)* [PASS] Prepare storage rewrite for unified buffer* more comments	4
[SCHEDULE] Fix code lowering when loop condition depends on outer axis. (#2208)	0
[QNN] Support CallNode inputs in qnn.concatenate (#5360)* [QNN] Support CallNode inputs in qnn.concatenateCurrently, qnn.concatenate assumes that its 1st arg(data) is a TupleNode. This may not necessarily be trueif the input is a CallNode which returns a value oftuple-type. This patch handles the CallNode case byinserting TupleGetItemNodes.* Fix lint* Add testChange-Id: I40b55517b8b1dabbeca89337f80c0c8e62e34981* Use isinstanceChange-Id: I731a231113c5214528373ef52b603a9f05ec502a* isinstance fixChange-Id: Ib3495532f6e4feb5aae3d3096cedd4dc4676cdb4* Use elif/else ifChange-Id: Id8123ea2dd9ce3d8267609de7b5602bb84b084fb* Fix lintChange-Id: Ib6899bb22260575aa3f5d8b51b5d2a0277ee2b10* Lint fixChange-Id: I56cf1930315344e42d956818a6c68e80836ae786* SpacesChange-Id: I3edab192e32bafa9ffdc915315791c63279d85dc	4
[WIP]    Linux/Android native deploy (#980)	5
Generalize pooling to support arbitrary layout (#1103)* generalize pool2d to arbitrary layout* explain more the layout support for pool* allow missing factor size for pooling* explain what factor size is used for* fix typo* name idx -> axis	2
[RUST][RUNTIME] Fix workspace (#5503)* [RUST][RUNTIME] Fix workspace* use ok_or_else instead of ok_or	1
[CI] Update GPU docker (#3709)	2
[RUNTIME] ShapeTuple Container (#8200)* Add ShapeTuple.* Update NDArray.* Documents.* Lint.* Lint.* Lint.* Address comment.* Address comment.* Address comment.* Lint.* Lint.	1
[RUNTIME]crt error handling (#5147)* crt error handling* Review comments fixed	0
[COMMUNITY] @phisiart -> Committer (#2165)	3
[Onnx] Add momentum (#9000)* add momentum* make tests pass for momentum* blacking* lintCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
[TIR] Preserve annotations after lower opaque block (#12572)	5
[Relay] Add support for relay expressions as pad value for static pad (#7860)* add support for expr as inputs to pad* fix improper amount of args* add dynamic padding test* infer type better test* add comments to type relations* fix infer type layouts* proper return shape* proper shape infer type* make the tests pass by setting the conditions* make codegen reflect reality* make ternary operations more pythonic* proper infer layout* fold explicit padding* fix pattern matching in contrib* revert tests for contrib now that pattern matching works* revert import changes* add newline	1
[TOPI] Add support for arbitrary dtypes to CSRMV and CSRMM (#8437)	1
[Relay] Create header file for realize.cc (#11093)* Move class definitions to header file* Trim out unnecessary includes* Run clang-format-10* Remove unnecessary class declarations* Adjust grammar to trigger CI* Change comment phrasing again to trigger CICo-authored-by: Jonathan Sparling <jsparling@westus2-ml-vm-sg01.2xo54b0zdm3epgab0khwgzehke.xx.internal.cloudapp.net>	4
[Torch] Support hard_swish op (#7174)* imp_hardswish* format* fix* hard_swish_inplace test case	3
Add VSCode directories to gitignore (#3095)	1
Add flaky test issue template (#10299)This adds a template so we can report (and label) flaky test issues separately from CI infra problems. This also helps others report flaky tests by pointing them to the relevant documentation.cc @areusch @denise-k @hpanda-naut @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[BYOC][CONTRIB] Vitis-AI codegen integration (#6343)* [BYOC][CONTRIB] VITIS-AI integration* Remove environment related files* Update vitis_ai.rst* Add review changes* Remove new lines and note frame in vitis_ai.rst* use sys.exit* Add condition for vitis_ai runtime exec function* remove unused graph_json* correct indentation* use code python instead of bash* Rename VITISAI.cmake to VitisAI.cmake* use relay.ext.vitis_ai.options.build_dir in comparison* Re-add deleted docker related files* Make use of PyXIR XGraph and RuntimeModule serialization & refactor flow* Fix linter errors* Fix linter errors* Address sphinx warnings* Add infertype to fix Vitis-AI annotation test* Renaming util to utils* Add Vitis-AI flag to config.cmake file* Move vitis-ai config options to compiler sources instead of runtime sources* Fix clang-format errorsCo-authored-by: Anil Martha <anil.martha@xilinx.com>Co-authored-by: anilm (generated by with_the_same_user script) <anilm@xhdabidk40.xilinx.com>Co-authored-by: Jorn Tuyls <jornt@xilinx.com>	1
[Relay][VM] Fix an ICHECK which never fails in ctor of VMFunction (#12241)	1
[BugFix] Add lock for ModuleNode::GetFuncFromEnv (#11467)* [BugFix] Add lock for ModuleNode::GetFuncFromEnv* [BugFix] Add lock for ModuleNode::GetFuncFromEnv	1
Expose testcase as bound inference to python, now push toward the goal!	5
[TopHub] Bump the versions (#6837)* [TopHub] Update version* trigger ci	5
[VTA] [APPS] [TSIM] update documentation (README) (#3318)* update README* update README* update README* update README* fix typo	2
[MAINTAINER] Add Pariksheet Pinjari as reviewer (#1266)	1
[community] @lunderberg -> Committer (#9773)	3
Replace type punning with memcpy. (#7415)The type punning in the existing code is undefined behaviour in C.In particular, the existing code fails when running on Arm Cortex-M devices.On Cortex-M, accessing a uint64_t that is not 8-byte aligned generates a hard fault.Change-Id: I2aecaa220e581af7c91a8bc7886499d70e2aa6f2	4
[Docker][CI][BYODT] add universal to Docker image (#6654)	2
Add link to docs and tutorials in the README. (#8832)Most project pages on GitHub have a README.md file with a clear link to installation or tutorial material for new users.While there is a link to Documentation, it's not that obvious, and adding a more explicit "getting started" link may behelpful for new TVM users trying to navigate the project.	1
[AutoScheduler] Improve SearchTask and ComputeDAG serialization (#7145)* Use self.dag in Python object* Add sch to ComputeDAG* address comment	1
[FRONTEND][TENSORFLOW]Add Split and realdiv op support (#2123)* Add Split and realdiv op support* Fix the pad calculation in the case of dilated convolution	0
ok	5
[Relay][Op] Add test for batch_flatten (#2134)* Add tests for batch_flatten and softmax* Softmax is already tested elsewhere	3
[Relay] add test for second order ad (#2754)* do second order* add comment* better name* use tvm assert all close* refire ci	3
Remove unused allocated memory in crt initialization (#8819)Currently TVMInitializeRuntime() allocates 250 bytes dynamically to backbuffer 'func_registry_memory' which is never used. That is not much ingeneral but besides being twice the current necessary amount for theruntime (allocated to back 'registry_backing_memory' buffer) that amountcan be important to be saved on memory-constrained devices (microTVM).This commit removes the 'func_registry_memory' buffer which is allocateddynamically in TVMInitializeRuntime() since it occupies 250 bytes and isnever used.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[Topi] Missing header (#4865)	5
Enable miopen transpose convolution and fp16 support (#3952)* Enable miopen transpose convolution and fp16 support* linter	1
[Minor][Test] Clean WASM environment before build (#5759)	4
[microNPU] Remove xfail from tests relating to #12511 (#12570)Removes tests previously marked as xfail since the issue has nowbeen resolved.	0
[UnitTest] Updated tolerances to avoid flaky unit test. (#8723)* [UnitTest] Updated tolerances to avoid flaky unit test.The result was correct, but the atol was just small enough to triggera CI error for a value that was close to zero in an unrelated PR at#8670.https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-8670/16/pipeline/#step-236-log-1703* Also updated 32-bit version of test_conv2d_nchw	3
[ROCM] Add Thrust support (#7458)* enable rocm thrust, confrimed to work on sort and scan* add rocm argsort strategy* Abort if CXX is not hipcc* add more strategy* add missing import* fix lint* show supported data type in err msg* try remove rocthrust* add missing include for rocthrust* more minor changeCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	4
Make spelling of "axes" consistent (#7460)	1
Fixes for using Python APIs from Rust. (#7085)* Rewrite the Rust Module API and change some imports causing crashes.This commit also updates the docs to remove outdated information.* Renable Python test and remove warnings* Python test still flaky* Fix broken module test* Fix broken test* Reset test file	2
[TENSORLFOW] PlaceholderWithDefault (limited) implementation. (#3184)	5
[RELAY][OP] ROI Align (#2618)	5
[RELAY]Reduce ops sum/max/min/mean/prod (#1927)	5
[Relay][Convert Layout] Handling batch norm layout change. (#4600)	4
[CI] Simplify labeling rules (#1554)	5
doxygen path update	5
rocm: fix miopen convolutions (#5179)* fix miopen convolutions* fix overly long lines	0
Update README.md	2
[lint] Fix black whitespace errors (#8124)Change-Id: I927b43df95a8db8b042bc3cf2a1f23739d102b9d	5
[Relay] Non-recursive Dtor for Let (#9461)* non-recursive let desctruction* fix serialization* Fix serialization* lint* lint* lint* add tests for serial/deserial* lintCo-authored-by: Haozheng Fan <hzfan@apache.com>	3
[LANGUAGE] Verify Compute with respect to Reduce operations (#1006)	5
[Bugfix][MetaSchedule] Filter out dynamic extents (#11747)Previously only static shape computation is allowed in our tuningsystem. However, one special case is overlooked: the reduction iter varscould still have dynamic iteration domains which depend on other dataparallel vars. This PR rules out this case by carefully checking all theloop extents during task extraction.Related issue: https://github.com/apache/tvm/issues/11746.	0
[RUNTIME] Add System Lib (#227)* [RUNTIME] Add System Lib* lint* lint* fix compile	0
[TIR] Add schedule primitive ReIndex (#11515)	1
Change error.h path in doc.h (#1794)	2
[BYOC][ETHOSN] Add support for quantized convolution (#6335)* [BYOC][ETHOSN] Add support for quantized convolutionThis PR adds support for quantized convolution. Thisincludes mapping it via a composite function and allthe necessary methods to convert from Relay to theAPIs in Support Library.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>* Fix padding changeChange-Id: I0794b0ac6190478e2d1b858ad0dd90f37fc0207b* Add docs to Tvm2Npu methodsChange-Id: Iab865619b449a3d0dd6bb0dbdcb198acd529fc4e* Remove generate testsChange-Id: I51f90499f7ce82a1ce49f0731d3d50627e1d0225Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>	3
Use variable in curl download url (#9330)* Use variable for curl download url* Replace qemu-5.1.0.tar.xz.sig with ${QEMU_SIG_FILE}	2
Use unsafe_get in nnvm (#2247)	1
[ci] Clarify message in ping-reviewers bot (#10807)This makes the next actions more clear (i.e. convert the PR to a draft if you don't plan to address it soon) and also fixes a bug where `@`-ed users would get double-tagged.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Relay][Frontend] Caffe2 Support (#2507)* [Relay][Frontend] Add Caffe2 Support* [Relay][Frontend] Add Caffe2 Support (fix unsed import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix model install and reflect code reviews)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 frontend import)* [Relay][Frontend] Add Caffe2 Support (rename function name in test_forward)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Doc] Caffe2 frontend tutorial* [Doc] Caffe2 frontend tutorial* [Doc] Caffe2 frontend tutorial* [Relay][Frontend] Add Caffe2 Support (remove unsed file)	2
Bug-fix] Fix tir allocation with multiple lanes (#6941)* Bug-fix] Fix tir allocation with multiple lanesThis PR stemmed from https://github.com/apache/incubator-tvm/pull/6907and it is fixing a small error in the getter and setter of a buffer forthe case where `t.lanes > 1`. I also added a test to stress the issue.* Address dtyped vs non-dtyped constant cases	1
[COMMUNITY] comaniac -> Committer (#6463)	3
Fix TVMArray layout on device (#5599)	0
Revert "[Vulkan] Support uniform buffer object for passing many scalar arguments (#7717)" (#7821)This reverts commit 5bc1cec4c4acf0a54889227c1d19a6b65b6803c2.	4
[TFLite] Add option to overwrite OperatorConverter class in relay.frontend.from_tflite (#9256)* [TFLite] Relay Frontend: Add option to overwrite OperatorConverter classThis allows to overwrite the mapping from TFLite Operators to TVM Relay Operators from external python scripts. This has the following advantages:- Adding support for unsupported builtin or even custom operators by adding a hand-written convert function- Enables overwriting of existing convert functions for supported operators by alternative implementations (useful for currently unsupported edge cases)Example Usage:```class CustomOperatorConverter(relay.frontend.tflite.OperatorConverter):    def __init__(self, model, subgraph, exp_tab):        super(CustomOperatorConverter, self).__init__(model, subgraph, exp_tab)        convert_map_overwrite = {"SUB": self.convert_sub_custom}        self.convert_map.update(convert_map_overwrite)    def convert_sub_custom(self, op):        ......relay_mod = relay.frontend.from_tflite(    tflite_model, shape_dict=shape_dict, dtype_dict=dtype_dict, op_converter=CustomOperatorConverter)```[TFLite] Make sure that even DETECTION_POSTPROCESS op can be overwrittenThis is desirable, because the current implementation of this CUSTOM op is incompatible with MicroTVM targets* Tests: added test case for overwriting op_converter in TFLite relay frontendKept the test as simple as possible by only comparing 2 differentimplementations of a SUB TFLite operator:1. Original: c = a - b2. Dummy: c = a + (-b)Comparison with TFLite reference output is not necessary because tis isalready covered by other test cases. Instead comparisons of the two TVMmodels are used.	1
Handle vectorize for LE statement (#3137)* Handle vectorize for LE statementFix a new cases introduced by commit 7afbca5691fdb599cd90b043d5a5036e55cae2d6* Add test	3
[FIX,PROFILING] Only check if ops duration is nonzero (#9568)We have seen intermittent failures in the profiling tests when some ofthe durations are not nonzero. This should fix it.	0
Test run triage (#9308)	1
fix topi.nn.global_pool layout="NHWC" (#4656)* Update topi.ccfix topi.nn.global_pool layout="NHWC"* add topi.nn.global_pool layout=NHWC test	3
[FIX] Fix issue with TypedPackedFunc template instatition (#1649)	0
[Relay] Add a PyTorch to Relay Parser (#4497)* Add a PyTorch to Relay parser* Add alexnet, googlenet, mnasnet, shufflenet wip* Fix lint* Remove fix for shufflenet* Lower check* Pull changes from neo-ai/tvm changes* Remove commented out section* Use infer_shape everywhere* Change back to using trace instead of path in from_pytorch* Parse state_dict to add param names* Umbrella single_op under test_forwards* Remove print and cleanup call* Check if update to test broke CI* Retrigger CI* Add back in updated tests* Try splitting up tests* First pass at flexible typing, implemented for ones* Add int32 for all ops* Remove print statements* Fix lint* Broad except* Add other tensor types* Temporarily use old tests* Retrigger CI* Lower type names* Use numpy to convert in dense op* Fix lint* Remove print* Need to cleanup but verify int32 works for add* Rough tests for different types, a lot of types are not supported on CPU* Probably doesn't build, need to save work as I have to switch branches (constantly)* Parse param type* Remove print stmt in parser* Clean up some code* Working on flaot32 for bn* Add resnet18 double type* Fix lint* Temporarily move PT tests first* Temporarily add back refactored tests to fix mem issue* Add more type test and temp remove some tests* Comment out tests, hopefully CI prints a trace* Get stack trace* Remove operator dict key, rename op_name to node_id, remove dead code* Make relay map a list* Remove some hacky string stuff* Move to PyTorch 1.4* Remove input_type as param* Remove _get_fill_value, fix full ops* Remove unused code and combine ops for identity and none* Remove fn_param* Clean up main loop* Remove useless if/else for outputs* Remove ir_names, only used once* Remove some string hacking* Remove string parsing to get output name* Fix bug with output sizes of nodes* Use attributeNames in parse ops* Remove continue and add_op in parse_op* Do this everywhere, use assert instead of explciitly type casting* Remove unnecessary swap* Slight refactor for elemwise input parse* Use a copy of graph everywhere* Rename nid_to_node_name* Refactor parse import prereqs* Clean up input node kind check* Clean up conditionals* Clean up add_op* Cleanup type for ones and zeros op* Fix lint* Add torch install to CI* Actually use torch* Try moving import torch to only where it's needed* Import torch for CI* Use take op for select* Temporarily add ignore for jit inline pass for CI* Use CompleteTensorType, might be a PT 1.2 only thing* Use different types in elemwise op* Use float16 ones* Fix float16 test* Remove the temp docker changes* Remove temp test* Temporarily comment out original tests* Remove file* Empty cache after each test* Add some prints and lower input sizes* Try using no grad* Trying to globally set grad off* Use no grad for torchvision* Remove xfail tests* Remove VGG and AlexNet due to some issues* Combine pooling tests* Remove extra test file* Remove single op, remove larger pooling tests* Remove maxpool3* Remove debug prints* Remove inference call and add no_grad in measure latency* Use standard string start char* Remove redundant infer_shape in slice* Convert most to checks to just expr* Remove extra paren* More refactor of isinstance* Add helper for creating typed constants* Assert instead of return when no matching type* Remove network variants* Add no_grad when forward, remove deatch, fix lint* Change isinstance to expr in transpose* Use opnotimplemented, refactor* Fix full ops, remove duplicate tests* Never use shape field unless we know the type* Remove comma, retrigger CI* Add paren, retrigger CI* Use inline if-else for flags* Throw exception instead of assert* Remove version check for CI* Check version when doing inline pass* Fix lint* Lower more input sizes* Add new line, conv2d only accepts weight as expr* Use tvm.runtime.ndarray* Remove change to torch version install* Try no grad for mobilenet* Fix lint* Fix lint again* Revert to last passing* Delete test files* Ignore lint* Revert back* Comment out mobilenet* Clean up compare compiled and baseline outputs* Use IRModule* Add todos* Refactor use_bias* Add todo for fix conv op channels* Change input to data type* Remove todo* Handle channel multiplier > 1	0
fix [RUNTIME][VULKAN] vkBuffer released before memory copy command send to GPU (#5388) (#5418)	1
[INFO] Add .asf.yaml for github info (#4761)	5
Revert "[Relay] Keep fixed dim when unifying dynamic shape (#5795)" (#6658)* Revert "[Relay] Keep fixed dim when unifying dynamic shape (#5795)"This reverts commit 782190e88b1941fdbe31101af260bee06b81bf72.* run infer type on test_sparse_dense_padded_alter_op() to fix CICo-authored-by: masa <masa@pop-os.localdomain>	0
[SCHEDULE] Fix inline with multiple outputs (#507)	0
producing simulation statistics instead of time to get useful information out of simulation runs (#3481)	1
Rename relay::Environment to relay::Module (#2054)	5
[Hexagon] Remove timeout on HAP_compute_res_acquire (#10713)* [Hexagon] Remove timeout on HAP_compute_res_acquireWhen run on the simulator, a non-zero timeout for this call willbusy-loop.* Added TODO for further investigation.	2
fix libtvm build dependencies when USE_MICRO is ON. (#6524)* previously, building from scratch would fail with Unix Makefiles   due to cmake limitation	1
[Hexagon] `llvm-options` attribute is an array of strings (#9011)Change the type from String to Array<String> in the code that looksthe attribute up.	4
[CI] Skip some additional tests that are failing in the wheel (#11969)This PR skips some additional tests that are failing in the nightly wheel.	0
[microTVM][Arduino] Cleanup template directory (#9289)* restructure* readme* fix readme* trigger	0
[BYOC][ACL] Support add operation (#6532)* [BYOC][ACL] Support add operationAdded support for an "add" operation implemented via ACLfor fp32 and quantized uint8 data types* Addressed lhutton1 comments* linter	1
move fallback out of the build interface (#2456)	4
[NNVM][TENSORFLOW] Cleanup redundant code. (#1551)	4
[Ansor] Improve OpenCL support (#10108)* Support OpenCL in Autoscheduler tuning* add warning* Update src/auto_scheduler/search_task.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[PASS] StorageFlatten and StorageSync, safe condition in schedul_ops, gemm example. (#31)	4
Fix deprecated use of numpy.asscalar. (#8292)Replace use of numpy.asscalar with the use of .item .Anyone know how to use pylint to catch these uses before they land.	1
[TOPI] Average Pool2D Bug. (#3607)* [TOPI] Average Pool2D Bug.Issue - https://github.com/dmlc/tvm/issues/3581* Add uint16 test.	3
[TEAM] adityaatluri -> committer (#2140)	5
[skip ci] Revert "[ci] Default to n=2 for test parallelism (#12376)" (#12413)This reverts commit 369e8b283083a3440c59431a9438ca17afb73e4e.There are certain tests that need to be serialized first before this canmerge or else failures likehttps://ci.tlcpack.ai/job/tvm/job/main/4040/display/redirect will happenbased on which tests happen to be run together or notCo-authored-by: driazati <driazati@users.noreply.github.com>	1
initialize base class in copy constructors (#2006)GCC issues warnings with -Wextra if we don't explicitly initializebase class in copy constructors. This commit fixed the issue.	0
fix restore layout in AlterOpLayout (#460)* fix restore layout in AlterOpLayout* lint test case	3
[BYOC] Refine DNNL Codegen (#5288)* Improve DNNL* Add bind params* trigger ci	2
Upgrade tensorflow to version to 2.6.x (#10084)Upgrade the following versions:keras - from 2.4.3 to 2.6tensorflow - from 2.4.2 to 2.6.2h5py - from version < 3.0 to version 3.1.0	5
[build] Update libinfo and add lint rule (#10774)* [build] Update libinfo and add lint ruleThis updates `tvm.support.libinfo()` to be in-line with the current tvm options. It also adds a lint rule to ensure these stay matched up in the future as well as a script to print out the options in more detail. This should add in communication when debugging (i.e. tell someone to run `python -c 'import tvm; tvm.support.describe()` to learn everything you need about their envrionment)* Fix pylintCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Include BUILD_NUMBER in rebuilt docker image. (#11165)* This allows folks to retrigger Jenkins runs through the Jenkins UI   rather than requiring the author to push an empty or amended commit.	1
[Hexagon] Remove use of designated initializers from hexagon_module.cc (#6055)They are an extension, not yet a part of the C++ standard.	5
[CODEGEN] More robust llvm intrin handling, remove graph executor (#519)	4
[TIR] Add int8 CUDA tensor core intrinsics (#12354)	1
[C API] add in C API for symbolic (#3)	1
[DOC] fix doc in api.py (#4580)	2
[Team] Eddie -> PMC (#3220)	5
[TOPI][CUDA] Add faster-rcnn proposal op (#2420)* [TOPI][CUDA] Add faster-rcnn proposal op* Fix doc* Add global barrier* Use vthread in argsort* Update sort and nms ir* Fix lint* Update sort ir in ssd nms	5
update depthwise convolution api (#344)	5
[QNN][Conv2D] Optimize lowering. (#4006)	5
[CUDA] Allow dynamic shmem of size > 48K in runtime (#11478)Currently, we have functioning dynamic shared memory support on cuda. But we haven't actually explored allocating more than 48KB of dynamic shmem. This PR updates the cuda runtime to support launching a kernel which wants to use dyn shmem of size > 48KB. This is already useful for manually rewritten schedules, but to integrate this feature into tuning requires more work (see the discussion on `VerifyGPUCode` below). I'll add a test which actually uses a big dyn shmem in the next PR (need to fix one bug in software pipelining transform). Reference in cutlass code:https://github.com/NVIDIA/cutlass/blob/master/include/cutlass/gemm/device/gemm.h#L479-L482	5
[microTVM] Fix `build` directory exists error (#12575)When you build a project from existing project directory using `tvm.micro.project.GeneratedProject.from_directory` it would show up error if build directory previously existed.	0
relu of dnnl json runtime only support 4-dims input (#9122)	1
[C++] Cleanup transform API nits (#3253)	4
[skip ci] Revert "Fix function number datatype from char to uint16_t (#10014)" (#11363)This reverts commit f34bd22ddc4e7064eabe9fac42c4c04f54ede399.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CI][microNPU]Running tests parallel using pytest-xdist (#9625)* [CI][microNPU]Running tests parallel using pytest-xdistThe microNPU tests runs safely, parallel using pytest-xdist.This commit introduces changes to run them in parallel onCI.Change-Id: Ia44d73203da320b81d7c8405ac4f201159654fec* [CI][microNPU]Running tests parallel using pytest-xdist* fixing a typoChange-Id: Iba8bed3d86b6aab64ba611a67e4751fa6b6b96c7	4
[MetaSchedule] Extract task weights during task extraction (#10810)* [MetaSchedule] Extract task weights on task extraction* Update test_meta_schedule_integration.py	3
[DOCS] Point docs to the ASF site. (#5178)* [DOCS] Point docs to the ASF site.We have migrated the main docs to the ASF site,which will be periodically updated using the docs generated by the CI.Points the docs to the ASF version.* [CI] Improve the docs generation script	2
[CONTRIB][CC] Enhance cc.cross_compiler (#4817)* [CONTRIB][CC] Enhance cc.cross_compiler- Enhance cc.cross_compiler to take str argument.- Remove cc.build_create_shared_func as it is dupilicated with cross_compiler- Add examples to cc.cross_compiler* address review comments	1
[ci] Run docker prune directly in Jenkins (#11275)* [ci] Run docker prune directly in Jenkins* Inline scriptCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Compile] Fix compile issue with LLVM 8.0 (#181)	0
[TIR, Schedule] Check consumer in-bound and covered in reverse_compute_inline (#12717)* [TIR, Schedule] Generate consumer-in-bound predicate after reverse_compute_inline* Check consumer block iters are covered* fix lint	0
Fixes for follow up on PR #9631 (#10205)	0
[BugFix][VTA] Fix bug in vta runtime DepPop function. (#3208)Issue:    One of existing illegal dependency check's condition always true,    the correct logic actually should be such check for store and load.Solution:    Fix the said logic issue.	0
[COMMUNITY] @kevinthesun -> PMC (#7803)	3
[Rust] Second stage of Rust Refactor (#5527)* Add tvm-rt crate* Backport changes from frontend branch* Format* Add ASF headers* Address self-code review* Replace with helper* Fix lint* Fix* Clean up repro debugging* WIP* Remove global resgistry to fix one memory issue* Fix* Format* Format* Update rust/tvm-rt/README.mdCo-authored-by: Jason Knight <binarybana@gmail.com>* Format* Duplicate TVM macros* Split macros* Restore old macro for old crates* Repair macros* Fix format* FormatCo-authored-by: Jason Knight <binarybana@gmail.com>	0
