Fix breeze kind-cluster deploy failing with ECONREFUSED (#17293)Currently, kind-cluster deploy fails occasionally due to yarn install when compilingassets. This PR fixes it by using the recommended option --network-concurrency=1 whenrunning yarn install	1
Helm chart 1.6.0 is released; bump chart version to 1.7.0-dev (#23840)	2
Merge pull request #564 from thoralf-gutierrez/Correct_logged_state_in_BranchPythonOperatorCorrect logged state in BranchPythonOperator	1
Set data_interval for dataset triggered runs to the range up "upstream" intervals (#25825)This won't be perfect for everyone, so the behaviour is controlled bythe timetable, not "core" scheduling logic.	2
[AIRFLOW-4939] Simplify Code for Default Task Retries (#6233)	5
ECSOperator: fix KeyError on missing exitCode (#20264)* ECSOperator: fix KeyError on missing exitCode* [tests] add unit test for ECSOperator initialization failure* check that exit code is not included in the context	0
Addressed some issues in the tutorial mentioned in discussion #22233 (#22236)* Streamlined the tutorial, repaired incorrect logic in the merge, and addressed some concerns in duscussion # 22233Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>	1
[AIRFLOW-2336] Use hmsclient in hive_hookThe package hmsclient is Python2/3 compatible andoffer a handy contextmanager to handle opening and closing connections.Closes #3239 from gglanzani/AIRFLOW-2336	0
Remove the `set -x` in mypy check producing verbose output (#15932)	1
[AIRFLOW-724] Adding City of San Diego to Airflow usersCloses #1965 from MrMaksimize/sandiego_use	1
Minor fixes	0
Adding nose to reqs	1
Display Graph with TI statuses after backfilling (#7776)	1
Support glob syntax in ``.airflowignore`` files (#21392) (#22051)A new configuration parameter "CORE_IGNORE_FILE_SYNTAX" is added toallow patterns in .airflowignore files to be interpreted as eitherregular expressions (the default) or glob expressions as found in.gitignore files. This allows users to use patterns they will befamiliar with from tools such as git, helm and docker.Glob expressions support wildcard matches ("*", "?") within a directoryas well as character classes ("[0-9]"). In addition, zero or moredirectories can be matched using "**". Patterns can be negated byprefixing a "!" at the beginning of the pattern.The "fnmatch" library in core Python does not produce patterns that arefully compliant with the kind of patterns that users will be used tofrom gitignore or dockerignore files, so the globs are parsed usingthe pathspec package from PyPI.To aid with debugging ignorefile patterns a more helpful errormessage is emitted in the logs for invalid patterns, which arenow skipped rather than causing a hard-to-read scheduler stack trace.closes: #21392	1
[AIRFLOW-6887] Do not check the state of fresh DAGRun (#7510)	2
missing quotes (#21990)Missing quotes throws the following error:```airflow:- env.0.value: Invalid type. Expected: string, given: boolean```	0
Normalize *_conn_id parameters in BigQuery sensors (#21430)It fixes deprecation warning in `BigQueryHook` because of `bigquery_conn_id` parameter usage from sensors code.Note: similar change for BigQuery operators was already performed in commit 042a9ba2c285772fcc2208847198c2e2c31d4424	1
Stacked and percent area charts	2
Adding SnowflakeOperator howto-documentation and example DAG (#11975)closes #11921	2
[AIRFLOW-6120] Rename GoogleCloudBaseHook (#6734)* [AIRFLOW-6120] Rename GoogleCloudBaseHook	5
[AIRFLOW-3701] Add Google Cloud Vision Product Search operators (#4665)	1
[AIRFLOW-5226] Consistent licences for all jinja templates (#5828)	5
Added ability for Snowflake to attribute usage to Airflow by adding an application parameter (#16420)	2
Updating ADX conn docs to reflect new custom fields (#18132)	1
update example	5
Check for minimum version of Sqlite (#13496)Some users testing Airlfow 2.0 with sqlite noticed that forold versions of sqlite, Airflow does not run tasks and fails with'sqlite3.OperationalError: near ",": syntax error' when runningtasks. More details about it in #13397.Bisecting had shown that minimum supported version of sqlite is3.15.0, therefore this PR adds checking if sqlite version ishigher than that and fails hard if it is not.Documentation has been updated with minimum requirements, someinconsisttencies have been removed, also the minimum requirementsfor stable 2.0 version were moved to installation.rst becausethe requirements were never explicitely stated in the user-facingdocumentation.	2
Enable Markdownlint rule MD014/commands-show-output (#12430)https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md#md014---dollar-signs-used-before-commands-without-showing-output	2
Debugging	0
[AIRFLOW-1512] Add PythonVirtualenvOperatorCloses #2446 from saguziel/aguziel-virtualenv	1
Organize S3 Classes in Amazon Provider (#20167)* Task: Organize S3 Classes in Amazon Provider	1
[AIRFLOW-6682] Move GCP classes to providers package (#7295)* [AIP-21] Move gcp.hooks.automl providers.google.cloud.hooks.automl* [AIP-21] Move gcp.hooks.bigquery providers.google.cloud.hooks.bigquery* [AIP-21] Move gcp.hooks.bigquery_dts providers.google.cloud.hooks.bigquery_dts* [AIP-21] Move gcp.hooks.bigtable providers.google.cloud.hooks.bigtable* [AIP-21] Move gcp.hooks.cloud_build providers.google.cloud.hooks.cloud_build* [AIP-21] Move gcp.hooks.cloud_memorystore providers.google.cloud.hooks.cloud_memorystore* [AIP-21] Move gcp.hooks.cloud_sql providers.google.cloud.hooks.cloud_sql* [AIP-21] Move gcp.hooks.cloud_storage_transfer_service providers.google.cloud.hooks.cloud_storage_transfer_service* [AIP-21] Move gcp.hooks.compute providers.google.cloud.hooks.compute* [AIP-21] Move gcp.hooks.dataflow providers.google.cloud.hooks.dataflow* [AIP-21] Move gcp.hooks.datastore providers.google.cloud.hooks.datastore* [AIP-21] Move gcp.hooks.dlp providers.google.cloud.hooks.dlp* [AIP-21] Move gcp.hooks.functions providers.google.cloud.hooks.functions* [AIP-21] Move gcp.hooks.gcs providers.google.cloud.hooks.gcs* [AIP-21] Move gcp.hooks.kms providers.google.cloud.hooks.kms* [AIP-21] Move gcp.hooks.kubernetes_engine providers.google.cloud.hooks.kubernetes_engine* [AIP-21] Move gcp.hooks.mlengine providers.google.cloud.hooks.mlengine* [AIP-21] Move gcp.hooks.spanner providers.google.cloud.hooks.spanner* [AIP-21] Move gcp.hooks.speech_to_text providers.google.cloud.hooks.speech_to_text* [AIP-21] Move gcp.hooks.text_to_speech providers.google.cloud.hooks.text_to_speech* [AIP-21] Move gcp.hooks.translate providers.google.cloud.hooks.translate* [AIP-21] Move gcp.hooks.discovery_api providers.google.cloud.hooks.discovery_api* [AIP-21] Move gcp.hooks.video_intelligence providers.google.cloud.hooks.video_intelligence* [AIP-21] Move gcp.sensors.bigquery providers.google.cloud.sensors.bigquery* [AIP-21] Move gcp.sensors.bigquery_dts providers.google.cloud.sensors.bigquery_dts* [AIP-21] Move gcp.sensors.bigtable providers.google.cloud.sensors.bigtable* [AIP-21] Move gcp.sensors.cloud_storage_transfer_service providers.google.cloud.sensors.cloud_storage_transfer_service* [AIP-21] Move gcp.operators.automl providers.google.cloud.operators.automl* [AIP-21] Move gcp.operators.bigquery providers.google.cloud.operators.bigquery* [AIP-21] Move gcp.operators.bigquery_dts providers.google.cloud.operators.bigquery_dts* [AIP-21] Move gcp.operators.bigtable providers.google.cloud.operators.bigtable* [AIP-21] Move gcp.operators.cloud_build providers.google.cloud.operators.cloud_build* [AIP-21] Move gcp.operators.cloud_memorystore providers.google.cloud.operators.cloud_memorystore* [AIP-21] Move gcp.operators.cloud_sql providers.google.cloud.operators.cloud_sql* [AIP-21] Move gcp.operators.cloud_storage_transfer_service providers.google.cloud.operators.cloud_storage_transfer_service* [AIP-21] Move gcp.operators.compute providers.google.cloud.operators.compute* [AIP-21] Move gcp.operators.datastore providers.google.cloud.operators.datastore* [AIP-21] Move gcp.operators.dlp providers.google.cloud.operators.dlp* [AIP-21] Move gcp.operators.functions providers.google.cloud.operators.functions* [AIP-21] Move gcp.operators.gcs providers.google.cloud.operators.gcs* [AIP-21] Move gcp.operators.kubernetes_engine providers.google.cloud.operators.kubernetes_engine* [AIP-21] Move gcp.operators.mlengine providers.google.cloud.operators.mlengine* [AIP-21] Move gcp.operators.spanner providers.google.cloud.operators.spanner* [AIP-21] Move gcp.operators.speech_to_text providers.google.cloud.operators.speech_to_text* [AIP-21] Move gcp.operators.tasks providers.google.cloud.operators.tasks* [AIP-21] Move gcp.operators.text_to_speech providers.google.cloud.operators.text_to_speech* [AIP-21] Move gcp.operators.translate providers.google.cloud.operators.translate* [AIP-21] Move gcp.operators.translate_speech providers.google.cloud.operators.translate_speech* [AIP-21] Move gcp.operators.video_intelligence providers.google.cloud.operators.video_intelligence* [AIP-21] Move operators.adls_to_gcs providers.google.cloud.operators.adls_to_gcs* [AIP-21] Move operators.bigquery_to_bigquery providers.google.cloud.operators.bigquery_to_bigquery* [AIP-21] Move operators.bigquery_to_gcs providers.google.cloud.operators.bigquery_to_gcs* [AIP-21] Move operators.bigquery_to_mysql providers.google.cloud.operators.bigquery_to_mysql* [AIP-21] Move operators.cassandra_to_gcs providers.google.cloud.operators.cassandra_to_gcs* [AIP-21] Move operators.gcs_to_bq providers.google.cloud.operators.gcs_to_bigquery* [AIP-21] Move operators.gcs_to_gcs providers.google.cloud.operators.gcs_to_gcs* [AIP-21] Move operators.gcs_to_sftp providers.google.cloud.operators.gcs_to_sftp* [AIP-21] Move operators.local_to_gcs providers.google.cloud.operators.local_to_gcs* [AIP-21] Move operators.mssql_to_gcs providers.google.cloud.operators.mssql_to_gcs* [AIP-21] Move operators.mysql_to_gcs providers.google.cloud.operators.mysql_to_gcs* [AIP-21] Move operators.postgres_to_gcs providers.google.cloud.operators.postgres_to_gcs* [AIP-21] Move operators.sql_to_gcs providers.google.cloud.operators.sql_to_gcs* Update docs* [AIP-21] Move gcp.hooks.base providers.google.cloud.hooks.base* Move system tests to providers package* Fix deprecation message* Remove gcp.{operators,hooks,sensors}* Move gcp.example_dags to providers.google.cloud.example_dags* Move gcp.utils to providers.google.cloud.utils* Remove airflow.gcp* Fix import paths in tests* Remove reference to airflow.gcp* Fix utils path in test_mlengine_utils.py	3
Removes unnecessary AzureContainerInstance connection type (#15514)The AzureContainerInstanceHook was derived from AzureBaseHookand it did not add any new connection fields. Insteadit duplicated the tennantId and subscriptionId extr behaviour,but it never worked, because it duplicated the fieldsfrom the AzureBaseHook - and caused a lot of warnings whenAzureProvider was added.This change sets the default connection type for theAzureContainerInstanceHook to be azure_default and theconnection type to be 'azure'.Those defaults are much more 'sane'. The existing connectionswill continue to work, only when you open them in the UI, theywill display the extras directly rather than in dedicated fieldsuntil the user changes the connection type to "azure" which willfix the display.So no disruptions, just temporary UI/Editing glitch. No more warningsprinted when Azure provider is added.	1
Extend HTTP extra_options to LivyHook and operator (#14816)The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.	1
[AIRFLOW-4807] Make GCS operators, hooks, sensors Pylint compatible (#5434)	1
Remove unused [github_enterprise] from ref docs (#24033)	2
Scheduler bugfix and docs tweaks	2
Minor touch up for async docs (#19539)	2
Add roles to create_user test (#20773)Flask App Builder 3.4.3 made role and conf_password obligatorywhen creating user:https://github.com/dpgaspar/Flask-AppBuilder/pull/1758Our test for user creation did not have the role set (though theUI used it and the cient also allows to set it).This change adds the `roles` in our tests to enable upgradeto FAB 3.4.3 for the CI (currently tests in main fail because ofthe test failure)	0
[AIRFLOW-599] Adding spotify to Airflow usersDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- Adds Spotify to list of Airflow usersCloses #1855 from znichols/spotify_use	1
Validate type of `priority_weight` during parsing (#16765)closes https://github.com/apache/airflow/issues/16762Without this the scheduler crashes as validation does not happen at DAG Parsing time.	2
Add a button to set all tasks to skipped (#20455)* Added Set Skipped button to tasks* Cleanup* PR changes; wrong function layout; tested* Cleanup* Update more typing-friendly enumCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Import for TaskInstanceState and tested locallyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	3
[AIRFLOW-XXX] Add protocol operators and hooks table (#6193)	1
Fix password masking in CLI action_logging (#15143)Currently as long as argument '-p' if present, code tries to mask it.However, '-p' may mean something else (not password), like a boolean flag. Such cases may result in exception	4
[AIRFLOW-6521] Add project param to BigQuery hook .getSchema method (#7118)BigQuery hook can now take project_id as a parameter, so that operators may specify project_ids other than the default specified in connection.	1
example and docs	2
[AIRFLOW-2782][Airflow 2782] Removes unused hard-coded dagreD3Closes #3635 from verdan/AIRFLOW-2782-dagred3-fix	2
Update azure connection documentation (#15352)Update the documentation on connecting to azure services. This PR creates a page in docs for each azure connection that explains how to setup the connection. This PR also connects the conn_id params in hooks and operators docstrings to connection documentation as shown below.	2
[AIRFLOW-XXXX] Add airflow/utils/dag_processing.py to boring-cyborg.yml (#7586)	5
Impoving the docs, adding autodocs for command line	2
Fixes pushing prod image directly from breeze (#9449)	0
[AIRFLOW-1437] Modify BigQueryTableDeleteOperatorBigQueryTableDeleteOperator should define deletion_dataset_tableas a template field.Closes #2459 from yu-iskw/bq-delete	4
[AIRFLOW-388] Add a new chart for Task_Tries for each DAG	2
Improve template capabilities of EMR job and step operators (#8572)Allow EmrCreateJobFlowOperator and EmrAddStepsOperatorto receive their 'job_flow_overrides', and 'steps'arguments respectively as Jinja template filenames.This is similar to BashOperator's capability ofreceiving a filename as its 'bash_command' argument.	2
Delete unnecessary parameters in EKSPodOperator (#17960)	1
TaskGroup add default_args (#16557)* TaskGroup add default_args* test case && pylint* TaskGroup default_args docs* Update docs/apache-airflow/concepts/dags.rstCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>Co-authored-by: Xinbin Huang <bin.huangxb@gmail.com>	2
[AIRFLOW-2839] Refine Doc Concepts->Connections (#3678)	2
[AIRFLOW-5800] Add a default connection entry for PinotDbApiHook (#6457)	5
Don't check execution_date in refresh_from_db (#16809)The native sqlalchemy DateTime type does not compare well when timezonesdon't match. This can happen if the current execution_date on a DagRuninstance is not in UTC (the db entry is always in UTC).Since DagRun has a unique constraint on (dag_id, run_id), these twoshould be able to return one unique result, and the executrion_datecolumn should not be needed anyway. Let's just remove that filter toprevent all the datetime comparison trouble.	5
[AIRFLOW-1637] Fix Travis CI build status linkIn the readme it is linkinghttps://travis-ci.org/apache/incubator-airflow.svginstead ofhttps://travis-ci.org/apache/incubator-airflow.svg?branch=masterSo this is showing the build status for the latestbuild insteadof the master branchCloses #2627 from dalupus/airflow-1637	3
Fix spelling in ``CeleryExecutor`` docs (#17553)	2
testing	3
[AIRFLOW-75] Fix bug in S3 config file parsing	2
0.4.7.1	5
[AIRFLOW-3515] Remove the run_duration option (#4320)	1
[AIRFLOW-4858] Deprecate "Historical convenience functions" in airflow.configuration (#5495)1. Issue old conf method deprecation warnings properly and remove current old conf method usages.2. Unify the way to use conf as `from airflow.configuration import conf`	5
Make Scheduler livenessProbe HA-compatible (#13705)To support running with more than a single scheduler pod we can nolonger rely on `most_recent_job` -- as that would simply select the mostrecent row to have beaten, which could be from a different pod.Close: #13677 #12098	1
GCSToBigQueryOperator allow for schema_object in alternate GCS Bucket (#26190)	1
[AIRFLOW-5928] Hive hooks load_file short circuit (#6582)If function load_file with parameter create and recreate areset to False, hql = '' and should not call functionHiveCliHook.run_cli	1
[AIRFLOW-6561] Add possibility to specify default resources for airflow k8s workers (#7168)Co-authored-by: Stijn De Haes <stijndehaes@gmail.com>	1
Add test to run DB downgrade in the CI (#21273)This attempts to add db upgrade/downgrade test to the CI	3
[AIRFLOW-XXX] Fix Prerequisites link in BREEZE.rst (#6160)	2
Support for all search_scope options, as per the ldap3 spec.Updated documentation to reflect all options + link to docs.	2
[AIRFLOW-4394] Don't test behaviour of BackfillJob from CLI tests (#5160)It is slow, and we already have tests of that behaviour. All we need totest is that we call `dag.run()` correctly. Mocking ftw.	2
Fix saving hash on image build. (#26152)There was a case when image was build with breeze, when the hashof important files were not modified after image has been built becausethe "latest" tag was set and comparision did not include it.This PR fixes it.	0
Simplify "invalid TI state" message (#19029)Currently in the web UI on task instance details page, if a task is in the "up for retry" statewe will see this message:"Task is in the up_for_retry state which is not a valid state for execution. The task must be cleared in order to be run."This might suggest to the user "you need to clear this in order for this task to run".  But this is not true.  But it is not simple to make it clearer, because this function is used for a lot of different scenarios.  For example, checking for "schedulable" tasks, and "queueable" tasks. So not only would we need to keep track of which states actually require user intervention, but the desired action (e.g. queue vs schedule vs run).So I think the best course of action is to simplify this to say only what is always true, and that is just the state.	1
``KubernetesExecutor`` should default to template image if used (#19484)Currently, the user must specify image and tag in airflow.cfg, even when they are using a pod template file.  If the pod template file specifies an image and tag, the user should not be forced to also specify this in airflow.cfg.	5
[AIRFLOW-XXX] Add Get Simpl to Companies (#4272)	1
Fix static error (tabs) introduced in #10971 (#10973)	0
BaseExecutor bugfix	0
Revert "[AIRFLOW-779] Task should fail with specific message when deleted"This reverts commit 9221587514e2a0155cdced2d3ae50129b0793a10.	4
Bash command for production image (#8579)	5
Fix reducing mapped length of a mapped task at runtime after a clear (#25531)The previous fix on task immutability after a run did not fix a case where the task was removed at runtime when the literal is dynamic. This PR addreses it	1
Enforce js linting for current ui in pre-commit (#15858)* first linting pass* fix errors in tree.js* add separate www lint for js* remove all lint errorsfix all linting errors, primarily in `graph.js`Still warnings, but those don't prevent eslint from passing* narrow linting scope to just www/static/js/	4
Add Postmates to Airflow users listCloses #1599 from Syeoryn/masterAdd Postmates to Airflow users list	1
[AIRFLOW-5819] Update AWSBatchOperator default value (#6473)	1
[AIRFLOW-1338][AIRFLOW-782] Add GCP dataflow hook runner change to UPDATING.mdCloses #2326 from yk5/df-python	5
[AIRFLOW-1340] Add S3 to Redshift transfer operatorCurrently RedshiftToS3Transfer (UNLOAD) exists butthe opposite doesn't.This PR adds COPY operation asS3ToRedshiftTransfer.Closes #3161 from sekikn/AIRFLOW-1340	1
Stop failing image refreshing on "warm-up" failure (#25046)The "warm-up" buld for parallel images might fail when you runit locally, but this should be ignored, because it is really onlyneeded at the CI when we want to make sure that several parallelbuilds do not try to create the same buildx container.It's safe to ignore any failure at the warm-up stage.	0
Mask sensitive values for not-yet-running TIs (#23807)Alternative approach to #22754.  Resolves  #22738.	0
Bump prismjs from 1.26.0 to 1.27.0 in /airflow/www (#22823)Bumps [prismjs](https://github.com/PrismJS/prism) from 1.26.0 to 1.27.0.- [Release notes](https://github.com/PrismJS/prism/releases)- [Changelog](https://github.com/PrismJS/prism/blob/master/CHANGELOG.md)- [Commits](https://github.com/PrismJS/prism/compare/v1.26.0...v1.27.0)---updated-dependencies:- dependency-name: prismjs  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Allow using _CMD / _SECRET to set `[webserver] secret_key` config (#12742)`[webserver] secret_key` is also a secret like Fernet key. Allowingit to be set via _CMD or _SECRET allows users to use the external secret store for it.	1
Fix task ID deduplication in @task_group (#20870)	0
Simplify cron preset language in docs (#10370)	2
Suggest using $http_host instead of $host (#14814)If the reverse proxy is not running in port 80, using $host won't forward the port in the HTTP request to airflow and it won't build the correct redirect URL.E.g.I have nginx and airflow running in docker in the default ports. My mapping for nginx x is 7003:80.The request http://myserver:7003/myorg/airflow/ will redirect to http://myserver/myorg/airflow/admin/ instead of http://myserver:7003/myorg/airflow/admin/.	2
Display docs errors summary (#8392)	0
allow hiding of all edges when highlighting states (#15281)	1
Add backwards compatibility for chain and cross_downstream (#7807)	1
[AIRFLOW-XXXX] Fix a typo in flower command (#7074)	2
Generate version documentation from single source of truth (#20594)We used to maintain supported versions separately in the docsand it led to discrepancies. Now we have single source of truth whichis used to generate it automatically with pre-commits	1
[AIRFLOW-XXXX] Prevent Docker cache-busting on when editing www templates (#7427)There is two parts to this PR:1. Only copying www/webpack.config.js and www/static/ before running the   asset pipeline2. Making sure that _all_ files (not just the critical ones) have the   same permissions.The goal of both of these is to make sure that the docker build cache for the "expensive"operations (installing NPM modules, running asset pipeline, installing python modules)isn't run when it isn't necessary.	1
Implement dry_run for KubernetesPodOperator (#20573)Calling task.dry_run() will print out the kubectl manifest for the pod that would be created (excluding labels that are derived from the task instance context).	1
skipsdist	5
Merge pull request #2137 from gwax/editorconfig	5
Constraints and PIP packages can be installed from local sources (#11382)* Constraints and PIP packages can be installed from local sourcesThis is the final part of implementing #11171 based on feedbackfrom enterprise customers we worked with. They want to havea capability of building the image using binary wheel packagesthat are locally available and the official Dockerfile. This meansthat besides the official APT sources the Dockerfile build shouldnot needd GitHub, nor any other external files pulled from outsideincluding PIP repository.This change also includes documentation on how to prepare set ofsuch binaries ready for inspection and review by security teamsin Enterprise environment. Such sets of "known-working-binary-whl"files can then be separately committed, tracked and scrutinizedin an artifact repository of such an Enterprise.Fixes: #11171* Update docs/production-deployment.rst	2
Merge pull request #3257 from artwr/awiedmer-fix-issue-with-jdbc-autocommit	5
Refactor DatabricksHook (#19835)	5
[AIRFLOW-XXXX] Fix gitignore (#7660)	0
[AIRFLOW-3353] Upgrade Redis client (#4834)Now that Celery/Kombu have updated and work with RedisPy 3.x (they infact force us to use 3.2) we should re-introduce this change.	4
[AIRFLOW-XXXX] Add Ternary Data to README.md (#7606)	2
pep8 change	4
[AIRFLOW-1971] Propagate hive config on impersonationCurrently, if hive specific settings are definedin the configurationfile, they are not being propagated when usingimpersonation.We need to propagate this configuration down tothe impersonatedprocess.Closes #2920 from edgarRd/erod-propagate-hive-conf	5
[AIRFLOW-6894] Prevent db query in example_dags (#7516)	2
[AIRFLOW-XXX] Adding new contributor to G Adventures (#5222)G Adventures' Airflow-based project has a new contributor[ci skip]	1
[AIRFLOW-1563] Catch OSError while symlinking the latest log directoryCloses #2564 from NielsZeilemaker/AIRFLOW-1563	1
Add kdc download and some test	3
[AIRFLOW-XXXX] Add cross-reference between connection and fernet (#7190)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	1
[AIRFLOW-4107] instrument executor (#4928)	2
Fix typo in scheduler_job.py (#23095)	2
[AIRFLOW-XXXX] Add Tink as an Airflow user (#7581)	1
More fixing the screenshots	2
[AIRFLOW-817] Check for None value of execution_date in endpointexecution_date can be present in json whileresolving to None.Closes #2034 from bolkedebruin/AIRFLOW-817	5
[AIRFLOW-5360] Type annotations for BaseSensorOperator (#5966)	1
Remove commented line (#12125)This line does not add any meaning and I think was left over in the PR	1
Remove titles from link buttons (#23736)	2
[AIRFLOW-2515] Add dependency on thrift_sasl to hive extraThis PR adds a dependency on thrift_sasl to hiveextraso that HiveServer2Hook.get_conn() works.Closes #3408 from sekikn/AIRFLOW-2515	1
Merge pull request #98 from mistercrunch/closest_dsClosest ds	1
Fix legacy timetable schedule interval params (#25999)	2
[AIRFLOW-6047] Simplify the logging configuration template (#6644)	5
Fix download logs from Grid/graph view (#23009)There Checking for metadata to be falsy before JSON-decoding it doesn'tmake sense.And since 99%+ of times we don't need this value I have made it optionaland don't pass it in the front end	4
Fix _PIP_ADDITIONAL_REQUIREMENTS case for docker-compose (#23517)Recent versions of Airflow do not allow to run `pip install` asroot but the `init` job runs as root so when the variable_PIP_ADDITIONAL_REQUIREMENTS is set, the init container fails.This PR forces _PIP_ADDITIONAL_REQUIREMENTS to be empty for the initjob.	5
Fixes timeout in helm chart tests (#12209)	3
Add question about PR to feature request template (#13087)	1
[AIRFLOW-5773] Migrate AWS Athena components to /providers/aws [AIP-21] (#6446)	1
Improve graph view refresh (#16696)* Improve graph view refresh- only refresh if state has actually changed- stop refresh if all states are final- swap out `.attr()` to `.prop()` for handling `checked` see https://stackoverflow.com/questions/5874652/prop-vs-attr* check only on final states instead of pending	4
Handle and log exceptions raised during task callback (#17347)Add missing exception handling in success/retry/failure callbacks	0
Move to watchtower 2.0.1 (#19907)- This version of watchtower contains patches that fixes #15279  where empty log lines would crash Watchtower.	2
Fix packages errors summary for docs build (#12658)	2
Dags-in-image pod template example should not have dag mounts (#19337)	2
added apply_defaults to SlackAPI operatorscleaned up docstring typesrenamed params attribute to api_call_params to avoid conflicts/confusion with task_instance paramsmoved the building of the api_call_params dict to it's own function which is run by the execute function.execute method will pass on failure. Notification should not cause a DAG to fail	0
Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665)These operators provide the ability to pause and resume a redshift cluster.	1
Allow null schedule_interval in OpenAPI spec for DAGs (#11532)	2
implement rich comparison operators	1
Fix race condition when starting DagProcessorAgent (#19935)As described in detail in #19860, there was a race condition instarting and terminating DagProcessorAgent that caused us a lotof headeaches with flaky test_scheduler_job failures on our CIand after long investigation, it turned out to be a racecondition. Not very likely, but possible to happen in production.The race condition involved starting DagProcessorAgent viamultiprocessing, where the first action of the agent was changingthe process GID to be the same as PID. If the DagProcessorAgentwas terminated quickly (on a busy system) before the processcould change the GID, the `reap_process_group` that was supposedto kill the whole group, was failing and the DagProcessorAgentremained running.This problem revealed a wrong behaviour of Airflow in some edgeconditions when 'spawn' mode was used for starting the DAG processorDetails are described in #19934, but this problem will have to besolved differently (avoiding ORM reinitialization during DAGprocessor starting).This change also moves the tests for `spawn` method out fromtest_scheduler_job.py (it was a remnant of old Airlfow and itdid not really test what it was supposed to test). Instead testswere added for different spawn modes and killing the processoragent in both spawn and "default" mode.	1
Add dependency to azure-core (#13715)Snowflake has implicit azure-core>=1.10.0 dependency because ituses AzureSASCredential via azure storage-blob.This dependency will be moved to Azure soon when we mergethe #12188 and azure-storage-blob will be added as dependency there	1
enabling pandas	0
[AIRFLOW-3799] Add compose method to GoogleCloudStorageHook (#4641)	1
Add displaying multiple dates in airflow next_execution command (#9072)The "next_execution" cli sub-command now accepts an optional number ofexecutions to be returned. This is particularly useful for checkingnon-regular schedule intervals, such as those created by some cronexpressions.Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
Allow multiple extra_packages in Dataflow (#8394)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>	5
Bump eventsource from 1.0.7 to 1.1.1 in /airflow/ui (#24062)Bumps [eventsource](https://github.com/EventSource/eventsource) from 1.0.7 to 1.1.1.- [Release notes](https://github.com/EventSource/eventsource/releases)- [Changelog](https://github.com/EventSource/eventsource/blob/master/HISTORY.md)- [Commits](https://github.com/EventSource/eventsource/compare/v1.0.7...v1.1.1)---updated-dependencies:- dependency-name: eventsource  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Fix failing Breeze2 tests after adding 3.6 version in main (#22514)	1
Update tutorial docs to include a definition of operators (#25012)	1
Remove redundant parentheses from Python file (#14336)	2
Fix failing master (#13001)	0
[AIRFLOW-5201] Move GCP Functions to core (#5804)This commit moves GCP Functions from contrib to core.For more information check AIP-21.	5
Doc: Restoring additional context in Slack operators how-to guide (#18985)A recent update to the Slack example DAG removed some context of using operators that users may find useful.  Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result.  This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.	1
[AIRFLOW-3997] Extend Variable.get so it can return None when var not found (#4819)This will not change existing regular functions in the `Variable` class. Ifvariable `foo` doesn't exist:```foo = Variable.get("foo")-> KeyError```For passing `default_var=None` to get, `None` is returned instead:```foo = Variable.get("foo", default_var=None)if foo is None:    handle_missing_foo()```	0
[AIRFLOW-4115] Multi-staging Aiflow Docker image (#4936)	2
first pass as encrypting passwords	4
Add Hurb.com as Airflow User (#10518)	1
Document configuration for email backend credentials. (#14006)	5
[AIRFLOW-511][Airflow 511] add success/failure callbacks on dag levelCloses #2934 from Acehaidrey/AIRFLOW-511	2
[AIRFLOW-5196] Move Google DLP to core (#5800)This commit moves GCP DLP from contrib to core.For more information check AIP-21.	5
Refactor SSH tests to not use SSH server in operator tests (#21326)This required a slight refactor to the SSHOperator (moving`exec_ssh_client_command` "down" in to the Hook) but the SSH _Operator_tests now just use stubbing, and the only place that connects to a realSSH server is the one test of `test_exec_ssh_client_command` in SSHHook.This is both better structured, and hopefully produces less (or ideallyno) random failures in our tests	3
Group Google services in one section (#8623)	5
Fix using .json template extension in GMP operators (#9566)	1
[AIRFLOW-5104] Set default schedule for GCP Transfer operators (#5726)The GCS Transfer Service REST API requires that a schedule be set, even forone-time immediate runs. This adds code to`S3ToGoogleCloudStorageTransferOperator` and`GoogleCloudStorageToGoogleCloudStorageTransferOperator` to set a defaultone-time immediate run schedule when no `schedule` argument is passed.	4
[AIRFLOW-2708] unittest2 is reqired for devel, not just devel_ci	3
Add `OpsgenieDeleteAlertOperator` (#23405)* Add `OpsgenieDeleteAlertOperator`	4
[AIRFLOW-2364] Warn when setting autocommit on a connection which does not support it	1
Ask for provider versions in bug reports (#17480)We get a lot of bug reports for providers, and it's rare that theversions being used are in the initial report. Let's ask for them.	5
Merge pull request #603 from airbnb/better_500Better 500 error handler, now with node name	0
[AIRFLOW-5027] Grab CloudWatch logs after ECS task has finished (#5645)	5
Fix missing dot (#20141)Co-authored-by: Bas Harenslak <bas@astronomer.io>	0
Tests for Docker images in Python (#19737)	2
[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207)	1
[AIRFLOW-6052] Add TypeHints to kubernetes_pod_operator (#6648)	1
[AIRFLOW-1478] Chart owner column should be sortableCloses #2493 from skudriashev/airflow-1478	2
Improving the tests	3
Merge pull request #560 from bolkedebruin/masterFix typos, correct principal host substitution and update documentation	2
Add support to replace S3 file on MySqlToS3Operator (#20506)	1
adds ability to pass config params to postgres operator (#21551)	1
Fix message on "Mark as" confirmation page (#19363)In an earlier refactor I created a macro called `message` which"stomped" on the variable of the same name set in the view, meaning thepage shows `<Macro message>` instead of the string we meant to set.This "fixes" it by using a less-likely-to-clash name for the macro (andfixing the typo in `dismissible` parameter.)	2
Better error messsage for pre-common-sql providers (#26051)When you are using a common-sql provider functionality suchas SQLColumnCheckOperator with a provider from before common-sqlprovider was released, attempts to instatiate such provider fromcommmon-sql will fail because the Hooks in those providers derive fromthe old airflow.hooks.dbapi_hook.DbApiHook rather than fromairflow.providers.common.sql.hooks.sql.DbApiHook.We cannot do much about it, simply speaking the old providersshould be upgraded to a version that supports common.sql provider.This PR raises a message that explicitly states the error.Closes: #26046	0
[AIRFLOW-XXX] Remove `of to` typo. (#4542)[AIRFLOW-XXX] Remove `of to` typo.	2
[AIRFLOW-447] Store source URIs in Python 3 compatible listIn Python 2 map would generate a list, in Python 3this is no longer true.Downstream a job with source_uris set to a Python3 map object cannot be json-serializedwhen attempting to store the job in the database.This fix keeps the source_uris in a json-serializable list in both Python 2 and 3.Closes #1754 fromwaltherg/fix/python3_map_incompatibility	0
Fix download method in GCSToBigQueryOperator (#12442)closes: #12439	1
[AIRFLOW-5751] add get_uri method to Connection (#6426)Add a convenience method `get_uri` on `Connection` object to generate the URI for a connection.	1
Add guide for AI Platform (previously Machine Learning Engine) Operators  (#9798)	1
[AIRFLOW-4565] instrument celery executor (#5321)* instrument celery executor* remove unused import* nit* add test to all executors* fix test	3
Enable specifying dictionary paths in `template_fields_renderers` (#17321)Added the handling of paths in `template_fields_renderers` which enables information contained in dictionaries to be unpacked and rendered appropriately.	5
BugFix: ``TimeSensorAsync`` returns a naive datetime (#17875)My fix in https://github.com/apache/airflow/pull/17748 was only partially correct and I missed one part. `TimeSensorAsync` passed a naive datetime which failed when passed to `DateTimeTrigger`. This PR fixes it and adds test to avoid regression.Error:```[2021-08-27 23:31:11,508] {taskinstance.py:1657} ERROR - Task failed with exceptionTraceback (most recent call last):  File "/opt/airflow/airflow/models/taskinstance.py", line 1296, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File "/opt/airflow/airflow/models/taskinstance.py", line 1415, in _prepare_and_execute_task_with_callbacks    result = self._execute_task(context, task_copy)  File "/opt/airflow/airflow/models/taskinstance.py", line 1471, in _execute_task    result = execute_callable(context=context)  File "/opt/airflow/airflow/sensors/time_sensor.py", line 60, in execute    trigger=DateTimeTrigger(moment=self.target_datetime),  File "/opt/airflow/airflow/triggers/temporal.py", line 40, in __init__    raise ValueError("You cannot pass naive datetimes")ValueError: You cannot pass naive datetimes```Example DAG:```pythonfrom datetime import timedelta, timefrom airflow import DAGfrom airflow.sensors.time_sensor import TimeSensorAsyncfrom airflow.utils import dates, timezonewith DAG(    dag_id='example_date_time_async_operator',    schedule_interval='0 0 * * *',    start_date=dates.days_ago(2),    dagrun_timeout=timedelta(minutes=60),    tags=['example', 'example2', 'async'],) as dag:    TimeSensorAsync(task_id="test-2", target_time=time(0, 38, 0))```	1
[AIRFLOW-6320] Add quarterly to crontab presets (#6873)	1
fixed Variable json deserialization - in case value is missing, previous behaviour was trying to deserialize the default value from json - added a convenience setter:  Variable.set(key, value) - cf https://github.com/airbnb/airflow/issues/701 - 2 new UTs	1
Merge pull request #918 from airbnb/revert_876Reverting production issues from 876 and undead	0
[AIRFLOW-6890] AzureDataLakeHook: Move DB call out of __init__ (#7513)	5
Bring back deprecated security manager functions (#23243)We need to deprecate these, not remove them, to keep from breaking ourpublic api.	4
Log BigQuery job id in insert method of BigQueryHook (#12056)	1
Update modules_management.rst (#18948)Fix typo	2
Fix race in test_celery_executor.py test_retry_on_error_sending_task test (#14273)The problem was that _sometimes_ the task would actually complete, andnot timeout like we wanted.This changes the test to _always_ raise a timeout error, instead ofrelying on any particular time offset.	1
Use Markup for htmlcontent for landing_times (#9242)	1
[AIRFLOW-554] Add Jinja support to Spark-sqlAllow SQL passed to Spark-SQL operator to be JinjatemplatedCloses #1828 fromdanielvdende/spark_sql_operator_jinja	1
Fix url generation for TriggerDagRunOperatorLink (#14990)Fixes: #14675Instead of building the relative url manually, we can leverage flask's url generation to account for differing airflow base URL and HTML base URL.	0
Change should_response to should_respond (#11978)	4
[AIRFLOW-3580] Adds tests for HiveToMySqlTransfer (#4387)- adds tests for hive_to_mysql operator- refactoring	4
Merge pull request #909 from rosner/patch-1Fixes typo	2
[AIRFLOW-1656] Tree view dags query changed[AIRFLOW-1656] Tree view query changesCloses #3427 from djo10/master	4
Add MySQL to Google cloud storage operator	1
Proper warning message when recorded PID is different from current PID (#17411)Currently, when the recorded PID is different from the current PID, inthe case of run_as_user, the warning is not clear because ti.pid is usedas the recorded PID instead of parent process of ti.pid. In this case,users would see that the PIDs are the same but there was a warning thatthey are not the sameThis change fixes it.	0
Chart link + email bug	0
Fix flaky redis tests (#18537)This test very rarely fails with sqlite on CI. Just retrying shouldget rid of the problem.	0
[AIRFLOW-2033] Add Google Cloud Storage List OperatorAdded an operator to get object names in a GCSbucket filtered by prefix and delimiter withexample.Closes #2974 from kaxil/gcs_list_op	0
Improve Google PubSub hook publish method (#7831)	1
Bring back code coverage (#10143)Fixes #10138	0
Add stats to backport packages (#9501)	1
Improve handling of job_id in BigQuery operators (#11287)Make autogenerated job_id more unique by using microseconds and hash of configuration. Replace dots in job_id.Closes: #11280	5
Getting a full stack trace in the logs	2
[AIRFLOW-3025] Enable specifying dns and dns_search options for DockerOperator (#3860)Enable specifying dns and dns_search options for DockerOperator	2
Add in before_send config option to sentry integration (#18261)	1
Docs: Changed macros to correct classes and modules (#20637)closes: #20545Fixed docs for time and random macros as the reference to what they are was incorrect.	2
Fix TOC on "How-to Guides/GCP" (#8295)	0
Remove RefreshConfiguration workaround for K8s token refreshing (#20759)A workaround was added (https://github.com/apache/airflow/pull/5731) to handle the refreshing of EKS tokens.  It was necessary because of an upstream bug.  It has since been fixed (https://github.com/kubernetes-client/python-base/commit/70b78cd8488068c014b6d762a0c8d358273865b4) and released in v21.7.0 (https://github.com/kubernetes-client/python/blob/master/CHANGELOG.md#v2170).	4
Better docs for PrestoCheckOperator	1
Make Secret Backend docs clearer about Variable & Connection View  (#8913)	2
Adding a UTC clarification to UI clock	1
[AIRFLOW-4725] Fix setup.py PEP440 & Sphinx-PyPI-upload dependency (#5363)	1
Make breeeze-complete Google Shell Guide compatible (#10708)Also added unit tests for breeze-completePart of #10576	3
[AIRFLOW-XXX] Dump logs in case of kube failure (#5472)Previously we were only dumping the logs in case of _success_, which wassomewhat pointless	2
Properly remove user for test_create_user (#15981)	3
Trying to pin flask-admin lib to fix build	0
Fixes failing docs upload on master (#15148)	2
[AIRFLOW-1296] Propagate SKIPPED to all downstream tasksThe ShortCircuitOperator and LatestOnlyOperatordid not markall downstream tasks as skipped, but only directdownstreamtasks.Closes #2365 from bolkedebruin/AIRFLOW-719-3	3
Improve the layout of TI modal when browser at narrower widths (#12456)	1
Fix crash when user clicks on  "Task Instance Details" caused by start_date being None (#14416)This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g.The previous TaskInstance has not yet run because its dependencies are not yet metThe previous TaskInstance has not yet run because scheduler is busy,the previous TaskInstance was marked success without running.This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.	1
[AIRFLOW-4797] Use same zombies in all DAG file processors	2
[AIRFLOW-4268] Add MsSqlToGoogleCloudStorageOperator (#5077)	1
CI Images are now pre-build and stored in registry (#10368)* CI Images are now pre-build and stored in registryWith this change we utilise the latest pull_request_targetevent type from Github Actions and we are building theCI image only once (per version) for the entire run.This safes from 2 to 10 minutes per job (!) depending onhow much of the Docker image needs to be rebuilt.It works in the way that the image is built only in thebuild-or-wait step. In case of direct push run orscheduled runs, the build-or-wait step builds and pushesto the GitHub registry the CI image. In case of thepull_request runs, the build-and-wait step waits untilseparate build-ci-image.yml workflow builds and pushesthe image and it will only move forward once the imageis ready.This has numerous advantages:1) Each job that requires CI image is much faster because   instead of pulling + rebuilding the image it only pulls   the image that was build once. This saves around 2 minutes   per job in regular builds but in case of python patch level   updates, or adding new requirements it can save up to 10   minutes per job (!)2) While the images are buing rebuilt we only block one job waiting   for all the images. The tests will start running in parallell   only when all images are ready, so we are not blocking   other runs from running.3) Whole run uses THE SAME image. Previously we could have some   variations because the images were built at different times   and potentially releases of dependencies in-between several   jobs could make different jobs in the same run use slightly   different image. This is not happening any more.4) Also when we push image to github or dockerhub we push the   very same image that was built and tested. Previously it could   happen that the image pushed was slightly different than the   one that was used for testing (for the same reason)5) Similar case is with the production images. We are now building   and pushing consistently the same images accross the board.6) Documentation building is split into two parallel jobs docs   building and spell checking - decreases elapsed time for   the docs build.7) Last but not least - we keep the history of al the images   - those images contain SHA of the commit. This means   that we can simply download and run the image locally to reproduce   any problem that anyone had in their PR (!). This is super useful   to be able to help others to test their problems.* fixup! CI Images are now pre-build and stored in registry* fixup! fixup! CI Images are now pre-build and stored in registry* fixup! fixup! fixup! CI Images are now pre-build and stored in registry* fixup! fixup! fixup! CI Images are now pre-build and stored in registry	1
check for baseOperator inside of task_or_task loop	1
Fix grammatical error in README.mdFixed "they becomes more" to "they become more"	2
[AIRFLOW-5480] Fix flaky impersonation (#6098)	0
Fix mypy errors in providers/amazon/aws/operators (#20401)	1
Editing TODO	2
Chart: Add tests for tolerations, affinity & node-selector (#15297)This commits adds tests and improve tests coverage for the Helm chartby testing that tolerations, affinity and node-selector can be modifiedfor the following components:- Flower- Pgbouncer- Scheduler- Statsd- Webserver- Worker	1
Upgrade `pip` to latest released 22.1.0 version (#23665)We are finally able to get rid of the annoying false-positivewarnings and we have finally a chance on having warning-freeinstallation during docker builds.	2
Add how-to guide for hive operator (#21590)	1
Migrate Google example automl_vision to new design AIP-47 (#25152)related: #22447, #22430	1
[AIRFLOW-491] Add feature to pass extra api configs to BQ Hook (#3733)	1
Merge pull request #608 from svendx4f/doc_and_UTsUT: documentation clarification and improvements	1
Fix order of failed deps (#14036)	0
Note that yarn dev needs webserver in debug mode (#24119)* Note that yarn dev needs webserver -d* Update CONTRIBUTING.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* Use -D* Revert "Use -D"This reverts commit 94d63adcf36aac13f5d94c2d4cd651907d833794.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>	1
fix	0
Mount ${HOME}/.aws in breeze environemnt if --forward-credentials (#8183)	5
Merge pull request #83 from airbnb/flag_upstream_failedAdding the upstream_failed state to allow the scheduler to move forward	4
[AIRFLOW-1600] Fix exception handling in get_fernetAlso adds LoggingMixin to Connection so it can useself.logger.Closes #2600 from GeorgeSirois/fix-fernet-no-cryptography	0
Speed up TestFlaskCli test (#14865)Creating a new process and reloading all of the app is slow, so insteaduse the built-in `runpy` module to run the `flask` module in the sameinterpreter, taking advantage of the already-cached app.This one test was taking about 20s, now down to 3s (still slow, but muchmuch faster)	3
Check that all pre-commits are synchronized code<>docs (#10789)Until pre-commit implements export of all configuredchecks, we need to maintain the list manually updated.We check both - pre-commit list in breeze-complete anddescriptions in STATIC_CODE_CHECKS.rst	5
Rare bug fix when around task instance duration	0
adding one UT for log_to_stdout	2
Remove deprecated import form within test zip file. (#16390)This was causing the following warning in the DagBag tests```/opt/airflow/tests/models/../dags/test_zip_invalid_cron.zip/test_invalid_cron.py:21: DeprecationWarning: This module is deprecated. Please use `airflow.operators.dummy`.```	1
Separate resources parameter for kerberos sidecar (#14342)	2
Add DagWarning model, and a check for missing pools (#23317)We can use this to track (and surface to the user in the web UI) configuration problems that don't rise to the level of failing the dag parse, such tasks that reference nonexistent pools.	2
Docs: Fix default 2.2.5 log_id_template (#24455)I accidentally got the wrong default for 2.2.5 when documenting howto fix elasticsearch remote logging after upgrading to 2.3.0+.	2
Update provider READMEs for up-coming 1.0.0beta1 releases (#12206)	1
GitHub PROD image build is pushed to GitHub Registry. (#13442)One of the earlier changes removed unneccessary pulling of the'build' segment from GitHub Registry in `ci_wait_for_prod_image.sh'.It is not needed, because the K8S tests only use the final imageand pulling the build image is not necessary.This has the undesired effect that 'ci_wait_for_prod_image.sh' wasalso used at the step where master image pushes the prod imageto the repository as the 'latest' cache. In this case both - the'final' prod image and the 'build' segment should be pulled,because both are needed for PROD image build optimizations.	1
[AIRFLOW-5343] Add pool_pre_ping to SQLAlchemy (#5949)SQLalchemy supports connection check while returning it from the pool. There is a need to allow this parameter (`pool_pre_ping`) while creating the connection to database.More info here: https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic	2
[AIRFLOW-XXXX] Update GitLab team members (#7308)@tlapiana is no longer with the company and @m_walker is new on the team!	1
[AIRFLOW-1439] Add max billing tier for the BQ Hook and OperatorCloses #2437 from aviDms/master	1
Added sas_token var to BlobServiceClient return. Updated tests (#19234)	3
Queue support for DaskExecutor using Dask Worker Resources (#16829)	1
fixed typo (#8294)	2
[AIRFLOW-6608] Change logging level for PythonOperator Env exports (#7229)	1
include the part about how to change the metadata database	5
Handle timetable exception in ``DAG.next_dagrun_info`` (#18729)For now, the exception is simply logged and the DAG not being scheduled.Currently, an incorrectly implemented timetable may crash the scheduler process entirely due to uncaught exceptions. This PR adds exception handlers around those calls. A failed infer_manual_data_interval() will cause the manual DAG run being skipped, and a failed next_dagrun_info() will cause the DAG run to not happen, and the DAG not being scheduled anymore (because None is set on DagModel.next_dagrun) until the DAG file is modified.For now, the exception is simply logged. In the future we'll add a new db model similar to ImportError and hold these errors and display them on the web UI. This new model class will also be designed to incorporate ImportError eventually.	2
Use click for building prepare_provider_packages.py CLI (#14480)Click is already a dev dependency and handles a lot of the CLI we neednicely.One change I have made here is to only have the various options onlyapply to the subcommands that use them, i.e. `list-providers-packages--release-version 2020.10.10` would be an error now.This does mean that the command has to come before the options -- allthe automated uses have been updated.	5
Docs: Fix grammar in ``docs/apache-airflow/start/docker.rst`` (#18484)Fixed grammatical issues  in ``docs/apache-airflow/start/docker.rst``	2
Deprecate helper utility `days_ago` (#21653)This helper function is not all that helpful, introduces confusion (e.g. which timezone? how to handle DST?), and results in a "moving" start date for dags.Vote thread: https://lists.apache.org/thread/qfqjb8m3v834yc8mxo1oqtjddhp9sggk	2
Pointing setup.py to then new repo	1
Update TIs with a proper lock (#11683)	5
Fix grid date ticks (#24738)* fix date ticks* fix linting* fix LinkButton type error	0
Improve documentation for tasks run command (#19580)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	1
Add S3KeySizeSensor (#13049)S3KeySizeSensor allows checking the S3 object sizes or perform any other actions needed by users.	1
Merge pull request #74 from mistercrunch/chartMore charts improvements	1
Type-annotate SkipMixin and BaseXCom (#20011)	5
[AIRFLOW-4204] Update super() calls (#5143)Replace super(_class, self) by super() for all filesexcept ones in _vendor.	2
Adding more licenses to pass checks	4
Fix direct use of cached_property module. (#16710)On Python 3.8 and 3.9 we use the built in functools decorator for this.	1
[AIRFLOW-1309] Allow hive_to_druid to take tblpropertiesDear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [ ] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, "[AIRFLOW-XXX] My Airflow PR"    -https://issues.apache.org/jira/browse/AIRFLOW-1309### Description- [ ] Here are some details about my PR, includingscreenshots of any UI changes: Add optionaltblproperties for the druid hook### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason: Will add### Commits- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from"[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood ("add", not"adding")    5. Body wraps at 72 characters    6. Body explains "what" and "why", not "how"Closes #2368 from saguziel/aguziel-update-hive-to-druid	5
[AIRFLOW-5354] Reduce scheduler CPU usage from 100% (#7552)A previous change ended up with the scheduler busily checking if theDagFDagFileProcessorAgent had collected any dags. This simple changetakes the CPU usage of the scheduler from an entire core, to barelyanything (dropped below 5%).Time for 10 dag runs of 9 dags with 108 total tasks: 50.3581s (±9.538s)vs master of Time for 10 dag runs of 9 dags with 108 total tasks: 49.6910s (±7.193s)The change is is basically no overall change, and is a quick fix fornow, and bigger changes are in store around DAG parsing anyway.	2
Ensure the messages from migration job show up early (#23479)The default for python is to buffer stdout, which means that log linesmight now show up in the output straight away (until a certain number oflines or number of bytes of output have been written) -- this isespecially problematic if the pre-migration checks taking a long time asit makes it look like it has hung	1
Better logging and testing	3
[AIRFLOW-2082] Resolve a bug in adding password_auth to api as auth method (#4343)	4
Fix helm chart unittests on public runners (#18553)The helm tests are now regularly taking longer than 25 minutes on publicGitHub Actions workers, so we will increase the timeout.	1
Adding indices	1
[AIRFLOW-6057] Update template_fields of the PythonSensor (#6656)Add op_args and op_kwargs to the template_fields of the PythonSensor.	1
[AIRFLOW-XXX] Add Twine Labs as an Airflow userCloses #3287 from ivorpeles/twine_airflow	1
Kubernetes worker pod doesn't use docker container entrypoint (#12766)* Kubernetes worker pod doesn't use docker container entrypointFixes issue on openshift caused by KubernetesExecutor pods not runningvia the entrypoint script* fix* Update UPGRADING_TO_2.0.mdCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* fix UPDGRADING* @ashb commentsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	0
[AIRFLOW-XXXX] Fix typo in error for when getting data flow jobs (#7260)	5
Move docs for max_db_retries option to core (#12167)	5
Merge pull request #477 from mtustin-handy/mtustin-handy-donot-pickleMake donot_pickle a configuration option	5
Fix spelling (#22054)	0
Log traceback in trigger excs (#21213)	2
Documentation text did not match the sample code	2
Remove Jira title requirement from Mergable check (#7780)Follow up to #7771, where we missed one final check	1
[AIRFLOW-XXXX] Add Rapido to Airflow Users list (#7244)	1
Cleaning up dead item in pool queue	4
Additional properties should be allowed in provider schema (#13440)The additional properties should be allowed in provider schema,otherwise future version of providers will not be compatible witholder versions of Airflow.Specifying 'additionalProperties' as allowed we are opening up toadding more properties to provider.yaml.This change fixes this is for now by removing extra fieldsadded since the Airlow 2.0.0 schema and verifying that the 2.0.0schema correctly validates such modified dictionary.In the future we might deprecate 2.0.0 and add >=2.0.1 limitationto the provider packages in which case we will be able to removethis modification of the provider_info dict.Also added additional test for provider packages whether theyinstall on Airflow 2.0.0. This tests might remain even after thedeprecation of 2.0.0 - we can just move it to 2.0.1. However thiswill give us much bigger confidence that the providers willcontinue work even for older versions of Airflow 2.0.We might have to modify that test and only include the providersthat are backwards-compatible, in case we have some providersthat depend on future Airflow versions. For now we assumeall providers should be installable from master on 2.0.0.	1
Reorder doc/spelling build order and improve spelling error message for CI (#14196)Generic Sphinx errors that were unrelated to spelling were being raised as spelling errors leading to potential confusion. Reversing the order of the build (i.e. running the docs build before the spelling build) will catch such errors and more appropriately denote the issue as a build issue and not a spelling issue. Additionally, the error message for Sphinx errors unrelated to spelling has been updated to be more clear for such cases.closes: #14051	5
[AIRFLOW-XXX] Better instructions for airflow flower (#4214)* Better instructions for airflow flowerIt is not clear in the documentation that you need to have flower installed to successful run airflow flower. If you don't have flower installed, running airflow flower will show the following error which is not of much help:airflow flower                                                                                       [2018-11-20 17:01:14,836] {__init__.py:51} INFO - Using executor SequentialExecutor                                                      Traceback (most recent call last):                                                                                                         File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/bin/airflow", line 32, in <module>                         args.func(args)                                                                                                                        File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/site-packages/airflow/utils/cli.py", line 74, in wrapper                                                                                                                              return f(*args, **kwargs)                                                                                                              File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/site-packages/airflow/bin/cli.py", line 1221, in flower                                                                                                                               broka, address, port, api, flower_conf, url_prefix])                                                                                   File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py", line 559, in execvp                  _execvpe(file, args)                                                                                                                   File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py", line 604, in _execvpe                raise last_exc.with_traceback(tb)                                                                                                      File "/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py", line 594, in _execvpe                exec_func(fullname, *argrest)                                                                                                        FileNotFoundError: [Errno 2] No such file or directory* Update use-celery.rst	1
Fixing bug around NaN in chart json + now exposing the js error	0
[AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML (#7619)* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML CODE REVIEW* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML CODE REVIEW 2Co-authored-by: Bjorn Olsen <BjornOlsen@capitecbank.co.za>	1
[AIRFLOW-3153] Send DAG processing stats to statsd (#4748)Add 2 stats under the `airflow.dag_processing` namespace. The metricnames follow the template: `dag_processing.<metric>.<dag_file>`, where`<dag_file>` is the name of a file in the dag_folder (without theextension) and `<metric>` is one of the following:- `last_runtime`: the number of seconds it took to process the DAG file  on the most recent iteration- `last_run.seconds_ago`: the number of seconds that have elapsed since  the DAG file was last processedI've verified the logging by running the scheduler on the examples DAGsand logging the value of the gauges with netcat:    $ nc -u -l -p 8125 | tr '|' '\n' | grep dag_processing      gairflow.dag_processing.last_runtime.example_docker_operator:2.002253      gairflow.dag_processing.last_run.seconds_ago.example_docker_operator:18.066831      gairflow.dag_processing.last_runtime.tutorial:2.001403      gairflow.dag_processing.last_run.seconds_ago.tutorial:36.114995      gairflow.dag_processing.last_runtime.docker_copy_data:2.003188      gairflow.dag_processing.last_run.seconds_ago.docker_copy_data:28.097275	5
Add elasticsearch to the fixes of backport providers (#14763)We have a new elasticsearch fix and it should be added to the releaseof backport providers. This change updates the documentation for theelasticserch release.	2
Remove test dependency from TestApiKerberos (#10950)TestApiKerberos::test_trigger_dag previously was dependent that the `example_bash_operator` exist in the Database.If one of the other tests didn't write it to the DB or if one of the other tests cleared it from the DB, this test failed.	0
invoke tox directly	5
[AIRFLOW-2174] Fix typos and wrongly rendered documentsFix typos and wrongly rendered documents in thefollowing pages:- tutorial.html#default-arguments- configuration.html#connections- code.html#airflow.operators.hive_stats_operator.HiveStatsCollectionOperator- code.html#airflow.operators.redshift_to_s3_operator.RedshiftToS3Transfer- code.html#airflow.contrib.operators.dataflow_operator.DataFlowJavaOperator- code.html#airflow.contrib.operators.dataflow_operator.DataflowTemplateOperator- code.html#airflow.contrib.operators.dataproc_operator.DataprocWorkflowTemplateInstantiateOperator- code.html#airflow.contrib.operators.mlengine_operator.MLEngineModelOperator- code.html#airflow.contrib.operators.mlengine_operator.MLEngineVersionOperator- code.html#airflow.models.DAG.following_schedule- code.html#airflow.models.DAG.previous_scheduleCloses #3093 from sekikn/AIRFLOW-2174	2
Merge pull request #1162 from himank/AddFernetKeyAdding fernet key to use it as part of stdout commands	1
Undeprecate private_key option in SFTPHook (#15348)Remove the deprecation warning for the private_key option in SFTPHook. There are valid use cases for storing a private key in the connection object (e.g. storing them in connections backed by an external secrets backend).	1
Use MongoDB color for MongoToS3Operator (#14103)Setting UI color for MongoToS3Operator the MongoDB brand oneCo-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>	5
Fixed Kubernetes Operator large xcom content Defect  (#23490)	1
[AIRFLOW-3152] Kubernetes Pod Operator should support init containers. (#6196)* Add support for init-containers to Kubernetes Pod OperatorEnables start-up related code to be added for an app container in K8s Pod operator.Add new init_container resource that can be attached to the K8s Pod.* Update init_container and fix testsFix the error in init_container and the associated tests.* Refactor and fix testsFix tests for init_containers* Fix init_container test errorsRemove unused mocks in init_container test* Fix init_container test errorsUpdate volume mount object used in init_container test* Fix init_container test errorsAdd the missing volume setup for the init_container test.* Fix init_container test failure.Fix the expected result in the init_container test.* Fix init_container test failuresUpdate expected results in the init_container tests* Update the KubernetesPodOperator guideUpdate the KubernetesPodOperator guide to document support for init containers* Update init-container testsFix test failures casued due python versions by sorting the output before assert test.* Update init-container to use k8s V1Container objectRemove custom object InitContainer.Allow users to pass List[k8s.V1Container] as init-container in K8sPodOperator* Add missing init_containers initalization in K8s pod operatorDue to rebase from master, certain sections of the kubernetes_pod_operator.py file was refactored which led to missing init_containers initalization in K8s pod operator. Add missing init_containers initalization in K8s pod operator. Update kubernetes pod operator configurations in init container test.	3
fixed indent + return obj for get_partitions	1
[AIRFLOW-589] Add templatable job_name[]The jobname is the name that will appear in theDataProc web console.It's helpfull to have a one-to-one mapping betweenthe airflow task andthe job running on the cluster. Adding a templatedparameter will allowyou to customize how airflow will construct thejobname.The default is to add the {{task_id}} +{{ds_nodash}} + random hash.Closes #1847 from alexvanboxel/feature/airflow-589-dataproc-templated-job-name	5
Fix ``DetachedInstanceError`` when dag_run attrs are accessed from ti (#18499)Loading a taskinstance doesn't load the corresponding dag_run withthe effect that when the dag_run attr is accessed from the ti it givesa DetachedInstanceError, dag_run is not bound to a session.This PR fixes it by making an extra query to get the dagrun using therelationship between the two objects	1
[AIRFLOW-275] Update contributing guidelinesThe current guidelines do not reflect the move to Apache. Thisupdates the links and update the PR template.	5
Improve guidance to users telling them what to do on import timeout (#18478)* Improve guidance to users telling them what to do on import timeoutThe message about import timeut does not explain an average userwhat the root cause is. The updated one links to the docuementationon how to reduce tpo-level Python code overhead and reducecomplexity of their DAGs.	2
Fixes continuous image rebuilding with Breeze (#12256)There was a problem that even if we pulled the right imagefrom the Airflow repository, we have not tagged it properly.Also added protection for people who have not yet at all pulledthe Python image from airflow, to force pull for the first time.	1
[AIRFLOW-963] Fix non-rendered code examplesPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-963Testing Done:- ran sphinx-build locally and confirmed correctlyrenderedCloses #2139 from sekikn/AIRFLOW-963	5
Avoid logging in to GitHub Container Registry when not in CI (#17169)* Avoid logging in to GitHub Container Registry when not in CIWhen GITHUB_TOKEN was set in environment, attempt to login tohttps://ghcr.io/ was made. But GITHUB_TOKEN is commonly used toauthenticate and if you happened to not have access there theattempt failed.This PR only attempts to login when the`AIRFLOW_LOGIN_TO_GITHUB_REGISTRY` variable is set to `true`and sets the variable in CI.	1
Dropbox uses Airflow (#13956)	1
Fix failing static check (#18891)Try 2 to fix failing check	0
Merge pull request #640 from SimpleHQ/allow_auto_commit_in_mysql_operator[MySqlOperator] Fix issue https://github.com/airbnb/airflow/issues/459 - mysql error 2014	0
Fix `TestSecurity.test_current_user_has_permissions` (#17916)This test wasn't working on python > 3.7.	1
[AIRFLOW-3232] More readable GCF operator documentation (#4067)	2
Fix intermittent orphan test (#18530)The orphan scheduler test fails intermittently likely due tothe way how pytest runs child processes. The fix is to run thistest in a fork and pass the result of the test back to theparent process. This way we can be pretty sure that the processhas no extra children.	3
Switch to python 3.4, 3.5 was a bit too ambitious for now	5
[bugfix] Missing a plus sign	0
[AIRFLOW-2264] Improve create_user cli help messageCloses #3168 from feng-tao/airflow-2264	1
Quick Update GCS Presto (#21855)	5
[AIRFLOW-5806] Simplify the xcom table (#6463)Remove the id column, since it isn't used anywhere	1
Chart: Fix Helm Hooks Weight for K8s Jobs (#20018)https://github.com/apache/airflow/pull/18776 introduced a bug where it changed the Helm Hook weight for Create User job from 2 to 1. It needs to be 2 as we want to run migrations first even before create-user-job	1
[AIRFLOW-297] support exponential backoff option for retry delayCloses #1639 from jgao54/support-retry-backoff	1
Remove assignment that assigns a variable to itself (#16413)	4
19943 Grid view status filters (#23392)* Move tree filtering inside react and add some filters* Move filters from context to utils* Fix tests for useTreeData* Fix last tests.* Add tests for useFilters* Refact to use existing SimpleStatus component* Additional fix after rebase.* Update following bbovenzi code review* Update following code review* Fix tests.* Fix page flickering issues from react-query* Fix side panel and small changes.* Use default_dag_run_display_number in the filter options* Handle timezone* Fix flaky testCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>	3
Chart: Support ``extraContainers`` and ``extraVolumes`` in flower (#16515)This allows for deploying sidecars in the flower pod.	1
[AIRFLOW-2650] Mark SchedulerJob as succeed when hitting Ctrl-cWithout this fix it turns out that the job wouldremain in the runningstate.This also sets things to failed in case of anyother exception.Closes #3525 from ashb/scheduler-job-status	0
[AIRFLOW-4076] Correct port type of beeline_default in init_db (#4908)airflow initdb will create default beeline connectionwith port "10000", but airflow.models.connectionvariable port is Integer type. It's better to setvalues in same type as int although it could autotransfer	1
Verify enough resources for Docker (#20957)	2
Moving templates to where they belong	4
[AIRFLOW-XXX] Add contributor from Easy companyCloses #3052 from diraol/add-contributor	1
Bugfix for retrying on provision failuers(#22137)	0
Enable 'Public function Missing Docstrings' PyDocStyle Check (#9463)	2
Fix Python Docstring parameters (#12513)	2
Remove generating temp remote manifest file in project dir (#9267)	2
[AIRFLOW-1523] Clicking on Graph View should display related DAG run (#5866)Add execution_date_argUse execution_date_arg in graph, gantt, and Back To {parent.dag} links.Add check of execution date	5
Breeze must create `hooks\` and `dags\` directories for bind mounts (#24122)  Now that breeze uses --mount instead of --volume (the former of which  does not create missing mount dirs like the latter does see docs here:  https://docs.docker.com/storage/bind-mounts/#differences-between--v-and---mount-behavior)  we need to create these directories explicitly.	1
[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#6138)	2
Adding index to speed zombie lokkup	1
Merge pull request #239 from airbnb/fix_timedeltaChanging the TimeDeltaSensor to set the delta relative to the start time	1
Use consistent message in SchedulerJob._process_executor_events (#9929)	1
Merge pull request #260 from jlowin/XComAdd XCom (cross-communication) functionality	1
Timeout and initdb	5
Bump dns-packet from 1.3.1 to 1.3.4 in /airflow/ui (#25806)Bumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.- [Release notes](https://github.com/mafintosh/dns-packet/releases)- [Changelog](https://github.com/mafintosh/dns-packet/blob/master/CHANGELOG.md)- [Commits](https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4)---updated-dependencies:- dependency-name: dns-packet  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Add remaining community guidelines to CONTRIBUTING.rst (#11312)We are cleaning up the docs from CWiki and this is what's left ofcommunity guidelines that were maintained there.Fixes #10181	0
[AIRFLOW-1507] Template parameters in file_to_gcs operatorCloses #2516 from aravinduv/feature	1
[AIRFLOW-5184] Move GCP Natural Language to core (#5792)	4
Fix mapped task immutability after clear (#23667)We should be able to detect if the structure of mapped task has changedand verify the integrity.This PR ensures thisCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	4
[AIRFLOW-855] Replace PickleType with LargeBinary in XComPickleType in Xcom allows remote code execution.In order to deprecateit without changing mysql table schema, changePickleType to LargeBinary because they both maps to blob type in mysql. Add"enable_pickling" tofunction signature to control using ether pickletype or JSON. "enable_pickling" should also be added to core section ofairflow.cfgPicked up where https://github.com/apache/incubator-airflow/pull/2132 left off. Took thisPR, fixed merge conflicts, addeddocumentation/tests, fixed broken tests/operators,and fixed the python3 issues.Closes #2518 from aoen/disable-pickle-type	0
[AIRFLOW-6860] Default ignore_first_depends_on_past to True (#7490)	5
Merge pull request #194 from airbnb/syspathAdd DAGS_FOLDER to sys.path earlier, to allow plugins to use it	1
[Airflow 13779] use provided parameters in the wait_for_pipeline_state hook (#17137)I removed wait_for_pipeline_state from start_pipeline hook. By this call, I think we have a bug in this operator, for example when we have pipeline which starting more than 300 seconds, so it have a starting status, we get the error because this pipepline is not in correct state after 300 seconds. Even when we pass our parameters sucess_states and pipeline_timeout we get this error in this case, so I think when I pass both parameters the logic should use them not default. Why we have 300 second and these SUCCESS_STATES + [PipelineStates.RUNNING], because we had these values in the wait_for_pipeline_state call which I removed from hook, I think we should replace this 300 second and use default value from __init__ method (this is a open question I think)	5
Improve diagnostics message when users have secret_key misconfigured (#17410)* Improve diagnostics message when users have secret_key misconfiguredRecently fixed log open-access vulnerability have causedquite a lot of questions and issues from the affected users whodid not have webserver/secret_key configured for their workers(effectively leading to random value for those keys for workers)This PR explicitly explains the possible reason for the problem andencourages the user to configure their webserver's secret_keyin both - workers and webserver.Related to: #17251 and a number of similar slack discussions.	1
Merge pull request #1270 from bolkedebruin/pypi_metaAdd pypi meta data and sync version number	5
[AIRFLOW-2548] Output plugin import errors to web UI (#3930)	0
Clean-up of google cloud example dags - batch 2 (#19527)- Use static start_date- Use catchup=False- Tidy up the chaining of tasks in some cases- Remove unnecessary specification of default conn ids	4
Upgrade to webpack 5 (#24485)* update webpack-cli, eslint, stylelint, babel* revert stylelint changes* update more plugins* update to webpack 5* remove all resolutions	4
Be build -> built, and a stray space (#20703)	5
Ensure @contextmanager decorates generator func (#23103)	5
Bump version to 2.1.0dev0 (#13382)	5
[AIRFLOW-3901] add role as optional config parameter for SnowflakeHook (#4721)	1
[AIRFLOW-4939] Add default_task_retries config (#5570)	5
Setting `max_tis_per_query` to 0 now correctly removes the limit (#13512)This config setting is documented as 0==unlimited, but in my HAscheduler work I rewrote the code that used this and mistakenly didn'tkeep this behaviour.This re-introduces the correct behaviour and also adds a test so that itis stays working in the future.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	1
Add test for Public role permissions. (#13965)In #13923, all permissions were removed from the Public role. This adds a test to ensure that the default public role doesn't have any permissions.related: #13923	3
closes apache/incubator-airflow#2291 *Obsolete PR.*	5
[AIRFLOW-3277] Correctly observe DST transitions for cron (#4117)`following_schedule` converts to naive time by using thelocal time zone. In case of a DST transition, say 3AM -> 2AM("summer time to winter time") we incorrectly re-appliedthe timezone information which meant that a "CEST -> CEST"could happen instead of a "CEST -> CET". This resultedin infinite loops.	5
[AIRFLOW-2534] Fix bug in HiveServer2HookThis commit also adds numerous tests forHiveServer2 and switchesImpyla for PyHive (0.6.0), making HiveServer2Python 2 compatible.Closes #3432 from gglanzani/AIRFLOW-2534	1
Display explicit error in case UID has no actual username (#15212)Fixes #9963 : Don't require a current usernamePreviously, we used getpass.getuser() with no fallback, which errors outif there is no username specified for the current UID (which happens alot more in environments like Docker & Kubernetes). This updates mostcalls to use our own copy which has a fallback to return the UID as astring if there is no username.	1
[AIRFLOW-6762] Fix link to "Suggest changes on this page" (#7387)	4
[AIRFLOW-945][AIRFLOW-941] Remove psycopg2 connection workaroundCloses #2272 from dlackty/AIRFLOW-945	1
Import ABC from collections.abc (#9649)	2
[AIRFLOW-1802] Convert database fields to timezone aware	5
[Airflow-XXXX] Fix a comment error in _utils.sh	0
Add "search_scope" as a configuration variable for LDAP (#796)This is the correct solution to #796 -- instead of completely droppingthe variable all together.Added a bit of "pretty" failure for this error as well--includingspecifying what is happening in the webserver log, and how it can befixed.	0
[AIRFLOW-XXX] Add sentry.io to list of airflow users (#5708)[ci-skip]	1
Merge pull request #462 from airbnb/traceback_for_500print traceback for internal server error	0
Add update endpoint for DAG (#9101) (#9740)	2
Fixed shellcheck error with static checks (#15450)Shellcheck released today broke static checks. This PR fixes itand pins shellcheck to specific version to avoid it in the future.	0
Run Dataflow for ML Engine summary in venv (#7809)	5
[AIRFLOW-XXX] Fix Minor issues with Azure Cosmos Operator (#4289)- Fixed Documentation in integration.rst- Fixed Incorrect type in docstring of `AzureCosmosInsertDocumentOperator`- Added the Hook, Sensor and Operator in code.rst- Updated the name of example DAG and its filename to follow the convention	2
[AIRFLOW-XXX] Bumping Airflow 1.10.0dev0+incubating version	5
Fix test - TestImpersonation (#12274)	3
make sure bytes get decoded to strings	1
Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677)closes: #10271related: #9844 #14184This PR refactor SQL/BigQuery Check operators to reduce duplicated code:create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() methodAdd a database kwarg *CheckOperators for a consistent interfacecreate _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuerycreate _QuboleCheckOperatorMixin to remove duplicate codereplace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class name, and reduce duplicate coderemove and deprecate DruidCheckOperator the same functionality can be achieved by SQLCheckOperator - the deprecation method is the same for PrestoCheckOperatorMisc:Fix docstringsUpdate deprecated Operator name and import pathRemove unnecessary if statements check parameters in SQLBranchOperator	1
Name default config_file param in KubernetesPodOperator docstring (#5153)Update airflow/contrib/operators/kubernetes_pod_operator.pyCo-Authored-By: leahecole <6719667+leahecole@users.noreply.github.com>Minor grammar tweak	1
Implement API endpoint for DAG deletion (#17980)Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>	4
[AIRFLOW-2147] Plugin manager: added 'sensors' attributeAirflowPlugin required both BaseOperator and BaseSensorOperatorto be included in its `operators` attribute. Add a `sensors`attribute and updated import logic so that anything added tothe new attribute can be imported from `airflow.sensors.{plugin_name}`The integration/`make_module` calls in `airflow.plugins_manager`for operators is also updated to maintain the ability to importsensors from `operators` to avoid breaking existing plugins- Update unit tests and documentation to reflect this- Added exclusion for flake8 module level import not at top of fileCloses #3075 from arcward/AIRFLOW-2147	2
Use sys.exit() instead of exit() (#12084)The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.	5
[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules* adde deprecation warnings to the contrib modules* fixed tests* updated UPDATING.md	5
Container specific extra environment variables (#24784)	5
Better description of UID/GID behaviour in image and quickstart (#15592)* Better description of UID/GID behaviour in image and quickstartFollowing the discussion inhttps://github.com/apache/airflow/discussions/15579seems that the AIRFLOW_UID/GID parameters were not clearlyexplained in the Docker Quick-start guide and some users couldfind it confusing.This PR attempts to clarify it.* fixup! Better description of UID/GID behaviour in image and quickstart	1
Flower should be enabled for CeleryKubernetesExecutor (#13248)	0
Add state details to EMR container failure reason (#19579)	0
Implement mapped value unpacking (#21641)	5
Update instructions of doc updates when removing providers from release (#22199)When removing providers from release it might be that we remmovejust added provider due to bugs found, in which case more detailewdinstructions on what should be removed from prepared dodcumentationis needed.Related to #22197	4
Upgrade API files to typescript (#25098)* Upgrade some of the migrations to ts.* Update types.* Update get type for useConfirmMarkTask.* Migrate useMarkFailedRun* migrate useMarkFailedTask, useMarkSuccessRun, useMarkSuccessTask* Migrate more* Use API types when possible* Migrate more* Add URLSearchParamsWrapper* Migrate more	2
[AIRFLOW-XXX] Remove images related profiling doc (#4599)	2
Merge pull request #114 from mistercrunch/fix_h2mFixing a bad attr name in HiveToMySqlTransfer	0
[AIRFLOW-3541] Add Avro logical type conversion to bigquery hook (#4553)	1
Remove some really old Airflow 1.10 compatibility shims (#22187)We used some Airflow 1.10 compatibility shims in systemtests and Breeze's entrypoints, in order to be able to run1.10 still if needed, but the need for that is completely gonefor months now. Time to remove it has long been overdue.	4
PyDocStyle: Enable D403: Capitalized first word of docstring (#10530)	2
[AIRFLOW-XXX] GCP operators documentation clarifications (#4273)	2
Add better feedback to Breeze users about expected action timing (#23827)There are a few actions in Breeze that might take more or less timewhen invoked. This is mostly when you need to upgrade Breeze orupdate to latest version of the image because some dependedncieswere added or image was modified.While we have improved significantly the waiting time involvednow (and caching problems have been fixed to make it as fastpossible), there are still a few situations that you need to havea good connectivity and a little time to run the upgrade. Whichis often not something you would like to loose your time on ina number of cases when you need to do things fast.Usually Breeeze does not force the user to perform such longactions - it allows to continue without doing them (either bytimeout or by letting user answer "no" to question asked.Previously Breeze have not informed the user about the exepctedtime of running such operation, but with this change it tellswhat is the expected delay - thus allowing the user to makeinformed action whether they want to run the upgrade or not.	1
[AIRFLOW-942] Add mytaxi to Airflow usersCloses #2111 from terezaif/patch-1	1
Adds the ability to use environment variables to get databaseconfigurations instead of storing it.	5
Azure: New sftp to wasb operator (#18877)* Azure: New sftp to wasb operatorCo-authored-by: Guilherme da Silva Goncalves <guilherme.goncalves@bancointer.com.br>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>	1
Production image is now built automatically in Dockerhub (#8314)	2
Fixed utf encoding of source code	0
[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)	5
[AIRFLOW-3174] Refine Docstring for SQL Operators & Hooks (#4043)Add missing arguments	1
Gix success endpoint for @once DAGs	2
[AIRFLOW-5587] Move airflow.contrib.task_runner.cgroup_task_runner to core (#6248)	1
Fix PR label detection in CI (#25148)Label detection for incoming PR had few bugs:* wrong name of output was used for Build Info* assumption in selective checks was that PR labels are  space separated, but they were really array formatted.* there was a $ typo in build-images.yamlThis PR fixes all that:* output name and typo is corrected* we use ast.literal_eval now to parse the PR labels.	1
The code was taken from: https://github.com/wndhydrnt/airflow/tree/docker_operatorCredit to: @wndhydrntThis branch, came to solve the CI problems.	0
Pass queue to BaseExecutor.execute_async like in airflow 1.10 (#14861)Any schedulers depending on the queue functionality that haven't overridden`trigger_tasks` method will see queue functionality break when upgrading to 2.0	4
Renames main workflow to `Tests` (#17650)This is a long-overdue change for CI workflows. Since we are buildingimages in a separate workflow, the `CI Builds` name of the workflowwas - first of all misleading, and secondly - too long. The workflownames displayed in the GitHub UI contains the workflow name as prefixso having as short as possible name is an advantage.The `Tests` names seems to be appropriate because this is in factwhat we do in this workflow.The change updates the name of workflow as well as documentationthat referred to it and fixes a few inconsistencies found innames of the `Build Image` -> `Build Images` workflow.The sequence diagrams showing the CI workflow have been alsoregenerated with the new name (thanks to mermaid it was super-easy)	1
Fix release check script (#19238)There have been some changes to the filename conventions over time  and the release check script was not updated to reflect this.  This PR fixes the script and tries to simplify it a little bit.  In particular, the regex approach used previously was broken by the removal of the `-bin` identifier.  It is easy enough to simply compute all the expected files exactly and look for them, so that is what we do here	2
[AIRFLOW-634] Add lumoslabs to readmeCloses #1887 from rfroetscher/add_lumoslabs	1
[AIRFLOW-5479] Normalize gcp_conn method in GCP Kubernetes Hook (#6099)	1
[AIRFLOW-2203] Cache signature in apply_defaultsCache inspect.signature for the wrapper closure to avoid calling it atevery decorated invocation. This is separate sig_cache created perdecoration, i.e. each function decorated using apply_defaults will havea different sig_cache.	1
[AIRFLOW-4961] Insert TaskFail.duration as int match DB schema column type (#5593)When writing a task failure record, convert 'duration' decimalvalue -> an int before persistence, to remove reliance on thedatabase doing this automatically and gracefully.	5
Disable suppress_logs_and_warning in cli when debugging (#13180)* Disable suppress_logs_and_warning in cli when debuggingIn some cases commands like 'dags list' can be used for debug purposes.The problem is that we are suppresing logs and warnings in some casesto make the output nice and clean. This commit disable this functionalityif logging level is set to DEBUG.* Add verbose flag* fixup! Add verbose flag	1
Use getfqdn to make sure urls are fully qualifiedgethostname only resolves host part while often fully qualified domain names are required.* Resolves #1437	0
Make macros.hive pylint compatible (#10495)	1
[AIRFLOW-3677] Improve CheckOperator test coverage (#4756)Add tests for check_operator module- add missing tests cases for CheckOperator- add missing tests cases for ValueCheckOperator- refactor all three classes- replace **locals in str.format by explicit args	4
Replace `DummyOperator` usage in test_zip.zip and test_zip_invalid_cron.zip (#23123)	3
[AIRFLOW-1634] Adds task_concurrency featureThis adds a feature to limit the concurrency ofindividual tasks. Thedefault will be to not change existing behavior.Closes #2624 from saguziel/aguziel-task-concurrency	4
[AIRFLOW-XXX] Fix DateTime in Tree ViewCloses #2687 from stas-em/www-tree-view-fix-displaytime	0
Allow hvac pakage installation using 'hashicorp' extra (#7915)	1
Merge pull request #675 from bolkedebruin/better_loggingBetter logging	2
Merge pull request #126 from airbnb/rootCarrying the ROOT filter through the DAG views	2
postgres_operator_howto_guide.rst (#23789)Saying "**the** PostgreSQL database" confused me. I thought it was implying that a user could/should connect to the airflow metadata db	5
Improving mysql loads to support numpy.datetime64	5
Make Kubernetes job description fit on one log line (#18377)Currently, when the Kubernetes executor creates a pod it prints a dictionary description of the pod across many lines (can easily be 20+ lines). This is fine if you're reading the log in a stream in a text file, but throws off log search tools like Kibana. A better practice would be to print the whole pod description on a single line. It is quite easy to prettify a dictionary if one wants to see it back in a more human-friendly form with the newlines.This update simply forces the log from this command into a single line.	2
#16976 Add json.dumps() for templated fields objects: 'dict' and 'list' (#17082)	5
[AIRFLOW-XXX] Speed up DagBagTest cases (#3974)I noticed that many of the tests of DagBags operate on a specific DAGonly, and don't need to load the example or test dags. By not loadingthe dags we don't need to this shaves about 10-20s of test time.	3
Update production Helm guide database section to use k8s secret (#19892)Co-authored-by: Bas Harenslak <bas@astronomer.io>	1
Run Airflow package preparation in docker in breeze/CI (#15723)Moved building airflow package to within the container similarlyas we do with provider packages. This has the following advantages:* common environment used to build airflow* protection against leaking SECRET_* variables in CI in case third  party packages are installed* specify --version-suffixes and renaming the packages according  to destination (SVN/PyPI) automatically* no need to have node installed in CI runner* no need to have node installed in DockerHub* no need to install PIP/Python3 in DockerHub runner (currently  Python2 is still default and it fails the build there)* always deleting egg-info and build before the build* cleaning up egg-info and build after the buildAlso following the way providers are released, the documentationis updated to change publishing Airflow using previously votedand renamed packages - the very same packages that were committedto SVN. This way anyone will be able to manually verify that thepackages in SVN are the same as those published in SVN and thereis no need to rebuild the packages when releasing them.	1
Make AirflowJsonEncoder uses Flask's JSONEncoder as a base  (#13050)Flask before 2.0 (unreleased at time of writing) will prefer simplejson if it is installed.But unfortunately simplejson is not compatible with the stock JSONEncoder -- it alwayspasses an encoding argument. Changing the base class for our encoder to be what everFlask is using makes this more resilient.	1
Fix typo in Athena sensor retries (#10079)Understanding that it is an attribute name, which could have downstreamconsequences, correct the spelling of max_retries and reword some of thedocstring.	2
Add csrf	1
[AIRFLOW-3550] Standardize GKE hook (#4364)	1
Quarantine test_cli_webserver_background (#12570)We unquarantined test_cli_webserver_background inhttps://github.com/apache/airflow/pull/12501 but seems like thetest is still flaky: https://github.com/apache/airflow/runs/1443468804#step:6:2531	1
Allowing different executors for the SubDagOperator	2
[AIRFLOW-3249] Make all take the same named `do_xcom_push` flag (#4345)	1
[AIRFLOW-1938] Clean up unused exceptionThere is no longer the possibility of aGitCommandError (since cc4404b5f75)Closes #2898 from wrp/setup	1
Doc Fix around Secret/Connection/Variable (#12571)Documentation fixes/improvements:- For Variables set by Environment Variable,   it was highlighted that it may not appear in the web UI.   But this was not highlighted for Connections set by Environment Variable.   This PR adds this note (in docs/howto/connection/index.rst).- Fix wrong docstring of airflow.secrets.base_secrets.BaseSecretsBackend.get_variable().- The Secret Backends don't properly mentioning Variables in the docstrings   (all the focus was put on Connections only). This PR addresses this.- Other a few minor changes.	4
Add logic to lock DB and avoid race conditionThe scheduler can encounter a queued task twice before thetask actually starts to run -- this locks the task and avoidsthat condition.	1
[AIRFLOW-XXX] Fix inconsistent comment in example_python_operator.py (#4337)	1
Add extra links endpoint (#9475)	2
Version 0.2.3.1 for airenv (internal, not pypi)	5
[AIRFLOW-7073] GKEStartPodOperator always use connection credentials (#7738)	1
Add metric for job start/end task run (#8680)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>	1
[AIRFLOW-1074] Don't count queued tasks for concurrency limitsThere may be orphaned tasks queued but not in arunning dag run thatwill not cleared. We should not count these asthey will interfere.I hate to do this, but I changed my mind oncounting queued tasks.1. Queued tasks that are actually queued generallyget set to running pretty quickly.2. Because of the worker-side check, we won'tactually pass concurrency.I don't think the queued thing is a big dealbecause of this, I'm more worried about orphanedtasks that are in QUEUED state but not in arunning dag_run (so they wont get reset)interfering with concurrency.There may be orphaned tasks queued but not in arunning dag run thatwill not cleared. We should not count these asthey will interfere.Closes #2221 from saguziel/aguziel-concurrency-2	1
Move setting of project ID after activating service account (#17866)Co-authored-by: Dmytro Khimich <khimich@google.com>	1
Add another way to dynamically generate DAGs to docs (#21297)Also, move dynamic DAG generation cases from "best practices" to "how to" section	2
[AIRFLOW-6116] [AIP-21] Rename MLEngine operators (#7021)	1
move isinstance check outside of loop	4
improve typing for openfaas provider (#9883)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
[AIRFLOW-4528] Cancel DataProc task on timeout (#5293)Make DataProc Operators cancel the underlying dataproc job on timeout.	5
Allow airflow.providers to be installed in multiple python folders (#10806)For example, this allows some providers to be installed in site packages(`/usr/local/python3.7/...`) and others to be installed in the user folder(`~/.local/lib/python3.7/...`) and both be importable.If we didn't have code in `airflow/__init__.py` this would be mucheasier to achieve (we simply delete the top level init file would beenough) - but sadly we can't take that route.From the docs of pkgutil: https://docs.python.org/3/library/pkgutil.html#module-pkgutil> This will add to the package’s __path__ all subdirectories of> directories on sys.path named after the package. This is useful if one> wants to distribute different parts of a single logical package as> multiple directories.Tested as follows:```$ pip install /wheels/apache_airflow-2.0.0.dev0-py3-none-any.whl$ ls -ald $(python -c 'import os; print(os.path.dirname(__import__("airflow").__file__))')/providersls: cannot access '/usr/local/lib/python3.7/site-packages/airflow/providers': No such file or directory$ pip install --constraint <(echo 'apache-airflow==2.0.0.dev0') apache-airflow-backport-providers-redis$ pip install --user --constraint <(echo 'apache-airflow==2.0.0.dev0') apache-airflow-backport-providers-imap$ python -c 'import airflow.providers.imap, airflow.providers.redis; print(airflow.providers.imap.__file__); print(airflow.providers.redis.__file__)'/root/.local/lib/python3.7/site-packages/airflow/providers/imap/__init__.py/usr/local/lib/python3.7/site-packages/airflow/providers/redis/__init__.py```	5
log then throw the exception	2
Easy switching between GitHub Container Registries (#14120)This change enables easy switching between GitHub Package Registryand GitHub Container Registry by simply adding GITHUB_REGISTRYsecret to be either `docker.package.github.com` or `ghcr.io`.This makes it easy to switch only by the Apache Airflow repositoryrun builds, as it requires preparation of images (to make thempublic and to add permissions to manage them) after they gotcreated for the first time. GitHub Package Registry worksout-of-the-box but it is less stable and considered a legacy,also it does not allow image retention.Documentation has been updated to reflect the reasoning of choosingthis solution as well as describing maintenance processes aroundimages (including adding new Python version)	1
Add unit tests for PigOperator (#9560)	1
Fix typo in Custom XCom backend (#10588)	2
[AIRFLOW-5125] Add gzip support for AdlsToGoogleCloudStorageOperator (#5737)	1
Moving to from mysql-python to pymysql	4
Fix broken MySQL Migration (#12904)We added a change https://github.com/apache/airflow/pull/12890 to fix type of `source_code` columnin `dag_code` table. But looks like MySQL does not like if we don't specify nullable field.	2
[AIRFLOW-2952] Fix Kubernetes CI (#3957)- Update outdated cli command to create user- Remove `airflow/example_dags_kubernetes` as the dag already exists in `contrib/example_dags/`- Update the path to copy K8s dags	2
[AIRFLOW-5838] Make all __init__ pylint compatible (#6503)	5
Don't schedule runs before the DAG's start_datePreviously the Scheduler would start scheduling immediatelyafter ANY execution date, irrespective of the DAG'sstart_date.	5
Generate constraints in PRs when upgrading dependencies (#20624)The constraints generation was only happening in push/scheduledruns, but sometimes it is useful to check what constraints wouldbe generated even in the PRs that change setup.py/setup.cfgThe change causes constraint generation also in the PRs and onlypushing the updated constraints is not executed in PRs.	5
[AIRFLOW-XXX] add note warning that bash>4.0 is required for docs build script (#6947)	2
Add type annotation to providers/jenkins (#9947)Part of #9708	1
Fixes quarantine parsing teething issues (#10145)* wrong issue id (from tests)* comment field was copied from status	3
perf(BigQuery): pass table_id as str type (#23141)	4
Add note on changes to configuration options (#15696)	5
Added My Money Bank to the list of companies using Apache Airflow (#21306)Co-authored-by: Guillaume Renard <guillaume.renard@mymoneybank.com>	1
Revert "Docs: Fix examples values in docs (#15878)" (#15880)This reverts commit b90bb6ed3a788ab28692a0d18adee4fbbe89ca44.	4
add timeout/fallback to tree traversal	1
Improve getting started section (#10680)	1
Make Migrations 1.10.14 Compatible (#12896)	1
[AIRFLOW-1827] Fix api endpoint date parsing	5
Use rich to render info and cheat-sheet command (#12689)	5
Mark required fields in Forms as required (#12856)We have a number of custom forms that have required fields that weren'texplicitly marked as required.This allowed you to submit the Connection form (for example) withnothing as the Conn Id, leading to an empty string being used as theconnection id. This marks that and all the other required fields asrequired.We also replace DataRequired with InputRequired. The previous one istested the truthyness of the value, rather than just that a value wassubmitted.	3
Limit Google Protobuf for compatibility with biggtable client (#25886)The bigtable client does not work well with protobuf > 3.20.0 andit should be limited until we solve the problem.	0
Merge pull request #204 from mistercrunch/modelview_filterUsing CRUD for main view	1
Graceful scheduler shutdown on error (#18092)closes: https://github.com/apache/airflow/issues/18096This solves a potential rare occurrence of a process deadlock when using logs serving.If the scheduler, for any reason, were to encounter an unrecoverable error (such as a loss of connectivity to the database, or anything forcing the process to exit), the serve_logs subprocess would not be properly terminated.Thus, the process hangs instead of gracefully shutting down, thus requiring external restart.By ensuring the `.terminate()` function is _always_ called in case of a failure, the parent process can properly shut itself down.	0
feat: Add Loadsmart in the list of companies using it (#17792)	1
Fixing minor templating issue in HivePartitionSensor	0
Add multiple roles when creating users (#18617)	1
Forward decorated function type to provide_session reusult (#20131)	1
(docs): update README.md (#17806)* (docs): update README.md- correct pronoun agreement- reduce verbiage- increase readability- correct capitalization- make capitalization cohesive- add punctuation- make `md` section spacing cohesive- make bolding of bullet items cohesive (i.e., **Note:** versus **Note**:)	1
Make sure to add the command	1
Fix typos in README.md and airflow_doc_issue_report.yml (#23294)	5
fix deprecation warning in test_default_views.py (#22346)	3
Merge pull request #10 from mistercrunch/db_connMaking the sqlachemy db connection a config param	2
[AIRFLOW-XXX] Add Jetlore to companies (#4096)	1
Merge pull request #258 from gepser/masterAdded Xoom	1
Update configuration.py	5
Documentation badge	2
[AIRFLOW-4746] Implement GCP Cloud Tasks' Hook and Operators (#5402)Implement GCP Cloud Tasks' Hook and Operators	1
Log task_instance execution duration as milliseconds (#10632)This is best achieved by passing a `timedelta()` to `Stats.timing()`, and leaveworrying about time units to that method.	4
Group UPDATING.md  entries into sections (#10090)	5
Fixes docstring for PubSubCreateSubscriptionOperator (#20237)	1
Add placement_strategy option (#9444)	1
Remove airflow-pr tool  (#10675)* Remove airflow-pr tool* Add PyGithub back in* Remove gitpython	4
Enable annotations to be added to the webserver service (#9776)	1
Fix deprecation warnings location in google provider (#16403)These warnings were being issued from the wrong location, making themhard for any users who hit them to fix```tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/dagbag.py:317: DeprecationWarning: This operator is deprecated. Please use BigQueryUpdateDatasetOperator.    loader.exec_module(new_module)tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_roundtrip_provider_example_dagstests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/baseoperator.py:181: DeprecationWarning: `destination_bucket` is deprecated please use `bucket_name`    result = func(self, *args, **kwargs)tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_roundtrip_provider_example_dagstests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/baseoperator.py:181: DeprecationWarning: `destination_object` is deprecated please use `object_name`    result = func(self, *args, **kwargs)```	1
Update tag color to be neutral (and match DAGs index view) (#12493)	2
HiveServer2 hack to run multi-statement in one session by passing a list	4
Fix some weird English in `create_dagrun` exceptions (#23003)	2
Removing done items from TODO.md	2
closes apache/incubator-airflow#1382 *Won't fix*	0
Airflow tutorial to use Decorated Flows (#11308)Created a new Airflow tutorial to use Decorated Flows (a.k.a. functionalDAGs). Also created a DAG to perform the same operations without usingfunctional DAGs to be compatible with Airflow 1.10.x and to show thedifference.* Apply suggestions from code reviewIt makes sense to simplify the return variables being passed around without needlessly converting to JSON and then reconverting back.* Update tutorial_functional_etl_dag.pyFixed data passing between tasks to be more natural without converting to JSON and converting back to variables.* Updated dag options and task doc formatingBased on feedback on the PR, updated the DAG options (including schedule) and the fixed the task documentation to avoid indentation.* Added documentation file for functional dag tutorialAdded the tutorial documentation to the docs directory. Fixed linting errors in the example dags.Tweaked some doc references in the example dags for inclusion into the tutorial documentation.Added the example dags to example tests.* Removed multiple_outputs from task defnHad a multiple_outputs=True defined in the Extract task defn, which was unnecessary. - Removed based on feedback.Co-authored-by: Gerard Casas Saez <casassg@users.noreply.github.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	1
Update `airflow tasks *` commands to lookup TaskInstances from DagRun Table (#16030)This change allows to lookup TaskInstances using DagRun.run_id in task commandsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	2
[AIRFLOW-6025] Add label to uniquely identify creator of Pod (#6621)	1
Add Dataproc SparkR Example (#8240)* GCP SparkR ExampleAllows you to schedule R, and sparkR jobs on a dataproc cluster.The functionality to run that kind of job is already in dataproc,but it was not so clear how to do that from Airflow.* Update airflow/providers/google/cloud/example_dags/example_dataproc.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Make sure R file finds correct libraryCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>	2
Add Task Logs to Grid details panel (#24249)* WIP* Add Common LogLink component.* Split details in two columns.* Retrieve task log url from metadata.* Checkbox for requesting full logs, add tabs.* Persist tab preference into the local storage.* Task group tab fallback fix.* Simple LinkButton test for shared component.* Use codeblock component and auto scroll to bottom* Add checkbox for line wrapping toggle* Remove animation scroll into view, fix logs mapped tasks.* Add more tests.* Fix replace issue for certain tasks.* Add LogLink Internal test.	3
Return output of last task from task_group wrapper. (#15779)	5
Move tests for airflow.utils.dates out of tests/core/ (#13088)These three functions were in test_core, but the separate test_datesfile is better suited.In addition I have removed the use of `assert_array_almost_equal` fromnumpy as pytest provides it's own version	1
Reduce lengths of the name, username and email fields for this test (#18263)	3
Rewrite taskflow-mapping argument validation (#21759)	5
Fix MyPy errors in leveldb (#20222)Part of #19891	5
Improve handling edge-cases in airlfow.models by applying mypy (#20000)* Fix many of the mypy typing issues in airflow.models.dagAnd to fix these, I needed to fix a few other mistakes that areused/called by DAG's methods* Fix timetable-related typing errors in dag.pyAlso moved the sentinel value implementation to a utils module. Thisshould be useful when fixing typing issues in other modules.* Add note about assert allowed inside a TYPE_CHECKING conditional* Fix docs build of airflow.models.dagrun* Apply NEW_SESSION to dag, dagrun, ti and operator.subdagCo-authored-by: Tzu-ping Chung <tp@astronomer.io>	2
[AIRFLOW-1909] Add away to list of usersCloses #2868 from trunsky/patch-1	1
Breeze: More fancy environment checking (#10329)* More fancy environment checking* fixup! More fancy environment checking	0
Allow disabling periodic committing when inserting rows with DbApiHook```>>> i = 1>>> commit_every = 0>>> bool(i % commit_every == 0)  # previouslyTraceback (most recent call last):  File "<stdin>", line 1, in <module>ZeroDivisionError: integer division or modulo by zero>>> bool(commit_every and i % commit_every == 0)  # with this changeFalse```	4
[AIRFLOW-XXX] Add SocialCops to Airflow usersCloses #3018 from vinayak-mehta/update_readme	5
Add robots.txt and X-Robots-Tag header (#17946)Co-authored-by: thejens <jens.larsson@tink.com>	5
[AIRFLOW-XXXX] Update autolabeler config (#7379)	5
[AIRFLOW-7062] Fix pydruid release breaking the build (#7720)	4
Enforce READ COMMITTED isolation when using mysql (#15714)* Enforce READ COMMITTED isolation when using mysql* Fixing up tests, removing indentation* Fixing test	3
[AIRFLOW-175] Run git-reset before checkout in PR toolIf the user made any changes, git checkout will fail because thechanges would be overwritten. Running git reset blows the changes away.	4
Add ClusterPolicyViolation support to airflow local settings (#10282)This change will allow users to throw other exceptions (namely `AirflowClusterPolicyViolation`) than `DagCycleException` as part of Cluster Policies.This can be helpful for running checks on tasks / DAGs (e.g. asserting task has a non-airflow owner) and failing to run tasks aren't compliant with these checks.This is meant as a tool for airflow admins to prevent user mistakes (especially in shared Airflow infrastructure with newbies) than as a strong technical control for security/compliance posture.	1
Upgrade the Dataproc package to 3.0.0 and migrate from v1beta2 to v1 api (#18879)	5
Add typing for jira provider (#10005)	1
[AIRFLOW-XXX] Update kubernetes.rst (#4280)import modules to complete the example set.	1
[AIRFLOW-412] Fix lxml dependencyDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-412Testing Done:-NoneCloses #1722 from normster/lxml	5
Improve UI file naming/patterns (#12486)* Use friendlier terms for file naming* Correlate asset names to template names	1
Ensure deps is set, convert BaseSensorOperator to classvar (#21815)	1
Rewrite DAG run retrieval in task command (#20737)	1
Replace Stale Bot with Stale Github Action (#14494)Looks like Stable Bot is not working anymore for us, so let's replace it with Github Action that runs everyday at 00:00	1
Jeremiah Lowin has resigned from the Airflow project (#13486)	5
Use relation in TI join to RTIF (#22159)Did not include map index; replaced with relation.	1
Move "additional" build args from required to optional in Breeze (#25567)The "required" build args in Breeze are replaced with empty`--build-arg arg=`. This is problematic if those parameters willhave default values set. We move them from required to optionalto skip the build args entirely when "build" command is run.	1
Circle fill color in tree view	5
Merge pull request #561 from airbnb/fix_schedulerMaking the end of celery_executor async by default	0
Treat `AirflowSensorTimeout` as immediate failure without retrying (#12058)## Expected behaviourFor a sensor like this, the intention of the DAG author is usually to fail the sensor if it's still not done after ten minutes. However, if the sensor fails prematurely due to other unexpected reasons (such as network outage), retry at most twice.```pythonsensor = PythonSensor(    task_id='sensor',    python_callable=python_callable,    timeout=60 * 10,    retries=2,    mode="reschedule",)```## Actual behaviourThe actual current behaviour of Airflow is to retry when the sensor times out. So the effective timeout of the sensor becomes 60 * 10 * (retries + 1) = 30min. This often causes confusion. It also makes it impossible to achieve the expected behaviour no matter how the author configures the sensor.## FixThis PR fixes this issue. `AirflowSensorTimeout` is now treated as immediate failure. This achieves the expected behaviour. The sensor will fail if timeout is reached. If someone really wants the previous behaviour, he can always increase the timeout. I.e instead of failing and retrying every ten minutes three times, just set the timeout to 30min.	1
[AIRFLOW-2895] Prevent scheduler from spamming heartbeats/logsReverts most of AIRFLOW-2027 until the issues withit can be fixed.Closes #3747 fromaoen/revert_min_file_parsing_time_commit	4
[AIRFLOW-XXX] Update the UPDATING.md file for 1.10.2	2
Allow celery workers without gossip or mingle modes (#13880)Arguments: --without_mingle and --without_gossip	1
Ensure that `dag_run.conf` is a dict (#15057)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	5
specify constraint key type & drop auto fkey referred to users tables (#13239)	1
Fix setup.py to install the right provider for mysql (#12476)	1
Increase typing for Apache and http provider package (#9729)	1
Add type annotations for redis provider (#9815)	1
[AIRFLOW-6424] Added a operator to modify EMR cluster (#7213)* [AIRFLOW-6424] Added a operator to modify EMR cluster* [AIRFLOW-6424] Updated docs for EMR modify cluster operator* [AIRFLOW-6424] Removed sanity check	4
[AIRFLOW-XXX] Fix docstrings for CassandraToGoogleCloudStorageOperator (#5103)	1
Merge pull request #724 from RealImpactAnalytics/svv_variablesfixed Variable json deserialization	5
postgres_hook_aws_conn_id (#16100)	1
Fix QueuedLocalWorker crashing with EOFError (#13215)LocalExecutor uses a multiprocessing.Queue to distribute tasks to theinstances of QueuedLocalWorker. If for some reason LocalExecutor exits(e.g. because it encountered an unhandled exception), then each of theQueuedLocalWorker instances that it manages will also exit while tryingto read from the task queue.This obfuscates the root cause of the issue, i.e. that the LocalExecutorterminated. By catching EOFError, logging an error and exiting gracefullywe circumvent this issue.	0
Next run datasets tooltip preview (#25694)* create dataset triggered preview tooltip* share datasets data btwn tooltip+modal* add backup tooltip message	1
Name and optionally preserve data volumes in Breeze (#11628)So far breeze used in-container data for persisting it (mysql redis,postgres). This means that the data was kept as long, as long thecontainers were running. If you stopped Breeze via `stop` commandthe data was always deleted.This changes the behaviour - each of the Breeze containers hasa named volume where data is kept. Those volumes are also deletedby default when Breeze is stopped, but you can choose to preservethem by adding ``--preserve-volumes`` when you run ``stop`` or``restart`` command.Fixes: #11625	0
[AIRFLOW-1294] Backfills can loose tasks to executeIn backfills we can loose tasks to execute due toa tasksetting its own state to NONE if concurrencylimits are reached,this makes them fall outside of the scope thebackfill ismanaging hence they will not be executed.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [X] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, "[AIRFLOW-XXX] My Airflow PR"    -https://issues.apache.org/jira/browse/AIRFLOW-1294### Description- [X] Here are some details about my PR, includingscreenshots of any UI changes:In backfills we can loose tasks to execute due toa tasksetting its own state to NONE if concurrencylimits are reached,this makes them fall outside of the scope thebackfill ismanaging hence they will not be executed.### Tests- [X] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Should be covered by current tests, will adjust ifrequired.### Commits- [X] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from"[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood ("add", not"adding")    5. Body wraps at 72 characters    6. Body explains "what" and "why", not "how"mistercrunch aoen saguziel This is a simplifiedfix that should be easier to digest in 1.8.2. Itdoes not address all underlying issues as inhttps://github.com/apache/incubator-airflow/pull/2356 , but those can be addressedseparately and in smaller bits.Closes #2360 from bolkedebruin/fix_race_backfill_2	0
Remove references to airflow.contrib in tests (#7882)	3
[AIRFLOW-2657] Add ability to delete dag from web UICloses #3531 from Noremac201/master	2
Use crsf token in forms, fix missing divUse lxml for parsing the form to obtain csrf for testing	3
AIRFLOW-[3823] Exclude branch's downstream tasks from the tasks to skip (#4666)	5
add hostnfly as users of airflowCloses #2845 from alexisrosuel/master	1
Fix broken Kubernetes PodRuntimeInfoEnv (#10478)closes https://github.com/apache/airflow/issues/10456	0
[AIRFLOW-665] Fix email attachmentsContent-Disposition must be set separately onthe MIMEApplication. Passing it to theconstructor just puts it in the Content-Type header.Closes #1916 from dgies/master	4
Make ``delete_pod`` change more prominent in K8s changelog (#20753)	4
[AIRFLOW-6055] Option for exponential backoff in Sensors (#6654)A new option "exponential_backoff" in Sensors, will increase the next poke or next reschedule time for sensors exponentially. Turned off by default.	1
Remove selective checks from the "release workflow" (#24655)Missed that one too :(	1
Add migration guide for CLI commands (#10078)	1
Fix dag_processing.last_duration metric random holes (#17769)* Fix dag_processing.last_duration metric random holes* Fix test* Fix mssql+sqlite test* move dag_processing.last_duration timing to _collect_results_from_processor	2
fixing s3 sensor missing '/'	0
New design of system tests (#22311)Migrate BigQuery system tests to new design (See AIP-47 for details)	1
[AIRFLOW-3062] Add Qubole in integration docs (#3946)	2
Update errors.rst (#24412)Fix wording in some sentences	0
Increase timeout for providers checks for 2.2 (#25779)Seems that we started to have problems with longer backtracking whenwe install new providers for Airflow 2.2. This is an attempt tostabilize it before we investigate the root cause.	1
Do not include mypy volume by default (#25958)MyPy volume was included by default in shell command, but it isnot needed and might lead to missing mypy-cache volume problem.It is useful for debugging mypy problems so it is still useful tohave it as an option of shell command.	1
[INTHEWILD] Update EBANX company users (#21220)	1
Resurface S3Log class eaten by rebase/push -fDuring my refactoring of utils, I rebased several times,it seems however that the S3Log class was not ported to the newutils structure. This PR fixes this.	0
Serve logs with Scheduler when using Local or Sequential Executor (#15557)Currently, the `serve_logs` endpoint only exists on Celery workers. Thismeans if someone launches Airflow with the `LocalExecutor` and wants tograb the logs from the scheduler, there is no way to move that to thewebserver if it is on a different pod/machine.This commit makes the scheduler automatically serves logs when using`LocalExecutor` or `SequentialExecutor`. However, it means forAirflow <= 2.0.2, the Helm Chart won't serve logs.closes https://github.com/apache/airflow/pull/15070closes https://github.com/apache/airflow/issues/13331closes https://github.com/apache/airflow/issues/15071closes https://github.com/apache/airflow/issues/14222	0
[AIRFLOW-5676] Rename CloudSpannerHook to SpannerHook (#6409)	1
Unpin ``pandas-gbq`` and remove unused code (#21915)* Unpin ``pandas-gbq`` and remove unused code`BigQueryPandasConnector` was previously used by `BigqueryHook.get_pandas_df`. This was fixed in https://github.com/apache/airflow/commit/ad308ea441372f2b44b4292c3779eb745f2ed48c (**in 2018**). However we forgot to remove `BigQueryPandasConnector` which is age-old code (2016) and used private methods.2016 code - https://github.com/apache/airflow/pull/1452/files	2
[AIRFLOW-256] Fix test_scheduler_reschedule heartratetest_scheduler_reschedule runs two schedulerjob quitefast after one another this sometimes is faster thanthe heartrate allows and thus the tasks will not getrescheduled and the test will fail. Fixed by settingheartrate to 0.	1
[AIRFLOW-1034] Make it possible to connect to S3 in sigv4 regionsCloses #2181 from buyology/fix-s3-in-sigv4-regions	2
Update README.md file in /templates/variables	2
[AIRFLOW-647] Restore dag.get_active_runsSimply added a getter back to dag that returns thelist of active dag run execution dates for the dagfrom the DB.Closes #1899 frombtallman/RestoreActiveRuns_feature	1
[AIRFLOW-2473] Fix wrong skip condition for TransferTestsThis PR fixes wrong @skipUnlessImported whichdecoratesTransferTests and does minor refactoring.Closes #3411 from sekikn/AIRFLOW-2473	4
[AIRFLOW-1045] Make log level configurable via airflow.cfgFor now, changing log level needs to modifysettings.py directly.It's inconvenient. This PR makes it configurablevia airflow.cfg.Closes #2191 from sekikn/loglevel	2
[AIRFLOW-5942] Pin PyMSSQL to <3.0 (#6592)	5
Making hive/presto tests optional	3
Support google-cloud-monitoring>=2.0.0 (#13769)	1
Fix spelling (#15699)Fix spelling of directory and PNG file name	2
[AIRFLOW-4681] Make sensors module pylint compatible (#7309)Remove all references to sensor modules from pylinttodo.txt and beginmaking changes, including using local variables where class ones are notneeded. Where possible, clarify some local variables names, such as thesnakebite client in the HDFSSensor module.Ignore corresponding pylint checks for higher impact code.	1
WTForms 2.3.0 break our Flask apps (#8512)* WTForms 2.3.0 break our Flask apps	4
Fix exception in mini task scheduler. (#24865)I introduced a bug in 2.3.0 as part of the dynamic task mapping workthat frequently made the mini scheduler fail for tasks involvingXComArgs.The fix is to alter the logic in BaseOperator's deepcopy to not set the`__instantiated` flag until all the other attributes are copied.For background the `__instantiated` flag is use so that when you do`task.some_attr = an_xcom_arg` the relationships are set appropriately,but since we are copying all the existing attributes we don't need to dothat, as the relationships will already be set!	1
Chart: Update Webserver update strategy based on Airflow Version (#15627)This commit adds the following things:- Add "airflowVersion" flag that will allow use to add some componentsthat are just available or work with certain Airflow version.Example: pod_template_file is available for Airflow >= 1.10.12- Update logic for selecting pre/post Airflow 2.0 CLI commands basedon that flag- Updates stragtegy of Airflow Webserver based on that flag as thewebserver in Airflow >= 2 does not need access to DAG files, hencewe don't need to recreate but can have a "true" rollingUpdate- Allow overriding webserver udpate strategy	1
add verbose to pip install	1
Making the scheduler more resilient	1
Runs scheduled quarantine tests always (#13288)Fixes: #12909	0
Use Pendulum's built-in UTC object (#21732)Co-authored-by: Tzu-ping Chung <tp@astronomer.io>	1
[AIRFLOW-3237] Refactor example DAGs (#4071)	2
fix DagBag.get_dag() for non existing dag_id- calling .get_dag('non existing dag_id') was previously leading to a crash related to a call to None.fileloc- the rest of the existing logic is already returning None for other case of Dag not found => fixing the bug by aligning the logic on that	2
[AIRFLOW-2805] Display multiple timezones on UI (#3687)	5
Don't display when None (#12415)	5
[AIRFLOW-5204] Shellcheck + common licences + executable shebangs in shell files (#5807)* [AIRFLOW-5204] Shellcheck + common licence in shell files	2
Fix and unquarantine TestDagFileProcessorAgent.test_parse_once (#10862)The SmartSensor PR introduces slightly different behaviour onlist_py_files happens when given a file path directly.Prior to that PR, if given a file path it would not include examples.After that PR was merged, it would return that path and the example dags(assuming they were enabled.)	0
[AIRFLOW-4084] fix ElasticSearch log download (#5177)	2
[AIRFLOW-2765] Set default mime_charset to UTF-8Closes #3627 from jeffkpayne/AIRFLOW-2765	1
Clarified installation docs around worker reqs	1
Pin github checkout action to v2 (#9938)	5
Add clear logging to tasks killed due to a Dagrun timeout (#19950)When a DagRun exceeds its `dagrun_timeout` value a few things happen: - The run is marked as `failed` - All unfinished tasks are marked as `skipped` (which causes running tasks to be SIGTERM'd) - A line is logged in the scheduler logs: `INFO: Run $RUN_NUMBER of $DAG_ID has timed-out` This has caused some confusion amongst users as its hard to tell why running tasks were killed without either:1) Cross-referencing the `dagrun_timeout` value with the execution time2) Reading the scheduler logs. This PR adds additional messaging into the task logs when it can be inferred that the task was killed due to a DagRun timeout. I'm not super happy with this implementation (as it kind of duplicates the timeout logic to infer that a timeout occurred) and would really appreciate some advice on other ways we can improve the clarity around timeouts. I'll add some tests for this once I get some feedback on the initial approach.	5
Fix webserver ingress annotations (#12619)The indentation under `web.annotations` was wrong (6 leading spaceson first line, 4 on the rest) leading to    Error converting YAMl to JSON: yaml: line 32: did not find expected keywhen you run helm chart with value `ingres.web.annotations`	2
Add support for non-default orientation in `dag show` command (#8834)	2
[AIRFLOW-718] Allow the query URI for DataProc PigThe query URI parameter was missing for theDataProc pig operator. Withthe addition of the parameter you can now storethe pig script in CloudStorage.Closes #1960 from alexvanboxel/feature/dataproc-pig-uri	5
includes the STS token if STS credentials are used (#11227)as per AWS docs https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-authorization.html#copy-credentials	3
[AIRFLOW-759] Use previous dag_run to verify depend_on_pastThe start_date and the schedule interval can be misaligned. Thisis automatically corrected in the scheduler. The dependency checkerhowever did not do this.	5
[AIRFLOW-XXX] Fix Typo & formatting in Updating.md (#5073)	5
[AIRFLOW-5465] Fix deprecated imports in examples (#6082)	2
[AIRFLOW-XXX] Update docstring for SchedulerJob (#5105)	2
[AIRFLOW-6733] Extend not replace template (#7366)* [AIRFLOW-6733] Extend, rather than replace, the base_templateThis commit just moves the existing template to airflow/master.html(without further changes)* [AIRFLOW-6733] Only change blocks we have customizedThis makes is easier to see which parts of the template we have changed.	4
Fix commands in docs/usage-cli.rst(#11847)* Add uSmart Securities to the INTHEWILD.md* change airflow dag to airflow dags in 2.0	2
BugFix: K8s Executor Multinamespace mode is evaluated to true by default (#10410)	0
Fix some Changelog entries (#19604)Some changelog entries were not formatted correctly	4
Fix Grid autoscroll with ResizeObserver (#23022)	0
Fix Flower network policy for CeleryKubernetesExecutor (#13301)	1
Increse number of runs for quarantined tests (#10220)	3
Replace INTHEWILD sorting with python (#26137)Part of #26020	5
bug fix for vertica_to_hvie	0
Merge pull request #884 from airbnb/retry_incFixing issue where try_number isn't incremented	1
[AIRFLOW-5119] Enable building from scratch in CRON jobs (#5733)	0
Merge pull request #188 from mistercrunch/S3FileTransferOperatorAdded new operator S3FileTransformOperator	2
simpler way to protect against bytes	2
[AIRFLOW-3932] Optionally skip dag discovery heuristic. (#4746)	2
[AIRFLOW-6820] split breeze into functions (#7433)	1
Ensure that `airflow dags tests` works for mapped DAGs (#21969)	2
Fix docker "after entrypoint" custom script example (#19495)	1
Untangle airflow/decorators/__init__.py[i] names (#21056)This by large distches the confusing "factory" thing, and use"collection" to name the type of '@task'. This should hopefullyclarify things up a bit.	1
update unit tests by removing /action endpoint	4
Remove turbaszek from CODEOWNERS (#17189)Due to limited time capacity I would like to reduce number of reviews I get so I can realy help.	1
Update models.py to increase password field lengthIncrease length to allow for RSA keys and such.	1
[AIRFLOW-273] Create an svg version of the airflow logo.Closes #1619 from gwax/svg_logo	2
Now caching/pickling jinja template objects	5
Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876)2.0.1 was missing from the breeze-complete list and the docs	2
Add `extraVolumeMounts` to flower (#22414)	1
[AIRFLOW-700] Update to reference to web authentication documentationCloses #1943 from alanmcruickshank/config_update	5
Call scheduler "book-keeping" operations less frequently. (#12139)This change makes it so that certain operations in the scheduler arecalled on a regular interval, instead of only once at start up, or everytime around the loop:- adopt_or_reset_orphaned_tasks (detecting SchedulerJobs that died) was  previously only called on start up.- _clean_tis_without_dagrun was previously called every time around the  scheduling loop, but this isn't so needed to be done every time as  this is a relatively rare cleanup operation- _emit_pool_metrics doesn't need to be called _every_ time around the  loop, once every 5 seconds is enough.This uses the built in ["sched" module][sched] to handle the "timers".[sched]: https://docs.python.org/3/library/sched.html	2
[AIRFLOW-XXX] Add Feng Tao to committers list (#3689)	1
Improve error message for BranchPythonOperator when no task_id to follow (#18471)* Improve error message for BranchPythonOperator when no task_id to follow	1
[AIRFLOW-4763] Allow list in DockerOperator.command (#5408)	2
import basestring for py3 compatibility	2
[Doc] Replace module path to Class with just Class Name (#13719)Instead of `the airflow.executors.sequential_executor.SequentialExecutor`just have `SequentialExecutor with the link to the actual class.	2
[AIRFLOW-4906] Improve debugging for the SparkSubmitHook (#5542)	1
Change `ds`, `ts`, etc. back to use logical date (#19088)	5
Improve documentation in docs/start.rst (#10243)`lay` -> `create`Remove ` # if you build with master` as this is the document for Master itself	2
Fix edge case in queue management	0
Adds Bwtech and Inter Platform Inc. to the list of companies using Apache Airflow (#18876)Co-authored-by: Guilherme da Silva Goncalves <guilherme.goncalves@bancointer.com.br>	1
Merge pull request #662 from airbnb/fix_docsFixing the function headers for decorated functions in the docs	2
All versions in CI yamls are not hard-coded any more (#10959)GitHub Actions allow to use `fromJson` method to read arraysor even more complex json objects into the CI workflow yaml files.This, connected with set::output commands, allows to read thelist of allowed versions as well as default ones from theenvironment variables configured in./scripts/ci/libraries/initialization.shThis means that we can have one plece in which versions areconfigured. We also need to do it in "breeze-complete" as this isa standalone script that should not source anything we addedBATS tests to verify if the versions in breeze-completecorrespond with those defined in the initialization.shAlso we do not limit tests any more in regular PRs now - we runall combinations of available versions. Our tests run quite abit faster now so we should be able to run more completematrixes. We can still exclude individual values of the matrixesif this is too much.MySQL 8 is disabled from breeze for now. I plan a separate followup PR where we will run MySQL 8 tests (they were not run so far)	1
[AIRFLOW-4160] Fix redirecting of 'Trigger Dag' Button in DAG Page (#4981)	2
Fix check_integration pre-commit test (#9869)	3
Add Workflow to delete old artifacts (#11064)	4
AIRFLOW-5492: added missing docstrings (#6107)	2
Chart: Add loadBalancerSourceRanges in webserver and flower services (#17666)	5
[AIRFLOW-4197] Remove Python2 CI jobs (#5022) (#5021)	4
Including reqs to build	5
[AIRFLOW-790] Clean up TaskInstances without DagRunsCloses #2886 from gwax/AIRFLOW-790_guard_against_orphans	2
Correcting wrong reference in docs from  to	2
[AIRFLOW-461] Restore parameter position for BQ run_load method (#4077)	1
Remove unused constant in serialization code (#15824)This was added back in by a PR that added links to Google DataProcoperators, but this is not used anymore, instead the code looks at theprovider registry for links.	2
Ensure target_dedicated_nodes or enable_auto_scale is set in AzureBatchOperator (#11251)	1
Bugfix: Don't warn on using ``LocalExecutor`` (#18625)The webserver showed the following error when using ``LocalExecutor``:```Do not use SequentialExecutor in production. Click here for more information.```	5
Made use of authentication consistent (#10610)Fixed a couple of places where authorization was used instead of authentication	1
Restrict changing XCom values from the Webserver (#9614)	4
Add Neo4j hook and operator (#13324)Close: #12873	1
Open src and dst in binary for samba copy (#18752)	5
Task Instance Modal UX Enhancements (#10944)* Improve modal UX with logical form ordering, semantic form elements, visual hierachy tweaks* make modal header prefix dynamic if SUBDAG* Add heading as demarcation between action sections within modal* Update doc screenshot w/ added modal heading	1
Use fab models (#19121)* Use FAB models.* Use FAB models.* Remove incorrect conversions to new permission naming scheme.* Fix missing FAB renames.* Remove unused FAB compatibility fixes in models.py.* Remove additional uses of view_menu.* Move airflow/www imports from global to function local.	1
Merge pull request #172 from james-woods/masterAdding Wooga to the list of companies using Airflow in the readme	1
Bugfix around dag navigation on DAG view	2
Merge pull request #773 from abridgett/feature/make_smtp_auth_optionalset default smtp_user, smtp_password so they are not needed in user config	5
Revert recent breeze changes (#10651 & #10670) (#10694)* Revert "Add packages to function names in bash (#10670)"This reverts commit cc551ba793344800d2d396c13d7fd0c8eed97352.* Revert "Implement Google Shell Conventions for breeze script … (#10651)"This reverts commit 46c8d6714c981746cc114b8b1af5cb27aa0018e2.	4
Merge pull request #1041 from caseybrown89/masterset celery_executor to use queue name as exchange	4
[AIRFLOW-XXX] Fix docstring minor issues in airflow/kubernetes/ (#6708)* Correct typo in pod_launcher* Remove param `secrets` in docstring since it's not specified/used* Always use triple double quotes around docstrings https://www.python.org/dev/peps/pep-0257/For consistency, always use """triple double quotes""" around docstrings	2
Disable SLAs for mapped operators (#22641)When trying to update SLA logic to handle mapped operators we discovered some odd behavior and decided to defer adding support for SLAs with mapped tasks.	1
Feature: Auto-refresh Graph view chart (#11534)	2
Clarifying a few operators docstrings	2
Linting	5
[AIRFLOW-5384] Improve dst param info in FileToGCSOperator (#5985)This commit add more info about dst parameter to indicate that the pathmust include file name.	2
Merge pull request #186 from mistercrunch/cssNew boostrap theme	1
🔒 Fix missing HTTPS on airflow site links (#13043)	2
pluggable executor needs to be instantiated	2
Add pre-commit hook limiting hook name length (#13319)* When hook names are too long, pre-commit dispay becomes very ugly with many blank linesCo-authored-by: Daniel Standish <dstandish@techstyle.com>	1
[AIRFLOW-6216] Allow pytests to be run without "tests" (#6770)With this change you should be able to simply run `pytest` to run all the tests in the main airflow directory.This consist of two changes:* moving pytest.ini to the main airflow directory* skipping collecting kubernetes tests when ENV != kubernetes	3
Bring MappedOperator members in sync with BaseOperator (#24034)	1
Remove unused import (#12371)	2
Add blue-yonder to Airflow usersCloses #1661 from ctrebing/extend_list_of_companies_blue_yonder	1
Allowing HiveServer2Hook to work with empty resultset	1
Move test tools from tests.utils to tests.test_utils (#10889)	3
Fix handling of GitHub Event types for new selective checks (#24665)One more finding after merging the selective checks in Python- I missed a case of "pull_request_target".Fixed and added more tests.	3
Use google cloud credentials when executing beam command in subprocess (#18992)	1
Remove unnecessary python 3.6 conditionals (#20549)Since Python 3.7 is now the lowest supported version, we no longer needto have conditionals to support 3.6.	1
[AIRFLOW-1367] Pass Content-IDTo reference inline images in an email, we need to be able to add<img src="cid:{}"/> to the HTML. However currently the Content-ID (cid)is not passed, so we need to add itCloses #2410 from aliceabe/master	1
close apache/incubator-airflow#1340 *superseded by other PRs*	5
[AIRFLOW-XXX] Add JULO to company list in readme (#5062)	1
[AIRFLOW-XXX] Add City of Toronto to official users list (#5526)[ci skip]	1
Merge pull request #1044 from r39132/masterUpdating the Readme with a link to the TriggerDagRunOperator post	2
Allow for uploading metadata with GCS Hook Upload (#22058)	1
Use default view in TriggerDagRunLink (#11778)	2
[AIRFLOW-59] Implement bulk_dump and bulk_load for the Postgres hookThis PR implements bulk_dump and bulk_load,which are inherited from DbApiHook andalready implemented for MySqlHook.Closes #3456 from sekikn/AIRFLOW-59	1
Merge pull request #221 from mistercrunch/artwr/s3_fuzzy_key_matchingAdding wildcard matching for S3 hook and operators	1
Add ContaAzul as an Airflow userCloses #2566 from sabino/patch-1	1
Fixed numeric list (#17813)	0
Merge pull request #181 from mistercrunch/debug_schedulerLogging master scheduler lags	2
[AIRFLOW-776] Add missing cgroups devel dependencyCloses #2009 from aminghadersohi/master	1
Add documentation links to README	2
Improve dag/task concurrency check (#17786)Currently, tasks can be run even if the dagrun is queued. Task instances of queued dagrunsshould only be run when the dagrun is in running state. This PR makes sure tis of queued dagrunsare not run thereby properly checking task concurrency.Also, we check max_active_runs when parsing dag which is no longer needed since dagrunsare created in queued state and the scheduler controls when to change the queued dagrunsto running considering the max_active_runs.This PR removes the checking of max_active_runs in the dag too.	2
Enable Github issues (#7779)As we don't want people using GitHub issues for asking for help, I havetried to make this clear in the issue template.Lets see if it works or not...	1
[AIRFLOW-3602] Improve ImapHook handling of retrieving no attachments (#4475)	1
Add read-only REST API endpoint for Datasets (#24696)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Jed Cunningham <jedcunningham@apache.org>	5
More customizable build process for Docker images (#11176)* Allows more customizations for image building.This is the third (and not last) part of making the Productionimage more corporate-environment friendly. It's been preparedfor the request of one of the big Airflow user (company) thathas rather strict security requirements when it comes topreparing and building images. They are committed tosynchronizing with the progress of Apache Airflow 2.0 developmentand making the image customizable so that they can build it usingonly sources controlled by them internally was one of the importantrequirements for them.This change adds the possibilty of customizing various steps inthe build process:* adding custom scripts to be run before installation of both  build image and runtime image. This allows for example to  add installing custom GPG keys, and adding custom sources.* customizing the way NodeJS and Yarn are installed in the  build image segment - as they might rely on their own way  of installation.* adding extra packages to be installed during both build and  dev segment build steps. This is crucial to achieve the same  size optimizations as the original image.* defining additional environment variables (for example  environment variables that indicate acceptance of the EULAs  in case of installing proprietary packages that require  EULA acceptance - both in the build image and runtime image  (again the goal is to keep the image optimized for size)The image build process remains the same when no customizationoptions are specified, but having those options increasesflexibility of the image build process in corporate environments.This is part of #11171.This change also fixes some of the issues opened and raised byother users of the Dockerfile.Fixes: #10730Fixes: #10555Fixes: #10856Input from those issues has been taken into account when thischange was designed so that the cases described in those issuescould be implemented. Example from one of the issue landed asan example way of building highly customized Airflow Imageusing those customization options.Depends on #11174* Update IMAGES.rstCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
[AIRFLOW-3612] Remove incubation/incubator mention (#4419)	4
Add 1.10.13 to CI, Breeze and Docs (#12652)	2
[AIRFLOW-5231] Fix S3Hook.delete_objects method (#7375)	4
Refactor installation pages (#18282)This PR splits ad improves the installation-related documentationfor Airflow. The "installation" page had become overloadedwiht everything-but-the-kitchen-sink and it became ratherdifficult to navigate and link to relevant sections.Also there was not a single page where one could have an overviewon different installation methods possible, cases wheneach instalation works best as well as understanding whatis involved in following each installation method in terms ofmaintenance, and expectations that users should have when itcomes to what Apache Airflow Community provides.The PR leaves the installation page as basically a summary ofall installation methods with all above explained and linksto detailed pages explaining prerequisites, dependencies,database setup and supported versions.	1
Fix depcrecated K8S api (#13575)Fix this K8S client deprecation warnings```/usr/local/lib/python3.6/site-packages/kubernetes/client/apis/__init__.py:12: DeprecationWarning: The package kubernetes.client.apis is renamed and deprecated, use kubernetes.client.api instead (please note that the trailing s was removed).      DeprecationWarning```	2
Updating deprecated configuration in examples (#26037)```/home/airflow/.local/lib/python3.9/site-packages/airflow/configuration.py:528 DeprecationWarning: The sql_alchemy_schema option in [core] has been moved to the sql_alchemy_schema option in [database] - the old setting has been used, but please update your config.```	5
Minor refactor of the login methods in tests.www.test_views (#10918)- Instead of supporting only an Admin user in the base test class, you can also use a normal User or Viewer- Only add users when they are being used so we can do a little less in the setup phase (minor speedup in TestDagACLView)	3
closes apache/incubator-airflow#1703 *Obsolute PR*	5
Fix RTD docs build (#12373)	2
Add location support to BigQueryDataTransferServiceTransferRunSensor.	5
[AIRFLOW-XXX] Update airflow-jira release management script (#6772)	5
Fix ``docker-stack`` docs build (#18419)	2
[AIRFLOW-XXXX] Fix typos in docs directory (#7571)	2
Limiting/parameterizing the max number of active runs per DAG	2
[AIRFLOW-4813] Add the client_info parameter during GCP's client library initialization (#5728)	5
Removing forced resetdb from ./run_unit_tests	3
Fixes optimisation where doc only change should build much faster (#10344)	4
Add Redoc Open API preview (#9504)	2
[AIRFLOW-3887] Downgrade dagre-d3 to 0.4.18 (#4713)dagre-d3 v0.6.3 has a bug that causes this Javascript error when loadingthe Graph View:    TypeError: previousPaths.merge is not a functionThe bug fix [1] has been merged to master, but hasn't been released tonpm yet. This change temporarily downgrades our version of dagre-d3until dagre-d3 v0.6.4 is released [2]I also fixed a bug I encountered in the `compile_assets.sh` where thescript would fail if the directory `airflow/www/static/dist/` exists butis empty.[1] https://github.com/dagrejs/dagre-d3/pull/350[2] https://github.com/dagrejs/dagre-d3/blob/5450627790ff42012ef50cef6b0e220199ae4fbe/package.json#L3	5
Ignore metastore	5
Updating Airbyte example DAG to use XComArgs (#16867)	1
Docs improvments to the tutorial	2
[AIRFLOW-3659] Create Google Cloud Transfer Service Operators (#4792)Co-authored-by: Antoni Smolinski <antoni.smolinski@polidea.com>	1
Chart: Allow setting an existing secret for PgBouncer config (#15296)Previously, if a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.	5
Temporarily disable the check-actions step (#14271)We are now getting "Error: Resource not accessible by integration"output when we try to run this, even from a triggered workflow shouldhave the write permissions needed in the token.To unblock CI I am temporarily disabling this step -- it is not required(though very nice) as it just creates links from the CI job back to thebuild image workflow	1
[AIRFLOW-1517] Remove authorship of secrets and init container	5
[AIRFLOW-695] Retries do not execute because dagrun is in FAILED stateThe scheduler checks the tasks instances without taking into accountif the executor already reported back. In this case the executorreports back several iterations later, but the task is queued nevertheless.Due to the fact tasks will not enter the queue when the task is consideredrunning, the task state will be "queued” indefinitely and in limbobetween the scheduler and the executor.	5
[FEATURE] google provider - split GkeStartPodOperator execute (#23518)	1
Changed word 'the' instead 'his' (#23493)	4
[AIRFLOW-XXX] Fix incorrect parameter in SFTPOperator example (#4344)	1
[AIRFLOW-5817] Improve BigQuery operators idempotency (#6470)	1
Remove extraneous treeData fields (#22763)	5
[AIRFLOW-5411] Remove the noise produced while running failed pre-commits	0
Remove Backport Providers (#14886)We are removing support for Backport Providers now.The last release was sent yesterday- as planned, on 17 March 2021 - thelast release of the Backport Providers.As agreed before, and documented here:https://github.com/apache/airflow/blob/master/dev/PROJECT_GUIDELINES.md#support-for-backport-providers> Backport providers within 1.10.x, will be supported for critical fixesfor three months (March 17, 2021) from Airflow 2.0.0 release date (Dec17, 2020).For the future reference, if anyone would like to build backportproviders with cherry-picking any fixes, the branch to start from is`legacy-backport-cutoff-point`. The documentation and tools to build thebackports are there, but there will be no more community releases forbackports.Good Bye Backport Providers.	1
Fix attempting to reattach in `ECSOperator` (#23370)* Updated ecs_task_id on reattaching	5
Airflow UI fix vulnerabilities - Prototype Pollution (#24201)	0
[AIRFLOW-1321] Fix hidden field key to ignore caseWebserver has a feature to hide sensitive variable fields,which key contain specific words. But its matching iscase-sensitive, so "google_api_key" is hidden but"GOOGLE_API_KEY" is not. This behaviour is not intuitive,so this PR fixes it to be case-insensitive.	0
Merge pull request #411 from jlowin/patch-7remove formatting from xcom docstring	2
[AIRFLOW-1206] TyposCloses #2294 from benrudolph/patch-1	2
[AIRFLOW-XXX] Adding walmart labs as user (#6027)[ci skip]	1
Add back deleted comment (#17884)Fixes comment deleted in #17304	4
Conform the tutorial to the code at the beginning	5
Speed up tests that use BackfillJob (#17648)Calling `heartbeat` was putting in a sleep in which isn'tnecessary/useful in tests, where we want it to run as quick as possible.The sleep has been kept in "normal" mode as otherwise the status output(`[backfill progress] | finished run %s of %s |` etc.) will beessentially spammed, rather than only being printed every few seconds.This makes the tests/jobs/ run in about 20s (vs 120s without thechange.)	4
Remove unnecessary asset compilaton for prod images (#25374)When prod image is built, we install airflow from packages andasset compilation happens as part of the package preparation.There is no need whatsoever to repeat it here for PROD images(it is still needed for CI images though).	1
[AIRFLOW-5522] BQ list dataset tables operator (#6151)This operator will fetch the tables of the specified dataset.Signed-off-by: Mohannad Albanayosy <m.banayosi@gmail.com>	5
adapt scheduler to use DagRun table for triggering	2
Fix typo in .github/ISSUE_TEMPLATE/bug_report.md (#10231)`sytle` -> `style`	0
BaseBranchOperator will push to xcom by default. (#13704) (#13763)This change will BaseBranchOperator to do xcom push of the branch it choose to follow.It will also add support to use the do_xcom_push parameter.The added change returns the result received by running choose_branch().Closes: #13704	1
Bump terser from 4.8.0 to 4.8.1 in /airflow/ui (#25178)Bumps [terser](https://github.com/terser/terser) from 4.8.0 to 4.8.1.- [Release notes](https://github.com/terser/terser/releases)- [Changelog](https://github.com/terser/terser/blob/master/CHANGELOG.md)- [Commits](https://github.com/terser/terser/commits)---updated-dependencies:- dependency-name: terser  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Fix GCSToGCSOperator ignores replace parameter when there is no wildcard (#23340)	2
[AIRFLOW-2857] Fix Read the Docs env (#3703)The Read the Docs build process was broken due to #3660. This PR fixes this.	0
Don't run pre-migration checks for downgrade (#23634)These checks are only make sense for upgrades.  Generally they exist to resolve referential integrity issues etc before adding constraints.  In the downgrade context, we generally only remove constraints, so it's a non-issue.	0
Update example SingularityOperator DAG (#8790)The main thing I was fixning here was `start_date=utcnow()` which isalways going to be wrong (discovered via a test in #8772).While I was updating the DAG I updated it to use context manager, andshift operators.	1
Fix XCom.delete error in Airflow 2.2.0 (#18956)In Airflow 2.2.0 XCom.delete causes error, by trying to update dag_run table dag_id and execution_date columns to NULLs.sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) null value in column "dag_id" violates not-null constraint[SQL: UPDATE dag_run SET dag_id=%(dag_id)s, execution_date=%(execution_date)s WHERE dag_run.id = %(dag_run_id)s][parameters: {'dag_id': None, 'execution_date': None, 'dag_run_id': 2409}]Setting passive_deletes to the string value ‘all’ will disable the “nulling out”	4
Use DAG context manager in examples (#13297)	2
Minor fixes to Stale Bot (#15184)- Update the Stale Issue message- Rename the workflow to `Close stale PRs & Issues` from `Close stale PRs`	0
[AIRFLOW-1189] Fix get a DataFrame using BigQueryHook failingCloses #2287 from mremes/master	0
Chart: Avoid `git-sync` sidecar on Websever when Airflow>=2.0.0 (#15814)For `apache-airflow>=2.0.0`, DAG Serialization is enabled by defaultand we don't need to have a sidecar on Websserver.Previously this was done using `gitSync.excludeWebserver`. Howeverwith https://github.com/apache/airflow/pull/15627 - we now have`airflowVerson` so we can just do a comparison of the version.	1
Uniform colors and more circle on the dashbaord	5
[AIRFLOW-2836] Minor improvement-contrib.sensors.FileSensor (#3674)- The default value of fs_conn_id was not proper.- Added a new test in which we try to ignore setting  fs_conn_id explicitly.- a minor change on how a path is concatenated	4
[AIRFLOW-3343] Update DockerOperator for Docker-py 3.0.0 API changes (#4187)The API of `wait()` changed to return a dict, not just a number so thisOperator wasn't actually working, but the tests were passing because thereturn was mocked in-correctly.I also removed `shm_size` from kwargs passed to BaseOperator to avoidthe deprecation warning about unknown args.	2
Adding documentation entry for BranchPythonOperator	1
[AIRFLOW-XXX] Add more GCP transfer operators (#6206)	1
make crypto setup option	1
Fix unchecked indexing in _build_metrics (#16744)I am not sure if this can happend in regular uses, but when running testcases `sys.argv` can be that args passed to the pytest. When this is thecase it is definently possible for argv to only contain a singleelement.	3
[AIRFLOW-2003] Use flask-caching instead of flask-cacheFlask-cache has been unmaintained for over threeyears,flask-caching is the community supported version.Closes #2944 from bolkedebruin/AIRFLOW-2003	1
[AIRFLOW-XXXX] remove vestigial sentence fragment in changelog (#8864)Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>	1
Replaces the usage of postgres:// with postgresql:// (#21205)After releasing 3.4.4 we can finally migrate to SQLAlchemy 1.4,however SQLAlchemy 1.4 removed the use of postgres:// as validspecification for Postgres DB SQL (leaving only postgresql://)Due to that we need to change:* postgres provider to return postgresql:// with get_db_uri()* fix a number of tests that expected postgres://We cannot do much if someone uses postgres:// specification.Technically it might be seen as breaking change, but this is notan airflow breaking change and users could still use SQLAlchemy1.3 to keep the old prefix, so we can introduce this changein Airflow without raising the major version.Details in the [SQLAlchemy Changelog](https://docs.sqlalchemy.org/en/14/changelog/changelog_14.html#change-3687655465c25a39b968b4f5f6e9170b).	4
Add permission "extra_links" for Viewer role and above (#10719)This change adds 'can extra links on Airflow' to the Viewer role and above. Currently, only Admins can see extra links by default.	2
Making sure serialization tests runs on all example dags (#25447)while checking the failure of `TestStringifiedDAGs::test_serialization` in https://github.com/apache/airflow/runs/7608813338?check_suite_focus=true  (PR https://github.com/apache/airflow/pull/25280 )I noticed that we have:https://github.com/apache/airflow/blob/7d95bd9f416c9319f6b5c00058b0a1e3bd5bf805/tests/serialization/test_dag_serialization.py#L255-L256This test suppose to collect all example dags including providers but now in AIP-47 we move the example dags away from this path so the more we move the less coverage this test has.To solve this I added also the new paths	1
[AIRFLOW-2777] speed up dag.sub_dag(...)previous version created the subdag by copyingover all the tasks, andthen filtering them down. it's a lot faster if weonly copy over thetasks we needCloses #3621 from abdul-stripe/faster-subdag	2
[AIRFLOW-6258] Add CloudFormation operators to AWS providers (#6824)	1
[AIRFLOW-XXXX] Fix broken static check failure on CI (#7551)	0
[AIRFLOW-1121][AIRFLOW-1004] Fix `airflow webserver --pid` to write out pid fileAfter AIRFLOW-1004, --pid option is no longerhonored andthe pid file is not being written out. This PRfixes it.Closes #2249 from sekikn/AIRFLOW-1121	0
Remove extra initialization overhead for mypy/flake (#22183)The #22127 change introduced a change how execution of dockercommnds was done (due to LD_PRELOAD change) and they started touse entrypoint_ci, however this caused undesired effect of runningdatabase initialization and printing extra lines which was not neededand cluttered the output.This PR introduced SKIP_ENVIRONMENT_INITIALIZATION that (if set to true)skips the entire initialization of the entrypoint_ci	1
Fixed tests failing on Python 3.8 (#21022)The change #21003 broke TestDeprecation class tests by removingTestCase and leaving self.skipTest.This change replaces self.skipTest with pytest.skipTest everywhere.	3
Initial commit for new Breeze project (#19867)It includes:* proposal for initial ADRs (Architecture Decision records)  where we will keep decision records about both - Breeze2 and CI* scaffolding for the new breeze command including command line,  pre-commit checks, automated tests in CI and requirements	1
Add Currency to INTHEWILD.md (#10607)	1
Update Airflow version in docker stack documentation to 2.1.2 (#17017)	2
Make `execution_date_or_run_id` optional in `tasks test` command (#26114)	3
[AIRFLOW-6038] AWS DataSync reworked (#6773)Added Amazon AWS how-to documentation scaffolding, plus example DAGs for AWS DataSync Operators with their respective how-to guides.Reworked AWS DataSync operators into a single, logically idempotent operator.	1
[AIRFLOW-XXX] Move examples note (#6250)	4
Add MSSQL link to breeze visuals.py and breeze-legacy	2
Implement BigQuery Table Schema Update Operator (#15367)Co-authored-by: Jens Larsson <jens.larsson@c02cv73mml85.lan>	1
Remove adding of "test-run" variables to dc_ci script (#18903)The RUN_*TEST variables are not part of the environmentso they are not set when the dc_ci is generated they areoverridden by Breeze when particular commands are executed.Therefore we should not hard-code those values in dc_ci script(this is useful for debugging to have the script but it is onlythere for environment configuration)	5
fix typo in google provider additional extras (#24431)	1
Removing QUEUED from runnable states list	1
Bump minimum required ``alembic`` version (#20153)Related to https://github.com/apache/airflow/pull/18453#issuecomment-989314399`1.5.0` was yanked so `>=1.5.1` is safe and we already have `1.7.5` in constraints-main	0
Allows for more than one warning in deprecation message (#13836)Sometimes in our tests we get more than one deprecationwarnings. It is likely caused by transitive warningsfrom importing other external libraries.In order to get rid of those side effects, we are nowaccepting more than one warning and we expect that at leastone of the warnings will come from the file being tested	3
store connection rather than call every time	5
[AIRFLOW-5160] Remove example DAG count test (#5775)	3
add num_runs query param for tree refresh (#16437)- add `num_runs` as a meta field to add to the tree refresh request	1
[AIRFLOW-823] Allow specifying execution date in task_info APICloses #2045 from robin-miller-ow/release/API_TaskInstanceInfo	5
[AIRFLOW-2574] Cope with '%' in SQLA DSN when running migrations (#3787)Alembic uses a ConfigParser like Airflow does, and "%% is a specialvalue in there, so we need to escape it. As per the Alembic docs:> Note that this value is passed to ConfigParser.set, which supports> variable interpolation using pyformat (e.g. `%(some_value)s`). A raw> percent sign not part of an interpolation symbol must therefore be> escaped, e.g. `%%`	1
[AIRFLOW-3540] Respect environment config when looking up config file. (#4340)	2
[AIRFLOW-535][AIRFLOW-1] Add OfferUp as an Airflow user.[]Closes #1814 from jghoman/AIRFLOW-535	1
[AIRFLOW-4217] Remove all usage of the six library (#5715)	4
Deprecate non-JSON conn.extra (#21816)Connection extra field is generally assumed to be JSON but we don't actually require it.  Here we deprecate non-JSON extra so that in 3.0 we can require it.  Further, we require that it not just be any json but must also parse as dict, because a string value such as '"hi"' or '[1,2,3]' is json, but a very bad practice.	5
[AIRFLOW-3349] Use None instead of False as value for encoding in StreamLogWriter (#7329)	2
Prefer the local Quick Start in docs (#25888)	2
[AIRFLOW-3250] Fix for Redis Hook for not authorised connection calls (#4090)Password stay None value and not None (str) in case there is no password set through webadmin interfaces.This is fix for connections for Redis that not expect autorisation from clients.	0
Improve example DAGs data by diversifying "tags" value (#11665)	5
Query TaskReschedule only if task is UP_FOR_RESCHEDULE (#9087)* Query TaskReschedule only if task is UP_FOR_RESCHEDULE* Query for single TaskReschedule when possible* Apply suggestions from code reviewCo-authored-by: Stefan Seelmann <mail@stefan-seelmann.de>* Adjust mocking in tests* fixup! Adjust mocking in tests* fixup! fixup! Adjust mocking in testsCo-authored-by: Stefan Seelmann <mail@stefan-seelmann.de>	3
[AIRFLOW-5117] Automatically refresh EKS API tokens when needed (#5731)	5
Testing/debugging	3
Update UPDATING.md (#11172)	5
Fixing markup logs	2
Typo	2
Update Breeze documentation (#9608)* Update Breeze documentation	2
Add Google Cloud Workflows Operators (#13366)Add Google Cloud Workflows Operators, system test, example and sensorCo-authored-by: Tobiasz Kędzierski <tobiasz.kedzierski@polidea.com>	3
update processor to fix broken download URLs (#23299)merging because it has the "Ok to merge" and all non-skipped tests passed!	4
make MsSQL tests runnable on Python 3.8 (2nd) (#25216)I missed the hive provider from the commit in https://github.com/apache/airflow/pull/25214	1
Update docker.rst (#17882)This didn't work on my 2019 Macbook 16 inch until I ran the Linux section.Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	1
Sets default timeout for the job waiting for images (#10517)In normal circumstances those jobs will wait for a short time(4-15 minutes depenfding on the state of the base image).However there might be some cases when there are a lot of jobsor when there is some queueing problems in GitHub thatthe "Build Images" job will be queued and not start quickly.This happened on 24th of August 2020 for example when severaljobs failed because the "Build Image" was queued and onlyrun after the "CI Build" job timed out.Usually those situations tends to be resolved by GitHub supportor they resolve themselves as the jobs will be finishing andfreing the queue. However in those cases we should give thewaiting job as much time as GitHub Action allows by defaultfor the job to run (360 minutes). This is no harm - we canalwayc cancel those jobs manually and they are just twojobs running so it should not cause any problem.Note that if someone would see that the job is running fora long time - the contributor will likely push amendedcommit and it will also cancel such waiting job, sothis is even less likely to have long runnning waiting jobs.	1
Merge pull request #1401 from mtp401/cloudant-hookCloudant Hook	1
Properly mocks UUID objects (#12381)The uuid methods return UUID objects not strings and there isa certain expectation about those - like hex property for exampleWe had a few cases where those uuid method's return values havebeen mocked with strings and it caused a problem - for examplewith migration to latest sentry library which used this veryhex method from the uuid4() return value.	1
[AIRFLOW-218] Added option to enable webserver gunicorn access/err logsCloses #1577 from aoen/ddavydov/better_http_response_loggingAdded an option to enable gunicorn access/error logs.The default config will now log webserver errors/accesses to stderr.Also made the 404 page hostname text default page color instead ofwhite, since white text is pretty hard to see against a whitebackground.	0
Fixes limits on Arrow for plexus test (#14781)Arrow must be <1.0.0 for plexus to work	1
Less hacky double-rendering prevention in mapped task (#25924)* Refactor _expand_mapped_kwargs to disallow None* Less hacky double-rendering preventionInstead of inventing a separate way to track what XComArg has beenrendered (due to task unmapping), we can actually track this withseen_oids. This makes the mechanism considerably less hacky.	1
Fix typo on ``necessary`` word (#19565)	2
Docs: Fix spacing bug in 'Dag Run' (#10372)	1
Add query count tests for _run_raw_task (#9509)	1
[AIRFLOW-XXX] Add M4U to user listCloses #3426 frommsantino/AIRFLOW-2437-add_m4u_to_users_list	1
Merge pull request #283 from mistercrunch/force_dopForce depends_on_past = True when wait_for_downstream is used	1
[AIRFLOW-6497] Avoid loading DAGs in the main scheduler loop (#7597)* [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler* fixup! [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler* fixup! fixup! [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler	2
Adding headers to CRUD views	1
Making HiveCliHook.run_cli return stdout	1
[AIRFLOW-2739] Always read default configuration files as utf-8Closes #3593 from cjgu/airflow-2739	2
Making master scheduler more resilient	1
[AIRFLOW-XXXX] Fix reference in concepts doc (#7135)Correcting reference in Concepts -> Cluster Policy doc from airflow_setting.py to airflow_local_settings.py	1
[AIRFLOW-600] Added BandwidthX as a user of AirflowCloses #1857 from dineshdsharma/master	1
Add workers extraVolumes to Kubernetes pod template for Helm Chart (#14743)	2
[AIRFLOW-1289] Removes restriction on number of scheduler threadsThis removes the restriction that the number ofthreads can be at mostthe number of CPU cores. There's no reason to havethis restriction.Closes #2353 from saguziel/aguziel-increase-cores	1
Use resource and action names. (#16380)	1
Fix typo in comments (#18626)Looks like it was added in (#18533)	1
Fix bigquery-hook when no  engine_kwargs are passed	4
[AIRFLOW-6809] Add tests for presto operators (#7422)	1
[AIRFLOW-370] Create AirflowConfigException in exceptions.pyAirflowConfigException should be created inthe exceptions utility file, not inconfiguration.py.All exceptions should be created in`exceptions.py`.https://issues.apache.org/jira/browse/AIRFLOW-370Closes #1689 from jlowin/refactor-exception	4
Make models/pool.py pylint compatible (#8068)* Make models/pool.py pylint compatible* Fixed for isortCo-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>	0
Masking extras in GET /connections/<connection> endpoint (#22227)Masking extras in GET /connections/<connection> endpoint	1
Fix pushing image cacheAfter converting to `breeze` commands, pushing cache started to failas the image tag was used (but for cache we always build and pushusing 'latest' images.	3
[AIRFLOW-2053] Fix quote character bug in BQ hookModified the condition to check if thequote_character is set. This will allow to set`quote_character` as empty string when the datadoesn't contain quoted sections.Closes #2996 from kaxil/bq_hook_quote_fix	0
Add 'queued' to DagRunState (#16854)This change adds 'queued' to DagRunState and improved typing for DagRun stateCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	2
fix: restore parameters support when sql passed to SnowflakeHook as str (#16102)	1
Do not forward cluster-identifier to psycopg2 (#15360)`cluster-identifier` is only used by botocore to fetch Redshift credentials.   If passed to psycopg2, it will produce error `psycopg2.ProgrammingError: invalid dsn: invalid connection option "cluster-identifier"`.Co-authored-by: Jordan Zhang <jorzhang@justin.tv>	0
Remove duplicate dependecies (#14611)	4
EdgeModifier refactoring (#21404)	4
[AIRFLOW-XXX] Add Pernod-ricard as a airflow userCloses #2983 from romain-nio/AddPernodRicardAsAirflowUser	1
[AIRFLOW-348] Fix code style warningsCloses #1672 from skudriashev/airflow-348	2
Fix BigQueryInsertJobOperator cancel_on_kill (#25342)	1
[AIRFLOW-1995][Airflow 1995] add on_kill method to SqoopOperatorCloses #2936 from Acehaidrey/AIRFLOW-1995	1
fixing small issue with qbol operator and hook	1
[AIRFLOW-4945] Use super() syntax (#5579)	1
[AIRFLOW-6254] obscure conn extra in logs (#6817)	2
fixing bug	0
[AIRFLOW-4414] AWSAthenaOperator: Push QueryExecutionID to XCom (#5276)Currently it is not possible to make use of QueryExecutionID (theunique identifier of the query submitted to Athena) in the tasks thatfollow.	1
Warning if start_date isn't datetime	5
Less verbose output for docs build (#12994)	2
Modify BigQueryCreateExternalTableOperator to use updated hook function (#24363)* Fixed BigQueryCreateExternalTableOperator and its unit test (#24160)	3
[AIRFLOW-1313] Fix license headerCloses #3281 from juise/master	0
[AIRFLOW-6204] Create GoogleSystemTest class (#7439)fixup! [AIRFLOW-6204] Create GoogleSystemTest class	5
docker operator - adding code example	1
Add root to tree refresh url (#17633)We were not passing the root to the `/tree_data` api call. Therefore, filtering upstream of a task would be reset during auto-refresh even though root was still defined.	1
Switch to downloaded pgbouncer_exporter (#10759)Fixes #10753	0
Fix typo in check_environment.sh (#12395)`Databsae` -> `Database`	5
Improve code quality of SLA mechanism in SchedulerJob (#11257)	1
Only list linked issues once in release issues (#20299)	0
Update changelog for helm chart 1.5.0 (#22090)	2
fix command and typo (#24282)	2
Only fetching dag states for active DAGs	2
Add separate example DAGs and system tests for google cloud speech (#8778)	3
Update CONTRIBUTORS_QUICK_START.rst with note on pyenv for Mac M1 (#23305)The Contributor's Quick Start [recommends pyenv](https://github.com/apache/airflow/blob/main/CONTRIBUTORS_QUICK_START.rst#pyenv-and-setting-up-virtual-env) to manage environments when developing on Airflow. There are [lots of issues trying to get pyenv to work on M1 the Mac M1 chip](https://www.google.com/search?q=pyenv+m1+mac+site:stackoverflow.com&client=firefox-b-1-d&channel=nus5&sa=X&ved=2ahUKEwjVqMyUhrX3AhVVRTABHc8vB_YQrQIoBHoECAsQBQ&biw=1744&bih=942), so it might be worth including a note on a pyenv alternative.	1
Fix `breeze flags` command. (#10766)This was missed in #10670	0
Revert "[AIRFLOW-5488]Remove unused variables from tmp_configuration_copy method (#6114)" (#6120)This reverts commit 31b7bc958ef3395a5f7307a852822eba81a5d663.	4
Make py3 compat	1
Revert "Send SLA callback to processor when DagRun has completed" (#20997)It turns out that while processing time for dags with slas were solved,the sla misses are not being recorded.	2
Calendar UI improvements (#16226)- change calendar borders from black to grey to match rest of the app better- remove unneeded title- remove "View" from all dag views to save space and consistency	2
[AIRFLOW-3724] Fix the broken refresh button on Graph View in RBAC UI (#4548)	0
Return empty dict if Pod JSON encoding fails (#24478)When UI unpickles executor_configs with outdated k8s objects it can run into the same issue as the scheduler does (see https://github.com/apache/airflow/issues/23727).Our JSON encoder therefore needs to suppress encoding errors arising from pod serialization, and fallback to a default value.	0
[AIRFLOW-1170] DbApiHook insert_rows inserts parameters separatelyInstead of creating a sql statement with allvalues, we send the valuesseparately to prevent sql injectionCloses #2270 from NielsZeilemaker/AIRFLOW-1170	1
Added logic to allow for an embed parameter in the URL to strip everything from the charts view except the chart itself.	2
[AIRFLOW-4362] Fix test_execution_limited_parallelism (#5141)	3
Remove fix-ownership after upload coverage and extra cache (#23203)There is no Breeze installed in upload coverage so fix-ownershipis not needed (and harmful because breeze is missing :))Also extra cache for ~/.local caused overriding of breezeinstallation and it should be removed.	4
Fix wrong query on running tis (#17631)Fix wrong query on my PR about deleting running dags #17630Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	2
FIX Make items nullable for TaskInstance related endpoints to avoid API errors (#26076) FIX Add missing nullable items to TaskInstance related endpointsCo-authored-by: jorrick <jorrick.sleijster@adyen.com>	1
Fix typo in Facebook Ads Provider (#10484)`missings_keys` -> `missing_keys`	1
[AIRFLOW-6804] Add the basic test for all example DAGs (#7419)Co-Authored-By: Tomek Urbaszek <tomasz.urbaszek@polidea.com>	2
Standardize AWS Redshift naming (#20374)* Standardize AWS Redshift naming	5
#16692 show schedule_interval/timetable description in UI (#16931)Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	2
[AIRFLOW-1331] add SparkSubmitOperator optionspark-submit has --packages option to useadditional java packages.but current version of SparkSubmitOperatorcouldn't handle it.I added "packages" option to SparkSubmitOperatorto resolve it.I added same option for TestSparkSubmitOperator,too.Closes #2622 from chie8842/AIRFLOW-1331	3
[AIRFLOW-2648] Update mapred job name in HiveOperatorCloses #3534 fromyrqls21/keivn_yang_reorder_mapred	1
Pattern parameter in S3ToSnowflakeOperator (#24571)	1
[AIRFLOW-4670] Make airflow/example_dags Pylint compatible (#5361)	2
add a parameter for number of shard in batch ingestion	2
Remove deprecated modules (#25543)	4
Replace Docs GIF with updated UI screenshots (#12044)Resolves #11175	0
Added missing sendgrid readme (#12245)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>	1
Cleaning up default connections	4
[AIRFLOW-XXX] Add Bonial International GmbH to who's using Airflow (#5484)[ci skip]	1
Databricks: Fix provider for Airflow 2.2.x (#25674)The problem was caused by using `ProviderInfo.is_source` field that was introduced only inAirflow 2.3.0.	5
Bump attrs and cattrs dependencies (#11969)`cattrs` now depends on `attrs >= 20.1.0`, because of `attr.resolve_types`.Source: https://github.com/Tinche/cattrs/blob/master/HISTORY.rst#110-2020-10-29closes https://github.com/apache/airflow/issues/11965	0
fix help message display for dags test subcommand (#8552)	3
Move images needed only during CI to `airflow-ci` DockerHub (#16116)We have now separate `apache/airflow-ci` DockerHub repo and wemove all our images needed only during CI there.The images from the main `apache/airflow` remaining are:* airflow tagged and latest tagged production images* images neded by the Helm Chart	2
Support google-cloud-datacatalog>=1.0.0 (#13097)	5
Fix modal import in graph.js (#15852)`call_modal` no longer existed. Switching to import `callModal` instead.	2
[AIRFLOW-2225] Update document to include DruidDbApiHookCloses #3140 from feng-tao/airflow-2225	5
AIRFLOW-16: Update Google cloud hooks to use new Google cloud platform UI.	1
Merge pull request #163 from mistercrunch/conditionalConditional	1
[AIRFLOW-5387] Fix show paused pagination bug (#6100)	0
Fix failed KubernetesPodOperator tests (#12461)Fixes failure in KubernetesPodOperator tests cause bynodeSelector arguments	1
Add SubprocessHook for running commands from operators (#13423)This extracts the "run this command" logic from the BashOperator in to areusable hook.Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	1
Databricks SQL operators are now Python 3.10 compatible (#22886)New version of databricks-sql-connector fixes incompatibility withPython 3.10, so rollig back #22221, and bumping dependency.This closes #22220	0
Adding root param to tree and graph view	2
Add Dag Runs CRUD endpoints (#9473)	1
Omit contrib from coverage report	3
Merge pull request #467 from jlowin/fix_env_varFix issue with expand_env_var	0
load env var configuration first	5
Make airflow info to work with pipes (#14528)After this change output from AirflowConsole can be piped in bashwithout loosing some information thanks to fixed width of output.Closes: #14518Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
Remove `xcom_push` flag from `BashOperator` (#24824)	1
[AIRFLOW-1090] Add HBOCloses #2230 from yiwang/AIRFLOW-1090	1
[AIRFLOW-2205] Remove unsupported args from JdbcHook docJdbcHook's docstring has unsupported argumentsand unimplemented feature description.This PR fixes them and adds JdbcHook to the API reference.	5
Update Helm Chart docs for 1.0.0 release (#15957)Updates repo name and chart name and some minor errors	0
The fix_ownership works independently of backend choice (#9664)The script failed on a "clean" installation if the imagerequired cleaning and the database was not started.	5
[AIRFLOW-3238] Fix models.DAG to deactivate unknown DAGs on initdb (#4073)Unknown dags are now deactivated on initdb	5
Final cleanup for 2020.6.23rc1 release preparation (#9404)	4
Add Nielsen to Airflow users list (#9954)	1
[AIRFLOW-3355] Fix BigQueryCursor.execute to work with Python3 (#4198)BigQueryCursor.execute uses dict.iteritems internally,so it fails with Python3 if binding parameters areprovided. This PR fixes this problem.	0
[AIRFLOW-3582] Adds tests for HiveStatsCollectionOperator (#4398)	1
Chart: Add tests to check labels, kind and annotations (#15313)This commits adds more unit tests to test:- labels are added to all pods- annotations are added to pods of Scheduler, Worker & Webserver deployment- kind of scheduler and worker deployment as it can be `StatefulSet` or `Deployment`  based on if persistence is enabled or not	0
Use urlparse for remote GCS logs, and add unit testsIt’s conceivable that the bucket starts with g or s, in which case thislstrip would remove characters from the bucket name. Instead, useurlparse to properly parse the string.Also clean up the equivalent function in the S3 and GCS Hooks, using`strip` to clean up leading/trailing slashes.	4
Hive Metastore Browser plugin	2
v0.4.2 , fixes around hooks conditional imports	2
[AIRFLOW-6790] Add basic Tableau Integration (#7410)	1
Doc entry	1
Merge branch 'master' into bigquery-hook	1
Migrate Datastore system tests to new design (AIP-47)Change-Id: Ibc6f0a03a0c6fb374de85d74a7ac62cf6fa55bec	4
Don't ignore legacy `concurrency` dag parameter (#18730)Currently, even if legacy `concurrency` dag parameter is specified, it is always ignored because `max_active_tasks` is always initialized from `core.max_active_tasks_per_dag`.In our case this caused unexpected throttling of the task concurrency on production and performance issues.`concurrency` parameter should always be used, if provided, as this preserves backward compatibility for users performing the migration.Tested locally:```DAG(    dag_id='xxx',...    concurrency=1,)```Before fix:```{scheduler_job.py:413} INFO - DAG xxx has 1/16 running and queued tasks```After fix:```{scheduler_job.py:413} INFO - DAG xxx has 1/1 running and queued tasks```Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).	5
Allowing to blacklist columns through the assignment function	1
Fix failing static checks in main (#17424)	0
Add context var hook to inject more env vars (#20361)Co-authored-by: Ping Zhang <ping.zhang@airbnb.com>	1
fix bug where multiple volume mounts created (#10915)	1
[AIRFLOW-3411]  Add OpenFaaS hook (#4267)	1
Adding postgres operator and hook	1
Allow `replace` flag in gcs_to_gcs operator. (#9667)* Allow `replace` flag in gcs_to_gcs operator.If we are not replacing, list all files in the Destination GCS bucket and only keep those files which are present in Source GCS bucket and not in Destination GCS bucket	2
enable UI feature to recursively set success=True for all operators within SubDagOperator	2
[AIRFLOW-5346] Add system tests for GKECluster (#5947)	3
Add http system test (#8591)	3
Set log level from settings	1
Add exclusions for new node_modules directory (#14935)	1
Add JSON output on SqlToS3Operator (#21779)	1
Merge pull request #835 from msumit/I832Issue 832: creating run_id if not given while executing trigger_dag	2
Bugfix for Invalid default value for timestamp	0
[AIRFLOW-836] Use POST and CSRF for state changing endpointsCloses #2054 from saguziel/aguziel-use-post	1
add more precise type hint for task callbacks (#10355)	1
added some documentation	2
Fix blank dag dependencies view (#17990)* Fix blank dag dependencies view* calculate graph if node and edges are empty	2
Add jupytercmd and fix task failure when notify set as true in qubole operator (#10599)Add jupytercmd in Qubole Operator which fires a JupyterNotebookCommand to the jupyter notebooks running on user's QDS account. Along with this, we have fixed a minor bug that caused the tasks to fail with --notify is set in Qubole Operator.Co-authored-by: Aaditya Sharma <asharma@qubole.com>	1
Add Markdown linting to pre-commit (#11465)	1
[AIRFLOW-XXX] Add another engineer to Bombora Inc's list of engineers (#4977)	1
[AIRFLOW-1559] Dispose SQLAlchemy engines on exitWhen a forked process or the entire interpreter terminates, we haveto close all pooled database connections. The database can run outof connections otherwise. At a minimum, it will print errors in itslog file.By using an atexit handler we ensure that connections are closedfor each interpreter and Gunicorn worker termination. Only usagesof multiprocessing.Process require special handling as thoseterminate via os._exit() which does not run finalizers.This commit is based on a contribution by @dhuanghttps://github.com/apache/incubator-airflow/pull/2767	1
Add more fields to REST API get DAG(dags/dag_id) endpoint (#22637)The DagModel columns have increased since this endpoint was created. This PR improves theendpoint by including all the missing fields of the DagModel on the endpoint.This update also touched on DAGDetails schema because it inherits from DAGSchema.In a future PR, when more details would be added to the DAGDetails endpoint, we couldseparate it from the DAGSchema. They are related but not really the same. One is adatabase object while the other is not	5
Stop SLA callbacks gazumping other callbacks and DOS'ing the DagProcessorManager queue (#25147)	2
Standardize AWS EKS naming (#20354)* Standardize AWS EKS naming	5
Fix tree view if config contains " (#9250)If you run DAG with `{"\"": ""}` configuration tree view will be broken:```tree:1 Uncaught SyntaxError: Unexpected string in JSON at position 806    at JSON.parse (<anonymous>)    at tree?dag_id=hightlight_test&num_runs=25:1190```JSON.parse is given incorrectly escaped json string.	5
[AIRFLOW-2611] Fix wrong dag volume mount path for kubernetes executorThere are two way of syncing dags, pvc and git-sync, if set both(dags_volume_claim and git_subpath), I won't getthe mountPath what Iwant. I think the priority of pvc should higherthan git-sync, so Ithink when dags_volume_claim is been set, themountPath shuold not join the git_subpath.Closes #3497 from imroc/AIRFLOW-2611	1
[AIRFLOW-XXXX] Add Docker installation in WSL (#7591)Add Docker setting in WSL and troubleshooting methodbecause of volume mount problem in WSL and dockerI referenced document with :https://nickjanetakis.com/blog/setting-up-docker-for-windows-and-wsl-to-work-flawlessly	2
Merge pull request #661 from patrickleotardif/fix_successFix success endpoint for @once DAGs	2
[AIRFLOW-3646] Rename plugins_manager.py to test_xx to trigger tests (#4464)	3
Update docs link in REST API spec (#13107)	2
[AIRFLOW-1017] get_task_instance shouldn't throw exception when no TIget_task_instance should return None instead ofthrowing exception in the case where dagrun does not have the taskinstance.Closes #2178 from aoen/ddavydov--one_instead_of_first_for_dagrun	2
Updating TODO list	2
Skip DAG perm sync during parsing if possible (#15464)For DAGs without `access_control` that already have their PermissionViewrecords, we can skip syncing their permissions during parsing. This cutsdown on database queries and is faster (~2 seconds, mostly import time).	2
[AIRFLOW-3888] HA for metastore connection (#4708)* HA for Metastore* [AIRFLOW-3888] HA for metastore connectionCreating a connection to a metasotor with two hosts for high avitablity (eg connection 1, connection 2) is not possible because the entire value entered is taken. For our needs, it is necessary to go through subsequent hosts and connect to the first working.This change allows you to check and then connect to a working metastor.* add function to base_hook* update webhdfs_hook* back to original version* back to original version* Update hive_hooks.pyThank you. I made a few changes because during the tests I detected several errors.I have a question, when I do marge to my pull it will be  still possible to land it in the airflow main branch?* [AIRFLOW-3888] HA for metastore connection flake8 code repair* [AIRFLOW-3888] HA for metastore connection Flake8 repair* [AIRFLOW-3888] HA for metastore connectionCode behavior improvements* [AIRFLOW-3888] HA for metastore connection Add test* [AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionAdd test[AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionAdd test[AIRFLOW-3888] HA for metastore connectiontest improvement[AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionImproving the typo in the variable name*  [AIRFLOW-3888] HA for metastore connectionMock return_value edit* [AIRFLOW-3888] HA for metastore connectionFlake8 repair* [AIRFLOW-3888] HA for metastore connectionTest repair* [AIRFLOW-3888] HA for metastore connectionFlake8 repair[AIRFLOW-3888] HA for metastore connectionTest repair	3
Merge pull request #1183 from obulpathi/masterdocs: fixes a spelling mistake in default config	5
Added default_var to Variable	1
Fix MyPY errors for google.cloud.example_dags (#20232)Part of #19891	2
Fix elasticsearch breaking the build (#7800)	4
Migrate JDBC example DAGs to new design #22450 (#24137)	1
[AIRFLOW-2526] dag_run.conf can override paramsMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, "\[AIRFLOW-XXX\] My Airflow PR"    -https://issues.apache.org/jira/browse/AIRFLOW-2526    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:params can be overridden by the dictionary passedthrough `airflow backfill -c````templated_command = """    echo "text = {{ params.text }}""""bash_operator = BashOperator(    task_id='bash_task',    bash_command=templated_command,    dag=dag,    params= {        "text" : "normal processing"    })```In daily processing it prints:```normal processing```In backfill processing `airflow trigger_dag -c"{"text": "override success"}"`, it prints```override success```### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from"[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood ("add", not"adding")    5. Body wraps at 72 characters    6. Body explains "what" and "why", not "how"### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --"*.py" | flake8 --diff`Closes #3422 from milton0825/params-overridden-through-cli	2
Untangle cyclic deps configuration <> secrets (#10559)	5
testing	3
Improving coverage by 5-6%	3
Self upgrade when refreshing images (#23686)When you have two branches, you should sefl-upgrade breeze to makesure you use the version that is tied with your branch.Usually we have two active branches - main and the last releasedline, so switching between then is not unlikely for maintainers.	1
[AIRFLOW-5256] Related pylint changes for common licences in python files (#5786)	2
Allow re-use of decorated tasks (#22941)This opens up the possibility of using one decorated taskin different dag files.Take for example the below task:- common.py@task(task_id='hello')def hello():    print('Hello')defined in a file and called in different dag files using different task ids:- dag_file1.py:from common import hello@dag()def mydag():    for i in range(3):        hello.override(task_id=f'myhellotask_{i}')()- dag_file2.py:from common import hello@dag():def mydag2():    for i in range(3):        hello.override(task_id=f'welcome_message_{i}')()They would all run with different task ids	1
Extra logging for celery queue	2
[AIRFLOW-XXXX] Move UPDATING changes into correct versions (#7166)	4
Removing highchart reference from NOTICE.txt	5
Fix tasks in an infinite slots pool were never scheduled (#15247)Infinite pools: Make their `total_slots` be `inf` instead of `-1`	5
[AIRFLOW-3323] Support HTTP basic authentication for Airflow Flower (#4166)The current `airflow flower` doesn't come with any authentication.This may make essential information exposed in an untrusted environment.This commit add support to HTTP basic authentication for Airflow FlowerRef:https://flower.readthedocs.io/en/latest/auth.html	3
Add Wisr to INTHEWILD.md (#16360)	1
logging from workers fix	0
Fix and speed up grid view (#23947)This fetches all TIs for a given task across dag runs, leading tosignifincatly faster response times. It also fixes a bug where Noneswere being passed to the UI when a new task was added to a DAG withexiting runs.	1
SID Oracle DB connection support (indent fix)	0
Py3 compatibility: make bytes explicit	1
[AIRFLOW-XXX] Add TEK to list of companies (#4240)	1
Allow `image` in `KubernetesPodOperator` to be templated (#10068)fixes https://github.com/apache/airflow/issues/10063	0
Move roles to CONTRIBUTING.rst (#10327)* feat: add initial roles, committers and contributors* refactor: move roles to after contributions* fix: remove extra spaces* fix: fix text according to PR discussion* refactor: move roles to 2nd point* Update CONTRIBUTING.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	5
Re-add Pandas support to BigQuery.	1
Fix and Unquarantine test_change_state_for_tis_without_dagrun (#12323)The test was simply wrong and failed since the new logic was added inhttps://github.com/apache/airflow/commit/c9a97baa86762b9ba37ef71432573b7949e47e2b	1
Move presto.execute inside try catch to handle errorThis commit fixes an issue where malformed SQL would raise aDatabaseError outside of the try catch block in the hook. Thisshould now raise a PrestoException as expected.	1
v0.5.0	5
Speed up www and api_connexion tests (#14684)This was accomplished by removing slow and unnecessary initialization steps from test setups, as well as mocking slow login methods.This was accomplished by removing slow and unnecessary initialization stepsfrom test setups, as well as mocking slow login methods.**Slow Initialization**Whenever create_app is called, multiple sub-modules are initialized, includingpermission roles, the old api, new api, logging, error handling, etc. Most ofthese are not relevant for many of the tests. This PR removes unnecessaryinitialization.**Slow login**Login functionality currently depends on hash functions to hash passwords. Bymocking the password hashing functionality in we can sidestep this slowdown.**Approaches analyzed but not used**- Adding indexes to FAB permissions tables. FAB queries roles and permissions  on unindexed name columns. Surprisingly, adding indexes didn't result in a  meaningful test speedup.- Caching api_connexion rendering. the `init_api_connexion` method is the slowest  initialization method, but is required for all `api_connexion` methods. It  looks like the Jinja rendering is the slowest part. I investigated storing  the rendered API in an intermediate state in a separate file, but this wasn't  feasible. If we want more speedups in the future, caching the api_connexion  app, much like we cache the current flask app, is likely the biggest win.	2
[AIRFLOW-821] Fix py3 compatibilityiteritems() does not exist in py3.Closes #2039 from bolkedebruin/AIRFLOW-821	0
[airflow/providers/cncf/kubernetes] correct hook methods name (#11008)	1
[AIRFLOW-5235] Fixes bug where K8s CI does not properly create user (#5838)	1
Make `airflow dags test` be able to execute Mapped Tasks (#21210)* Make `airflow dags test` be able to execute Mapped TasksIn order to do this there were two steps required:- The BackfillJob needs to know about mapped tasks, both to expand them,  and in order to update it's TI tracking- The DebugExecutor needed to "unmap" the mapped task to get the real  operator backI was testing this with the following dag:```from airflow import DAGfrom airflow.decorators import taskfrom airflow.operators.python import PythonOperatorimport pendulum@taskdef make_list():    return list(map(lambda a: f'echo "{a!r}"', [1, 2, {'a': 'b'}]))def consumer(*args):     print(repr(args))with DAG(dag_id='maptest', start_date=pendulum.DateTime(2022, 1, 18)) as dag:    PythonOperator(task_id='consumer', python_callable=consumer).map(op_args=make_list())```It can't "unmap" decorated operators successfully yet, so we're usingold-school PythonOperatorWe also just pass the whole value to the operator, not just the currentmapping value(s)* Always have a `task_group` property on DAGNodesAnd since TaskGroup is a DAGNode, we don't need to store parent groupdirectly anymore -- it'll already be stored* Add "integation" tests for running mapped tasks via BackfillJob* Only show "Map Index" in Backfill report when relevantCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	1
[AIRFLOW-XXX] Add How-To-Guide to GCP PubSub (#6497)	1
Depreciate private_key_pass in SFTPHook conn extra and rename to private_key_passphrase (#14028)	4
[AIRFLOW-XXX] Improve docstring of SQSHook (#6041)	1
Adding postgres dependencies	1
Bring back reset db explicitly called at CI entry (#7798)* Fix elasticsearch breaking the build* Bring back reset db explicitly called at CI entry	1
Backfill: Don't create a DagRun if no tasks match task regex (#16461)Backfill should not create a DagRun in case there is no any task that matches the regex.closes: #16460	2
Fix set task instance form test (#14501)* Fix set task instance form test* Return include_future non-string test	3
[AIRFLOW-XXXX] Description for how to create an user is incorrect in some docs (#7101)	2
[AIRFLOW-4838] Surface Athena errors in AWSAthenaOperator (#5467)When a Athena query results in a failure statethe Athena error message is available from theboto3 response. This commit surfaces that errormessage in the AWSAthenaOperator through theget_state_change_reason in AWSAthenaHook.	1
Fixing tests	3
Merge pull request #2480 from martinzlocha/emrAddStepsTemplate	1
[AIRFLOW-7058] Add support for different DB versions (#7717)	5
Fix typo in docs/stable-rest-api/redoc.rst (#10248)`shpinx` -> `Sphinx`	2
[AIRFLOW-1119] Fix unload query so headers are on first row[]Closes #2245 from th11/airflow-1119-fix	0
Adapting CONTRIBUTING.md	5
Add link to 2.0 Blog post in Changelog (#14602)We currently don't have anything mentioned about 2.0.0 in the changelog, this commit should help users get to our blog post highlighting 2.0 features.	2
Added Quizlet to "Who uses Airflow?" listCloses #2910 from dustinstansbury/patch-1	1
Merge pull request #1093 from criccomini/fix-empty-loadsAdd MySQL to BQ support for TINYINT	1
[AIRFLOW-5585] Remove docker context from build	2
Adding slot pool management to Airflow	1
[AIRFLOW-XXX] Add note about backwards incompatible changes (#4843)	4
Add Movember to users list (#8289)	1
Update limits of dependencies after `dask` test disabling (#22046)Some of the tests failed previously with typing extensions above 4.This PR attempts to relax the limit and check if the problemsstill appear.Also new tests (S3) started to fail when a new `responses` libraryversion has been released today.So this change also add limits to the responses library in orderto make sure the tests pass.Issue https://github.com/getsentry/responses/issues/511 has beenopened to raise it to `responses` library maintainers.	0
added code for ds_format	1
Add twine check for provider packages (#20619)Twine (which we use to upload packages to PyPI) has theability to run checks of packages before uploading them.This allows to detect cases like when we are using forbiddendirectives in README.rst (which delayed slightly preparing theDecember 2021 provider packages and resulted in #20614With this PR Twine check will be run for all packages in CIbefore we even attempt to merge such change that could breakthem.	4
[AIRFLOW-1343] Fix dataproc label formatDataproc label must conform to the following regex:[a-z]([-a-z0-9]*[a-z0-9])?. Current "airflow_version"label violates this format. This commit fixes the formatand updates the unittest to prevent future violations.Closes #2413 from fenglu-g/master	3
[AIRFLOW-3729] Support "DownwardAPI" in env variables for KubernetesPodOperator (#4554)https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#the-downward-api	5
Squelch more deprecation warnings (#21003)	2
[AIRFLOW-4106] instrument staving tasks in pool (#4927)	2
Add Zendesk as a company which is using Airflow	1
Show dataset readiness for the next run (#25141)	1
[AIRFLOW-XXX] Pin psycopg2 due to breaking change (#5036)	4
Simplify using XComArg in jinja template string (#12405)This changes XComArg string representation from 'task_instance.pull(...)'to '{{ task_instance.xcom_pull(...) }}' so users can use XComArgs withf-string (and other) in simpler way. Instead of doingf'echo {{{{ {op.output} }}}}' they can simply do f'echo {op.output}'.	1
Fixed a failing example	0
Update utils.pyadd support for Data Profiling with Oracle SQL	5
[AIRFLOW-1188] Add max_bad_records param to GoogleCloudStorageToBigQueryOperatorCloses #2286 from ckpklos/master	1
[AIRFLOW-XXXX] Add -- to rm in install_released_airflow_version (#7548)Adding this -- after rm -f helps in case when you have some weirdfiles that have spaces and -W in names.	2
Convert SQS Sample DAG to System Test (#24513)	3
[AIRFLOW-2542][AIRFLOW-1790] Rename AWS Batch Operator queue to job_queue- Improved the retries times to jobs below 60s- Renamed property queue to job_queue to preventAWS Batch and CeleryExecutor queue conflict- Added Breaking Chain note for the UPDATING.mdmaster- Fixed operator infinit loop- Added documentation warning about the Breakingchain- Fixed the commit parameter to keep it on Airflowguidelines- Fixed logging typo- rebased with masterChanges to be committed:modified:   ../../../UPDATING.mdmodified:   awsbatch_operator.pymodified:   ../../../tests/contrib/operators/test_awsbatch_operator.pyCloses #3436 from hprudent/master	3
[AIRFLOW-2748] add new command args for dbimport and dbexport command to qubole hookCloses #3597 from Joylal4896/master	1
Chart: Support job level annotations; fix jobs scheduling config (#16331)Add job level annotations as some tooling needs to be able to add them.Also fix jobs to use their own nodeSelector, affinity, andtolerations.closes #16291	5
Add support to specify kernel name in PapermillOperator (#20035)	1
Restructure the extras in setup.py and described them (#12548)Closes: #12544Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	1
[AIRFLOW-5900] avoid unnecessary system calls in heartbeat_callback (#6551)	5
add_section is only part of configuration.conf	5
[AIRFLOW-701] Add Lemann Foundation as an Airflow userCloses #1944 from fernandosjp/patch-2	1
[AIRFLOW-3821] Add replicas logic to GCP SQL example DAG (#4662)	2
[AIRFLOW-431] Add CLI for CRUD operations on poolsDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-431Testing Done:- Added unit testsCloses #1735 from r39132/master	3
Enforce code-block directives in doc (#9443)	2
DagFileProcessorManager: Start a new process group only if current process not a session leader (#23872)	1
Cache minikdc requirements	1
BugFix: Correctly handle custom `deps` and `task_group` during DAG Serialization (#16734)We check if the dag changed or not via dag_hash, so we need to correctly handle deps and task_group during DAG serialization to ensure that the generation of dag_hash is stable.closes https://github.com/apache/airflow/issues/16690	0
[AIRFLOW-437] Send TI context in kill zombiesFix to provide proper TI context while calling ti.handle_failure duringkill_zombies, as without the context handler_failure is of no use andits equivalent of marking those TIs as failed directly.This patch had conflicts when merged, resolved byCommitter: Ash Berlin-Taylor<ash_github@firemirror.com>Closes #1796 from msumit/AIRFLOW-437-2	0
Bugfix	0
Prepare mid-April provider documentation. (#22819)	2
Add __repr__ for Executors (#13753)Before:```python>>> from airflow.executors.local_executor import LocalExecutor>>> LocalExecutor()<airflow.executors.local_executor.LocalExecutor object at 0x7f49b47f8d68>```After:```python>>> from airflow.executors.local_executor import LocalExecutor>>> LocalExecutor()LocalExecutor(parallelism=32)```	2
[AIRFLOW-4750] Log identified zombie task instances (#5389)	2
Security scans are also selective now (#11674)The security scans take a long time, especially for python code- it is about ~18 minutes now. This PR reduces strain on theGitHub actions by only running the scan in pull requestswhen any of python/javascript code changed respectively.	4
Reduce response payload size of /dag_stats and /task_stats (#8633)Their response format is like {"example_dag_id": [{"state": "success", "dag_id": "example_dag_id"}, ...], ...}The dag_id is already used as the "key", but still repeatedly appear in each element,which makes the response payload size unnecessarily bigger	1
Chart: Add custom_airflow_environment to flower container (#12630)	1
Remove coerce_datetime usage from GCSTimeSpanFileTransformOperator (#22501)	2
Correct compile assets command in tmux welcome message (#25570)	1
Bugfix: allowing None value in templated fields	1
[AIRFLOW-5051] Better coverage integration (#5732)	3
Switch BigQuery hook to use BaseHook instead of DbApiHook	5
[AIRFLOW-XXX] Use full command in examples (#5973)	1
Pylint: Enable dict-*-not-iterating check (#7840)	0
Bugfix - scheduler skipping pools	0
[AIRFLOW-2622] add confirm option to SFTPOperator[]surfaces the confirm option in the SFTPOperatorprovided by theunderlying parmiko library, useful for when thereceiving servermoves the incoming file before the confirmationstep can be completedCloses #3542 from caddac/master	1
Chart: changelog for 1.1.0, add UPDATING to docs (#17149)	2
[AIRFLOW-7049] Persistent display/filtering of DAG status (#8106)	2
Add bulk_dump abstract method to DbApiHook (#1471)	5
Clearer information for webserver_config.py (#12412)So easier for doc readers who never used FAB-based RBAC UI.	1
hive provider: restore HA support for metastore (#19777)	1
Handle naive datetimes in REST APIi (#12248)	5
Ensure Tableau connection is active to access wait_for_state (#20433)	2
[AIRFLOW-979] Add GovTech GDSCloses #2149 from chrissng/add-govtech-gds	1
n Improved compatibility with Python 3.5+ - Convert signal.SIGTERM to int (#9207)Co-authored-by: Jiening Wen <phill84@Jienings-MacBook-Pro.local>	1
Add logo info to readme (#6349)	5
Fix S3ToRedshiftOperator (#19358)	1
Automatically create section when migrating config (#16814)Previously, if a config is migrated to a new section, the migration codewould crash with NoSectionError if the user does not add that section toairflow.cfg after upgrading Airflow. This patch automatically creates anempty section when that happens to avoid Airflow from crashing.	1
Cleaner default output when breeze starts (#23341)There was a bit of noise printed when Breeze started:* information about branch/python/image/backend used* information about actions performed (like fixing permissions)* information that docke image build is not needed* warnings about missing variablesThis PR marks all the messages as "info" and only prints themwhen --verbose flag is used and it adds default values for thevariables that generated warnings.	2
Adding doc_md feature to dag object	2
[AIRFLOW-1915] Relax flask-wtf dependency specificationCloses #2876 from wrp/flask-wtf	5
[AIRFLOW-1897][AIRFLOW-1873] Task Logs for running instance not visible in WebUIDue to the change in AIRFLOW-1873 we inadvertentlychanged the behavioursuch that task logs for a try wouldn't show up inthe UI until after thetask run had completed.Closes #2859 from ashb/AIRFLOW-1897-view-logs-for-running-instance	2
Updated tutorial_decorated_flows.rst to add links (#11510)Added links to the Decorated Flows AIP and to the Decorated Flows section of the Concepts doc.	2
Fix typo in timed_out (#10459)`timeouted` -> `timed_out`	2
Revert "[AIRFLOW-2903] Change default owner from "Airflow" to "airflow""This reverts commit 54ae12b59affd71d2de641826d7d62008f09bb4a.	4
[AIRFLOW-247] Add EMR hook, operators and sensors. Add AWS base hookCloses #1630 from rfroetscher/emr	1
More doc fixes	0
Revert "Switch to Debian 11 (bullseye) as base for our dockerfiles (#21378)" (#21874)This reverts commit 5d89dea56843d7b76d5e308e373ba16ecbcffa77.The issue is not a random IO timeout -- it's a problem with the file in the repo.Reverting this right now as all PRs are failing :(	0
Add map_index to pods launched by KubernetesExecutor (#21871)I also did a slight drive-by-refactor (sorry!) to rename `queued_tasksand `task` inside `clear_not_launched_queued_tasks` to `queued_tis` and`ti` to reflect what they are.	4
Small docs readme update (#14062)* Add instruction for running docs locally* Fix RST syntax* Update docs/README.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>	2
Update installation page (#15737)The installation page is updated with:* updated Python/Kubernetes support policies* added download information to installation page	5
Pass location using parmamter in Dataflow integration (#8382)	5
Fix docstring in DagFileProcessor._schedule_task_instances (#8948)	2
[AIRFLOW-6515] Change Log Levels from Info/Warn to Error (#8170)	0
Add Snowflake provider to boring cyborg automation (#14432)	1
Adding a PR Template	1
[AIRFLOW-6565] BigQuery - replace deprecated connection parameters (#7173)	2
[AIRFLOW-XXX] Speed up RBAC view tests (#4162)Not re-creating the FAB app ones per test functions took the run time ofthe TestAirflowBaseViews from 223s down to 53s on my laptop, _and_ madeit only print the deprecation warning (fixed in another PR already open)once instead of 10+ times.	0
[AIRFLOW-5744] Environment variables not correctly set in Spark submit operator (#6796)	1
option to push xcom from bashoperator	1
[AIRFLOW-XXX] Add resources & links to CONTRIBUTING.rst (#6405)	2
Update MySqlOperator example dag (#21434)	2
Add Snowflake operators based on SQL Checks  (#17741)Add three new Snowflake operators based on SQL ChecksThe SnowflakeCheckOperator, SnowflakeValueCheckOperator, andSnowflakeIntervalCheckOperators are added as subclasses of their respectiveSQL Operators. These additions follow the conventions set in the BigQueryOperatorssubclassing from the same SQL_CheckOperators.closes: #17694	1
Remove CodeQL from PRS. (#12406)As discussed in https://lists.apache.org/thread.html/r18cc605bbdb6695c1d31e0706f1b033401f6fa6a19cd0584d7be6cc9%40%3Cdev.airflow.apache.org%3Eremoving CodeQL from PRs.	5
Sanity check for MySQL's TIMESTAMP column (#19821)	5
Fix failing main (#20871)When I merged #18724 the jobs ran successfully but it's now failing in main.This PR fixes itCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	0
Even more typing in operators (template_fields/ext) (#20608)Part of #19891There were few more places where I missed adding Sequencetyping - including examples (also converted to tuples) andalso template_ext. Also in a few places iterable was left	1
Typo fix	0
Resolve 404s when trying to click through to the task instances view	1
[AIRFLOW-3742] Fix handling of "fallback" for AirflowConfigParsxer.getint/boolean (#4674)We added (and used) fallback as an argument on `getboolean` but didn'tadd it to the method, or add tests covering those "casting" accessors,so they broke.This fixes those methods, and adds tests covering them	3
[AIRFLOW-XXX] Mention Oracle in the Extra Packages documentation (#4987)Add the `oracle` subpackage to the list of available subpackages	1
interia.pl use Airflow too (#5081)	1
set max tree width to 1200px (#16067)the totalwidth of the tree view will depend on the window size like before, but max out at 1200px	1
[AIRFLOW-1330] Add conn_type argument to CLI when adding connectionCloses #2525 from mrkm4ntr/airflow-1330	1
Correct the :mod: documentation for s3_to_redshift_operator (#17115)	1
Don't try to create automigration for celery tables (#22120)	1
Merge pull request #679 from airbnb/dag_detailsAdding a dag details page	2
Fix Viewing Dag Code for Stateless Webserver (#8178)Porting it from https://github.com/apache/airflow/commit/5e6aa3cc9c78d6be6d9578222469acab57251ef7 (v1-10-test)	3
Type TaskInstance.task to Operator and call unmap() when needed (#21563)	1
Fix tests for mssql after SQLA 1.4 upgrade (#21303)The way SQLA 1.4 constructed the query then `exeuction_date.in_([])`changed, and as a result it started failing.But we don't even need to ask the database in this case, as we know itwon't return any rows.	5
[AIRFLOW-1863][AIRFLOW-2529] Add dag run selection widgets to gantt viewAdd same widgets to filter and select dag runknown from graph viewto the gantt chart view. Extract common code tohandle requestparameters and DB query.Closes #3450 from seelmann/AIRFLOW-1863-gantt-view	5
Remove duplicate line from 1.10.10 CHANGELOG (#10289)	4
Updating Google Cloud example DAGs to use XComArgs (#16875)	1
[AIRFLOW-2617] add imagePullPolicy config for kubernetes executorCloses #3500 from Cplo/k8sexecutor	5
Update local_task_job.py (#9746)Removing the suicide joke.	4
Fix docs about login for hdfs connections (#17936)	2
Revert "Support google-cloud-datacatalog 3.0.0 (#13224)" (#13482)This reverts commit feb84057d34b2f64e3b5dcbaae2d3b18f5f564e4.	4
[AIRFLOW-2715] Use region setting when launching Dataflow templates (#4139)To launch an instance of a Dataflow template in the configured region,the API service.projects().locations().teplates() instead ofservice.projects().templates() has to be used. Otherwise, all jobs willalways be started in us-central1.In case there is no region configured, the default region `us-central1`will get picked up.To make it even worse, the polling for the job status already honors theregion parameter and will search for the job in the wrong region in thecurrent implementation. Because the job's status is not found, thecorresponding Airflow task will hang.	1
Fix #21096: Support boolean in extra__snowflake__insecure_mode (#21155)	1
Added DataprepGetJobsForJobGroupOperator (#10246)	5
Merge pull request #1128 from bolkedebruin/hivemeta_saslAdd GSSAPI SASL to HiveMetaStoreHook.	1
[AIRFLOW-3639] Fix request creation in Jenkins Operator (#4450)Change Jenkins Operator to work with native Jenkins library method to configure REST request headers correctly	5
Update upgrading.rst with detailed code example of how to resolve post-upgrade warning (#19993)	2
Update ``README.md`` to point to Airflow 2.1.3 (#17793)	2
[AIRFLOW-5561] Relax httplib2 version required for gcp extra (#6194)As part of [AIRFLOW-3971] a minor version range dependency wasintroduced that means that only older versions of httplib2 canbe used.I'm relaxing this dependency as I tested with the latest 0.13.1and it works correctly	1
CLI: Fail ``backfill`` command before loading DAGs if missing args (#18994)I was looking through some of the CLI code last week trying to improve the speed of `airflow user` commands and I noticed this small issue. If neither the `start_date` or `end_date` argument is provided then the command will fail, but it will first parse all of the DAGs which can take up to several minutes in large deployments. Now the command will fail faster, allowing the user to adjust their command and retry.	1
[AIRFLOW-6387] print details of success/skipped task (#6956)	5
[AIRFLOW-2163] Add HBC Digital to users of airflow	1
Postgres operator unit tests	3
fix typo in firebase/example_filestore DAG (#10875)	2
Move the old ./breeze script to scripts/tools/setup_breeze (#25584)We used to use Breeze via ./breeze script in the main airflowfolder, but it's already long enough time after new breezeintroduction to get-rid of it. However, the Bash script itselfis pretty useful to automate Breeze installation on POSIX-compliantOS-es, so turning it into an installation script seems like a goodidea.	1
Improve system tests for Cloud Build (#8003)	3
[AIRFLOW-3742] Respect the `fallback` arg in airflow.configuration.get (#4567)This argument is part of the API from our parent class, but we didn'tsupport it because of the various steps we perform in `get()` - thismakes it behave more like the parent class, and can simplify a fewinstances in our code (I've only included one that I found here)	1
Support extraContainers configuration in Helm Chart (#13735)closes https://github.com/apache/airflow/issues/13211	0
Updating command to run all tests on the last commit (#16997)The previous command read `./breeze static-check all -- --ref-from HEAD^ --ref-to HEAD`, however the `options` used should be `--from-ref` and `--to-ref`.	1
[AIRFLOW-2502] Change Single triple quotes to double for docstrings- Changed single triple quotes to double quotecharacters to be consistent with the docstringconvention in PEP 257Closes #3396 from kaxil/AIRFLOW-2502	2
Replace license setup.cfg file pre-commit with Python (#26140)Part of: #26020	2
[AIRFLOW-1716] Fix multiple __init__ def in SimpleDagCloses #2692 from MortalViews/master	2
Fix broken Markdown refernces in Providers README (#10483)`#provider-class-summary` -> `#provider-classes-summary`	1
Fix grammar in UPDATING.md (#10841)`changes` -> `changed`	4
[AIRFLOW-2150] Use lighter call in HiveMetastoreHook().max_partition()Call self.metastore.get_partition_names() instead ofself.metastore.get_partitions(), which is extremely expensive forlarge tables, in HiveMetastoreHook().max_partition().Closes #3082 fromyrqls21/kevin_yang_fix_hive_max_partition	0
[AIRFLOW-3576] Remove unnecessray arg 'root' for /delete in dag.html (#4380)'root' is not used anywhere in `delete` method in eitherwww/views.py or www_rbac/views.py.Having it in url_for("airflow.delete", dag_id=dag.dag_id, root=root)in dag.html is meaningless.	2
Fix external elasticsearch logs link (#16357)During the 2.0 upgrade, the external log link when using elasticsearchremote logs was broken. This fixes it, including it only being shown if`[elasticsearch] frontend` is set.	1
Support creation of configmaps & secrets and extra env & envFrom configuration in Helm Chart (#12164)* Enable provisionning of extra secrets and configmaps in helm chartAdded 2 new values:*  extraSecrets*  extraConfigMapsThose values enable the provisionning of ConfigMapsand secrets directly from the airflow chart.Those objects could be used for storing airflow variablesor (secret) connections info for instance(the plan is to add support for extraEnv and extraEnvFrom later).Docs and tests updated accordingly.* Add support for extra env and envFrom items in helm chartAdded 2 new values:*  extraEnv*  extraEnvFromThose values will be added to the defintion ofairflow containers. They are expected to be string(they can be templated).Those new values won't be supported by "legacy" kubernetesexecutor configuration (you must use the pod template).Therefore, the value 'env' is also deprecated as it's kindof a duplicate for extraEnv.Docs and tests updated accordingly.	5
[AIRFLOW-2203] Remove Useless Commands.self.tasks is a temp list gen from self.task_dict. no reason to appendto it	1
[AIRFLOW-213] Add "Closes #X" phrase to commit messages	1
[AIRFLOW-2018][AIRFLOW-2] Make Sensors backward compatibleTo keep compatibility until Airflow 2.0, we wantto keep the sensorsapi compatible.Closes #2961 from Fokko/fd-sensor-backward-compatible	1
Unit tests jenkins hook (#9767)	1
Migrate Google example automl_nl_text_extraction to new design AIP-47 (#25418)related: #22447, #22430	1
Databricks: Correctly handle HTTP exception (#22885)Exception for non-existent repo wasn't correctly handled for DatabricksRepos operations	5
[AIRFLOW-7023] Remove duplicated package definitions in setup.py (#7675)	1
[AIRFLOW-1290] set docs author to 'Apache Airflow'	2
Add "Greater/Smaller than or Equal" to filters in the browse views (#20602) (#20798)	1
[AIRFLOW-342] Do not use amqp, rpc as result backendamqp and rpc (and redis most likely) cannot storeresults for taskslong enough.Closes #2830 from bolkedebruin/AIRFLOW-342	1
Divide commands into "Actions"/"Groups" sections (#8456)	5
Add docker-context-files detection and cleanup flag. (#15593)When building images for production we are using docker-context-fileswhere we build packages to install. However if those context filesare not cleaned up, they unnecessary increase size and time neededto build image and they invalidate the COPY . layer of the image.This PR checks if docker-context-files folder contains just readmewhen Breeze build-image command is run (for cases whereimages are not built from docker-context-files). Inversely italso checks that there are some files in case the image isbuilt with --install-from-docker-context-files switch.This PR also ads a --cleanup-docker-context-files switch toclean-up the folder automatically. The error mesages also helpthe user instructing the user what to do.	1
Update link to match what is in pre-commit (#16408)[The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema) in chart pytests too. This updated fork's latest schema is 1.21.1, and has had changes within the last month.	4
[AIRFLOW-XXX] Pin version of tornado pulled in by Celery. (#4815)https://github.com/tornadoweb/tornado/issues/2604	0
Much easier to use and better documented Docker image (#14911)Previously you had to specify AIRFLOW_VERSION_REFERENCE andAIRFLOW_CONSTRAINTS_REFERENCE to point to the right versionof Airflow. Now those values are auto-detected if not specified(but you can still override them)This change allowed to simplify and restructure the Dockerfiledocumentation - following the recent change in separating outthe docker-stack, production image building documentation hasbeen improved to reflect those simplifications. It should bemuch easier to grasp by the novice users now - very cleardistinction and separation is made between the two types ofbuilding your own images - customizing or extending - and itis now much easier to follow examples and find out how tobuild your own image. The criteria on which approach tochoose were put first and forefront.Examples have been reviewed, fixed and put in a logicalsequence. From the most basic ones to the most advanced,with clear indication where the basic aproach ends and wherethe "power-user" one starts. The examples were also separatedout to separate files and included from there - also theexample Docker images and build commands are executableand tested automatically in CI, so they are guaranteedto work.Finally The build arguments were split into sections - from mostbasic to most advanced and each section links to appropriateexample section, showing how to use those parameters.Fixes: #14848Fixes: #14255	0
Add more metadata to `Chart.yaml` (#15866)- Adds more metadata like `appVersion`, `home`, `maintainers`, `sources` etc- Remove `tests` from `helm package`	3
Add __repr__ to ParamsDict class (#25305)Fixes #25295	0
[AIRFLOW-6987] Avoid creating default connections (#7629)	1
Add a dedicated "free disk space" step to fix CI (#8426)	0
SID Oracle DB connection support	1
Add `uri_pattern` query param to Get `/datasets` endpoint (#25411)	5
SLA can be set at task level, email notifications get sent	1
Docs: Change 10 minutes to 100 minutes in ``worker_refresh_interval`` (#16369)6000 seconds = 100 minutes not 10 minutesI forgot one zero when writing that down :)	1
[AIRFLOW-XXXX] Move airflow-config-yaml pre-commit before pylint (#7108)	5
Use found pod for deletion in KubernetesPodOperator (#22092)Due to bad user configuration, it's possible that pod creation fails because pod with name already exists.  Then in cleanup, the pod that was already there is deleted.  When we use find_pod it looks up based on more than name, so we are confident if we found a pod there it's safe to delete.	4
[AIRFLOW-1682] Make S3TaskHandler write to S3 on closeApplies fix for GCSTaskHandler made as part of AIRFLOW-1676 to fixthe same bug in S3TaskHandler. Also rename s3_log_read function tos3_read for consistency with other functions in airflow.utils.logCloses #2664 from ahvigil/AIRFLOW-1682	2
Stronger language about Docker Compose customizability (#22304)* Stronger language about Docker Compose customizabilityDespite our warnings, our users continue treating the DockerCompose that we exposed as something that should be easy toextend and customize for their own needs, yet they continueto struggle with some basic behaviour of containers, Docker Composeand how they interact. This results in vast space of potentialproblems as Docker Compose gives the user a false premise ofsomething that "just works" where it requires quite a deepunderstanding on how it works.When you get things wrong with Docker Compose, you often end upwith extremely confusing messages, that might suggest that theproblem is with Airflow, but really the problem is with how usersinteract with their custom Docker images, registries, pulling,networking, mounting volumes and plenty other things.While this is the same with Kubernetes and Helm Chart, Helm Chart makesit infinitely easier to customize in declarative way (this is whatour values.yaml does) and anything that has not been foreseen by HelmChart developers is "hard" by definition.Docker Compose makes no such distinction. You really can't make DockerCompose customizable by configuration, and any customization in itrequires modifying the compose file and for people who do not knowwhat they are doing will eventually lead to errors that they are notable to diagnose and leads to creation of "Airlfow isssues", where theyshould be brought to "Docker Compose" issues.Example of that is here: https://github.com/apache/airflow/discussions/22301where there are at least two issues that are not reproducible withoutknowing in detail what the user has done, how the image was buildand distributed, and how the docker-compose installation interactedwith them. This leads to a terrible distraction for supportingusers of Airflow as the issues are really Docker Compose issues andAirflow maintainers should not be involved in solving those.This PR adds a bit stronger language and statement about the scopeand customizability of the Quick Start Docker Compose of ours. Notonly mentioning "Lack of Production Readiness" but also theresponsibility of the user to understand and diagnose docker composeerrors on their own and setting expectations that issues with DockerCompose running should be directed elsewhere.* Update docs/apache-airflow/start/docker.rst* Update docs/apache-airflow/start/docker.rstCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>* Update docs/apache-airflow/start/docker.rstCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>	1
:bug: (BigQueryHook) fix compatibility with sqlalchemy engine (#19508)	0
Add more operators to example DAGs for Cloud Tasks (#13235)	2
Fix "run_id" k8s and elasticsearch compatibility with Airflow 2.1 (#22385)The execution_date -> run_id change (#21960) attempted to make itAirflow 2.1 backwards-compatible, but the problem is that inAirflo2 2.1 retrieving `run_id` attribute of TaskInstance throwsAttributeError rather than returns None. It turns out that whenyou have a field defined in an ORM model, it will never throwAtributeError (even if you delete the attribute it will returnNone.Accesising `run_id` with getattr raisesAttributeError in Airflow 2.1 (because there TaskInstance has norun_id defined).This PR adds automated pre-commit to check if other providershave not suffered (and will not suffer) the same problem.	0
add auto refresh to dags home page (#22900)* add auto refresh to dags home page* fix lint errors* fix lint* stop refresh when page is not focused or no active dag runs. change css for layout* remove margin for refresh switch* Update airflow/www/static/css/main.cssCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* fix text case* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* add comment on refresh interval* refactor last dag run handler date updateCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>	5
Revert "[AIRFLOW-1955] Do not reference unassigned variable"This reverts commit 9565a9879280d83c6c3987d3a6f8b8933168cedf.	4
Add %z for %(asctime)s to fix timezone for logs on UI (#24373)	2
remove json parse for gantt chart (#22780)	2
Force installing Breeze on CI (#23196)Since our environment is re-used between runs it might be thatdifferent version of Breeze has been installed in the cachepreviously. This change force-installs breeze every time thejob is started to make sure current Breeze version is used.	1
Increase the default ``min_file_process_interval`` to decrease CPU Usage (#13664)With the previous default of `0`, the CPU Usage mostly stays around 100.As in Airflow 2.0.0, the scheduling decisions have been moved out fromDagFileProcessor to Scheduler, we can keep this number high.closes https://github.com/apache/airflow/issues/13637	0
Add documentation create/update community providers (#15061)	1
Spinner loading wheel on tree view	5
Make tag fetching when preparing providers optional. (#25236)* Make tag fetching when preparing providers optional.Tag fetching is only needed to make sure we do not generatepackages that have already been generated. However this is justan optimisation for CI runs. There is no harm if the tags arenot refreshed and we generate the package again locally.Tag fetching might faile in various cases - for example whenyou are in corporate environment and require specific certificatesto be available or when you are working from a worktree.After this change fetching tag will produce warning when there isan error and instruction on how you can fetch tags manually.* Update dev/provider_packages/prepare_provider_packages.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>	1
doc: note on skipping a branch	2
Pin `kubernetes` to a max version of 11.0.0. (#11974)12.0.0 introduces `TypeError: cannot serialize '_io.TextIOWrapper'object` when serializing V1Pod's in `executor_config`.	5
Old json boto compat removed from dynamodb_to_s3 operator (#8987)	1
Fix calling `get_client` in BigQueryHook.table_exists (#9916)Adding `project_id` argument to `get_client` method otherwise this call always falls back to the default connection id.	1
Fix race condition with dagrun callbacks (#16741)Instead of immediately sending callbacks to be processed, wait untilafter we commit so the dagrun.end_date is guaranteed to be there whenthe callback runs.	1
Centralizing logging level into settings.py file	2
Fix static checks	0
Don't pickle when running subdags	2
Only schedule DagRuns between start and end dates	5
Chart: Allow setting annotations on Airflow pods & `Configmap` (#15238)This PR adds a new field (`airflowConfigAnnotations`) that allows users to add `annotations` to the main `configmap.yaml` file. I ended up setting up a new testing file as I didn't find a file where this specifically fit, but if it should be moved elsewhere let me know.closes https://github.com/apache/airflow/issues/13643	0
[AIRFLOW-XXX] Update PR template	5
Close issues that are pending response from the issue author (#15170)Based on the meeting notes in https://docs.google.com/document/d/1Fx46SoOnNLiqZKtrC-tOHj3zFlZfQwWuR2LRFXJnWqw/ we had decided that we will automate closing the issues if an issue does not receive response from the issue author in 30 days (+7 days after stale).	0
Add support for managed identity in WASB hook (#16628)* Add support for managed identity in WASB hook* Log info that we're using managed identity credential* Must use managed identity credential here if previous branch is engaged* Add comment that managed identity will be attempted if no other authentication is provided	1
[AIRFLOW-XXX] Remove trailing whitespaces from UPDATING.md (#6940)	5
UI scaffold views, routes, and layout containers for Runs and Tasks (#15041)	1
Switch postgres from 10 to 13 (#11785)Seems that postgres is really stable when it comes to upgrades,so we take the assumption that if we test 9.6 and 13, and theywork, all the versions between will also work.This PR changes Postgres 10 to 13 in tests  and updates documentationwith all the versions in between.	2
Change 2.0.1 to 2.0.2 in docs (#15459)Since 2.0.2 was released yesterday, our guides and Breeze should pointto that.	2
CHANGELOG for 1.8Closes #2000 from alexvanboxel/pr/changelog	4
Add max_ingestion_time to DruidOperator docstring (#18693)	2
Update INSTALL_PROVIDERS_FROM_SOURCES instructions. (#23938)	1
[AIRFLOW-5389] better organized scripts for building CI docker deps	2
Reverting	4
[AIRFLOW-3367] Run celery integration test with redis broker. (#4207)	3
Split contributor's quick start into separate guides. (#23762)The foldable parts were not good. They made links not to work aswell as they were not too discoverable.Fixes: #23174	0
Bump undici from 5.8.0 to 5.9.1 in /airflow/www (#25801)Bumps [undici](https://github.com/nodejs/undici) from 5.8.0 to 5.9.1.- [Release notes](https://github.com/nodejs/undici/releases)- [Commits](https://github.com/nodejs/undici/compare/v5.8.0...v5.9.1)---updated-dependencies:- dependency-name: undici  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Remove WTforms from setup.py (#8590)	1
Fix Amazon EKS example DAG raises warning during Imports (#23849)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>	1
[AIRFLOW-XXX] Make string type uniform in docstrings (#5750)	2
[AIRFLOW-2471] Fix HiveCliHook.load_df to use unused parametersThis PR fixes HiveCliHook.load_df to passload_file the parameter called create andrecreate, which are currently ignored, aspart of kwargs.Closes #3390 from sekikn/AIRFLOW-2471	1
Add autodetect arg in BQCreateExternalTable Operator (#22710)* Add autodetect parameter* Update docstring* Update google provider documentation	2
[AIRFLOW-2041] Correct Syntax in python examplesI parsed it with the ol' eyeball compiler. Someonecould flake8 it better, perhaps.Changes: - correct `def` syntax on line 50 - use literal dict on line 67Closes #2479 from 0atman/patch-1	1
drop alembic version_table in resetdb	5
[AIRFLOW-6104] [AIP-21] Rename datastore service (#6853)	5
[AIRFLOW-6340] Make tests/contrib pylint compatible (#6896)	3
Getting Gitter badge to line up in README	1
Fix spelling (#11457)	0
List upstream dataset events (#25300)* add upstream dataset events to run details* show only task id link* fix timestamps and extra field* fix relative import path* improve dataset events table	5
use exceptions intead of returning tuple	1
Allow to specify path to kubeconfig in KubernetesHook (#10453)	1
[AIRFLOW-7045] Update SQL query to delete RenderedTaskInstanceFields (#8051)This is because "The composite IN construct is not supported by all backends"Based on discussion in https://github.com/apache/airflow/pull/6788#discussion_r391268396	1
Fix Clear task instances endpoint resets all DAG runs bug (#17961)	0
[AIRFLOW-4760] Fix zip-packaged DAGs disappearing from DagBag when reloaded (#5404)	2
Replace foreign key constraints with foreign annotation (#12603)closes https://github.com/apache/airflow/issues/12448	0
[readme] add Max's november conf ETL tips & tricks	5
Add Google leveldb hook and operator (#13109) (#14105)* Add Google leveldb hook (#13109)* Add write_batch, options for DB creation(comparator and other) plus fixes (#13109)* Fix some static checks, add docs (#13109)* Apply suggestions from code reviewCo-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>* Fix some otger static checks and docs (#13109)* Fix tests and some build-docs checks (#13109)* Apply suggestions from code reviewCo-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>* Fix build-docs checks (#13109)* Fix build-docs checks (#13109)* fixup! Fix build-docs checks (#13109)* Rewrite example dag as in google package (#13109)* Add extra options in operator, fix docstrings (#13109)* Update airflow/providers/google/leveldb/operators/leveldb.pyCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>* Add system testing and docstrings (#13109)* Fix comparator place in spelling wordlist(#13109)Co-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>Co-authored-by: Kamil Bregula <kamilbregula@Kamils-MacBook-Pro.local>Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>	1
Making collapsed nodes stand out	1
[AIRFLOW-3464] Move SkipMixin out of models.py (#4386)	4
Update more occurrences of gcp to google (#9842)	5
Fix Py SDK version (#20046)	0
[AIRFLOW-6608] Change logging level for PythonOperator Env exports (#7246)	1
SLA Alert Callback : Supporting the ability to do optional SLA alert call backs and emailing	1
expand airflow.cfg and defaults	5
Utilize util method to yield versioned doc link (#14047)	2
[AIRFLOW-225] Better units for task duration graphRight now the job duration window defaults to hours, which for short lived tasksresults in numbers out to five decimals. This patch adjusts the scale of the Y-axisin accordance with the maximum value of the durations to be shown.	1
Update ImportError items instead of deleting and recreating them (#22928)* Update ImportError items instead of deleting and recreating themEach time a dag with import error is parsed, the ImportError record is deletedand a new record is created. For example, say I have two dags with import errors,initially, the import error id will be dag_1:import_error.id=1, dag2:import_error.id=2.In the next dag parsing, the import error will increase. dag_1:import_error.id=3,dag_2:import_error.id=4 and it continues like that.This makes it impossible for the get import error REST API endpoint to be consistentThis PR fixes this issue by updating the existing record and creating a new one if no recordexists	1
refactor connection tests (#18881)	3
Revert "[AIRFLOW-2860] DruidHook: time variable is not updated correctly when checking for timeout (#3707)"This reverts commit d12aacd552878308f9b1c3663414bb7c00c0632b.	4
[AIRFLOW-3382] Fix incorrect docstring in DatastoreHook (#4222)Correct docstring in DatastoreHook	5
GCP Secrets Optional Lookup (#12360)	5
Merge pull request #1079 from biln/masterSID Oracle DB connection support	1
[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737)Adds an additional endpoint to the experimental APIto return the paused state of a DAG.	2
Merge pull request #624 from airbnb/max_runs_flagAdding a warning when the max number of active DAG runs has been reached	1
Adding to inits	5
Quarantine test TestSchedulerJob.test_scheduler_task_start_date (#12860)	5
Use proper default airflow_constraints_reference (#26148)In case image was built by `breeze` and not by `build` command.default value of the arg was wrong (empty) rather than defaultconstraint branch. That led to early cache invalidation and muchlonger image build than necessary.	5
Merge pull request #1245 from underyx/patch-4Fix airflow.utils deprecation warning code being Python 3 incompatible	2
[AIRFLOW-4741] Optionally report task errors to Sentry (#5407)This commit intends to add Sentry to the core functionality ofAirflow. It takes an approach like Statsd for simple integration.The commit makes use of tagging and breadcrumbs for more errorinformation.	5
Add and document description fields (#25370)* Add description to variable API resultsVariables can have descriptions, so that should be included in API results.	1
Properly style code blocks in links (#20938)	2
[AIRFLOW-2511] Fix improper failed session commit handling causing deadlocks (#4769)	1
[AIRFLOW-6639] Remove duplicate Output format choices from CLI docs (#7259)	2
Merge pull request #1207 from underyx/patch-1Allow disabling periodic committing when inserting rows with DbApiHook	5
[AIRFLOW-472] Add liligo as an Airflow userDear Airflow Maintainers,Could you pls add liligo to the Airflow users listin the Readme?Thanks in advance!Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-472Closes #1769 from tromika/liligo	0
[AIRFLOW-6885] Delete worker on success (#7507)Users now have the option to only delete worker pods when they are successfulCo-authored-by: Daniel Imberman <daniel@astronomer.io>	1
[AIRFLOW-947] Improve exceptions for unavailable Presto clusterThis improves error logging when the Presto cluster is unavailableand the underlying error is a 503 http response. This introspectsthe error to prevent trying to access the 'message' attribute whennot present.	1
Pin google-cloud-kms to ..,<2.0.0 due to breaking changes (#10088)	4
Fix left-over function in breeze-legacy (#23276)Fixes: #23272	0
Change default DAG view from tree view to graph view	2
made changes suggested by arthur	4
Cosmetic <nobr> on icon list in DAGs view	2
[AIRFLOW-6566][AIRFLOW-4029] Replace uses of imp still left with importlib. (#7174)	2
[AIRFLOW-7009] Use keywords arguments for DagFileStat (#7651)	2
removing requirements.txt as it is uni-dimensional	5
Fix typo in pre_commit_breeze_cmd_line.sh (#9682)`genereate` -> `generate`	2
[AIRFLOW-2662][AIRFLOW-2397] Add k8s node_selectors and affinityAdd the ability to set the node selection and the affinityfor the k8s executorCloses #3535 from Cplo/affinity	5
[AIRFLOW-2282] Fix grammar in UPDATING.mdAlso remove trailing whitespace.	4
Correctly deserialize dagrun_timeout field on DAGs (#8735)We weren't deserializing this correctly (it was left as a float) butnothing _was_ using it, and we hadn't explicitly tested it.We already have example dags with this field, so we just need to checkfor this field.	2
[AIRFLOW-542] Add tooltip to DAGs links iconsCloses #1817 from mmmaia/master	2
Fix case when SHELL variable is not set in kubernetes tests (#26235)When SHELL variable is not set, kubernetes tests will fall backto using 'bash'	1
`DataprocHook`: Remove deprecated function `submit` (#23389)	1
add buttons for Mark Success Future+Past	1
Remove skipping tests of Mssql for Python 3.8 (#25800)in https://github.com/apache/airflow/pull/25214 we missed the deprecation tests in always folder	3
Update ui.rst (#24514)Fix minor typos	2
Add map_index to XCom model and interface (#22112)* Add map_index to XCom primary keyThis is not actually stored correctly yet. We still need to fix the XCominterface.* Add map_index to XCom interfaceThis adds an additional (optional) map_index argument to XCom'sget/set/clear interface so mapped task instances can push to thecorrect entries, and have them pulled correctly by a downstream.To make the XCom interface easier to use for common scenarios, aconvenience method get_value is added to take a TaskInstanceKey thatautomatically performs argument unpacking and call get_one underneath.This is not done as a get_one overload to simplify the implementationand typing.	1
Fix to allow deepcopying of DAG with subdags	2
Re-add --force-build flag (#24061)After #24052 we also need to add --force-build flag as forPython 3.7 rebuilding CI cache would have been silently ignored asno image building would be needed	1
Bump ws from 6.2.1 to 6.2.2 in /airflow/ui (#25804)Bumps [ws](https://github.com/websockets/ws) from 6.2.1 to 6.2.2.- [Release notes](https://github.com/websockets/ws/releases)- [Commits](https://github.com/websockets/ws/compare/6.2.1...6.2.2)---updated-dependencies:- dependency-name: ws  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>	1
Add test to guard against command arg help message regression (#8561)follow up for PR #8552.	3
Add filter by state in DagRun REST API (List Dag Runs) (#20485)	1
[AIRFLOW-488] Fix test_simple failMake unittest test_simple pass on all platformsincluding MacOs.Closes #1782 from forevernull/master	4
Bugfix	0
Temporarily disable PROD image check until Azure Blob is fixed (#12679)This PR disables temporarily PIP check result for productionimage, until the fix to switch Azure Blob to v12 is fixed.	0
Test exact match of Executor name (#10465)Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match	3
Rename 'resources' arg in Kub op to k8s_resources (#24673)	5
Add new committers (#14544)https://lists.apache.org/thread.html/r33d43764cfb4a3a5f8e463c543229de3f13ee86a9713e7263ef34d39%40%3Cdev.airflow.apache.org%3E	1
Handle IntegrityError while creating TIs (#10136)While doing a trigger_dag from UI, DagRun gets created first and then WebServer starts creating TIs. Meanwhile, Scheduler also picks up the DagRun and starts creating the TIs, which results in IntegrityError as the Primary key constraint gets violated. This happens when a DAG has a good number of tasks.Also, changing the TIs array with a set for faster lookups for Dags with too many tasks.	2
AIP-47 - Migrate Airbyte DAGs to new design (#25135)	1
Bump version of sphinx-airflow-theme (#13054)	5
Adding new SageMaker operator for ProcessingJobs (#9594)	1
Fix precedence of affinity, nodeSelector, and tolerations (#20641)	5
Standardize AWS Lambda naming (#20365)	5
Doc: Fix incorrect filename references (#20277)Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.	2
Merge pull request #2 from airbnb/masterUpdate from origin	5
Allow searching/filtering Browse Task Instances view by map_index (#22117)	1
Add verification steps when releasing the images. (#24520)After the images are pushed in CI we are running the verificationof the AMD image now.This cannot be really done during building and pushing the image,because we are using multi-platform images using remote buildersso the image is not even available locally, so we need to actuallypull the images after they are built in order to verify them.This PR adds those features:* ability to pull images for verification with --pull-flag* ability to verify slim images (regular tests are skipped and  we only expect the preinstalled providers to be available* the steps to verify the images (both regular and slim) are  added to the workflow	1
Dev:`constraints-latest` needs to be force-pushed (#21746)	1
Elasticsearch Provider: Fix logs downloading for tasks (#14686)Without this, Webserver fails with:```[2021-03-09 18:55:19,640] {base.py:122} INFO - POST http://aa.aa:9200/_count [status:200 request:0.142s][2021-03-09 18:55:19 +0000] [64] [ERROR] Error handling requestTraceback (most recent call last):  File "/usr/local/lib/python3.7/site-packages/gunicorn/workers/sync.py", line 181, in handle_request    for item in respiter:  File "/usr/local/lib/python3.7/site-packages/werkzeug/wsgi.py", line 506, in __next__    return self._next()  File "/usr/local/lib/python3.7/site-packages/werkzeug/wrappers/base_response.py", line 45, in _iter_encoded    for item in iterable:  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py", line 84, in read_log_stream    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py", line 58, in read_log_chunks    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 217, in read    log, metadata = self._read(task_instance, try_number_element, metadata)  File "/usr/local/lib/python3.7/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py", line 186, in _read    and offset >= metadata['max_offset']TypeError: '>=' not supported between instances of 'str' and 'int'```	1
Getting setup.py working, moving folders around	4
Add triggering dataset events to ti context (#26168)This allows downstream tasks to get details about the events thattriggered the dagrun.Co-authored-by: Jed Cunningham <jedcunningham@apache.org>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>	2
Don't let webserver run with dangerous config (#12747)	5
Merge pull request #233 from airbnb/slack_operatorSlack operator	1
[AIRFLOW-4269] Minor acceleration of jobs._process_task_instances() (#5076)* [AIRFLOW-4269] Minor acceleration of jobs._process_task_instances()by breaking from unnecessary steps of a for-loop* [AIRFLOW-4269] Improve log.info a bit	5
[AIRFLOW-3096] Reduce DaysUntilStale for probot/stale	5
Better multiline string formatting for chart docs (#15881)This will properly format multiline strings in the helm chart parameterdocs.	2
[AIRFLOW-2445] Allow templating in kubernetes operatorCloses #3338 from ese/k8s-templating	1
Improved unit tests for open_maybe_zipped function. (#14114)Implemented "real" tests for open_maybe_zipped that test file contentreading without relying on mocks.	2
Add documentation for SpannerDeployInstanceOperator (#8750)	1
fix: Scheduler in helm chart cannot access DAG volume with git sync (#14203)	2
[AIRFLOW-5925] Relax funcsigs and psutil version requirements (#6580)	1
Add script to verify that all artefacts are in svn (#14777)* Add script to verify all is in svnThis change adds simple tool to verify that all expected filesare present in airflow svn when doing release. Also in caseof providers/backport releases it generates simple dockerfilethat can be used to verify installation.	1
Dumps more logs in case of CI failure (#11614)We do not dump airflow logs on success any more, but we dump themand all the container logs in case of failure, so that we canbetter investigate cases like #11543 - that includes enablingfull deadlock information dumping in our mysql database.	5
Add param to CloudDataTransferServiceOperator (#14118)When a one-time job is created with `CloudDataTransferServiceS3(GCS)ToGCSOperator`, the job remains on the GCP console even after the job is completed.This is a specification of the data transfer service, but I would like to add this parameter because there are normally cases where don't want to leave a one-time job.	1
[AIRFLOW-640] Install and enable nose-ignore-docstringCloses #1896 from zodiac/nose-ignore-docstring	2
[AIRFLOW-4447] Display task duration as human friendly format in UI (#5218)	5
AIRFLOW-5489: Remove unneeded assignment of variable (#6106)	4
[AIRFLOW-5274] dag loading duration metric name too long (#5890)	2
Fix wrong link for taskflow tutorial (#26007)	2
Chart docs: better note for logs existing pvc permissions (#17177)	2
Pin `itsdangerous` to < 2 (#15804)Looks like new version of `itsdangerous` has broken some logging configs: https://itsdangerous.palletsprojects.com/en/2.0.x/changes/#version-2-0-0	4
[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937)	2
Fix assuming "Feature" answer on CI when generating docs (#23640)We have now different answers posisble when generating docs, andfor testing we assume we answered randomly during the generationof documentation.	2
Handle connection parameters added to Extra and custom fields (#17269)	1
use label instead of id for dynamic task labels in graph (#26108)	1
Fix spelling (#12421)	0
Fixes undefined variables (#12155)There are few more variables that (if not defined) preventfrom using the CI image directly without breeze or theCI scripts.With this change you can run:`docker run -it apache/airflow:master-python3.6-ci`and enter the image without errors.	0
Chart bug fix	0
[AIRFLOW-710] Add OneFineStay as official userCloses #1952 from slangwald/patch-1	1
[AIRFLOW-6142] Fix different local/Travis pylint results (#6705)* [AIRFLOW-6142] Fix different local/Travis pylint resultsSometimes Pylint on Travis CI gives still different results than the one runlocally. This was happening because we were using theAIRFLOW_MOUNT_SOURCE_DIR_FOR_STATIC_CHECKS="true" for static checks. This isneeded for checklicence check only - just to make sure that all source files(including scripts etc.) are mounted to the container.However this makes it slightly different when it comes to pylint checks. Wewould like to have it exactly identical when run locally and in CI so in caseof static checks we should rather useAIRFLOW_MOUNT_HOST_VOLUMES_FOR_STATIC_CHECKS="true" for all checks but theChecklicence one - same as used locally.This way running:pre-commit run pylint --all-filesShould always give the same results locally and in Travis.* Update scripts/ci/_utils.shCo-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>	1
Improve error handling/messaging around bucket exist check (#25805)S3Hook.check_for_bucket() method uses the boto3 s3 client method `head_bucket`to check for bucket existence. This client method does not work likemost boto3 APIs, it only returns a small subset of error codes.	0
[AIRFLOW-1970] Let empty Fernet key or special `no encryption` phrase. (#4038)Once the user has installed Fernet package then the application enforces setting valid Fernet key.This change will alter this behavior into letting empty Fernet key or special `no encryption` phrase and interpreting those two cases as no encryption desirable.	4
Fix timestamp defaults for sensorinstance (#24638)Constant values were used where callables were intended.	1
[AIRFLOW-5227] Consistent licence for .sql files (#5829)	2
Add wheel and cache	1
Update grammar & typos in dag-serialization.rst (#14992)Fixing minor stylistic mistakes and some typos	2
Support tables in DAG docs (#13533)	2
Support clearing and updating state of individual mapped task instances (#22958)* Allow marking/clearing mapped taskinstances from the UI* Refactor to straighten up types* Accept multiple map_index param from front endThis allows setting multiple instances of the same task to SUCCESS orFAILED in one request. This is translated to multiple task specifiertuples (task_id, map_index) when passed to set_state().Also made some drive-through improvements adding types and clean someformatting up.* Introduce tuple_().in_() shim for MSSQL compatCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>	4
Fix EcsOperatorError, so it can be loaded from a picklefile (#21441)	2
[AIRFLOW-7022] Simplify DagFileProcessor.process_file method (#7674)	2
Add support for latest Apache Beam SDK in Dataflow operators (#9323)* A* Add support for Apache Beam latest SDK in Dataflow operators* fixup! Add support for Apache Beam latest SDK in Dataflow operators* fixup! fixup! Add support for Apache Beam latest SDK in Dataflow operators* fixup! fixup! fixup! Add support for Apache Beam latest SDK in Dataflow operators	1
[AIRFLOW-2258] Allow import of Parquet-format files into BigQueryCloses #3164 from stevesoundcloud/stevesoundcloud/allow-import-of-parquet-format-files-into-bigquery-AIRFLOW-2258	2
[AIRFLOW-278] Support utf-8 ecoding for SQLSupport utf-8 encoding for SQL queries, needed for Python 2users who have unicode strings inside the queriesCloses #1622 from biln/master	1
[AIRFLOW-5305] Sort extra links by name (#5905)	2
AIRFLOW-202: Fixes stray print line	0
limit scope to user email only AIRFLOW-386	1
[AIRFLOW-5912] Expose lineage API (#7138)Lineage data is exposed via the experimental api endpointper dag.	2
[AIRFLOW-XXX] Remove redundant space in Kerberos (#3866)	4
Temporarily remove mypy checks to stop PRs from failing (#19345)After we moved to Python 3.7 as default, it had a ripple effectthat MyPy checks started failing. We aim to fix itpermanently in #19334 but this needs a bit more changes, so forthe moment we skip the checks.	4
Mark `test_send_tas_to_celery_hang` as quarantined (#16169)The test_send_tasks_to_celery_hang hangs on self-hosted runners moreoften than not.It's been introduced in #15989 and while the test does not usually hangon regular GitHub runners, or in case of running it locally (I could notmake it fail), it does hang almost always when run on self-hostedrunners.Marking it as quarantined for now.Issue #16168 created to keep track of it.	1
[AIRFLOW-3220] Add Instance Group Manager Operators for GCE (#4167)	1
Revert "Adds --install-wheels flag to breeze command line (#11317)" (#11348)This reverts commit de07d135ae1bda3f71dd83951bcfafc2b3ad9f89.	4
[AIRFLOW-4679] Make airflow/operators Pylint compatible (#7757)	1
Merge pull request #11 from airbnb/bugfixBugfix when airflow.cfg boolean had inline comments would eval to none	5
[AIRFLOW-814] Fix Presto*CheckOperator.__init__Use keyword args when initializing aPresto*CheckOperator.Closes #2029 from patrickmckenna/fix-presto-check-operators	0
Better signing instructions for helm chart releases (#20796)The primary change here is using the [helm gpg](https://github.com/technosophos/helm-gpg) plugin to sign the chart instead of using the default sign/verify in helm. This allows more modern GPG features to be used, e.g. a smartcard, instead of relying on a GnuPG v1 binary keyring containing the private key.	5
Proper sqlalchemy syntax for desc	5
Remove unused licenses: `python-nvd3` & `python-slugify` (#15860)This was added when we vendorized nvd3 and slugify: https://github.com/apache/airflow/commit/e36bdef0b34c16def20ecbb8248950070eb5fa33but we forgot to remove it in https://github.com/apache/airflow/pull/9136	4
Revert "[AIRFLOW-626] HTML Content does not show up when sending email with attachment"This reverts commit 55af3e04f8aa2062715370c8feec10308938715e.Master is currently broken as shown on https://travis-ci.org/apache/incubator-airflow/jobs/175858834======================================================================FAIL: test_custom_backend (tests.EmailTest)----------------------------------------------------------------------Traceback (most recent call last):  File "/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/mock/mock.py", line 1305, in patched    return func(*args, **keywargs)  File "/home/travis/build/apache/incubator-airflow/tests/core.py", line 1927, in test_custom_backend    send_email_test.assert_called_with('to', 'subject', 'content', files=None, dryrun=False, cc=None, bcc=None)  File "/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/mock/mock.py", line 937, in assert_called_with    six.raise_from(AssertionError(_error_message(cause)), cause)  File "/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/six.py", line 718, in raise_from    raise valueAssertionError: Expected call: mock('to', 'subject', 'content', bcc=None, cc=None, dryrun=False, files=None)Actual call: mock('to', 'subject', 'content', bcc=None, cc=None, dryrun=False, files=None, mime_subtype=u'mixed')	2
Fix mistake and typos in doc/docstrings (#15180)- Fix an apparent mistake in doc relating to catchup- Fix typo pickable (should be picklable)	2
[AIRFLOW-149] Task Dependency Engine + Why Isn't My Task Running ViewHere is the original PR with Max's LGTM:https://github.com/aoen/incubator-airflow/pull/1Since then I have made some fixes but this PR is essentially the same.It could definitely use more eyes as there are likely still issues.**Goals**- Simplify, consolidate, and make consistent the logic of whether or not  a task should be run- Provide a view/better logging that gives insight into why a task  instance is not currently running (no more viewing the scheduler logs  to find out why a task instance isn't running for the majority of  cases):![image](https://cloud.githubusercontent.com/assets/1592778/17637621/aa669f5e-6099-11e6-81c2-d988d2073aac.png)**Notable Functional Changes**- Webserver view + task_failing_deps CLI command to explain why a given  task instance isn't being run by the scheduler- Running a backfill in the command line and running a task in the UI  will now display detailed error messages based on which dependencies  were not met for a task instead of appearing to succeed but actually  failing silently- Maximum task concurrency and pools are not respected by backfills- Backfill now has the equivalent of the old force flag to run even for  successful tasks  This will break one use case:  Using pools to restrict some resource on airflow executors themselves  (rather than an external resource like a DB), e.g. some task uses 60%  of cpu on a worker so we restrict that task's pool size to 1 to  prevent two of the tasks from running on the same host. When  backfilling a task of this type, now the backfill will wait on the  pool to have slots open up before running the task even though we  don't need to do this if backfilling on a different host outside of  the pool. I think breaking this use case is OK since the use case is a  hack due to not having a proper resource isolation solution (e.g.  mesos should be used in this case instead).- To make things less confusing for users, there is now a "ignore all  dependencies" option for running tasks, "ignore dependencies" has been  renamed to "ignore task dependencies", and "force" has been renamed to  "ignore task instance state". The new "Ignore all dependencies" flag  will ignore the following:  - task instance's pool being full  - execution date for a task instance being in the future  - a task instance being in the retry waiting period  - the task instance's task ending prior to the task instance's    execution date  - task instance is already queued  - task instance has already completed  - task instance is in the shutdown state  - WILL NOT IGNORE task instance is already running- SLA miss emails will now include all tasks that did not finish for a  particular DAG run, even if  the tasks didn't run because depends_on_past was not met for a task- Tasks with pools won't get queued automatically the first time they  reach a worker; if they are ready to run they will be run immediately- Running a task via the UI or via the command line (backfill/run  commands) will now log why a task could not get run if one if it's  dependencies isn't met. For tasks kicked off via the web UI this  means that tasks don't silently fail to get queued despite a  successful message in the UI.- Queuing a task into a pool that doesn't exist will now get stopped in  the scheduler instead of a worker**Follow Up Items**- Update the docs to reference the new explainer views/CLI commandCloses #1729 from aoen/ddavydov/blockedTIExplainerRebasedMaster	1
[AIRFLOW-3103][AIRFLOW-3147] Update flask-appbuilder (#3937)	5
Docs: Fix typo in ``dag-run.rst`` (#19340)	2
Add bulk_insert_rows() for more performant inserts.	1
[AIRFLOW-649] Support non-sched DAGs in LatestOnlyOpCloses #1956 from r39132/master	3
[AIRFLOW-1115] fix github oauth api URLCloses #3469 from renzofrigato/airflow_1115	0
Fix tooltip typo (#11593)	2
[AIRFLOW-917] Fix formatting of error messageVariables were interpolated into error messagein the wrong order.Closes #2109 from vijaykramesh/vijay/incorrect_format_of_slots_available	0
Allowing to pause	1
[AIRFLOW-XXX] Fix development packages installtion instructions (#6942)	0
Adding utility function to get to models.Variable	1
Fix typos in HiveOperator	1
Reduce duplication in pre_commit_check_order_setup.py script (#14731)Now that we are using Py3.6+ we can rely on dictionary key order to befixed (it was always fixed in 3.6, just not explicitly documented assuch from 3.7) -- as a result we can load the source, rather than try toparse it with regexes	1
[AIRFLOW-2467][AIRFLOW-2] Update import direct warn message to use the module nameCloses #3361 from dan-sf/AIRFLOW-2467	1
Making queued squares gray	1
log documentation	2
Add sensor for AWS Batch (#19850) (#19885)* Add sensor for AWS Batch (#19850)Adds a sensor implementation to ask for the status of anAWS Batch job. The sensor will enable DAGs to wait for thebatch job to reach a terminal state before proceeding to thedownstream tasks.	2
[AIRFLOW-5343] Remove legacy way of pessimistic disconnect handling (#6034)Based on discussions in https://github.com/apache/airflow/pull/5949it was figured out that there is already pessimistic disconnecttimeout handling. So instead of hand-written one only SQLAlchemyembedded way should be used.'sqlalchemy~=1.3' is in `setup.py` requirements and `pool_pre_ping`appeared in SQLAlchemy 1.2.	1
Adding coveralls dep to tox	1
Setting celery workers optimization to fair	1
Do not create a separate process for one task in CeleryExecutor (#8855)	1
Updated clean-logs.sh (#16978)	4
Fix typo in test_views.py (#9522)"parmeters_only" -> "parameters_only"	2
Brings back testing providers against 2.1.0 (#16006)We temporarily disabled testing of providers against Airflow 2.1.0until it gets released. New providers are going to be 2.1+compatible only so we stopped testing them against 2.0.0.This PR restores the tests.	3
Add support for BeamGoPipelineOperator (#20386)closes: https://github.com/apache/airflow/issues/20283In this PR:- [x]  Upgrade the minimum package requirement to 2.33.0 for apache-beam (first stable for beam go sdk)- [x]  Refactor `operators/beam.py` with an abstract `BeamBasePipelineOperator` class to factorize initialization and common code, also fixed mypy hook on ``BeamDataflowMixin``- [x] Add `BeamRunGoPipelineOperator` and `BeamHook.start_go_pipeline` (+tests)- [x]  Add `utils/go_module.py` to handle initialisation and dependency installation for a module. (+ tests)- [x]  Slightly modified `process_util` + tests to be able to handle an extra optional parameter `cwd`. (This way we can move to the module directory to build it)- [x]  Write docs	2
[AIRFLOW-XXX] Upgrade FAB to 1.12.3 (#4694)	5
Randomize pod name (#12117)	5
Use glyph icon for logout button.	2
Support remote logging in elasticsearch with filebeat 7 (#14625)Filebeat 7 renamed some fields (offset->log.offset and host->host.name),so allow the field names Airflow uses to be configured.Airflow isn't directly involved with getting the logs _to_elasticsearch, so we should allow easy configuration to accomodatewhatever tools are used in that process.	1
Fix CloudSecretsManagerBackend invalid connections_prefix (#7861)	0
Rename test_local_setting.py to test_settings.py (#12437)	3
Rename ``task_concurrency`` to ``max_active_tis_per_dag`` (#17708)Follow-up of https://github.com/apache/airflow/pull/16267Renames `task_concurrency` to `max_active_tis_per_dag`Some of Airflow's concurrency settings have been a source of confusion for a lot of users (including me), for example:https://stackoverflow.com/questions/56370720/how-to-control-the-parallelism-or-concurrency-of-an-airflow-installationhttps://stackoverflow.com/questions/38200666/airflow-parallelismThis PR is an attempt to make the settings easier to understand	1
[AIRFLOW-413] Fix unset path bug when backfilling via picklePlease accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-413This fixes a bug when a pickled DAG is used in a backfill.Testing Done:- Existing unit tests + running a backfill command that used to fail  before on this errormistercrunch artwr plypaulCloses #1723 from aoen/ddavydov/fix_undefined_path_for_backfilling	0
Add pip install to make sure to generate output fast enough for travis	1
Add support of capacity provider strategy for ECSOperator (#15848)	1
Move setup order check back to pre-commit (#9010)* Move setup order check back to pre-commitThe order check used to be working from pre-commit but then itwas moved to be regular test case. That was a mistakeThe test is super-fast and actually making it use assertEqualswas not very useful and it was very late when you found it out.I changed it to be normal python script which made it works again(it did not work when it was a test because pre-commit does notrun tests - it runs python scripts).The messages printed now are much more informative as well.	5
Fix wrong reference in tracking-user-activity.rst (#22745)	1
Fixing screenshot	2
[AIRFLOW-6022] Move FS defn into awk BEGIN (#6619)	4
Add submodules in added workflow job (#13631)The most recent submodule change for actions #13514 was done inparallel to Optimising worklfows in #13562 and the job added inthe #13562 still uses non-submodule version of check action.Also few checkout steps missed:'submodules: recursive' inputThis PR fixes that and all 3rd-party actions now are usedfrom submodule.	1
[AIRFLOW-196] Fix bug that exception is not handled in HttpSensorDear Airflow Maintainers,Please accept this PR that addresses the following issues:- [*AIRFLOW-196*](https://issues.apache.org/jira/browse/AIRFLOW-196)If exception happens in poke function in HttpSensor, it is notwell handled that make the sensor finish successfully, which isincorrect obviously.Author: Junwei Wang <i.junwei.wang@gmail.com>Closes #1561 from junwei-wang/master.	5
Add allowed_jenkins_states to JenkinsJobTriggerOperator (#14131)	1
Deprecate experimental API (#9888)	5
add show statements to hql filtering.	1
[AIRFLOW-1314] Add executor_config and tests* Added in executor_config to the task_instance table and the base_operator table* Fix test; bump up number of examples* Fix up comments from PR* Exclude the kubernetes example dag from a test* Fix dict -> KubernetesExecutorConfig* fixed up executor_config comment and type hint	5
[AIRFLOW-2849] Add flake8 to setup.pyCloses #3694 from eyaltrabelsi/AIRFLOW-2849-add-flake8-to-setup.py-dev-req-to-run-quality-check-locally	1
closes apache/incubator-airflow#1301 *obsolete*	5
[airflow] chart now using the Airbnb colors!	1
[AIRFLOW-XXX] Add Daniel to committer list (#4961)	1
Set task state to failed when pod is DELETED while running (#18095)There is a bug in the Kubernetes Job Watcher that occurs when a node with a running worker pod is removed from the cluster. If the worker pod doesn't complete before the node is removed, it is orphaned and forced deleted by the garbage collector. This is communicated by the API with a status='Running' but an event with type='DELETED'Because in the if statement the Job Watcher doesn't check the event type, the last information we get from the pod is that is it running. The running scheduler never gets any information about the pod and shows it as stuck in a queued state. This situation is fixed when the scheduler/executor restarts and this function is run.	1
Add tagging image as latest for CI image wait (#23775)The "wait for image" step lacked --tag-as-latest which made thesubsequent "fix-ownership" step run sometimes far longer thanneeded - because it rebuilt the image for fix-ownership case.Also the "fix-ownership" command has been changed to just pullthe image if one is missing locally rather than build. Thiscommand might be run in an environment where the image is missingor any other image was build (for example in jobs where an imagewas build for different Python version) in this case the commandwill simply use whatever Python version is available (it doesnot matter), or in case no image is available, it will pull the imageas the last resort.	1
[AIRFLOW-331] modify the LDAP authentication config lines in  'Security' sample codesCloses #1674 from impangt/master	5
Artifacts in Github Action have a short retention period (#12793)	5
Prepare ad-hoc release of the four previously excluded providers (#14655)Documentation update for the four previously excluded providers thatgot extra fixes/bumping to the latest version of the libraries.* apache.beam* apache.druid* microsoft.azure* snowflake	3
Reduce logs from imported/vendored FAB class (#19875)When we imported/vendored this class from FAB, we unintentionallychanged the logger name it was using, which caused more logs to appear(we configured the default level for FAB to warning, but that PR meant "FAB"used a different logger so info messages were showing up.)	5
Databricks SQL operators (#21363)	1
Cache 1 10 ci images (#8955)* Push CI images to Docker packcage cache for v1-10 branchesThis is done as a commit to master so that we can keep the two branchesin syncCo-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>* Run Github Actions against v1-10-stable tooCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>	1
Fix Breeze documentation typo (#23919)	2
[AIRFLOW-XXX] Add Chagelog for 1.10.5	2
Add Compute Engine SSH hook (#9879)	1
Correct table alignment in CI doc (#19794)	2
Some refactoring work on scheduling code (#21414)	1
Updates Oracle.rst documentation (#13871)Resolves Issue #10186 (Move Tips & Tricks for Oracle shops should be moved to Airflow Docs)Fixes broken link, add UI connection documentation link, and connection tips.	2
Update sample dag and doc for RDS (#23651)	2
Added namespace as a template field in the KPO. (#19718)	1
[AIRFLOW-XXX] Remove wheelhouse files from travis not owned by travis	2
Update CODEOWNERS for the chart docs and k8s provider (#21202)	1
Remove Unnecessary list literal in Tuple for Kylin Operator (#10252)It is unnecessary to use a list or tuple literal within a call to tuple.Before:```In [1]: tuple(["ERROR", "DISCARDED", "KILLED", "SUICIDAL", "STOPPED"])Out[1]: ('ERROR', 'DISCARDED', 'KILLED', 'SUICIDAL', 'STOPPED')```After:```In [4]: ("ERROR", "DISCARDED", "KILLED", "SUICIDAL", "STOPPED",)Out[4]: ('ERROR', 'DISCARDED', 'KILLED', 'SUICIDAL', 'STOPPED')```	0
Sagemaker System Tests - Part 3 of 3 - example_sagemaker_endpoint.py (AIP-47) (#25134)* Sagemaker System Tests - Part 3 of 3 - example_sagemaker_endpoint.py* PR Fixes* Fix failing static checks - unused import	2
Add Helm Chart logo to docs index (#14762)	2
Fixes failing test_views tests (#14599)This reverts commit 49952e79b04da932242ebf3981883e591b467994.Not sure what happended. We made that change because of cPythonvulnerability (https://github.com/python/cpython/pull/24297/files)in #14341I am not sure what happened here but this should fix the Master.	0
Access task type via the property, not dundervars (#11274)We don't currently create TIs form serialized dags, but we are about tostart -- at which point some of these cases would have just shown"SerializedBaseOperator", rather than the _real_ class name.The other changes are just for "consistency" -- we should always get thetask type from this property, not via `__class__.__name__`.I haven't set up a pre-commit rule for this as using this dunderaccessor is used elsewhere on things that are not BaseOperatorinstances, and detecting that is hard to do in a pre-commit rule.	1
[AIRFLOW-5640] Document and test `email` parameters of BaseOperator (#6315)* Refactored get_email_address_list to have a better  separation between string handling and other iterables.* Explicitely casting get_email_address_list argument  to a list in case the argument was an iterable. This  enables direct support for tuples, sets or the like.* Fixed type annotation of email parameter of  BaseOperator to show that iterables are directly  supported.* Added docstring entries for email, email_on_retry,  email_on_failure and queue in BaseOperator.	1
Add missing variable in run_cli_tool.sh (#9239)	1
Add description about `not a directory` errors in WSL2 (#18976)	0
Use full version string for deprecated config (#22930)	5
Create CustomJob and Datasets operators for Vertex AI service (#20077)	1
Fix: Unnecessary downloads in ``GCSToLocalFilesystemOperator`` (#16171)Fixes #15005 GCSToLocalFilesystemOperator unnecessarily downloads objects when it checks object size.Co-authored-by: Pavel Kachalov <pavel_kachalov@epam.com>	5
Add is_mapped field to Task response. (#23319)* Add is_mapped field to Task response.* Add is_mapped to schema file and add test for GetTasks.	1
fix: aws hook should work without conn id (#8534)This patch makes behavior of hook consistent with documentation.AWS hooks should support falling back to using default credential chainlookup behavior when connection id is not specified.add test for conn_id equals Nonemore elegant way to set role session name	1
[AIRFLOW-519] Add 99 as an Airflow userCloses #1795 from fbenevides/patch-1	1
Adding list and dict support to the UI template renderer	1
Passing self to setAutocommit	1
migrate system test gcs_to_bigquery into new design (#22753)	1
Merge pull request #2352 from mistercrunch/remove_max_author	4
[AIRFLOW-6558] Campaign Manager operators for conversions (#7420)	1
Move parse_once to quarantine (#10857)	4
Fix spelling problem introduced in #11923 (#11927)	0
[AIRFLOW-453] Add XCom Admin PageCloses #1756 from msumit/AIRFLOW-453	1
Remove redundant word (#13466)`for for` -> `for`	4
[AIRFLOW-4844] Add optional is_paused_upon_creation argument to DAG (#5473)If this option is set (not None) it will be used in place of what ever the configsetting is. If it's not passed the config will be used.	1
Restore airflow.www.app.csrf to avoid breaking change (#9402)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>	4
[Airflow-2760] Decouple DAG parsing loop from scheduler loop (#3873)	2
[AIRFLOW-2878] Fix www_rbac display issueThe new RBAC UI has some issues about layout/UI display.The header (<h2>) is not shown ("hidden" by the Nav Bar),or tables are not shown completely.This is addressed by a simple change ontemplates/appbuilder/baselayout.html	4
Move non-opencontainer labeling of the image to breeze from Dockerfile (#23379)* Extract "extra" labeling of the image to breeze from DockerfileFixes: #21046* Add more ArtifictHub-specific labelsCo-authored-by: Kamil Breguła <kamilbregula@apache.org>	1
[AIRFLOW-3233] Fix deletion of DAGs in the UI (#4069)	2
Serialize the template_ext attribute to show it in UI (#17985)Co-authored-by: Bas Harenslak <bas@astronomer.io>	5
Chart: Fix network policy issue for webserver and flowerui (#20199)	0
[AIRFLOW-2521] backfill - make variable name and logging messages more acurate[AIRFLOW-2521] backfill - make variable name andlogging messages more accurateThe term kicked_off in logging and the variablestarted are used torefer to `running` task instances. Let's clarifythe variable names andmessages here.Fixing unit testsCloses #3416 from mistercrunch/kicked_off_running	1
[AIRFLOW-4201] Replace unicode strings by normal strings (#5026)	5
Fix HiveToMySqlOperator's wrong docstring (#23316)Replace `metastore_conn_id` with `hiveserver2_conn_id`	2
Save pod name to xcom for KubernetesPodOperator (#15755)* Save pod name to xcom for KubernetesPodOperator* fix kubernetes test	3
Add tool to bulk-create issues. (#22462)	0
[AIRFLOW-2097] tz referenced before assignmentUnboundLocalError: local variable 'tz' referenced before assignmentCloses #3076 from feng-tao/airflow-2097	0
Chart: Ability to access http k8s via multiple hostnames (#18257)Add support for accessing the airflow ui and the flower ui (ifapplicable) from multiple hostnames when install via helm chart onto akubernetes cluster.closes: #18216.	2
don't modify non-string env_vars	5
Quickfix for VerticaHook with no password	4
[AIRFLOW-5508] Add config setting to limit which StatsD metrics are emitted (#6130)	1
Fix setting of project ID in ``provide_authorized_gcloud`` (#20428)fixes: #20426 and change in logic introduced (seemingly accidentally) in 2fadf3c	2
Replace deprecated ``dag.sub_dag`` with ``dag.partial_subset`` (#16179)This is follow up of https://github.com/apache/airflow/pull/11542to all the missed places in the codebase.	2
Revert "Show `extra` data on the connection list view" (#1438)	5
Update ``INTHEWILD.md`` to add Breezeline (#20855)Add Breezeline (formerly Atlantic Broadband) to INTHEWILD.mdWe use it for building pipelines to ingest into BigQuery and automate many SFTP processes!	1
s3 sensor fix logging	2
Update Helm version in Breeze as well (#25589)Follow-up after #25582 - Helm version was still not upgraded inBreeze and Docker CI image.This PR fixes it.	0
Fix mypy errors reported by tests/serializsation/ (#20117)	3
[AIRFLOW-XXXX] Change CONTRIBUTING.md to CONTRIBUTING.rst (#7695)	4
Mark image as refreshed when pulled on CI (#23410)One of the recent changes in Breeze (#23395) caused unnecessaryrebuilding of image when "build-docs" is run. On CI we buildimage once and reuse it. However in case of Buld docs we missedinformation that the image is "fresh" and we started rebuildingit. This change marks the image as "refreshed" when it is pulledwith `--tag-as-latest` flag (which happens in CI).	3
Prevent unused scrollbars from appearing in FF on Linux (#12795)	1
Merge pull request #242 from airbnb/brokenWarning on unparsable files in DAGS_FOLDER	2
Use Debian's provided JRE from Buster (#8919)Installing the JDK (not even the JRE) from Sid is starting to break onBuster as the versions of packages conflict:> The following packages have unmet dependencies:> libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed>                Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installedThis changes our CI docker images to:1. Not install something from Sid (unstable, packages change/get   updated) when we are using Buster (stable, only security fixes).2. Installed the JRE, not the JDK. We don't need to compile Java code.	0
Typo	2
Commenting out content of example_docker_operator.py	2
Fix breeze redirect on macOS (#14506)	0
Added missing return parameter in read function of FileTaskHandler (#14001)this issue ouccurs when invalid try_number value is passed in get logs apiFIXES: #13638	0
Increase the number of expected queries on index view to 38 (#9263)I am not sure what bumped this number from 37 to 38 but fixing the test for now. We can investidate it later	5
[AIRFLOW-2980] ReadTheDocs - Fix Missing API Reference	0
closes apache/incubator-airflow#3276 *Messed up PR - hundreds of old commits.*	5
error early if virtualenv is missing (#15788)	0
Retrieve principal from extra connection settings and make beeline work with kerberos/sasl	1
added explaining concept of logical date in DAG run docs (#21433)	2
use find links of pip wheel build, remove extra cache not required anymore	1
Remove Outdated SQLCheckOperator Docstring (#10589)This method is now implemented	2
[AIRFLOW-XXX] Update pydoc of mlengine_operator (#5419)	1
Adding postgres setAutocommit	1
[AIRFLOW-3183] Fix bug in DagFileProcessorManager.max_runs_reached() (#4031)The condition is intended to ensure the functionwill return False if any file's run_count is still smallerthan max_run. But the operator used here is "!=".Instead, it should be "<".This is because in DagFileProcessorManager,there is no statement helping limit the upperlimit of run_count. It's possible thatfiles' run_count will be bigger than max_run.In such case, max_runs_reached() methodmay fail its purpose.	0
Fix timestamp test (#17365)	3
[AIRFLOW-XXXX] Fix typo in BREEZE.rst (#7554)	2
Update AWS connection example to show how to set from env var (#9191)The trailing `@` wasn't obvious/documented anywhere (and took me sometrial and error to work out) so to save time for the next person let'sadd it to the docs	2
Doc change: XCOM / Taskflow (#18212)Added a description and code sample of going FROM normal Airflow Operators TO Taskflow Operators, passing in XCOMs	4
[AIRFLOW-179] Fix DbApiHook with non-ASCII charsString serialization fails when string contains non-ASCII charactersCloses #1553 fromjohnbodley/dbapi_hook_serialization-remedy	5
[AIRFLOW-322] Fix typo in FAQ sectionCloses #1693 from ajayyadava/322	2
closes apache/incubator-airflow#1538 *Not acceptable*	2
Chart: Update default airflow version to `2.3.1` (#23913)	5
Update models.pyadd Oracle SQL support through the OracleHook	1
[AIRFLOW-5189] Move GCP Vision to core (#5796)This commit moves GCP Vision from contrib to core.For more information check AIP-21.	5
[AIRFLOW-611] source_format in BigQueryBaseCursorCheck source_format in BigQueryBaseCursorThe edits to `bigquery_hook.py` are made to`BigQueryBaseCursor`.Closes #1873 from Jalepeno112/bug/AIRFLOW-611	0
Bump pre-commit hooks (#17000)Just upgrades to latest minor versions of pre-commit hooks	1
Merge pull request #1208 from underyx/patch-2Update link to Common Pitfalls wiki page in README	2
[AIRFLOW-XXXX] Fix Path for Github Action (#7040)	0
corrected invalid port location in connection defaults ... was meant for mssql_default not http_default	5
[AIRFLOW-5402] Remove deprecated logger (#6006)* [AIRFLOW-5402] Remove deprecated logger* Remove the related test* Less is more	3
Fix task instance modal in gantt view (#19258)	0
Fix process_subdir bug	0
Further decrease of amount of parallelism (#7991)	5
Fix grammar in local.rst (#18001)	0
Get rid of TimedJSONWebSignatureSerializer (#24519)The TimedJSONWebSignatureSerializer has been deprecated from theitsdangerous library and they recommended to use dedicatedlibraries for it.https://github.com/pallets/itsdangerous/issues/129Since we are going to move to FAB 4+ with #22397 where newer version ofitsdangerous is used, we need to switch to another library.We are already using PyJWT so the choice is obvious.Additionally to switching, the following improvements were done:* the use of JWT claims has been fixed to follow JWT standard.  We were using "iat" header wrongly. The specification of JWT only  expects the header to be there and be valid UTC timestamp, but the  claim does not impact maturity of the signature - the signature  is valid if iat is in the future.  Instead "nbf" - "not before" claim should be used to verify if the  request is not coming from the future. We now require all claims  to be present in the request.* rather than using salt/signing_context we switched to standard  JWT "audience" claim (same end result)* we have now much better diagnostics on the server side of the  reason why request is forbidden - explicit error messages  are printed in server logs and details of the exception. This  is secure, we do not spill the information about the reason  to the client, it's only available in server logs, so there is  no risk attacker could use it.* the JWTSigner is "use-agnostic". We should be able to use the  same class for any other signatures (Internal API from AIP-44)  with just different audience* Short, 5 seconds default clock skew is allowed, to account for  systems that have "almost" synchronized time* more tests addded with proper time freezing testing both  expiry and immaturity of the requestThis change is not a breaking one because the JWT authenticationdetails are not "public API" - but in case someone reverse engineeredour claims and implemented their own log file retrieval, weshould add a change in our changelog - therefore newsfragmentis added.	1
BigQueryTableExistenceSensor needs to specify keyword arguments (#9832)	2
Switch from zdesk to zenpy in ZendeskHook (#21349)	1
Fix timezone display for logs on UI (#23075)	2
Common SQLCheckOperators Various Functionality Update (#25164)* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Move alias to proper query build statementThe table alias should be in the self.sql query build statementas that is where the table it needs to alias is defined.* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Bug fixes and updates to test and operatorFixed bug in test where the dataframe column names did not matchthe operator's expected dataframe column names. Added more infoto the SQLColumnCheckOperator's batch arg. Fixed the location oftable aliasing in SQLTableCheckOperator.* Remove merge conflict lines* Rename parameter batch to partition_clauseGives a clearer name to the parameter and adds templating tothe SQLTableCheckOperator.* Fix typo in docstring* Reformat operator file	2
Fixing problem with missing output in pre-commits in some cases (#11684)Dumping logs from container should only be done in CI.Problem was introduced in #11614	0
Add reference to the ASF Code of Conduct (#9453)* Add reference to the ASF Code of Conduct* Update CONTRIBUTING.rst	5
Add more testing methods to dev/README.md (#11458)	2
Disabling caching for chart data	5
[AIRFLOW-4131] Make template undefined behavior configurable. (#4951)	5
Merge pull request #396 from wooga/af-1.5.1add extra fields to form columns	1
Add tip about airflow config command on docs/howto/secrets-backend/index.rst (#10239)	2
use list to be safe	1
Found what was slowing down the master scheduler as it ran longer	5
Switches to manually building docker images (#16570)According tohttps://www.docker.com/blog/changes-to-docker-hub-autobuilds/Docker is going to disable autobuilds for free tiers. We might be exemptfrom that via ASF, but docker autobuilds never worked well for us formultitude of reasons.This PR turns manually preparing the image into obligatory, manual stepwhen releasing Airflow.Part of #16555	1
Run mini scheduler in LocalTaskJob during task exit (#16289)Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high.This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled,we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat,the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exitwas not called after the task succeeded. Hence, the heartbeat thinks that this task was externally marked failed/successful.This change resolves this by moving the mini scheduler to LocalTaskJob at the handle_task_exit method ensuringthat the task will no longer be killed by the next heartbeat	0
[AIRFLOW-296] template_ext is being treated as a string rather than a tuple in qubole operatorCloses #1638 from msumit/AIRFLOW-296	1
Add support for Salesforce bulk api (#24473)* add support for Salesforce bulk api	1
Ability to test connections from UI or API (#15795)	3
Merge pull request #26 from mistercrunch/refresh_dagbagDebug master scheduler, added file timestamp based dagbag update	5
[AIRFLOW-2436] Remove cli_logger in initdbCloses #3330 from jinhyukchang/master	4
[AIRFLOW-1813] Bug SSH Operator empty bufferThe SSH Operator will throw an empty "SSH operatorerror" when runningcommands that do not immediately log something tothe terminal. This isdue to a call to stdout.channel.recv when thechannel currently has a0-size buffer, either because the command has notyet logged anything,or never will (e.g. sleep 5)Make code PEP8 compliantCloses #2785 from RJKeevil/fix-ssh-operator-no-terminal-output	0
Making sure a fernet_key exists in airflow.cfg	5
Remove badly merged conflict for BREEZE.rst (#22953)Missed the conflict when merging #22876. Github hidessuch big changes by default :(	4
Add a session backend to store session data in the database (#21478)Co-authored-by: Jed Cunningham <jedcunningham@apache.org>	5
[AIRFLOW-6325] Clear cli hint (#6880)	5
[AIRFLOW-2891] Make DockerOperator container_name be templateable (#5696)	2
Mask other forms of password arguments in SparkSubmitOperator (#9615)This is a follow-up to #6917 before modifying the masking code.Related: #9595.	1
Add D200 pydocstyle check (#11688)	2
Added SQL Server to connection admin drop down	4
Adding unixname to task instance view	1
Update Google Cloud branding (#10642)	5
[AIRFLOW-2152] Add Multiply to list of companies using AirflowCloses #3080 from nrhvyc/master	1
Fix Invalid log order in ElasticsearchTaskHandler (#17551)	0
Allow DagParam to hold falsy values (#22964)	2
Improve test for the next_execution cli command (#9058)* Improve test for the next_execution cli commandIt still tests the same functionality, but it is now more efficient andreadable. The changes are: - The DB is cleaned only once instead of eight times - Use a fixed datetime instead of timezone.utcnow (deterministic) - Use redirect_stdout instead of subprocess - Clean up pylint warning - Test None output once instead of four times* Address PR commentsIssues addressed: - Use create_session - Use DagRunType.MANUAL.value - Remove DagRuns created by the test* Remove test from quarantineCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>	3
[AIRFLOW-2994] Fix command status check in Qubole Check operator (#3790)	1
[AIRFLOW-5696] Add GoogleCloudStorageToSFTPOperator (#6366)	1
Providing a way to specify the executor to use while constructing the DAG objectadding host=0.0.0.0 to ensure the service is accesible from outside localhostRevert "adding host=0.0.0.0 to ensure the service is accesible from outside localhost"This reverts commit ce5436bc7dcd70dcb129c1dfa46f9fd7ce743e43.	4
Fix MyPy Errors for Databricks provider. (#20265)	1
Added mysql_preoperator templated fields in hive_to_mysql	1
Port of the kt_renewer from hue to airflow. It requires new settings in airflow.cfg in [security]. kinit_path to specify the location of kinit. keytab to specify the location of the keytab file, principal for the kerberos principal to use, reinit_frequency the interval for ticket renewal, ccache the location of the ccache file.	2
Update pre-commit hooks (#14627)- `pyupgrade`: `v2.7.4` to `v2.10.0`- `pygrep-hooks`: `v1.7.0` to `v1.8.0`- `yamllint`: `v1.25.0` to `v1.26.0`	1
Remove DELETE /importErrors/{import_error_id} endpoint (#9325)	2
[AIRFLOW-4953] Remove unused variables from core (#5587)	1
[hotfix] dag missing from dagbag	2
[AIRFLOW-6707] Simplify Connection.get_hook method (#7328)* [AIRFLOW-6707] Simplify Connection.get_hook method* fixup! [AIRFLOW-6707] Simplify Connection.get_hook method	1
[AIRFLOW-6564] Additional diagnostics information on CI check failure (#7172)	0
Merge pull request #327 from kapil-malik/master#326 default_login.py does not work out of the box	1
Removed bad characters from AWS operator (#10590)	1
Improve process terminating in scheduler_job (#8064)	1
added wetransfer to airflow users	1
[AIRFLOW-3924] Fix try number in alert emails (#4741)Alert emails sent via email_alert() have the correct try numberin the body of the text.Add a test to ensure the first email sent says `Try 1`.	1
Added small health check server and endpoint in scheduler and updated… (#23905)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
Add dunnhumby to INTHEWILD.md (#17204)Co-authored-by: Deepak Kumar <deepak.kumar1@dunnhumby.com>	1
Change the name of link to ASF downloads (#19441)The ASF used to use mirrors to distribute their software, howeverrecently they changed to use CDN. The mechanism might change inthe future (even if currently CDN is used the ASF 'mirrors' pageand closer.lua script provide a fully ASF-controlled mechanism toswitch to the right mechanism, however technically speaking thecurrent solution is not 'mirrors' but it is CDN, therefore it makessense to rename it to generic downloads.	1
Fix MyPy errors for google.cloud.tasks (#20233)Part of #19891	0
Use Viewer role as example public role (#19215)Currently the sample public role is the `Public` role.  But this role provids no access and still shows the login modal, which makes it seem like the config is not working at all.More intuitive would be to use `Viewer` role in example in the default config because you can at least view the dags.	2
Fixes case where output log is missing for image waiting (#14784)	2
[AIRFLOW-XXX] Adding Home Depot as users of Apache airflow (#4013)* Adding Home Depot as users of Apache airflow	1
Log trigger status only if at least one is running (#21191)	1
Improve Committer's guide docs (#11338)	2
[AIRFLOW-1779] Add keepalive packets to ssh hookMake use of paramiko's set_keepalive method tosend keepalive packets everykeepalive_interval seconds.  This will preventlong running queries with no terminaloutput from being termanated as idle, for exampleby an intermediate NAT.Set on by default with a 30 second interval.Closes #2749 from RJKeevil/add-sshhook-keepalive	1
Fix S3Hook transfer config arguments validation (#25544)* Fix S3Hook transfer config arguments validation* Add proper exception type	1
Update error message to guide the user into self-help mostly (#17929)The error report generated, when there is a crash on webserver,directed the user to open an issue in Apache Airflow without anyextra explanation or suggesting other actions. This is not a goodidea because it might lead people to thinking that they canjust follow the link, open issue and it will be solved, However,more often than not such issue can be caused by misconfiguration,networking or other actions that the user should - in many cases -be able to fix or workaround on their own, with a little effortof gathering logs/information and searching for relevant problems.This PR changes the message as folows:* explains that user should gather more information (the link  and bug report does not contain any information)* search for similar problem (we provide links to the places which  can be searched and suggest using regular search engine as well* direct the managed services users to open the issue using  relevant channels* only as the last resort, opening an issue in Airflow GitHub,  providing sufficient information.	5
Add imap_attachment_to_s3 example dag and system test (#8669)	3
Typo in docs/apache-airflow/installation/index.rst (#18689)	2
[AIRFLOW-7066] Use sphinx syntax in concepts.rst (#7729)	1
Add guide for DataprocInstantiateInlineWorkflowTemplateOperator (#22062)	5
Fixing the macros	0
Fix labels on the pod created by ``KubernetsPodOperator`` (#15492)When using `pod_template_file` or `full_pod_spec`, Pod identifying labelswere not applied to the POD which meant `reattach_on_restart` did notwork for them.```python{"dag_id": "dag","kubernetes_pod_operator": "True","task_id": "task","try_number": "1","airflow_version": 2.0.2,"execution_date": "2016-01-01T0100000100",}```This commit fixes that and makes the labels consistent whether usersuse `pod_template_file`, `full_pod_spec` or just pass params toKubernetesPodOperator.closes https://github.com/apache/airflow/issues/13918	0
[AIRFLOW-3239] Fix/refine tests for api/common/experimental/ (#4255)Follow-up on [AIRFLOW-3239]Related PRs: #4074, #41311. Fix (test_)trigger_dag.py2. Fix (test_)mark_tasks.py  2-1. properly name the file  2-2. Correct the name of sample DAG  2-3. Correct the range of sample execution_dates       (earlier one conflict with the start_date of the sample DAG)  2-4. Skip for test running on MySQL       Seems something is wrong with       airflow.api.common.experimental.mark_tasks.set_state,       Corresponding test case works on Postgres & SQLite,       but fails when on MySQL ("(1062, "Duplicate entry '110' for key 'PRIMARY'")").       A TODO note is added to remind us fix it for MySQL later.3. Remove unnecessary lines in test_pool.py	3
Fix occasional cleartask failures (#18859)The cleartask tests occasionally failed due to not consistentsequence in which task clearing was performed.The query did not have ordering and sometimes the tasks werereturned in different order than expected.	0
Improve breeze resource check (#17492)The resource check in breeze was slow (3 docker commands insteadof one) and it used an extra image which needed to be downloaded.The new check uses already available airflow CI image and itperforms all check in one docker command - thus is a lot fasterand it also checks the image at the same time.	2
Add Vodafone to the list of organisations using Airflow (#17359)Vodafone uses Airflow to orchestrate data pipelines in the on-premises.This commit adds Vodafone to the INTHEWILD.md file, as one of theorganisations that use Airflow	1
Fixed wrong "-e" on md5 file status check (#10803)The "-e" flag was not reset properly in the md5 status checkwhich could lead in some cases to removing output of flake check.	4
Add new lint check to now allow realtive imports (#10825)Relative and absolute imports are functionally equivalent, the onlypratical difference is that relative is shorter.But it is also less obvious what exactly is imported, and harder to findsuch imports with simple tools (such as grep).Thus we have decided that Airflow house style is to use absolute importsonly	2
Optimizes structure of the Dockerfiles and use latest tools (#17418)* Remove CONTINUE_ON_PIP_CHECK_FAILURE parameterThis parameter was useful when upgrading new dependencies,however it is going to be replaced with better approach in theupcoming image convention change.* Optimizes structure of the Dockerfiles and use latest toolsThis PR optimizes the structure of Dockerfile by moving someexpensive operations before the COPY sources so thatrebuilding image when only few sources change is much faster.At the same time, we upgrade PIP and HELM chart used to latestversions and clean-up some parameter inconsistencies.	2
[AIRFLOW-3664] Fix interpreter errors in test_python_operator.py (#4472)	3
[AIRFLOW-4194] Set dag_run state to failed when user terminate backfill (#5016)	1
Add Listener Plugin API that tracks TaskInstance state changes (#20443)This adds new Plugin API - "listeners". It enables plugin authors to write[pluggy hook implementation][1] that will be called on certain formalized extensionpoints. To differentiate between current Airflow extension points, likeplugins, and current Airflow hooks, implementations of those hooks are calledlisteners.The API is ment to be called across all dags, and all operators - in contrastto current on_success_callback, pre_execute and related family which are meantto provide callbacks for particular dag authors, or operator creators.pluggy mechanism enables us to execute multiple, or none, listeners thatimplement particular extension point, so that users can use multiple listenersseamlessly.In this PR, three such extension points are added. When TaskInstance's state ischanged to RUNNING, on_task_instance_running hook is called. On changetoSUCCESS on_task_instance_success is called, similarly on FAILEDon_task_instance_failed is called.Actual notification mechanism is be implemented using [SQLAlchemy’s eventsmechanism][2]. This ensures that plugins will get every change of state,regardless of where in the codebase it happened, and not require manualannotation of TI state changes across the codebase.To make sure that this change is not affecting performance, running thismechanism on scheduler is disabled by default. The SQLAlchemy event mechanismis also not affected by default - the event listener is only added if we haveany plugin which actually provides any listener.[1]: https://pluggy.readthedocs.io/en/stable/[2]: https://docs.sqlalchemy.org/en/13/orm/session_events.html#after-flushSigned-off-by: Maciej Obuchowski <obuchowski.maciej@gmail.com>	2
Call `Session.remove` after each run, to survive DB restartWithout this, we get the following exception for subsequent runs after aDB restart, indefinitely:    sqlalchemy.exc.InvalidRequestError: This Session's transaction has    been rolled back due to a previous exception during flush. To begin    a new transaction with this Session, first issue Session.rollback().    Original exception was: (psycopg2.OperationalError) terminating    connection due to administrator command	5
Add more tests for the kubernetes executor (#15992)This change adds tests to improve coverage for the kubernetes executorPart of #15523	3
Databricks jobs 2.1 (#19544)	5
Fix error in seeding elastic `log_id` template (#24960)There was a mistake in seeding the elastic log id. We used hyphens instead of underscores	1
Rework contract of try_adopt_task_instances method (#23188)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>	1
Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)	2
Fix mypy databricks operator (#20598)* [16185] Added LocalKubernetesExecutor to breeze supported executors* Revert "[16185] Added LocalKubernetesExecutor to breeze supported executors"This reverts commit a1c532eacfeddcbefaa3e565a0522e25315286c4.* Fixed mypy errors in databricks/operators	5
Use `dag_maker` fixture in some test files under tests/models (#17556)This PR uses dag_maker fixtures in tests under tests/modelsCo-authored-by: Ash Berlin-Taylor <ash@apache.org>	3
fix job deletion (#11272)	4
Remove references to Jira for tickets/PR from docs (#7783)	2
Upgrade black to 20.8b1 (#10818)	5
Merge pull request #108 from airbnb/beeline_opAdding support for beeline as part of HiveCliHook	1
Revert "Fix Helm GitSync dag volume mount from pod-template-file (#15331)" (#15390)This reverts commit e4c0689535f1353c9e647773c06bedf8cd22b239.	4
[AIRFLOW-XXX] Fix Typo in SFTPOperator docstring (#4016)	2
[AIRFLOW-5630] Improve BigQueryGetDataOperator to handle no rows (#6298)	0
Separate Installing from sources section and add more details (#18171)This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples	1
Simplify DAG.set_dag_runs_state method (#8232)* Simplify DAG.set_dag_runs_state method* fixup! Simplify DAG.set_dag_runs_state method	2
Fixes "development" and "rc" cross dependencies between providers (#17023)In case we have additional dependencies between providers releasedat the same time (for example we need to release sftp and sshpackages now where sftp package depends on release of sshat the same time) we have to add suffix to the version of theadditional_dependency.PIP does not take into account unfortunately that developmentdependencies should likely be considered as fulfilling therequirement of >=. For example if you have:sftp depends on ssh>=2.1.0 and you release ssh 2.1.0.dev0 atthe same time the ssh>=2.1.0 condition is not fulfilled.Same case will be with rc1. Therefore we need to add the suffix in suchcross-provider dependencies to be able to install them in CIand in rc candidates.In the future we might ask PIP to change behaviour in such case.	4
Suggest to use secrets backend for variable when it contains sensitive data (#17319)	5
Adding landscape.io code health badge	1
Make BaseSecretsBackend.build_path generic (#7948)Currently the arguments required for it are `connections_prefix` and `conn_id` . Changing this to `path_prefix` and `secret_id` allows using that method for retrieving variables too	1
Bump pyupgrade v2.13.0 to v2.18.1 (#15991)* Bump pyupgrade v2.13.0 to v2.18.1* fixup! Bump pyupgrade v2.13.0 to v2.18.1	0
Optimize count query on /home (#8729)	5
Add ING to list of users	1
Remove back 3.6 to unblock PRs (#22516)	4
[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254)Breeze did not have PYTHONPATH set in the interactive Breezeentry - which is different than in tests and makes it difficultto run tests outside of the main directory.	3
Add wall clock time to next scheduled run log message	2
[AIRFLOW-395] Fix colon/equal signs typo for resources in default configCloses #1708 from aoen/ddavydov/fix_colon_typo	2
Adding badges to README	1
AWSBatchOperator <> ClientHook relation changed to composition (#9306)	4
add europcar to list of companies using Airflow (#25279)Co-authored-by: aninda bhattacharjee <aninda.bhattacharjee@europcar.com>	1
fix typing errors reported by dmypy (#8773)	0
[AIRFLOW-1430] Include INSTALL instructions to avoid GPL	2
[AIRFLOW-XXX] Simplify AWS/Azure/Databricks operators listing (#6047)	1
[AIRFLOW-XXXX] Fix tutorial that initialize the database tables for 2.0.0 (#7590)The aiflow initdb part of the commit 61455c69ddb5677ce02af626fbbeaf4bc29c15d3was added to tutorial. However, if using 'initdb' command in version 2.0.0as current document, it doesn't work like below. Because `db init` is usedinstead of `initdb` in v2.0.0, so must fix it.```  $ airflow initdb  airflow command error: argument subcommand: invalid choice: 'initdb'  (choose from 'config', 'connections', 'dags', 'db', 'kerberos', 'pools',  'roles', 'rotate_fernet_key', 'scheduler', 'sync_perm', 'tasks', 'users',  'variables', 'version', 'webserver'), see help above.```	1
Closes apache/incubator-airflow#2065 *Do not take CI time away from us*	5
[AIRFLOW-4363] Fix JSON encoding error (#7628)From the docker-py code comments for APIClient pull,the decode parameter should be set to True, when thestream parameter is also set to True. This will allowdecoding JSON data returned from the docker registryserver into dictsSigned-off-by: Raymond Etornam <retornam@users.noreply.github.com>	1
[AIRFLOW-XXX] Fix a typo of config (#4544)	5
Disables provider's manager warning for source-installed prod image. (#13729)When production image is built for development purpose, by defaultit installs all providers from sources, but not all dependenciesare installed for all providers. Many providers require moredependencies and when you try to import those packages viaprovider's manager, they fail to import and print warnings.Those warnings are now turned into debug messages, in caseAIRFLOW_INSTALLATION_METHOD=".", which is set whenproduction image is built locally from sources. This is helpfulespecially when you use locally build production image torun K8S tests - otherwise the logs are flooded withwarnings.This problem does not happe in CI, because there by defaultproduction image is built from locally prepared packagesand it does not contain sources from providers that are notinstalled via packages.	1
Merge pull request #699 from airbnb/docsImprovments to documentation	2
Merge pull request #403 from jlowin/patch-4Don't warn if start_date is None	5
[AIRFLOW-6709] Fixed failing git sync (#7332)	0
[AIRFLOW-3681] All GCP operators have now optional GCP Project ID (#4500)	1
[Airflow 1332] Split logs based on try numberThis PR splits logs based on try number and addtabs to display different task instance tries.**Note this PR is a temporary change forseparating task attempts. The code in this PR willbe refactored in the future. Please refer to #2422for Airflow logging abstractions redesign.**Testing:1. Added unit tests.2. Tested on localhost.3. Tested on production environment with S3 remotestorage, MySQL database, Redis, one Airflowscheduler and two airflow workers.Closes #2383 from AllisonWang/allison--add-task-attempt	1
Add `2.3.0rc2` to issue templates (#23298)	0
The scheduled Quarantined build is removed (#15436)This build is not really needed any more gathering statsabout quarantined builds was not very successful experiment.	4
Fix static checks after merging #17093 (#17096)	0
Add redbubble link to Airflow merch (#10359)* Add redbubble link to Airflow merch* Update README.mdCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>	1
[AIRFLOW-5631] Change way of running GCP system tests (#6299)* [AIRFLOW-5631] Change way of running GCP system testsThis commit proposes a new way of running GCP related system tests.It uses SystemTests base class and authentication is provided by acontext manager thus it's easier to understand what's going on.	1
Shorten max pre-commit hook name length (#23677)When names are too long, pre-commit output looks very ugly and takes up 2x lines. Here I reduce max length just a little bit further so that pre-commit output renders properly on a macbook pro 16" with terminal window splitting screen horizontally.	2
Added haodf to INTHEWILD.md (#21510)Added haodf to the list of companies using Apache Airflow	1
use sasl/kerberos for snakebite if configured as such	5
Limit colorlog version (6.x is incompatible) (#18099)The "color" method seems to have been removed.	4
