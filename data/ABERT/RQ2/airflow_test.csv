commit_msg,labels
Merge pull request #786 from abridgett/feature/remove_duplicate_macroremove duplicated definition,5
A few tweaks while running core_cx,1
"Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581)- Fix functionality  last_scheduler_run was missed in the process of  migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db.  This issue will fail DAG.deactivate_stale_dags() method,  and blocks users from checking the last schedule time of each DAG in DB- Change name last_scheduler_run to last_parsed_time,  to better reflect what it does now.  Migration script is added, and codebase is updated- To ensure the migration scripts can work,  we have to limit the columns needed in create_dag_specific_permissions(),  so migration 2c6edca13270 can work with the renamed column.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Enforce formatting on helm chart json schema files (#15895),2
"[AIRFLOW-1930] Convert func.now() to timezone.utcnow()func.now() defaults to the timezone of thedatabase,we assume that everything is in UTC which mightnot bethe case if func.now() is used.Closes #2882 from bolkedebruin/AIRFLOW-1930",1
Improve test coverage for test_common_schema.py (#10740)Adds test that an error is raised with specific message when unkown object type is passed,4
[AIRFLOW-3607] Optimize dep checking when depends on past set and concurrency limit,1
Breeze2 autocomplete options (#20066),5
A few tweaks around alembic/migrations,5
limit table of content at the main Airflow doc page (#12561),2
"Add delimiter argument to WasbHook delete_file method (#15637)This change adds a delimiter to the delete_file method of Wasbhook. This way, users will be able to locate the files they want to delete using a delimiter.Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",1
More unit tests,3
"Databricks - allow Azure SP authentication on other Azure clouds (#19722)* Databricks - allow Azure SP authentication on other cloudsWhen other Azure clouds are used (US GovCloud, China, ...) otherauthentication endpoints should be used.  This PR allows to overwritethe authentication endpoint when using other clouds",1
Unify command names in CLI (#10720)* Unify command names in CLI* fixup! Unify command names in CLI,0
Adds separate scheduled-only workflow to cancel duplicates (#9999)Unfortunately cancelling workflows does not work from the workflowsexecuted by forks because their tokens do not allow doing that(they are read only). So we have to run a separate cron-triggeredaction (in the context of apache/airflow repository to cancelall duplicate workflows. This action stops all the workflowsrunning from the same fork/branch except the last one.,1
Merge pull request #1167 from xiaoliangsc/fix-bigquery-hookfix bigquery hook bugs,0
[AIRFLOW-6072] aws_hook: Outbound http proxy setting and other enhancements (#6686),1
[AIRFLOW-XXX] Correct Typo in sensor's exception (#4545),2
Log migrations info in consisten way (#13458)Resource based permissions migration changes logging handlersso each next migration is differently formatted when doingairflow db reset. This commit fixes this behavior.closes: #13214,0
Order filenames for migrations (#22168)Sometimes you want to quickly find recent migrations.  This makes it easier,1
"Workaround libstdcpp TLS error (#19010)Workaround https://github.com/apache/airflow/issues/17546 issue with/usr/lib/x86_64-linux-gnu/libstdc++.so.6: cannot allocate memory instatic TLS block.We do not yet a more ""correct"" solution to the problem but in order toavoid raising new issues by users of the prod image, we implement theworkaround now.The side effect of this is slightly (in the range of 100s ofmilliseconds) slower load for any binary started and a little memoryused for Heap allocated by initialization of libstdc++.This overhead is not happening for binaries that already linkdynamically libstdc++.",2
Open relative extra links in place (#17477),2
Bump FAB to 3.1 (#11475)FAB released a new version today - https://pypi.org/project/Flask-AppBuilder/3.1.0/ which removes the annoying missing font file format for glyphicons error,0
add example dag and system test for GoogleSheetsToGCSOperator (#9056)* add example dag and system test for sheets_to_gcs* remove sheets_to_gcs in missing example dags,2
[AIRFLOW-XXX] Update to new logo (#6066),2
Make python 3 compatible,1
[AIRFLOW-XXX] Add Liberty Global to company list (#3685),1
[AIRFLOW-1000] Rebrand distribution to Apache AirflowPer Apache requirements Airflow should be brandedApache Airflow.It is impossible to provide a forward compatibleautomatic updatepath and users will be required to manuallyupgrade.Closes #2172 from bolkedebruin/AIRFLOW-1000,1
Fix AttributeError when starting schedulerThis PR fixes the exception`AttributeError: 'SchedulerJob' object has no attribute 'num_runs'`that is thrown when starting the scheduler without setting `-n`.,1
"Only allows supported field types to be used in custom connections (#17194)* Only allows supported field types to be used in custom connectionsOnly four field types are supported in Connection Forms:String, Password, Integer, Boolean.Previously when custom connections tried to use other fieldtype, ConnectionForm behaved in a very strange way - theconnection form reloaded quickly hiding the actual errorand no error was printed making it next to impossible to figureout the root cause of the problem.With this change, non-supported field types generate Warningand the Connections that use them are not added to the list ofsupported connections.Fixes: #17193",0
Help pip resolver make better decision on Pyarrow (#25791),1
Random spelling updates.,5
Adding missing `replace` param in docstring (#18241),2
[AIRFLOW-3307] Upgrade rbac node deps via `npm audit fix`. (#4147)Also drop the unused istanbul package.,1
Update documentation regarding Python 3.9 support (#17611)https://github.com/apache/airflow#requirements,1
[AIRFLOW-6705] One less chatty message at breeze initialisation (#7326)This message was still left preventing nice .... progress,5
[AIRFLOW-5742] Move Google Cloud Vision to providers package (#6424),1
Add reference for SubDagOperator (#12297)It would be better for users to have a link when they see SubDagOperator where they can read about it instead of just Class names,2
Add homebrew/python/setproctitle issue to FAQ (#12025),0
"Update flask_wtf version to work with werkzeug>=1.0 (#11939)Werkzeug 0.16 deprecated werkzeug.url_encode, and removed it in 1.0, sowe need the fixed version of flask_wtf",0
Merge pull request #86 from woodlee/docs-editsVarious documentation spelling and grammar edits,2
adding a sed comand to replace airflow_home automatically in default airflow.cfg,5
Clarifying docs entry for trigger_rule,1
Merge branch 'master' into v1-8-test,3
Google cloud operator strict type check (#11450)import optimisation,2
Remove failed DockerOperator tasks with auto_remove=True (#13532) (#13993)* Remove failed DockerOperator tasks with auto_remove=TrueRemoves exited containers if a task based on DockerOperator failswith StatusCode!=0 and auto_remove=True,4
debugging PR,0
"Chart: Update postgres subchart to 10.5.3 (#17041)We were on 6.3.12 and the current latest version is 10.5.3.We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only used for development, not production.",1
Prevent running `airflow db init\upgrade` migrations and setup in parallel. (#17078),1
Unhide changelog entry for databricks (#20128),5
"Better compatibility/diagnostics for arbitrary UID in docker image (#15162)The PROD image of airflow is OpenShift compatible and it can berun with either 'airflow' user (UID=50000) or with any otheruser with (GID=0).This change adds umask 0002 to make sure that whenever the imageis extended and new directories get created, the directories aregroup-writeable for GID=0. This is added in the defaultentrypoint.The entrypoint will fail if it is not run as airflow user or ifother, arbitrary user is used with GID != 0.Fixes: #15107",0
Minor Helm Chart doc enhancements (#15124)- Better formatting for `tip`- Correct listing,1
[AIRFLOW-XXX] Add cleartax to companies list (#5331),1
"Add partition related methods to GlueCatalogHook: (#23857)* ""get_partition"" to retrieve a Partition* ""create_partition"" to create a Partition",1
[AIRFLOW-1207] Enable utils.helpers unit testsCloses #2300 from skudriashev/airflow-1207,3
[AIRFLOW-5390] Remove provide context (#5990),1
Fix typo in the word 'available' (#9599)`avaible` -> `available`,2
Adding extra requirements for build and runtime of the PROD image. (#16170)This PR adds capability of adding extra requirements to PROD image:1) During the build by placing requirements.txt in the   ``docker-context-files`` folder2) During execution of the container - by passing   _PIP_ADDITIONAL_REQUIREMENTS variableThe second case is only useful durint quick test/development andshould not be used in production.Also updated documentation to contain all development/testvariables for docker compose and clarifying that the optionsstarting with _ are ment to be only used for quick testing.,3
Make vrious scripts Google Shell Guide compatible (#10812)Part of #10576,1
"Revert ""Work around change in GH Actions concurrency expression evaluation (#20023)"" (#20025)This reverts commit 42c46842f415da6bca09fa83d636ef1751243692.",4
[AIRFLOW-2149] Add link to apache Beam documentation to create self executing JarCloses #3077 from lcaggio/master,1
Add generic CLI tool wrapper (#9223)* Add generic  CLI tool wrapper* Pas working directory to container* Share namespaces between all containers* Fix permissions hack* Unify code styleCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>* Detect standalone execution by checking symboli link* User friendly error message when env var is missing* Display error to stderr* Display errors on stderr* Fix permission hack* Fix condition in if* Fix missing env-file* TEST: Install airflow without copying ssources* Update scripts/ci/in_container/run_prepare_backport_readme.shCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
"[AIRFLOW-779] Task should fail with specific message when deletedTesting Done:- Killed a task while it was running using thetask instances UI, verified behavior is the sameas before, and logging workedCloses #2006 from saguziel/aguziel-terminate-nonexistent",1
[AIRFLOW-3004] Add config disabling scheduler cron (#3899),5
Support unknown backends in entrypoint_prod.sh (#22883),1
Fix typo in BREEZE.rst (#9199)Changed 'y' to 'by' since it was incorrect.,4
Chart: Make PgBouncer cmd/args configurable (#18910),5
[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086),2
[AIRFLOW-6262] add on_execute_callback to operators (#6831),1
Merge pull request #220 from mistercrunch/tmpfolder_hive_opRunning HiveCliHook execute in a temp working directory,1
Doc: Fix typo in Triggerer docs (#18560),2
Bugfixes,0
Using more plain language and making the reference to a task consistent,1
"[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496)* [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing* Fixed problem that Kubernetes tests were testing latest master  rather than what came from the local sources.* Moved Kubernetes scripts to 'in_container' dir where they belong now* Kubernetes tests are now better suited for running locally* Kubernetes cluster is not deleted until environment is stopped* Kubernetes image is built outside of the container and passed as .tar* Kubectl version name is corrected in the Dockerfile* Kubernetes Version can be used to select Kubernetes versio* Running kubernetes scripts is now easy in Breeze* Instructions on how to run Kubernetes tests are updated* Better flags in Breeze are used to run Kubernetes environment/tests* The old ""bare"" environment is replaced by --no-deps switch",3
Add batch option to `SqsSensor` (#24554)Add batch option to `SqsSensor`Co-authored-by: TungHoang <st.hoang@jellysmack.com>Co-authored-by: D. Ferruzzi <ferruzzi@amazon.com>,1
[AIRFLOW-1497] Reset hidden fields when changing connection typeCloses #2507 from mrkm4ntr/airflow-1497,4
Rename amazon EMR hook name (#20767)Co-authored-by: vinit payal <vinit@tribes.ai>,5
[documentation] document that max_active_runs can prevent a DAG from running,1
Merge pull request #288 from jlowin/env_var_configSet configuration with env vars,5
Merge pull request #1175 from airbnb/pep8Let's merge this to get builds working again! Enforcing PEP8,1
Add note about image regeneration in June 2022 (#24524)The note is about image changes after refreshing are now betterorganized (around date of the change) - this should be more usefulby the users who will look why their images have been refreshed.Related to: #24516,1
[AIRFLOW-5581] Cleanly shutdown KubernetesJobWatcher for safe Scheduler shutdown on SIGTERM (#6237),4
[AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods (#4551)* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* Resolved conflicts.* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* Resolved conflicts.* Resolve conflicts.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* [AIRFLOW-2955] clear up commits.* Resolve nits form @galuszkak and @dimberman.,0
Add more CODEOWNERS (#16761)- This PR/commit ads TP to `airflow/timetables/` dir and Jed to Helm Charts,2
Pickling the content of files referenced instead of file locations,2
Merge pull request #177 from mistercrunch/ops_cleanopsMaking operator constructor leaner,1
docs: amazon-provider retry modes (#23906),1
Deprecate BaseHook.get_connections method (#10135) (#10192)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
"[AIRFLOW-6761] Fix WorkGroup param in AWSAthenaHook (#7386)Unknown parameter in input: ""Workgroup"", must be one of: QueryString, ClientRequestToken, QueryExecutionContext, ResultConfiguration, WorkGroup",1
Merge pull request #28 from airbnb/logging_levelCentralizing logging level into settings.py file,2
Improve handling server errors in DataprocSubmitJobOperator (#11947)* Improve handling server errors in DataprocSubmitJobOperator* fixup! Improve handling server errors in DataprocSubmitJobOperator,5
Merge pull request #568 from airbnb/metastore_sensorAdding a sql metastore partition sensor,1
"Build CI images for the merge result of a PR, not the tip of the PR (#18060)The change to use pull_request_target had the unintended side-effect ofmeaning that the images were built using the tip of the PR, instead ofthe merge of main and the PR branch.This restores that behaviour (and should mean that fewer PRs will needrebasing to fix their builds when master gets broken)",1
Merge pull request #1013 from rdavison/masterFixed issue 1012: pool not used with celery executor,1
[AIRFLOW-4905] Add colours to flake8 output (#5541),1
Move Backport Providers docs to our docsite (#11136),2
[AIRFLOW-926] Fix JDBC HookJayDeBeApi made a backwards incompatible changeThis updates the JDBC Hook's implementationand changes the required JayDeBeApi to >= 1.1.1Closes #2651 from r-richmond/AIRFLOW-926,1
Merge pull request #154 from jeremyclover/embed-viewAdded logic to allow for an embed parameter in the URL,2
Update version added field in config after 2.1.4 release (#18355),5
[AIRFLOW-414] Improve error message for missing FERNET_KEYCloses #1724 from r39132/more_descriptive_error,0
Update __init__.pyAdd oracle_hook integration,1
Remove/refactor default_args pattern for Microsoft example DAGs (#16873),2
Force order in list API endpoints (#9366),1
split out get_one/get_many,1
Docs: Move part of timetable guide to concepts (#18786),4
Bugfix for when authenticate=false and airflow_login is present,2
Security upgrade lodash from 4.17.19 to 4.17.20 (#11095)Details: https://snyk.io/vuln/SNYK-JS-LODASH-590103,5
[AIRFLOW-5743] Move Google PubSub to providers package (#6476),1
[AIRFLOW-6060] Improve conf_vars context manager (#6658)This commit adds try / finally clause to conf_vars contextmanager to assure that initail values are reseted in caseof an exception in yield.,1
Hooking up hostname and port to defaults and command line,1
Python 3.8.4 release breaks our builds (#9820),4
"Add documentation for the HTTP connection (#15379)Add documentation for the http connection so it is more clear to a new user how to set them up. This PR also links the new documentation to operators, hooks, and sensors that use a http connection.",1
Use refactored utils module in unit test imports,2
[AIRFLOW-752] Add Mercadoni to list of Airflow usersCloses #1991 from demorenoc/master,1
[AIRFLOW-XXX] Changelog for 1.9.0,4
"[AIRFLOW-2344] Fix `connections -l` to work with pipe/redirect`airflow connections -l` uses 'tabulate' packagewithfancy_grid format, which outputs box drawingcharacters.It can occur UnicodeEncodeError with pipe orredirect,since the default encoding for Python 2.x isascii.This PR fixes it and contains some flask8 relatedfixes.Closes #3244 from sekikn/AIRFLOW-2344",0
Fix Google BigQueryHook method get_schema() (#13136)Co-authored-by: Manuel Bordes <manuel.bordes@gamesys.co.uk>,5
"Merge pull request #305 from airbnb/revert-261-dep_ruleRevert ""Making the dependency engine more flexible""",1
[AIRFLOW-2682] Add how-to guides for bash and python operatorsCloses #3552 from tswast/airflow-2682-bash-python-how-to,1
Making sure DAGS_FOLDER is in the PYTHONPATH,2
"Rescue if a DagRun's DAG was removed from db (#17544)Fix #17442.The exception happens when a DAG is removed from the database (via web UI or something else), but there are still unfinished runs associated to it. This catches the scenario and use the existing fallback setting `max_active_runs` to zero.",1
Clarify manual merging of PR in release doc (#23928)It was not clear to me what this really means,2
[AIRFLOW-2402] Fix RBAC task logCloses #3319 from yrqls21/kevin_yang_fix_rbac_view,0
"[AIRFLOW-1895] Fix primary key integrity for mysqlsla_miss and task_instances cannot have NULLexecution_dates. The timezone migration scripts forgot to set this properly. Inaddition to make sureMySQL does not set ""ON UPDATE CURRENT_TIMESTAMP""or MariaDB ""DEFAULT0000-00-00 00:00:00"" we now check ifexplicit_defaults_for_timestamp is turnedon and otherwise fail an database upgrade.Closes #2969, #2857Closes #2979 from bolkedebruin/AIRFLOW-1895",5
fixin spellin,0
"Adding ``TaskGroup`` support in ``BaseOperator.chain()`` (#17456)Related to: #17083, #16635",1
Add materialized view support for BigQuery (#14201),1
Only send email if task.email exists,5
"Fix edge detection iteration (#26188)In #26175, edge detection was changed to use iteration instead ofrecursion, however it wasn't keeping track of the parent tasks.This changes the set we iterate over to be `(task, {children})` pairs insteadof just adding all the children tasks themselves.",1
stuff,5
Pythoneskifying,5
Update the tree view of dag on Concepts Last Run Only (#8268)Resolves #8246,0
Add License to INTHEWILD.md (#10570),1
Merge pull request #57 from mistercrunch/emailAdding email functionality,1
Pass explicit overrides in `DbtCloudJobRunOperator` to `DbtCloudHook` (#22136),5
"Base date and run number for duration/landingThis adds the possibility to specify the base date and the number of runs displayed, for the Task duration tab as well as the Landing Times.",1
Fix grammar and remove duplicate words (#14647)* chore: fix grammar and remove duplicate words,4
"[AIRFLOW-1914] Add other charset support to email utilsThe built-in email utils does not supportmultibyte string content, for example,Japanese or emojis. The fix is to addmime_charset parameter to allow for othervalues such as `utf-8`.Closes #3308 from wolfier/AIRFLOW-1914",1
[AIRFLOW-5257] ElasticSearch log handler errors when attemping to close logs (#5863),2
Merge pull request #1400 from jlowin/missing-args-clearAdd missing args to `airflow clear` - confirmed this change works locally.,1
"Call delete_dag on subdag without AttributeError (#8017)The DagRun object does not have a task_id attribute, DagRuns are deleted in theblock above so no need to do it here or remove ones belonging to theparent.",4
Update list of non-core files/paths (#21907),2
Correct a typo in upgrading-to-2.rst (#14225)Update `list_dag` to `list_dags`,2
Fix UI flash when triggering with dup logical date (#26094),5
Merge pull request #1466 from r39132/master[AIRFLOW-39] Don't insert dag_runs beyond the min task end_date,5
Add template support for external_task_ids. (#22809),1
Adds new warning to provider verification (#25310)There is a new warning generated when you import snowflake. This issomething we cannot do much about (and it is rather harmlessbecause it only impacts pyarrow functionality of snowflakeconnector).Adding it will silence provider check in main builds.,1
Add OrangeBank to the official users of AirFlow (#9210)Adding OrangeBank to the official users of AirFlow.,1
[AIRFLOW-XXX] Added Jeitto as one of happy Airflow users! (#3902)[AIRFLOW-XXX] Add Jeitto as one happy Airflow user!,1
"[AIRFLOW-219][AIRFLOW-398] Cgroups + impersonationSubmitting on behalf of plypaulPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-219-https://issues.apache.org/jira/browse/AIRFLOW-398Testing Done:- Running on Airbnb prod (though on a differentmergebase) for many monthsCredits:Impersonation Work: georgeke did most of the workbut plypaul did quite a bit of work too.Cgroups: plypaul did most of the work, I just didsome touch up/bug fixes (see commit history,cgroups + impersonation commit is actually plypaul's not mine)Closes #1934 from aoen/ddavydov/cgroups_and_impersonation_after_rebase",0
"add env parameter to BashOperator, allow for passing env mapping to subprocess",4
Adding macro ds_add,1
"Clarifies installation/runtime options for CI/PROD images. (#15320)After PROD images were added, some of the flags had two meaningsThese behaved differently in PROD image and CI image and were thesource of confusion especially when start-airflow command was used.For PROD image, the image can be customized during image building,and packages could be installed from .whl or .sdist packagesavailable in `docker-context-files`. Thisis used at CI and dockerhub building time to produce image builtpackages that were prepared using local sources.The CI image is always built from local sources but airflow canbe removed and re-installed at runtime from pypi.Both airflow and provider packages can be installedfrom .whl or .sdist packages available in dist folder. This isused in CI to test current provider packages with olderAirflow released (2.0.0) and to test provider packages locally.After the change we have two sets of flags/variables:PROD image (building image):* install-airflow-version, install-airflow-reference,  install-from-docker-context-filesCI image (runtime):* use-airflow-version, use-packages-from-distThat should avoid confusion and failures of commands such as`start-airflow` that is used to test provider packages andairflow itself.",1
Fixing ISSUE_TEMPLATE name to include .md suffix,0
[AIRFLOW-6958] Fix Intermittent CI failure (#7592),0
[AIRFLOW-337] Add __repr__ to VariableAccessor and VariableJsonAccessorThe VariableJsonAccessor and VariableAccessor were missing the __repr__function that leads to a VariableError when printing out the contextbeing passed to for example a PythonOperator.,1
Patching ExternalTaskSensor for non mysql DBs,5
Reorganize sql to gcs operators. (#5504),1
[AIRFLOW-1736] Add HotelQuickly to Who Uses AirflowCloses #2705 from zinuzoid/AIRFLOW-1736,1
"[AIRFLOW-XXX] Solve lodash security warning (#4820)We don't use lodash at runtime, so this won't affect anything, but it's nice to remove the warning",2
"Don't show ""Access deined"" message on login page (#12846)It's kind of obvious that you'd need to login to access something if presentedwith a login page.",2
[AIRFLOW-5502] Move GCP base hook to core,1
closes apache/incubator-airflow#1692 *closed for inactivity*,5
[AIRFLOW-XXX] add ConnectWise to list of users (#5348),1
Merge pull request #782 from airbnb/close_conn[core] Closing db connection during execution,5
[AIRFLOW-1210] Enable DbApiHook unit testsCloses #2302 from skudriashev/airflow-1210,3
[AIRFLOW-6432] Raise appropriate exception in EmrAddStepsOperator when using job_flow_name and no cluster is found (#6898)* [AIRFLOW-6432] fixes in EmrAddStepsOperatorfix EmrAddStepsOperator broken ref & faulty test* changes after CR #1* Add exception and test case* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/operators/emr_add_steps_operator.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update tests/contrib/operators/test_emr_add_steps_operator.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* changes after CR #2Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,4
Merge pull request #510 from patrickleotardif/patch-2Change function call in render templates,1
Fix typo (#12424),2
Helm logo no longer a link (#23977),2
Allow users to submit issues for 2.2.0beta1 (#18165)This will allow users to test 2.2.0beta1 and we can track them in a better way,1
Stop loading Extra Operator links in Scheduler (#13932)closes #13099,2
"Added instructions on what to do if your command images are regenerated (#24581)In case there are conflicting changes to breeze command in severalPRs, you might get conflicting images printed. in such case youshould run `breeze regenerate-command-images`",1
"[AIRFLOW-2335] fix issue with jdk8 download for ciMake sure you have checked _all_ steps below.- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2335    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.- [x] Here are some details about my PR, includingscreenshots of any UI changes:There is an issue with travis pulling jdk8 that ispreventing CI jobs from running. This blocksfurther development of the project.Reference: https://github.com/travis-ci/travis-ci/issues/9512#issuecomment-382235301- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:This PR can't be unit tested since it is justconfiguration. However, the fact that unit testsrun successfully should show that it works.- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""- [ ] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.- [ ] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3236 from dimberman/AIRFLOW-2335_travis_issue",0
[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070)Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.,1
"Migrate Google example DAG bigquery_transfer to new design AIP-47 (#24543)related: #22447, #22430",1
[AIRFLOW-7025] Fix SparkSqlHook.run_query to handle its parameter properly (#7677),2
"Updates link to ""stable"" URLs for providers ""installing from sources"" (#18735)",1
More explicit zindex on modals,5
[AIRFLOW-5233] Fixed consistency in whitespace (tabs/eols) + common problems (#5835)* [AIRFLOW-5233] Fixed consistency in whitespace (tabs/eols) + common problems,0
Removing redundant max_tis_per_query initialisation on SchedulerJob (#19020),5
[AIRFLOW-XXXX] Update docs on example files for k8s (#7095),2
Chart: Make cleanup cronjob cmd/args configurable (#17970),5
"Speed up Breeze experience on Mac OS (#23866)This change should significantly speed up Breeze experience (andespecially iterating over a change in Breeze for MacOS users -independently if you are using x86 or arm architecture.The problem with MacOS with docker is particularly slow filesystemused to map sources from Host to Docker VM. It is particularly badwhen there are multiple small files involved.The improvement come from two areas:* removing duplicate pycache cleaning* moving MyPy cache to docker volumeWhen entering breeze we are - just in case - cleaning .pyc and__pychache__ files potentially generated outside of the dockercontainer - this is particularly useful if you use local IDEand you do not have bytecode generation disabled (we have itdisabled in Breeze). Generating python bytecode might lead tovarious problems when you are switching branches and Pythonversions, so for Breeze development where the files changeoften anyway, disabling them and removing when they are foundis important. This happens at entering breeze and it might takea second or two depending if you have locally generated.It could happen that __init script was called twice (depending whichscript was called - therefore the time could be double the onethat was actually needed. Also if you ever generated providerpackages, the time could be much longer, because node_modulesgenerated in provider sources were not excluded from searching(and on MacOS it takes a LOT of time).This also led to duplicate time of exit as the initialization codeinstalled traps that were also run twice. The traps however wererather fast so had no negative influence on performance.The change adds a guard so that initialization is only ever executedonce.Second part of the change is moving the cache of mypy to a dockervolume rather than being used from local source folder (defaultwhen complete sources are mounted). We were already using selectivemount to make sure MacOS filesystem slowness affects us in minimalway - but with this change, the cache will be stored in dockervolume that does not suffer from the same problems as mountingvolumes from host. The Docker volume is preserved until the`docker stop` command is run - which means that iterating overa change should be WAY faster now - observed speed-up were around5x speedups for MyPy pre-commit.",4
Applying preventative abs only to timedelta since relativedelta doesn't support it,1
Update recipe for Google Cloud SDK (#21268),5
"Optimize GitLab CI configuration (#8499)* tests are not executed for doc-only changes* images will be (once merged) downloaded from GitHub Registry so likely much faster* we have a ""scheduled"" nightly build that will build everything from scratch and check if no requirements have been broken* improved split of static checks between two static check jobs - to utilise parallelism better.* reorganised some fast jobs (requirements, prod image) that do not depend on tests so that they can run earlier* shorter names for jobs so that they are nicer to view in the actions view* matrix definitions of the jobs so that we can manage them better",1
Fixing a format in the wrong place raise the wrong error,0
"Don't emit first_task_scheduling_delay metric for only-once dags (#12835)Dags with a schedule interval of None, or `@once` don't have a followingschedule, so we can't realistically calculate this metric.Additionally, this changes the emitted metric from seconds tomilliseconds -- all timers to statsd should be in milliseconds -- thisis what Statsd and apps that consume data from there expect. See #10629for more details.This will be a ""breaking"" change from 1.10.14, where the metric wasback-ported to, but was (incorrectly) emitting seconds.",4
"Drop support for Helm 2 (#16575)Helm 2 is EOL, so bump our chart to the v2 apiVersionhttps://helm.sh/blog/helm-v2-deprecation-timeline/",2
Merge pull request #173 from airbnb/smarden1-generalize-check-operatorSmarden1 generalize check operator,1
"Expose snowflake query_id in snowflake hook and operator, support multiple statements in sql string (#15533)",1
Bring back limits on branches/tags builds in Airlfow repo (#22855)The change #22542 accidentally removed limit on branchesthat trigger direct push workflows in CI.Currently the builds are also triggered when a new TAG is pushednot only when new branch is created and this is quite too muchespecially when we push multiple tags for providers :(,1
"Fixes versioning for pre-release provider packages (#11586)When we prepare pre-release versions, they are not intended to beconverted to final release versions, so there is no need to replaceversion number for them artificially,For release candidates on the other hand, we should internally use the""final"" version because those packages might be simply renamed to thefinal ""production"" versions.Fixes #11585",0
[AIRFLOW-XXXX] Fix typo in task_command.py (#6997),2
Migrate Google Cloud Build from Discovery API to Python SDK (#18184),5
Adding @mention in Airbnb and links to airbnb open source projects,2
mssql operator,1
Enable interpretation of backslash escapes for colored message (#13418),0
Remove unused usage of logging module (#14632)Unless I am missing something the logging module was imported in the following files but was not used:- airflow/api_connexion/endpoints/dag_source_endpoint.py- airflow/api_connexion/endpoints/version_endpoint.py,2
"[AIRFLOW-1359] Use default_args in Cloud ML evalThis change makes the create_evaluate_ops utilitymethod make use of the default_args parameters ofthe DAG when possible. This simplifies the usageof the create_evaluate_ops method, and improvesthe usefulness of a variety of default_args.To further the usefulness of default_args forCloud ML Operators, this change also introducesversion_name to the CloudMLVersionOperator,allowing model_name and version_name to bespecified across an entire pipeline.This change also resolves a small TODO by makingthe DataFlowPythonOperator's `options` and`dataflow_default_options` variables templatized.Closes #2445 frompeterjdolan/eval_ops_arguments_from_default_args",5
Remove redundant asserts in tests/www/test_views.py (#12176)Methods 'check_content_not_in_response'/'check_content_in_response' alreadytake care of status code check (by default asserts against 200)So no need to check status code explicitly if either of these two methods areused to check the response.,1
Fix 'Upload documentation' step in CI (#10981),2
Merge pull request #723 from thibault-ketterer/masterfix bytestring in xcom with python3,3
"Fix error with quick-failing tasks in KubernetesPodOperator (#13621)* Fix error with quick-failing tasks in KubernetesPodOperatorAddresses an issue with the KubernetesPodOperator where tasks that diequickly are not patched with ""already_checked"" because they never makeit to the monitoring logic.* static fix",0
Fix grid details header text overlap (#23728)Move top margin to each breadcrumb component to make sure that there is no overlap when the header wraps with long names.,1
[AIRFLOW-1568] Fix typo in BigQueryHookCloses #2575 from jgao54/ds-import-export,2
removing carriage returns,4
Update the TEST_ENDPOINT_ID to use the valid format,1
Update AWS Connection docs and deprecate some extras (#24670)* Update AWS Connection docs and deprecate some extras* Update docs and deprecated legacy local credentials file,2
"Fix broken task instance link in xcom list (#23367)* Fix broken task instance link in xcom listAdd execution date back to the xcom list to be able to pass to the `task_instance_link()` function.Long term, we should swap out the execution_date param for run_id* Make execution date a search column",5
"Deprecate some functions in the experimental API (#19931)This PR seeks to deprecate some functions in the experimental API.Some of the deprecated functions are only used in the experimental REST API,others that are valid are being moved out of the experimental package.",4
Remove deprecated method call (blob.download_as_string) (#20091),4
XCom class,5
[AIRFLOW-1964] Add Upsight to list of Airflow usersCloses #2912 from dhuang/upsight,1
Migrate Microsoft example DAGs to new design #22452 - winrm (#24140)* Migrate Microsoft example DAGs to new design #22452 - winrm* Fix static checks,0
"Task instances could be double triggered when using the --force, not anymore",1
[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747),2
Merge pull request #102 from mistercrunch/ti_keyAdding task_instance_key_str to default template macros,1
"[GH-9708] Add type coverage to Sendgrid module (#10134)Declare a ""custom"" AddressesType that takes a string or iterable ofstrings, which covers to, cc, and bcc, then declare this and other datatypes for the sendgrid plugin.",5
Refresh list of committers (#24398),5
Fix BigQuery system tests (#24013)* Change execution_date to data_interval_start in BigQueryInsertJobOperator job_idChange-Id: Ie1f3bba701169ceb2b39d693da320564de145c0c* Change jinja template path to relative pathChange-Id: I6cced215124f69e9f4edf8ac08bb71d3ec3c8afcCo-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,4
Create failing test for duplicate run_id marked externally triggered,1
Invalidate Vault cached prop when not authenticated (#17387),5
Merge pull request #429 from jlowin/import_errmore informative error message,0
Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404),2
Adding extra connection expected in tests,3
breeze setup-autocomplete zshrc reload (#18893),1
[AIRFLOW-1193] Add Checkr to company using AirflowCloses #2276 from tongboh/patch-1,1
make requirements work,1
GCP Secret Manager error handling for missing credentials (#17264),0
"Do not fail KubernetesPodOperator tasks if log reading fails (#17649)In very long running airflow tasks using KubernetesPodOperator,especially when airflow is running in a different k8s cluster than wherethe pod is started with, we see sporadic, but reasonably frequentfailures like this, after 5-13 hours of runtime:[2021-08-16 04:00:25,871] {pod_launcher.py:198} INFO - Event: foo-bar.xyz had an event of type Running[2021-08-16 04:00:25,893] {pod_launcher.py:149} INFO - 210816.0400+0000 app-specific-logs......... (~few log lines ever few minutes from the app)...[2021-08-16 17:20:29,585] {pod_launcher.py:149} INFO - 210816.1720+0000 app-specific-logs....[2021-08-16 17:27:36,105] {taskinstance.py:1501} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 436, in _error_catcher    yield  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 763, in read_chunked    self._update_chunk_length()  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 693, in _update_chunk_length    line = self._fp.fp.readline()  File ""/usr/local/lib/python3.7/socket.py"", line 589, in readinto    return self._sock.recv_into(b)  File ""/usr/local/lib/python3.7/ssl.py"", line 1071, in recv_into    return self.read(nbytes, buffer)  File ""/usr/local/lib/python3.7/ssl.py"", line 929, in read    return self._sslobj.read(len, buffer)TimeoutError: [Errno 110] Connection timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1157, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1331, in _prepare_and_execute_task_with_callbacks    result = self._execute_task(context, task_copy)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1361, in _execute_task    result = task_copy.execute(context=context)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 366, in execute    final_state, remote_pod, result = self.create_new_pod_for_operator(labels, launcher)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 520, in create_new_pod_for_operator    final_state, remote_pod, result = launcher.monitor_pod(pod=self.pod, get_logs=self.get_logs)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/utils/pod_launcher.py"", line 147, in monitor_pod    for line in logs:  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 807, in __iter__    for chunk in self.stream(decode_content=True):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 571, in stream    for line in self.read_chunked(amt, decode_content=decode_content):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 792, in read_chunked    self._original_response.close()  File ""/usr/local/lib/python3.7/contextlib.py"", line 130, in __exit__    self.gen.throw(type, value, traceback)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 454, in _error_catcher    raise ProtocolError(""Connection broken: %r"" % e, e)urllib3.exceptions.ProtocolError: (""Connection broken: TimeoutError(110, 'Connection timed out')"", TimeoutError(110, 'Connection timed out'))Most likely because the task is not emitting a lot of logs, or simply dueto sporadic network slowdown between clusters.So, if this fails, do not fail whole operator and terminate the task,until the call to `self.base_container_is_running` function also fails orreturns false.",0
Doc: Fix absolute Doc link (#19780),2
Merge pull request #1317 from jlowin/run-example-unit-testsInclude all example dags in backfill unit test,3
"Optional import error tracebacks in web ui (#10663)This PR allows for partial import error tracebacks to be exposed on the UI, if enabled. This extra context can be very helpful for users without access to the parsing logs to determine why their DAGs are failing to import properly.",2
Add Python 3.9 support (#15515)This includes several things:* added per-provider support for python version. Each provider  can now declare python versions it does not support* excluded ldap core extra from Python 3.9.* skip relevant tests in Python 3.9,3
"[AIRFLOW-402] Remove NamedHivePartitionSensor static check, add docsAddresses the following issues:[https://issues.apache.org/jira/browse/AIRFLOW-402](https://issues.apache.org/jira/browse/AIRFLOW-402)Closes #1711 from zodiac/fix_named_hive_partition_sensor",0
[AIRFLOW-4479] - Include s3_overwrite kwarg in load_bytes method (#5312),5
Add FanDuel to the list of users in the wild.  (#11864),1
[AIRFLOW-XXXX] Add Changelog for 1.10.8 (#7383),4
[AIRFLOW-XXX] Add pecan.ai to the users list (#6005),1
Try to fix deprecation warnings from distutils update (#20420),5
[AIRFLOW-5131] Create scopes property in GoogleCloudBaseHook (#5745),5
[AIRFLOW-XXX] Updating instructions about logging changes in 1.10 (#3715)We had a few other logging changes that weren't mentioned in here thatmeant previous logs were not viewable anymore.,2
[AIRFLOW-7055] Verbose logging option for google provider (#7711),1
Bring back some inclusions before we solve cyclic deps problems (#10551),0
Fixed wrongly escaped characters in amazon's changelog (#17020)* Fixed documenation generation for July providers.* fixed a problem with documentation generation problem with  html-escaped characters* regenerated the documentation to include the changelog changes.,4
Make generated job_id more informative in BQ insert_job (#9203)* Make generated job_id more informative in BQ insert_job* fixup! Make generated job_id more informative in BQ insert_job* fixup! fixup! Make generated job_id more informative in BQ insert_job* fixup! fixup! fixup! Make generated job_id more informative in BQ insert_job,5
Update manage-dags-files.rst to fix some inconsistencies (#20630),0
Standardize AwsLambda (#25100)* Standardize AwsLambdaThe `aws_lambda.py` is not the standard file path. The `aws_` prefix is not needed.For the file name I used the same path as the hook.* fix deprecated_classes.py* update verify_providers.py,1
"[AIRFLOW-1884][AIRFLOW-1059] Reset orphaned task state for external dagrunsOn scheduler startup, orphaned task instances havetheir state cleared and are rescheduled to avoidhaving tasks that are stuck in a QUEUED stateforever. Previously, this check ignored backfilledand externally triggered dagruns, meaning thatbackfilled and externally triggered dagruns couldhave orphaned tasks that are stuck forever. Thischangeset removes the special case logic forexternally triggered dagruns, ensuring thatexternally triggered dagruns are crash safe. Thissame fix cannot be applied to backfilled dagruns,so for now backfilled dagruns are not crash safe.Closes #2843 from grantnicholas/AIRFLOW-1884",2
Fix missing space for breeze build-image hint command (#25204),0
[AIRFLOW-635] Encryption option for S3 hookS3 gives the option of storing objects encryptedon the server side. Thischange exposes the boto S3 encrypt option in theload_file method of theS3 hook. It also updates missing documentation forthe load_string method.Closes #1888 from kerzhner/master,2
Merge pull request #1278 from bolkedebruin/ISSUE-1083Rename user table to users to avoid conflict with postgres,5
Add dataset-triggered run type icon (#25244)* add dataset triggered run type* add comment* fix linting,0
Make static checks Google Shell Guide compatible (#10750)Part of #10576,1
Add missing licenses and update `.rat-excludes` (#23296),5
[AIRFLOW-5716] Simplify DataflowJobsController logic (#6386),2
Fixing some unit tests,3
"[AIRFLOW-2545] Eliminate DeprecationWarningDo not import BaseOperator as KubernetesOperator.This eliminates confusing DeprecationWarnings whensetting up a default airflow install whereadditionalkubernetes modules are not installed.Also, use LoggingMixin instead of logger module.Closes #3442 from rodrigc/AIRFLOW-2545",2
[AIRFLOW-7105] Unify Secrets Backend method interfaces (#7830),5
Add BigQueryHook,1
"Update `check_files.py` to support many provider releases (#25077)This allows you to put many provider releases in `packages.txt` andcheck them all in one go. In fact, you can copy/paste the list of pypiurls from the vote email!",5
Add Easy Taxi to list of companies using Airflow,1
[AIRFLOW-3012] Fix Bug when passing emails for SLA,4
Alleviate import warning for `EmrClusterLink` in deprecated AWS module (#21195),2
Update scheduler deployment - dags volume mount (#10630),2
Hide tooltip when next run is none (#19112)* Hide Next Run when it is NoneCheck for `is not none` instead of `is defined` when showing the next run tooltip* check next_dagrun is defined and not none,2
Don't include provider datafiles in the apache-airflow sdist (#12196)We shouldn't include these in the main sdist now we've split providersto separate distributions.,1
"Prepares release for Salesforce provider (#17272)By mistake, Salesforce provider has been marked as doc-only where ithad new connection type added. This PR updates Salesforce docs preparingit for separate release.",2
Rotate session id during login (#25771),2
"[AIRFLOW-356][AIRFLOW-355][AIRFLOW-354] Replace nobr, enable DAG only exists locally message, change edit DAG iconAddresses the following issues:- [https://issues.apache.org/jira/browse/AIRFLOW-356](https://issues.apache.org/jira/browse/AIRFLOW-356)- [https://issues.apache.org/jira/browse/AIRFLOW-355](https://issues.apache.org/jira/browse/AIRFLOW-355)- [https://issues.apache.org/jira/browse/AIRFLOW-354](https://issues.apache.org/jira/browse/AIRFLOW-354)- Replace `<nobr>` with `flexbox`- ""This DAG seems to be existing only locally"" now shows up- Change edit DAG icon from info to edit- Rename `dttm` variable to `file_last_changed_on_disk`- Rename `dags` variable to `webserver_dags`- Adds a comment clarifying what `self.file_last_changed` is- Clarifies what the `dag.last_expired` column represents- Refactors some previously very nested logic in `views.py` and adds comments- Properly indents `dags.html` and adds comments to it- Edit DAG icon changed- Home page now sort of responsive, no longer fixed width- User will occasionally see ""This DAG seems to be existing only locally"" message- Verify that edit dag button is now an edit icon and click on it- Resized home page, check that last column does not wrap![image](https://cloud.githubusercontent.com/assets/130362/17126889/2e7adb12-52b6-11e6-9a18-b31e424e4be8.png)Clean up html, replace nobr with flexboxRefactor HomeViewRename variables and update commentsCloses #1678 from zodiac/xuanji_refactor",4
Fixed all remaining code usage of old task import (#15118)This finishes the job of removing all imports of task from`operators.python` and replaces them with the new import from`decorators`.,2
Replaces depreated set-env with env file (#11292)Github Actions deprecated the set-env action due to moderate securityvulnerability they found.https://github.blog/changelog/2020-10-01-github-actions-deprecating-set-env-and-add-path-commands/This commit replaces set-env with env file as explained inhttps://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-commands-for-github-actions#environment-files,3
[AIRFLOW-5781] AIP-21 Migrate AWS Kinesis to /providers/amazon/aws (#6588),1
Deprecate Tableau personal token authentication (#16916)* Deprecate Tableau personal token authentication* Fix spelling in documentation* Fix styling issue in TableauHook,1
Add emr cluster link (#18691),2
[AIRFLOW-5898] fix alembic crash due to typing import (#6547),2
update tree data fetching (#19605)- add `base_date` to refresh api request- sort runs only on the webserver- add test for auto-refresh stop,3
Avoid broad exceptions when json.loads is used (#9432),1
"[AIRFLOW-2859] Implement own UtcDateTime (#3708)The different UtcDateTime implementations all have issues.Either they replace tzinfo directly without convertingor they do not convert to UTC at all.We also ensure all mysql connections are in UTCin order to keep sanity, as mysql will ignore thetimezone of a field when inserting/updating.",5
Removing js flicker on page load,4
Addressed the comments on shorter import and space in configuration.py,5
Fixed month in backport packages to October (#11242),0
[AIRFLOW-4361] Fix flaky test_integration_run_dag_with_scheduler_failure (#5182),3
Support impersonation service account parameter for Dataflow runner (#23961),1
Make ci/scripts/pre-commit Google Shell Guide compatible (#10748)Part of #10576,1
Merge pull request #150 from kerzhner/tree_view_baseTree view base time fix,0
[AIRFLOW-5042] Improve mocking in Dataproc operator tests (#5662),3
Bump browserslist from 4.16.3 to 4.21.3 in /airflow/ui (#25807)Bumps [browserslist](https://github.com/browserslist/browserslist) from 4.16.3 to 4.21.3.- [Release notes](https://github.com/browserslist/browserslist/releases)- [Changelog](https://github.com/browserslist/browserslist/blob/main/CHANGELOG.md)- [Commits](https://github.com/browserslist/browserslist/compare/4.16.3...4.21.3)---updated-dependencies:- dependency-name: browserslist  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
[AIRFLOW-2430] Extend query batching to additional slow queriesCloses #3324 from gsilk/batch-inserts,1
use path.join,1
Add ODBC extra for the production image (#18407)The ODBC extra has been missing from #18382. This PR adds themissing extra and verifies if pyodbc is importable in the PRODimage.,2
Pylint: Enable deprecated-sys-function check (#7838),5
[AIRFLOW-2345] pip is not used in this setup.pyCloses #3241 from sinemetu1/patch-1,1
[AIRFLOW-613][AIRFLOW-1] Add Astronomer as Airflow userCloses #1864 from schnie/master,1
Fixing file timestamp read race condition,2
[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677),2
Merge pull request #1134 from airbnb/dagbag_timeoutParameterizing DagBag import timeouts,2
TimeSensor should respect DAG timezone (#9882),2
Improve logging of optional provider features messages (#23037)The optional provider features are now better detected and weare just logging an info message in case some missing importsare detected during provider importing hooks.Fixes: #23033,0
"Merge pull request #101 from mistercrunch/clear_killClearing running jobs now kills them properly, regardless of the executor",1
Logging which host is being usedUseful when using multiple hosts with same conn_id and one is defective,1
Adding a warning when the max number of active DAG runs has been reached,1
"Merge pull request #898 from iddoav/making_force_ti_memberMaking force a task instance member, so it becomes available for",1
add s3_log_folder config option,5
[AIRFLOW-813] Fix unterminated scheduler unit testsCloses #2028 from fenglu-g/master,3
Use Popen with CeleryExecutorUse the same calling format as the other Executors,1
CONTRIBUTING.rst minor link fix and spelling (#14237),0
Adding a config param to not load example dags,2
Always restart scheduler if it stops after 5 runs,1
[AIRFLOW-XXX] Add Etsy to companies list (#4204),1
Merge pull request #1049 from bolkedebruin/hdfs_hookAllow the use of the autoconfig client,5
"Fix cursor execution with None parameters with cx_Oracle (dbapi_hook)cx_Oracle doesn't like None parameters, so I changed all cursor execution lines to check if parameters are given or notIf not, the parameters are left out completely from the cursor execution",2
Remove unused dependency (#15762),1
Switch providers-init pre-commit to Python (#26125)Part of #26020,5
Hide configuration,5
Wait for successful airflow-init service completion (#16180),5
bumping simple-salesforce to 1.0.0 (#7857),1
[AIRFLOW-6297] Add Airflow website link to UI docs (#6849),2
Bump stylelint to remove vulnerable sub-dependency (#15784),4
Add more information about using GoogleAdsHook (#9951)This hook requires two connections and it's not obvious howto use it and what is the purpose of each connection.,1
[AIRFLOW-6858] Decouple DagBag and Executor (#7479),2
Fix `email_on_failure` with `render_template_as_native_obj` (#22770)Co-authored-by: andyhuang <andyhuang@mirrormedia.mg>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,0
Merge pull request #707 from airbnb/bugfix_dagbagimport[bugfix] removing rogue dagbag parsing in module head,2
"Serialize mapped tasks and task groups (#20743)* Basic serialization support for MappedOperators* Serialization support for TaskGroups* Make downstream_task_ids a normal writeable propertyIt simplifies a few things.We also deal with (and test) the old name when deserializing* Remove task_id from kwargs only in MappedOperatorSince `task_id` is handled speically in the serialization ofMappedOperators, we don't want it duplicated in to the partial_kwargs.Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
"Fix behaviour of build/pull after recent Breeze changes (#24657)The behaviour of Breeze after some recent changes related topulling and building images in parallel have been slightly broken.Nothing serious but slightly annoying behaviour:* when starting breeze shell, the image was attempted to be  build even if it was not needed (but cache efficiency made it fast  enough to not be too annoying (unless we updated to newer  python base image* breeze pull command for ""latest"" branch makes no sense any more -  we stopped pushing ""latest"" image to ghcr.io, we only push  cache and ""tagged"" images. We are now turning --image-tag as  required in ""pull_image"" and when someone specifies latest,  error and helpful message is printed* --force-build flag in ""shell-related-commands"" was not  properly propagated to build-image so it did not actually  force image building.All those problems are fixed now.",0
"[AIRFLOW-504] Store fractional seconds in MySQL tablesBoth utcnow() and now() return fractional seconds. Theseare sometimes used in primary_keys (eg. in task_instance).If MySQL is not configured to store these fractional secondsa primary key might fail (eg. at session.merge) resulting ina duplicate entry being added or worse.Postgres does store fractional seconds if left unconfigured,sqlite needs to be examined.",5
[AIRFLOW-1517] addressed PR comments,1
[AIRFLOW-335] Fix simple style errors/warningsCloses #1665 from skudriashev/airflow-335,2
Improve web server stopping (#11734),1
Create Dataproc operators for GKE,1
Merge pull request #76 from woodlee/message-fixMinor exception message spacing fix,0
"[AIRFLOW-2130] Add missing Operators to API Reference docs* Fix autodoc import path for BaseSensorOperator* Add autodoc imports for many core & contrib Operators that were missing* Rename ""Operator API"" section to ""Core Operators"" for better contrast with the following ""Community-contributed Operators"" section* Add subheadings to API Reference#Operators. Since all the Sensors were already alphabetized separately from the rest of the operators, this formalizes that distinction and moves all the Transfer operators to their own section as well.* Alphabetize Operator class names* Improve formatting in top-level Operators sectionThis also fixes the earlier and more narrowly scoped [AIRFLOW-951]",0
"Update oracle_hook.pyadd comments, remove unnecessary code",4
Allow setting specific cwd for BashOperator (#17751)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
add warning with instructions for creating a key,1
Bump ssri from 6.0.1 to 6.0.2 in /airflow/www (#15437)Bumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.- [Release notes](https://github.com/npm/ssri/releases)- [Changelog](https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md)- [Commits](https://github.com/npm/ssri/compare/v6.0.1...v6.0.2)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Fix mypy in  providers/salesforce (#20325),1
[AIRFLOW-2303] Lists the keys inside an S3 bucketLists the keys matching a prefix and a delimiter inside an S3 bucketCloses #3203 from wileeam/s3-list-operator,1
[AIRFLOW-6660] Fix deprecation warning in check_environment.sh (#7275),2
clean up verify_providers.py (#26075)Some entries are not needed any more as the modules that generated them were removed in latest major release of Amazon provider,1
"Fix task group tooltip (#18406)The task group tooltip wasn't just not loading the right info, but it wasn't rendering at all. This PR fixes that.Also:- add `deferred` as a group node status- show all possible status summaries in the group tooltip but only show them if the count is >0",1
Add files to generate Airflow's Python SDK (#14739),2
[AIRFLOW-2004] Import flash from flask not flask.loginCloses #2943 from bolkedebruin/AIRFLOW-2004,2
"[AIRFLOW-1445] Changing HivePartitionSensor UI color to lighter shadeMy PR is simply to improve the readability of thetext using the HivePartitionSensor. The screenshots below show the before and after. The darkershade (nearly black) is the before, and the purplecolor is the after.Closes #2476 from Acehaidrey/master",2
[AIRFLOW-XXX] Add Format to list of companies (#3824),1
Fix typo in docs link (#19837),2
"doc: reload pods when using the same DAG tag (#24576)Using the same tag for airflow dags cannot work as kubernetes will not refresh pods (scheduler, workers, webservers) if they are not updated.Then you need to trigger the reloading of those pods manually.",5
Decouple name randomization from name kwarg (#19398),5
Update example dataset DAGs names (#25910)* Update example dataset DAGs namesThe terminology upstream/downstream in datasets was previously renamedto produces/consumes.,5
Move doc around Manual Trigger Visual diff to Tree View page (#12565),2
Fix last remaining MyPy errors (#21020)Closes: #19891,0
Set Postgres autocommit as supported only if server version is < 7.4The server-side autocommit setting was removed here http://www.postgresql.org/docs/7.4/static/release-7-4.htmlResolves: #690,2
Removed interfering force of index. (#25404),1
[AIRFLOW-5000] Remove duplicate end_date and reorder template (#5618)* Remove the duplicate end_date in template context* Reorder template context key to make ds together,1
"Adds capability of Warnings for incompatible community providers (#18020)When we release providers, we do not know if some future versionof Airflow will be incompatible with them, so we cannot add hardlimits there. We have constraints that contain the ""latest""providers at the moment of release but if someone has an oldversions of providers installed and just upgrades Airflow, theincompatible versions of providers might be still installed.From now on Airflow will print warnings in case such incompatibleprovider is detected.",1
"Fix broken links to celery documentation (#22364)The celery documentation have been moved from https://docs.celeryproject.org/ to https://docs.celeryq.dev/. The old links now refer to a 404 error page, the new links to the actual documentation.",2
Fix DagBag bug when a dag has invalid schedule_interval (#11344),2
[AIRFLOW-5142] Fixed flaky cassandra test (#5758),3
[AIRFLOW-5878] Use JobID to monitor statuses when running a Dataflow  (#6530)* [AIRFLOW-5878] Use JobID to monitor statuses when running a Dataflow template,5
Update secrets backends to use get_conn_value instead of get_conn_uri (#22348)In #19857 we enabled storing connections as JSON instead of URI and renamed get_conn_uri to get_conn_value to be consistent with this change.  The method get_conn_uri is now deprecated and should warn when used.,1
Remove color change for highly nested groups (#23482),4
Stop running Doc tests with Spelling tests (#10938)Currently we run the following tests too when checking spell-check errors- check_guide_links_in_operator_descriptions()- check_class_links_in_operators_and_hooks_ref()- check_guide_links_in_operators_and_hooks_ref()- check_enforce_code_block()- check_exampleinclude_for_example_dags()- check_google_guides(),2
Adds HiveToDynamoDB Transfer Sample DAG and Docs (#22517),2
Update start.rst (#11886),5
[AIRFLOW-4393] Add exponential backoff retry (#5284),1
"Fixes dependencies to pre-release versions of apache-airflow (#11578)The dependencies in Alphas are currently >= 2.0.0 and should be>=2.0.0a0 in order to work with Alphas in cases which are not PEP440-compliant.According to https://www.python.org/dev/peps/pep-0440/, >= 2.0.0 shouldalso work with alpha/beta releases (a1/a2) but in some cases it does not(https://apache-airflow.slack.com/archives/C0146STM600/p1602774750041800)Changing to "">=2.0.0a0"" should help.Fixes #11577",0
Add GoogleCalendarToGCSOperator (#20769),1
Fix field ordering in mysql_to_hive,0
closes apache/incubator-airflow#3389 *Obsolete PR*,5
Use pfromat instead of str to render arguments in WebUI (#9587),1
[AIRFLOW-5495] Remove unneeded parens in dataproc.py (#6105)* AIRFLOW-5495 removing the parens* [AIRFLOW-5495] Update airflow/gcp/operators/dataproc.pyCo-Authored-By: Kamil Bregua <mik-laj@users.noreply.github.com>,1
[AIRFLOW-6393] Ensure rendering of all lineage items and record source (#6953)Only lineage items obtained from XCom were rendered rather thanall. Additionally source tasks are recorded.,1
"Fix parallelism after KubeExecutor pod adoption (#15555)* Fix parallelism after KubeExecutor pod adoptionThis fixes a bug where adopted pods didn't count as being run by theexecutor for the purposes of honoring parallelism, nor for metrics beingexported. Now adopted tasks will be reflected like a task the executoractually started.* Fix circular import",2
Fix `oauth_whitelists` in BaseSecurityManager (#21308),0
[AIRFLOW-1057][AIRFLOW-1380][AIRFLOW-2362][2362] AIRFLOW Update DockerOperator to new APIupdate import to docker's new API version >=2.0.0changed dependency for docker package; now dockerrather than docker-pyupdated test cases to align to new docker classCloses #3407 from Noremac201/fixer,0
Set `webhook_endpoint` as templated field in `DiscordWebhookOperator`(#22570),1
add an option for run id in the ui trigger screen (#21851),2
[AIRFLOW-6181] Add InProcessExecutor (#6740)Adds new executor that is meant to be used mainlyfor debugging and DAG development purposes. Thisexecutor executes single task instance at time andis able to work with SQLLite and sensors.,1
Setting up necessary dependencies for tests,3
Fix docs spelling errors (#15884),0
Update spelling_wordlist for docs,2
[AIRFLOW-2536] docs about how to deal with airflow initdb failureAdd docs to faq.rst to talk about how to deal withException: Global variableexplicit_defaults_for_timestamp needs to be on (1)for mysqlCloses #3429 from milton0825/fix-docs,2
Require can_edit on DAG privileges to modify TaskInstances and DagRuns (#16634),2
Adding LendUp to company list,1
"Revert ""Verify enough resources for breeze (#20763)"" (#20948)This reverts commit 75755d7f65fb06c6e2e74f805b877774bfa7fcda.",4
Improve Helm Chart Git-Sync documentation (#15937)Mounting DAGs from a private Github repo using Git-Sync sidecar isquite complex because of the several steps required and the manyother moving parts.The PR aims to ameliorate some of these pain points so that users canhave a smoother experience when mounting their DAGs from private reposon Github.,2
import next,2
Extend config window on UI (#20052)* Extend config window on UI* Use min and max height logic,2
[AIRFLOW-4665] Remove contrib/plugins from Pylint todo (#5851)This commit intends to remove files under contrib/plugins outof the Pylint todo list.,2
Merge pull request #39 from mistercrunch/fixesDisplay bug for duration,0
Remove trailing comma in setup_backport_packages.py (#9284),1
Move the test_process_dags_queries_count test to quarantine (#11455)The test (test_process_dags_queries_count)randomly produces bigger number of counts. Example here:https://github.com/apache/airflow/runs/1239572585#step:6:421,1
[AIRFLOW-5826] Added auto-generation of breeze output in BREEZE.rst (#6490)This is needed to keep breeze --help in sync with the documentation.It makes it easier for the follow-up changes needed for productionimage to keep the docs in sync with the code.,2
Define datetime and StringID column types centrally in migrations (#19408)We have various flavours of the code all over the place in manymigration files -- which leads to duplication and things not being insync.This pulls them once in to a central location.,2
"utils: fix process races when killing processes (#5721)airflow.utils.helpers.reap_process_group() can throw uncaught OSErrorsif processes exit at the wrong time in its execution. Fix this bycatching all OSErrors that can arise due to a process race, andreturning from them when the error is ESRCH (process not found).",0
"Remove duplicate call to sync_metadata inside DagFileProcessorManager (#15121)`_process_message` already calls sync_metadata if it's a DagParsingStat,so there's no need to call it again.  This isn't expensive, but no pointdoing itI've also added a runtime check to ensure this function is only used insync mode, which makes the purpose/logic used in this function clearer.",1
"[AIRFLOW-2538] Update faq doc on how to reduce airflow scheduler latencyMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2538    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Update the faq doc on how to reduce airflowscheduler latency. This comes from our internalproduction setting which also aligns with Maxime'semail(https://lists.apache.org/thread.html/%3CCAHEEp7WFAivyMJZ0N+0Zd1T3nvfyCJRudL3XSRLM4utSigR3dQmail.gmail.com%3E).### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [ ] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [ ] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3434 from feng-tao/update_faq",5
[AIRFLOW-XXX] Link to correct class for timedelta in macros.rst (#5226)The `macros.timedelta` variable previously referred to`datetime.datetime` which was incorrect.,5
Fix mypy tests in tests/providers/amazon/aws/sensors (#20431)Fix mypy tests in tests/providers/amazon/aws/sensors,3
"Remove up-to-date checks for Python (#21208)The up-to-date check for Python run for ~week so all the PRsraised in the last week will need to be rebased to account forMyPy changes. Hopefully there will be no more PRs from before,that have not been rebased (we had one serious MyPy problem forthe #20077 change that was approved before the up-to-date checkerwas enabled in #21016",0
[AIRFLOW-1774] Allow consistent templating of arguments in MLEngineBatchPredictionOperatorFix a minor typo and a wrong non-defaultassignmentFix one more typoAdapt tests to new error messages and fix anothertypoFix exception type in utils operator test classImprove cleansing of non-valid training andprediciton job namesCloses #2746 from wileeam/ml-engine-prediction-job-normalization,5
[AIRFLOW-5065] Add colors to console log (#5681)This commit adds custom formatter for Airflow console log to display colourswhen connected to a TTY,2
[AIRFLOW-2704] Add support for labels in the bigquery_operator[AIRFLOW-2704]Add support for labels in thebigquery_operatorAdds support for bigquery labels in the bigqueryoperator and hook.Make labels template fieldsCloses #3573 from mastoj/AIRFLOW-2704,1
Fix ``AttributeError``: ``datetime.timezone`` object has no attribute ``name`` (#16599)closes: #16551Previous implementation tried to force / coerce the provided timezone (from the dag's `start_date`) into a `pendulum.tz.timezone.*` that only worked if the provided timezone was already a pendulum's timezone and it specifically failed when with `datetime.timezone.utc` as timezone.,5
[AIRFLOW-1299] Support imageVersion in Google Dataproc clusterCloses #2358 from yu-iskw/dataproc-image-version,5
Update doc and sample dag for EMR Containers (#24087),2
[AIRFLOW-4965] Handle quote exceptions in GCP operators (#6305),1
[AIRFLOW-6818] Prevent Docker cache-busting on when editing www templates (#7432)There is two parts to this PR:1. Only copying www/webpack.config.js and www/static/ before running the   asset pipeline2. Making sure that _all_ files (not just the critical ones) have the   same permissions.,2
Add Sidecar Interactive to list of companies using Airflow,1
[AIRFLOW-2108] Fix log indentation in BashOperator- Changed from `strip` to `rstrip` to retainleading whitespacesCloses #3045 from kaxil/AIRFLOW-2108,4
Pass X-Presto-Client-Info in presto hook (#22416),1
[AIRFLOW-5314] Create test for new import paths (#5920),2
[AIRFLOW-2876] Update Tenacity to 4.12 (#3723)Tenacity 4.8 is not python 3.7 compatible because it containsreserved keywords in the code,1
Allow ./run_tmux.sh script to run standalone (#13420),1
Document Pagerduty provider in installation.rst (#12054),1
Only refresh active dags on dags page (#24770)* Only refresh active dags on home page* Make feedback updates* Replace function parameters with object* Fix eslint errors* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,2
Add some tasks using BashOperator in TaskGroup example dag (#11072)Previously all the tasks in airflow/example_dags/example_task_group.py were using DummyOperator which does not go to executor and is marked as success in Scheduler itself so it would be good to have some tasks that aren't dummy operator to properly test TaskGroup functionality,1
"[AIRFLOW-713] Jinjafy {EmrCreateJobFlow,EmrAddSteps}Operator attributesTo dynamically templat the fields of the Emr Operators, we needto pass the fields to jinjaCloses #3016 from Swalloow/emr-jinjafied",4
[AIRFLOW-6654] AWS DataSync - bugfix when creating locations (#7270),1
Fix git archive command in Release Management guide (#12526)There was a trailing back-tick which I found when cutting 1.10.13rc1,0
Add timeout option to gcs hook methods. (#13156),1
Fix failing Black test on connexion (#10547),3
Merge branch 'master' of https://github.com/mistercrunch/Airflow,1
[AIRFLOW-6625] Explicitly log using utf-8 encoding (#7247)This is the standard encoding supported by Airflow. We should be explicit about logging usingthis encoding.,1
[AIRFLOW-5636] Allow adding or overriding existing Operator Links (#6302),2
Catch Permission Denied exception when getting secret from GCP Secret Manager. (#10326),1
Add more .mailmap entries (#9489),1
lengend color fix for dark ops,0
Less verbose logging in ssh operator (#24915)* Less verbose logging in ssh operator,1
Create links for Biqtable operators (#23164),1
Added Bloomreach to the list of companies using Apache Airflow (#11995),1
[AIRFLOW-XXX] Add TokenAnalyst to Airflow Users (#6605),1
Add Credit Karma to company listCloses #2436 from greg-finley-ck/ck,2
Merge pull request #338 from airbnb/generic_transferAdding a GenericTransfer operator,1
Call super.tearDown in SystemTest tearDown (#9196),5
"[AIRFLOW-5660] Attempt to find the task in DB from Kubernetes pod labels (#6340)Try to find the task in DB before regressing to searching every task, and explicitly warn about the performance regressions.Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>",5
"[AIRFLOW-3905] Allow using ""parameters"" in SqlSensor (#4723)* [AIRFLOW-3905] Allow 'parameters' in SqlSensor* Add check on conn_type & add testNot all SQL-related connections are supported by SqlSensor,due to limitation in Connection model and hook implementation.",1
Discard semicolon stripping in SQL hook (#25855),1
"[AIRFLOW-3623] Fix bugs in Download task logs (#5005)They didn't work in Python 3 (Bytes vs Strings), the graph_view wasn'tspecifying the try_number correctly, and the ""All"" logs had ""None"" asthe suffix instead of ""all"".",0
Remove unused 'context' variable in task_instance.py (#14049),1
Migrate Qubole example DAGs to new design #22460 (#24149)* Migrate Qubole example DAGs to new design #22460,1
Memorystore assets & system tests migration (AIP-47) (#25361),3
Upgrade azure blob to v12 (#12188),5
Adding a GenericTransfer operator,1
"AIRFLOW-124 Implement create_dagrunThis adds the create_dagrun function to DAG and the staticmethodDagRun.find. create_dagrun will create a dagrun including its tasks.By having taskinstances created at dagrun instantiation time,deadlocks that were tested for will not take place anymore. Testshave been adjusted accordingly.In addition, integrity has been improved by a bugfix to add_taskof the BaseOperator to make sure to always assign a Dag if it ispresent to a task.DagRun.find is a convenience function that returns the DagRunsfor a given dag. It makes sure to have a single place how tofind dagruns.",2
Doc improvments,2
Bump async from 2.6.3 to 2.6.4 in /airflow/ui (#23034)Bumps [async](https://github.com/caolan/async) from 2.6.3 to 2.6.4.- [Release notes](https://github.com/caolan/async/releases)- [Changelog](https://github.com/caolan/async/blob/v2.6.4/CHANGELOG.md)- [Commits](https://github.com/caolan/async/compare/v2.6.3...v2.6.4)---updated-dependencies:- dependency-name: async  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Make airflow/logging_config.py Pylint Compatible (#9672),5
Implement run_as functionality based on the login of the connection or the owner of the DAG,2
HivePartitionSensor doc fix,0
Improve speed of `dag.partial_subset` by not deep-copying TaskGroup (#23088)This resulted in the _entire_ dag being copied over and over many times.For a task with 500 dags this takes the time of this function down from60s(!) to just over 1s.,1
Fix docstring of SqlSensor (#15466)Fix docstring of SqlSensor by removing `:` from `type:`.,4
Doc: Use ``closer.lua`` script for downloading sources (#18179)- Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors- Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178),0
Fix airflow_local_settings.py showing up as directory (#10999)Fixes a bug where the airflow_local_settings.py mounts as a volumeif there is no value (this causes k8sExecutor pods to fail),0
[AIRFLOW-XXX] Move article about defining links (#5170),2
"[AIRFLOW-3558] Improve default tox flake8 excludes (#4361)Right now our gitignore skips a bunch of temporary Python directoriesbut our flake8 config will still test against them, leading tounnecessary error messages. This changes the excludesto skip the common directories that can cause false flake8 failures.",0
Switch to `pipx` as the only installation Breeze2 method (#22740)Switching Breeze2 to only use `pipx` for installation of Breeze2due to problems it might cause for autocompletion if entrypointis not avaiable on PATH.,1
Fix deprecation messages in airflow.utils.helpers (#9398),0
adding quick description for singularity container operator (#12047)Signed-off-by: vsoch <vsochat@stanford.edu>,1
Return non-zero error code when variable is missing(#8438),0
Increase width for Run column (#17817),1
closes apache/incubator-airflow#1580 *PR abandonned by submitter*,5
Added Arrive (Parkwhiz) to list of users (#5082),1
docs: fix inconsistencies in configuration docs (#17317),2
[AIRFLOW-XXX] Fix typos in docsCloses #3625 from feluelle/master,2
Resolve mypy issue in athena example dag (#22020)* Resolve mypy issue in athena example dag* fixup! Resolve mypy issue in athena example dag,2
Add BigQuery to Google Cloud Storage operator.,1
Move `airflowLocalSettings` to be a top level chart param (#15838)Moving it to be a top level chart param makes sense as it is used inmost of the Airflow components.,1
Make DbApiHook use get_uri from Connection (#21764)DBApi has its own get_uri method which does not dealwith quoting properly and neither with empty passwords.Connection also has a get_uri method that deals properlywith the above issues.This also fixes issues with RFC compliancy.,0
[AIRFLOW-308] Add link to refresh DAG within DAG view headerCloses #1646 from Firehed/refresh_on_dag,2
Add number of node params only for single-node cluster in RedshiftCreateClusterOperator (#23839),1
Simplify `default_args` in Kubernetes example DAGs (#16870),2
"Used many-to-many relation for finding attached dataset events. (#25758)The previous PR added this join table -- by using it we make theassociation of which events created a DagRun fixed at DagRun creationtime; previously if you deleted old DagRuns then the oldest remainingrun would ""collect"" all the oldest events.",5
[AIRFLOW-XXX] Add THE ICONIC to the list of orgs using AirflowCloses #3807 from ksaagariconic/patch-2,1
closes apache/incubator-airflow#989 *no movement from submitter*,4
[AIRFLOW-4971] Add Google Display & Video 360 integration (#6170),1
"Add sample dags and update doc for RedshiftClusterSensor, RedshiftPauseClusterOperator and RedshiftResumeClusterOperator (#22128)",1
"Add issue stats to PRotM score, enhance terminal output (#25741)",0
Faster grid view (#23951),5
Warn about precedence of env var when getting variables (#13501),1
Handle invalid date parsing in webserver views. (#23161)* Handle invalid date from query parameters in views.* Add tests.* Use common parsing helper.* Add type hint.* Remove unwanted error check.* Fix extra_links endpoint.,2
Improve code quality of ExternalTaskSensor (#12574),1
"Clarify dag-not-found error message (#19338)In this context, what's really happening is, we can't find the dag.  From a userperspective, when you encounter this error, 'could not find the dag' isa more intuitive representation of the problem than 'could not find the dag_id'.",2
"Perform ""mini scheduling run"" after task has finished (#11589)In order to further reduce intra-dag task scheduling lag we add anoptimization: when a task has just finished executing (success orfailure) we can look at the downstream tasks of just that task, and thenmake scheduling decisions for those tasks there -- we've already got thedag loaded, and we know they are likely actionable as we just finished.We should set tasks to scheduled if we can (but no further, i.e. not toqueued, as the scheduler has to make that decision with info about thePool usage etc.).Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",5
Prevent pop-over elements from being cut off by hidden overflow (#11574),5
"Improve MySqlToHiveOperator tests (#17958)These tests were actually just testing the hook again (by checking thecommand executed) but were asserting nothing about the CSV passed to thehook.This change makes the operator tests check the logic in the operator andno longer tests the hook code again.`test_mysql_to_hive_verify_loaded_values` was removed as since weremoved Java/a real hive CLI from our tests this has only been testingour mock, not the real code.",3
Add more refactor steps for providers.google (#8010)* Add more refactor steps for providers.google* fixup! Add more refactor steps for providers.google,1
doc: fixed docstring for sql param in Neo4jOperator (#17407),1
[AIRFLOW-1794] Remove uses of Exception.message for Python 3Closes #2766 from dhuang/AIRFLOW-1794,1
Fix typo in scripts/ci/libraries/_initialization.sh (#11517)`initialized` -> `initialize`,5
Move BigQuery hook to contrib.,1
Merge pull request #170 from mistercrunch/template_list_dictsSupport for dicts and list in operators template_fields,1
Fix stylelint violation (#8236),0
"[AIRFLOW-201] Fix for HiveMetastoreHook + kerberosDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-201Author: Alexey Ustyantsev <a.ustyantsev@corp.mail.ru>Closes #1563 from hudbrog/airflow_201.",5
Add Audit Log View to Dag View (#20733),2
Helm Chart: Add option to enable/disable flower (#14011),0
Change `Github` to `GitHub` (#23764),4
[AIRFLOW-3070] Refine web UI authentication-related docs (#3863),2
Added missing init file,2
Fix bug where fileloc didn't trickle into subdags,2
Merge pull request #583 from airbnb/fix_webFix blueprint/links issue when creating using multithreaded gunicorn,1
Add unit tests to infer `multiple_outputs` when invoking decorator `__call__` (#21773),5
Merge pull request #993 from 0xR/add-yesterday_ds_nodashAdded yesterday_ds_nodash and tommorow_ds_nodash,1
Fix the AttributeError with text field in TelegramOperator (#13990)closes: #13989,1
"Exclude ``yarn.lock`` from built Python wheel file (#16577)Same as https://github.com/apache/airflow/pull/16494 - However that PR had to be reverted in https://github.com/apache/airflow/pull/16518 as it failed building of PROD image, this PR/commit will fix it.PROBLEM: Currently the airflow wheel is built with the yarn.lock which is not actually used by the airflow itself. Having this file in the docker image causes the clair and trivy scanners to failFIX: The fix is to exclude the yarn.lock by specifying it in the manifest.in",0
Fix BQ hook system test (#13944),3
Merge pull request #667 from airbnb/macro_dateutilAdding datetil to macros and documenting macros references,2
"Update docs about tableau and salesforce provider (#14495)Since #14030 tableau is no longer deprecated, we can remove this doc entry",1
[AIRFLOW-2792] change parameter in post requests.Closes #3633 from happyjulie/AIRFLOW-2792,2
[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626),2
AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796)* add run_job_kwargs to glue job run* add run_kwargs to hook and operator tests,3
Remove limit of presto-python-client version (#24305),4
Improving the Hive2SambaOperator,1
[AIRFLOW-766] Skip conn.commit() when in Auto-commit,5
[AIRFLOW-XXXX] Add ShopBack as an Airflow user (#7418),1
"Fix misleading statement on sqlite (#14317)The statement ```By default, Airflow uses **SQLite**, which is *not* intended for development purposes only.```is confusing. If `postgres/mysql` are production worthy db backends, and `sqlite` as default db for `airflow` is for development purposes only, this statement is not correct. If I'm mistaken and `sqlite` is for both `production` and `development` purposes, please ignore this PR",5
"Revert ""[AIRFLOW-6596] Enforce description should not be empty (#7211)"" (#7219)This reverts commit d553813b0c35f212b2fa10a49e6a542d23e5ee83.",4
Merge pull request #209 from mistercrunch/hdfs_hookadd hdfs hook and update hdfs sensor to work with HA configuration,5
Merge pull request #887 from airbnb/undead_suicideKilling tasks that aren't in a running state,1
"Removes stable tests from quarantine (#10768)We've observed the tests for last couple of weeks and it seemsmost of the tests marked with ""quarantine"" marker are succeedingin a stable way (https://github.com/apache/airflow/issues/10118)The removed tests have success ratio of > 95% (20 runs withoutproblems) and this has been verified a week ago as well,so it seems they are rather stable.There are literally few that are either failing or causingthe Quarantined builds to hang. I manually reviewed themaster tests that failed for last few weeks and added thetests that are causing the build to hang.Seems that stability has improved - which might be casuedby some temporary problems when we marked the quarantined buildsor too ""generous"" way of marking test as quarantined, ormaybe improvement comes from the #10368 as the docker engineand machines used to run the builds in GitHub experience farless load (image builds are executed in separate builds) soit might be that resource usage is decreased. Another reasonmight be Github Actions stability improvements.Or simply those tests are more stable when run isolation.We might still add failing tests back as soon we see them behavein a flaky way.The remaining quarantined tests that need to be fixed: * test_local_run (often hangs the build) * test_retry_handling_job * test_clear_multiple_external_task_marker * test_should_force_kill_process * test_change_state_for_tis_without_dagrun * test_cli_webserver_backgroundWe also move some of those tests to ""heisentests"" categoryThose testst run fine in isolation but failthe builds when run with all other tests: * TestImpersonation testsWe might find that those heisentest can be fixed but fornow we are going to run them in isolation.Also - since those quarantined tests are failing more oftenthe ""num runs"" to track for those has been decreased to 10to keep track of 10 last runs only.",1
Conditional skipping of provider tag when it exists (#22172),1
[AIRFLOW-5126] Read aws_session_token in extra_config of the aws hook (#6303),1
"Adds option to disable mounting temporary folder in DockerOperator (#16932)* Adds option to disable mounting temporary folder in DockerOperatorThe DockerOperator by default mounts temporary folder to insidethe container in order to allow to store files bigger thandefault size of disk for the container, however this did not workwhen remote Docker engine or Docker-In-Docker solution was used.This worked before the #15843 change, because the /tmp hasbeen ignored, however when we change to ""Mounts"", the ""/tmp""mount fails when using remote docker engine.This PR adds parameter that allows to disable this temporarydirectory mounting (and adds a note that it can be replacedwith mounting existing volumes). Also it prints a warningif the directory cannot be mounted and attempts to re-runsuch failed attempt without mounting the temporarydirectory which brings back backwards-compatible behaviourfor remote engines and docker-in-docker.Fixes: #16803Fixes: #16806",0
Fix pylint issues in airflow/models/dagbag.py (#9666),2
[AIRFLOW-1221] Fix templating bug with DatabricksSubmitRunOperatorIn our implementation ofDatabricksSubmitRunOperator we mistakenlyswitched the order of the tuples returned byenumerate. The bugis fixed in this commit and an additional unittest is added toverify that this bug is fixed.Closes #2308 from andrewmchen/fix-templating-bug,0
[AIRFLOW-6895] AzureFileShareHook: Move DB call out of __init__ (#7519)* [AIRFLOW-6895] AzureFileShareHook: Move DB call out of __init__* Mock client* Fix test,3
[AIRFLOW-2178] Add handling on SLA miss errorsCloses #3173 from d3cay1/airflow2178-master,0
[AIRFLOW-782] Add support for DataFlowPythonOperator.DataFlowPythonOperator allows users to definie GCPdataflow task wherethe pipeline job is specified in Python. Thecorresponding unit testsare also included. To run the tests:nosetests tests.contrib.hooks.gcp_dataflow_hook:DataFlowHookTest andnosetests tests.contrib.operators.dataflow_operator:DataFlowPythonOperatorTest.Closes #2025 from fenglu-g/master,5
"AIRFLOW-181 Fix failing unpacking of hadoop by redownloadingcurl compares timestamps, but if the file is corrupt this canresult in hadoop tars that are never updated. This adds a retrywithout using the cache.",1
"Wait for xcom sidecar container to start before sidecar exec (#25055)According to k8s's docs on pod phase [1], ""Running"" state means ""The Pod has been bound to a node,and all of the containers have been created. At least one container is still running,or is in the process of starting or restarting."".This means there is no guarantee that the xcom sidecar container will have beenrunning by the time the `extract_com` is used by KPO's `execute` even ifwe wait for pod to be a ""Running"" state - this further means if the base containerhas completed before the xcom sidecar container is running, the entireoperator fails because you cannot exec against a non-running container.Fix is to wait for the xcom sidecar container to be in the running statebefore we exec against it, which this commit implements.[1] - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase[2] - https://github.com/schattian/airflow/blob/86171ff43f8977021708cfa241da64f96c4c15e2/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py#L398",1
"Switches to ""cancel-all-duplicates' mode of cancelling. (#12004)The cancel-workflow-runs action in version 4.1 got the capabilityof cancelling not only duplicates of own run (including future,queued duplicates) but also cancelling all duplicates from allrunning worklfows - regardless if they were triggered by my ownPR or some other PRs. This will be even more helpful with handlingthe queues and optimising our builds, because in case ANY ofthe build image workflows starts to run, it will cancel ALLduplicates immediately.",1
Cloud Build assets & system tests migration (AIP-47) (#25895),3
Make James Timmins a code owner for everything permissions related (#15366),1
"Fail tasks in scheduler when executor reports they failed (#15929)When a task fails in executor while still queued in scheduler, the executor reportsthis failure but scheduler doesn't change the task state resulting in the taskbeing queued until the scheduler is restarted. This commit fixes it by ensuringthat when a task is reported to have failed in the executor, the task is failedin scheduler",0
Add more tips about health checks (#14537)* Add more tips about health checks* fixup! Add more tips about health checks* Apply suggestions from code reviewCo-authored-by: Kamil Bregua <kamilbregula@apache.org>,1
"Adds an s3 list prefixes operator (#17145)- Adds an operator to return a list of prefixes from an S3 bucket- Updates `list_prefixes()` unit test to assert on a nested dir with a prefix variable- Removes duplicate calls to `list_keys()` that were in the `test_list_prefixes()` unit test (likely a copy/paste boo boo?)There are two suggestion from [this conversation](https://github.com/apache/airflow/pull/8464) that I have not included here:1. Combine or otherwise simplify `s3_list_keys()` and `s3_list_prefixes()` into one - this makes sense to me but I don't quite know how people tend to use these operators or if there is a valid argument for keeping them separate. 2. Combining all the s3 operators into one file like [gcs.py](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/gcs.py) - this also makes sense to me, but it's not consistent with the other AWS operators. Might be worth opening a new issue to refactor them all if we want to go in this direction?Issue Link: #[8448](https://github.com/apache/airflow/issues/8448)",0
Improve Kubernetes Executor docs (#19339),2
Fixing ParamValidationError when executing load_file in Glue hooks/operators (#16012)* Fixing ParamValidationError when executing load_file in Glue hooks/operatorsCo-authored-by: Rahul Raina <raina_rahul@singaporeair.com.sg>,1
"Improvments to command line tools: test, list_dags, list_tasks",2
Setting autocommit default,1
[AIRFLOW-5562] Skip grant single DAG permissions for Admin role. (#6199)* [AIRFLOW-5562] Skip grant single DAG permissions for Admin role.- Admin role have all permissions so it does not need to be re-authorized.- Too many permissions for role is not good for view and performance.* [AIRFLOW-5562] Fix typo in last change.,4
[AIRFLOW-3109] Bugfix to allow user/op roles to clear task intance via UI by default,1
Adding Authentication to webhdfs sensor  (#25110),1
Add myself as code owner for AIP-42 related stuffs (#25600),1
Merge pull request #3413 from ashb/pr-tool-git-config[AIRFLOW-2238] Switch PR tool to push to Github,5
Doc: Strip unnecessary arguments from MariaDB JIRA URL (#17296)This was included in https://github.com/apache/airflow/pull/17287 but we can strip other args,5
[AIRFLOW-3100][AIRFLOW-3101] Improve docker compose local testing (#3933),3
"[AIRFLOW-4240] State-changing actions should be POST requests (#5044)To make the requests POSTs and to follow the redirect that the backendissue I turned the ""toggle"" buttons in to an actual form, which makesthere much less logic needed to build up the URL - the browser handlesit all for us. The only thing we have to do is set the ""action"" on theURL.For the ""link"" ones (delete,trigger,refresh) I wrote a short`postAsForm` which takes the URL and submits a form. A little bit messy,but it works.",1
Remove redundant None provided as default to dict.get() (#11448),1
Use plain asserts in tests. (#12951),3
Merge remote-tracking branch 'upstream/master' into conf_multiple,5
Stop polling when Webserver doesn't start up in Kube tests (#19598),3
"For now cloud tools are not needed in CI (#9818)Currently there is ""unbound"" variable error printed in CI logsbecause of that.",2
"Secrets backend failover (#16404)Currently Airflow does not check the default secrets backends (env and metastore db) if there is any sort of connection related error to an Alternative Backend, causing related tasks to fail. The change proposed here allows it to fail over to checking the default backends when this happens.Additionally GCP Secrets Manager causes the Airflow Webserver to crash at startup if credentials for the backend cannot be found. This behavior seems to be unique to GCP Secret Manager and this PR addresses that for parity in behavior regarding missing credentials across all backends.closes: #14592",1
Fix typo. 'Depreciation' to 'deprecation'. (#9160),2
fix Build docs failure,0
"Pin versions of ""untrusted"" 3rd-party GitHub Actions (#11319)According to https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/security-hardening-for-github-actions#using-third-party-actionsait's best practice not to use tags in case of untrusted3rd-party actions in order to avoid potential attacks.",1
Add test_remove_unused_code to Quarantined test (#9268)This test is passing locally and on breeze. Not sure of a reason why it is failing on CI,0
Merge pull request #1182 from xavierp/fix-systemctl-scriptsFix systemd scripts,5
Acquire lock on db for the time of migration (#10151)We have a situation when both web server and k8s airflow-initdb canstart migrations at the same time.Alembic in no way supports concurrent migrations.The solution is based on https://github.com/sqlalchemy/alembic/issues/633Change-Id: I5b894c947ec2e56efab622357e160e7c300b7b99Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>Co-authored-by: Cloud Composer Team <no-reply@google.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-5364] Fix missing port numbers for local ci scripts,0
Allow setting port in IMAP Connection (#20440),1
Improve type hinting to provider cloudant (#9825)Co-authored-by: Refael Y <refael@seadata.co.il>,5
Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441)These tests are flaky and fail sometimes,0
[AIRFLOW-2000] Support non-main dataflow job classCloses #2942 from fenglu-g/master,5
Remove redundant parentheses from Python files (#10967),2
[AIRFLOW-550] Make ssl config check empty string safeThis config check on the ssl certificate makes itsafe for empty string. The empty string is providedby default configuration settings and could causethe webserver not starting up.Closes #1824 from alexvanboxel/bugfix/airflow-550,0
[AIRFLOW-XXX] FIX typo in Breeze.rst (#6477),2
Add typescript (#24337),1
Doc: Update description for executor-bound dependencies (#22601)These suggestions were missed in https://github.com/apache/airflow/pull/22573- https://github.com/apache/airflow/pull/22573/files#r837754616- https://github.com/apache/airflow/pull/22573/files#r837755144- https://github.com/apache/airflow/pull/22573/files#r837755355,2
"[AIRFLOW-3034]: Readme updates : Add Slack & Twitter, remove Gitter",4
[AIRFLOW-3370] Fix bug in Elasticsearch task log handler (#5667),0
"Python base image version is retrieved in the right place (#9931)When quick-fixing Python 3.8.4 error #9820 PYTHON_BASE_IMAGE_VERSIONvariable was added but it was initialized too early in Breeze andit took the default version of Python rather than the one chosenby --python switch. This caused the generated requirements(locally by Breeze only) to generate wrong set of requirementsand images built locally for different python versions werebased on default Python version, not the one chosen by --pythonswitch.",1
"[AIRFLOW-4173] Improve SchedulerJob.process_file() (#4993)By avoid processing paused DAGs.The actions we avoid here is mainly the dagbag.get_dag() on paused DAGs.DagBag.get_dag() itself is relatively expensive, so this change bringsconsiderable performance improvement.",1
ldap auth mechanism wasnt cleaning up after itself,4
[AIRFLOW-461]  Support autodetected schemas in BigQuery run_load (#3880),1
[AIRFLOW-1200] Forbid creation of a variable with an empty keyCloses #2299 from skudriashev/airflow-1200,1
visible_on -> execution_date,5
Merge pull request #654 from CloverHealth/fix_uri_conn_typeFix conn_type parsing from uris.,0
TI push/pull,5
"fix lossing duration < 1 secs in tree (#13537)truncat duration to 3dp when duration < 10Update airflow/www/views.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>fix import, retrigger ci",2
"UI: Add queued_by_job_id & external_executor_id Columns to TI View (#13266)While debugging one of issues with Celery, it would have helped if `queued_by_job_id` & `external_executor_id` were available to see in Webserver under Browse TaskInstances instead of running psql.",1
Better docs settings,1
Fix hidden tooltip position (#19261)Only apply a large z-index when the tooltip is supposed to be display.,0
[AIRFLOW-6191] Adjust pytest verbosity in CI and local environment (#6746),3
fix deprecated import path of tests (#14923),3
Add service_account to Google ML Engine operator (#11619),1
Remove legacy hack for TaskInstance without DagRun (#20108),2
Make CI and PROD image builds consistent (#23841)Simple refactoring to make the jobs more consistent.,1
[AIRFLOW-5148] Adding GA and privacy notice to website (#5930)Co-authored-by: kaxil <kaxilnaik@gmail.com>Co-authored-by: mik-laj <kamil.bregula@polidea.com>,1
docker operator - example,1
Adding pool to task_instance list view,1
"[AIRFLOW-6871] optimize tree view for large DAGs (#7492)This change reduces page size by more than 10X andreduces page load time by 3-5X. As a result, thetree view can now load large DAGs that were causing5XX error without the patch.List of optimizations applied to the view handler:* only seralize used task instance attributes to json instead of the  whole ORM object* encode task instance attributes as array instead of dict* encode datetime in unix timestamp instead of iso formmat string* push task instance attribute construction into client side JS* remove redundant task instance attributes* simplify reduce_nodes() logic, remove unnecessary if statements* seralize JSON as string to be used with JSON.parse on the client side  to speed up browser JS parse time* remove spaces in seralized JSON string to reduce payload size",5
Move celery-exclusive feature to CeleryExecutor page (#10242),4
"Add Connection Documentation to more Providers (#15408)* add/update connections docs for ftp, sftp, ssh and snowflake.* fix errors* add to spelling list* fix ftp conn id",0
ExasolHook get_pandas_df does not return pandas dataframe but None (#17850)closes #17135ExasolHook get_pandas_df does not return pandas dataframe but None,5
[AIRFLOW-2832] Lint and resolve inconsistencies in Markdown files (#3670)Clean up the Markdown files and make the formatting consistent,1
Update deferring.rst (#19509)add missing ')' in code example,1
"Correctly restore upstream_task_ids when deserializing Operators (#8775)This test exposed a bug in one of the example dags, that wasn't caughtby #6549. That will be a fixed in a separate issue, but it caused theround-trip tests to fail hereFixes #8720",0
Add slim images to docker-stack docs index (#23601),2
support P12 credentials; compatibility with existing GCS connections,1
use list to allow indexing,1
Merge pull request #126 from mistercrunch/colorAdding light colors to the graph view,1
Fix pause/unpause Dag on dag view (#15865)This PR adds a `pausedUrl` via `<meta>` tag that is readable by js instead of a jinja template. This was causing a bug where a user couldn't pause/unpause a dag from the dag page.,2
Extract dbapi cell serialization into its own method,5
[AIRFLOW-771] Make S3 logs append instead of clobberCloses #2003 fromaoen/ddavydov/make_airflow_logs_append_only,2
"Update the name of static check without pylint in CI.rst (#10909)We changed the name of the static check without pylint in https://github.com/apache/airflow/commit/a1032805bc2ac96a507660a813a3a304cf4a2f8c#diff-e9f950f17198d3d5e3122a44230a09b9, this commit updates it's name in the docs",2
Get rid of pydruid limitation (#9965)Pydruid version 0.5.8 failed on python 3.7 but pydruid 0.5.11fixed it apparently.,0
Fix missing dash in flag for statsd container (#10691)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,0
Add banner_timeout feature to SSH Hook/Operator (#21262)Recently ssh tests in CI started to fail intermittently withError reading SSH protocol banner error. This error is raisedwhen SSH server is slow to start (which might happen forexample when there is not enough entropy to generate keys)This can be mitigated by adding banner_timeout.,1
[AIRFLOW-6728] Change various DAG info methods to POST (#7364)If the number of dags was large and/or the length of the DAG ids were too large this would exceed the maximum possible query string limit.To work around that we have made these endpoints always make POST requests,1
[AIRFLOW-4175] S3Hook load_file should support ACL policy paramete (#7733)- Added acl_policy parameter to all the S3Hook.load_*() and S3Hook.copy_object() function                     - Added unittest to test the response permissions when the policy is passed                     - Updated the docstring of the functionCo-authored-by: retornam <retornam@users.noreply.github.com>,1
closes apache/incubator-airflow#1933 *Closed for inactivity*,5
docs(impersonation): update note so avoid misintrepretation (#17701),5
[AIRFLOW-2030] Fix KeyError:`i` in DbApiHook for insertCloses #2972 from untwal/dbahook/fix_key_error,5
Merge pull request #1082 from jeffwidman/masterCleanup Contributing.md,4
Excludes .git-modules from rat-check (#14759),5
Update to latest pygrep pre-commit hook (#8489),1
[AIRFLOW-6506] do_xcom_push defaulting to True (#7122),5
Fix file permissions in all build commands (#22656)Moves group permission cleanup to build command in new Breeze2.Seems that we should fix group permissions in all build commands.That will shave extra few minutes on most builds where theymodify sources. Also the output from the build will be a bitcleaner by removing come of the console outputs.,4
[AIRFLOW-XXX] Add Aizhamal Nurmamat kyzy to contributors list (#5370),1
Support google-cloud-datacatalog 3.0.0 (#13224),5
"Add an Amazon EMR on EKS provider package (#16766)* Add an Amazon EMR on EKS provider package - Adds an operator, sensor, and hook for running Spark jobs on EMR on EKS as well as docs and an example DAG",2
Fix external_executor_id not being set for manually run jobs (#17207),1
"Cleanup of build-image scripts (#22687)This PR re-arranges tags int GitHub actions to be more explicit inthe pull/build/push steps on which tag of the image is currentlypulled/pushed/built.Previously the actual configuration of those steps was determinedby combining ""top"", ""job"" and ""step"" variables, but that gavelittle insight on what combinations of evn variables was used inthe step eventually. After this change, in all the pull/push/buildsteps you can see what environment is used for that build.Also, since we often build and push and then pull the same image inthe same workflow, the PUSH tag was sometimes used in PULLvariable as input. We change it now by introducing single`IMAGE_TAG_FOR_THE_BUILD` which will be used as PUSH and PULLtag as needed.This one also fixes failure of PROD cache builds which did notpull the right CI image.",0
"Adds new Airbyte provider (#14492)This commit add hook, operators and sensors to interact with Airbyte external service.",1
"Chart: Allow ingress multiple hostnames w/diff secrets (#18542)This allows the ingress to specify multiple hosts with multipledifferent secrets, rather than being contrainted to one secret for allhosts specified.",1
Update `CODEOWNERS` (#20027)I am not looking at the APIs that frequent so removing myself from CODEOWNERS !,4
"[AIRFLOW-2848] Ensure dag_id in metadata ""job"" for LocalTaskJob (#3693)dag_id is missing for all entries in metadatatable ""job"" with job_type ""LocalTaskJob"".This is due to that dag_id was not specifiedwithin class LocalTaskJob.A test is added to check if essentialattributes of LocalTaskJob can be assignedwith proper values without intervention.",1
"extra docker-py update to resolve docker op issues (#15731)Due to changes in the docker api in api version 1.41, the docker pythonclient needs an update to properly handle the filter param imagejsonendpoint.  Without this fix, the `DockerOperator` will not pull the imageunless `force_pull` is set to True.Fixes #13905",0
"Revert ""Add back-compat shim for BranchDateTimeOperator rename (#15849)"" (#15853)This reverts commit 3f0a90429575286f03ba28b0e29da71a3aba8098.Backcompat is not needed as the original feature hasn't been released yet.",4
[AIRFLOW-1319] Fix misleading SparkSubmitOperator and SparkSubmitHook docstring- Reword docstring to reduce ambigiuity in theusage of `--files` option.- See JIRA issuehttps://issues.apache.org/jira/browse/AIRFLOW-1319for more info.Closes #2377 from chrissng/airflow-1319,5
Show warning if '/' is used in a DAG run ID (#23106),1
"[AIRFLOW-3086] Add extras group for google auth to setup.py. (#3917)To clarify installation instructions for the google auth backend, add aninstall group to `setup.py` that installs dependencies google auth via`pip install apache-airflow[google_auth]`.",1
Standardize AWS ECS naming (#20332)* Rename ECS Hook and Operator,1
Updating explicit arg example in TaskFlow API tutorial doc (#18907),2
Added location parameter to BigQueryCheckOperator (#8273),1
[AIRFLOW-4085] FileSensor now takes glob patterns for `filepath` (#5358),2
[AIRFLOW-6238] Filter dags returned by dag_stats (#6803)Add dag_ids parameter to the dag_stats end point and only request thedags on the current page. This is intended to speed up the responsetimes for systems running a large number of DAGS.,2
Add type annotations to providers/vertica (#9936)Co-authored-by: Johan Eklund <jeklund@zynga.com>,1
[AIRFLOW-853] use utf8 encoding for stdout line decodeCloses #2060 from ming-wu/master,1
"Add params dag_id, task_id etc to XCom.serialize_value (#19505)When implementing a custom XCom backend, in order to store XCom objects organized by dag_id, run_id etc, we need to pass those params to `serialize_value`.",2
[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHookExecutes SQL using psycopg2 copy_expert methodNecessary to execute COPY command without access to a superuserCloses #2698 from andyxhadji/AIRFLOW-1726,1
Add enum validation for [webserver]analytics_tool (#24032),5
[AIRFLOW-2309] Fix duration calculation on TaskFailCloses #3208 from johnarnold/duration,0
adding the workerclass (-k) option when starting gunicorn for the webserver,1
[AIRFLOW-6418] Remove SystemTest.skip decorator (#6991),5
Update DAG Serialization docs (#13722)- Updated the figure to show Scheduler uses Serialized DAGs (also added in https://cwiki.apache.org/confluence/display/AIRFLOW/Drawio+Diagrams)- And updated the description,5
Allow custom timetable as a DAG argument (#17414),2
add documentation on kerberos authentication,2
"[AIRFLOW-XXXX] Used fixed periodic auto-labeler from potiuk's repo (#7047)The periodic labeler had a bug that it was not using pagination.In effect the labeler was checking only 30 oldest request andif they all contained proper labels, it was not checking any morePRs.Details are available inhttps://github.com/paulfantom/periodic-labeler/pull/4But until it is merged, we switch to our own fixed version.",0
Helm Chart is using 1.10.12 image by default (#10639),1
"Revert ""Revert ""[AIRFLOW-1955] Do not reference unassigned variable""""This reverts commit fdd7f43fe2aa64bc9001db4f2555c59d568e249a.",5
[AIRFLOW-307] There is no __neq__ python magic method.Closes #1645 from TheSAS/patch-1,5
[AIRFLOW-1933] Fix some typosCloses #2474 from Philippus/patch-1,2
closes apache/incubator-airflow#1787 *PR abandonned by submitter*,5
[AIRFLOW-6220] Remove redundant BigQuery hook tests (#6776),3
"[AIRFLOW-3051] Change CLI to make users ops similar to connectionsThe ability to manipulate users from the  command line is a bit clunky.  Currently 'airflow create_user' and 'airflow delete_user' and 'airflow list_users'.  It seems that these ought to be made more like connections, so that it becomes 'airflow users list ...', 'airflow users delete ...' and 'airflow users create ...'",1
"Add optional features in providers. (#21074)Some features in providers can be optional, depending on thepresence of some libraries. Since Providers Manager triesto import the right classes that are exposed via providers itshould not - in this case - log warning message for thoseoptional features. Previously, all ImportErrors were turned intodebug log but now we only turn them in debug log when creatorof the provider deliberately raisedan AirflowOptionalProviderFeatureException.Instructions on how to raise such exception in the way to keepbackwards compatibility were updated in proider's documentation.Fixes: #20709",2
Organize Sagemaker classes in Amazon provider (#20370)Organize Sagemaker classes in Amazon provider (#20370),1
Support extra_args in S3Hook and GCSToS3Operator (#11001),1
Fix get_context_data doctest import (#14288)This commit fixes the doctest replacing the old import - fromairflow.task.context - to the newer one.,1
Add Helm Chart 1.4.0 in airflow_helmchart_bug_report.yml (#20828),5
Update chain() and cross_downstream() to support XComArgs (#16732)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
"Validate newsfragment types (#25536)We should validate the a type is provided, and that it is a supported type.",1
Fix stop_airflow typos in CONTRIBUTORS_QUICK_START.rst (#18656),2
Add spark_kubernetes system test (#7875)fix reformatfix default connection sort,0
[AIRFLOW-4445] mushroom cloud errors too verbose (#6952)* [AIRFLOW-4445] mushroom cloud errors too verbose,0
"[AIRFLOW-1588] Cast Variable value to stringVariables are considered a ""Key-Value"" pair, and usually the values areconsidered as strings (like when encrypting/decrypting with fernet key).Thus, we need to cast it to string before storing it (when loaded from aJSON).Closes #3037 from diraol/fix1588-json-parsing",5
"Change TaskInstance and TaskReschedule PK from execution_date to run_id (#17719)Since TaskReschedule had an existing FK to TaskInstance we had to movechange both of these at the same time.This puts an explicit FK constraint between TaskInstance and DagRun,meaning that we can remove a lot of ""find TIs without DagRun"" code inthe scheduler too, as that is no longer a possible situation.Since there is now an explicit foreign key between TaskInstance andDagRun, we can remove a lot of the ""cleanup"" code in the scheduler thatwas dealing with this.This change was made as part of AIP-39Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",4
"[AIRFLOW-2063] Add missing docs for GCP- Add missing operator in `code.rst` and`integration.rst`- Fix documentation in DataProc operator- Minor doc fix in GCS operators- Fixed codeblocks & links in docstrings forBigQuery, DataProc, DataFlow, MLEngine, GCS hooks& operatorsCloses #3003 from kaxil/doc_update",5
[AIRFLOW-5330] Add project_id to Datastore hook and operators (#5935),1
fixed typo in confirm script (#8419)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,5
Fix web view rendering errors without a DAG run (#18244)* Make sure task view can be rendered without a ti* Ensure rendered view works without DAG run* Fix table when ti is not available in task view* Tests for task view without DAG run,1
[AIRFLOW-XXX] Add note on ASF licensing (#4909)[ci skip],1
Azure storage 0.37.0 is not installable any more (#8833)2020.05.12 release of the azure-storage is not installable any more(it is deprecated). For now we should switch to latest workingversion,1
Adding header/tooltip for pause toggle column,1
Added template_fields_renderers for MySQL Operator (#16914),1
[AIRFLOW-1544] Add DataFox to companies listCloses #2544 from sudowork/datafox-companies,5
Replace incorrect raising of the constant NotImplemented with raising of the Exception NotImplementedError.,0
"update default key for xcom_pull, and docs/example",2
"Decode Remote Google Logs (#13115)* decode remote google logs before returningThe `Blob.download_as_string` function returns a byte which causethe log result to be displayed in a single line like:b""line1\nline2""instead ofline1line2added an isinstance check to make sure it doesn't break if itreturns string in some case and not others",4
Merge pull request #1116 from criccomini/update-readmeAdd WePay and committer list to README.md,2
"Fix zombie task handling with multiple schedulers (#24906)Each scheduler was looking at all running tasks for zombies, leading tomultiple schedulers handling the zombies. This causes problems withretries (e.g. being marked as FAILED instead of UP_FOR_RETRY) andcallbacks (e.g. `on_failure_callback` being called multiple times).When the second scheduler tries to determine if the task is able to be retried,and it's already in UP_FOR_RETRY (the first scheduler already finished),it sees the ""next"" try_number (as it's no longer running),which then leads it to be FAILED instead.The easy fix is to simply restrict each scheduler to its own TIs, asorphaned running TIs will be adopted anyways.",1
Add role export/import to cli tools (#18916)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
"Update docs/tools for releasing core Airflow (#20211)When building the ""testing status"" issue, don't include things skippedon the changelog or doc-only changes.Also, don't add skipped changelog entries in the changelog.",4
"Improve handling of entry and exit to common Breeze commands (#23395)This PR improves handling of both entry and exit to commonBreeze commands:* at entry all common commands check if rebuild of image is needed* when you exit and there is an error from shell commands, rather  than printing stack trace an error message is printed",0
added TUD to INTHEWILD.md (#18123),1
Wait option for dagrun operator (#12126)* Add wait_for_completion option to dag run operator.* Add wait_for_completion option to dag run operator.* Change code format to pass sanity check.* Simplify the logic to check dag run state.* Move sleep in the beginning of loop and update pydoc.* Change elif to if on checking allowed_statesCo-authored-by: Kaz Ukigai <kukigai@apple.com>,1
"[AIRFLOW-1064] Change default sort to job_id for TaskInstanceModelViewThe TaskInstanceModelView default sort column ison an unindexed column.We shouldn't need an index on start_date, andjob_id is just as logicalof a default sort.Closes #2215 from saguziel/aguziel-fix-ti-page",0
"Allow switching xcom_pickling to JSON/Pickle (#12724)Without this commit, the Webserver throws an error whenenabling xcom_pickling in the airflow_config by setting `enable_xcom_pickling = True`(the default is `False`).Example error:```>           return pickle.loads(result.value)E           _pickle.UnpicklingError: invalid load key, '{'.airflow/models/xcom.py:250: UnpicklingError--------------------------------------------------```",0
Fix building assets on ``breeze start-airflow`` (#15663)Error because `webpack` is not install because `yarn install --frozen-lockfile` is not run:```root@f5fc5cfc9a43:/opt/airflow# cd /opt/airflow/airflow/www/; yarn devyarn run v1.22.5$ NODE_ENV=dev webpack --watch --colors --progress --debug --output-pathinfo --devtool eval-cheap-source-map --mode development/bin/sh: 1: webpack: not founderror Command failed with exit code 127.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.root@f5fc5cfc9a43:/opt/airflow/airflow/www#```This commits adds `yarn install --frozen-lockfile` to the command which fixes it.This was missed in https://github.com/apache/airflow/pull/13313/files,2
[AIRFLOW-717] Add Cloud Storage updated sensorAdd a Cloud Storage sensor that triggers when aobject is createdor updated after a specific date. Allow setting acallback thatdefines the update requirements. The default isexecution_date+ trigger_interval.Closes #1959 fromalexvanboxel/feature/gcp_sensor_updated,5
Merge pull request #153 from mistercrunch/clear_recurseMaking clear recurse through subdags / refactor,4
Merge pull request #464 from jlowin/fix_tree_roundingconsolidate base_date rounding in tree view,5
[AIRFLOW-3426] Bugfix / Correct Python Version Documentation Reference (#4259)- Change all Python 3.4 references to 3.5 as 3.4 to reflect the currently supported version within the CI test suite (2.7 and 3.5),3
Fixing variable scope,0
add oracle  connection link (#15632),2
[AIRFLOW-6656] Fix AIP-21 moving (#7272),4
"Add region to Snowflake URI. (#18650)Without adding the AWS region to the URL, SQLAlchemy engines created byAirflow can't write dataframes to snowflake using pd_writer. This PRfixes this.",0
Update Airflow version in ``README.md`` (#17009)This commit/PR also updates the release date in CHANGELOG.txt,5
"[AIRFLOW-365] Set dag.fileloc explicitly and use for Code viewCode view for subdag has not been working. I donot think we are ablecleanly figure out where the code for the factorymethod lives when weprocess the dags, so we need to save the locationwhen the subdag iscreated.Previously for a subdag, its `fileloc` attributewould be set to thelocation of the parent dag. I think it isappropriate to instead setit to the actual child dag location instead. We donot lose anyinformation this way (we still have the link tothe parent dag thathas its location) and now we can always read thisattribute for thecode view. This should not affect the use of thisfield for refreshingdags, because we always refresh the parent for asubdag.Closes #2043 from dhuang/AIRFLOW-365",2
Expose flower and redis ports in breeze (#11624),5
[AIRFLOW-6931] Fixed migrations to find all dependencies for MSSQL (#9891),0
"use explicit --mount with types of mounts rather than --volume flags (#23982)The --volume flag is an old style of specifying mounts used by docker,the newer and more explicit version is --mount where you have tospecify type, source, destination in the form of key/value pairs.This is more explicit and avoids some guesswork when volumes aremounted (for example seems that on WSL2 volume name might beguessed as path wrongly). The change explicitly specifies whichof the mounts are bind mounts and which are volume mounts.Another nice side effect of this change is that when source ismissing, docker will not automatically create directories with themissing name but it will fail. This is nicer because before itled to creating directories when they were missing (for example.bash_aliases and similar). This allows us to avoid some cleanupsto account for those files being created - instead we simplyskip those mounts if the file/folder does not exist.",2
"[AIRFLOW-3768] Escape search parameter in pagination controls (#4911)The ""minidom"" we were using from lxml didn't cope with the &gt; etcentities (because it is an XML parser, not an HTML parser): rather thanspecial casing each one I have instead swapped out lxml-based parser forBeautifulSoup which 1) handles these, and 2) is pure-python so is easierto install :)",0
Fix cron schedule on cancelling workflow (#10002),1
Fix gcs to sftp system test (#25934)gcs_to_sftp example was missing setup and teardown stages aftermigration to the new design.Change-Id: I68e29f49fb4e1ac69f419c70a9ac46d8b8442e3bCo-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,4
"[AIRFLOW-729] Add Google Cloud Dataproc cluster creation operatorThe operator checks if there is already a clusterrunning with the provided name in the providedproject.If so, the operator finishes successfully.Otherwise, the operator issues a rest API call toinitiatethe cluster creation and waits until the creationis successful before exiting.Closes #1971 frombodschut/feature/dataproc_operator",5
Store grid view selection in url params (#23290)* Add url params for dag_run_id and task_id* Persist other search params* simplify useSelection* delete extra params* remove API change,4
More Chart ipmrovements,2
Merge branch 'master' into bigquery-operator,1
"Fixes image tag readonly failure (#11194)The image builds fine, but produces an unnecessary error message.Bug Introduced in c9a34d2ef9ccf6c18b379bbcb81b9381027eb803",0
teardown tests by clearing dags,2
Updated template_fields_rendereds for PostgresOperator and SimpleHttpOperator (#11555)Co-authored-by: Michal Niemiec/IT/CREDITSAFE <Michal.Niemiec@creditsafe.com>,2
Merge pull request #912 from vansivallab/patch-1Add Clover Health to Airflow users,1
Allows to build production images for 1.10.2 and 1.10.1 Airflow (#10983)Airflow below 1.10.2 required SLUGIFY_USES_TEXT_UNIDECODE envvariable to be set to yes.Our production Dockerfile and Breeze supports building imagesfor any version of airflow >= 1.10.1 but it failed on1.10.2 and 1.10.1 because this variable was not set.You can now set the variable when building image manuallyand Breeze does it automatically if image is 1.10.1 or 1.10.2Fixes #10974,0
[AIRFLOW-2106] SalesForce hook sandbox optionCannot pass sandbox argument to sales_force hook preventing sandbox connection.Closes #3111 from feng-tao/airflow-2106,5
[AIRFLOW-5750] Licence check is done also for non-executable .sh (#6425),2
[AIRFLOW-6362] Fix typehint for CommandType (#6906),0
[AIRFLOW-4931] Add KMS Encryption Configuration to BigQuery Hook and Operators (#5567)Add KMS to BigQuery,1
"[AIRFLOW-995] Remove reference to actual Airflow issueRemove example reference to AIRFLOW-[one] becauseit confuses merge tools. In addition, simplifythe checkboxes because Github displays how manyof them have been checked off.Closes #2163 from jlowin/pr-template-2",1
"Add ""no-issue-needed"" rule directly in CONTRIBUTING.rst (#23802)The rule was not really explained directly where you'd expect it,it was hidden deeply in ""triage"" process where many contributorswould not even get to.This PR adds appropriate explanation and also explains thatdiscussions is the preferred way to discuss things in Airflowrather than issues.",0
"Add DAGS_FOLDER to sys.path earlier, to allow plugins to use it",1
"Fixes ""--force-clean-images"" flag in Breeze (#12156)The flag was broken - bad cache parameter value was passed.This PR fixes it.",0
Fix extra tests,3
Merge pull request #117 from airbnb/http_operator_sensorHttp operator sensor,1
[AIRFLOW-5012] Add typehints for gcp_*_hook.py (#5634),1
"Use KubernetesHook to create api client in KubernetesPodOperator (#20578)Add support for k8s hook in KPO; use it always (even when no conn id); continue to consider the core k8s settings that KPO already takes into account but emit deprecation warning about them.KPO historically takes into account a few settings from core airflow cfg (e.g. verify ssl, tcp keepalive, context, config file, and in_cluster). So to use the hook to generate the client, somehow the hook has to take these settings into account. But we don't want the hook to consider these settings in general.  So we read them in KPO and if necessary patch the hook and warn.",1
[AIRFLOW-4883] Bug-fix for Kill hung file process managers  (#5639)Previous PR (#5605) was missing some code after a rebase. This adds the codeand adds unit tests,3
Include all example dags in backfill unit test,3
Add endpoints for task instances (#9597),1
Remove deprecated template_fields from GoogleDriveToGCSOperator (#19991)This prevented `op.dry_run()` from working.,1
[Bash] minimal amount of isolation in a self cleaning temp directory,4
Standardize Amazon SES naming (#20367),5
Bring in more resolution to hivestats,5
Chart: Update the default Airflow version to ``2.1.4`` (#18354)Since 2.1.4 is out we should use that as the default Airflow version.,1
Add INT24 (MEDIUMINT) support to MySQL to Google cloud storage operator,1
"Fix doc link permission name (#14972)Converts the docs_link permission resource name to Documentation.This is an extension of #14946, which standardized default FAB permissions.",2
Fixing typos in GlacierToGCSOperator documentation (#16899),2
Moving the fernet msg some place else,4
[AIRFLOW-734] Fix SMTP auth regression when not using user/passCloses #1974 from criccomini/AIRFLOW-734,5
Update qubole_hook to not remove pool as an arg for qubole_operator (#10820),1
Inherit `AirflowNotFound` from `connextion.NotFound` for better readability (#18523),1
"Remove Unnecessary comprehension in 'any' builtin function (#11188)The inbuilt functions `any()` support short-circuiting (evaluation stops as soon as the overall return value of the function is known), but this behavior is lost if you use comprehension. This affects performance.",1
Use kerberos_service_name = 'hive' as standard instead of 'impala'.,1
Fix LDAP error messages when login fails.,0
"Fix mini scheduler not respecting ``wait_for_downstream`` dep (#18338)When ``wait_for_downstream`` is set on a task, mini scheduler doesn't respect itand goes ahead to schedule unrunnable task instances.This PR fixes it by including all direct downstream tasks so that ``wait_for_downstream`` check works correctly.Resolves https://github.com/apache/airflow/pull/18310#discussion_r711101901Closes: #18229",0
import warnings,2
"Remove  redundant ``numpy`` dependency (#17594)Missed removing ``numpy`` from `setup.cfg` in https://github.com/apache/airflow/pull/17575. It was only added in setup.cfg in https://github.com/apache/airflow/pull/15209/files#diff-380c6a8ebbbce17d55d50ef17d3cf906numpy already has `python_requires` metadata: https://github.com/numpy/numpy/blob/v1.20.3/setup.py#L473so we don't need to set `numpy<1.20;python_version<""3.7""`",1
Change log level from debug to info when spawning new gunicorn workers (#13780),1
[AIRFLOW-4062] Improve docs on install extra package commands (#4897)Some command for installing extra packages are`pip install apache-airflow[devel]` we shouldclear install extra package command to`pip install 'apache-airflow[devel]'`[ci skip],2
[AIRFLOW-2410][AIRFLOW-75] Set the timezone in the RBAC Web UISqlAlchemy does not know how to handle thetimestamp since it isnttimezone awareCloses #3303 from Fokko/AIRFLOW-2410,0
Fix mypy errors in aws/sensors (#20402),0
[AIRFLOW-3638] Add tests for PrestoToMySqlTransfer (#4449)- reformatting[AIRFLOW-3638] Add tests for PrestoToMySqlTransfer- add missing license header,1
"Small improvements for Airflow UI (#18715)I slightly improved some small UI elements that were a bit off. Changes:- Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly.- Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word)- Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change)- Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed- Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit numbers. I think we can even go with 10, but then 4-digit numbers slightly overlay the circle (who have 4-digit runs though?).Here I highlighted with red color the old font size and misaligned icons inside button. The green highlights the elements after changes.![image](https://user-images.githubusercontent.com/7412964/135896512-7b44b5d0-9cda-42d5-a8ea-3151c0dde2a1.png)If there are any other places that I missed or there is something that shouldn't be changed or maybe the change should be reflected in some places that I missed - let me know!",4
[AIRFLOW-XXX] Add `BigQueryGetDataOperator` to Integration Docs (#4063),2
Known events,5
"Fix moving of dangling TaskInstance rows for SQL Server (#19425)SQL server uses a different syntax for creating a table from a select tothe other DBs we support.And to make the ""where_query"" reusable across all DBs (SQL Serverdoesn't support `WHERE (col1,col2) IN ...`) the delete has beenre-written too.",4
[AIRFLOW-1590] fix unused module and variableCloses #2652 from ProstoMaxim/master,1
[AIRFLOW-3332] Add method to allow inserting rows into BQ table (#4179),1
Merge branch 'master' into bigquery-hook,1
"Add scripts that provide good links to example dags (#24348)The documentation generated used ""main"" in the URL of theexample DAGs.The generation of the links have been fixed in the #24307, but thisPR adds a tool that has been used to fix existing links in generateddocumentation resulting in https://github.com/apache/airflow-site/pull/610Fixes: #24331",0
[AIRFLOW-6332] Extract logging options to new section (#6887)* [AIRFLOW-6332] Extract logging options to new section,1
Fixing bug which restricted the visibility of ImportErrors (#17924),2
Fix EMR serverless system test (#25969),3
Adjust trimmed_pod_id and replace '.' with '-' (#19036)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Make container creation configurable when uploading files via WasbHook (#20510),1
[AIRFLOW-2363] Fix return type bug in TaskHandlerCloses #3259 fromyrqls21/kevin_yang_fix_s3_logging,2
"[AIRFLOW-6932] Add restart-environment command to Breeze (#7557)When you switch between versions of Aiflow installed, you want to delete thedatabase so that the scripts for resetdb work",1
"[AIRFLOW-1879] Handle ti log entirely within tiPreviously logging was setup outside aTaskInstance,this puts everything inside. Also propery closesthe logging.Closes #2837 from bolkedebruin/AIRFLOW-1879",2
"[AIRFLOW-2429] Fix dag, example_dags, executors flake8 errorCloses #3398 from feng-tao/flake8_p3",0
[AIRFLOW-1772] Add support for cron expression in GoogleCloudStorageObjectUpdatedSensor (#5730),5
Simply imports in dev github script (#21702),2
Chart: Allow podTemplate to be templated (#17560),1
Fix flaky test_external_task_marker_cyclic_deep test (#18802)The test failed very rarely with this error:``` >       assert task_instance.state == state  E       AssertionError: assert None == <TaskInstanceState.SUCCESS: 'success'>  E        +  where None = <TaskInstance: dag_3.task_b_3 manual__2015-01-01T00:00:00+00:00 [None]>.state```The change makes sure that the ti instance is merged with thelatest DB changes when the assert runs.,1
[AIRFLOW-626][AIRFLOW-1] HTML Content does not show up when sending email with attachmentCloses #1880 from illop/send_email_mimetype,5
[AIRFLOW-3581] Fix next_ds/prev_ds semantics for manual runs (#4385),1
[AIRFLOW-3980] Unify logger (#4804),2
[AIRFLOW-8057] [AIP-31]  Add @task decorator (#8962)Closes #8057. Closes #8056.,1
Cleanup Astronomer contributors list in README (#10520),4
Implement expand_kwargs() against a literal list (#25925),2
"[AIRFLOW-877] Remove .sql template extension from GCS download operatorPrior to this patch, if you use a templated file with a .sql extension,and it's templated, you'd receive an exception because Airflow would tryand load the file (that hasn't yet been downloaded) to template it. Thispatch fixes that.Closes #2083 fromsarfarazsoomro/sas/fix_gcs_download_op",0
"Improving PrestoHook to use REST, DatabaseConnection model",5
add system test for azure local to adls operator (#13190),1
[AIRFLOW-XXX] Improve linking to classes (#4655),2
Nightly tag was not pushed on scheduled run (#9924),1
Change function call in render templates,1
[AIRFLOW-5446] Rewrite Google KMS Hook to Google Cloud Python (#6065),1
Add docs about security on GCP (#12187),2
Pprint default args and wrap (#14345),5
Fix task instance url in webserver utils (#18418)execution_date was omitted when generating the task instance url.This PR fixes it,0
"[AIRFLOW-7106] Cloud data fusion integration - Allow to pass args to start pipeline (#7849)* Add the possibility of passing args to data fusion start pipeline* [AIRFLOW-7106] Cloud data fusion integration - Allow to pass args to start pipeline* [AIRFLOW-7106] modified arguments type from string to dict, fixed tests* [AIRFLOW-7106] reverted wrong comment on tests* Update airflow/providers/google/cloud/hooks/datafusion.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* [AIRFLOW-7106] fixed static checks* reverting conftest.py* removed trailing whitespaces* added hook test for start pipeline with parameters* fixed test on google data fusion hook* fixed all pre-commit testsCo-authored-by: Davide Malagoli <malagoli@malagoli-macbookpro.roam.corp.google.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>",3
"Add specific warning when Task asks for more slots than pool defined with (#20178)In cases where task asks for more pool slots than the total number it has the scheduler generates the following warning:`[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be an action from the user:1.  User to increase the total number of slots in the Pool2. User need to change the task code to requests less slots.This PR add specific log notice for this case.",2
"Refactor GlueJobHook get_or_create_glue_job method. (#24215)When invoked, create_job takes into account the provided 'Command' argument instead of having it hardcoded.",1
[AIRFLOW-XXX] Add vipul007ravi as user (#6181),1
Databricks hook - retry on HTTP Status 429 as well (#21852)* Databricks hook - retry on HTTP Status 429 as wellthis fixes #21559* Reimplement retries using tenacityit's now uses exponential backoff by default,1
[AIRFLOW-4192] Remove tables from the task context variables (#5723),4
#9803 fix bug in copy operation without wildcard  (#13919),0
Remove redundant ``dataclass`` dependency (#20879)We had to install `dataclasses` because it was not available in Python <3.7This is no longer required as `main` is Python 3.7 only.,1
add entry in release readme to update milestone in Issues (#20890),0
"Set template_fields on RDS operators (#26005)Related: #24099 #25952### SummaryThis PR sets the `template_fields` property on `RdsCreateDbInstanceOperator` and `RdsDeleteDbInstanceOperator` to allow users to programmatically change the database id, instance size, allocated storage, etc..It also replaces use of `@task` decorated functions with their appropriate operator in system tests.Co-authored-by: D. Ferruzzi <ferruzzi@amazon.com>",3
"Add GCS timespan transform operator (#13996)This change adds a new GCS transform operator. It is based on the existing transform operator with the addition that it will transform multiple files that match the prefix and that were updated within a time-span. The time-span is implicitly defined: it is the time between the current execution timestamp of the DAG instance (time-span start) and the next execution timestamp of the DAG (time-span end).The use-case is some entity generates files at irregular intervals and an undefined number. The operator will pick up all files that were updated since it executed last. Typically the transform script will iterate over the files, open them, extract some information, collate them into one or more files and upload them to GCS. These result files can then be loaded into BigQuery or processed further or served via a webserver.",2
Added select_query to the templated fields in RedshiftToS3Operator (#16767)Co-authored-by: Weiping He <weiping.he@cirium.com>,1
"[AIRFLOW-2855] Check Cron Expression Validity in DagBag.process_file() (#3698)A DAG can be imported as a .py script properly,but the Cron expression inside as ""schedule_interval"" may beinvalid, like ""0 100 * * *"".This commit helps check the validity of Cron expression in DAGfiles (.py) and packaged DAG files (.zip), and help showexception messages in web UI by add these exceptions intometadata ""import_error"".",2
Get boto3.session.Session by appropriate method (#25569),1
"[AIRFLOW-3647] Add archives config option to SparkSubmitOperator (#4467)To enable to spark behavior of transporting and extracting an archiveon job launch,  making the _contents_ of the archive available to thedriver as well as the workers (not just the jar or archive as a zipfile) - this configuration attribute is necessary.This is required if you have no ability to modify the Python env onthe worker / driver nodes, but you wish to use versions, modules, orfeatures not installed.We transport a full Python 3.5 environment to our CDH cluster usingthis option and the alias ""#PYTHON"" paired an additional configurationto spark to use it:    --archives ""hdfs:///user/myuser/my_python_env.zip#PYTHON""    --conf ""spark.yarn.appMasterEnv.PYSPARK_PYTHON=./PYTHON/python35/bin/python3""",5
"[Airflow-2202] Add filter support in HiveMetastoreHook().max_partition()Adding back support for filter in max_partition()which could be used by some valid use cases. Itwill work for tables with multiple partitions,which is the behavior before (tho the doc statedit only works for single partitioned table). Thischange also kept the behavior when trying to getmax partition on a sub-partitioned table withoutsupplying filter--it will return the max partitionvalue of the partition key even it is not unique.Some extra checks are added to provide moremeaningful exception messages.Closes #3117 from yrqls21/kevin_yang_add_filter",1
[AIRFLOW-6756] Drop also deprecated tables in reset (#7381),1
Merge pull request #79 from woodlee/taskinstance-404sResolve 404s when trying to click through to the task instances view,1
"Add amazon glacier to GCS transfer operator (#10947)Add Amazon Glacier to GCS transfer operator, Glacier job operator and sensor.",1
Chart: configurable number of retention days for log groomers (#17764),2
[AIRFLOW-3375] Support returning multiple tasks with BranchPythonOperator (#4215),1
[AIRFLOW-XXX] Add missing class references to docs (#4644),2
Fix bug in mark TI success api (#16524)Mistakenly checking for the wrong args in TI success API. Introduced in PR https://github.com/apache/airflow/pull/16233.,0
"[AIRFLOW-1235] Fix webserver's odd behaviourIn some cases, the gunicorn master shuts downbut the webserver monitor process doesn't.This PR add timeout functionality to shutdownall related processes in such cases.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1235### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:In some cases, the gunicorn master shuts downbut the webserver monitor process doesn't.This PR add timeout functionality to shutdownall related processes in such cases.### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:tests.core:CliTests.test_cli_webserver_shutdown_when_gunicorn_master_is_killed### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2330 from sekikn/AIRFLOW-1235",1
Fix downgrade for a DB Migration (#19390)The downgrade was not working because of the issues fixed in this PR,0
Remove useless statement in task_group_to_grid (#25654),1
Breeze doc tmux instruction (#20006)* breeze setup-autocomplete zshrc reload* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* change assignment operator to comparison* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* added a new word into spelling exception* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* added a new word into spelling exception* modified the instruction with link to pyenv README* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* modified the instruction with link to pyenv README* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* pyenv issue fix -macOS Big Sur* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* modified the instruction with link to pyenv README* change in tmux session related documentation,2
[AIRFLOW-5072] gcs_hook should download once (#5685)When a user supplied a filename the expected behaviour is that airflowdownloads the file and does not return it's content as a string.,2
Fix bigquery type error when export format is parquet (#16027),0
"[AIRFLOW-1675] Fix docstrings for API docsSome docstrings were missing spaces, causing themto render strangelyin documentation. This corrects the issue byadding in the spaces.Closes #2667 from cjonesy/master",1
[AIRFLOW-5193] Move GCP Cloud Build to core (#5798)This commit moves GCP Cloud Build from contrib to core.For more information check AIP-21.,5
Hide some task instance attributes (#23338),2
Add ability to specify pod_template_file in executor_config (#11784)* Add pod_template_override to executor_configUsers will be able to override the base pod_template_file on a per-taskbasis.* change docstring* fix doc* fix static checks* add description,1
Configurable logging of XCOM value in PythonOperator (#19378),1
Switch to new selective-checks in label-when-reviewed workflow (#24651)When #24610 was implemented I missed the label-when-reviewed workflow,1
Cleaned up formatting for Upgrading doc (#12977)Cleaned up some of the formatting in the document and also fixed a coupleof spelling errors.,0
[AIRFLOW-3268] Better handling of extras field in MySQL connection (#4113),1
Replace dictionary creation with dictionary literal (#13474)Instead of:```ctx = {}ctx['filename'] = filename```we can use:```ctx = {'filename': filename}```,2
[AIRFLOW-3578] Fix Type Error for BigQueryOperator (#4384)* Fix Type Error for BigQueryOperator and support the unicode object.* Add tests,3
"Add map_index to RenderedTaskInstanceFields (#22004)In order to do this ""properly"" we have also migrated it from usingexecution_date to run_id in its PK",1
[AIRFLOW-1517] Created more accurate failures for kube cluster issues,0
[AIRFLOW-XXXX] Add pre-commit check for utf-8 file encoding (#7347)Note: From Python 3.x onwards the explicit utf-8 header is no longer required. It is utf-8 encoded by default.,1
fixes query status polling logic (#21423),2
"When `breeze stop` is called all integrations are enabled (#14825)Sometimes when an integration got broken, it could be brokenpermanently due to left-over volume. This was because whenbreeze stop was called the integration were not enabled andsome volumes were not deleted.As result, breeze sometimes could not start with integrations, due tomisterious 'unhealthy' condition of one of the integrations.This change enables all integrations automatically whenbreeze stop is called so that all volumes are removed.",4
"[AIRFLOW-360] Launch custom images to Airflow CI tests (#4416)To help move away from Minikube, we need to remove the dependency ona local docker registry and move towards a solution that can be usedin any kubernetes cluster. Custom image names allow users to usesystems like docker, artifactory and gcr",2
[AIRFLOW-577] Output BigQuery job for improved debuggingCloses #1838 from waltherg/fix/bq_error_message,0
"Fix handling some None parameters in kubernetes 23 libs. (#21905)Kubernetes 23.* is more picky when it comes to values passed toPod Generator - it requires:* imagePullPolicy* dnsPolicy* restartPolicyto be not None.We are fixing it in the way, that we simply skip setting thoseif they are None.",1
Add type hinting for discord provider (#9773),1
Add back-compat modules from 1.10.10 for SecretsBackends (#8413),1
"Revert ""[AIRFLOW-1613] Handle binary field in MySqlToGoogleCloudStorageOperator""Reverting due to improper handling of binary description_flag.This reverts commit d578b292e96d5fdd87b5168508005cd73edc4f96.",4
Reverting production issues from 876 and undead,0
Make scripts/ci/libraries Google Shell Guide Compliant (#15973)* Make scripts/ci/libraries Google Shell Guide CompliantPart of #10576,1
[AIRFLOW-5368] Display DAG from the CLI,2
Pickle dag exception string fix (#22760),0
"Adding max_partition macro, fixing bugs",0
Add tool to automaticaly update status of AIP-47 issues. (#23745),0
fix: change disable_verify_ssl behaviour (#25023)The problem is that verify_ssl is overwritten by theconfiguration from the kube_config or load_incluster_config file.,2
[AIRFLOW-5896] Move email stuff from tests/core.py (#6545)* [AIRFLOW-5896] Move email stuff from tests/ccore.py,3
Update Production Guide for Helm Chart docs (#23836)Explain that db initialization is not necessary if using the helm chart.,2
Fix md formatting of UPDATING.md (#21347),5
Version 0.2,5
"Fix Kerberos network creation on older docker-compose (#14070)`attachable` is only a property of compose version 3.1 files, but we areon 2.2 still.This was failing on self-hosted runners with an error`networks.example.com value Additional properties are not allowed('attachable' was unexpected)`",1
[AIRFLOW-XXX] Group references in one section (#5776),5
Add drain option when canceling Dataflow pipelines (#11374)* Add drain option when cancel Dataflow pipelines* fixup! Add drain option when cancel Dataflow pipelines* fixup! fixup! Add drain option when cancel Dataflow pipelines* fixup! fixup! fixup! Add drain option when cancel Dataflow pipelines,5
[AIRFLOW-6596] Enforce description should not be empty (#7211),1
Allowing to specify pool in CLI,1
[AIRFLOW-6247] Fix sort order in Alembic Migration template (#6809)* [AIRFLOW-6247] Fix sort order in Alembic Migration template,0
[AIRFLOW-3636] Fix a test introduced in #4425 (#4446),3
Adding bulk load option for HiveToMySqlTransfer,1
Fix mypy errors for google.cloud_build (#20234)Part of #19891,0
upgrade celery 5.2.3 (#19703),5
[AIRFLOW-1333] Enable copy function for Google Cloud Storage HookCloses #2385 from yk5/gcs_hook_copy,1
Bump cattrs version (#25689)Co-authored-by: Radek Tomej <radek.tomsej@almamedia.com>,5
"Adding documentation explaining ""strange"" URI required when using AWS (#13355)",1
Fix TaskNotFound in log endpoint (#13872),2
Update to latest isort & pre-commit-hooks (#11813),1
Fix logo issues due to generic scripting selector use (#11028)Resolves #11025,0
"Chart: Adds labels to Kubernetes worker pods (#16203)We want to set labels on the KubernetesExecutor worker pods as well asone would expect those pods to show up when, say, filtering running podsby release name.",1
"Check bag DAG schedule_interval match tiemtable (#23113)This guards against the DAG's timetable or schedule_interval from beingchanged after it's created. Validation is done by creating a timetableand check its summary matches schedule_interval. The logic is notbullet-proof, especially if a custom timetable does not provide a usefulsummary. But this is the best we can do.",1
[AIRFLOW-2634][AIRFLOW-2534] Remove dependency for impylaCloses #3514 from sekikn/AIRFLOW-2634,4
Add airflow info command (#8290),5
BugFix: Fix remote log in azure storage blob displays in one line (#14313)* Fix wasb_task_handler.py for reading remote log on blog with one line* Fix wasb_task_handler.py for reading remote log on blog with one line* Fix wasb_task_handler.py for reading remote log on blog with one line* modify wasb readall() to content_as_text,2
Add dataproc metastore operators (#18945),1
Fix the default value for store_dag_code (#9554)related to #8255 (fixes the issue mentioned with `store_dag_code` but does not address Config interpolation)The default value of `store_dag_code` should be same as `store_serialized_dags` setting.  But if the value is set it should use that value,1
Add __init__ method to Variable class (#9470),5
"[AIRFLOW-2735] Use equality, not identity, check for detecting AWS Batch failures[]Closes #3589 from craigforster/master",0
[AIRFLOW-1301] Add New Relic to list of companiesCloses #2359 from marcweil/patch-1,1
Make hive macros py3 compatible (#8598)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,1
Add incubating specifier to version,1
"clarify breeze initialize virtualenv instructions (#9319)* you need to activate virtualenv, not enter breeze, before running the command",1
Update example DAGs (#21372),2
Fixing the unit tests,3
"Reset warnings.showwarning on interpreter shutdown (#22677)Since our custom showwarning hook lazy-imports Rich, it may not be ableto execute correctly if a warning is emitted after the interpreter hasalready cleaned up the import system. This adds an atexit hook torestore the original showwarning hook, so that warnings emitted afterthe clean-up phase are logged plainly without using additional modules.",1
[AIRFLOW-3009] Import Hashable from collection.abc to fix Python 3.7 deprecation warning (#3849),2
[AIRFLOW-6801] Make use of ImportError.timestamp (#7425),2
[AIRFLOW-6312] Unpin marshmallow-sqlalchemy for Py>3.5 (#6861) (#6866),1
[AIRFLOW-3732] Fix issue when trying to edit connection in RBAC UI (#4559),2
Rename example bucket names to use INVALID BUCKET NAME by default (#15651),1
[AIRFLOW-3129] Backfill mysql hook unit tests. (#3970),3
"Add `airflow_kpo_in_cluster` label to KPO pods (#24658)This allows one to determine if the pod was created with in_cluster configor not, both on the k8s side and in pod_mutation_hooks.",1
[AIRFLOW-5661] Fix create_cluster method of GKEClusterHook (#6339),1
Adding dagrun timeout parameter to DAG,2
"Tree view: Highlight dag run on hover (#21476)* vertical highlighting when hover on group* full horizontal/vertical crosshairs* fix ref, js only classname, linting* calculate vertical bar height* improve vertical highlight calculation* update package.json",5
Update connection object to ``cached_property`` in ``DatabricksHook`` (#20526),5
Updating TODO,2
User-friendly error messages when the configuration is incorrect (#8463)* Clearer error messages when the configuration is incorrect,5
"Remove cargo-culted local in-page ToCs (#18668)This pattern of having a local table of contents seems to have beencargo-culted to all of the providers, and other pages, but our Sphinxtheme already has the lage contents on the right hand side -- this isjust leading to duplication.(Not to mention that for the majority of cases the page is so short thatyou can easily see everytine at once anyway.)I have left it in a few places -- for instance the mainconfigurations-ref.rst I have kept it.",5
[Doc] Separate supported Postgres versions with comma (#7892),1
"[AIRFLOW-1893][AIRFLOW-1901] Propagate PYTHONPATH when using impersonationWhen using impersonation via `run_as_user`, thePYTHONPATH environmentvariable is not propagated hence there may beissues when depending onspecific custom packages used in DAGs.This PR propagates only the PYTHONPATH in theprocess creating thesub-process with impersonation, if any.Tested in staging environment; impersonation testsin airflow are not very portable and fixing themwould take additional work, leaving as TODO andtracking with jira ticket: https://issues.apache.org/jira/browse/AIRFLOW-1901.Closes #2860 from edgarRd/erod-pythonpath_run_as_user",1
"Revert ""Clean up the pre-commit config (#15703)"" (#15707)This reverts commit 803850ad22e5995c7fffc5389a29e48d9ca0bb9a.",4
[AIRFLOW-5083] Move image building to before_install for licence (#5695),4
Add brief examples of integration test dags you might want (#22009),2
Clean up f-strings in logging calls (#23597),2
Disallow any dag tags longer than 100 char (#25196),2
[AIRFLOW-XXXX] Update .mailmap with some missing authors (#7290),5
Fix typo: `parsed_results` -> `parse_results` (#19588),2
Adds hive as extra in pyhive (#9075)Seems that apache hive needs to install [hive] extra of pyhivein order to be usable \_()_/.Fixes: #8933,0
"Chart: Fix applying labels on Triggerer (#18299)Currently it errors with:```{""result"":{""message"":""YAML parse error on airflow/charts/airflow/templates/triggerer/triggerer-deployment.yaml: error converting YAML to JSON: yaml: line 30: mapping values are not allowed in this context""},""deployment"":{}}```This PR fixes that and adds unit tests around it.",3
[AIRFLOW-XXX] Replace airflow with apache-airflow (#4246),5
This seems to address logging #hack,2
[AIRFLOW-4026] Add filter by DAG tags (#6489),2
"Correctly link to Dag parsing context in docs (#25722)* Render docs for new dag-parsing contextThere were two problems: 1) the tag was incorrect (missing leadingcolon) so it didn't render as a link, and 2) The docs for the new modulewere being excluded.",1
[AIRFLOW-1694] Stop using itertools.izipItertools.zip does not exist in Python 3.Closes #2674 from yati-sagade/fix-py3-zip,0
Fix formatting errors introduced in #11720 (#11733),0
"Optionally not follow logs in KPO pod_manager (#22412)When writing an async KPO, you want to be able to read logs up to the current moment and exit (i.e. and not follow the logs).  Additionally you want to be able to resume from a particular moment in time.  That's what this PR enables.",0
Don't crash attempting to mask secrets in dict with non-string keys (#16601),5
Fix broken backtick usage in Timezone docs (#11575),2
"Add 'queued' state to DagRun (#16401)This change adds queued state to DagRun. Newly created DagRunsstart in the queued state, are then moved to the running state aftersatisfying the DAG's max_active_runs. If the Dag doesn't havemax_active_runs, the DagRuns are moved to running state immediatelyClearing a DagRun sets the state to queued stateCloses: #9975, #16366",1
Re-enables MyPy in non-failure mode (#19890)This PR enables mypy back as pre-commit for local changes afterthe #19317 switched to Python 3.7 but also it separates outmypy to a separate non-failing step in CI.In the CI we will be able to see remaining mypy errors.This will allow us to gradually fix all the mypy errors and enablemypy back when we got all the problems fixed.,0
Merge pull request #1053 from geeknam/readme_usersAdd Kogan.com to the list of users,1
Action to redirect where you come from,5
Update INTHEWILD.md (#20452)Add Narrativa as a Company that uses Airflow. Thank you so much for such a great product guys!,1
[AIRFLOW-112] no-op README change to close this jira's PR,4
Databricks: fix provider name in the User-Agent string (#25873)`databricks-aiflow` -> `databricks-airflow` (:facepalm:),5
Fix Grammar in PIP warning (#13380)`might leads to errors` -> `might lead to errors`,0
Merge pull request #1257 from airbnb/ddavydov/configurable_gunicorn_timeout[webserver] Make webserver worker timeout configurable,5
Add tenant specification to dbt Cloud conn doc (#24907),2
"Use `Lax` for `cookie_samesite` when empty string is passed (#14183)closes https://github.com/apache/airflow/issues/13971The value of `[webserver] cookie_samesite` was changed to `Lax` in >=2.0from `''` (empty string) in 1.10.x.This causes the following error for users migrating from 1.10.x to 2.0if the old airflow.cfg already exists.```Traceback (most recent call last):File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2447, in wsgi_appresponse = self.full_dispatch_request()File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1953, in full_dispatch_requestreturn self.finalize_request(rv)File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1970, in finalize_requestresponse = self.process_response(response)File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2269, in process_responseself.session_interface.save_session(self, ctx.session, response)File ""/usr/local/lib/python3.9/site-packages/flask/sessions.py"", line 379, in save_sessionresponse.set_cookie(File ""/usr/local/lib/python3.9/site-packages/werkzeug/wrappers/base_response.py"", line 468, in set_cookiedump_cookie(File ""/usr/local/lib/python3.9/site-packages/werkzeug/http.py"", line 1217, in dump_cookieraise ValueError(""SameSite must be 'Strict', 'Lax', or 'None'."")ValueError: SameSite must be 'Strict', 'Lax', or 'None'.**```This commit takes care of it by using `Lax` when the value is empty string (``)",1
Make function purpose clearer in example_kubernetes_executor example dag (#13216),2
Merge pull request #3189 from tedmiston/updating-doc-grammar-fix,5
Small updates to provider preparation docs. (#11240),2
Add Parquet data type to BaseSQLToGCSOperator (#13359),1
"Remove chmod +x for installation script for docker build. (#13772)We've introduced chmod a+x for installation scripts in Dockerfiles.but this turned out to be a bad idea. This was to accomodatebuilding on Azure Deveops which has filesystem that does notkeep executable bit. But the side-effect of it that thelayer of the script is invalidated when the permission is changedto +x on linux. The problem is that the script has locally (oncheckout) different permissions depending on umask setting.Therefore changing permissions for the image to +a is not best.Instead we are running the scripts with bash directly, which doesnot require changing of executable bit.",4
Update Helm Chart release guide (#17173)This PR/commit update the Helm chart release guide with some minor corrections:- Fixes sed command- Adds step to remove old artifacts from dev repo,4
"Add IPV6 form of the address in cassandra status check (#23537)This PR fixes problem introduced in 3.0.26 of cassandra image whichadds square brackets around IP address regardless of its type.The problem was workarounded by pinning cassandra to 3.0.25 inthe ##23522 as a quick fix, but this one introducec permanent,future-proof solution.Based on discussion in https://issues.apache.org/jira/browse/CASSANDRA-17612Fixes: #23523",0
Fixing docs automodule command,2
Fixing failing quarantined test cases in test_task_command (#19864),3
Fix command to filter package provider when building docs (#12984),2
"Fallback Provider's doc URL to ""Documentation"" meta-data (#23012)When Airflow displays provider's Doc URLs it builds theURL to documentation for community providers but for third partyproviders it was wrong.With this change:* if provider already has Documentation standard Project-URL  metadata, Airflow will use it* if provider is an ""apache-airflow"" one, it will dynamically  build the URL from provider info* if neither of two is available it will direct user to the  place in documentation where requirements for custom providers  are explainedFixes: ##22248",0
Cleanup Google provider CHANGELOG.rst (#23390),4
Fix some small typos also TESTING.rst (#14594)- Small typos in Jdbc provider documentation;- Find in the Concepts page an example using the old xcom_push arg and changed it;- Some changes in TESTING.rst that I found when trying the quick start.,1
[AIRFLOW-1816] Add region param to Dataproc operatorsCloses #2788 from DanSedov/master,1
"Fix corner case with joining processes/queues (#1473)If a process places items in a queue and the process is joined before the queue is emptied, it can lead to a deadlock under some circumstances. Closes AIRFLOW-61.See for example: https://docs.python.org/3/library/multiprocessing.html#all-start-methods (""Joining processes that use queues"")http://stackoverflow.com/questions/31665328/python-3-multiprocessing-queue-deadlock-when-calling-join-before-the-queue-is-emhttp://stackoverflow.com/questions/31708646/process-join-and-queue-dont-work-with-large-numbershttp://stackoverflow.com/questions/19071529/python-multiprocessing-125-list-never-finishes",5
[AIRFLOW-XXX] Add note about moving GCP from contrib to core (#6119),4
Don't create empty modules for plugins (#9078),1
"Simplify ""scheduled"" conditons to follow today's change in GA (#11876)GitHub Actions policy about runnig scheduled workflows in forkshas changed recently and as of October 27 2020 scheduled workflowsin forks will be disabled. Thanks to that change we can simplifysome of our conditions that disallowed running scheduled workflowsin forks.The change is active as of today (email from GitHub):> What will happen to scheduled workflows in forks I already have?> If you already have scheduled workflows in forks of public  repositories, they will be disabled on October 27, 2020. Repository  owners will receive a reminder email 7 days prior and can choose to  keep the workflows running at that time if they are needed.",1
[AIRFLOW-XXXX] Prohibit non-finished PR (#7543),5
[AIRFLOW-1001] Fix landing times if there is no following schedule@once does not have a following schedule. This wasnot checked forand therefore the landing times page could bork.Closes #2213 from bolkedebruin/AIRFLOW-1001,0
Moves Commiter's guide to CONTRIBUTING.rst (#11314)I decided to move it to CONTRIBUTING.rst as is it is an importantdocumentation on what policies we have agreed to as community andalso it is a great resource for the contributor to learn what arethe committer's responsibilities.Fixes: #10179,0
Fix recording console for new rich-click 1.5 (#24611),1
"Remove thrift as a core dependency (#13471)`thrift` is a dependency for Apache Hive and it is not required by Core Airflow:```airflow/providers/apache/hive/hooks/hive.py:489:        # This is for pickling to work despite the thrift hive client notairflow/providers/apache/hive/hooks/hive.py:500:        """"""Returns a Hive thrift client.""""""airflow/providers/apache/hive/hooks/hive.py:502:        from thrift.protocol import TBinaryProtocolairflow/providers/apache/hive/hooks/hive.py:503:        from thrift.transport import TSocket, TTransportairflow/providers/apache/hive/hooks/hive.py:531:            from thrift_sasl import TSaslClientTransportairflow/providers/apache/hive/sensors/hive_partition.py:41:    :param metastore_conn_id: reference to the metastore thrift serviceairflow/providers/apache/hive/sensors/metastore_partition.py:28:    queries generated by the Metastore thrift service when hittingairflow/providers/apache/hive/sensors/named_hive_partition.py:36:    :param metastore_conn_id: reference to the metastore thrift service```",2
Bumping poke_interval to 180 seconds for HivePartitionSensor,5
"Disables self-hosted runs for non-apache-owned repositories (#14239)When the user wants to test pull requests or - especilly - masterbuild in their own fork, they will not likely have self-hostedrunners configured and the builds will fail.This change uses self-hosted runners only if the build is run inthe context of the `apache/airflow` repository.",1
Fix typo in DagFileProcessorAgent._last_parsing_stat_received_at (#11022)`_last_parsing_stat_recieved_at` -> `_last_parsing_stat_received_at`,2
Fix typo (#20314)Build should be built.,2
[AIRFLOW-1579] Adds support for jagged rows in Bigquery hook for BQ load jobsCloses #2582 from DannyLee12/master,1
add script to initialise virtualenv (#22971)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
"Fixed runs-on for non-apache repository (#14737)The change #14718 by mistake left the 'self-hosted"" runs-on in case ofpush or schedule. This caused failures on non-apache repositories.",0
Run individual tasks from the UI,1
adding a fix for unicodecsv,0
Add doc and sample dag for S3ToFTPOperator and FTPToS3Operator (#22534),1
[AIRFLOW-1889] Split sensors into separate filesMoving the sensors to seperate files increasesreadability of thecode. Also this reduces the code in the bigcore.py file.Closes #2875 from Fokko/AIRFLOW-1889-move-sensors-to-separate-package,4
[AIRFLOW-5453] Improve reading inputs from Dataflow console,5
Merge branch 'master' into v1-8-test,3
Add a link to Databricks Job Run (#22541)It will be easier for users/admins to go to the specific run ofDatabricks Job,5
"For worker log servers only bind to IPV6 when dual stack is available (#26222)This function only exists on Python 3.8, so Python 3.7 will always listen on IPv4 only.",1
[AIRFLOW-720] Add Shopkick to Airflow usersCloses #1962 from ecesena/patch-1,1
More documentation for BigQuery to GCS operator.,1
[AIRFLOW-5731] Make output format from list commands configurable (#6400),5
Add ``Scribd`` to INTHEWILD.md (#17685)* Update INTHEWILD.mdAdd Scribd* Update INTHEWILD.md* use single-bracket style - makes more sense that way,1
Update `version_added` for `[email] from_email` (#21138),1
Add tests for gitSync in helm chart (#14316)Added Missing tests for #14203,3
Fix docstrings in BigQueryGetDataOperator (#10042),5
Rename Permissions to Permission Pairs. (#24065),5
Upgrade ramaining context file to typescript. (#25096)* Upgrade timezone to typescript.* Update props type.,5
Update setup.py to get non-conflicting set of dependencies (#12636)This change upgrades setup.py and setup.cfg to provide non-conflicting`pip check` valid set of constraints for CI image.Fixes #10854Co-authored-by: Tomek Urbaszek <turbaszek@apache.org>Co-authored-by: Tomek Urbaszek <turbaszek@apache.org>,0
Fix MyPy errors for Airflow decorators (#20034)Related: #19891,0
"Amazon Athena Example (#18785)This example, and associated documentation, shows how to use Amazon Athena to read a table from a CSV file in an S3 bucket, read from that table, and clean up all resources.",4
Support for dicts and list in operators template_fields,1
[AIRFLOW-6095] Filter dags returned by task_stats (#6684)Add dag_ids parameter to task_stats so can filter by a set of dag_idspresent on the dags view. This is intended to speed up the response timeand reduce the size of the payload when running a large number of dags.,2
[AIRFLOW-4818] Remove valid files from pylint_todo.txt (#5439),5
[AIRFLOW-XXX] Add next/prev ds not correct in faq (#5454)* [AIRFLOW-XXX] Add next/prev ds not correct in faq,1
Merge pull request #494 from jlowin/s3_logSupport worker logs on S3,2
Example xcom update (#17749),5
azure key vault optional lookup (#12174),5
Fixing MyPy issues inside tests/providers/amazon (#20561),3
Move apache-airflow docs to subdirectory (#12715),2
"[AIRFLOW-6839] even more mypy speed improvements (#7460)Require_serial:true is better choice than pass_filename: false as it canspeed-up mypy for single file changes.Significant gains can be achieved for single file changes and no cache for allother files. This is majority of cases for our users who have pre-commitsinstalled as hooks because most people change only few files and never runcheck with --all-filesWhen just one file is changed and no cache is built, the difference is drastic:require_serial: true = 4spass_filenames: false =  13s",4
Trying https,1
Also compile assets in non-main (#25220)Assets compilation when CI and PROD images are prepared shouldalso be run in non-main branch.,1
Fix typos in code.rst,2
Adding missing dependency,1
S3Hook.load_file should accept Path object in addition to str (#15232)Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>,1
added vertica hook,1
Fix for missing edit actions due to flask-admin upgrade,2
AIRFLOW-5496: delete unneeded variable assignment (#6110),4
closes  apache/incubator-airflow#1841 *Closing dummy PR*,5
Fix deprecation error message rather than silencing it (#18126),0
[AIRFLOW-XXX] Update README.md with Craig@WorkCloses #3311 from allanjsx/patch-1,1
[AIRFLOW-XXX] Simplify GCP operators listing,1
[AIRFLOW-XXXX] Fix typo in tests/conftest.py (#7181),5
Latest debian-buster release broke image build (#8758),3
Update Operators and Hooks doc to reflect latest (#19501),3
Fix typo in doc of DagFileProcessorManager.emit_metrics (#8175),2
[AIRFLOW-137] Fix max_active_runs on clearing tasks,1
Add description on the vendoring process we use (#22204)We need to vendor in cgroupspy library in order to make Airflowcompatible with Python 3.10 (see #22050) so this is the right timeto make our vendoring process more organized.I based in parts on the readme described by `bleach` package.,1
[AIRFLOW-6833] HA for webhdfs connection (#7454),5
Fix `collect_dags` docstring typo.,2
Fixed up the Tutorial section in the docs,2
Remove `xcom_push` flag from providers (#24823),1
"Revert ""Print configuration on scheduler startup. (#22588)"" (#22851)This reverts commit 78586b45a0f6007ab6b94c35b33790a944856e5e.",4
Fix CloudMemorystoreCreateInstanceAndImportOperator operator (#7856),1
Update documentation with LDAP info. Move authenticaton from installation to security.,4
[AIRFLOW-1050] Do not count up_for_retry as not readyup_for_retry tasks were incorrectly countedtowards not_readytherefore marking a dag run deadlocked instead ofretrying.Closes #2225 from bolkedebruin/AIRFLOW-1050,1
"[AIRFLOW-6510] Fix druid operator templating (#7127)Remove manual parsing of druid specification from druid operatorand replace it with template_fields, as in most operators.The reason for this change is because current implementation doesnot work well with relative paths, plus template_fields appears tobe the most common pattern in the project.",1
Added 2RP Net name in INTHEWILD file (#17345),2
Updating Docker example DAGs to use XComArgs (#16871),1
Detect partial examples DAGs for Google (#12277),2
Fix mallformed table in production-deployment.rst (#13395),0
Add missing session.commit() at end of initdbThe chart is added to the session but not committed.,1
Fix Harcoded Airflow version (#11483)This test will fail or will need fixing whenever we release new Airflowversion,1
Stop recursion after 5000 nodes,5
CI - OpenID Connect authorication to AWS (#19894)* CI - OpenID Connect authorication to AWS* fixup! CI - OpenID Connect authorication to AWS* Update ci.ymlCo-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>,1
Add Github Code Scanning (#11211)Github just released Github Code Scanning:https://github.blog/2020-09-30-code-scanning-is-now-available/,2
Disable health checks for ad-hoc containers (#14536)Co-authored-by: Kamil Bregua <kamilbregula@apache.org>,2
[AIRFLOW-5365] No need to do image rebuild when switching master/v1-10-test (#5972),3
20496 fix port standalone mode (#20505),0
[AIRFLOW-121] Documenting dag doc_md feature,2
Fixing the table layout for code,0
Add pre-commit to sort INTHEWILD.md file automatically (#10851),2
Clarify default Python rules after dropping 3.6 (#21180),4
"Chart: Use `v2-0-stable` branch for `git-sync` (#15809)Since the default Airflow versions we are using is 2.0.2, we should use DAGs from `v2-0-stable` for `git-sync`",2
[AIRFLOW-1855][AIRFLOW-1866] Add GCS Copy Operator to copy multiple filesCloses #2819 from kaxil/master,2
"Fixes tests that was not compatible with MySQL8 (#12615)In MySQL8 you cannot create table LIKE an INFORMATION_SCHEMAtable because they are not ""real"" tables and generic_transfertests failed on MySQL8 because of that.This PR changes the test to use connection table instead asa base for that test",3
[AIRFLOW-3444] Explicitly set transfer operator description. (#4279),1
closes apache/incubator-airflow#652 *no movement from submitter*,4
Merge pull request #207 from mistercrunch/priorityPrioritization and concurency limitation on executor queues,1
Add support for kinit options [-f|-F] and [-a|-A] (#17816)kinit can now emit non forwardable ticket and ticket without originate IP.,5
Migrate Yandex example DAGs to new design AIP-47 (#24082)closes: #22470,1
Properly measure number of task retry attemptsA tasks try_number is unbounded (incremented by 1 on every run) soit needs to be adjusted both for logging and for seeing if a taskhas eclipsed the retry cap. Rerunning a task (either because itfailed or with the `force` option) not only leads to nonsensicalerror messages (Attempt 2 of 1) but also would never kick off aretry attempt (because try_number > retries). The solution is to modthe `try_number` with `retries` to keep everything sensible.Fixed: use the correct attempt number when loggingFixed: log when tasks are queued (log message was being created butnot logged)Fixed: situation where tasks being run after the first time wouldnot be put up for retry,1
add version life cycle table (#15936)* add version life cycle table* add to airflow docs* fixes,0
"Update index.rst (#25184)This release is 3.0.1, not 3.1.0 --right?",5
"Bugfix: ``TypeError`` when Serializing & sorting iterables (#15395)This bug got introduced in #14909. Removed sorting from list and tuple as list & tuples preserve order unlike set.The following DAG errors with: `TypeError: '<' not supported between instances of 'dict' and 'dict'````pythonfrom airflow import modelsfrom airflow.operators.dummy import DummyOperatorfrom datetime import datetime, timedeltaparams = {    ""staging_schema"": [{""key:"":""foo"",""value"":""bar""},                       {""key:"":""this"",""value"":""that""}]}with models.DAG(dag_id='test-dag',                start_date=datetime(2019, 2, 14),                schedule_interval='30 13 * * *',                catchup=False,                max_active_runs=1,                params=params                ) as dag:    my_task = DummyOperator(        task_id='task1'    )```Full Error:```  File ""/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py"", line 210, in <dictcomp>    return cls._encode({str(k): cls._serialize(v) for k, v in var.items()}, type_=DAT.DICT)  File ""/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py"", line 212, in _serialize    return sorted(cls._serialize(v) for v in var)TypeError: '<' not supported between instances of 'dict' and 'dict'During handling of the above exception, another exception occurred:...```This is because `sorted()` does not work with dict as it can't compare. Removed sorting from list & tuples which fixes it.It also fails when we have set with multiple types.",1
[AIRFLOW-6336] Make tests/utils pylint compatible (#6892),3
Fix the behavior for deactivate the authentication option and documenting the process to do it (#13191),2
Add labels param to Google MLEngine Operators (#10222),1
[AIRFLOW-XXXX] Adjust celery defaults to work with breeze (#7205),1
[AIRFLOW-1556][Airflow 1556] Add support for SQL parameters in BigQueryBaseCursorCloses #2557 from rajivpb/sql-parameters,2
[AIRFLOW-1160] Update Spark parameters for MesosCloses #2265 from cameres/master,2
[AIRFLOW-987] pass kerberos cli args keytab and principal to kerberos.run() (#4238),1
Remove `host` from hidden fields in `WasbHook` (#19475),1
Set default logger in logging Mixin (#20355)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Add migration job resources (#19175)* Add migration job resources* Add change notes* Fix test* Linter* Update chart/values.schema.json* Update chart/values.schema.jsonCo-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>* LinterCo-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>,1
Merge pull request #1108 from kretes/patch-1new company + link to pitfalls,2
Moved subdag_operator.py to subdag.py (#11307)Part of #11178,2
Hide unused fields for Amazon Web Services connection (#25416),1
Fix failing main. (#20094),0
Fix dead link in MySQL Provider page (#25387),1
Moving test to the right place,3
Standardize the `pre-commit` config (#22686)Minor sorting of files/folder/terms,2
Adding a parameter for exclusion of trashed files in GoogleDriveHook (#25675)* Adding a parameter for exclusion of trashed files to GoogleDriveHook,1
Add unit tests,3
Working on HiveHook,1
Adding Hive2FtpOperator,1
"Ignore airflow/_vendor for building python API docs (#16270)On 3.6 it's slower than needed, but on Py 3.8 it causes a doc builderror (on a type comment of all things)",0
Add max_map_size to limit XCom task mapping size (#20976),1
Merge pull request #333 from r39132/masterAdding a link to the Agari Blog Post about Airflow,2
Adding support for .airflowignore files in DAGS_FOLDERs,2
Convert RDS Event and Snapshot Sample DAGs to System Tests (#24932),3
Fix a test case inside tests/models that leaves a trace in the DB (#20881),5
Add DAG Source endpoint (#9322),2
Remove un-needed/left over environment variables in ci.yml (#14732)AIRFLOW_COMMITERS was left over from a previous PR and should have neverbeen included.EVENT_NAME and TARGET_COMMIT_SHA aren't needed as GitHub already provideus GITHUB_EVENT_NAME and GITHUB_SHA with the same values.,1
[hotfix] fixing infinite retries in prod,5
[AIRFLOW-XXXX] Fix typo in bigquery_dts.rst (#7588),2
Align the default version with Facebook business SDK (#18883),5
Adding unixname to Job and TAskInstance models,1
[AIRFLOW-1969] Always use HTTPS URIs for Google OAuth2Closes #2900 from intellectronica/google-auth-force-scheme,1
"Remove cattrs from lineage processing. (#26134)Cattrs was used for two reasons:1. As a hacky way of forcing templated fields on classes2. As a way to store the outlets in XCom without needing pickle1 has been fixed in core for a while now and classes can have  ""template_fields"" properties (deeply)2 is now done by using a combo of BaseSerialization and `attr.asdict`",1
Support mssql in airflow db shell (#21511)We currently do not support mssql shell in the DB. This would ease troubleshooting for mssql,5
Updating Apache example DAGs to use XComArgs (#16869),1
"Move fallible ti.task.dag assignment back inside try/except block (#24533) (#24592)* Move fallible ti.task.dag assignment back inside try/except blockIt looks like ti.task.dag was originally protected inside try/except,but was moved out at commit 7be87d* Remove unneeded variable annotationCo-authored-by: EJ Kreinar <ej@he360.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",4
[AIRFLOW-6802] Fix bug where dag.max_active_run wasn't always honored by scheduler (#7416)commit 50efda5c69c1ddfaa869b408540182fb19f1a286 introduced a bug thatprevents the scheduler from enforcing max active run config for allDAGs.this commit fixes the regression as well as the test.,3
Enable Sphinx spellcheck for doc generation (#10280),2
S3ToSnowflakeOperator: escape single quote in s3_keys (#24607),1
[AIRFLOW-3201] Add sid88in to Glassdoor (#4045),1
add jdbc extension,5
Merge pull request #619 from airbnb/add_multipart_uploads_to_s3_hookAdd multipart uploads to s3 hook,1
Add Global Fashion Group as an Airflow userCloses #2815 from duynguyenhoang/Add-GFG-Using-Airflow,1
"A bunch of web UI backend tests (#15993)* Add test coverage to task instance clear view* Add tests for all TaskInstanceModelView viewsThe failure case for action_clear is not currently working. Bugfix PRfor that is pending, we'll remove the xfail marker when it goes in.* Test all DagRunModelView action views* Remove needless str-bytes coersion codeIn Python 3.6+, json.loads() can take bytes directly, so we don't needto decode ourselves.* Add test for VariableModelView.action_varexport* Add test for VariableModelView.action_muldelete* Test PoolModelView.action_muldelete",4
[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059),1
Python3 compatibility for MesosExecutor and documentation changes,4
[AIRFLOW-185] Handle empty versions list,0
Remove framer-motion from custom tooltip (#24449)* remove framer-motion from custom tooltip* remove framer-motion type declaration file,2
Enhanced TaskFlow API tutorial to use @dag decorator (#12937)Updated both the tutorial python file in the example_dags directoryand the tutorial documentation,2
Merge pull request #557 from thoralf-gutierrez/Fix_typos_in_HiveOperatorFix typos in HiveOperator,1
Use project_id to get authenticated client (#25984),1
"Revert ""Adjust built-in base_aws methods to avoid Deprecation warnings (#19725)"" (#19791)This reverts commit 4be04143a5f7e246127e942bf1d73abcd22ce189.",4
"Fix Commands to install Airflow in docker/install_airflow.sh (#14099)It was trying to run the following command and failing```pip install apache-airflow[amazon,google]2.0.0rc2```",0
Use pandas.io.gbq for BigQuery integration.,1
Kuba openfaas sync call (#13356)* adds invoke_function() - synchronous call in addition to existing asynchronous call* adds invoke_function() - synchronous call in addition to existing asynchronous call* airflow faas upgrade* passing body consistently without json=body* removing return() statement that must be causing static checks to fail* removing return() statement that must be causing static checks to fail* resolving static check issues* resolving static check issues* resolving static check issues* resolving static check issues,0
"[AIRFLOW-2068] MesosExecutor allows optional Docker imageIn its current form, MesosExecutor schedules taskson mesos slaves whichjust contain airflow commands assuming that themesos slaves alreadyhave airflow installed and configured on them.This assumption goesagainst the Mesos philosophy of having aheterogeneous cluster.Since Mesos provides an option to pull a Dockerimage before actuallyrunning the actual task/command so thisimprovement changes themesos_executor.py to specify an optional dockerimage containingairflow which can be pulled on slaves beforerunning the actualairflow command. This also opens the door for anoptimization ofresources in a future PR, by allowing thespecification of CPU andmemory needed for each airflow task.Closes #3008 from agrajm/AIRFLOW-2068",1
Switch XCom implementation to use run_id (#20975),1
Switch back temporarily to deprecated package registry (#16603),1
Upgrade grid Table component to ts. (#25074),2
Hide password in connection's form dom,4
Enable Black - Python Auto Formmatter (#9550),0
Allow compile_assets.sh script to be run from any directory (#8097),1
"AWS Glue Crawler Integration (#13072)This change integrates an AWS glue crawler operator, hook and sensor that can be used to trigger glue crawlers from Airflow.Co-authored-by: Kamil Bregua <kamil.bregula@polidea.com>",1
BugFix: Editing a DAG run or Task Instance on UI causes an Error (#12770)closes https://github.com/apache/airflow/issues/12489,0
Merge pull request #975 from jtschoonhoven/issue-974statuses column on /admin shows only active or most recent dag_runs,2
Update license check to include TypeScript file extensions (#14868),2
Fix `task_instance_mutation_hook` when importing airflow.models.dagrun (#15851)If a dag imported `airflow.models.dagrun` it would cause task_instance_mutation_hook from the site local settings to not be picked up.,1
Add Lucid to list of users,1
[AIRFLOW-XXXX] Fix broken static check for BREEZE.rst (#7555),0
Add MSSQL variables to dc_ci script (#18873)The dc_ci script is generated in `.build` folder so that youcan easily run docker compose command with multiple combineddocker-compose files and reproduce manually what breeze does.Useful for debugging. The MSSQL_* variables were missing fromthe script so MSSQL setup could not be easily debugged.,0
"Update known warnings for Python 3.7 (#19333)After seting 3.7 the default (#19317) the warning printed byPython during importing all providers (specifically apache beam)has slightly changed. Apparently collections.abc warning wasa bit more ""scary"" - warning that it's 3.9 not 3.10 where theold collection imports will stop working (Note that actuallythis did not happen even in 3.10, apparently)This PR fixes the ""known"" warning message to match it but alsoa separate PR (https://github.com/apache/beam/pull/15850) wasopened to Beam to get rid of the warnings altogether.Also seems 'dns` stopped generating this warning so I removed itand in case warnings are generated, they are printed outside ofthe folded group, so that it's immediately visible.",2
Fix case of JavaScript. (#10957),0
Fix case of GitHub (#12433)Github -> GitHub,0
"[AIRFLOW-4460] Remove __future__ import in models (#5237)Remove _future_ should done inhttps://github.com/apache/airflow/pull/5020, but inthe same time our code base refactor models andmove some code out of _init.py. This PR to remove__future__ in models.",4
Fix MyPy issues in ``airflow/jobs`` (#20612)Part of https://github.com/apache/airflow/issues/19891,0
"improved logs readability and config- passing --logging-level=DEBUG  at the command line is no longer ignored- classes can now implement WithLogger to get access to convenience logging function (e.g. self.log_info(""blah"")), producing log statement with a logger whose name is the name of the classe.g.:2015-12-07 17:22:43,084 - BackfillJob - INFO - [backfill progress] waiting: 0 | succeeded: 1 | kicked_off: 1 | failed: 0 | wont_run: 02015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - Calling the <class 'airflow.executors.sequential_executor.SequentialExecutor'> sync method2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 0 running task instances2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 0 in queue2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 32 open slots2015-12-07 17:22:43,085 - BackfillJob - INFO - All done. Exiting.- configured this WithLogger super class in a couple of key classes to improve log readability",2
"We don't need to build against Python 2.7 or 3.5 anymore (#16433)Airflow 1.10 has reached end of life on June 17th 2021, so we can tidyup our build scripts and not have to build these versions anymore",2
tMaking the gantt chart clickable,2
[AIRFLOW-4929] Improve display of JSON Variables in UI (#5641),5
"Don't run UI tests when python files have changed (#15138)The intent of the `set_outputs_run_all_tests` function wasn'timmediately clear to me, so I mistakenly set `needs_ui_tests true` inthere, which resulted in running the React UI test jobs in cases wherewe only changed python files -- a waste of a job!",2
Support secret backends/airflow.cfg for celery broker in entrypoint_prod.sh (#17069)* Support secret backends in entrypoint_prod.sh* Update entrypoint_prod.sh,1
Change the host in the example of using the API auth backend (#13548),1
Remove references to deprecated operators/params in PubSub operators (#22519),1
[AIRFLOW-5621] - Failure callback is not triggered when marked Failed on UI (#7025),0
new company + link to pitfallsallegro added as a company using airflow + link to common pitfalls,2
"Fix Celery Tests (#12166)Celery tests on Master are failing and then timing out on Postgres and MySQL.Stacktrace (link: https://github.com/apache/airflow/runs/1367123586#step:6:4860)```>           self.task_publish_retries.pop(key)E           KeyError: ('success', 'fake_simple_ti', datetime.datetime(2020, 11, 7, 8, 0, 13, 62424), 0)```This commit fixes the error introduced in https://github.com/apache/airflow/pull/12140",0
"Run helm chart tests in parallel (#15706)* Allow helm chart tests to run in parallelThe helm chart tests are pretty slow when run sequentially. Modifyingthem so they can be run in parallel saves a lot of time, from 10 minutesto 3 minutes on my machine with 8 cores.The only test that needed modification was `test_pod_template_file.py`,as it temporarily moves a file into the templates directorywhich was causing other tests to fail as they weren't expecting anyobjects from that temporary file. This is resolved by giving thepod_template_file test an isolated chart directory it can modify.`helm dep update` also doesn't work when it is called in parallel, sothe fixture responsible for running it now ensures we only run it one ata time.* Enable parallelism for helm unit tests in CICo-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>Co-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>",1
Change from Instance attribute to variable in JdbcOperator.execute (#7819),5
Add typing annotations to Segment provider (#10120),1
Fix Chart doc build (#18302)Error on main:``` WARNING: Title underline too short.```,2
"Revert ""Add automated retries on retryable condition for building images in CI (#24006)"" (#24016)This reverts commit 7cf0e43b70eb1c57a90ee7e2ff14b03487ffb018.",4
[AIRFLOW-4255] Replace Discovery based api with client based for GCS (#5054),5
"Sourcing the profile file should be sufficient to update the PATH, re-login is not required. (#11588)",1
[AIRFLOW-6319] Add support for AWS Athena workgroups (#6871),1
Datafusion assets (#21518),1
Update base python image to be Python 3.7 by default (#20978)Default base python image should be set to Python 3.7 as wedropped Python 3.6 support.,1
"Remove Version suffix for SVN while releasing (#15905)Reason explained in https://lists.apache.org/thread.html/rca430c836e5286b6d848831bdbc4f37fe5d6620ee9757e93b401495a%40%3Cdev.airflow.apache.org%3EWhile working on the Helm Chart release, I was verifying what we were doing for ""apache-airflow/python dists"" over the weekend, which is ""wrong"".We should be renaming the files as the SHA512 check fails on ""Release"" repo: https://dist.apache.org/repos/dist/release/airflow/2.0.2/For example, check out 2.0.2 release on Airflow:Since the SHA512 were generated with the original filename (with rc in it), it fails now in filename part:```shell for i in *.sha512do    echo ""Checking $i""; shasum -a 512 `basename $i .sha512 ` | diff - $idoneChecking apache-airflow-2.0.2-bin.tar.gz.sha5121c1< 4281b3ff5d5b483c74970f8128d7ad8ba699081086fd098e10b12f8b52a7d0f92a205d7ea334c29e813ac06af7a26de416294fd18c3a1a949388a4824955ce2e  apache-airflow-2.0.2-bin.tar.gz---> 4281b3ff5d5b483c74970f8128d7ad8ba699081086fd098e10b12f8b52a7d0f92a205d7ea334c29e813ac06af7a26de416294fd18c3a1a949388a4824955ce2e  apache-airflow-2.0.2rc1-bin.tar.gzChecking apache-airflow-2.0.2-source.tar.gz.sha5121c1< ca783369f9044796bc575bf18b986ac86998b007d01f8ff2a8c9635454d05f39fb09ce010d62249cf91badc83fd5b38c04f2b39e32830ccef70f601c5829dcb7  apache-airflow-2.0.2-source.tar.gz---> ca783369f9044796bc575bf18b986ac86998b007d01f8ff2a8c9635454d05f39fb09ce010d62249cf91badc83fd5b38c04f2b39e32830ccef70f601c5829dcb7  apache-airflow-2.0.2rc1-source.tar.gzChecking apache_airflow-2.0.2-py3-none-any.whl.sha5121c1< 779563fd88256980ff8a994a9796d7fd18e579853c33d61e1603b084f4d150e83b3209bf1a9cd438c4dd08240b1ee48b139690ee208f80478b5b2465b7183e50  apache_airflow-2.0.2-py3-none-any.whl---> 779563fd88256980ff8a994a9796d7fd18e579853c33d61e1603b084f4d150e83b3209bf1a9cd438c4dd08240b1ee48b139690ee208f80478b5b2465b7183e50  apache_airflow-2.0.2rc1-py3-none-any.whl```I was also checking how other projects did it, Apache Spark for instance, they also just have the ""rc"" name in the directoryand that is all: https://dist.apache.org/repos/dist/dev/spark/v2.4.8-rc3-bin/ so it is easy to""just move"" from ""dev"" to ""release"" without changing anything.For Airflow releases since we already do it in a directory that is named ""VERSION-rcX"" (example: `2.1.0rc1` in https://dist.apache.org/repos/dist/dev/airflow/2.1.0rc1/)we don't need to add rcX in the filename of the artifact.This helps us just moving files without renaming and changing filenames in SHA512 and is released exactly with what was voted on.",2
Fix unexpected bug in exiting hook context manager (#18014),1
Change logging level details of connection info in `get_connection()` (#21162),1
DockerOperator extra_hosts argument support added (#10546),1
[AIRFLOW-6589] BAT tests run in pre-commit on bash script changes (#7203),4
Remove reference to deprecated operator in example_dataproc (#19619),5
[AIRFLOW-1948] Include details for on_kill failureRemove the bare exception and propagate errorinformation to the log.Closes #2897 from wrp/onkill,2
[AIRFLOW-1359] Add Google CloudML utils for model evaluationCloses #2407 from yk5/evaluate,1
[AIRFLOW-6672] AWS DataSync - better logging of error message (#7288),0
Migrate Salesforce example DAGs to new design #22463 (#24127),1
Fix Link in Upgrading to 2.0 guide (#13584)The link used Markdown syntax instead of rst,1
[AIRFLOW-XXX] Remove quotes from domains in Google Oauth (#4226)Related SO: https://stackoverflow.com/a/52528091/10638329,4
Merge pull request #1193 from jlowin/remove-main_sessionRemove one more session reference,4
"Use assertEqual instead of assertTrue in tests/utils/test_dates.py for proper diff (#10457)assertEqual will show show the proper diff instead of just ""False is not True"" error",0
Fix empty image preparation (#23304)Empty image preparation failed in CI because it was impossible tobuild an empty image without buildkit. This change sets DOCKER_BUILDKITvariable for empty image build which make it always use the buildkit.,1
[AIRFLOW-2107] add time_partitioning to run_query on BigQueryBaseCursorCloses #3043 from marengaz/query_time_part,1
"Add reference to the ASF CoC for First Time Contributors (#9454)Similar to https://github.com/apache/airflow/issues/9453 but this would inform all the ""First Time Contributors""",5
Fix tag link on dag detail page (#24918),2
[AIRFLOW-XXXX] Replace conversion from list to set (#7696),1
[AIRFLOW-XXXX] correct path to deploy_airflow_to_kubernetes.sh in TESTING.rst (#7522),3
[AIRFLOW-1286] Use universal newline when opening log filesCloses #2363 from ronfung/Airflow-1286,2
Add dataset read permission to viewer role (#26181),5
Merge pull request #293 from jlowin/follow_symlinksFollow symlinks in DAG and Plugin folders,2
Rename docs fields in chart values schema (#15828)We will use the `x-` prefix to signify these are custom non-standardfields in our JSON schema.,5
Bump Boto3 (#7851),5
Addding a few TODO items,2
"[AIRFLOW-2166] Restore BQ run_query dialect paramRestores the use_legacy_sql parameter in the run_query method ofBigQueryBaseCursor.This method was removed by commitd5d2c01f37f345458d9eeb8cdfbb0e77b55eb7ea, which introduced abackward-incompatible change for direct calls to the cursormethods.Closes #3087 from ji-han/AIRFLOW-2166",4
Add trigger rule tooltip (#26043)* Display operator trigger rule in graph view tooltip* Change order of trigger rule in tooltip* Fix linting errors in airflow/www* Fix additional lint errors in airflow/www/views.py* Wrap trigger rule display in if statement* Add null as 2nd argument to tiTooltip in gantt.jsAccomodates the additional argument for the tiTooltip object* Update airflow/www/static/js/task_instances.jsChange condition for determining if task.trigger_rule is presentCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,5
[AIRFLOW-2485] Fix Incorrect logging for Qubole Sensor- Replace incorrect `this.log` keyword for loggingof Qubole Sensor to `self.log`I am sure https://github.com/apache/incubator-airflow/pull/3297#issuecomment-385988083 wassuppose to mean `self.log.info` instead of`this.log.info`.Closes #3378 from kaxil/AIRFLOW-2485,5
[AIRFLOW-XXX] Add autogenerated TOC (#6038),1
Enhance CLI Test command to accept a JSON-formatted dictionary of params that can be added to a task's params dict.The CLI-provided params will overwrite params of the same name defined in the task definition if a key conflict occurs. This change will allow us to provide parameters to a DAG at runtime that are specific to a 'test' command run.,1
[AIRFLOW-XXX] Move Dag level access control out of 1.10 section (#3882)It isn't in 1.10 (and wasn't in this section when the PR was created).,1
Add typing for airflow/configuration.py (#23716)* Add typing for airflow/configuration.pyThe configuraiton.py did not have typing information and it madeit rather difficult to reason about it-especially that it wenta few changes in the past that made it rather complex tounderstand.This PR adds typing information all over the configuration file,2
[AIRFLOW-353] Fix dag run status update failureWhen multiple tasks gets removed update statusfails as the list is updated in place.Closes #1675 from yiqingj/master,5
Add smtp_starttls flag to config,5
[AIRFLOW-5379] Add Google Search Ads 360 operators (#6228),1
"[AIRFLOW-319]AIRFLOW-319] xcom push response in HTTP OperatorAdds optional parameter to push response of theHTTP Operator to XComDocumentation suggests that any operator's executemethod that returnsvalue should push it into XCom:http://airflow.incubator.apache.org/concepts.html#xcoms> In addition, if a task returns a value (eitherfrom its Operatorsexecute() method, or from a PythonOperatorspython_callablefunction), then an XCom containing that value isautomatically pushed.Closes #1658 from jzelenkov/AIRFLOW-319_xcom_push_http_response",5
"Fix statsd metrics not sending when using daemon mode (#14454)It seems that the daemonContext will close the socket of statsd.```    return self.statsd.incr(stat, count, rate)  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 35, in incr    self._send_stat(stat, '%s|c' % count, rate)  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 59, in _send_stat    self._after(self._prepare(stat, value, rate))  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 74, in _after    self._send(data)  File ""/opt/airflow/airflow/stats.py"", line 40, in _send    self._sock.sendto(data.encode('ascii'), self._addr)OSError: [Errno 9] Bad file descriptor```",2
[AIRFLOW-3129] Improve test coverage of airflow.models. (#3982),3
[AIRFLOW-1173] Add Robinhood to who uses AirflowCloses #2271 from vineet-rh/patch-2,1
Merge pull request #441 from jlowin/Py3Py3 fixes,0
Adding --local option to run command,1
Merge pull request #1119 from jlowin/gcpAdd gcloud-based GCSHook,1
[AIRFLOW-XXXX] Add HomeToGo in Airflow users list (#7769),1
remove truncation,1
[AIRFLOW-XXX] Include Pagar.me in list of users of Airflow (#4026),1
[AIRFLOW-3655] Escape links generated in model views (#4522),2
"Create Endpoint and Model Service, Batch Prediction and Hyperparameter Tuning Jobs operators for Vertex AI service (#22088)",1
[AIRFLOW-2474] Only import snakebite if using py2Closes #3365 from jgao54/snakebite-import,2
Suppress hook warnings from the Bigquery transfers (#20119),2
fix bigquery hook,1
move autocommit to own method for easier overwriting,4
Remove mapped operator validation code (#25870),5
"[AIRFLOW-4416] Revert ""Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)"" (#5191)This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.",4
Added test for bigquery sensor (#8986)* added small test for bigquery sensor* removed file from missing_test_files,3
Remove ``dags.gitSync.excludeWebserver`` from chart ``values.schema.json`` (#16070)This was mistakenly added back to the schema and can safely be removed.,4
Update breeze setup instruction (#20352)use $(brew --prefix) to get the path to gnu-getoptthe prefix in newer version homebrew in Apple Silicon changed,4
"Update black precommit (#22521)Use latest version of black, drop py36, and add py310.",1
Dataproc: Remove default value of `region` (#23350)* `region` parameter has no default value.  affected functions/classes:  `DataprocHook.cancel_job`  `DataprocCreateClusterOperator`  `DataprocJobBaseOperator`  * `DataprocJobBaseOperator`: order of parameters has changed,4
Add Tabular provider (#23704),1
[AIRFLOW-5711] Add fallback for connection's project ID in Dataflow integration (#6383)* [AIRFLOW-5711] Use keywords arguments as a parameter,2
Bump tmpl from 1.0.4 to 1.0.5 in /airflow/ui (#25809)Bumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.- [Release notes](https://github.com/daaku/nodejs-tmpl/releases)- [Commits](https://github.com/daaku/nodejs-tmpl/commits/v1.0.5)---updated-dependencies:- dependency-name: tmpl  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396),0
"SquashedFixing jobsLining up db revisionsAdding CRUD in the adminSuccess, backend running, next is UI changesUpdating the docs to reflect the new behaviorGot the UI to show externaly triggered runs, root object for DAG RunUI improvments, mostly functionalDAG concurrency limitCommit resets dag runsMore unit testsAdapting the UIFixing brutal amount of merge conflictsPolish around UI and eventsAdding DB migration scriptFixed the chartsAdding schedule info in the dag pageAdding cron presetsFixing up the testsAdding @once as a schedule_interval optionA layer of polish and bug fixes",0
Create CommandExecutor for raising an exception in case of error during cmd execution (#17651),0
"Move out get_python_source from www, Move get_dag to www.utils (#7899)",2
allow annotations on helm dag pvc (#22261),2
"Fix grammar and typos in ""Logging for Tasks"" guide (#20146)",2
Add Bodastage and BTS-CE to current user listCloses #3110 from erssebaggala/master,1
[AIRFLOW-291] Add index for state in TI tableCloses #1635 from aoen/ddavydov/add_index_to_task_instance_state,1
Chart: 1.5.0 changelog (#21906)Add changelog for chart version 1.5.0.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
[AIRFLOW-XXXX] Add cross-references for operators guide (#7760),1
[AIRFLOW-6678] Pull event logs from Kubernetes (#7292)Adds an option (defaults to True) to pull and log events from aKubernetes pod that fails.,0
Adjust heuristics for PR of the Month script (#25239),5
Fix bug that log can't be shown when task runs failed (#16768)The log can't be shown normally when the task runs failed. Users can only get useless logs as follows. #13692<pre>*** Log file does not exist: /home/airflow/airflow/logs/dag_id/task_id/2021-06-28T00:00:00+08:00/28.log*** Fetching from: http://:8793/log/dag_id/task_id/2021-06-28T00:00:00+08:00/28.log*** Failed to fetch log file from worker. Unsupported URL protocol </pre>The root cause is that scheduler will overwrite the hostname info into the task_instance table in DB by using blank str in the progress of `_execute_task_callbacks` when tasks into failed.  Webserver can't get the right host of the task from task_instance because the hostname info of  task_instance table is lost in the progress.Co-authored-by: huozhanfeng <huozhanfeng@vipkid.cn>,5
remove debug log,2
closes apache/incubator-airflow#2675 *Closed for inactivity.*,5
Merge pull request #792 from abridgett/feature/add_error_handling_to_slack_operatoradd error handling for slack api,0
Mention about .sh commands and templates in BashOperator docs (#11566),2
Adds compile_assets to INSTALL (#17377),1
update remaining old import paths of operators (#15127),1
[AIRFLOW-638] Add schema_update_options to GCP opsCloses #1891 fromJalepeno112/feature/gcs_to_bq_schemaUpdateOptions,5
hdfs provider: allow SSL webhdfs connections (#17637),1
Fix SystemsManagerParameterStoreBackend test (#25751)This has been failing since the backend moved use_ssl to a separatedattribute instead of bundling it in kwargs.,5
"Revise ""Project Focus"" copy (#12011)",5
Use jdk selector to set required jdk,1
[AIRFLOW-XXX] Adding Palo Alto Networks as a user (#4495),1
[AIRFLOW-2293] Fix S3FileTransformOperator to work with boto3S3FileTransformOperator doen't work for now sinceit uses a functionwhich is no longer supported by boto3. This PRreplaces it with avalid function and adds an unit test for thisoperator.Closes #3200 from sekikn/AIRFLOW-2293,1
Fix incorrect Env Var to stop Scheduler from creating DagRuns (#8920),2
Merge pull request #178 from mistercrunch/statsHiveStatsCollectionOperator!,1
Update Google Cloud branding (#10615),5
Augment xcom docs (#20755),2
[AIRFLOW-6770] Run particular test using breeze CLI bug fix (#7396)* [AIRFLOW-6770] Run particular test using breeze CLI bug fix* [AIRFLOW-6770] Fix typo in travis config* [AIRFLOW-6770] Fix variable name and remove unnecessary travis command,4
Fix MyPy issues in ``tests/executors`` (#20714),3
"task, dag -> task_id, dag_id",2
[AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook (#8380)* [AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook* [AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook* Use Github Actions to run CI (#8376)* Use Github Actions to run CI* Fix backports buildCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,0
Pass image_pull_policy in KubernetesPodOperator correctly (#13289)* pass image_pull_policy to V1Containerimage_pull_policy is not being passed into the V1Container inKubernetesPodOperator. This commit fixes this.* add test for image_pull_policy not setimage_pull_policy should be IfNotPresent by default ifit's not set. The test ensure the correct value is passedto the V1Container object.,4
"Add fixing of ownership also for sources in Breeze (#18464)In some cases files created inside the container on linux insources are owned by root - so far we cleaned only some directoriesthat we knew were touched by Airflow, but in case of branchswitching some folders/files might also got created as ownedby root.This PR adds AIRFLOW_SOURCES and ""dags"" folders to the listof folders to fix ownership of.This might lead to slightly longer exit time when exitingfrom Breeze, but since we are running it only when the host isLinux and the fixung is rather performant - with explicitly findingfiles and dirs owned by root, this should not be a problem.It would be much bigger problem on MacOS,Windows due to slow filesystem,but on MacOS it is not needed as there ownership mapping is doneusing the filesystem itself.",5
"[AIRFLOW-2113] Address missing DagRun callbacksGiven that the handle_callback method belongs tothe DAG object, we are able to get the list oftask directly with get_task and reduce thecommunication with the database, making airflowmore lightweight.Closes #3038 from wolfier/master",1
[AIRFLOW-3611] Simplified development environment (#4932),5
"[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359)This parameter is deprecated by werkzeug, see:https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120However, it is also broken. The value is acquired as string from theconfig, while it should be int like the other `x_*` attributes. Thosewere fixed in #6901, but `num_proxies` was forgotten.I think we can safely remove it because:* There is virtually no possibility that someone is using that parameter  in their config without raising an exception.* The configuration variable is not present in Airflow's docs or   anywhere else anymore. The removed line is the only trace of it.More details:https://issues.apache.org/jira/browse/AIRFLOW-6740",0
"[AIRFLOW-2415] Make airflow DAG templating render numbersCurrently, if you have an operator with a templatefields argument, that is a dictionary, e.g.:template_fields = ([dict_args])And you populate that dictionary with a field thatan integer in a DAG, e.g.:...dict_args = {'ds': '{{ ds }}', num_times: 5}...Then ariflow will give you the following error:{base_task_runner.py:95} INFO - Subtask:airflow.exceptions.AirflowException: Type '<type'int'>' used for parameter 'dict_args[num_times]'is not supported for templatingThis fix aims to resolves that issue byimmediately resolving numbers without attemptingto template themCloses #3410 fromArgentFalcon/support_numeric_template_fields",1
Compressing / caching improvments around charts,2
Cope with `@task.docker` decorated function not returning anything (#18463),1
Add type hints to  aws provider (#11531)* Added type hints to aws provider* Update airflow/providers/amazon/aws/log/s3_task_handler.py* Fix expectation for submit_job* Fix documentationCo-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>,1
[AIRFLOW-2903] Change default owner to `airflow` (#4151)This makes the default owner when one is not provided in the dag the same as in the examples and docs for consistency.,2
Restore image rendering in AWS Secrets Manager Backend doc (#21772),2
Turn Airflow versions into a free-form field for Helm/Providers (#25564)* Turn Airflow versions in free-form field for Helm/Providers,1
Improve environment variables in GCS system test (#13792),3
Clarify guidance on folder locations for newsfragments (#23170),1
YAML file supports extra json parameters (#9549)Co-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>Co-authored-by: Vinay <vinay@synctactic.ai>Co-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>,1
"The librabbitmq library stopped installing for python3.7 (#8853)When preparing backport relases I found that rabbitmq was notincluded in the ""devel_ci"" extras. It turned out that librabbitmq wasnot installing in python3.7 and the reason it turned out to bethat librabbitmq is not maintained for 2 years already and ithas been replaced by py-amqp library. The pythhon py-amqplibrary has been improved using cython compilation, so itbecame production ready and librabbitmq has been abandoned.We are switching to the py-amqp library here and addingrabbitmq back to ""devel_ci"" dependencies.Details in: https://github.com/celery/librabbitmq/issues/153",0
Add example DAG for ECSOperator (#8452),1
Replace old SubDag zoom screenshot with new (#9621),1
Merge pull request #46 from mistercrunch/celery_fixCelery execution fixes,0
"Merge pull request #77 from mistercrunch/celery_paramMaking concurency a conf param, stting CELERYD_PREFETCH_MULTIPLIER=1",2
Add EMR operators howto docs (#8863),2
"Fix assignment of unassigned triggers (#21770)Previously, the query returned no alive triggerers which resultedin all triggers to be assigned to the current triggerer. This worksfine, despite the logic bug, in the case where there's a singletriggerer. But with multiple triggerers, concurrent iterations ofthe TriggerJob loop would bounce trigger ownership to whicheverloop ran last.Addresses https://github.com/apache/airflow/issues/21616",0
Update tree doc references to grid (#22966)* Update tree doc references to grid* Update docs/apache-airflow/ui.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* Update docs/apache-airflow/ui.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* revert Chart changes. move main grid view* Update docs/apache-airflow/ui.rstCo-authored-by: Tzu-ping Chung <tp@astronomer.io>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
"Move all ""old"" SQL operators to common.sql providers (#25350)Previously, in #24836 we moved Hooks and added some new operators to thecommon.sql package. Now we are salso moving the operatorsand sensors to common.sql.",1
[AIRFLOW-XXX] Add Kamil as committer (#5216),1
More user-oriented change titles in Python API sections (#10099),4
Add Travix to INTHEWILD.md  (#20494)Co-authored-by: bozturk <bozturk@travix.com>,1
"Fix chart: parameterize namespace (#10213)Replace fixed namespace ""airflow"" with variable {{ .Release.Namespace }}",0
Fix BigQuery system test (#18373),3
"[AIRFLOW-XXX] Remove default/wrong values from test config. (#5684)Most of the values I've removed here are the current defaults, so wedon't need to specify them again.The reason I am removing them is that `email_backend` of`airflow.utils.send_email_smtp` has been incorrect since 1.7.2(!) buthasn't mattered until #5379 somehow triggered it. By removing thedefault values it should make it easier to update in future.",5
Fix missing whitespace in ``apply_default`` deprecation message (#17799),0
Triggering DAG with Future Date (#10249)THis will allow linking this section,2
Remove /dagrun/create and disable edit form generated by F.A.B (#17376),2
[AIRFLOW-4072] enable GKEPodOperator xcom (#4905)this provides consistent functionality with KubernetesPodOperator,1
AIRFLOW-119: List support for tags in QuboleOperator,1
use exceptions intead of returning tuple,1
Use python client in BQ hook create_empty_table/dataset and table_exists (#8377)* Use python client in BQ hook create_empty_table method* Refactor table_exists and create_empty_dataset* Add note in UPDATING,5
[AIRFLOW-3112] Make SFTP hook to inherit SSH hook (#3945)This is to aline the arguments of SFTP hook with SSH hook,1
Support log download in task log view (#22804)* Add download button to ti_log view* Use native anchor tag for ti_log download button* Replace regex with data attribute* Update airflow/www/static/js/ti_log.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,2
[AIRFLOW-2168] Remote logging for Azure Blob Storageadd wasb_task_handler.py to enable remote loggingon Azure Blob Storage.add file and read capabilities to wasb_hook tosupport wasb_task_handlerCloses #3095 from marcusrehm/master,0
Fix small errors in image building documentation (#9792),2
Add Notifications of build failures (#22552)There is new feature to receive notifications about build failuresfor git repos. It is supposed to only send info about changedstatus -> failed/succeeded or succeeded-> failed transition.Trying it out.,1
Fix Google DLP example and improve ops idempotency (#10608),1
Minor grammar and sentence flow corrections in pip installation docs (#19468)* Minor grammar and sentence flow corrections in pip installation docs (#19468),2
"Revert ""[AIRFLOW-XXX] Add Grab to companies list (#4041)""This reverts commit eaafff295f6d67fa35a3b379f9b7c4ecbe0cbf08.",4
"[AIRFLOW-160] Parse DAG files through child processesInstead of parsing the DAG definition files in the same process as thescheduler, this change parses the files in a child process. This helpsto isolate the scheduler from bad user code.Closes #1636 from plypaul/plypaul_schedule_by_file_rebase_master",2
[AIRFLOW-5555] Remove Hipchat integration (#6184),4
"Split tests to more sub-types (#11402)We seem to have a problem with running all tests at once - mostlikely due to some resource problems in our CI, therefore it makessense to split the tests into more batches. This is not yet fullimplementation of selective tests but it is going in this directionby splitting to Core/Providers/API/CLI tests. The full selectivetests approach will be implemented as part of #10507 issue.This split is possible thanks to #10422 which moved building imageto a separate workflow - this way each image is only built onceand it is uploaded to a shared registry, where it is quicklydownloaded from rather than built by all the jobs separately - thisway we can have many more jobs as there is very little per-joboverhead before the tests start runnning.",1
"Handle leading slash in samba path (#18847)Fix issue that occurs when the path to a file on a samba share has aslash prepended to it, then the `SambaHook` will treat the path as thehost instead likely resulting trying to connect to the wrong samba host.",0
"Add missing crypto and s3 extras (#12850)We are missing two extras that were existingin Airflow 1.10 - crypto and s3. They are both deprecatedbecause s3 should be replaced with amazon and crypto isalready installed by default by Airflow as 'install_requires',but for backwards compatibility, we  add them to 2.0 as wellwith deprecation comment. Also yandexcloud has been removed(it was not present in 1.10 so no need to deprecate it)and new table with deprecated extras is added to the docs.",2
[AIRFLOW-4911] Silence the FORBIDDEN errors from the KubernetesExecutor (#5547)* [AIRFLOW-4911] Silence the FORBIDDEN errors from the KubernetesExecutor* switch to WARN* add to show message from ApiException,1
[AIRFLOW-XXXX] Remove duplication in BaseOperator docstring (#7321),2
[AIRFLOW-6683] REST API respects store_serialized_dag setting (#7296)Make REST API respect core.store_serialized_dags setting,1
Add SalesforceApexRestOperator (#18819),1
[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warningLogging functionality for SSHOperator was added in[AIRFLOW-1712] but itonly logged stdout.This commit also logs stderr to log.warningCloses #2761 from OpringaoDoTurno/stderr_in_ssh,2
Chart: less fragile webserver deployment tests (#18332)Simply make the log persistence tests be less fragile by not caringabout volume and volume mount order.,3
"Add per-DAG delete permissions (#21938)This PR adds per-DAG delete permissions and extends the sync-perms subcommand to add delete permissions to the database for all existing DAGs (it does not, however, grant any of the new DAG delete permissions to any roles).",4
Adding an example to illustrate the TriggerDagRunOperator,2
Improving the TriggerDagRunOperator example,2
[AIRFLOW-1843] Add Google Cloud Storage Sensor with prefixSensor for checking if there any files in bucketat certain prefixCloses #2809 from litdeviant/gcs_prefix_sensor,0
Add str to task_ids typing in BaseOperator.xcom_pull(#21541)Co-authored-by: Alexander Chen <alexchen@apple.com>,1
[AIRFLOW-4831] conf.has_option no longer throws if section is missing. (#5455),5
[AIRFLOW-1678] Fix erroneously repeated word in function docstringsCloses #2660 from thundergolfer/thundergolfer/small-docstring-fix,2
[AIRFLOW-2596] Add Oracle to Azure Datalake Transfer OperatorCloses #3613 frommarcusrehm/oracle_to_azure_datalake_transfer,5
"[AIRFLOW-2207] Fix flaky test that uses app.cached_app()tests.www.test_views:TestMountPoint.test_mount changes base_urlthen calls airflow.www.app.cached_app().But if another test calls app.cached_app() first without changingbase_url, succeeding test_mount fails on Travis.So test_mount should clear cached app for itself in its setup methodso as to remount base_url forcefully.",1
Adds Hacktoberfest label to participate in Hacktoberfest 2021 (#18781),1
Disable row level locking for Mariadb and MySQL <8 (#14031)closes #11899closes #13668This PR disable row-level locking for MySQL variants that do not support skip_locked and no_wait -- MySQL < 8 and MariaDB,5
[AIRFLOW-6172] BigQuery - Move BigQuery hook Pandas data frame system tests to separate file (#6729)This is done due to the fact that these tests require authorization to BigQuery,1
"[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625)PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() methodwith the replace=True flag. However, the underlying generated SQL is specific to MySQL's""REPLACE INTO"" syntax and is not applicable to PostgreSQL.This pulls out the sql generation code for insert/upsert out in to a method that is thenoverridden in the PostgreSQL subclass to generate the ""INSERT ... ON CONFLICT DOUPDATE"" syntax (""new"" since Postgres 9.5)",1
Add Github Actions badge to README (#8386),1
"Update CONTRIBUTORS_QUICK_START.rst, TESTING.rst(#21140)",3
"Implement ExternalPythonOperator (#25780)This Operator works very similarly to PythonVirtualenvOperator - butinstead of creating a virtualenv dynamically, it expects theenv to be available in the environment that Airlfow is run in.The PR adds not only the implemenat the operator, but alsodocuments the operator's use and adds best-practices chapterthat explains the differences between different ways how you canachieve separation of dependencies between different tasks. Thishas been a question added many times in by our users, so addingthis operator and outlining future aspects of AIP-46 and AIP-43that will make separate docker images another option is alsopart of this change.",4
Fix GCStoGCS operator with replace diabled and existing destination object (#16991),1
Fix typo in PostgresHook (#10529)`PostrgresHook` -> `PostgresHook`,1
Merge pull request #230 from airbnb/faqAdding an FAQ entry to the docs,2
"Setup.cfg change triggers full build (#12684)Since we moved part of the setup.py specification tosetup.cfg, we should trigger full build when only that filechanges.",4
Clarify guidance re trust of keys in release docs (#19480)* Clarify guidance re trust of keys in release docs1. Kaxil's key referenced in the docs is expired.  I update with the current key.2. keys.openpgp.org no longer seems to be set as the default (at least it was not on my machine).  So I update the key import to specify this server i.e.3. clarify language concerning the remote key servers* fix spelling,0
Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052)* Replacing non-attribute template_fields for BigQueryToMsSqlOperator* Updating source_project_dataset_table arg in example DAG,2
Update v1.yaml (#21024),5
Merge pull request #743 from praveev/run-time-acquisitionGet connection string containing username and password during runtime,1
[AIRFLOW-1848][Airflow-1848] Fix DataFlowPythonOperator py_file extension doc commentCloses #2816 from cjqian/1848,2
Add Databricks provider to boring cyborg (#25238),1
Add Opus Interactive to INTHEWILD (#18423),1
"[AIRFLOW-2614] Speed up trigger_dag API call when lots of DAGs in systemRather than loading all dags in the DagBag, find the path to thespecific DAG from the ORM and load only that one.Closes #3590 from mishikaSingh/master",2
Merge pull request #547 from easytaxibr/docsAdd Easy Taxi to list of companies using Airflow,1
"Enable Connection creation from Vault parameters (#15013)Currently using the Vault secrets backends requires that users storethe secrets in connection URI format:https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#connection-uri-formatUnfortunately the connection URI format is not capable of expressingall values of the Connection class. In particular the Connectionclass allows for arbitrary string values for the  `extra` parameter,while the URI format requires that this parameter be unnested JSONso that it can serialize into query parameters.```>>> Connection(conn_id='id', conn_type='http', extra='foobar').get_uri()[2021-03-25 13:31:07,535] {connection.py:337} ERROR - Expecting value: line 1 column 1 (char 0)Traceback (most recent call last):  File ""/Users/da.lum/code/python/airflow/airflow/models/connection.py"", line 335, in extra_dejson    obj = json.loads(self.extra)  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/__init__.py"", line 348, in loads    return _default_decoder.decode(s)  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/decoder.py"", line 337, in decode    obj, end = self.raw_decode(s, idx=_w(s, 0).end())  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/decoder.py"", line 355, in raw_decode    raise JSONDecodeError(""Expecting value"", s, err.value) from Nonejson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)[2021-03-25 13:31:07,535] {connection.py:338} ERROR - Failed parsing the json for conn_id id'http://'```As shown, the `extra` data is missing from the return value `http://`.Although there is an error logged, this does not help users who werepreviously able to store other data.",5
documentation,2
[AIRFLOW-3218] add support for poking a whole DAG (#4058)Add support for poking a whole DAG,2
left env_variable in the connection model,5
Temporary limit Pandas version (#21045)This is likely only for couple of days to avoid test failuresin `main`. When the 3.4.4 version of Flask Builder getsreleased we should be able to relax the limit as it will allowus to migrate to sqlalchemy 1.4,1
Fix mypy typing for airflow/models and their tests (#20272)The re-ordering of setting attributes in BaseOperator is because_something_ about that function (throwing the exceptions?) causes mypyto think that BaseOperator objects could be missing those attributes,1
[AIRFLOW-XXX] Improve GCP documentation (#6433),2
Add 1.10 import fallback in GCS-Presto system test (#8066),3
Adding a timeout Context object and using it when importing dags,2
Add bucket_name to template fileds in S3 operators (#13973)Without that it's impossible to create buckets using for exampleexecution date. And that is quite common case.,5
"Make Production Dockerfile OpenShift-compatible (#9545)OpenShift (and other Kubernetes platforms) often use the approachthat they start containers with random user and root group. This isdescribed in the https://docs.openshift.com/container-platform/3.7/creating_images/guidelines.htmlAll the files created by the ""airflow"" user are now belonging to'root' group and the root group has the same access to thosefiles as the Airflow user.Additionally, the random user gets automatically added/etc/passwd entry which is name 'default'. The name of the usercan be set by setting the USER_NAME variable when starting thecontainer.Closes #9248Closes #8706",1
Improving the 'root' filter,1
Last minute tweaks,5
"[AIRFLOW-900] Fixes bugs in LocalTaskJob for double run protectionRight now, a second task instance being triggeredwill causeboth itself and the original task to run becausethe hostnameand pid fields are updated regardless if the taskis already running.Also, pid field is not refreshed from db properly.Also, we shouldcheck against parent's pid.Will be followed up by working tests.Closes #2102 from saguziel/aguziel-fix-trigger-2",0
[AIRFLOW-4925] Improve css style for Variables Import file field (#5552),2
[AIRFLOW-2181] Convert password_auth and test_password_endpoints from DOS to UNIXCloses #3102 from dan-sf/AIRFLOW-2181,3
"Merge pull request #7 from mistercrunch/minor_touchupsMinor fixes, utility functions",1
Bugfix around saving a chart,2
"[AIRFLOW-6250] Ensure on_failure_callback always has a populate context (#6812)on_failure_callback almost always want to know the dag_id and taskinstance that failed. These info are in the context passed to on_failure_callback, which is passed in from handle_failure(). However, in some rare scenarios, if handle_failure is called in scheduler_job.py and backfill_job.py, the only argument passed is the error message. context is left as None.So in these cases, on_failure_callback will not even know what's the dag_id of the dag that just failed.This PR fixes this by setting context to get_template_context() if it's not given.",1
Remove first_task_scheduling_delay from Updating.md (#12885)We backported the fix to 1.10.14rc4 so this is no longer a breaking change,4
[AIRFLOW-6330] Show cli help when param blank or typo (#6883)When enter Airflow cli with blank parameteror typo parameter or wrong parameter willshow Airflow cli help just like enter`Airflow [command] -h` command,2
[AIRFLOW-3123] Use a stack for DAG context management (#3956),2
Type in CONTRIBUTING.md,5
Organizing gitignore,2
Minor touchup to operator constructor decorator,1
Updating TODO.md,2
use num_shards instead of partitions to be consistent with batch ingestion,1
"check DAGs for None before the loop, to allow [None] escape",1
Update command from breeze to command (#24741),5
"Merge pull request #1204 from Glassdoor/masteradded Glassdoor to ""who uses airflow""",1
[AIRFLOW-1107] Add support for ftps non-default portCloses #2240 from jesusfcr/ftps,1
[AIRFLOW-6259] Reset page with each new search (#6828)Search incorrectly persists the page number which can confusinglyyield no results when making searches from 2nd page onwards,1
Check for run_id for grid group summaries (#24327),1
Fix typo in chart docs (#21814),2
Fix mypy apache beam operators (#20610),1
Migrate files to ts (#25267),2
Merge pull request #2786 from x/postgres_to_bigquery_operator,1
Update CODEOWNERS (#20152)The Changes are self-explanatory in the diff :),4
Add The Climate Corporation to user list (#9726),1
Alembic migration filename should match revision id (#21621)This is the one mismatch we have.,2
Remove duplicated py37 in dev/breeze/pyproject.toml,5
Use new logging options on values.yaml (#13173),2
[AIRFLOW-XXX] Fix python3 and flake8 errors in dev/airflow-jiraThis is a script that checks if the Jira's marked as fixed in a releaseare actually merged in - getting this working is helpful to me inpreparing 1.10.1,1
"Add dev tool to review and classify cherry-picked commits (#21032)Until we have Towncrier, this is a useful tool to classify commitsto one of three categories (in v*-test) branches1) a/add - add to milestone2) d/doc - doc-only change3) e/excluded - change that is skipped from changelog (dev tools)This is done via label and milestone assignment.We can also skip the PR or quit.Information about the PR is nicely printed including its currentlabels and URL that allows to quickly review the PR in question.",1
Restore description for provider packages. (#11239)The change #10445 caused empty descriptions for all packages.This change restores it and also makes sure package creation workswhen there is no README.md,2
[AIRFLOW-5944] Rendering templated_fields without accessing DAG files (#6788),2
Fix comment in Helm Chart for worker ``logGroomerSidecar`` (#17742)It said `scheduler` instead of `worker`,1
Merge pull request #3665 from XD-DENG/patch-6[AIRFLOW-2825] Fix S3ToHiveTransfer bug due to case,0
[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7032),1
Webserver: Further Sanitize values passed to origin param (#12459)Follow-up of https://github.com/apache/airflow/pull/10334,2
v0.3.0.1,5
Make sure the cfg can be copied in any travis environment,5
"[AIRFLOW-2864] Fix docstrings for SubDagOperator (#3712)The docstrings are currently in the `__init__` method, due to which they are not automatically shown in the Sphinx documentation.",2
[AIRFLOW-XXX] Update company name and user admin of the platform (#6528),1
"Fix connection add/edit for spark (#8685)connection add/edit UI pages were not working correctly for Spark connections. The root-cause is that ""spark"" is not listed in models.Connection._types.So when www/forms.py tries to produce the UI,""spark"" is not available and it always tried to ""fall back"" to the option listwhose first entry is ""Docker""In addition, we should hide irrelevant entries forspark connections (""schema"", ""login"", and ""password"")",4
"Rename backport packages to provider packages (#11459)In preparation for adding provider packages to 2.0 line weare renaming backport packages to provider packages.We want to implement this in stages - first to rename thepackages, then split-out backport/2.0 providers as part ofthe #11421 issue.",0
[AIRFLOW-3327] Add support for location in BigQueryHook (#4324),1
Optimization that gets a list of task instances to skip as a batch from the db,5
Better doctest integration,3
Adding ssh connection type to webform,1
"Add `--clean-build` option for breeze build-docs (#24951)This option removes all previously generated docs files so thatbuild docs can run using clean state. Prevents cases where localinventory has been updated from local providers rather thanfrom released ones, breaking the docs building.",2
[AIRFLOW-1185] Fix PyPi URL in templatesCloses #2283 from maksim77/master,0
[AIRFLOW-2412] Fix HiveCliHook.load_file to address HIVE-10541HiveCliHook.load_file doesn't actually executeLOAD DATA statement via beeline bundled withHive under 2.0 due to HIVE-10541.This PR provides a workaround for this problem.Closes #3327 from sekikn/AIRFLOW-2412,0
[AIRFLOW-6615] Remove double sorted in task_list CLI (#7240),4
Update instructions to signify commands as breeze environment commands (#17366)The earlier instructions indirectly indicate that the commands that havebeen modified by this commit will be executed outside the breezeenvironment. Update the instruction to be more clear and also indicatethat these will be executed as airflow commands within the breeze CIenvironment.,5
[AIRFLOW-1615] SSHHook: use port specified by ConnectionCloses #3056 from CaptainYANG/master,1
"Adding an endpoint to get the raw config, will be used to automate setting up a sandbox",5
test env variables,3
Improve grid rendering performance with a custom tooltip (#24417)* fully custom tooltip* use a customized chakra tooltip* update notice,5
"[AIRFLOW-3205] Support multipart uploads to GCS (#4084)* [AIRFLOW-3205] Support multipart uploads to GCSCloud Storage supports resumable/multipart uploads for large files,which can be used to avoid limitations on the size of a single HTTPrequest, or by adding a retry behaviour, increase the reliability oflarge transfers.* [AIRFLOW-3205] Use only the multipart keywordThis removes the chunksize keyword, using instead multipart=True for adefault chunk size or multipart=int to override the default.",1
Update INTHEWILD.md (#23892),5
Add Google Cloud Memorystore Memcached Operators (#10121)Co-authored-by: Tobiasz Kdzierski <tobiasz.kedzierski@polidea.com>Co-authored-by: Kamil Bregua <mik-laj@users.noreply.github.com>,1
"Lower the recommended disk space requirements (#19775)The recommended disk space requirements for Breeze were set to40GB which is way to high (and our Public Runners do not have thatmuch of a disk space - this generated false warnings).Lowering it to 20GB should be quite enough for most ""casual"" users.",1
Add Colgate-Palmolive to users list (#7848),1
Cleanup imports,2
"Mounting from sources is disabled for tests (#10472)We had to enable mounting from sources for a short whilebecause we had to find a way to add new scripts to the""workflow_run"" workflow we have. This also requiresthe #10470 to be merged - perf_kit to be moved to tests.utils becauseit was in a separate directory and image without mounting sourcescould not run the tests.It also partially addresses the #10445 problem wherethere was difference between sources in the image and comingfrom the master. This comes from GitHub running merge onnon-conflicting changes in the PR and something that willbe addressed shortly.The issue #10471 discusses this in detail.",0
[AIRFLOW-XXX] Fix undocumented params in S3_hookSome function parameters were undocumented. Additional docstringswere added for clarity.,1
"Use ParamSpec to replace ... in Callable (#25658)For whatever reason, this makes PyLance able to infer the wrappedfunction's signature, which is a nice improvement.",1
Updated provider's releasing instructions after 27.02.2021 release (#14514)* Updated provider's releasing instructions after 27.02.2021 release* Update dev/README_RELEASE_PROVIDER_PACKAGES.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
[AIRFLOW-1907] Pass max_ingestion_time to Druid hookFrom the Druid operator we want to pass themax_ingestion_time to thehook since some jobs might take considerably moretime than the othersBy default we dont want to set a max ingestiontime.Closes #2866 from Fokko/AIRFLOW-1907-pass-max-ingestion-time,4
Merge pull request #622 from RealImpactAnalytics/fix_migration_scriptFixes for SQLite support,1
"Fix failing test on MSSQL (#17334)This change fixes the failing test on MSSQL by disposing and creating a new connectionso as to use the new connection to make queriesAlso, some other flaky tests were resolved",0
"Move CTAS logic into a CTAS function in db pre-upgrade (#22791)Will be reusing this in later PRs that need to do the same thing, but doing in separate PR for ease of review.",1
Update to new helm stable repo (#12137)Switch out deprecated helm repo for new stable repo.- https://www.cncf.io/blog/2020/11/05/helm-chart-repository-deprecation-update/- https://helm.sh/docs/faq/#i-am-getting-a-warning-about-unable-to-get-an-update-from-the-stable-chart-repository,5
Unbreak main after missing classes were added (#23819),1
[AIRFLOW-3767] Correct bulk insert function (#4773)* [AIRFLOW-3767] Correct bulk insert functionFix Oracle hook bulk_insert bug whenparam target_fields is None or rowsis empty iterable* change without overwriting variables as Fokko said,4
Fix skipping non-GCS located jars (#22302)* Fix #21989 indentation. A test is added to confirm job is executed on DataFlow with local jar file.Co-authored-by: Kyaw <kyawtuns@gmail.com>,2
Do not include testing and directories in coverage reporting,3
[AIRFLOW-XXXX] Remove duplicate line from breeze (#7491),4
add guidance re yarn build for local virtualenv development (#9411),1
"When exec fails in breeze we do not print stack-trace (#23342)When you run exec and breeze is not running, there was a stacktrace printed rather than straightforward error message.This fixes it - stacktrace is only printed now when verbose isused. If not just error message is printed.",0
"[AIRFLOW-4343] Show warning in UI if scheduler is not running (#5127)Now that the webserver is more stateless, if the scheduler is notrunning the list of dags won't populate, making it harder for newstarters to work out what is going on.New dep is BSD-2 which is Cat-A under ASF",1
Update changelog to the latest (#18811)Adds recent changes from main too,4
Docs: Fix task order in overview example (#21282),0
Remove reimported AirflowException class (#9525)It is imported at the top of the file and L1060 too,2
Fix typo for store_serialized_dags config (#7952),5
"[AIRFLOW-1817] use boto3 for s3 dependencySince S3Hook is reimplemented based on the AwsHookusing boto3, its package dependencies need to beupdated as well.Closes #2790 from m1racoli/fix-setup-s3",0
[AIRFLOW-1810] Remove unused mysql import in migrations.Closes #2782 from MortalViews/master,2
[AIRFLOW-5253] Move GCP KMS to core (#5859)This commit moves GCP KMS from contrib to core.For more information check AIP-21.,5
"Bugfix: resources in `executor_config` breaks Graph View in UI (#15199)closes https://github.com/apache/airflow/issues/14327When using `KubernetesExecutor` and the task as follows:```pythonPythonOperator(    task_id=f""sync_{table_name}"",    python_callable=sync_table,    provide_context=True,    op_kwargs={""table_name"": table_name},    executor_config={""KubernetesExecutor"": {""request_cpu"": ""1""}},    retries=5,    dag=dag,)```it breaks the UI as settings resources in such a way is only therefor backwards compatibility.This commits fixes it.",0
[AIRFLOW-1535] Add service account/scopes in dataprocCloses #2546 from fenglu-g/master,5
Replace deprecated dummy operator path in test_zip.zip (#13172)Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`:```from airflow.operators.dummy_operator import DummyOperator```with```from airflow.operators.dummy import DummyOperator```,1
action logging anonymous user case,1
[AIRFLOW-5183] Preprare documentation for new GCP import paths (#5791),2
[AIRFLOW-3714] Correct DAG name in docs/start.rst (#4550),2
Upgrade FAB to 4.1.3 (#24884)no relevant changes found when comparing `airflow/www/fab_security` with FABsee https://github.com/dpgaspar/Flask-AppBuilder/compare/v4.1.2...v4.1.3,4
Support insecure mode in SnowflakeHook (#20106)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add more info on dry-run CLI option (#9582)fixes  https://github.com/apache/airflow/issues/9561,0
Adds --install-wheels flag to breeze command line (#11317)If this flag is specified it will look for wheel packages placed in distfolder and it will install the wheels from there after installingAirflow. This is useful for testing backport packages as well as in thefuture for testing provider packages for 2.0.,1
closes apache/incubator-airflow#1497 *closed for inactivity*,5
"Fix failing doc build (#14986)- For some reason pymongo's stable inventory fetch is redirecting, I can reproduce it locally too if I try to access https://pymongo.readthedocs.io/en/stable/objects.inv from by browser.",2
"[AIRFLOW-5615] Reduce duplicated logic around job heartbeating (#6311)Both SchedulerJob and LocalTaskJob have their own timers and decide whento call heartbeat based upon that. This makes those functions harder tofollow, (and the logs more confusing) so I've moved the logic to BaseJob",2
Update to the README such as adding Hootsuite,1
[AIRFLOW-3192] Remove deprecated post_execute logic (#4040),2
"Run ""third party"" github actions from submodules instead (#13514)Rather than having to mirror all the repos we can instead usegit submodules to pull in the third party actions we want to use - withrecent(ish) changes in review for submodules on GitHub we still get thesame ""review/audit"" visibility for changes, but this way we don't haveto either ""pollute"" our repo with the actions code, nor do we have tomaintain a fork of the third party action.",4
"Constraints generation runs regardless from test status (#11838)Constraints generation did not run when tests were failingbut this was wrong. In case of tests that fail due to constraintsupgrade we neeed to see the output of constraint generation, toknow which constraints failed. Only pushing the constraintshould be limited to the case where everything succeeded.This should make investigation of problems similar to #11837 mucheasier.",0
Clarify `reattach_on_restart` behavior (#23377),5
[AIRFLOW-2306] Add Bonnier Broadcasting to list of current usersCloses #3206 from wileeam/add-bbr-to-readme,1
Add hook_params in SqlSensor using the latest changes from PR #18718. (#18431),4
Use resource and action names. (#16410),1
Adjust DAG/TI details panel (#22877)* Clean up Dag details* Clean up TI details* Add mapped task count,1
raise error instead of returning,0
"Impoving the docs, adding autodocs for command line",2
TextToSpeech assets & system tests migration (AIP-47) (#23247),3
Switches backend to sqlite when building images (#22140)The backend is now checked at execution so we should notset it to postgres when postgres DB is not started.,5
[AIRFLOW-4900] Resolve incompatible version of Werkzeug (#5535)* [AIRFLOW-4900] Resolve incompatible version of WerkzeugPin Werkzeug version to >= 0.15 as flask requirement needs it.* Update setup.py,1
closes apache/incubator-airflow#1425 *Closed for inactivity*,5
Re-serialize all DAGs on 'airflow db upgrade' (#24518),5
Fix to check if values are integer or float and convert accordingly. (#21277)This code will prevent the loss of data if the value is a float it will convert to float if it is not then int.  It will use pd.Float64Dtype() for floats instead of using the the pd.Int64Dtype(). Since there could be floating-point values in the array this will fix the exception for safely casting the array to data type.fixes error when using mysql_to_s3 (TypeError: cannot safely cast non-equivalent object to int64) #16919,0
Fix occasional test failures where pid matches part of another PID (#24584)Solves problems that intermittently fail our builds:```  '135' is contained here:    ith pid: 10135  ?            +++```,0
[AIRFLOW-6537] Fix backticks in rst files (#7140),2
Remove redundant single quote from Breeze build image script (#18173)Before:```ERROR: Bad value for install-airflow-version: '2.2.0b1'. Only numerical versions allowed for PROD image here'!```After:```ERROR: Bad value for install-airflow-version: '2.2.0b1'. Only numerical versions allowed for PROD image here !```,1
"Revert ""Making the dependency engine more flexible""",1
Add support for impersonation in GCP hooks (#9915)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,1
[AIRFLOW-XXX] Improve linking in docs (#6244),2
Fix failing test in TestSchedulerJob (#12906)This change was missed in https://github.com/apache/airflow/pull/12899,4
"Remove the ability to add hooks to airflow.hooks namespace (#12108)Hooks do not need to live under ""airflow.hooks"" namespace for them towork -- so remove the ability to create them under there in plugins.Using them as normal python imports is good enough!We still allow them to be ""registered"" to support dynamically populatingthe connections list in the UI (which won't be done for 2.0)Closes #9507",1
[AIRFLOW-XXX] Add Strongmind to the ReadmeCloses #3655 from tomchapin/master,1
Remove redundant parentheses (#19846),4
Fix MyPy errors in `dev/*` (#20261),0
[AIRFLOW-5254] Move GCP Tasks to core (#5860)This commit moves GCP Tasks from contrib to core.For more information check AIP-21.,5
Add enviromnment variables important for secured minicluster,5
Merge pull request #868 from CloverHealth/encrypt_logsEncrypt logs,2
"Add docs for new scheduler ""clean-up"" tunables. (#12899)",4
[AIRFLOW-4391] Fix tooltip for None-State Tasks in 'Recent Tasks' (#5909),0
Teardown of webserver tests is not picky about processes. (#11616)Fixes random failure when processes are still runningon teardown of some webserver tests. We simply ignor thatafter we send sigkill to those processes.Fixes #11615,0
"Increase timeouts even longer for on_kill test (#20056)Seems that when the system is busy, the timeouts we had towait for tasks to start were a bit to short. Increasing them.Related to #20054",1
Reducing sqla connection pool,5
[AIRFLOW-5906] Add authenticator parameter to snowflake_hook (#8642),1
Add Handy to list of users,1
Extend documentation for states of DAGs & tasks and update trigger rules docs (#21382),2
[AIRFLOW-XXX] Add Chao-Han to committer list (#5846),1
correct state mentioned in logs,2
[AIRFLOW-XXXX] Fix typo in BREEZE.rst (#7556)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Don't use the root logger in KPO _suppress function (#23835),1
[AIRFLOW-3484] Fix Over-logging in the k8s executor (#4296)There are two log lines in the k8sexecutor that can cause schedulers to crashdue to too many logs.,2
Move last chart test to `tests` (#22198)This test was added after the rest of the tests were moved into `tests/charts`.,3
[AIRFLOW-XXX] add Asana to companies list (#4711),1
"Update initialize-database.rst (#11109)* Update initialize-database.rstRemove ambiguity in the language as only MySQL, Postgres and SQLite are supported backends.* Update docs/howto/initialize-database.rstCo-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>",5
Merge pull request #1345 from r39132/masterFixing misnamed PULL_REQUEST_TEMPLATE,0
[AIRFLOW-6381] Remove styling based on DAG id from DAGs page (#6985),2
[AIRFLOW-XXX] Alphabetical table and remove duplicate (#6487),4
Enable AWS Secrets Manager backend to retrieve conns using different fields (#18764),1
"Switches to better BATS asserts (#10718)BATS has additional libraries of asserts that are much morestraightforward and nicer to write tests for bash scriptsThere is no dockerfile from BATS that contains those, so wehad to build our own (but it follows the same structureas #9652 - where we keep our dev docker imagesources inside our repository and the generated docker imagesin ""apache/airflow:<tool>-CALVER-TOOLVER format.We have more BATS unit test to add - following #10576and this change will be of great help.",4
Add recipes for installing a few common tools in Docker image (#13655),2
"Pass exception to ``run_finished_callback`` for Debug Executor (#17983)When running the Debug Executor, the context inside the `on_failure_callback` method doesn't have the `exception` object.This is because the `exception` is not passed to `run_finished_callback` with the Debug Executor",0
Merge pull request #85 from airbnb/backfill_loggingImproved backfill progress logging info,5
fix: instance name env var (#16749),0
CI: Remove ``sleep`` from Static Check Step (#16178)I think this was added as a DEBUG step which was forgotten to removein https://github.com/apache/airflow/pull/15944,4
Commenting the default cfg file,2
Calculate duration in UI (#23259)* Calculate duration in UI* calculate mapped instance duration too,2
[AIRFLOW-1489] Fix typo in BigQueryCheckOperatorCloses #2501 from mrkm4ntr/airflow-1489,1
[AIRFLOW-3458] Move models.Connection into separate file (#4335),2
Rename `OpsgenieAlertOperator` to `OpsgenieCreateAlertOperator` (#20514),1
Skip some tests for Databricks from running on Python 3.10 (#22221)This is a temporary measure until the Databricks SQL libraryis released in Python 3.10 - compatible version.Related to: #22220,5
"Don't try to expand mapped tasks if they can't run (#22155)This lead to the case where we tried to expand a mapped task when itsupstream was failed, but we hadn't yet set the (unexpanded) mapped TI toUPSTREAM_FAILED.In order to support this behaviour we need to add a new ""ignore"" flag toDepContext -- not the best pattern, but we should follow it for now",1
Fixed automated check for image rebuild (#7912),0
Remove duplicated entries in changelog (#19331)Amazon provider had wrong tag set (2.3.0 pointed to rc1 insteadof rc2) which resulted in duplicated entries in changelog.This PR fixes it,0
Add sample dag and doc for S3ListPrefixesOperator (#23448)* Add sample dag and doc for S3ListPrefixesOperator* Fix static checks,0
"Fix docker behaviour with byte lines returned (#21429)* Fix docker behaviour with byte lines returnedThe fix from #21175 did not actually fix the logging behaviourwith non-ascii characters returned by docker logs when xcompush was enabled. The problem is that DockerOperator usesdifferent methods to stream the logs as they come (using attachstream) and different method to retrieve the logs to actuallyreturn the Xcom value. The latter uses ""logs"" method of dockerclient. The tests have not caught it, because the two methodswere mocked in two different places.This PR uses the same ""stringify()"" function to convert both""logged"" logs and those that are pushed as xcom. Also addedtest for ""no lines returned"" case.Fixes: #19027* Update tests/providers/docker/operators/test_docker.pyCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
Replace remaining decorated DAGs reference (#12299)`decorated DAGs` -> `DAGs with Task Flow API`,2
[AIRFLOW-6751] Pin Werkzeug < 1.0.0 release - 1.0.0 is not compatible (#7377),5
`WebHDFSHook` Bugfix/optional port (#24550),0
"Add CI jobs and tooling to aid with tracking backtracking pip issues (#21825)* Add CI jobs and tooling to aid with tracking backtracking pip issuesThis is a follow-up after investigation done with failing mainbuilds (resulting in #21824 and tracked inhttps://github.com/pypa/pip/issues/10924)Handling failure of image building has been also improved as partof the PR. Instead of separate ""cancel"" job (which did not reallywork anyway) we build and push empty images instead and theempty images are handled in the ""wait for images"" jobwith appropriate message.* Update dev/breeze/src/airflow_ci/find_newer_dependencies.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Add more strict Helm Chart schema checks for image pullPolicy & dags accessMode (#15040)`values.schema.json` can be used to help check the structure & the values in `values.yaml`. - In this PR I extend the `values.schema.json` to ensure we have more strict checks on the values users add in `values.yaml`.- Tests are added.,1
"Get all ""tags"" parameters not just one (#12324)",2
[AIRFLOW-4959] Add .hql support for the DataProcHiveOperator (#5591),5
Merge pull request #154 from mistercrunch/s3sensor_logfixs3 sensor fix logging,2
More conservative connection pooling,5
[DOC] update README 2.3.3 (#25016),5
Handle outdated webserver session timeout gracefully. (#12332),5
"Skip updating constraints when only datetime changes (#19023)After adding datetime to generated constraints it became possiblethat constraints remained the same but generated constraint fileschanged (because of the date time). It was a rare occurencebecause we rarely had ""all-green"" build in `main`, but since wemanaged to fix most of the flaky tests it became more probable(and happened already several times).This PR ignores comment files when comparing generated constraintsand commits should only happen if something else than generatedcomment changes.",4
Fix changelog for Azure Provider (#18736)One of the commits has wrong description (Initial Commit) and hasbeen removed from the changelog by mistake.This PR adds it back,1
Remove code duplication in the test suite test_views_acl.py (#20887),3
"Check and run migration in commands if necessary (#18439)This PR adds db initialization and upgrade in webserver startup.When the webserver is started, we check the migrations and decide whetherto initialize the database or upgrade it.",5
Fix Amazon SES emailer signature (#21681)Amazon SES Had a wrong signature to become a mailer in Airflowintroduced in #18042. As the result setting SES emailer asemail backend resulted in:```TypeError: send_email() missing 1 required positional argument:'html_content'```Fixes: #21671,0
Doc: Improve installing from sources (#18194)Based on https://github.com/apache/airflow/pull/18187/#discussion_r706861500,1
"[AIRFLOW-6460] - Reverting ""Reduce timeout in pytest (#7051)"" (#7062)This reverts commit bf3fa1fda2f45946e2bf4b2d3377a5c0bc23664d.",4
Merge pull request #946 from wil5for/dag_state_for_cliAdd dag_state to cli,2
Make sure to be py3 compatible,1
Merge pull request #787 from neovintage/password_auth_docsPassword auth docs,2
[AIRFLOW-XXX] Add more AWS transfer operators (#6205),1
Add autocommit property for snowflake connection (#10838),5
[AIRFLOW-5246] Remove unused source constructor parameter in BaseHook,1
Integrate Breeze2 documentation together (#20836),1
Upgrade more javascript files to typescript (#24715)* upgrade more js files to ts* upgrade more component files* fix linting issues,0
[AIRFLOW-6792] Remove _operator/_hook/_sensor in providers package and add tests (#7412),3
Add common parameter to example URI for MSSQL (#18404),2
"Unset PIP_USER in prod entrypoint. (#15774)In Airflow image we are setting PIP_USER variable to true, in order toinstall all the packages by default with the ``--user`` flag. Howeverthis is a problem if a virtualenv is created later which happens inPythonVirtualenvOperator. We are unsetting this variable here, so thatit is not set when PIP is run by Airflow later on.Fixes: #15768",0
Removing ftp related files,2
Fix CLI 'kubernetes cleanup-pods' which fails on invalid label key (#17298)Fix for #16013 - CLI 'kubernetes cleanup-pods' fails on invalid label key,0
[hotfix] fixing landscape requirement detection,1
Remove redundant curly brace from breeze echo message (#11012)Before:``` ./breeze --github-image-id 260274893GitHub image id: 260274893}```After:``` ./breeze --github-image-id 260274893GitHub image id: 260274893```,4
Upgrade to support Google Ads v10 (#22965),1
Stronger language re: SQLite (#12727),5
[AIRFLOW-XXXX] Add useful tips to first PR msg (#7082),1
Fix literal cross product expansion (#23434),0
"allow bq base cursor methods run_extract, run_copy, run_load to alltake in source or destination table strings that include projects.For backwards compatibility reasons, the project is not required.This allows for decoupling of the execution of these methods fromprojects that have the information they access.",5
Minor doc improvements in blob_storage_to_gcs.rst (#11607)- Sentence completion- code-block rendering- Link to english docs for Azure instructions,2
Correctly show rendered templates for mapped task instances. (#22984),2
[AIRFLOW-875] Add template to HttpSensor paramsCloses #2080 from jlowin/httpsensor,2
Make K8sPodOperator backwards compatible (#12384)* Make the KubernetesPodOperator backwards compatibleThis PR significantly reduces the pain of upgrading to Airflow 2.0for users of the KubernetesPodOperator. Users will be allowed to    continue using the airflow.kubernetes custom classes* spellcheck* spelling* clean up unecessary files in 1.10* clean up unecessary files in 1.10* clean up unecessary files in 1.10,2
"[AIRFLOW-XXX] Remove unnecessary ""# noqa"" in airflow/bin/cli.py (#4223)",4
Merge pull request #162 from seibert/no_tlsAdd smtp_starttls flag to config,5
"[AIRFLOW-6471] Add pytest-instafail plugin (#7064)We have a problem currently that if a test fails in CI we do not see thefailures immediately - only when it finishes, but when tests hang, sometimesthe details about failed tests are not shown immediately. We tried to increaseverbosity but it's not very helpful.The pytest-instafail plugin solves the problem without increasing verbosity.",1
Translate non-ascii characters (#17057),5
Check that edge nodes actually exist (#24166),5
[AIRFLOW-1876] Write subtask id to task log headerCloses #2835 from wrp/subtask-id,2
Apache Airflow 2.3.4 has been released (#25913),5
Added SNS example DAG and rst (#21475),2
[AIRFLOW-1983] Parse environment parameter as templateCloses #2928 frompatsak/f/environment_paramater_as_template,2
Improvements for Docker Image docs (#14843)* Improvments for Docker Image docs* fixup! Improvments for Docker Image docs* fixup! fixup! Improvments for Docker Image docs,2
Add classic installation scripts for additional tools (#13587),1
Migrate Influx example DAGs to new design #22449 (#24136)* Migrate Influx example DAGs to new design #22449* Fix static checks,0
Support project_id argument in BigQueryGetDataOperator (#25782),5
Refactor operator links to not create ad hoc TaskInstances (#21285),1
Chart: minor doc fixes (#21189),0
[AIRFLOW-1896] FIX bleach <> html5lib incompatibilityRunning airflow with bleach 2.0.0 can cause:`ImportError: No module named base`https://github.com/mozilla/bleach/issues/267This was resolved in https://github.com/mozilla/bleach/releases/tag/v2.1.2Closes #2858 from kamilchm/patch-1,0
"Hard-remove parallell locks at cleanup (#17514)Some stale parallel locks might be a reason for hanging testsThis is an attempt to hard-remove all the .parallel remnantdirectories in hope that it will avoid some of the hangs.Unfortunately in our approach we try to reuse runners betweenruns which might cause some remnants from the previous runs toimpact subsequent runs. When we move to GCP we will try to spinoff a new runner for every job to make sure the real ""true cleanstate"" is used when running a job.",1
Update Oracle library to latest version (#24311),3
Adding podAnnotations to Redis statefulset (#23708),1
Unnecessary use of list comprehension (#10416)It is better to use list constructor which is faster and more readable,1
[AIRFLOW-5520] Add options to run Dataflow in a virtual environment (#6590),5
Remove unused dependency - contextdecorator (#13455),1
Add Skyscanner to companies list (#5258),1
[AIRFLOW-6924] Fix Google DLP operators return types (#7546),1
[AIRFLOW-1688] Support load.time_partitioning in bigquery_hookCloses #2820 from albertocalderari/master,1
more informative error message,0
Session factory,5
Fix various typos in airflow/cli/commands (#9983),2
Checks-out missing commits on selective checks (#16470)Recent improvement of the build images to use pull_request_targetremoved by accident fetching of the PR commits which makes itimpossible to determine which files has changed in PR in thePR build image workflow.This commit brings it back.,1
Merge pull request #927 from airbnb/fix_adhoc_dr[fix] disregarding adhoc tasks when closing dag runs,1
[AIRFLOW-6575] Entropy source for CI tests is changed to unblocking (#7185)On Travis CI blocking entropy source slows startup time of a number ofcontainers. This change changes the entropy source to unblocking one.,4
Merge pull request #284 from mistercrunch/flower_portTying the flower_port configuration param to the CLI,2
[AIRFLOW-5111] Remove apt-get upgrade (#5722),1
Using CRUD for DAGs view,2
[AIRFLOW-2333] Add Segment Hook and TrackEventOperatorAdd support for Segment with an accompanying hookand anoperator for sending track eventsCloses #3335 from jzucker2/add-segment-support,1
Manage Flask AppBuilder Tables using Alembic Migrations (#12352)closes https://github.com/apache/airflow/issues/9155The Migration is idempotent and allows both upgrade and downgrade.It also takes care of https://github.com/dpgaspar/Flask-AppBuilder/pull/1368i.e. increasing the length of ab_view_menu.name column from 100 to 250,1
added check for unicode type before decoding to fix a decoding bug,0
Fix bug when checking for existence of a Variable (#19395)`check_for_write_conflict` was a `staticmethod` but for some reason it was ignored,5
Add example on airflow users create --help (#10662),1
SSHHook: Using correct hostname for host_key when using non-default ssh port (#15964),1
"Webserver: Sanitize string passed to origin param (#14738)Follow-up of #12459 & #10334Since https://github.com/python/cpython/pull/24297/files (bpo-42967)also removed ';' as query argument separator, we remove query argumentswith semicolons.",4
Dataflow Operators - use project and location from job in on_kill method. (#18699)Reason why we need this is because we can have situation where project_id is set to None but we define it in the dataflow_default_options. Job will start normally without error but in case when we decide to mark running task to different state we will get a error that the job does not exits.,0
"Revert ""Fixes selective tests in case of missing merge commits (#11641)"" (#11646)This reverts commit 4fcc71c2ffbf87585759f49ab7e426ccb9516f87.",4
"Fix log links on graph TI modal (#17862)The graph view should show the ""Download Log"" and ""View Logs in {remotelogging system}"", like is done on the tree view.",5
Merge pull request #546 from bolkedebruin/masterclamp flask-login to 0.2.11,2
Fix how we recurse through collection-like template fields.,0
Adding HdfsSensor operator,1
Push CI image using new Python Breeze (#22888)Fixes: #22821,0
"[AIRFLOW-830][AIRFLOW-829][AIRFLOW-88] Reduce Travis log verbosity[AIRFLOW-829][AIRFLOW-88] Reduce verbosity ofTravis testsRemove the -s flag for Travis unit tests tosuppress outputfrom successful tests.[AIRFLOW-830] Reduce plugins manager verbosityThe plugin manager prints all status to INFO,which is unnecessary andoverly verbose.Closes #2049 from jlowin/reduce-logs",2
Fix gpg verification command (#13035),0
"[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6516)* Fixed problem that Kubernetes tests were testing latest master  rather than what came from the local sources.* Kind (Kubernetes in Dcocker) is run in the same Docker as Breeze env* Moved Kubernetes scripts to 'in_container' dir where they belong now* Kubernetes cluster is reused until it is stopped* Kubernetes image is build from image in docker already + mounted sources* Kubectl version name is corrected in the Dockerfile* KUBERNETES_VERSION can now be used to select Kubernetes version* Running kubernetes scripts is now easy in Breeze* We can start/recreate/stop cluster using  --<ACTION>-kind-cluster* Instructions on how to run Kubernetes tests are updated* The old ""bare"" environment is replaced by --no-deps switch",5
"[AIRFLOW-3761] Decommission User & Chart models & Update doc accordingly (#4577)In master branch, we have already decommissioned the Flask-Admin UI.In model definitions, User and Chart are only applicable for the""old"" UI based on Flask-Admin.Hence we should decommission these two models as well.Related doc are updated in this commit as well.",5
"Remove cache for kubernetes tests (#16927)Different python versions are used for different tests for k8sso we should not attempt to cache the venv for tests, otherwisethey will randomly fail.",0
Change location of bucket creation for Datastore (#18569)Co-authored-by: Dmytro Khimich <khimich@google.com>,5
"When marking future tasks, ensure we don't touch other mapped TIs (#23177)We had a logic bug where if you selected ""Map Index 1"" to mark assuccess, the other mapped TIs of that run would get cleared too.This was because the `partial_dag.clear` was picking up the othermapped instances of the task.Co-authored-by: Jed Cunningham <jedcunningham@apache.org>",2
Replace deprecated wtforms HTMLString with markupsafe.MarkUp (#9487)WTForms uses `MarkUp` to escape strings now and removed their internal class HTMLString in Master. Details: https://github.com/wtforms/wtforms/pull/400That change previously broke Airflow for new users (in 2.3.0). However on users request they added `HTMLString` that just passes all args to `markupsafe.MarkUp` back for temporary Backward compatbility with deprecation warning in 2.3.1. Details: https://github.com/wtforms/wtforms/issues/581,0
[AIRFLOW-XXXX] Fix breeze build-docs (#7445),2
Merge pull request #602 from bolkedebruin/conf_multipleImplement ldap authentication including tests,3
[AIRFLOW-5276] remove unused is_in helper function (#5878),1
"[AIRFLOW-1631] Fix local executor unbound parallelismBefore, if unlimited parallelism was used passing`0` for theparallelism value, the local executor would stallexecution since noworker was being created, violating theBaseExecutor contract on theparallelism option.Now, if unbound parallelism is used, processeswill be created on demandfor each task submitted for execution.Closes #2658 from edgarRd/erod-localexecutor-fix",0
Remove remnant kubernetes stuff from breeze scripts (#9138),4
[AIRFLOW-XXX] Add Strava to airflow users list (#4100),1
In case of worktree .git might be a file - rat-check fails with it (#9435),0
[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384),5
"Fix failing docs build on Master (#11951)This was merged in #11472, but for some reason that didn't run tests sothe failure wasn't noticed.a7ad204 fixed 1 error but the docs error was missed",0
Fixed some web dag views test,3
"Add Python 3 compatibility fixIn Python 3, errors dont have a `message` attribute",0
"[AIRFLOW-3779] Don't install enum34 backport when not needed (#4620)https://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependenciesInstalling this in more recent versions causes a ""AttributeError: module'enum' has no attribute 'IntFlag'`"" in re.py",0
"[AIRFLOW-1812] Update logging exampleThe logging has changed, therefore we should alsoupdate theupdating.md guideCloses #2784 from Fokko/AIRFLOW-1812-update-logging-example",5
Fix - TestSchedulerJobQueriesCount::test_process_dags_queries_count (#12273),3
Improve Cloud Memorystore for Redis example (#11735),1
"[Airflow-15245] - passing custom image family name to the DataProcClusterCreateoperator (#15250)* [airflow-15245] - custom_image_family added as a parameter to DataprocCreateClusterOperatorSigned-off-by: ashish <ashishpatel0720@gmail.com>* [airflow-15245] - test added to check both custom_image and custom_image_family must not be passedSigned-off-by: ashish <ashishpatel0720@gmail.com>* [airflow-#15245] - typo fixed in documentationSigned-off-by: ashish <ashishpatel0720@gmail.com>* [Airflow-15245] - comments updated, more info provided.* [Airflow-15245] - sanity check added for image_version and custom_image_family.* Update airflow/providers/google/cloud/operators/dataproc.pyCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>* Apply suggestions from code reviewCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>* [Airflow-15245] - added a test case to verify the generated cluster config is as expected with custom_image_family and single_node.* Remove print() from test caseCo-authored-by: Ashish Patel <Ashish.Patel@walmartlabs.com>Co-authored-by: Xinbin Huang <bin.huangxb@gmail.com>",3
Add call to Super call in apache providers (#7820),1
[AIRFLOW-1835] Update docs: Variable file is jsonSearching through all the documentation I couldn'tfind anywherethat explained what file format it expected foruploading settings.Closes #2802 from bovard/variable_files_are_json,5
[AIRFLOW-3917] Specify alternate kube config file/context when running out of cluster (#4859),1
Merge pull request #755 from mtustin-handy/patch-3Add link to airflow blog post from handy,2
"Switch Alibaba OSS tests to us-east-1 (#17616)The tests were using cn-hengzou before which - due to thelimitations in cross-border communication with China behavederratically and caused a number of transient errors.Since GitHub Actions Public runners and our Self-hosted runnersrun in US-east (Azure/AWS), switching to us-east-1 should addressthe stability of tests.Ideally we should switch to mocking, but that might be doneseparately.",3
Add tests around schedule_dag. Make run_unit_tests.sh idempotent. Fix abug with @once dags,2
"Attempt to reduce flakiness of PythonVirtualeEnv test_airflow_context (#17486)We have a global limit (60 seconds) for individual test execution,however 'test_airflow_context' of the Python Virtualenv test might take longer incase they are run in parallel - because they are using dill serializationincluding a lot of serializable data from the context of the task.We give the test 120 seconds to complete now.",3
"Disable yarn-dev in start-airflow command (#19626)When you run start-airflow, by default it also run `yarn dev`command, however if you've never built assets before, yarn devis very slow first time and the webserver started before the distfolder was even created which caused asset-less airflow experience.There was another race condition even if you did build theassets before. If you run start-airflow on MacOS or Windows whenthe filesystem was slow, there could be a case that yarn devcleaned up the dist folder while webserver was starting andit could lead again to asset-less experience if you were unlucky.Also running `yarn dev` has the side effect of removing the checksumfile which is used to see if any of the assets changed and whetherthey need recompilation. As the result after running `start-airflow`you always got the warning that the assets need recompilation.This PR disables automated start of `yarn dev` and suggests to runit manually instead if there is a need for dynamic assetrecompilation. Also when `start-airflow` is run and we are startingairflow from sources rather than PyPI, asset compilation isexecuted if the checksum is missing or does not match the sourcefiles.Related to: #19566",2
[AIRFLOW-3607] fix scheduler bug related to concurrency and depends on past (#7402)commit 50efda5c69c1ddfaa869b408540182fb19f1a286 introduced a bug thatprevents scheduler from scheduling tasks with the following properties:* has depends on past set to True* has custom concurrency limit,1
[AIRFLOW-XXXX] Add user and DAGs folder notes to BREEZE.rst (#7362)Co-authored-by: Matthew Bowden <bowdenm@spu.edu>,2
Create new databases from the ORM (#24156)This PR opens up to creating new databases from the ORM instead of going through the migration files.`airflow db init` creates the new db.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Wait for pipeline state in Data Fusion operators (#8954)* Wait for pipeline state in Data Fusion operatorsfixup! Wait for pipeline state in Data Fusion operatorsfixup! fixup! Wait for pipeline state in Data Fusion operatorsfixup! fixup! fixup! Wait for pipeline state in Data Fusion operators* Use quote to encode url parts* fixup! Use quote to encode url parts,1
"Fix Celery executor getting stuck randomly because of reset_signals in multiprocessing (#15989)Fixes #15938multiprocessing.Pool is known to often become stuck. It causes celery_executor to hang randomly. This happens at least on Debian, Ubuntu using Python 3.8.7 and Python 3.8.10. The issue is reproducible by running test_send_tasks_to_celery_hang in this PR several times (with db backend set to something other than sqlite because sqlite disables some parallelization)The issue goes away once switched to concurrent.futures.ProcessPoolExecutor. In python 3.6 and earlier, ProcessPoolExecutor has no initializer argument. Fortunately, it's not needed because reset_signal is no longer needed because the signal handler now checks if the current process is the parent.",0
Chart: Allow configuration of pod resources in helm chart (#16425)Currently it's possible to configure some of the pod resources via the helm chart values like for example the scheduler pod main container but it's not possible for the `scheduler-log-groomer` container. In deployments with `ResourceQuotas` it's desirable to be able to control the `limits` of each container specifically to avoid pods reserve way too much cpu and memory  and eat up the quota unnecessarily. Even though the current helm chart allows to add a `LimitRange` that defines the default `limits` that is not enough. This introduces the ability to provide a specific resource blocks for * `pod_template_file`: for the pods started by the `KubernetesPodOperator` itself . ([pod_template_file doc](https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html#pod-template-file))* pgbouncer's `metrics-exporter`* scheduler's  `scheduler-log-groomer`* webserver's initContainer `wait-for-airflow-migrations`* worker's `worker-log-groomer`,2
"Add ""packaging"" to core requirements (#18122)",1
Chart improvements,1
Make celery worker_prefetch_multiplier configurable (#8695),5
[AIRFLOW-521] Add IFTTT as Airflow userWe at IFTTT currently use Airflow to monitor andschedule all our Data Pipelines and ETLs.Closes #1805 from apurvajoshi/patch-1,5
Adding support for multiple task-ids in the external task sensor (#17339)* Adding support for multiple task-ids in the external task sensor* Fixing flake8 errors* Fixing import errors,0
Update tutorial.rst,5
"Fix ""breeze-legacy"" after building images was removed (#23404)The `breeze-legacy` stopped working after building images wereremoved as few parameters were still checked for allowed valuesbut they were missing,This PR fixes it by removing the parameters.",2
"Update pre-commit checks-flynt to 0.66 (#17672)Additionally, we now download flynt configurations from the official repository, which allows us to automatically download updates using the pre-commit autoupdate command",5
[AIRFLOW-2912] Add Deploy and Delete operators for GCF (#3969)Both Deploy and Delete operators interact with GoogleCloud Functions to manage functions. Both are idempotentand make use of GcfHook - hook that encapsulatescommunication with GCP over GCP API.,1
"Speed up `dag.clear()` when clearing lots of ExternalTaskSensor and  ExternalTaskMarker (#11184)This is an improvement to the UI response time when clearing dozens of DagRuns of large DAGs (thousands of tasks) containing many ExternalTaskSensor + ExternalTaskMarker pairs. In the current implementation, clearing tasks can get slow especially if the user chooses to clear with Future, Downstream and Recursive all selected.This PR speeds it up. There are two major improvements:Updating self._task_group in dag.sub_dag() is improved to not deep copy _task_group because it's a waste of time. Instead, do something like dag.task_dict, set it to None first and then copy explicitly.Pass the TaskInstance already visited down the recursive calls of dag.clear() as visited_external_tis. This speeds up the example in test_clear_overlapping_external_task_marker by almost five folds.For real large dags containing 500 tasks set up in a similar manner, the time it takes to clear 30 DagRun is cut from around 100s to less than 10s.",2
Validate DagRun state is valid on assignment (#19898),2
Add aws ses email backend for use with EmailOperator. (#13986),1
HiveServer2 improvements,1
Adding Agari to the list of companies using Airflow in the readme,1
Merge pull request #381 from Arkoniak/mysql_hookAdded charset and cursor functionality to mysql hook,1
[AIRFLOW-7064] Add CloudFirestoreExportDatabaseOperator (#7725),5
"[AIRFLOW-3272] Add base grpc hook (#4101)* [AIRFLOW-3272] add base grpc hook* [AIRFLOW-3272] fix based on comments and add more docs* [AIRFLOW-3272] add extra fields to www_rabc view in connection model* [AIRFLOW-3272] change url for grpc, fix some bugs* [AIRFLOW-3272] Add mcck for grpc* [AIRFLOW-3272] add unit tests for grpc hook* [AIRFLOW-3272] add gRPC connection howto doc",2
[AIRFLOW-6867] Decouple DagBag and TaskInstance (#7488),2
Merge branch 'fix_travis',0
[AIRFLOW-6479] Update celery command calls (#7068),5
[AIRFLOW-5936] Allow explicit get_pty in SSHOperator (#6586),1
[AIRFLOW-6051] Make DAG optional during displaying the log (#6650),2
[AIRFLOW-6144] Improve the log message of airflow scheduler (#6710),2
[AIRFLOW-1076] Add get method for template variable accessor (#6793)Support getting variables in templates by string. This is necessary whenfetching variables with characters not allowed in a class attributename. We can then also support returning default values when a variable doesnot exist.,1
Static check in Breeze2 (#20848),5
Prepares documentation for rc2 release of Providers (#16501)* adds clear information that the provider is for 2.1+* adds explicit dependency to apache-airflow>=2.1.0 in dependency list* adds capability of specifying additional dependencies* different providers now can depend on different Airflow version* removed pre-commit check and provider info update for  provider-schema 2.0.0 compatibility  (not needed any more after >= 2.1.0 is used as Airflow >2.0.1  allows additional properties in provider_info)* Update changelog documentation for all providersCo-authored-by: jarek <jarek@penguini>,5
[AIRFLOW-XXX] Extract reverse proxy info to a separate file (#4657),2
Deprecate Read the Docs (#12541),2
Improve argument handling in entrypoint_prod.sh (#16258),1
Merge pull request #879 from thibault-ketterer/readme_updateReadme update,5
[AIRFLOW-2095] Add operator to create External BigQuery Table- Added operator to create External BigQuery Table- Added documentation- Added tests- Fixed documentation for GCSCloses #3028 from kaxil/bq-external-tb-op,2
Merge pull request #287 from airbnb/druidImproving the HiveToDruidOperator,1
Glue system test cleanup (#25966),4
"[AIRFLOW-XXXX]: Set test env vars in confttest.py, not Breeze entrypoint (#7164)These environment variables need to be set beforecollection/file-parsing happens otherwise the test collection will failwith a KeyError.When running under breeze these were set in the entrypoint, but to makenon-breeze behave the same I have moved them out of there and to the toplevel of conftest.py.",5
"Check for missing dagrun should know version (#22752)When we apply the pre-upgrade check for tables with records missing a matching dagrun record, we create a temp table, and we name the temp table with a suffix including the version where the migration appears.  Tables added to the check after Airflow 2.2 were using the wrong version number.",0
"Unified ""dash-name"" convention for outputs in ci workflows. (#24802)There were errors with retieving constraints branch caused byusing different convention for output names (sometimes dash,sometimes camelCase as suggested by most GitHub documents).The ""dash-name"" looks much better and is far more readable sowe shoud unify all internal outputs to follow it.During that rename some old, unused outputs were removed,also it turned out that the new selective-check canreplace previous ""dynamic outputs"" written in Bash as well.Additionally, the ""defaults"" are now retrieved via Python script, notbash script which will make it much more readable - both build_imagesand ci.yaml use it in the right place - before replacingthe scripts and dev with the version coming in from PR in caseof build_images.yaml.",1
"Refactor plugins command output using AirflowConsole (#13036)This PR refactors the airflow plugins command to be compatible with'output' parameter which allows users to get output in form of table,json or yaml.",5
[AIRFLOW-XXX] Clarify documentation related to autodetect parameter in GCS_to_BQ Op (#5294),2
Merge pull request #225 from mistercrunch/run_remote_worksRemote run won't pickle by default anymore,1
"[AIRFLOW-264] Adding workload management for HiveDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-264CC: Original PR by Jparks2532https://github.com/apache/incubator-airflow/pull/1384Add workload management to the hive hook and operator.Edited operator_helper to avoid KeyError on retrieving conf values.Refactored hive_cli command preparation in a separate privatemethod.Added a small helper to flatten one level of an iterator to a list.Closes #1614 from artwr/artwr_fixing_hive_queue_PR",0
Add default weight rule configuration option (#18627)* Add default weight rule configuration option,5
[CASSANDRA-16814] Fix cassandra to gcs type inconsistency. (#17183),0
"Revert ""Build CI images for the merge result of a PR, not the tip of the PR (#18060)"" (#18063)This reverts commit 1bfb5722a8917cbf770922a26dc784ea97aacf33. (temporarily until we add gh to our AMI)",1
"Remove errors raised during initialiation of virtualenv (#10896)Our migration scripts are written in the way that if youare running them to reset the db and you have some files inexample_dags or dags, or some plugins in your plugins folder,you will get various kinds of errors.Those errors are printed to the user and even if they areultimately ignored by the SqlAlchemy migration process,they are confusing whether the database reset(and thus the whole initialize-virtualenv) were successfulor not.This change disables any DAGs that could be loaded during theDB reset (in case of running './breeze initialize-virtualenv'.Fixes #10894",5
Merge pull request #500 from DinoCow/persistent-state-focus-on-clickAdded ability to filter node beyond mouse over by clicking on the state legend,1
"Add different modes to sort dag files for parsing (#15046)This commit adds the feature to allow users to set one of the following modes, the scheduler will list and sort the dag files to decide the parsing order.:- `modified_time`: Sort by modified time of the files. This is useful on large scale to parse the recently modified DAGs first.- `random_seeded_by_host`: Sort randomly across multiple Schedulers but with same order on the same host. This is useful when running with Scheduler in HA mode where each scheduler can parse different DAG files.- `alphabetical`: Sort by filename",2
[AIRFLOW-6111] [AIP-21]  Rename GCP spanner operator and hook (#7004),1
Making operator constructor leaner,1
`2.3.1` has been released (#23912),5
Remove Travis CI badge from README (#9074)* Remove Travis CI badge from README* fixup! Remove Travis CI badge from README,4
Merge pull request #938 from airbnb/airbnb_prod[hotfix] dag missing from dagbag,2
Better logging for the scheduler,2
Small fixes on the TabularConnection (#24874),0
Add D204 pydocstyle check (#11031),2
"Fix entire DAG stops when one task has end_date (#20920)related #19917 , #20471",5
"Make DagRunType inherit from `str` too for easier use. (#11621)This approach is documented in https://docs.python.org/3.6/library/enum.html#others:```While IntEnum is part of the enum module, it would be very simple toimplement independently:class IntEnum(int, Enum):    pass```We just extend this to a str -- this means the SQLAlchemy has no troubleputting these in to queries, and `""scheduled"" == DagRunType.SCHEDULED`is true.This change makes it simpler to use `dagrun.run_type`.",2
Add securitySchemes in openapi spec (#10652)openapi-generator relies on this component to generate auth code insome of the clients.,1
Default for autocommit,5
[AIRFLOW-XXXX] Add Geekie to Airflow Users list (#7434),1
Add stack overflow link to Github Issues (#12407),0
Merge pull request #82 from airbnb/fix_hive_hook_to_use_internal_tableUse internal table. Using external was leading to duplicates.,1
Improvements for transfer operators references (#12482),1
[AIRFLOW-5064] Switched to python 3.5 (#5678),5
Update Param example code to add a default (#23212)A param without a default value causes import error. Updated this example tobe runnable.,1
"Merge pull request #1135 from RvN76/masterAdded start_date initialization for DagRun creation within schedule_dag(self, dag_id)",2
"Further improves image caching for Breeze (#22625)In order to allow ""big"" rebuilds we remove the image tagbefore rebuilding, to make sure that the remote and not local imageis used as source of cache. This is consequence of using inlinedcache.Files are added to the Python Base image image - the baselayers will be shared with other stages but we will make surethat the script stages are different for different platforms.Also - when there is no image at all we fail pre-commit. Thisshould handle the situation when we tried to build the image andstopped it in-between.Hadolint released a new version of their checker - with supportfor the new Dockerfile buildkit features and native support forARM so we are enabling it back.See https://github.com/hadolint/hadolint/pull/803Finally we still need some files that we cannot inline, becausethey are only needed for source build and they are too longto inline (yarn.lock for example). In order to keep the cacheworking for all umasks we need to bring back group fixing.",0
add missing help text (currently it reprints an earlier text),1
Fixed failing Kubernetes tests after deny_all for experimental API (#9647)The tests were broken by #9611,3
[AIRFLOW-7041] make bowler dependency local (#7691),1
Code smell fixes for BackfillJob (#12005)- merge IFs when we can;- One of the IF condition check should reuse STATES_COUNT_AS_RUNNING which has been already defined;- Remove unused parameter for intermediate function _per_task_process(),1
Merge pull request #628 from RealImpactAnalytics/ria/run_travis_on_all_backends_/3Now executing Travis build also on sqlite,1
First official commit,5
Dump Pod as YAML in logs for KubernetesPodOperator (#9895),1
Add `wait_for_termination` argument for Databricks Operators (#20536),1
Add links for BigQuery Data Transfer (#22280),5
BoringCyborg Bot: Fix Automated Labels for serialized & secrets (#10228)- docs/howto/use-alternative-secrets-backend.rst has been renamed & split into docs/howto/secrets-backend/*- Add more paths for Serialization,1
"[AIRFLOW-1669] Fix Docker and pin Moto to 1.1.19https://github.com/spulec/moto/pull/1048 introduced `docker` as adependency in Moto, causing a conflict as Airflow uses `docker-py`. Asboth packages don't work together, Moto is pinned to the versionprior to that change.",4
"Clean up unnecessary Airflow config in helm chart (#15729)This removes Airflow config from the chart that isn't necessary.The configs are either already the default, don't need to changed from thedefault, or aren't real Airflow configs anyways.",5
Update INTHEWILD.md (#12293),5
