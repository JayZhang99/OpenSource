[BUILD] Enable more visible symbols by default (#3365)	0
Added declare of aluBits for TensorAlu (#4624)	1
[CPP-RPC] Fix GetPath to use relative file path (#12242)* [CPP-RPC] Fix GetPath to use relative file path* change file_path to file_name	2
Consider version too #508 (#514)	5
[Relay][OP] MultiboxPrior (#1882)* Relay MultiboxPrior Operator* Fix lint* Fix build* Add test for default args	3
[TVM] Fix operator!= for Tensor (#1753)	1
Use opencv reisze method for preprocessing of image in darknet (#4883)* Use opencv reisze method for preprocessing of image in darknet* Use opencv reisze method for preprocessing of image in darknet* Fix pylint issues	0
[Relay][VM] Support execution on devices (#3678)* [Relay][VM] Support execution on devices* Reduce Copy calls* Cleanup* Lint* CR comments* Merge test into test_vm.py	3
properly pass through command-line args in docker/bash.sh (#6599)	2
[REFACTOR] topi -> tvm/topi (#6186)This PR migrates the topi library as a sub namespace of tvm.	4
relax reorder primitive's  affineness check (#10887)	5
fix a bug of instance norm. (#9806)	0
Relay reshape reshape_like compute and schedule (#2159)	5
[QNN] Add qnn.rsqrt op (#9982)* Add qnn.rsqrt op* Add comment	1
[ONNX] Add support for QLinearConcat contrib op (#8907)* add qlinearconcat op* fix tests* Fix* lint* lint* review* boop ci* fix regression* noop* jostle ci	0
[CMSIS-NN] Removed redudant arguments to CMSIS-NN wrapper function (#11431)Removed input_scale and filter_scale from CMSIS-NNwrapper function. These are not needed by CMSIS-NNAPI which gets called from the generated C wrapperfunction for Conv2D.	1
Update ci_qemu to v0.10 and Zephyr project generator for Zephyr 2.7 (#10117)This commit updates the ci_qemu image, in order to do this the Zephyrgenerator had to be updated to account for the newer version in theimage.	1
[Hexagon]Use requires_hexagon instead of requires_hexagon_toolchain if running on hexagon target (#11294)* refactor requires_hexagon_toolchain* trigger* lint	1
Implement 1d deconvolution (#4476)	5
Update let_list.h (#2987)	5
[microNPU] Use TF reference kernels for codegen tests where possible (#10762)Uses reference kernels for the codegen tests when the version ofTensorflow is >= 2.5.0 (as this is the first version this functionalitywas added). Otherwise, fallback to running the tests without.Change-Id: I92b24ad259d2fda2fed497aa0fe6d7f11a0db85a	5
Split_indices negative axis added (#1595)	1
[AutoScheduler] Add task scheduler (#6663)* Add task scheduler* fix lint* fix tests* fix tests* fix tests* fix test cases* fix test cases* fix tests* address comments	1
fixed cutlass byoc build break (#11686)	4
Implemented rpc logging (#10967)Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
Improve error messages for memory verifier and gpu memory verifier (#6281)* [FIX] Print exactly what issues the GPU memory verifier encountered.* [FIX] Print exactly why memory verifier failed.	0
[CONTRIB] Add PopenWorker process recycling (#11094)* [CONTRIB] Add PopenWorker process recycling* Clarify docstringsCo-authored-by: Peter Salas <psalas@octoml.ai>	5
[microTVM] Fix warnings on Zephyr tests (#8740)Fix the following warning message on Zephyr tests:DeprecationWarning: Please use input parameter mod (tvm.IRModule)instead of deprecated parameter mod (tvm.relay.function.Function)Signd-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
Implement flop support for int8 models (#2776)	1
[PyTorch] Fix aten::arange for pytorch (#12681)fix arange for pytorch nightly 20220815	0
[VTA] add doc to tsim-example driver and update verilator env variable (#3302)* add documentation and check for extension* add env variable for verilator include* fix typo* this will test if path exist otherwise it won't buid* check if verilator path and binary is set properly* add ?* remove export* no longer needed	4
[TOPI] Fix flaky testcase for floor div (#4382)* [TOPI] Fix flaky testcase for floor div* avoid check at 0.0	3
[Bugfix] Fix target host for vm compiler (#4057)* fix* tweak	0
[IR][TRANSFORM] Enable CopyOnWrite for passes. (#5309)This PR enables the copy on write optimizations passes:- Enable COW for IRModule both TIR and relay passes.- Enabled COW for PrimFunc in TIR passes.Need more thoughts into whether/how to enable COWfor relay::Function, due to some function passes dependon the presence of IRModule for context information,and the std::move of the related function to nullptrmight affect the related behavior.	1
[DOCS] Fix links (#1263)	2
Remove micro_dev (#11169)	4
[DOC] Various documentation improvements (#3133)	1
[Schedule] Allowed typing.Tuple in tir.schedule._type_checker (#11289)* [Schedule] Allowed typing.Tuple in tir.schedule._type_checkerPreviously, `typing.Tuple` annotations could not be used with`tir.schedule._type_checker.type_checked` annotations.  This allows`Tuple` type annotations to be type-checked.* Revert change, allow tuples input as List arguments* Suppress mypy errorsDirectly interacting with a type object would otherwise cause somefalse positives.* Corrected unit test for allowing tuples to be used as typing.List* Represent multi-type lists as List[Union[...]] instead of List[Any]This gives a better error message and plays nicely with _type2str,since `typing.Any` doesn't have a `__name__` field.	0
[VTA] Bringing group convolution support  (#4421)* group conv operator support for VTA* autotvm tuning script for group conv2d* lint fix* lint fix* lint fix* addressing comments	1
[TIR] An analysis pass to calculate workspace size for primfuncs (#7859)* Add workspace size calculation for primfuncsThis commit introduces functionality to query the workspace sizeas required by a tir primfunc by looking at tir.allocates inside of itChange-Id: I6f8ca90408b6e35d17ec818998a0f158a268a2a6* Add workspace size calculation for primfuncs*change int --> size_tChange-Id: If7fafec0269937d70184e7696e44386b74116d86* Add workspace size calculation for primfuncs* int --> size_t change for analysis.hChange-Id: I9e5c5e5f8458663390c50cf56f1a11910687928d* Add workspace size calculation for primfuncs* lambda scope fixChange-Id: I0c9b4c529150de8e0a5e170887cc935b7d0f6af2Co-authored-by: Chenfan <jcf94@outlook.com>	4
[PyTorch][Frontend] Semantic difference of 'bias_add' between relay and pytorch (#9204)* fix bias_add* add test* add test	3
[Hexagon] Add mobilenet test with AOT (#11204)* add mobilenet AOT test* Add _serial_number to super class	1
[TOPI] FIFO buffer op, to accelerate sequence modeling with dilated convolutions (#4039)* Add FIFO buffer op to enable explicit computation re-use in convolution* Add a test* Add end-to-end test with 1D convolution* Add a stub in MXNet frontend* Address reviewer comments* Add back stub for MXNet frontend	1
[TEST] Upgrade gpu docker to cudnn7 (#306)* [TEST] Upgrade gpu docker to cudnn7* fx	2
[Tutorial] Autoscheduler on ARM devices (#7326)* arm tuning tutorial* adjustment to get RPC working* fix lint* fix target* integrate Leandros comments* dont request remote in CI* use API from auto_scheduler, not autoTVM and updated comments* make ci-runnable* fix the formatting* address Zhaos comments* full run stats* taking Zhaos comments into consideration	1
Update SGX example (#1879)	5
[DOCS][APP] Add Example for C++ deployment (#398)* [DOCS][APP] Add Example for C++ deployment* fix lint	0
[DOCS] Improve documents on deployment (#1412)* [DOCS] Improve documents on deployment* minor updates	5
[metal] Remove support of `double` type (#7118)According to latest Metal spec (v2.3), `double` is still not supported.	1
[NNPACK] Add argument nthreads (#631)	1
[ci][docker] Prune all non-relevant images (#11497)* [skip ci][ci][docker] Prune all non-relevant images (#11491)Before this would leave around any image that could be used in CI. ThisPR changes it so that the `docker rmi` knows exactly which image isbeing used in CI so all others (even those that are being used in thesame build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on andshould help keep disk usage down.Co-authored-by: driazati <driazati@users.noreply.github.com>* [skip ci] Revert "[skip ci][ci][docker] Prune all non-relevant images (#11491)" (#11496)* [ci][docker] Prune all non-relevant images(this is a re-do of #11491)Before this would leave around any image that could be used in CI. This PR changes it so that the `docker rmi` knows exactly which image is being used in CI so all others (even those that are being used in the same build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on and should help keep disk usage down. Skipped CI since this runs during lint.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Checkin reduce	5
Don't fuse take with dynamic inputs (#6979)* add a regression test for fusing dynamic take* add legalize for take that stops fusion on dynamic inputs* fix lint* fix typo	2
[PASS] Change IRVisitor interfaces to function override (#42)* [PASS] Change IRVisitor interfaces to function override* [PASS] Change IRMutator interfaces to overloadable function	1
Support for AddV2 in Relay Tensorflow frontend converter. (#5046)	1
[Pass] Add utility that asserts that IRModule is not mutated in a pass. (#11498)	4
[LowerVTCMAlloc] Move LowerVtcmAlloc to after StorageRewrite (#12364)* [LowerVTCMAlloc] Move LowerVtcmAlloc to after StorageRewriteVtcm allocations were being moved inside loops even if they wereoriginally allocated outside of the loops. NormallyPlanAndUpdateBufferAllocationLocation moves allocations as close to useas possible and then StorageRewrite moves them back out as far aspossible. However, with Vtcm allocation,PlanAndUpdateBufferAllocationLocation would move the Vtcm allocationclose to the compute, then LowerVtcm would convert the allocation to aLetStmt. StorageRewrite would not move this LetStmt as it only handlesallocations. Moving LowerVtcmAlloc to after StorageRewrite ensures thatthe vtcm allocations are in their final spot before converting them to aLetStmt.* fix issues with tagging and storage rewrite	0
Fix more type annotation (#1490)	0
[ci] Manually merge code after checkout (#10778)* [ci] Manually merge code after checkoutThis is step 1 of 2 to fix the issue where Jenkins schedules a PR rebuild on non-code changes like editing the title. When the title is changed, GitHub sends an `pull_request` webhook to GitHub. Jenkins would ordinarily look at the webhook, query the PR and checks if the commit hash for the build has already been built. However, since we have it set in the Jenkins job to merge with main before this check occurs, the commit hash is different nearly every time. Disabling that setting fixes the issue, but we still need to merge the code with `main` to ensure that the build is valid if it completes successfully.* Use the same commit everywhere to mergeCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Passing dilation argument to account for API change. (#3510)	4
[CI] bash.sh, build.sh: add option to set the container name and hostname (#9110)This commit adds option "--name" to bash.sh and build.sh to enable the userspecify the name of the container and set the hostname inside thecontainer as well.This helps the developer idenitfy that they are inside the containerand which container they are working inside.	1
[TVMC] Fix tvmc run when using rpc (#11757)* [TVMC] Fix tvmc run when using rpcAs described in #11707, the RPC mechanism does not supportobjects of type Map which breaks the use of tvmc run when usingRPC after #9889. This commit intends to workaround this issue byproviding a fallback to the old implementation when RPC is beingused. Further, a test has been provided to help prevent thisregression in the future.Change-Id: I70c1863d00098270e27c08ba834a3587e9132d69* fix lintChange-Id: I958cf4e19988d047bdd2e02f6475b9f70afe80c8	4
Print right number of parentheses for LoadNode (#5965)Stop printing the unnecessary ')' after each LoadNode that didn'thave a matching '('.	5
change APIVariant to class	4
[RELAY][OP]log_softmax op (#1857)	2
[RELAY][PASS] Support Negative Scale in FoldScaleAxis (#2426)* [RELAY][PASS] Support Negative Scale in FoldScaleAxis* Fix comment	0
fix compilation	0
add GPU checking before compilation for rocm (#4394)Previously, we would rely on the later phases to error out(often for using too much shared memory). This enables thechecks on the IR that already exist for CUDA and OpenCL alsofor ROCm.	0
Fix typo in typing of space generator (#11424)	2
Many fixes to get unit tests passing on Windows. (#7431)	4
[DOCS] Fix tutorial (#2724)* fix docments* delete e	4
Fix boolean type (#862)	0
make adt tag signed (#4605)	1
[AutoTVM] Added @functools.wraps to function decorators (#8237)This helps in debugging, as the function name, arguments, anddocstrings show the function name from the source code instead of thewrapper function.(e.g.`<function tvm.topi.cuda.dense.dense_small_batch(cfg, data, weight, bias=None, out_dtype=None)>`instead of`<function tvm.autotvm.task.topi_integration.register_topi_compute.<locals>._decorate.<locals>.wrapper(*args, **kwargs)>`.)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
Update README.md	2
[Relay][Frontend][Onnx] Enable group_conv1d import through conv2d conversion. (#8321)* Enable group conv1d import through conv2d hack.* remove silly commented out lines.	4
[Hexagon] Implement model launcher (#8986)* [Hexagon] Implement model launcherThis implements a launcher that allows execution of ML models compiledinto a shared library on Hexagon DSP. It consists of two parts: theHexagon-side skel library and `launcher_android` to be used from`adb shell`.The launcher does not implement any performance-related optimizations,it's built on top of the `graph_executor` from TVM runtime, and so itexecutes a single layer at a time. This launcher should not be used tomeasure performance (because if will be highly suboptimal), its mainpurpose is to help in validating correctness.* Address review comments: explanations and elaborations in README.md* Rename cmake variables to be same as for TVM- `HEXAGON_SDK_ROOT` -> `USE_HEXAGON_SDK`- `HEXAGON_ARCH` -> `USE_HEXAGON_ARCH`* Address more review comments* Error out in cmake when USE_HEXAGON_SDK/USE_HEXAGON_ARCH are undefined* Change FATAL_ERROR to SEND_ERROR in cmake file	2
[Torch] Avoid adding unnecessary slicing (#7479)* simplyfing* improved fast path for slice* update rewrite pattern for maskrcnn	5
check to avoid crash in opt_level=0 vm build (#10347)	5
fix relu tests (#1149)	3
[TVMScript] Add object path tracing to StructuralEqual (#12101)Motivation: when two IR objects fail a structural equality check, currently there is no easy way tofind out which part of the IR caused the mismatch. In this PR, we modify the `StructuralEqual`infrastructure to also optionally return a pair of `ObjectPath` objects that point to the mismatch.(See https://github.com/apache/tvm/pull/11977). In the upcoming PRs, we will pass these paths to theTIR printer, so that it could highlight the mismatch location nicely.Tracking issue: https://github.com/apache/tvm/issues/11912	0
Fix np.int and np.float usage in the tree. (#8389)* Fix np.int and np.float usage in the tree.Newer versions of numpy give loads of warnings that suggestthat np.int and np.float will be deprecated. CI uses pytestand these warning logs clog memory for testing and make itslower.* Fix formatting	0
[BugFix] Fix a predicate bug in TIR schedule primitive `rfactor` (#9228)* Fix predicate bug of rfactor* Add regression test	3
[VTA][Chisel,de10nano] Chisel fixes and de10nano support (#4986)* [VTA][de10nano] Enable user defined target frequency.Issue:The VTA target frequency on the DE10-Nano is hardcoded to 50MHzunnecessarily limiting performance.Solution:Add a PLL to the FPGA sub-system along with support for theselection of a user specified frequency at build time. The boardsuccessfully builds and runs at 100MHz.* Added a PLL in the soc_system.tcl platform designer generator  script.* Modified the Makefile to automatically set the target frequency  from that specified in the pkg_config.py file.* Modified the Makefile to generate a bitstream with an RBF  format that enables programming of the FPGA directly from  the on-board processor. Specifically, the RBF is generated in  FastParallel32 mode with compression, which corresponds to the  default MSEL switch setting on the board, i.e. 01010.* Added a false path override to file set_clocks.sdc to turn off  unconstrained path warnings on the VTA pulse LED.* [VTA][TSIM] Add more debug and tracing options.* Modified Makefile to change default config to DafaultDe10Config.* Added option in Makefile to produce more detailed tracing  for extra observability in debugging complex scenarios.* Added option in Makefile to produce traces in FST format which  are 2 orders of magnitude smaller, although much slower to  generate.* Added option in Makefile to build the simulator with GCC address  sanitizer.* Modified Makefile to not lint the scala code by default avoiding  unintended wrong indentation. Linting should be better performed  manually on a per-need basis.* [VTA][de10nano] Enable remote programming of FPGA.Issue:The Cyclone V FPGA on board of the DE10-Nano can only be programmedusing the JTAG port, which is a limiting option for users.Solution:Add support for the remote programming of the FPGA implementingthe FPGA programming manager protocol published in the Cyclone Vuser manual.* Added file de10nano_mgr.h implementing an FPGA manager class  that supports handling of control and status registers as well  as a push-button option to program the FPGA. The class can be  easily extended to include more registers if needed.* Used an instance of the FPGA manager to implement function  VTAProgram also warning users when incompatible bitstream  files are used.* Registered VTAProgram as a global function and modified  the program_bitstream python class to use it.* [VTA][de10nano] Enhance de10nano runtime support.Issue:The de10nano target has incomplete, non-working supportfor runtime reconfiguration, bitstream programming, andexamples of usage.Solution:Complete runtime support for the de10nano target.* Modified VTA.cmake to comment out a default override for  VTA_MAX_XFER to 21 bit wide.* Modified VTA.cmake to add needed de10nano include dirs.* Modified relevant files to support de10nano same way as  other targets for VTA runtime reconfiguration and FPGA  programming.* Added test_program_rpc.py example as a runtime FPGA  programming example. Note that unlike the pynq target  no bitstream is either downloaded or programmed when  the bitstream argument is set to None.* Cosmetic changes to vta config files.* [VTA][Chisel] LoadUop FSM bug fix.Issue:The LoadUop FSM incorrectly advances the address of the nextuop to read from DRAM when the DRAM data valid bit is deassertedand asserted at the end of a read. This is caused by a mismatchin the logic of the state and output portions of the FSM.This is one of two issues that was gating the correct operationof VTA on the DE10-Nano target.Solution:Modify the logic of the output section of the FSM to includea check on the DRAM read valid bit or fold the output assignemntinto the state section.* Folded the assignemnt of the next uop address in the state  section of the FSM.* [VTA][Chisel] Dynamically adjust DMA tranfer size.Issue:In the DE10-Nano target and possibly in others, DMA transfers thatcross the boundaries of memory pages result in incorrect reads andwrites from and to DRAM. When this happens depending on differentinput values, VTA loads and stores exhibit incorrect results forDMA pulses at the end of a transfer. This is one of two issues thatwere gating the DE10-Nano target from functioning correctly, but mayaffect other Chisel based targets.Solution:Add support for dynamically adjustble DMA transfer sizes in loadand store operations. For a more elegant and modular implementationthe feature can be enabled at compile time with a static constantthat can be passed as a configuration option.* Modified the load and store finite state machines to dynamically  adjust the size of initial and stride DMA transfers. The feature  is enabled by default by virtue of the static constant  ADAPTIVE_DMA_XFER_ENABLE.* [VTA][Chisel] Improve FSIM/TSIM/FPGA xref debug.Issue:Cross reference between FSIM, TSIM, and Chisel based FPGA tracesis an invaluable instrument that enables fast analysis on FSIM,and analysis/debug on TSIM and FPGA, especially for complex flowslike conv2d or full inferences. Currently this cannot be doneeasily since a suitable reference is missing. The clock cycleevent counter cannot be used since it is undefined in FSIM andnot reliable between TSIM and FPGA because of different latencies.Solution:Introduce a new event counter that preserves a program order acrossFSIM, TSIM, FPGA. We propose adding the accumulator write eventcounter in the Chisel EventCounter class and a simple instrumentationin the FSIM runtime code. Note that this technique enabled finding theChisel issues reportes in the PR, which would have been otherwisefar more difficult.* Added the acc_wr_count event counter and changed interfaces  accordingly.* [VTA][de10nano] Comply with linting rules.* [VTA] Appease make lint.* [VTA] Disable pylint import not top level error.* [VTA][Chisel,de10nano] Linting changes.* Use CamelCase class names.* Use C++ style C include header files.* Add comments to Chisel makefile.* [VTA][de10nano]* Reorder C and C++ includes in de10nano_mgr.h.* Restore lint as default target in Chisel Makefile.* [VTA][de10nano] Do not use f string in pkg_config.py.* [VTA][de10nano] Remove overlooked f strings in pkg_config.py.* [VTA][de10nano] Fixed typo.* [VTA][TSIM] Check if gcc has align-new.* [VTA][Chisel] Make adaptive DMA transfer default.* [VTA][RPC] Renamed VTA_PYNQ_RPC_* to VTA_RPC_*.Issue:With more FPGA targets coming online the initial method ofusing individual environment variables to specify target IP and portdoes not scale well.Solution:Use a single VTA_RPC_HOST, VTA_RPC_PORT pair to be changedevery time a different target is used. For instance in a scriptused to benchmark all targets.* Replaced every instance of VTA_PYNQ_RPC_HOST and VTA_PYNQ_RPC_PORT  with VTA_RPC_HOST and VTA_RPC_PORT, respectively.* [VTA][Chisel] Comply with new linter.	1
[CI] Update CI Images to include `pytest-lazy-fixture` (#10999)Closes #10984	3
[VTA] Update Jenkinsfile for VTA test with TSIM (#4734)* [VTA] Update Jenkinsfile for VTA test with TSIM* duplicate task_python_vta.sh multiple copies for now	3
[CMSIS-NN] Moved TFLite model making to common area (#10939)* [CMSIS-NN] Moved TFLite model making to common areaChange-Id: Ic4dbc1919ff0b481c05daf7e57cf9b055c714c9c* Fixed lint issues with tensorflow importChange-Id: I7a520beec9c244e9c790d3e82733c2fb476f7e5e* Resolved merge conflict with mainChange-Id: Iefe58dd321efae6eae26cd54a31c5923d0f1e32b* Made TFLite layer creation explicitChange-Id: I7fbf6a5a2163c1fada49477f86d84f1bc09bd57c* Lint fix: added a missing docstringChange-Id: If1fb8bb09c538c04e333ccab65a20cff247a504d	4
Remove CODEOWNERS (#10192)See RFC: <link tbd>Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] [bfloat16] add bfloat16 promotion for CallNode (#12370)* add bfloat16 promotion for CallNode* add softmax to bfloat16 build test	3
[TOPI][CUDA] Schedule for pool_grad (#3622)* [TOPI][CUDA] Schedule for pool_grad* Relay test* Fix fused op* doc* Remove set scope local	1
Also package core.rly (#4679)	5
[PYTHON] Enhance with_attr API, cleanup MakeAPILegacy in testcases (#5335)	3
[Runtime] Pipeline Executor Initial patch. (#8702)* [Runtime] Pipeline Executor Initial patch.This patch is one of serial patch for PR 7892 splitting.this is the initial partof the pipeline executor, this patch include the cmake change and python and C++interface for pipeline executor.* add pipeline config* add config connect logic.* fix build issue.* set output index start from 0* address review commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* address review comments.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* address review comments* add topology sort* add binding check logic.* fix plint error.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* fix plint issue.* address review comments.* trigger build.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>address review comments* address review comments.* polish doc and comments.* polish doc and address review comments.* address review comments.* doc change.* doc change.* Trigger build.* trigge build.* address review comments.* address review comments.* address review comments.* polish documents.* Polish the document.* address review comments.Co-authored-by: Cody Yu <comaniac0422@gmail.com>	1
[AutoScheduler] Enable winograd for conv2d and layout rewrite for conv3d (#7168)* [AutoScheduler] Enable winograd for conv2d & Enable layout rewrite for conv3d* fix test* fix test* update tutorials	5
[CI] Enable integration tests on AArch64 (#10677)As part of this any failing tests have been marked for follow up as part of https://github.com/apache/tvm/issues/10673.This depends on fixes in https://github.com/apache/tvm/pull/10659, https://github.com/apache/tvm/pull/10672 and https://github.com/apache/tvm/pull/10674 to scope other tests correctly.	3
Fix a typo. (#5611)Co-authored-by: Zeng Liyong <liyong.zeng@streamcomputing.com>	2
Add nnvm target to Makefile (#1245)	2
[Frontend][PaddlePaddle][Part1] Add 100+ operators for PaddlePaddle (#9126)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes	3
[TVMSCRIPT] add more type support in script function parameter (#8235)* [TVMSCRIPT] add float type support in script function* [TVMSCRIPT] add more type support in script function parameterCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	2
[EXPR] ir_operator.h->expr_operator.h Centralize const folder logic (#2719)	2
[tvm4j] support kNDArrayContainer (#1510)	1
Lower cache_read and cache_write to Hexagon DMA via tensorize (#10365)* Lower cache_read and cache_write to Hexagon DMA via tensorize* rework test to be compatible with launcher* remove cpu device api mem_copy implementation and test	3
fix rpc server proxy connect (#290)	0
[Intrinsic] Add log1p, ldexp, atan2, hypot, nextafter, copysign (#5312)* [Intrinsic] Add log1p, ldexp, atan2, hypot, nextafter, copysign* Lint	2
[docs] Getting Started with TVM: AutoTVM and Matrix Multiply (#7643)* [docs] Getting Started with TVM: AutoTVM and Matrix MultiplyThis patch moves the matrix multiplcation example tuningwith AutoTVM to the tutorial directory, and expands on thecontent. This follows and builds on the section on TE* Applying lint style* Fix license* Apply suggestions from code reviewCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Change comparison tolerance to smaller valueCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	4
[Frontend][Onnx] Added broadcasting to prelu alpha. (#6549)	1
don't rename onnx input tensors (#491)	5
[ONNX] Support optional outputs for ONNX nodes (#7818)* Support optional outputs for ONNX nodes* add comments	1
Fix pylint issue in vta/python/vta/top/graphpack.py (#3519)This appears in linting using the docker scripts. I'm not surewhy this isn't failing in the standard CI for TVM and it mightbe that the docker images haven't been updated in the CI system.python3 -m pylint vta/python/vta --rcfile=/workspace/tests/lint/pylintrcUsing config file /workspace/tests/lint/pylintrc************* Module vta.top.graphpackC:131, 4: Missing method docstring (missing-docstring)	2
[ci][tvmbot] Ignore irrelevant Actions jobs (#12351)GitHub associates the automation run on PRs for user tagging and reviewsas CI jobs on that PR, and since these often get re-run, skipped, or mayfail in the background they prevent merges. This PR ignores all of thesejobs so only the ones defined in `main.yml` matter, which are the onesthat actually build / test TVM.This is a simpler version of #11569	3
[MetaSchedule] Fine-Grained Rewrite Unbound Block (#10823)In this PR we introduced more fine-grained loop spliting and reordering for Rewrite-Unbound-Block post processor based on given cuda target's attribute (`max_threads_per_block`). After this PR the performance of non-reductional kernels could improve by ~20%. Regression tests are also added.	1
[Caffe Frontend] Add support for Embed layer (#9257)* [Caffe Frontend] Add support for Embed layer* [Caffe Frontend] Add support for Embed layer	1
[CI] Assert some unittests are not skipped in CI (#12436)This PR adds a script that does a diff of skipped tests between the latest successful build on the main and the current branch. Then, it posts a comment with the report on the open PR. #11670	3
[Ansor] Add HW param for Vulkan tuning (#7626)* add HW param for VK* query warp size properly* guard against warp_size < 4 caseCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>	2
[Fix][VM] Fix VM invoke with set_params (#4079)* Fix VM invoke with set_params* add test* tweak	3
remove batch_norm_inference (#2626)	5
[skip ci][ci] Add missing guard to skip CI check (#10625)This was failing in the docker image validation since the `BRANCH_NAME` environment variable wasn't set. The part in question still runs even with the `skip ci`https://ci.tlcpack.ai/blue/organizations/jenkins/docker-images-ci%2Fdocker-image-run-tests/detail/docker-image-run-tests/62/pipeline/cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[BugFix][TIR] Fix construction of new IterVars in CacheRead/Write for non-int32 dtypes (#10795)_This PR is a follow-up effort of #10789, which enables the `int64` support for TIR schedule primitive Cache-Read and Cache-Write._Prior to this PR, the IterVars of the generated cache stage block are always `int32`-typed, which might conflict with the dtypes of the domains of the IterVars.In this PR, the dtype of new IterVars are constructed according to the data types of their domains, and thereby the possible conflicts are resolved. Meanwhile the data types of the read/write regions of the cache stage blocks are also constructed according to correct data types.	5
ffi: add missing binding for FixedPointMultiplyAttrs (#8353)	0
Documentation issues fixed (#1016)	0
trigger ci (#1620)	5
meta schedule misc update (#10389)	5
[Relay][Frontend] Prune redundant logging (#9545)* [Relay][Frontend] Prune redundant logging* lint	2
VTA and TVM on PYTHONPATH, also pass to sudo (#42)	4
[AOT][Tests] Use pre-built libraries in Reference System tests (#9271)This is a follow up to remove the rebuild of dependencies in each testrun from the AOT test utils and favouring those prebuilt into thecontainer.	3
[AutoScheduler] Skip useless calls to RewriteLayout (#6993)* [AutoScheduler] Skip useless calls of RewriteLayout* fix lint* fix lint	0
[Relay][Pass] SimplifyCastLike/Cast and ConcretizeFullLikeRewrite rewrites for SimplifyExpr (#7827)	4
[Frontend][Pytorch]Add Pytorch advanced indexing (#6318)* Add Pytorch advanced indexing* Minor fix for test* Fix for cuda	0
[Caffe Frontend] adding Reduction op (#8015)* [Caffe Frontend] adding Reduction op* reformatting Reduction op test script* reformatting Reduction test script* [Caffe frontend] Reduction op- adding more test cases; handling '0 < axis < num_axes - 1' case to give the result equivalent to Caffe framework- skipping Relay multiplication if coeff is 1Signed-off-by: zotanika <zotanika@gmail.com>* linting test script* linting* typo fixCo-authored-by: sunchul.jung <sunchul.jung@samsung.com>	0
[ARITH] Improve min/max/div cases in RewriteSimplify (#3463)[PASS] Use new infra for lower warp memory[ARITH] EvalSet recursively evaluates set in case dom_map contains set that need to be relaxed.	1
Introduce module_loader to AutoTVM. (#7337)* Introduce code_loader to AutoTVM. * Prepares for autotuning with microTVM, and provides extension hook   for VTA.* add vta hook* git-black* pylint* Add missing import* Fix import problem* add missing import* rename code_loader to module_loader* rename remote_kw to remote_kwargs* black format	2
[ci][docs] Seed autotvm tutorial (#11147)* [ci][docs] Seed autotvm tutorialThe runtime on this one varies a lot, so this tries adding a seed to get it consistent* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>	1
initial commit (#12301)	5
[TFLite] Quantized version of unit test for Dense (#7113)Added quantized version of unit test for FullyConnected/DenseAdded check for -1 in case if bias not supplied	1
[microTVM][tutorial] Add ENV variable to enable testing on physical hardware (#9993)* Add env variable to micro tflite tutorial* Address @gromero comments* address @areusch comment* fix scope* trigger* trigger	0
[Hexagon] Tighten requirements on inclusion of runtime sources (#11635)* Tighten requirements on when Hexagon runtime sourcesare included in the runtime build. Specifically only include themwhen building for hexagon rpc on hardware and do not include themfor x86 (host, simulator) or android builds.* Remove device_api.cpu binding to hexagon in simulator rpc session.Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Karl Koscher <kkoscher@octoml.ai>* if(BUILD_FOR_HEXAGON)Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Karl Koscher <kkoscher@octoml.ai>	5
[Thread Backend]Fix CPU Thread Binding for Multiple Sockets (#5918)* Fix CPU Thread Binding for Multiple Sockets* Backward compatibility	0
[ci] Check existing reviews and requested reviews before cc-ing (#10734)This will re-request reviews from existing reviewers as in #10714 which is not intended, so this lists out existing reviews/requests and filters the new reviews to add based on those usernames.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>	1
Make missing desired layout non-fatal (#6553)	1
leaky_relu bug fix (#1218)	0
remove clang compile warnings (#9942)	2
[NODE] Keep base node system in HalideIR (#1793)	5
[Auto Scheduler] Add target host to measure record (#7046)* [Auto Scheduler] Add target host to measure record* Fix PyLint* Fix lint* Solve the serialization logic when we don't have hardware params* update auto scheduler log	2
Add workgroup size attribute to AMDGPU functions in codegen (#4342)When we did not set the workgroup size, LLVM will use too many registersfor kernel launches with many threads. This resulted in "invalid ISA"errors. Here we set the maximum workgroup size to the maximum threadsper block from the device API.Of course, one might look into allowing configurations with fewerthreads at runtime to use more registers.	1
[VM] Per-input, data dependence specification for shape func (#7210)* made TShapeDataDependant array* add stub* dyn strided slice working* reshape also working* remove log* works on maskrcnn* lint fix* fix cpp test* remove stale pop back* add more doc* dependant -> dependent* remove redundant check* remove data_dependent_	5
bump tolerances (#9054)	5
[ci] Use smaller ARM nodes for build/test (#11445)This applies the new instances from https://github.com/tlc-pack/ci-terraform/pull/32Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[TIR] Fix assert for tensorcore int8 intrinsics (#12365)	3
[CI] use llvm9 for the gpu tests (#4224)* [CI] use llvm9 for the gpu tests* Update Docker script to support new nvidia docker	2
Tutorial: Build a Graph Convolutional Network on TVM (#3681)* add build gcn tutorial* add dgl to docker file* add dgl to docker file* Apply suggestions from code reviewCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* add dgl to docker file* rerun checks* Revert "add build gcn tutorial"This reverts commit dbe8b5f0e02a13fdd586a9faa58fd1326653afb0.* resolve git issue* resolve git issue* resolve git issue* apply marisa's comment	0
RPC session connection request information persist app locally (#1098)	5
[REFACTOR][RELAY] Replace build_config with PassContext (#5698)	4
[microTVM][Zephyr][projectAPI] Minimize project build commands (#12209)* move cmake args to generate project* remove zephyr board from flash and run	1
[CMSIS-NN] Initial operator support for Add (#9167)This patch aims to add initial support for the `Add` operator to CMSIS NN, which was actually similar enough to the `Mul` operator that it shares quite a bit of code - exciting times.	1
[BUILD] Add clang to build matrix, -Werror (#1273)	0
[CI][Lint] Disable no-else-return check in pylint (#11327)* [CI][Lint] Disabled no-else-return check in pylint* Line breaks and alphabetical order for readability* Added description of reasoning/style in the code_guide	1
ONNX frontend operator support: And (#3878)	1
Fix JUnit failure reporting (#10121)* Fix spacing* Add try..finally everywhere* trigger ci* Fix pytest invocations* Remove junit collection where no files existCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[microNPU] Add a pass to reorder copy and compute nodes (#10959)	4
[FIX,HEXAGON] Gitignore generated Hexagon files (#10552)Ignore these files so they are not accidentally committed.	2
[Relay][heterogeneous pass] remove on_device op after annotation (#3204)* remove on_device op after annotation* Update src/relay/pass/device_annotation.ccCo-Authored-By: MORINAGA <34588258+imorinaga@users.noreply.github.com>	1
[FQ2I] Add abs to FQ2I (#10922)* add abs to fq2i* lint* special case for fq2i* np iinfo	5
[REFACTOR] TVM_REGISTER_API -> TVM_REGISTER_GLOBAL (#4621)TVM_REGSISTER_API is an alias of TVM_REGISTER_GLOBAL.In the spirit of simplify redirections, this PR removesthe original TVM_REGISTER_API macro and directly use TVM_REGISTER_GLOBAL.This type of refactor will also simplify the IDE navigation toolssuch as FFI navigator to provide better code reading experiences.Move EnvFunc's definition to node.	5
[PASS] Make placedevice compatible with backward op (#88)	1
[UnitTests][CMSISNN] Mark Binary Ops CMSIS NN tests as skipped (#9200)https://github.com/apache/tvm/pull/9179 and https://github.com/apache/tvm/pull/9167 both landed at the same time which resulted in new tests without the skipping.	3
[REFACTOR][IR] Move to runtime::String (#5276)* Use runtime::String* move string to tvm namespace* add const char* constructor* implicit cast from std::string	1
Fix plint error. (#10394)plint complain error in parser.py and test_vm.py just fix it.	0
Update docs/dev/virtual_machine.rstCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>	2
[Relay] GetItem (#1861)	1
hotfix for onnx (#3387)	0
Update ISSUE_TEMPLATE.md	0
Early checking added and new test cases added for schedule fuse (#5010)* [1] New test case added for fuse* [2] New test case added for fuse* [3] New test case added for fuse* [4] New test case added for fuse* [5] Early check added	1
[Meta Schedule] Add `ApplyHisotryBest` Meta Schedule Context (#10049)* Add ApplyHisotryBest.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Update integration.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
Updates (#14)* Remove outstanding cython functions* Add in operator overload* Enable JSON to save version	5
fix Makefile ifdef & typo (#97)	2
[RUNTIME] Separate runtime related contrib into runtime/contrib (#4207)	1
[TOPI] Support ceil_mode in pooling (#593)	1
[CODEGEN][CUDA] Fix a bug when vectorized load&store was involved for… (#5428)* [CODEGEN][CUDA] Fix a bug when vectorized load&store was involved for "char2"* Add unittest for char2* vector element load support char2&add some unittest for vector element load* Merge common up logic&Support char3&Add unittest for char3	3
[Tutorial] tutorial to writing a costumized pass (#1671)	4
[Bug][Meta Schedule] Fix Infinite Loop Caused When Calling Methods Not Overrided In PyClass. (#9451)* Fix Infinite Loop Caused When Calling Methods Not Overrided In PyClass.* Add new line.* Lint.	1
[ETHOSN] Minor corner case fixes (#11218)Minor issues in corner cases from static analysis and a code standardviolation fix.	0
Add ci_qemu docker image (#6485)* Add ci_qemu docker image* build qemu from source* add ubuntu1804 install script* receive keys from keyserver* pin to zephyr 2.3.0* install 2.4.0* add cache directory* revert 3rdparty/vta-hw	4
[RELAY][OP]  Maketuple to be resolved when containing incompleteType (#2031)	5
[ETHOSN] Only use mock inference when whole graph is offloaded (#12296)The mock inference functionality is only supported when the wholegraph is offloaded to the NPU, otherwise it can result in undefinedbehaviour. This patch makes sure the mock inference functionality isnot run on test cases where some parts of the graph are not offloadedto the NPU, while ensuring the module is still built as a sanity check.Change-Id: I27052d118ff976f9adbfc3f5b5b96185318e1573	5
Run full build when no files were changed over main. (#9221)	4
Move the allocates of AoT codegen to be TVMBAWs (#9065)* Move the allocates of AoT codegen to be TVMBAWsThis commit introduces changes to aot_executor_codegen.ccto place tir.allocate to use storage_scope = 'global.workspace'.The lower_tvm_builtin pass is modified slightly to generateTVMBAW calls.Change-Id: Iba4ba437c1431c5197bf11abb826e03807bbcf66* Move the allocates of AoT codegen to be TVMBAWs*Adding more comments and descriptions*Modified the test case to use primitive relayChange-Id: Ia18a169d94bded3f81af7b3081c7d1ac29c669bc	4
Update Jenkinsfile	2
More CHECK to ICHECK (#6758)* Address apps, docs, and nnvm directories* Catch some that were missed* crt has it's own logging.h* Fix missing include	0
Enhance upsample operator to adapt onnx opset version 9 for nnvm comp… (#2968)* Enhance upsample operator to adapt onnx opset version 9 for nnvm compiler* Add upsample test case for newer opset in nnvm* re-trigger the CI	1
Update ICHECK error message with link to documentation page. (#7869)	2
fix inconsistent tag name (#4134)	0
Revert "[CI.Lint.Black] Use "en_US.UTF-8" for Red Hat 6&7 Compatibility (#9537)" (#9548)This reverts commit ecd8a9ce33991262f4184cb857f1088d9d8e1bb1.	4
[Hexagon] Change "scalar" and "stack" in IDL from "inrout" to "in" (#5487)Co-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>	4
[ARITH] Remove the legacy Simplify, migrate to Analyzer. (#5385)The legacy Simplify/CanonicalSimplify are now a thin wrapper around the Analyzer.This PR removes these functions and migrated every place that requiressimplification to enforce Analyzer creation.The new API would encourage more Analyzer sharing and potentially enablecontext-aware analyzer-based simplification.	0
[MetaSchedule] Improve Error Message in JSON Database (#11940)	5
[Doc] ConvertLayout - Call RemoveUnunsedFunctions.	4
[Relay][Docs] Fix broken bullet points in Relay operator addition tutorial (#2325)	1
[dep] psutil (#3780)	5
[Torch, QNN] Add missing upcast to uint8 avg_pool conversion  (#5089)* add missing upcast to avgpool* add avg pool test	3
[FIX] Fix deploy_sparse tutorial (#7939)	0
[Relay] Re-run PlanDevices after LowerTE to flow new memory scope constraints. (#9613)* [Relay] Re-run PlanDevices after LowerTE to flow new memory scope constraints.This PR: 1) Makes PlanDevices consider lowered calls when solving device domain constraints. 2) Connects the storage scopes on PrimFunc parameters (encoded in their Buffer data    Var type annotation PointerTypes storage_scope fields) to the memory_scope    fields of the SEScopes which PlanDevices unifies over. 3) Allows new device_copies to be inserted on the arguments and results of lowered    calls so as to acount for any memory scope mismatches which are now apparent.[device_planner.cc has main changes, rest is secondary.]In the short term we'd like to use this machinery to flow memory scope choices madeduring lowering back out into the overall Relay program. In the longer term we'dalso like to be able to use memory scopes to influence the lowering ofyet-to-be-lowered functions (or lowered functions which have yet to been scheduled,a distinction now possible with TensorIR). - Memory scope constraints can flow both out of and in to PrimFuncs   introduced by LowerTE. In TIR memory scopes are represented by   'storage scopes' on the PointerType type annotations on TIR Buffer data   variables.    - It is straightforward to extract memory scopes from PrimFuncs by      looking at the PrimFunc's buffer_map. We do this is 'phase 1' of      PlanDevices, which collects all the device constraints implied by    - However, pushing memory constraints in to PrimFuncs is more challenging      due to buffer aliasing. This aspect is still experimental. - Allow device_copies to be inserted for both arguments and   results of PrimFunc calls, on the assumption PlanDevices has   already established a consistent device assignment prior to   lowering and any new mismatch is required to match up memory scopes.   We use the new 'free' on_device annotations to implement this.Coming along for the ride: - To make unit tests of mixed Relay/TIR functions possible needed   to be able to supply a checked_type to GlobalVar since that's currently   the only way to give a Relay type to PrimFuncs. - Use GenSym to get unique var names in ANF & partial eval so easier   to diff debug output between passes and connect program fragments   back into the overall program. Relying on pretty-printing to   automagically unique-ify var names is certainly cute but until we   have better span support is very hard to work with. - Realized both dead_code.cc and fold_constant.cc would   happily move values into a different lexical virtual   device context since device_planner.cc was being   'clever' and eliding on_devices for let-bound values   when there's no change. Fixed so that every let-bound   value has an on_device. Will be much better after   https://github.com/apache/tvm-rfcs/pull/45 is implemented. - Make build -Werror clean for clang-12 (mostly move fixups). - Address post-submit comments from #9693.* [checkpoint] thread safe GenSym	1
Fixed deprecation warning issue for pipeline executor. (#9770)	0
make tvm compilable by gcc 4.9.2 (#4032)please see https://stackoverflow.com/a/26949099	1
[ONNX] Initial work to import pre-quantized ONNX Models (#7802)* Add QuantizeLinear and DequantizeLinear* DynamicDequantizeLinear	1
[TEST] Fix duplicate definition error for gpu export mod testcase (#9538)	3
[Autoscheduler][Sparse] Add sparse dense end to end model tuning support for x86/arm cpu & Some bug fix (#7635)* Add sparse dense end to end model tuning support* Add sparse tuning for arm network* Bug fix for tflite frontend dense with layout rewrite* Move the random_bsr_matrix to sparse.utils	4
[COMMUNITY] New Reviewer -- Meteorix (#7944)	1
[PYTORCH]ImplicitTensorToNum support added (#5603)	1
[ARM] Support NCHWc alter layout in the fallback mode (#10724)* [ARM] Support NCHWc alter layout in the fallback mode* remove fallback path* add test* fixed int32_lanes and add channel check* fixed schedule dispatch bug* add workaround fallback path for NHWC im2col based GEMM schedule* int32_lanes=4 by default* typo* update test	3
[BYOC][ACL] Add maximum support for float32 (#6506)* ACL integration: add maximum support for float32.* Added the code generation flow in arm_compute_lib.py* Added the runtime calls in acl_runtime.ccChange-Id: I69c5522f05a46c1dd235da5d57fe499134de0425* Add maximum to the list of supported functionsChange-Id: Ia49087756be4c3ac92a3dc76fe03fb00de468f8d	4
[ETHOSN] Per-channel int8 quantization for conv2d (#10131)	5
Support dump ir for each pass (#693) (#791)* Support dump ir for each pass(#693)* expose DumpIR* fix comments* fix comments	0
Change behavior of onnx importer to throw when user provides an input no in the graph. (#7699)	1
[Fix] Remove the duplicate PrintIR pass in Relay (#5403)	4
use phone EditText for numerical fields (#1587)	2
[Relay][Frontend][TF] Fix Size operator (#4175)* [Relay][Frontend][TF] Fix Size operator* Uncomment tests	3
[Fix] Fix conv2d alter op for arm cpu (#5532)	0
[skip ci][ci] Fix bad merge after moving to templated Jenkinsfile (#10792)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
ROCm: Add warp shuffles and enable reductions (#5727)Thank you @masahi and @wpan11nv for the feedback	5
[PYTHON] Enable environment scoping (#33)	0
[Bugfix] Auto scheduler tutorial failure on CI (#6723)	0
pooling.cc improvements (#4767)	1
[Relay][Frontend][Keras] NHWC import support. (#4899)* Basic test working* Almost all tests working.* all tests passing.* Fixed lint.* Improved Style.	1
[Relay][Op] Pad operator (#1843)	1
[Docs] Prevented docs/1 file from being generated. (#8029)* [Docs] Prevented docs/1 file from being generated.Typo in tests/scripts/task_sphinx_precheck.sh caused $TVM_HOME/docs/1file to be created with stderr output, rather than merged stderr andstdout.* [Docs] Corrected sphinx build warnings- Previously, several warnings were generated by sphinx, but were  unintentionally suppressed.  This PR resolves the sphinx warnings.* [Docs] Corrected additional sphinx build warnings.- Rebased on main and corrected warnings, now up to date as of commit  53e4c603.* [Docs] Corrected additional sphinx build warnings- Rebased on main and corrected warnings, now up to date as of commit  1f2ca068c.* [Docs] Corrected additional sphinx build warnings- Rebased on main and corrected warnings, now up to date as of commit  d0791d3db.* [Docs] Ignore sphinx warnings from missing "git describe" and sckit-learn versions.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[microNPU] Introduce a pass to remove redundant identity operations (#10254)* [microNPU] Introduce a pass to remove redundant identity operationsIntroduces a  pass that aims to remove identity operations,introduced during legalization, that are immediately followed by an NPUcompute operation e.g. Convolution.Change-Id: Ia3b2c7bebf8cba1f827af8e3f3335677ba8f6371* fix lintChange-Id: Idf9341ce757b849f8819944dab2fb3b1496a2caf* Addressing commentsChanges in test_identity_optimizer.py:* Fixed typo in docstring* Removed print* Fixed same output test to use correct input shapeChanges in codegen.cc:* Remove unnecessary constructorChange-Id: Ie4a053725110ce52d8be039ca1ce48084bc66545* skip tests when required packages are not availableChange-Id: I0a88d92dd31ca3dd07a2a495f18c10a2ebf2fc9e* support multiple output identities and add more testsChange-Id: Ib54031fe1c70159728876a23f96b72adb2ea17b0	5
[RUNTIME] Remove Extension VTable in favor of Unified Object system. (#4578)Before the unified object protocol, we support passadditional extension objects around by declaring a type as an extension type.The old extension mechanism requires the types to register theirconstructor and deleter to a VTable and does not enjoy the benefit of theself-contained deletion property of the new Object system.This PR upgrades the extension example to make use of the new object systemand removed the old Extension VTable.Note that the register_extension funtion in the python side continues to workwhen the passed argument does not require explicit container copy/deletion,which covers the current usecases of the extension mechanism.	1
[DOCS] Improve the docs build instructions (#6173)	2
Pass first basic case of bound inference	5
[NNVM] Enhance operator fusion for more element wise patterns (#1548)	1
[Fix] Remove the tvm web from version update (#6122)	5
Change the way to import ONNX model. (#186, #187) (#189)* Change the way to import ONNX model.* Decide ONNX version and change the processing* Check whether onnx has the attribute '__version__'	4
[Relay][Frontend] Support TF Gather (#2935)* [Relay][Frontend] Support TF Gather* fix comments	0
Fix a typo (#3913)	2
[RELAY][OP] Left shift operator (#1839)	1
Fix intel conv2d auto tune (#5200)* Fix x86 conv2d and depthwise conv2d auto tuning* Fix depthwise conv2d infer layout* Use random data instead of empty data for autotvm* Fix pylint* Keep empty array for now for autotvm	0
[Torch] Object detection support update for PyTorch 1.6 (#6659)* update split* fix* cast nms output to int64* add more comment and numel test* fix lint* also supported the latest master (1.7)Co-authored-by: masa <masa@pop-os.localdomain>	3
[Relay][AlterOp] NHWC to NCHWc support for Pool, pad, concatenate, sum. (#4059)	1
[TVMC] --disable-pass option added to compile mode (#7816)* [TVMC] --disable-pass option added to compile modeAdded --disable-pass option to TVMC compile mode to disallowcertain supplied passes in PassContext for the compiler.Change-Id: Iae1849d7b051ac9288509dc458a58788c865537a* Added test, addressed requestsChange-Id: If688f65441d3aa9967ab823adf899cfc704bd097* added printing of available passesChange-Id: I7a4706c03c0d64cade4977d431bcb25b3708f213* C0415(import-outside-toplevel)Change-Id: I33d6f6f86d182de2e21e895ec2dfe9f11f5916dd	4
[BYOC][DNNL] Enable layer normalization in DNNL byoc. (#11508)* Enable layer normalization in DNNL byoc.* Added unittest for layer norm and make code compatible after introducing TensorRequisite(PR-11345)* Fix lint issue* Fix clang format issue	0
[Graph Debug] bug fix (#897)Need to break after executing intended operation (not before).	4
[RELAY][EXPR] Make const numpy consistent (#2349)	1
upd (#10809)	5
[UnitTest] Fuzz based on seed rather than random value. (#10515)Some extensions to run tests in parallel (e.g. `pytest-xdist`) requirethat test collection be deterministic.  Using the random seed as thetest parameter instead of the random value makes the test collectionbe deterministic.	5
fix narrow conversion on gcc-8 (#1376)	0
[RUNTIME] Stream API  (#953)	1
[Torch] Support returning quantized weights and bias for BYOC use cases (#9135)* [Torch] refactored the way is bias quantization done* support returning 8bit weight* add test* add doc* pylint* return_int8_weight -> keep_quantized_weight* fixed for dynamic linear case* remove test function call* simplifying	1
Fix tutorial broken by Docker build (#6694)	2
[Hexagon] Use HEXAGON_SDK_ROOT in gtest path (#11421)	3
fix symbol.attr('op_name') (#149)	0
Improve task_lint.sh robustness (#2711)* [SCRIPT] Refactor grep for multiple patternsTidy up the use of grep.  Use -E rather than run multiple grepinstances.* [SCRIPT] Refactor grep use in pipeline.Prefer to use stdin redirection rather than create a pipeline.* [SCRIPT] Refactor placement and cleanup of temporary files.Place temporary files in the conventional /tmp location. Avoidpoisoning file name space by using $$. Ensure the temporary files getcleaned up, even when the script fails / exits early.* [SCRIPT] Improve robustness of task_lint.sh error handling.Ensure script failures are caught and propagated.  Rather than tryingto explicitly catch and propagate failures with explicit "|| exit"annotations, use the "set -e" idom from docker/install scripts andhave the shell catch and propagate errors in the general case andspecial case the grep instances where non zero exit is permitted andshould be ignored.	0
[PYTORCH]Rsub, Embedded, OneHot ops support (#5434)	1
[DOC] various assorted grammar fixes (#3127)* Correct spelling of 'inavlid'* [DOC] correct spelling of 'schdule'.* [DOC] clean up use of abbreviation "interop"* [DOC] capitalize API abbreviation consistently* [DOC] correct spelling of 'peformed'.* [DOC] correct spelling of 'intermidiate'* Remove trailing white space.* Correct spelling of 'parametrization'.* [DOC] minor improvements to Range documentation.	2
Remove backward index, use gradient guessing instead (#85)* Remove backward index, use gradient guessing instead* minor fix* bugfix	0
[LICENSE] clarify the blockingqueue license, update version to 0.6.0 (#4414)	5
Removed std::unary_function because it is deprecated and removed in newer c++(https://en.cppreference.com/w/cpp/utility/functional/unary_function) (#2962)	1
[ci] Add manual workflow to upload files to CI bucket (#11856)This adds a `workflow_dispatch` only GitHub Action that committers can use to upload files to the CI bucket for use like in #11839	1
[Requantize] Cleanup and Optimize Lowering (#5286)* Adding Cast back to Int32 in FixedPointMultiply.* Removing extra clip.* Fix space.* Retrigger.* Retrigger.	0
[Relay][Frontend][TensorFlow] Support BatchMatMul with input dimensions larger than 3 (#3732)* Support BatchMatMul with shapes greater than length 3* Fixes* Add tests* Remove dependency on Python3* Clean up* Merge with master* Resolve comments	0
[microTVM] TVMCon 2021 Zephyr Demo with CMSIS-NN (#10144)This is adds a Zephyr Demo showing how to integrate TVM directly into your embedded application. It runs a keyword spotting model with the Zephyr RTOS using CMSIS-NN with the Ahead-of-Time (AOT) executor and the stack allocation strategy.	1
conv2d data re-layout fix out of threads bug (#514)* conv2d layout change bug fixed* remove debug msg* misaligned error fixed	0
Add MXNet converter for RNN layer ops (#3125)	1
[BYOC][Verilator] add support to dynamically load hardware library (#7286)* add files* remove import* remove os import* reorder header* fix header order cpplint* lint fix	0
[TE] Fix `te.CreatePrimFunc` for 0-dim computation (#11518)For 0-dimensional computation, `te.CreatePrimFunc` creates an opaque block with 0 block iters,which is mistakenly passed into TVMScript auto-completion that failed to add the root block properly.As an example,```python>> from tvm import te>> a = te.placeholder((), name="a", dtype="int32")>> b = te.placeholder((), name="b", dtype="int32")>> c = te.compute(a.shape, lambda *i: a(*i) + b(*i), name="c")>> f = te.create_prim_func([a, b, c])>> print(f.body.block.reads)[a[], b[]]>> print(f.body.block.writes)[c[]]```This PR fixes this issue by enforcing the consistency that `te.CreatePrimFunc`always creates scheduleable blocks with at least 1 block iter:```python@T.prim_funcdef func(a: T.Buffer[(), "int32"], b: T.Buffer[(), "int32"], c: T.Buffer[(), "int32"]) -> None:    # function attr dict    T.func_attr({"global_symbol": "main", "tir.noalias": True})    # body    # with T.block("root")    with T.block("c"):        vi = T.axis.spatial(1, 0)        T.reads(a[()], b[()])        T.writes(c[()])        c[()] = a[()] + b[()]```	1
[GO] Fix go bindings (#7696)	0
[PYTHON][RPC] Make rpc proxy jupyter friendly via PopenWorker. (#7757)* [PYTHON][RPC] Make rpc proxy jupyter friendly via PopenWorker.* Rework the contrib tests that was previous broken.	3
Resolve constexpr related link error in debug mode (#4641)	0
Modify Jenkinsfile to prevent builds from triggering on branch indexing (#10432)Co-authored-by: Noah <nkontur@octoml.ai>	5
add tvm external registry entrypoint (#1562)	1
Add top level redirect from tutorials to tutorial (#9673)	1
[Debug] Add Dump function for Object type (NFC) (#5207)Signed-off-by: Wei Pan <weip@nvidia.com>	1
[TEAM] New reviewer: nishi-t (#1637)	1
[ci] Always assume num executors == 1 (#11014)This is true now and we've seen problems like https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10753/17/pipeline/. This could have arisen from the EC2 user data script that is supposed to set up this env variable failing or something, but we don't really need it in the first place.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Add support for MXNet pad operator. (#3739)MXNet pad is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.padAdd support for parameter 'None' in MXNet slice operator.MXNet 'slice' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.sliceAdd support for MXNet cos, sin, arctanMXNet 'cos' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.cosMXNet 'sin' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.sinMXNet arctan is descirbed athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.arctanAdd support for MXNet 1D Convolution and 1D DeconvolutionMXNet convolution is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.ConvolutionMXNet Deconvolution is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Deconvolution	1
[TOPI] Print shape information when the input shape not compatible with (#9876)reshaped shape.[Issue]When the input shape not compatible with reshaped shape and not dynamicin topi reshape, the two shape sum value will get printed out as an errormessage, but such shape sum value is not helpful for the trouble shooting.[Solution]Print the shape detail, user can use such information to located relatedoperator for trouble shooting.	1
[microNPU] Fix output mismatch in Leaky ReLU (#11397)* [microNPU] Fix output mismatch in Leaky ReLUAll codegen tests have been running with a representative datasetbetween 0,1 which masked an output mismatch in Leaky ReLU when comparedto TFLite kernels. This issue can be replicated by replacing therepresentative dataset range with something like -1,1.To fix this mismatch, we use the same implementation for calculatingLUT values as Vela which uses arithmetic constrained to quantizedvalues, rather than the previously used floating point calculations.Change-Id: I0ed52215acd27722873be609271971b6fc4aaef1* fix lintChange-Id: Ica7de0c000ee015e79fe10985b2ec7a9b341861f* fix lint againChange-Id: I005d90ad248bfff7090f99d161eefbdc962cba48	4
Updated install from source docs to include additional instructions for M1 macs. (#11675)Co-authored-by: Noah Verke <noahverke@nverke-MBP.local>	1
[COMMUNITY] Hongyi Jin -> Reviewer (#11998)	3
[BYOC][DNNL] Improve performance of DNNL BYOC dense operator (#11513)* Enhance dnnl byoc dense operators performance by 1) introducing gelu fusion and 2) introducing alter dense weight layout.* fix lint issue* add unittest for dense pack* Make code compatible after introducing TensorRequisite(PR-11345)* Fix comments & refactor code* Fix lint* Fix partition graph unittest case* Fix comments* Fix comments* Fix lint	0
Update .gitignore (#3199)	5
[hexagon][benchmark] Add workload with [1,?] shape (#11163)Re-enable a benchmark configuration that was failing becauseof a bug in TVM's new dimension-mapping code.	1
[QNN] Fix padding changes due to #3739 (#3989)	4
add support for softmax and log_softmax with MIOpen (#8543)	2
[NNVM][TOPI] Add mean and product operators (#1628)* Add mean and product operators* Fix typo* Fix lint* fix test* Fix gpu schedule* Update doc* remove mean from topi* Add nnvm test* Fix cuda schedule* Remove cuda schedule	4
[TOPI] Fix declaration_conv2d_transpose_impl (#6428)	0
Update docs for some new modules (#2454)	1
[RELAY] [VIRTUALDEVICE] Change syntax for device planning and store parameter virtual devices in virtual_device_ field (#10352)* parent 33082e0032fb57b0516ad7e3eabd11fe0203437eauthor electriclilies <lilyorthsmith@gmail.com> 1643141097 -0800committer Lily Orth-Smith <lilyorthsmith@gmail.com> 1645560059 -0800Store function param virtual devices in virtual_device_ fieldFix test_annotation.py and change result_virtual_device to virtual_device* Change plan devices tests to use the new syntax for function parameters* Fix free var problem* Fix attribute parsing if there is virtual device; most device planning tests passgit status* fixed lambda lifting* Debugging high order functions -- right now FunctionOnDevice and Bind are mutually recursive. This needs to not be the case.* tests pass wootgit status* Remove FunctionOnDevice from device planner* Don't use MaybeFunctionOnDevice in VM compiler* Remove MaybeFunctionOnDevice from lambda lifter* Delete FunctionOnDevice and MaybeFunctionOnDevice!* Reomve GetFunctionResultVirtualDevice* Remove GetFunctionParamVirtualDevice* lint* lint* Python formatting* Remove FunctionOnDevice python test* Fix bug in binds & debug output* Fix text printer* lint* Remove function on device from fold constant tests* Mark nits* Revert behavior of bind* clean up debug* Make ExprBinder public interface and use instead of Bind* Fix lambda lift* This is broken but not sure how to fix* passes all device planning tests yay!* Add substitution helper and use in device planner* Remove unnecessary check* Respond to comments* Update comment	5
[TVMC] use common function to obtain target from --target value on 'tvmc compile' (#6788)- This is solving a TODO item on tvmc	2
Fix docstring of ConcatRel (#1912)	2
[relay][pass] Annotation for heterogeneous compilation (#2361)	4
fix:getcwd not work on android platform (#7390)* fix:getcwd not work on android platform* replace `exit()` with `_exit()` on subprocess in `cpp_rpc`Co-authored-by: rqg <ranqingguo318@gmail.com>	1
[hexagon][testing] sequential input tensors (#12168)Provide mechanism to let unit tests initialize input tensors with sequential element values.	5
[JIT] Force finalization of JITed code, expose sf/hf runtime functions (#12187)* [JIT] Force finalization of JITed code, expose sf/hf runtime functionsCode that handles fp16 and fp32 may end up calling builtins that do theconversions between these types. LLVM emits calls to __truncsfhf2, and__extendhfsf2, which are not present in TVM or TVM runtime. This createstwo problems:- Problem 1: JITed code that does the conversions will fail because it  calls non-existent functions.Adding these functions to libtvm.so/libtvm_runtime.so solves this part,but there is another issue:- Problem 2: JITed code may still not call these functions, because thegenerated object may not be fully resolved.To force full resolution, try to obtain an address of a non-existentsymbol.* Restart CI	1
[DOCKER] update to include deps (#1326)	5
fix miopen pad (#5433)	0
[TIR] Disallow unused rhs vars in GetAutoTensorizeMapping (#12225)	1
[ci] Re-balance shards (#12473)Replace '> >' in templates with >>, NFC (#12615)The problem with greedy lexing of >> as an operator was solved inC++11, and now templates no longer require spaces between >'s.Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>	1
[Relay][Training] Additional gradients (#8307)	1
[RUNTIME] Make rutnime DLPack compatible, allow new device plugin (#71)* [RUNTIME] Refactor runtime to be DLPack compatible. Enable plugin of new runtime.* fix mac compile* ok	0
Support standardize runtime module (#4532)	1
[TOPI] Add valid auto tvm for Intel Graphics (#4078)* add valid autotune* fix pylint	0
[TVMC] fail gracefully in case no subcommand is provided (#6625)	1
[COMMUNITY] new committer -- giuseros (#8956)	1
[OPENCL] Make use of cpu device when gpu device doesn't exist. (#2076)	1
[ARITH] Fix lowering of FloorMod (#4236)	0
[CI] Fix Rust path and remove wasmtime from ci_qemu (#10001)This fixes the Rust path to ensure `cargo` is accessible in thecontainer.As ci_qemu is targetted at microTVM it likely won't make use of wasmtimeas a dependency - it's used instead for the JS and WASM standalone applications asfar as I can see.	1
[Rust] fix #6205 (#6207)	0
[BUILD] Remove libtopi from the build (#6189)	4
Add operation scatter_add to relay, based on scatter implementation. (#6030)	1
[REFACTOR][CODEGEN] codegen->target, build_module->driver (#4742)This PR moves the codegen related code into the target folder,as they are target specific functionalities.We also adopt the term "compiler driver" in common compiler infrasuch as rust, GHC and clang.As a result, build_module is moved into the driver folder.	4
[CI] Update to latest (#6812)- Fix Keras related regression.	0
[Fq2i][ fix output type on fq2i binary ops with constant inputs (#12236)* fix output type on fq2i binary ops with constant inputs* allow off by one on test	3
[MetaSchedule] Upstream the leftover changes (#10689)* [MetaSchedule] Upstream the leftover changes* Update driver_api.cc	5
TVM debugresult dump to Chrome Tracing (#2922)	0
Add support for Tflite operator SPLIT (#3520)* [RFC] Initial support for Tflite operator SPLITThis patch adds initial support for the tflite operator split. HoweverI am not yet sure how to handle the axis parameter for the splitoperator and support it in the test infrastructure. Putting this up foran initial review and comment.The split operator in tflite according tohttps://www.tensorflow.org/lite/guide/ops_compatibilityappears to take num_or_size_split as a 0D tensor.I also note that tflite.split is one of the few operators that returnsmultiple outputs and thus the helper routines in the tests needed somemassaging to make this work.@apivarov , could you please review this ?Thanks,Ramana* Fix the axis parameterAdd more tests* Address review comments* Try out frozen_gene's suggestion* Handle split of 1 element* int32 is only supported in tflite 1.14, let's check that version here.* Keep this at python3.5* Add packaging as a python package to be installed	1
[Rust] Improve NDArray, GraphRt, and Relay bindings (#6563)* WIPWIP* Add support for loading Python packed functions* Flesh out Relay AST in Rust* More tweeks for getting functions out* Deploy Rust docs as part of build* Add some more types* Introduce NDArray 2.0* Work on NDArray 2.0 before restoring tests* Formatting and code fixes to get it to compile* Add more Rust bindings- Converts Conv2d attrs to use tvm::String, so that we can add Rust binding- Uses Type for checked_type in Rust bindings- Fix type key in Rust bindings- Make data field contain NDArray in Rust bindings* Clean up object ptr passing.* WIP* Add debugging for NDArray and fix all test cases* Add breaking test* Dispatch some todos* Format* Fix ndarray size and len* Add BiasAddAttrs rust bindings* Add DenseAttrs rust bindings* Change to TVM string* Add more Rust bindingsAdd GlobalPool2DAttrs Rust bindingAdd ExpandDimsAttrs Rust bindingsAdd MaxPool2DAttrs rust bindings* Fix some test attributes* Improve the NDArray api* Fix some more ndarray stuff* Get the resnet demo kinda working* Add SoftmaxAttrs Rust bindings* Implement Hash and Eq for Relay Exprs* Add underscore to unused function* Fix broken ass resnet script* Improve some ndarray conversions* Make sure the build script runs correctly* Clean up ResNet example tremedouslyExpose C++ graph runtime via cleaner Rust API rewrite example.* Add ASF header* Format* Format* Format resnet rust python script* Add type files and refactor span* Format* Format* Change types from std::string to tvm::String in packed function* Add ASF header* Fix test w/ ndarray's API change* Fix array test* Fix anyhow import* Put back some anyhow stuff* Clean up* Try and fix tests/scripts/task_rust.sh* Disable ResNet for now* Turn off building of Rust docs until we update CI* Actually disableCo-authored-by: Jared Roesch <jroesch@octoml.ai>Co-authored-by: Gus Smith <guscomps@gmail.com>	5
[skip ci][ci] Skip flaky test_cudnn test (#10747)See #10746Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[DOC/TOPI] Add API doc for topi (#226)* [DOC/TOPI] Add API doc for topi* fix lint	0
[TIR] Expose Stack-related TVM builtins in Python (#12429)Added the following operations in TIR:- `tvm_stack_alloca`- `tvm_stack_make_shape`- `tvm_stack_make_array`Co-Authored-By: yongwww <yongcale@gmail.com>	1
iadd conv2d_transpose alter layout (#6358)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>	1
[Relay][Autoscheduler] Fix autoscheduler matmul without units. (#7957)* Fix autoscheduler matmul without units.* Fix lint.	0
add a simplify rule for floordiv(x*8+7, 16) => floordiv(x, 2) (#10232)	1
[Target] Add "features" property to Target (#12121)This adds a generated property "features" to the `Target` which cancontain a read-only list of available features in line withhttps://github.com/apache/tvm-rfcs/pull/78.Features are re-generated upon parsing into a `Target` object rather than beingattached as `attrs`. The `Target` JSON is therefore stored without theinferred `features` attached.	5
[CI] Update GPU docker to cuda10 (#4228)* [CI] Update the ci-gpu to use cuda10* [CI] Enforce tensorcore gpu for unittest	3
[ARITH] RewriteSimplifier: improved cmp simplification (#2851)	1
[CI] Increase CPU Intergration tests shards to speedup runtime (#12316)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>	5
[EXT] Allow easy extraction of extern module (#926)	4
remove compile warning complained by macos clang-13.0 (#9522)	2
[Relay] Enforce static dim for non-concat axis if one or more tensors have static dim (#7487)* enforce static dim for non-concat axis* assign any when all dims are dyn* add missing case* simplify* add test* only enforce static dim constraint if concat output is dynamic* more update to concat type rel* update tests* fixed compile warning	2
[LLVM] Remove PrintModule (defined in llvm_common.cc) (#11851)* [LLVM] Remove PrintModule (defined in llvm_common.cc)The only use of that function is commented out. `llvm::Module` can beprinted directly to `llvm::raw_ostream` via <<, so it's quite easy toinsert printing code when needed:```  std::string s;  llvm::raw_string_ostream os(s);  os << module;   // s (or os.str()) has the LLVM IR text```* Restart CI	1
[Hexagon] Add USMP tests (#11279)* Add USMP tests* Address Chris comments* Address Chris comment on assert* trigger	3
[Target] Add target host field for target specification (#7462)* Add target host field in Target* Add host as a config field to target* Add target host support for Python api* Add unit tests* Adjust format for cpplint* Remove unnecessary  after  in Python file* Remove redundancy and add param description* Fix format issue* Fix param description* Add unit test for duplicate target hosts	1
[Parser] Parser 2.0 part 2  (#6162)* Add code from livestream with JK* Fix errors parsing ResNet* Parse metadata section efficiently and do most of plumbing to resolve metadata section references.* WIP* Change meta reference to an operator* Meta references now work* MetaReference expansion now works* Start working on source map and move diagnostic context* Convert tokenizer and parser to use new machinery* Kill to_json* Fix comment in type_infer.cc* Remove old parser* Rename parser tests and remove old ones* Record span end information* Convert to using spans everywhere* Add span fields back to all Relay constructors* Start passing spans* Pass spans around visitors* Format* Fix* Fix* disable reference lint from string helpers* Fix tokenizer* Fix issue with empty metadata section* Document new span fields and small tweaks* Formatting* Add span doc fields* Add format tweak* Improve errors and fix the semantic version tags in Prelude* Update gradient.rly* Clean up broken spans* Clean up parser tests and turn on previously skipped tests* Update errors to handle skipped cases* Tweak* Tweak* Format* Fix some minor issues with ADT tests* Format* Fix path* WIP* WIP* Fix ir_text_printer* format* Formatted* More formatting* Repair test cases* Fix CI* Retrigger CI	0
Finalize tensor and operation	5
[CRT runtime] Added functions TVMPlatformBeforeMeasurement and TVMPlatformAfterMeasurement (#11244)* Added functions with weak links before and after TVMFuncCall in the TimeEvaluator* Fixed lint* Clang changes* Added clang proposal* clang-format proposed changesCo-authored-by: Federico Peccia <peccia@fzi.de>	4
[CMSIS-NN] Scalar to tensor constant pass to support only qnn.add and qnn.multiply (#10563)* Scalar to tensor constant pass to support qnn.add and qnn.multiply only.Co-authored-by: Luke Hutton <luke.hutton@arm.com>Change-Id: If9cb41d0dd3f56666b6a2c0d9903502d3f9e4eae* Created a function to check if an expr is worthy of passChange-Id: I67250a6214a2d54ef07d54d84eac4ce91474bb0eCo-authored-by: Luke Hutton <luke.hutton@arm.com>	4
[MXNET]abs, round, reciprocal, sign, softsign, hard_sigmoid (#5587)	5
extending FindVulkan to build RPC server on Windows correctly (#6498)Co-authored-by: Mei Ye <meiandmimi@yahoo.com>Co-authored-by: Mei Ye <meiandmimi@yahoo.com>	5
[Frontend][Tensorflow] Sparse_Dense Op CSR scheduling issue resolved for Cuda & X86 (#7148)* [Frontend][Tensorflow] Sparse_Dense Op CSR scheduling issue resolved for both cuda & x86* [1] Review comments handled* [2] Review comments handled* [3] Review comments handled	0
Better to check Infer result with topi results at build time instead of leaving to a runtime error. (#476)	0
[Docker] Pin sphinx version to workaround sphinx-gallery bug (#9822)* [Docker] Pin sphinx version to workaround sphinx-gallery bug* also pin xgboost to suppress a warning from coremltools	2
fix c api	0
[Arith] Support integer BufferLoad in IntervalSetEvaluator (#10327)* Deal with BufferLoad* Test for BufferLoad* Fix comments and CI	0
add a test for assymetric padding in ONNX conv and fix importer (#6646)	2
[Meta Schedule][M3b] Builder (#9044)* [Meta Schedule][M3b] BuilderThis PR is part of the meta schedule project (https://github.com/apache/tvm/issues/8473)Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* add typing* unreachableCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[ci] Mark `test_autotune_conv2d` flaky (#10298)See #10297cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[PlanMemory]Avoid inplace for kNullop requests (#68)	5
[VTA][TSIM] Serial GEMM Application Added (#4082)* app init push* fix on readme* change name, add bit serial explanantion* rm serialLoadMM, change doc* syntax change for readme* add parallel test functionality* fix readme* add python doc* syntax	2
Add logging to diagnose flaky ci-qemu test (#7610)	3
[Relay][TOPI][OP] intel_graphics conv2d alterlayout support relay, added stack op (#2729)* add stack op frontend* concate moved* topi stack added* stack added* fix stack bugs and tested* conv2d alterlayout udpated for relay* fix pylint* fix cmake warnings* cmake warnings fixed	0
[CI] Update `ci-qemu` to use python 3.7 (#10815)	1
[TEST] Xavie initialization for benchmarks (#54)* [TEST] Xavie initialization for benchmarks* remove additional line	1
[PASS] add a pass for the specific hardware accelarator when it is not binded (#1999)	4
[CI] Add apt repository for clang-11 and llvm-11 (#6256)- Add specific apt repositories to install clang-11 and llvm-11 - Fix #6255	0
[BYOC] Remove kCompiler attr from external functions (#5615)Functions destined for external codegen keep their kCompiler attribute which means SkipFunction returns true when running a pass over such functions during the codegen step. This makes sense during graph partitioning, however when lowering the functions for codegen the is no reason to keep this behaviour.Allowing this behaviour will mean a codegen can run a pass on functions only intended for one 3rd party library. Specifically, allowing pre-processing of a series of sub-graphs right before it is passes through codegen. This helps ensure that the functions destined for the 3rd party library are in the expected format. For example, we may want to ensure that these functions have a kernel layout of OHWI because the 3rd party library only supports OHWI. This wouldn't be possible before partitioning the graph as we don't know how the graph will be partitioned ahead of time.Change-Id: Ia68b9da335ef1acfc405a8528aac823de60a65c2	4
fix asan check heap-use-after-free (#2071)	1
[PY] Require python3.6 (#5057)	1
TIR Schedule primitive - decompose_padding (#12174)Co-authored-by: baoxinqi <wrongtest@intellif.com>	3
Clean up uTVM demo runtime, add ONNX model test and tutorial (#7557)* Some docstring fixes.* Couple of small fixes:- Use `west attach` instead of `west debug` in commandline to prevent  debugger from resetting device.- Fix warning on use of led_pin in zephyr-runtime/src/main.c.* Adding Zephyr demo runtime.* Cleanup of uTVM tests and demo runtime.* Working on QEMU support.Need to add board-specific prj.conf files.* Adding board-specific prj.conf files.* Some cleanup.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Adding data for unit tests.* Cleanup demo_runtime code.* Fix up tutorial.* Couple of small fixes:- Use `west attach` instead of `west debug` in commandline to prevent  debugger from resetting device.- Fix warning on use of led_pin in zephyr-runtime/src/main.c.* Adding Zephyr demo runtime.* Cleanup of uTVM tests and demo runtime.* Working on QEMU support.Need to add board-specific prj.conf files.* Adding board-specific prj.conf files.* Some cleanup.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Adding data for unit tests.* Cleanup demo_runtime code.* Fix up tutorial.* Fix tutorial.* Fix tutorial and runtime.* Fix merge conflicts.* Fix merge conflict.* Remove redundant files.* Revert dep.* Fixup* Add new files to check_file_type.py.* Adding missing ONNX file.* Fixup docs.* Fix linting rule.* small fixes* Add missing file to check_file_type.py.* clang-format this file.* Fix formatting.* Black formatting.* Lint comments.* Fix path for test.* Bump CI.* Update from_onnx.* fix path* Fixing* Revert dmlc-core to 21cc7de0dc9fd6acb796e1be6181fa8e6b6c8f41* Fix path again.* Fix tutorial to not use actual Zephyr.* Revert submodule version change* Fix bad merge.* Trying to fix this mess.* Fix formatting.* context -> device* Removing tutorial since I can't get it to pass CI.Co-authored-by: Mehrdad Hessar <mehrdad.hessar@gmail.com>Co-authored-by: Andrew Reusch <areusch@octoml.ai>	5
[Target] Several minor corrections to the device property query (#8651)- Pass parameters through TVMRetValue as std::string instead of  runtime::String- Remove escaping of spaces inside quotes for target attributes.  Updated unit test to verify round-trip behavior.- Added missing "device_type" query for Vulkan.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[RPC] Fix Server connecting to RPC Tracker through a Proxy (#9210)	0
Pluggable Thread Launching Mechanism (#991)	2
[Community] Bohan -> Committer (#9833)* [Community] Bohan -> Committer* Update CONTRIBUTORS.mdCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	5
check in (#3089)	5
[Metal] Reduce number of threads for reduction layers (#8206)Reduced default number of threads in reduction kernels for Metal.Default code generation generated thread block with the following size:32x32x1. With this size number of threads per threadgroup was equal to1024 (32 * 32 * 1). Sometimes device doesn't have enough resources andin this case we will get an exception that the block size is greaterthan value of maxTotalThreadsPerThreadgroup.To prevent such situation we decrease default number of threads. Withthis fix every model should work with default codegen and auto-tuning orauto-scheduling will select the optimal number of threads.	1
Delete pytest-results as part of CI workspace preparation. (#8594)* Otherwise, stale pytest-results could appear in builds.	3
[Frontend] [Tensorflow] ReadVariableOp operator support (#4952)* tf frontend read variable op* pylint fix* tf frontend freezed graph pruned ops	1
[Relay] Fix bug in transpose_shape_func (#6180)	0
Fix conda 2 (#1456)	0
[MetaSchedule][Refactor] Clarify Integration Logic (#10927)	2
[CI] Update docker image ci_cpu,i386 to include verilator (#3738)	2
[RELAY][OP] conv2d_transpose (#1862)	5
force code object v2 for amd gpu backend (#4099)	1
[QNN] Renaming tests to follow the Relay nomenclature. (#3975)	3
fix an index out of bound problem of cache write block (#10203)	0
[AUTOTVM] Fix measurement for CPU (#1956)	0
[microNPU] Mean legalization support (#9576)Supports legalizing a Relay mean operation to an equivalent series ofNPU operations. Mean can be legalized given one of three cases:    - Case 1 (axis == [1, 2] and keepsdims == True):        depthwise_conv2d + binary_elementwise    - Case 2 (ifm qparams == ofm qparams):        pooling    - Case 3 (else):        depthwise_conv2dCo-authored-by: Rishabh Jain <rishabh.jain2@arm.com>	2
[RPC] Improve RPCServer AsyncIO support. (#5544)* [RPC] Improve RPCServer AsyncIO support.When the RPCServer is in the async IO mode, it is possible for the serverto directly serve async function that may return its value via a callback in the future.This mode is particular useful to the web environment, where blocking is not an option.This PR introduces the Async support to the RPCSession, allowing the AsyncIO driven serversto serve the async functions. These functions will still be presented as synchronized versionon the client side.Followup PR will refactor the web runtime to make use of this feature.* Address comments	1
Support QLinearAdd from onnx runtime com.microsoft contrib ops. (#8305)* support QLinearAdd* fix comment line length* use platform independent temp directory	1
[skip ci] Fix onnx frontend lint (#10363)This was broken in #10267, not sure how that commit passed CI (maybe some logic to figure out the PR diff in pylint is broken).Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[RELAY][TRANSFORM] Migrate buildmodule to transform (#3251)	5
minor tweak of the runtime doc to fix some grammatical and expression issues (#828)	0
[TOPI] Improve performance for dilated convolution (#2107)	1
[DOCS][REFACTOR] Clarify Docs Categorization (#6155)This PR categorizes the docs into a few categories:- How To- Tutorials- References- Deep Dive- MISCCo-authored-by: Chris Hoge <chris@hogepodge.com>Co-authored-by: Chris Hoge <chris@hogepodge.com>	2
[KERAS]Global MaxPool3d and AvgPool3d support (#5098)	1
[Relay][Frontend] add log op in tf frontend (#3111)* [Relay][Frontend] add log op in tf frontend* address comment	1
Remove clang-7 requirement for vulkan. (#8107)* Breaks build with new 18.04 ci-gpu docker container.	2
[Infer] More robust inference, support backward inference (#54)	5
[VTA][TSIM] Enable TSIM CI Testing (#4407)* Update task_python_vta.sh* install sbt=1.1.1 with apt-get* update verilator_opt* install verilator with major version 4.0* disable multi-threading for now* bug fix for correcting uop fetch address in LoadUop module* bug fix for correcting uop fetch address in LoadUop module* adjustment to read from dram_offset* enable USE_THREADS with verilator 4.x* DEBUG: try avoid core dump with verilator 4.x* bug fix in LoadUop module* log mega cycles in tsim* download cat.png to avoid fetching in each run* bug fix in LoadUop module* solve dram_even/sram_even issue* bug fix* introduce scalalint in ci* speedup tsim in ci* bug fix* lint scala code before building* disable multi-threading* split fsim/tsim script* update Jenkins settings* duplicate task_python_vta_fsim.sh as task_python_vta.sh for nowCo-authored-by: Thierry Moreau <tmoreau@octoml.ai>	5
[Docker][Vulkan] Allow Vulkan GPU access in docker container. (#8784)- The environment variable NVIDIA_DRIVER_CAPABILITIES must include  "graphics" in order to expose Vulkan drivers to the container.  This  is added both to Dockerfile.ci_gpu for future image builds, and to  docker/bash.sh for compatibility with current images.- The configuration files needed by the vulkan launcher and glvnd must  be exposed to the container.  These are only included in  `docker/bash.sh`, as they may vary by host and so cannot be baked  into the image.	2
[Refactor][VM] Port memory_alloc to c++ (#7369)* Port memory_alloc to c++* remove memory python pass	4
[TVM] Fixed SPIR-V codegen for OpControlBarrier (#1409)	0
[Relay] Show yolo detection result in text. (#6367)* [Relay] Show yolo detection result in text.Issue:Current yolo detection only provide a image drawing solutionto output detection result, but for console user, such resultwould not available for a view.Solution:Here we add a text show function to show detection result intext format for testing in console scenario.* rebase upstream and merge code change.* address review comments.	1
Silence unnecessary 'host' deprecation warnings (#11499)	2
[Relay][VM]VM Profiler (#3727)* [Relay][VM]VM debugger* Report mean/min/max for op duration* Typos* Lint* Lint* Lint* Support build debug VM in CMake* Lint* Enable VM debug in unit test* Disable debug vm test until new docker image is built* Add device sync code* Fix qnn unit test* Disable vm debug by default* Rename files* Rename classes* Fix comment* Fix comment	0
[IR] Add storage scope to PointerType (#8017)* Add storage scope to PointerType.* Apply suggestions from code reviewCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[RELAY][OP] Support MXNet-style attributes for reshape_like (#6851)* add MXNet-style reshape_like attrs support* lint* document, switch to int, add more tests, style* add example usage in documentation* fix doc formatting	2
[Doc] TVM release process (#5151)* [Doc] TVM release process* fix tag* remove things not apply	4
[Codegen] Support broadcast op with symbolic shape (#3389)* [Codegen] Support broadcast op with symbolic shape* fix case where last dim = 1* use enum; simplify stride calculation; improve doc* fix lint* improve py doc	2
[TEAM] yzhliu -> committer (#1509)	5
[TVMScript] Use // and % for FloorDiv/FloorMod (#9437)	1
[Parser] Fix parsing op string attributes (#6605)Co-authored-by: Lei Liu <lei.liu@streamcomputing.com>	0
[Relay] Handle memory scope during lowering from relay level (#11874)Relay expressions can have assigned virtual devices with certainmemory scope. This change landing of memory scope information fromRelay level to tir	5
[CRT]fix to reduce RAM size during loading model (#5507)* [CRT]fix to reduce RAM size during loading model* Release graph_json memory immediately after reading	5
[DOCS][PY] Sphinx docs about tvm.ir	2
Fix mutate auto unroll (#6807)	0
[Bugfix] Nms_ir data_race solved (#2600)* nms data race solved* tst_topi_vision reference results are gonna be updated in PR #2353* proposal nms_ir updated	5
[Relay] Add Rsqrt to SimplifyExpr (#12363)* Add Rsqrt to SimplifyExpr* fix unit tests	3
Fix Arduino workspace alignment (#10886)* Fix Arduino workspace alignment* Fix linter error* Rerun tests	3
[BYOC][ACL] Prevent dilated pooling (#8149)* [BYOC][ACL] Prevent dilated pooling Added check preventing avg_pool2d and max_pool2d to bescheduled for execution via ACL* runtime if dilation otherthan (1, 1) is provided as ACL does not currently supportdilation attribute in pooling layer.*ACL stands for "Compute Library for the Arm® Architecture"Change-Id: If8f65d3a154e09f880bec73dd756d9f985a20ff2* linterChange-Id: If91809350786e69f59596301e0cbd3def6815cd0	4
Update TVM Version and CI scripts (#46)	5
Fix lib64 not found error when building with cuda for OSX (#782)	0
[TOP][COMPILER] sum, min, max, transpose, fix dense (#28)	0
[CMAKE] Windows support upgrade (#125)* [CMAKE] Windows support upgrade* Fix lint	0
[microNPU] Replace ICHECK with diagnostic context in type inference (#9470)[microNPU] Replace ICHECK with diagnostic context in type inferenceConvolution and depthwise convolution use the ICHECK format oferror checking during type inference. This PR updates these checks touse the diagnostic context.	1
[MetaSchedule] XGB-based Cost Model (#9859)* [MetaSchedule] XGB-based Cost ModelCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix lint* fix doc* fix mypyCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>	1
[RUNTIME][VULKAN] Fix compiler warning (#4559)	2
Explicitly disable pylint warning subprocess-popen-preexec-fn (#2656)	2
Add support for Tensorflow operators log1p, cos, sin (#3614)The patch adds support for Tensorflow operators log1p and cosTensorflow log1p is described at https://www.tensorflow.org/api_docs/python/tf/math/log1pTensorflow cos is described at https://www.tensorflow.org/api_docs/python/tf/math/cosTensorflow sin is described at https://www.tensorflow.org/api_docs/python/tf/math/sin	2
Fix inplace. Only permit shared when shape and dtype is the same (#45)	0
[AutoTVM] select model with the most tuned schedules (#4404)* select model with the most tuned schedules* change detect empty map method* modify model description for load_reference_log	2
[topi] add ARM v8.2 udot (uint8) support (#3978)* [topi] add ARM v8.2 udot (uint8) support* fix test case* fix common conv2d schedule* add back fp32_time in test* fix lint* fix doc, add support for int32_lanes=4, signed int* fix lint* add ic_bn % 4 checker in schedule	1
[microNPU] Add support for transpose convolution (#9855)Adds support for legalizing transpose convolution toa microNPU conv2d operation for the case when strides==(2, 2),dilation==(1, 1) and no padding of the output is required.Change-Id: I485e2571913b3dcd7c75c46304f2f9a82f630ee0	4
[FRONTEND][TENSORFLOW] fix the convertion of sum and add testcase for it (#1654)* [TENSORFLOW] fix the convertion of sum and add testcase for it* delete checking tyoe of axis and divide reduce test	3
[Unittests] Added a meta-test for tvm.testing.fixture behavior in case of a broken fixture. (#8343)In these cases, the test function should be marked as failing thesetup, and should not run.  This is pytest's default behavior, andshould work whether or not a fixture is cached.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[TVMC] Separate model loading from model compilation in TVMC. (#7739)* add to init files for clean tvmc python* adjust tests to new imports* add to compiler.py* update so model loads in drive_compile* update test_compiler.py to load outside of tvmc.compile, need to correct one error* fix mock.patch test* remove merge artifact (circular import issue)* change typo and merge artifact* fix import in test_compiler.py* black needed files* remove unnecessary argument model_format from compile_module* load before compile in conftest.py* fix conftest.py issue* fix typo in test_compiler.pyCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>Co-authored-by: Josh Fromm <jwfromm@uw.edu>	3
[SCHEDULE] New Reduction Mode for Tensorize (#727)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.	0
add pool (#478)	1
[bug fix] skip "__nop" functions in graph_executor_debug (#11353)* bug fix, skip __nop functions in running operation over RPCCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
conv1d_transpose speedup. (#6840)Improve performance of transposed convolution by avoidingredundant multiplication by zero values from dilated data.Co-authored-by: Ubuntu <ubuntu@ip-172-31-74-104.ec2.internal>	5
Fix use of wrong variable (#8227)* Fix use of wrong variable* Fix docstrings	2
Fix conv2d int8 schedule on CUDA (#2074)	0
[CONTRIB] PopenPoolExecutor (#6959)PopenPoolExecutor implements a ProcessPoolExecutor backed by popen.- Only handles invoking functions in tvm namespace.- Unlike multiprocessing, does not require __main__ block,  which means it can directly run on jupyter notebook.- Come with timeout and fault tolerant support to timeout  long running jobs, and restart the process when an error happens.Recommended usage: it is recommended to create a pool and reuseit in a long running job(e.g. autotuning) so that the processare reused when possible.	1
[Torch] Fix aten::max and aten::min conversion (#6372)* fix aten::max and aten::min conversion* remove print	4
[TIR] Fix reverse_compute_at for trivial region with trivial block var (#11234)* [TIR] Fix reverse_compute_at for trivial region with trivial block var* Prevent handle arithmetics	0
[TOPI][x86] Cascade lake support. (#4123)* [TOPI][x86] Cascade lake support.* Jenkins test debug 1.* Testing cascade lake alone.	3
Fix missing round(), floor(), ceil() for target C lowering (#7382)	1
[ARITH][SCHEDULE] Update schedule to use the new analyzer (#3466)	1
[LLVM] Update uses of AllocaInst::getAlign[ment] (#11718)Today's LLVM main branch removed AllocaInst::getAlignment.	1
[runtime] fix: BooleanToTranspose function definition conflict (#6452)	5
Conv2d scheduler tweaked for super resolution perf (#652)* scheduler tweaked for super resolution perf* lint error fixed* lint error fixed* conv2d_transpose schedule error fixed	0
Fix small typo in nn.conv2d_gemm_weight_transform (#5925)* Fix small typo in nn.conv2d_gemm_weight_transformChange-Id: I7844d898ebf82592f78f478982262ef95f83cc3e* Add TOPI conv2d_gemm unit testsChange-Id: I9ed82a68acffcf0dd9720781f8be4aada9d8e6e4	3
[Relay][Pass] Avoid FoldConstant folding some ops (#4245)* [Relay][Pass] Avoid FoldConstant folding some ops* rename	4
[TIR][Arith] Implemented padded inverses in IndexMap (#11235)* [Debug] Error logging in DetectIterMap* [Affine] Allowed PrimExpr argument to NormalizeIterMapToExprThis allows it to be used for any expression containing an`IterMapExpr`, not just expressions whose top-level node is an`IterMapExpr`.* [Affine] Implemented DetectPaddedIterMapThe existing DetectIterMap tries to rewrite index expression as alinear combination of split/fused iterators, where the new iteratorscover the exact same indices as the original expression.DetectPaddedIterMap relaxes this condition, allowing the new iteratorsto cover a superset of indices that the initial index expressioncovered.  It uses the minimum amount of padding necessary to representthese transformations, and also a predicate that identifies anypadding that has been added.This is a utility function to be used for layout transformations ofbuffers, in cases where the pre-transformation shape of the bufferdoes not evenly fit into the post-transformation shape.* [IndexMap] Implemented IndexMap::NonSurjectiveInverseAllow non-surjective transformations, with DetectIterMap used todetermine the minimum padding to insert.  Returns the inversefunction, along with a predicate that identifies padding indices.  Thepredicate is in terms of the transformed variables.* [IndexMap] Exposed methods to python- `IndexMap::Inverse` exposed as `IndexMap.inverse`- `IndexMap::MapShape` exposed as `IndexMap.map_shape`- `IndexMap::NonSurjectiveInverse` exposed as `IndexMap.non_surjective_inverse`* [IndexMap] Extracted _assert_equal_index_map into class methodIn preparation for adding additional tests for the IndexMap class,which will require this functionality.* [IndexMap] Added unit tests for new behavior* Re-enabled divisibility check in CheckMappingInitially disabled as dynamic shapes resulted in padded lengths whosedivisiblity couldn't be proven.  Re-enabled along with asimplification rule to resolve it.* Fixed breakage in compute_at primitive* Corrected typos/examples in docstring	2
[DOC][TUTORIAL] Fix typo for deploy_model_on_android.py (#5123)	2
fix pynq 32-bit address pointers (#3558)	1
Fix rust tests (#2482)	3
Fix a spelling mistake (#128)	0
check for CMSIS_PATH in project generation (#12547)Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
[Auto Scheduler]add task name during printing table info (#11098)* add task name during printing table info* address comments and fix lint* better look* fix linting* fix linting again	0
[DOCS] Cleanup docs build instructions. (#6094)	2
add favicon in rtd (#3379)	1
bumping vta version (#6495)	5
[Autotvm] Use VM compile to extract autotvm tasks (#4328)* [AutoTVM] Use vm compile in extracting task from relay* update* restructure vm compiler to reduce task extraction time* x* fix* update doc* udpate doc* lint	2
[Frontend][PaddlePaddle] Add autopad for conv/pool (#9295)* Add autopad for conv/pool* add autopad for conv/pool* fix pylint warning* add some annotations* add som annotations* add som annotations* Refactor autopad in the onnx.py and paddlepaddle.py to relay/frontend/common.py* add comment for conv2dCo-authored-by: heliqi <1101791222@qq.com>	1
[RPC][RUNTIME] Support dynamic reload of runtime API according to config (#19)	5
Switch PlanDevices pass to be w.r.t. SEScopes instead of DLDeviceTypes. (#9326)* Switch PlanDevices pass to be w.r.t. SEScopes instead of DLDeviceTypes.CAUTION: Breaking VM executable serialization change. I needed a new 'virtual devices' array in the executable so that instructions can continue to refer to devices by a simple index yet the VM can respect both the device type and id for runtime devices.Continuing from #9313, and as part of apache/tvm-rfcs#38, we switch PlanDevices to plan with respect to SEScopes instead of just DLDeviceTypes. Our ultimate goal is to be able to flow memory scopes between PrimFuncs by re-running PlanDevices after the LowerTE pass. This PR at least gets us to being able to flow the memory scopes, but the actual changes to PlanDevices to look inside PrimFuncs is still two PR's in the future.However, we get two nice side effects right away: - Since SEScopes contain Targets we can isolate all the device-to-target resolution machinery within PlanDevices (with the help of CompilationConfig). After PlanDevices has run we can retrieve the Target for any sub-expression directly from that sub-expression's SEScope. For now we retain the one-Target-per-DLDeviceType constraint since it baked into the public 'TargetMap' API, but the path to breaking that constraint is clearer. - Device ids are now respected all the way from annotation to executor. Previously though we had a bit of plumbing using Devices the device_id therein was ignored or defaulted to zero. The Python "on_device" annotation helpers still work w.r.t. devices. Thus though they now respect device ids, they do not allow the user to specify a Target or memory scope as supported by the underlying SEScope.* [checkpoint] Revert emitter.py, must have run 'black .' by mistake.* [checkpoint] Address PR commentsAlso add back SplitArgs pass in build_module.cc which somehow got lost in the shuffle.(try again -- flaky test_crt.py test_autotune?)* [checkpoint] Fix after rebase on CallLowered.	0
[Hexagon] Refactor directory structure to accommodate new runtime (#9354)* Move hexagon runtime files used for ARM offload to Hexagoninto runtime/hexagon/android and runtime files compiledfor hexagon into runtime/hexagon/hexagon. Use commonhexagon_module.h for both hexagon and android runtimes.* Apply clang formatting to hexagon files on main.* Apply cpp-lint	2
[ci][docs] Disable matplotlib intersphinx (#10808)This is failing in CI so let's disable it until we come up with a better way to deal with network failures like theseCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Frontend][TFLite] Add parser support for shape and range (#5329)* [Relay][Frontend][TFLite] Add parser support for shape and rangeSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Incorporated review comments and used new functionsSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Few cosmetic changesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Removed an extra line added by rebase...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>	1
[Community] @Hzfengsy -> Committer (#8908)	3
[ETHOSN] Ethos(TM)-N 21.11 update (#10061)Minor update in inference buffer mapping due to changes in the driverlibrary	4
[Object] Unify StrMapNode and MapNode (#5687)* Pass cpptest and py unittest* fix graph runtime* right fix* fix a bug that runtime::String's operator < is actually compare by address* Update container.py* Renaming* Address comments* lint* Replace ObjectHash in object.py	1
Refactor to expose MakeOp functions to C++ (#6047)* Initial Refactor* add templated nn Make* functions* fix build typo* inline functions, fix unit tests	3
[TIR] Fix lower_warp_memory when there are >1 warp buffers (#5368)* fix recursion in lower_warp_memory* post-order mutation	0
[Hexagon] Replace strlen in constant initialization with sizeof (#10381)Strlen is not constexpr everywhere, so replace it with sizeof.In C++ sizeof("string") works fine, since "string" has type"const char [...]".	1
[DNNL] Fix end of line in test_dnnl UT file (#11560)	2
[BYOC][ETHOSN] Introduce the Ethos-N BYOC integration (#6222)* [BYOC][ETHOSN] Introduce the Ethos-N BYOC integrationThis is the first of 3 PRs to introduce the Ethos-Nintegration into TVM via the BYOC framework. It addssupport for partitioning and compiling for theEthos-N77 target with CPU fallback for unsupportedoperators. Additionally, runtime support is added inthe form of an Ethos-N runtime module. In this initialPR, only quantized concatenate and split are supportedwith follow-up PRs adding support for many further operators.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>* Turn off USE_ETHOSN_HW by defaultChange-Id: Ie2ce4528e16e93aa83df46f8a229c0ce89b45252* Update capabilities fileChange-Id: Iebd0c62d6bc7e446662abdee4882ac874ad98aa3* Fix missing headerChange-Id: I0c89e380dd1d795755a1884c06a7b317a99fe297* Update cmake comments on ETHOSN_HWChange-Id: I2e96a1c818a82e5174fd94e483b0bdb3e4375a7d* Add checker for case when USE_ETHOSN=OFF and USE_ETHOSN_HW=ONChange-Id: Id5c9cfb866914a0298b44ead40fcbe3764ce443c* Fix 'available' booleanChange-Id: I78e54fb9f472d2815886bea4d94b7247e0d129de* Check availability in op registrationChange-Id: Iecfea7dca7301dd684199c9b32f99f2113fdfd56* Remove unnecessary lineChange-Id: Idf5cab853027adb0b0292de877e6dc02683821d7* Simplify getting output_sizeChange-Id: If4643924768c2d7ea98525e9f792b7223cc2bcdf* Remove unnecessary new lineChange-Id: Ia689c59cac28bd91e237ceecd829d8cf56d0d9c1* Remove NOLINTSChange-Id: I149b97b28b516c7d9288a0858b2fbf1497e70250* Remove unused parts of PRChange-Id: I2db5b89d8fe2c114ab92305cdcf06d0fc45f4d2a* Fix CI Ethos-N settingsChange-Id: Idd955755d6f6d1cd3843462f627d0d952729e467* Removed unnecessary line in infraChange-Id: I0ea866adf5d9166db85dd82d013a631d991ae633* Remove unnecessary len in infraChange-Id: I869e8233d41c6ab7c2dc80f47d976c974043b80c* Rename 'cpu_ops' to 'host_ops'Change-Id: I79a6ffcfd48cd055d279f493c672ec82f0c68e5c* Added explanation on mockingChange-Id: I1e88c07a47464e44cb45c6a327ec9c7e2d70cc94* IsEthosOp -> IsEthosnOpChange-Id: I4fc1b462a74f8fae231ebafac614dd8d45be0feb* Improve documentation in ethosn_api.hChange-Id: I5586a7ba7ce71da667a6a9c6dd2e591028eb43b2* No longer iterate over module when compilingChange-Id: I80e1d494c6d574be06a2375e831343485712914d* Move EthosnCompiler implementations into codegen.ccChange-Id: I5bb6e9f62722d930d9dc040ac62bf87f29dd74c5* Fix lintingChange-Id: Ia44ec741a5330ad289cc6b5cd2bb1ed784fe6afc* Refactor EthosnAPI compilation functions into EthosnCompilerChange-Id: Iee0aecbe43a84fefb437ab9ff064e3f8b42c80a4* Improve docs for Tvm2NpuChange-Id: Ia39e9e1508513ca39c1d585fbccc3ae38fcbb9fb* Move more implementation out of headersChange-Id: I1e33084ceb520b75f06b4d7a4acff5b9b2225bd5* Move implementation in ethosn_api.hChange-Id: I51ab386892a2aa84aa47d03641aac8468f5737ae* Improve docs for capabilities.hChange-Id: Iaaee508aafa1cbb7650a04ed87bd6c1b91823a58* Use else() in cmakeChange-Id: I4b64a87f32b3616ec87c9937d9fc998b8dc5d7b4* Use GetDataSizeChange-Id: I16988f3adbe6e03fc47fa0a77cb5febb7a02eaab* Use const&Change-Id: I664982d219f9a7d1f961dbfe84d12f66e2e5f5cb* Fix python lintingChange-Id: Id965ccc037fd40cbdfcb58d922cc8d5fb8c87dfe* Remove load/save to fileChange-Id: I7f8c3f5c8948c3f15551d28e3fee6e00120663ef* data->dataChange-Id: Ifb861ebbfeaaf4b154f4b1515f83a46aecf86e50* Remove specific cpu targetChange-Id: I920568cc7a81cd77d44f8604f571340a330f3e62* Test export/load moduleChange-Id: Ib605458127485e2015ac012ec515ced5900705f3* Fix cmake garbageChange-Id: I32f3c967192c7c278ef33c52cac5fb5da682cd1bCo-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>	4
[VTA][Runtime] clear sram index in reset (#5470)Co-authored-by: Zhang Hao <zhanghao@4paradigm.com>	1
Add support for FusedBatchNormV3 (#5065)No changes seem to be needed to _fused_batch_norm. It just works.	1
[INTRIN] Enable popcount (#606)* enable popcount intrin* fix lint* add test* fix python3	0
[Fix] fix python setup.py file bug (#12000)* fix setup.py bugSigned-off-by: Zhengqiang Yin <codle@outlook.com>* remove data_files field* keep a init setup_kwargs	1
adding ArgReduceAttr to to inherit from Attrs (#9606)* adding ArgReduceAttrs as python class with register_object wrapper* cleaning	4
[µTVM] Zephyr: Add STM32F746 disco board as a test platform (#7863)Add STM32F746 Discovery board as test platform so tests can run againstit by using:$ pytest test_zephyr.py --microtvm-platforms=stm32f746xx_discoSince that board has the same MCU identifier as the ST Nucleo board,the test platform identifier for Nucleo board is renamed and asuffix _nucleo is added to differentiate it from the ST Disco board.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>	1
[BYOC] Enhance partitioning and external codegen (#5310)* Remove duplicated output args* address comment* fix codegen c* improve comment* VisitExprDefault_* deduce type	1
Unify Python and C++ TIR lower API (#8110)	5
[REFACTOR][DTYPE] Isolate dtype to runtime (#4560)dtype.h -> runtime/data_type.hChanges:- Rename all old reference of tvm::Type to DataType- ExprNode.type -> ExprNode.dtype- Expr.type() -> Expr.dtype()- Change Expr related functions to expr_operator.  - DataType::min() -> min_value(DataType)  - DataType::max() -> max_value(DataType)- Move type constructor Int, UInt, Float, Handle, Bool into DataType.  - Int(bits) -> DataType::Int(bits)  - UInt(bits) -> DataType::UInt(bits)	5
[TIR] compact buffer region (#10557)	5
[Onnx] Add Adagrad (#9001)* adagrad impl* passing tests* docstringCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
Remove an extra print from the relay astext tests (#8342)	3
[DLL] Use local dll, not de-allocate function in shutdown	1
[Relay] Fix relay op strategy for cuda dense int8 (#7586)* [Relay] Fix relay op strategy for cuda dense int8* Remove uint8 && Add autotvm task extraction test for relay graph that contains dense op (int8 * int8 -> int32)* Reformat the code of test case	3
[RELAY][FRONTEND] Initial OneFlow frontend support.  (#8790)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.pyCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>Co-authored-by: BBuf <1182563586@qq.com>	1
[CMAKE] Fix cmake build (#520)	1
[Adreno] Change compute/schedule for ToMixedPrecision pass (#12537)* [Adreno] Change compute/schedule for ToMixedPrecision pass* Address CI fails* address PR comments* Fix AutoTVM flow	0
[Makefile] Fixed error in "make clean" (#10048)The top-level makefile should delegate `make clean` to the cmakefolder of each enabled build, similar to the existing delegation of`make all` and `make runtime`.	1
[Relay] Higher order reverse mode automatic differentiation that work with control flow (#2496)add testremove dead codestashdo itadd more test	3
[VTA] hotfix for de10-nano driver (#4081)Issue:git clone latest TVM/VTA and run VTA on xilinx FPGA board, applicationcrashed due to the "call stack overflow" which caused by a infinite recursivefunction call. this issue ever happen before and get addressed by PR 3843.Analysis:seems like de10-nano driver PR  used old code base then the  logic changeof 3843 get eliminated.Solution:add the logic back.	2
Add manupa-arm to CODEOWNERS (#8911)	1
[COMMUNITY] New Committer -- lhutton1 (#11049)	1
Update frontend for keras 2.1.3 compatibility (#314)* Keras keeps renaming properties. Update frontend for keras 2.1.3 compatibility* Add error message when inbound_nodes is not found	0
[Relay] Fix bug in test_op_level3 (#8508)* [Relay] Fix bug in test_op_level3Test case failed due to missing mode="add"* Empty	1
Add support for keras upsampling (#305)* Add support for keras upsampling* Fix code formatting* Fix indentation* Fix indentation round 2* Hide unused parameter* Only enable UpSampling2D since the others are not currently supported by TVM* Improve error messages and code layout	0
[runtime] reduce set_input and set_input_zero_copy overhead (#3805)	1
[Relay][OP]NMS (#1929)	5
[skip ci][ci] Disable `test_solution_consistency` (#11460)See #11458Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[Contrib][ONNX] Handle removal of onnx.utils.polish_model (#9178)Onnx 1.9 removed optimizers from the core repository (see discussionin https://github.com/onnx/onnx/pull/2834), includingonnx.utils.polish_model, breaking RelayToONNXConverter.  This PR addonnxoptimizer.optimize as a fallback method if onnx.utils.polish_modelis unavailable.Also updates tutorials/documentation to recommend installingonnxoptimizer when installing onnx, because the current PyPI versionof onnx is above version 1.9.	1
Missed out_layout field of conv1d attrs (#11325)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>	5
temporarily disable opengl in test_runtime_ndarray.py (#804)	3
[BugFix] Print doubles with precision 17 in SaveJSON and TVM script printer (#7846)* [BugFix] SaveJSON type double with precision 17* [BugFix] Fix for TVM script printer	0
[SUBMODULE] update submodule to latest (#1728)	3
[Bugfix] Simultaneous layout transform and axis separators. (#10553)Previously, SchedulePostProcToPrimFunc would first generate the mapfrom buffer object to layout transformation, then would update bufferswith the axis separators.  However, it failed to replace the bufferobjects in the layout transformation map, so the transformationwasn't applied.This PR correctly updates the layout transformation map, andadds a unit test to catch this failure mode.	0
[COMMUNITY] new community guideline (#2077)	1
Fix padding in pooling op (#4738)	1
[Graph Debugger] Expose way to benchmark individual nodes. (#11000)* initial* secondary commit* docs* match tests* fix test* use std::fixed, max precision, typed pack func, fix isnan* comments on docs* address tristan comments* add test* tristan comments* use skipif* empty commit* empty commit* jostle again* remove assert statement	3
[TOPI] update depthconv padding api; fix shared memory overflow (#365)* modify depthconv padding* fix shared memory overflow in depthconv schedule	0
[ANALYSIS] Mac count deconv (#3469)* add mac count for conv 2d transpose* add the explanation of missing parameter in docstring* typo* fix pylint	0
Explain how to generate module library in Quick Start tutorial (#323)* Explain how to generate module library* Small fix	0
Hide registration errors in `test_myfloat` (#12268)This should fix the error from #11580, the test will still get run butthe registration failing will be ignored if it happensCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[Arith] Simplify MatchFusePattern in InverseAffineMap (#8427)* [Arith] Simplify MatchFusePattern in InverseAffineMap* fix	0
[COMMUNITY] siyuan to PMC (#10688)	3
[AutoTVM] Use popenpool in local_executor (#8851)* use popenpool in local_executor* move auto_tvm_common to tvm.testing* refactor* nit* remove LocalFutureNoFork* exception handling* handling two exceptions* handling error* add initiazlier	5
[MSVC] Make able to compile with MSVC (#6341)* fix: make suitable for msvc, clang* clang-format* refactor: use DMLC_ATTRIBUTE	1
[TOPI][Relay] Fix default `out_dtype` for `conv2d_NCHWc` and Relay (#2707)	0
[Relay][MergeComposite] Support TupleGetItem in body of pattern (#5106)* Support TupleGetItemNode in body of pattern only* Add bn_relu test case for MergeComposite with TupleGetItem* formatting* TupleGetItemNode::make -> TupleGetItem()	1
[Frontend][Tensorflow2] Stridedslice and concat_v2 fix (#8483)* fix for strided_slice when begin > end in case of shrinkaxis_mask* fix for name_hint missing error for concat_v2 op* removing a local fix* adding more testing capability to concat_v2	3
[Relay][DefuseOps pass] bug fix: To support function body types other than call node (#10069)Co-authored-by: pranav jonnalagadda-SJ1 Eng_ML <pjonnalagadd@sj1mach1.caveonetworks.com>	1
Fix clear-stale-images.sh with multiple worktree. (#11921)* Process substitution doesn't error out in bash.	0
[CODEGEN] add fp16 and fp64 enable pragma for opencl (#697)* [CODEGEN] add fp16 and fp64 enable pragma for opencl* fix style	0
[Relay] format text_printer.cc (#1946)	5
[DEPRECATION] Cleanup legacy verilog support (#4576)This PR cleans up the left over code for legacy verilog support which was experimental.The new hardware backend path is now support by VTA via TSIM.	1
[LANG] Support for Bitwise Operation (#502)* [LANG] Support for Bitwise Operation* Add test	3
[High level OPT][RFC] NNVMv2 IR - Relay (#1672)	5
Several type mismatch fixes and checks (#12041)* Compute common type for shape elements in BroadcastHelperThe corresponding dimensions in the input/output tensors in a broadcastoperations may have the same value, but different types (e.g. int32 vsint64).When the broadcast helper tries to unify the dimensions it also needsto compute the common type to hold the dimension.* Cast and simplify both members of `Range`Only the `min` member was type-casted, which could lead to ranges withdifferent types for `min` and `extent`.Move the casts to the argument of Simplify, so that they can be eliminatedif they aren't needed.* Type-check iv domain ranges, use cast only if needed in MakeLoopNestIn some cases the domain ranges had the `min` and the `extent` valuesbe of different types (e.g. [(int64)0, 32)). This is an error, and itcan lead to compilation failures later on. Add a check for equal typeshere to catch this early.Also, only add the cast operation when the desired type differs fromthe current one to keep the expressions simpler.* Check that variable and substituted expression have same typesAdd a check to IRSubstitute to detect when the type of a variable andthe type of the expression to replace it with have different types.* Add testcase* [TVMScript] Use void for lambda parameters, allow mismatch in SubstituteWhen the script parser deals with lambdas, it creates Var objects for eachparameter. Their actual types are not known at the time, and the properlytyped variables are subtituted in the body later. Since the default dtypeof a Var is "int32", this could lead to a type mismatch in Substitute.To deal with this scenario, use "void" for newly created Vars in theparser, and add an exception to Substitute to allow replacing void Varswith expressions of any type.* Fix type error in test_reduce_combiner_simplify* Restart CICo-authored-by: Jiawei Liu <jaway.liu@gmail.com>	3
Add colors to compute_at edges and thread/block indices. (#5111)	1
[TVMScript] Doc Definition (#12244)This single-file PR is automatically generated by a script that describes the Doc AST.	2
[RUNTIME] Fix Metal runtime compile (#241)	1
fixed algorithm. padding should be on both sides (#489)	1
[TORCH] Implement avg_pool1d (#7694)* [TORCH] Implement avg_pool1d* [TORCH] Unify creation of avg_pooling operations* [TORCH] Add tests for avg pooling with padding* [TORCH] Make format checks happy with unified avg_pool	1
[AMP] Fix IsMixedPrecisionType Edge Case (#9856)* fix ismixedprecisiontype* fix test* jostle* Update tests/python/relay/test_to_mixed_precision.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>	3
Change Azure pipeline badge to GH actions (#5081)	4
CMake/make adjustments and warning fix (#106)* CMake/make adjustments and warning fix* Fix warnings	2
[Topi] Fix arm_cpu bitserial schedule with elemwise ops. (#7929)	0
Add tensorflow Einsum op converter (#12064)* Add tensorflow Einsum op converter* fix lint* fix lint	0
[TOPI] Initial NHWC layout support (#882)* add 4 dim softmax* update for NHWC layout* remove layout param from softmax* fix typo* minor fix to poolsupport axis=1 ndims=5 softmax.add softmax axis* few fix for softmax* fix typo* add more doc* minor doc fix* fix upsampling output shape* fix lint* cleanup softmax* minor fix* raise exception instead of assert, handles negative axis* check axis after axis transformation	0
[BUILD] enhance vta build (#1434)	5
Implementation of tile for TFLite (#3814)	5
Pin black version (#8139)This commit pins the black version to provide stability.It is expected that the pinned version will be moved forward periodically.Change-Id: Ied866bff85a1a832959bc1d4673a7fdec68128a7	4
add Xiaoqiang Dan as reviewer (#1976)	1
[TOPI] modify conv2d_transpose schedule (#613)	5
Fix custom_address serialization in c++ tracker client. (#9192)* Make sure that if a custom address is passed then it is json serialized surrounded by quotes.* Better handling of custom address.* Revert "Better handling of custom address."This reverts commit 77935c53cfb22d7e75067c6218dca5e7deb6953b.	4
[microTVM] Add fixture to zephyr test (#8393)* fix testing* trigger	3
[MicroTVM][PyTest] Explicitly skip MicroTVM unittests. (#9335)* [MicroTVM][PyTest] Explicitly skip MicroTVM unittests.Refactor unit tests so they will show as skipped if `USE_MICRO=OFF`.* Updates following PR review.- Updated to avoid name shadowing of BaseTestHandler- Updated test_micro_transport to use fixture for setup.  Ended up  needing to refactor to use pytest instead of unittest, split up test  functionality during refactor.	4
[PASS] Fix vthread when extern access touching (#636)	0
[TVM][Bugfix] fix storage_rewrite bug when input is big (#2580)* fix storage_rewrite bug when input is big* cast when necessary* simplification* simplification* int64->uint32* revert uint32->int64	4
[RELAY] CompileEngine update, nn conv2d, fix dense, pool. (#2082)	0
Update iOS RPC README (#2847)	5
[Meta Schedule] Add Auto-Thread Binding Rule (#11177)The current meta-schedule uses a PostProc `RewriteUnboundBlock` to auto-bind blocks to threads. However, it's a post proc, which means there are no search opportunities, and always splits with `factor=1024`. This PR adds a new search rule called `AutoBind` to do a similar thing to bind threads with sampled factors. Also with a corresponding mutator. After applying this rule, we get some positive perf results (on RTX-3080):Element-wise: from 2.76 us to 2.48 usConv2d Winograd: from 29.45 us to 18.96 us (ansor 22.00 us)Resnet18: from  0.591 ms to 0.531 ms (ansor 0.565 ms)	1
[LINT] Use fmt off to disable problematic black fmt (#6519)	0
[Relay][OpStrategy] Tweak cublas/cudnn priority level (#5820)* Tweak cublas plevel* update* trigger ci	5
[TVMScript] Make classes derived from ObjectPath non-nullable (#12304)`ObjectPath` is marked as non-nullable, which causes it not to have adefault constructor. The derived classes, on the other hand, are notmarked as such, thus getting an explicitly defaulted default constructor(via TVM macros). This constructor can't actually be called since itends up being deleted, so the derived classes are effectively non-nullable.	4
[intrin] a few more math functions (#5468)	1
[CODEGEN/PASS] add restricted, alignment option (#221)* [CODEGEN/PASS] add restricted, alignment option* fix lint* Fix the alloca	0
[X86][TOPI] Add AutoTVM template for dense (#2392)* Add GEMM autotvm template for x86* Fix tophub link* Disable RPC server logging file delete* Update dense autotvm template* Fix tests* Fix lint* tweak* Register two templates with different tags	0
[QNN] Doc fix on convolution and dequantize (#4799)* QNN doc fix on conv and dequantize* fix param name in tflite frontend* make different fix	0
[relay] Relay annotation and partitioning for external compilers (#4570)* [relay] Relay annotation and partitioning for codegen* Add fusion unit test* fix comments* Update include/tvm/relay/attrs/annotation.hCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* rebase* remove annotation helper* rebase againCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: 雾雨魔理沙 <lolisa@marisa.moe>	4
[Relay][VTA] Add ChangeBatch pass  (#3656)* init* lint* lint	5
fix lower_warp_memory (#5247)	0
[TIR][Schedule] Change BufferIndexType enum in python side to string (#10737)This is a follow-up of #10538 to use string instead of enum in python side.	1
[Fix] remove unnecessary spliting in the cached chunk (#4935)* remove unnecessary spliting in the cached chunk* remove unnecessary spliting in the cached chunk	4
[CI] Move cpu-only frontend tests to a CPU stage (#5807)	3
[Relay] add ClipByValue and Neg in tf frontend converter (#3211)	1
[WIP] [NNVM] Fix softmax gradient (#1201)[NNVM] Fix softmax gradient	0
[DOCS] Neural network Deployment Guide with System Module Mode #1523 (#1533)	5
Refactor Dynamic to Static (#7368)* DynamicToStatic Refactor* fix test* add regression tests* cleanup* skip PrepareInput if the arg is already a constant* fix an issue with type inference with global functions	1
[C Codegen] Remove global packed variables when interface_api="packed" and target="c" (#10645)* fix pack global variables* address comments	1
[Hexagon] Export `ir_lower_vtcm_pass` function in the init file (#10330)	2
[SCHEDULE] Improve bound inference, support reduce codegen. (#30)	1
Add Hexagon VTCM and discontiguous allocation support (#9525)* WIP Allocation abstraction for VTCM and DDR.* Add Hexagon VTCM and discontiguous allocation support* differentiate between dimensions and allocations* remove change to llvm codegen* add integration test_add_vtcm to demo vtcm alloc* remove cmake change* forcing contiguous allocation in device API, for nowCo-authored-by: Chris Sullivan <csullivan@octoml.ai>	5
[docs] Update publication list (#11137)* [docs] Update publication listThis PR updates some publications that use or built on top of TVM.* Fix CI	0
[CMSIS-NN] code generator for softmax (#8833)	5
[TEST] Fix flaky test nll (#8344)* [TEST] Fix flaky test nll* Update the tol	5
implement conv3d op (#4400)* implement conv3d op* add back missed conv2d_output_shape by mistake* fix typo and docs, add topi test* rebase to master and merge 2d/3d unification* use cudnn.conv_forward	1
[Runtime][Contrib][Verilator] remove explicit destructor call (#7485)	4
[REFACTOR][PY][API-CHANGE] Establish tvm.targetMove the related target modules into tvm.target.API change:- tvm.target.current_target -> tvm.target.Target.current- tvm.datatype -> tvm.target.datatype	5
[Pytest] Sort unit tests before running. (#9188)* [Pytest] Sort unit tests before running.By default, pytest will sort tests to maximize the re-use of fixtures.However, this assumes that all fixtures have an equal cost togenerate, and no caches outside of those managed by pytest.  A fixturefor a `tvm.testing.parameter` is effectively free, while a fixturemaintaining a cache of reference data`tvm.testing.utils._fixture_cache` be quite large.Since most of the TVM fixtures are specific to a python function, sortthe test ordering by python function, so thattvm.testing.utils._fixture_cache can be cleared sooner rather thanlater.* Updated TestTargetAutoParametrizationWhen sorting the tests, the order of parametrizations may change.Therefore, the tests checking for automatic target parametrizationshouldn't depend on order.	2
[TOPI] flip (#1161)	5
[Test] enable NHWC of `relay.testing.mobilenet` (#3886)* [Relay] enable NHWC of `relay.testing.mobilenet`In this way, we can play around NHWC inside TVM regardless ofthe frontends.* [Test] test for NHWC of relay.testing.mobilenet	3
[Refactor] Rename TVMContext to Device (#7721)	4
[microTVM] Add config space to dense_dsp schedule (#12444)* add config space* lint* lint	5
Added pool autopadding and simplified parsers. (#4672)	1
[TEAM] jroesch -> Reviewer (#1746)	5
[microNPU] Add support for requantize (#9910)* [microNPU] Add support for requantizeAdds support for stand-alone requantize operation which is legalized toan identity operation on the NPU.Change-Id: Ie2450c5fc72f405eddf517593236074aa4716c3b* fix concatenate tests failing due to not being bit exactSince requantize is now offloaded, concatenate tests were failingdue a reference not being used.Change-Id: I44b26b5daecfefb776ca19e6646f3690f5570f52* test multiple requantize offloadChange-Id: I60a3283461a7a7083c05289e84f570698388077b* address commentsChange-Id: I7196a0fa468eb7c6a96f2b8a68f3a2dcf5a5693c	4
[LANG] Comparison operators support for Imm expressions (#3283)	1
rm unused variable (#143)	1
Build system and dynamic library fixes (#283)* Install rules and dynamic library loading fixes.A batch of fixes:- Added 'install' rule to cmake and make, which installs runtime  headers and library (libtvm_runtime).- Added 'installdev' rule to make, which also installs the compiler  infrastructure headers and library (libtvm)- Added 'INSTALL_DEV' option to cmake, for toggling installation  of compiler infrastructure headers and library- cmake no longer builds into lib/ directory; instead all build  products go in your build directory- New algorithm for dynamic library loading, as described in #281.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Emit dylib on OS XSigned-off-by: Edward Z. Yang <ezyang@fb.com>* Lint fixes.Signed-off-by: Edward Z. Yang <ezyang@fb.com>	0
[Pass] Profiling TVM compiler passes (#7500)* basic pass profiler prototype* allow enable/disable of pass profiling* lint* add example pass profiler usage as test* render pass profiles to String instead of stdout	2
[TIR][REFACTOR] Add tir prefix to type keys (#5802)	0
Handle case where ListConstruct makes a python list which is output of whole model (#7088)	1
Documentation error fixed (#1337)Updated documentation error	0
Add flake8 to docker/lint.sh (#9055)	2
[MetaSchedule] Add Profiler Support For Tuning Efficiency Optimization (#11486)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[Runtime][PipelineExecutor] Added Interface to Track Number of Global Inputs (#11315)* [Runtime][PipleineExecutor] Added Interface to Track Number of Global InputsAdded a feature to PipelineExecutor to track number of Global Inputs.* Fixed CI Error* Fixed remaining CI Error	0
Revert "Conditions updated to cover better user scenarios (#4951)" (#5032)This reverts commit fe74b37ab578e6d3c540b0f6ac187a220ccc028a.	4
add device name to context attribute (#1090)* add device name to context attribute* update for other backends	5
[docs] Add links to v0.8.0 docs (#11647)This uses the new code from https://github.com/tlc-pack/tlcpack-sphinx-addon/pull/5 with a link to the v0.8.0 docs. We can update this in the future as we add more releases.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[ETHOSN] Adding support for Leaky ReLU (#11261)* [ETHOSN] Adding support for Leaky ReLUChange-Id: Icad69b2ae6ed4b3f3949cf5673efe2571aa66f5f* add some missing error reportingChange-Id: I935054c4d19a939e122092fab3c6c77204d9ead8	4
Fix for tvm.build()'s name warning (#9678)* change default name of tvm.build() to None* change lower name default to None* fix lint* lower() use "main" -> "default_function"* remove default name's warning	2
update docs (#8736)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
avoiding cast None to int errors (#3578)	0
[RELAY] Modify some passes to not stack overflow on many lets. (#7558)* [RELAY] Modify some passes to not stack overflow on many lets.Passes modified:- inline primitives- dead code- lambda lift* one fix* small fix* .at -> []* fix	0
[Relay][Dataflow] Fix test_rewrite_function_with_fuzzy_body test check (#8287)	3
[Arith] Fix iter_affine_map with non-const extent (#7437)	0
[Convolution] Error while importing onnx model with weights. (#345)* [Convolution] Error while imported onnx model has weights.Fix the use_bias based on the input parameters to use bias (Ex: sqeezenet). Ref #336* Review corrections.	1
Remove javah support (#10104)* Remove javah support* Remove unused compiler option* Osx pom.xml update	5
Packing and data layout change added to conv2d_nchw (#479)* conv2d layout change and packing added for the last workload* packing added for other workloads* conv2d added packing for first workload* fix pylint error	0
[REFACTOR][TIR] Introduce ExprDeepEqual, Remove IRDeepCompare (#5206)* [REFACTOR][TIR] Introduce ExprDeepEqual, Remove IRDeepCompareThis PR introduces ExprDeepEqual which reuses the StructuralEqual infra.We migrated the usecases of ir_pass::Equal to ExprDeepEqual and StructuralEqual.* Address comments	1
[TOPI, x86] Properly handle fused ops in TE softmax schedule   (#12015)* fix x86 softmax fusion* properly handle the case where softmax and fuseed op having different layout* add test	3
[TIR][REFACTOR] Cleanup unused classes (#5789)	1
Fix typo in get_output doc-string (#4237)	2
Pattern Language, Matcher, Rewriter, and Function Paritioner (#5231)	1
[CORE][Relay] Swap and remove compile_engine with te_compiler followup of #8775 (#9282)* Remove compile_engine.h for real* Fix format* RM compile_engine.cc* Swap compile engine with TECompiler* Cleanup on compile engine py leftovers* [WIP] Exposing legacy compile engine capabilities through TE Compiler* Swap usages for depreciated compile engine with TE compiler* Track and replace usages of compile engine refactor them to TE compiler* [Docs] Log helper mod* Remove depreciated function for lookup compile engine cachce* Fix typos* Debug misc cleanups* Register global pass for using te compiler for auto scheduler* Fix tests using the legacy compile engine* Fix broken autotuner tests and minor cleanups* Swap compile engine with te_compiler in rst config* PR nits* Fix failed testCo-authored-by: Jared Roesch <roeschinc@gmail.com>	3
Check function attr for alpha equal (#4479)	1
[FRONTEND][TEST] Remove duplicated frontend tests (#2414)* Remove duplicated frontend tests* Add test for nnvm to relay* Add test to script* Remove dcgan in nnvm_to_relay test due to unsupported op in cuda* Fix bug in converting conv2d from nnvm to relay* Fix dropout	4
[TEST] test_cuddn flaky (#4846)	3
[PASS] PostOrderVisit (#2169)	4
[AutoTVM] Fix database APIs (#3821)* [AutoTVM] Fix database APIs* Refactor the byte conversion	4
[Relay/TOPI][Frontend] Add tile and repeat operators in Relay and TOPI (#2720)* tile and repeat operator added in rely* fix pylint* fix make warnings* comments addressed* fix lint error* comment addressed	1
Fix structural error reporting on root block (#11477)	0
[µTVM] Use standalone_crt build tree for all µTVM builds (#7333)* Build microTVM using standalone_crt in build tree.* black format* pylint* try stashing entire standalone_crt in hopes it will not upset jenkins* Put standalone_crt in correct Jenkinsfile stash bundle* include build prefix* switch to python script for expanding globs* revert attempt to use globs in pack_libs, switch to building standalone_crt* properly revert pack_lib changes* fix typo* retrigger CI* revert pyproject.toml* update Jenkinsfile approach to use task_ci_setup.sh	1
[TOPI] Fix conv2d for small input channels (#331)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed* conv2d gpu scheduler for two layouts moved to tvm* changes made according to CR* conv2d_nchw formating updated, conv2d_hwcn tests updated* lint error fixed* element wise operator schedule fusing fixed for conv2d* conv2d_nchw topi test added, all resnet workloads now pass* conv compute lint error fixed* fixed python 3 compatibility problem* conv2d tensor input support added, test typo fixed, ir_pass.Simplify changed to util.get_const_int* fixed channel numer < 4 error, also made sure other splitting factor woudn't be 0	0
[ci] Disable flaky onnx tests (#11376)Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[DOCS] Revamp the docs (#34)	2
[Relay] Alter Op Layout (#2150)* [RELAY] Finish alter op pass* [RELAY] AlterOpLayout Pass* fix broadcast operators* fix broadcast operators* fix broadcast operators* Support concatenate* address comments* address comments* add comments* rebase	1
Halide -> HalideIR (#698)	5
[fix] vec * mat in matmul in onnx converter (#11174)* fix: vec * mat in matmul in onnx converter* fix: pylint* fix: vec-mat matmul* fix test* fix test	3
[ONNX] Add imports for Gelu, BiasGelu (#10898)As title. Adds imports for Gelu, BiasGelu from the com.microsoft onnx op domain.	2
[TEST] Add scipy to test dep (#231)	3
Better reflect allocator names in CRT tests (#8828)When the AOT executor was introduced, the Stack Allocator was associatedwith it by test name whereas the Page Allocator was left as justmemory_test.cc. This cleans that up a bit to clarify which tests whichallocator.	3
[PYTHON, TVM] Python TVM library, unit tests and end to end example* VTA python library* Python unit tests* End to end example with Resnet18* README instructions* Bug fixes	0
[RELAY][PY] Fix relay node registration after refactor (#5083)	4
[MetaSchedule] Misc improvement of the Measurer (#9757)Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[Relay] Adding _contrib_BilinearResize2D op from mxnet (#2777)* adding _contrib_BilinearResize2D op from mxnet* error fixed* use resize instead of upsample	1
[Relay] Finish implementations of WithFields (#11674)	5
fix opengl runtime to use OpenGL/gl3.h for macOS (#833)* fix opengl to OpenGL/gl3.h for APPLE* use glfw3 to include gl.h header	1
Bring back GraphRuntimeFactory loader for now. (#7868)* Address issue #7822.	0
[TUTORIAL] Optimize gemm on CPU add! (#270)* [TUTORIAL] Optimize gemm add!* temp commitment* [TUTORIAL] python3 compatiblity made; doc generation updated!* [DOCS] gen_modules clean and ignore add!* [TUTORIAL] title modified!* [TUTORIAL] some rolled-back modification re-write* [TUTORIAL] title underscore extended!* CONTRIBUTORS add me!	1
[MetaSchedule] Update Tuning Interfaces. (#10367)This PR is further improvement of the meta schedule project (https://github.com/apache/tvm/issues/8473).Co-authored-by: Junru Shao <<junrushao1994@gmail.com>>Co-authored-by: Bohan Hou <<32121147+spectrometerHBH@users.noreply.github.com>>Co-authored-by: Ruihang Lai <<lairuihangdongdong@qq.com>>Co-authored-by: Hongyi Jin <<3231950289@qq.com>>Co-authored-by: Wuwei Lin <<wuwei@apache.org>>Co-authored-by: Siyuan Feng <<Hzfengsy@sjtu.edu.cn>>	1
[PASS] PrecomputePrune, add testcase (#14)* [PASS] PrecomputePrune, add testcase* update comment	5
[RUNTIME] Update graph runtime to rely on smarter planner, add get_input (#990)	1
Add CUDA conv2d for NHWC layout (#4737)	1
[Fix Bug] fix the bug of pool_impl_nd when computing avgpool_nd whith ceil_mode and count_include_pad are True. (#9835)* Added the offset[i] for getting the correct  boundary* Added corresponding test case	3
[PASS] Support for partition loops with thread_axis (#81)* [PASS] Support for partition loops with thread_axis* Add check for AttrStmt.attr_key	1
[FRONTEND][COREML]More ops are added (#1619)	1
Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU (#9307)* Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPUChange-Id: If1e1134b56639021036862e8ea65a8e9d33dceb7* Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU- Moved tutorials/micro/cortex_m_ethosu.py to gallery/how_to/work_with_microtvm/micro_ethosu.pyChange-Id: Ib554df6649b7313d4414187d5334ec5b03f35f33* [micronpu] Update and cleanup tutorials.- Moved tutorials/micro/cortex_m_ethosu.py to gallery/how_to/work_with_microtvm/micro_ethosu.py- Replaced full linker script document with a link to the linker script on githubChange-Id: Ic77648a4fc3dd76161d689774d21a3347a577b90* [micronpu] Update and cleanup tutorial- Replace Makefile code with a link to Makefile on github- Replace header files with a link to header files on github- Update demo.c with changes introduced by Device API- Update tvmc command line argumentsChange-Id: If6a254b368550c0f3effb8d1cb15f062279964e2	4
Update CONTRIBUTORS.md	5
[BUGFIX] Fix topi matrix multiplication using tensorcore to run faster (#6749)	1
[CODEGEN][CONTRIB] CoreML codegen (#5634)* [CODEGEN][CONTRIB] CoreML codegen* import coremltools only when it is necessary* fix pylint errors* don't import contrib.coreml when using runtime lib* skip coreml codegen test in CI* don't register relay.ext.coremlcompiler in __init__.py* move tvm/contrib/coreml.py to tvm/contrib/target/coreml.py* use existing transformers for graph partitioning* skip test only when coremltools is not available* add check for annotation* move _register_coreml_op to python/tvm/relay/op/contrib/coreml.py* skip compile when xcode is unavailable* relay.op.Op -> tvm.ir.Op* set USE_COREML on* refine test	3
[Relay][Frontend][ONNX] Add support for broadcasting to Where and MatMul (#4267)	1
[BugFix][Relay] Fix type relation for batch_matmul (#8376)* fix type relation for batch_matmul* fix lint	0
[TOPI] VNNI support for batch matmul (#10332)* add test* compute added* schedule works* reuse dense_vnni schedule* try an alternative approach to scheduling layout transform* introduce a tunable knob to decide if compute_root* check transpose condition* support s8 + s8 input* pylint	1
[Bugfix] tvm.scan follow by tvm.compute segfault (#3723)* [bugfix] tvm.scan follow by tvm.compute segfault* more strict bound condition check* access k + 1 -> k* fix scan test	3
[ONNX]GatherNd, Round, IsNaN, IsInf (#5445)	5
[TOPI] Improve conv2d_transpose schedule on X86 and CUDA (#3948)* improve conv2d_transpose x86 performance by reusing conv2d schedule* parallelize across batches to make large-batch conv2d and conv2d_transpose faster* improve doc for autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log* add fallback schedule for schedule_conv2d_transpose_nchw_cuda* fix pylint* fix pylint* unify conv2d_transpose declaration in topi.nn and topi.x86	0
Support MKL on Windows (#3837)	1
Fix LayoutRewriter (#10118)* Fix layout pass* add unit test* fix lint* fix lint* fix lint	0
[NNVM][ONNX] Squeeze and Unsqueese operators. (#1339)	1
Fixed namespacing issues in schedules (#873)* Fixed namespacing issues in schedules* Fixed compile error	0
[DOCS] Mention Ninja build system in install/from_source.rst (#4554)* [DOCS] Mention Ninja build system in install/from_source.rst* Address comments	1
[REFACTOR] Use more TypedPackedFuncs (#2981)* Add `set_body_simple` to Registry, refactor a lot of code to use it* Add more types to Relay PackedFuncs* Add Registry::set_body_method to easily make Node methods intoPackedFuncs* Add set_body_method, set_body_node_method; start typing api_lang* Add some docs, remove unused script* Fix mysterious linter problem* Touch up api_ir.cc* Fix some issues with TOPI argument counts* Revert changes to topi.cc to avoid problems with optional arguments* A little more cleanup* Type more of the api _ functions* Whitespace* Finalize names and docs for new registry helpers* Update docs	2
[Ansor][AutoTVM v2.0] Phase 1: XGBoost Cost Model (#6270)* port xgb cost model* add xgboost cost model* fix lint* address comments* address comments* Fix	0
Update rustc (#2524)	5
[Object][FFI] Introduce runtime::String::CanConvertFrom (#5718)* [Object][FFI] Introduce runtime::String::CanConvertFrom* Update container.h	5
[TIR] cast disparate floating point types for binary ops (#8517)* handle upcasting case* test upcasting tests for tir* address comaniac comments* formatting* add negative tests* fix failing test now allow other thingsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
General Layout Support (#447)	1
[Vulkan] Add device capabilities to Target, use in codegen (#8127)* [Vulkan] Enable instance/device extensions- Vulkan requires that extensions be explicitly enabled if used.  Explicitly list out which extensions are required (currently none)  and which are optional.* [Vulkan] Extract device information from vulkan API.- Based on vkGetPhysicalDeviceProperties and  vkGetPhysicalDeviceFeatures, determine which Vulkan capabilities are  supported, pack into a Target.* [Vulkan] Query instance-supported apiVersion before creating instance- Previously, vkCreateInstance was called to initialize Vulkan 1.0.* [Vulkan] Moved options for dedicated allocation and push descriptors to environment variables- Query support for dedicated allocation and push descriptors along  with the rest of the device support.  Move the options to disable  their use from compile-time variables to environment variables  `TVM_VULKAN_DISABLE_PUSH_DESCRIPTOR` and  `TVM_VULKAN_DISABLE_DEDICATED_ALLOCATION`.* [Vulkan] Move option for vulkan validation layers to environment variable- Moved to enable faster use as a debug tool.  If  `TVM_VULKAN_ENABLE_VALIDATION_LAYERS` is a non-empty string,  validation layers will be enabled.* [Vulkan] Explicitly enable vulkan features in device creation- Vulkan requires that features be explicitly enabled before use.  For  each feature that the device supports and a shader might use,  declare it in the call to `vkCreateDevice`.* [Vulkan] Avoid repeated queries for device attributes.- Implement `VulkanDeviceAPI::GetAttr` based on the per-device values  stored in the Target.  This pulls all logic for querying device  parameters is in a single location.* [Vulkan] Implement "from_device" flag for the vulkan target.- With the number of device capabilities that may or may not be  supported by a vulkan driver, it can be tedious to input them.  Specifying "-from_device=0" now indicate that any unspecified values  should be read from the device.* [Vulkan][Codegen] Read vulkan device capabilities/limits from Target- Previously, the codegen assumed that all device features were  present.  Now, the codegen reads device capabilities from the  Target, and throws an error if codegen would require use of an  unsupported feature.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[Hexagon] AoT with LLVM Codegen on Hexagon (#11065)* AOT with LLVM Codegen on Hexagon* Address comments	1
[TVM][BUGFIX] Fix missing reduction init predicates (#2495)* [TVM][BUGFIX] Fix reductions in split axes* A test case for the problem* Fix the fix: skip loops that are related to reduction AND are unrelated to axis	0
Fix rpc testcase (#1538)	3
[tir] remove unused member variable (#11248)Remove unused member variable`tvm::tir::PackedCallLegalizer::tvm_value_index_`.This also fixes a GCC 7.5 compiler warning.	2
[Relay][ADT]Static Tensor Array (#5103)* Add other static tensor array ops* Add tensor array get data* Minor refactor* Fix pylint* Update docstring* Make get data more generic* Improve test* Improve split test* Improve get data* Minor fix* Further improvement for static shape* Improve shape parsing* Unify get_static_name	1
[Parser] Typo in mod creation (#6165)	1
fix two check typo in codegen (#11240)	2
Contrib: add mkl blas support (#1336)	1
correct conv2d workload for resnet18 (#750)	1
Add Python Classes for all Attrs (#5853)	1
[ARITH] Explicitly state truncdiv/mod in pattern matching. (#3986)* [ARITH] Explicitly state truncdiv/mod in pattern matching.* Fix the dependent cpp test	3
[CI][DOCKER] Update ci-lint to pylint2.4.4 (#4851)	5
Add -i option to fix ASF headers to lint scripts. (#10284)* Add -i option to fix ASF headers to lint scripts.* address driazati comments	1
[MetaSchedule, Testing] Generalize in/out dtype of testing te workloads (#12122)* [MetaSchedule, Testing] Generalize in/out dtype of testing te workloads* Fix tests	3
[QNN] Channel wise quantization - Quantize & Requantize (#4629)	5
[Frontend][Relay] Fix MXNet frontend to support NLP backbones in GluonNLP (#6699)* updateUpdate type_relations.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyupdateUpdate mxnet.pydebugUpdate generic.pyUpdate topi_integration.pyfix bugupdateUpdate test_forward.pyUpdate test_forward.pyfix test caseUpdate mxnet.pyupdateUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pydebugUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.py* address comments* Update mxnet.py* Update mxnet.py* fix* improve where test* Update test_forward.py* Update test_forward.py* Update test_forward.py* update* Update mxnet.py* Update mxnet.py* Update mxnet.pydebugUpdate common.pyupdateUpdate mxnet.pyupdateUpdate test_forward.pyUpdate test_forward.py* update* fix lint* Update mxnet.py* Update test_op_level1.py* fix lint	0
fix dump ir (#2235)	0
[Quantization] Make calibration faster and more memory usage friendly (#4589)* Use memory efficient calibrate* Fixed indexing* add cpp kl stub* ported KL cpp from mxnet* Fixed std::distance arguments order* remove python implementation* fix lint and indent* fix indent* refactoring* fix lint* fix for i386	0
[UnitTests] Automatic parametrization over targets, with explicit opt-out (#8010)* [UnitTests] Explicitly list tests that were enabled by TVM_TEST_TARGETS but were skippedPreviously, these were removed by a filter intvm.testing._get_targets(), and weren't listed at all.  With thischange, they are instead removed by pytest.skipif, and show up asexplicitly skipped tests in pytest's summary when usingtvm.testing.parametrize_targets.* [UnitTests] Automatic parametrize_targets for tests that use (target,dev)Should make it easier to convert tests from usingtvm.testing.enabled_targets to use pytest's parametrized testsinstead.* [UnitTests] Added ability to explicitly exclude a target from a particular testUses tvm_exclude_targets variable, which can be set (1) in theconftest.py to apply to a test directory, (2) in a test script toapply to that module, or (3) on an individual test function to applyto it.  The @tvm.testing.exclude_targets decorator is provided forreadability in case #3.* [UnitTests] Refactored test_topi_relu.py to use pytest.mark.parametrize* [UnitTests] Added tvm_known_failing_targets option for the unittests.Intended to mark tests that fail for a particular target, and areintended to be fixed in the future.  Typically, these would resulteither from implementing a new test, or from an in-progressimplementation of a new target.* [UnitTests] Known failing targets now marked with xfail instead of skipif* [UnitTests] Removed tvm_excluded_targets and tvm_known_failing_targetsThese were implemented to exclude or mark as failing an entire file ordirectory of tests.  Inhttps://discuss.tvm.apache.org/t/rfc-parametrized-unit-tests/9946/4,it was pointed out that the global variables would be vulnerable totypos in the names, resulting in the option being silently ignored.The decorators `@tvm.testing.exclude_targets` and`@tvm.testing.known_failing_targets` do not have this failure mode,and are the preferred version.* [UnitTests] Added helper functions to tvm.testing.- tvm.testing.parameter() defines a parameter that can be passed to  tests.  Tests that accept more than one parameter are run for all  combinations of parameter values.- tvm.testing.parameters() defines multiple sets of parameter values.  Tests that accept more than one parameter are run once for each set  of parameter values.- tvm.testing.fixture() is a decorator that defines setup code.  The  `cache=True` argument can be passed to avoid repeating expensive  setup across multiple tests.* [UnitTests] Bugfix for auto parametrizing of "target"Previously, if the @parametrize_targets were present, but had other@pytest.mark.parametrize after it, "target" would get parametrized asecond time.  Now, it checks more than just the closest "parametrize"marker.* [UnitTests] Renamed "cache" argument of tvm.testing.fixture to "cache_return_value"* [UnitTests] Minor updates to parametrized test implementation.As recommended by @tkonolige:- Avoid infinite loop if LLVM target isn't enabled- Update documentation for preferred use cases of  tvm.testing.parametrize_targets, and recommended alternatives.* [UnitTests] Minor updates to parametrized test implementation- Documentation, removed previous example usage of tvm.testing.parametrize_targets* [UnitTests] Changed accidental use of pytest fixtures to a NameError.- Previously, a fixture function defined in a module was accessible  through the global scope, and the function definition is accessible  if a test function uses that name but fails to declare the fixture  as a parameter.  Now, it will result in a NameError instead.* [UnitTests] More careful removal of fixture functions from module global scope.- Initial implementation only checked hasattr(obj, "_pytestfixturefunction")  before removing obj, which gave false positives for objects that implement  __getattr__, such as caffe.layers.  Now, also check that the value  contained is a FixtureFunctionMarker.* [UnitTests] Copy cached values when using tvm.testing.fixture(cache_return_value=True)To avoid unit tests being able to influence each other through ashared cache, all cached fixtures are passed through copy.deepcopyprior to use.* [UnitTests] Added meta-tests for tvm.testing functionalityCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
[DOCKER] CUDA upgrade to 9.0 to acommodate tensorflow-gpu (1.10.0). (#1761)	5
Update jenkins file first to use sort (#1147)	1
[AutoScheduler] Querying and sampling in task extraction (#7571)* [AutoScheduler] Query in task extraction* trigger ci	4
[DOCS][SDACCEL] Update AWS F1 deployment (#1447)	5
int32 pooling with int64 shapes (#6687)* Failing tests for Int32 avg_pooling with Int64 shapes* fix pooling implementations	0
[Keras] fix weight shape in dilated conv (#1715)	0
[COMPILER] Refactor compiler to enable configuration (#21)	5
[Relay][VirtualDevice] Expose WithFields to Python to do proper copy in ExprMutator (#11882)* [Relay][VirtualDevice] Expose WithFields to Python to do proper copy in ExprMutator* [Relay] give FunctionWithFields optional arguments* [lint] fix wrong line length* [lint] missing newline* [doc] add doc string to FunctionWithFields	1
Unified error handling in NNVM and Relay frontends (#2828)	0
[AutoTVM] fix argument type for curve feature (#3004)	0
[TVMC] Keep quantized weights when importing PyTorch model (#9417)BYOC requires `keep_quantized_weight` be set to true when convertingPyTorch models using `from_torch`. Setting this to be True when usingTVMC.Change-Id: I8c183f9f802ea54d24679a4017e56481d84e5655	4
[TensorIR][Tutorial] Blitz course (#9315)* blitz course* update* black* update* update* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* typo* change linksCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	2
[Bugfix][Printer] Avoid adding annotation twice for ConstantNode (#6364)* [Relay] Add user-defined constant node printer* fix constant node printer, which appends annotator twice when meta=true* fix lint	0
change docker install script (#3524)	2
[Relay][Frontend][Onnx] Small bug fix for Conv1D imports. (#5995)* Fix autopad bug in onnx importer for conv1d.* Fix output shape in test.* Undo commented out lines oops.	3
[CI] Update CPU image to v0.81 (#10305)	5
[VTA] Make more explicit error message during sim lib loading failures. (#7761)	0
[TIR] Expose tir.call_cpacked in python (#11563)	5
[Relay][ConvertLayout] Support for qnn.conv2d_transpose (#9139)	1
[DOC] Add missing targets to target_name documentation. (#3128)	2
[VISITOR] New ExprFunctor, StmtFunctor Interface. Modular analysis (#58)* [ARITH/VISITOR] Modular Analysis, ExprFunctor, StmtFunctor* retrigger* [IRFunctor] Migrated CodegenC* [IRFUNCTOR] Migrate CodeGenLLVM* [IRFunctor] Migrate canonical* [IRFunctor] Migrate vectorize* [IRFunctor] migrate CodeGenStackVM	1
[Relay][Runtime] Add VM compiler.  (#3139)* Implement the VM compiler* Fix issues* Fix ASF headers* Fix test issue* Apply typo fixes.* Update src/relay/backend/vm/compiler.ccCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Refactor compiler* Fix* Fix* Fix in benchmark* Fix* Address comments	1
[microNPU] Enable the codegen tests for 256 mac Arm(R) Ethos(TM)-U65 NPU (#9815)This patch has necessary changes to the Makefile and testinfra to run the FVP based codegen tests on a newer NPU variant.	1
[Relay][Dynamic] OneHot operation (#6209)* Dynamic OneHot Op* refactor dynamic_to_static* add onehot to dynamic_to_static pass	4
[Meta Scheduler] Add cleanup for localrunner (#9191)* add cleanup for localrunner* remove build results	4
add attr option mfloat-abi for arm32 (#6123)* add attr option mfloat-abi for arm32* retrigger	1
[TIR][REFACTIR] Update TIR nodes std::string->String. (#5793)This PR updates the remaining TIR node's member to useString instead of std::string.	1
[BugFix]: Convert tuple to int (#7880)* DEBUG: Convert tuple to int* Add test cases for test_conv2d_hwcn()* CI pass	4
[ci] Remove hardcoded test shards (#10743)This moves the sharding logic from being inlined in the Jenkinsfile to templated, so we can change just the number of shards and the test allocation in `conftest.py` and the Jenkinsfile will work to match. This also changes the test allocation from a manual balancing before to be random between shards. Each shard needs to know only its shard number and the total number of shards, then it hashes each test and skips it unless that hash falls within its allocated tests. This breaks up related tests across shards but has the downside that any change to the number of shards will shuffle around where the tests end up (but ideally this is rare as we settle on a good number of shards to use).This only does this for the GPU frontend tests but eventually we could expand it to more.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
Update docs/dev/virtual_machine.rstCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>	2
inline AMD GPU functions (#625)* Support vector operations for AMD (llvm IR)* fix whitespace* update comments, docstring* inline AMD GPU functions	1
Add MXnNet parser for box_decode (#5967)	1
[CUDA] dense_tensorcore/batch_matmul_tensorcore support int8/int4 (#8402)* add int8/int tensorcore for dense/batch_matmul* fix bug* fix lint* Apply suggestions from code reviewCo-authored-by: Chenfan <jcf94@outlook.com>* fix for reviewer* fix lintCo-authored-by: Chenfan <jcf94@outlook.com>	0
Use std::string_view, remove experimental or pre-14 variants, NFC (#12460)	4
[Hexagon] Add RPC Mechanism for Hexagon (#9631)* Add Hexagon RPC* removed android remote and updated Readme* Add check for workspace size* Make libtvm_runtime consistent for Android* Remove root access* Fix some docstrings* Make stack remote size as parameter* add documentation* Refactor test conftest* clang format* Decoupled USE_HEXAGON_RPC* fix creation of test base directory on android* Address global variable* Fix format and Cleanup cmake* Fix build for other targets	1
Check in inline and test	3
[ci] Default to n=2 for test parallelism (#12414)* Revert "[skip ci] Revert "[ci] Default to n=2 for test parallelism (#12376)" (#12413)"This reverts commit 478b672f2b7bb37f529fa6477b3c4ac353217b7a.* [ci] Default to n=2 for test parallelismThis is attempt #2 of #12376 which was reverted in #12413. The changesin `plugin.py` should keep all the tests on the same node so sporadicfailures don't happen due to scheduling.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[CI][DOCKER] Add ONNX runtime dep (#4314)* [DOCKER] Add ONNX runtime dep* Improve ci script	1
[FIX,VM] Fix get_outputs on the vm with a single output (#7902)* [FIX,VM] Fix get_outputs on the vm with a single outputThe VM uses an ADT for multiple outputs and an NDArray for a singleoutput. The single output case was not being handled.* check if the user specified the correct index	1
[TVMC] Split common tvmc file into more specific files (#9529)This follows from #9206 and splits common.py into multiple smaller and more focussed files.	2
[CUDA] FP16 support  (#1413)	1
[Relay][OP] Add fast_erf implementation (#5241)* add fast erf* doc* lint* fix* fix indent	0
Fixed point multiplication improvements for AArch64 (#5980)* Fixed point multiplication improvements for AArch64Change-Id: Ib3c10348d4c0eac11fa92b39cc6e792560e9eba4* Fix python linting errorsChange-Id: I4cf5ac18aa24b39374b83805dcc8e1663e173909* Fix doxygen errorsChange-Id: Ie3c861f8ead3f1ea5b30d5e9d7d94e222299d407* Fix arm_cpu injective testsChange-Id: I6ad9da61b61e6bd737627f26fba59767418c07cd* Fix python linting errors - 2Change-Id: Ic864a235aa5da5786393cbf6146dd815c121df5e* Fix arm_cpu injective tests - 2Change-Id: If9ca1cc3d947b1656c836c7f88de90470d92f979* Redesign: introduce a qmuls (q-multiply and shift) general intrinsicChange-Id: I1966fef9aee32eab50e4b984bbe81018488c8c02* Fix python linting errors - 3Change-Id: Ib87a19a8ee2d532954a7db1eb5793666e7aef366* Addressing review commentsChange-Id: Ie82e75204e5a421d17660f381f3e31fc325cd26c* Fixing test failuresChange-Id: I74cc675764cf8d260fe68a41e770b1ec7e84729a* Renaming qmuls to q_multiply_shiftChange-Id: I5a8ed60ba855208040304fcdf6e1ea28061f06ad	4
[BUGFIX] [Hybrid Script] fix in-correct value index in hybrid script (#2268)	0
make shape inference of BatchNorm layout neutral (#301)* make shape inference of BatchNorm layout neutral* refactor to use the axis variable to do BatchNorm shape inference* refactor to use the axis variable to do BatchNorm shape inference* add unittest to the axis param for batch norm shape inference	5
[USMP] Register hill climb algorithm (#10182)* USMP: Register hill_climb algo in algorithms map* USMP: Add missing space to assertion message* USMP: Integrate hill_climb algo in test_crt_aot_usmp	3
[BYOC] Retire the example json runtime (#6177)	1
GitHub actions/checkout@v1 --> v2 (#4680)https://github.com/actions/checkout/releases	5
[OP/LANG] Support Extern Call, more regression tests (#69)* [OP/LANG] Support Extern Call, more regression tests* [TEST] Include pylintrc	3
Remove PrimExpr from String (#5311)	4
[Relay][Fix] Fix alter op layout when calling a global var (#4454)* [Relay][Fix] Fix alter op layout when calling a global var* add test case	3
[CYTHON] Fix exception propagation for cython3 (#1046)	0
Removing header arm_math.h which has disappeared after CMSIS upgrade (#12217)Change-Id: I51a852321296aa6ac3e74eb44f37e8e693cb3f8f	4
Update device_annotation.cc (#5291)	5
[TFLite Runtime] Fix bug and re-enable RPC execution test (#5436)	3
[TOPI][Winograd] Optimization of Conv2d Winograd algorithm on Tensor Core (#5485)	5
[DOCKER] Pin pylint==1.9.4 (#2727)	2
Adding source types to C++ reduce functions (#1771)	1
[ETHOSN] Supply output tensor to issupported checks (#11944)Some operations were being offloaded when they are not supportedby the NPU, for example mean could get offloaded with differentquantization parameters for the input and output which is notsupported. Consequently, this meant that there would be a failureduring compilation or an output mismatch at runtime. Fixing this bysupplying the output information to the issupported checks thatdetermine whether an operation should be offloaded.Change-Id: I8896f83dad3d1c837fbb85bf2836fc9325f9dec9	4
[RUNTIME] Improved Packed FFI for optional. (#5478)Allows Optional<NDArray> and module to be passed with the right type code.	4
[FIX] tvm.testing.parametrize_targets documentation for arguments does not match what it is acutally using. (#7778)	1
[TIR] Minor refactor to tir.transform.StorageFlatten (#9260)Expressed each step as a separate `transform::Pass`, so they can beused/inspected individually.	1
[DOC] Fix doc rendering  (#3897)* Update from_source.rst* Update deploy_ssd_gluoncv.py	5
Check for toolchain when marking reference system tests (#10659)This mimics the behaviour of aot_test_utils.py to ensure the tests don't start running when the toolchain isn't available.	1
Fix ordering of tf and tflite installs in ci_cpu (#8312)The recently merged 8306 PR introduced a depedencyfor tflite installation that tf must be installed first.However, that PR did not correct the ordering in ci_cpu whichdoes not have that ordering.Change-Id: Ib82c2b33e4e123d4562682e9e97b21bfe23cc0ef	4
[TIR] Add 'global_symbol' and 'tir.noalias' as default attributes in script auto completion (#9744)* [TVMScript] Update default TIR prefix to T* [TIR] Add 'global_symbol' and 'tir.noalias' as default attributes in script auto completionCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
Use CMake for make clean (#1280)	4
[Frontend][PaddlePaddle] Add operators of interploate/flatten and modify try_infer_value (#9459)* add interploate and flatten* fix spells* fix diff* rename unit test name* add parameters for common:try_infer_value* eliminate unnecessary diff* fix pylint problem* fix pylint problem* eliminate unnecessary diff	0
[Refactor] Unify the shared pass prefix between vm and graph (#8526)	0
[TIR] Add tir::builtin::undef (#12266)* [UnitTest] RemoveStoreUndef, simplest behavior* [RemoveStoreUndef] First implementation* [UnitTest] RemoveStoreUndef, stores that depend through LetStmt* [UnitTest] RemoveStoreUndef, LetStmt handling, error on illegal usage* [RemoveStoreUndef] Added error checking for illegal T.undef() usage* Fix lint error* Use const ref for list of stores to remove* Verify that removed expression has no other side effects* Fix lint error	0
fix undefined reference to dlopen, etc (#2957)	0
[FRONTEND][PYTORCH] Support fo nn.SiLU added (#8753)	1
Tensorflow script upgrade from 1.13.1 to 2.0.0, so that it can run in both versionsw (#4963)	1
Fixed repo change for llvm-9 to resolve missing dependency issue when building images with llvm enabled (#3826)	0
[Cuda][Codegen] Check for cuda include dir in /usr/include. (#8135)Currently, on linux platforms, only checks for cuda install directoryin /usr/local/cuda/include.  The `nvidia-cuda-dev` package of Ubuntu20.04 installs at /usr/include, so it would be good to check thatlocation as well.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>	5
use shorter title (#58)	1
introduce pass lower_init_block (#7806)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>	5
[microTVM] Fix Stack Size Issue for Zephyr AOT Demo on Physical Hardware (#8453)* increase size* add --unpacked-api=1 option* format	1
Revert "support overlapped itersum (#12039)" (#12137)This reverts commit 3e7a2ad9568a79fb775c0ca9d09a3fa2f51f792f.	4
Fix error during loading library in python3 (#130)	0
[tvm4j] fix java build (#1471)	0
[microTVM][Zephyr] Update RVM to Zephyr 2.7 (#10138)* Update to zephyr2.7 and Refactor* Temporary for testing* Update cmake version* fix import path and format* Fix test script* address comments* fix path* fix image name	0
Made tensorflow IsNan actually work (#7320)* Made tensorflow IsNan actually workIsNan was added to tensorflow.rst in fa1b859f but this commit makes IsNan actually work* Added test case for tensorflow.is_nan	3
[Docker] Update onnx to 1.10.2, ORT to 1.9 (#9882)* [Docker] Update onnx to 1.10.2, ORT to 1.9* restore disabled testsThis reverts commit b29a4438e217ed84f7ee8c40254756c604d90fa3.	4
[QNN][TFLite] Parsing TFLite quantized models. (#3900)	5
[DOC] Add doc for Relay op strategy (#5078)* [DOC] Add doc for Relay op strategy* update* address more comments* update* update	5
[VirtualMachine] new method allowing to set one input tensor by its index or name (#10293)* set_input_with_index was implemented for VM* clean code* add getInputIndexFromName. add function descriptions. lint fix* fix lint* transfer comparison of parameter names number and assigned devices number to VMFunction constructor* add GetVMFunctionWithName to Executable API* clean code* add SetInputWithName (set_input_with_name) to VM API* join SetInputWithIndex and SetInputWithName to SetOneInputTensor (set_one_input) to VM API, the joined methods were removed* fix lint* some fixes after review* add set_one_input method to python API of VirtualMachine* pytests for set_input and set_one_input methods of VirtualMachine were implemented and checked* CI restart* construct simple model for pytests by relay instead of onnx tools (need for correct CI)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>	3
[PASS] Add storage alignment info to heap allocated data (#254)	5
[Relay, Topi] [Frontend][TFLite, MXNet] ReverseSequence operator (#5495)* TFLite reverse_sequence op* TFLite add_n implementation* reverse_sequence implementation* reverse_sequence implementation* reverse sequence* TOPI,Relay,TFLite - Reverse SequenceSigned-off-by: maheshambule <mahesh_ambule@persistent.com>* Reverse Sequence small fixesSigned-off-by: maheshambule <mahesh_ambule@persistent.com>* lint fixesSigned-off-by: maheshambule <mdambule07@gmail.com>* TFLite reverse_sequence opSigned-off-by: maheshambule* MXNet SequenceReverse implementation* clang format* clang format* review comment fixes	0
[BugFix][TVMScript] Use operator `is` when recognizing TIR Module (#10175)* [BugFix][TVMScript] Use operator `is` when recognizing TIR module* Test	3
[Frontend][TFLite] Add parser support for l2_normalization (#4966)* [Frontend][TFLite] Add parser support for l2_normalization* TF doesn't provide uint8 support* TFL does the normalization only if it's over the last axis* TFL uses only the default value for expilon* Change error message	0
[Relay][ONNX] fix #3134 converter where initializers were not registered as nodes (#3143)	5
[Graph Runtime] Run_individual for benchmarking individual layers (#2569)	1
[Sparse] add sparse tensor computation support (#1289)	1
[TOPI] Improve conv2d for resnet18 workload  (#427)* relu activation migrated to topi* reviews addressed* relu compute deleted* conv2d_nchw updated* resnet18 hand tuned schedule added* pylint error fixed* one more workload test for conv2d_nchw* conv2d schedule subfunctions added for different patterns* reviews addressed	1
enhance access_ptr that args can support Expr (#970)	1
[VTA] Fix vta rpc server, refactor launch cond to not depend on sys.argv (#8671)	5
[Adreno] Modify default AutoTVM params for conv2d (#12005)	2
[TIR][OP][API-CHANGE] Remove CallNode.call_type in favor of attribute. (#5937)This is a followup refactor for tir::Call.Now that we have switched call->name to call->op, the function effect propertycan be registered through the op itself, so we no longer need the call_type in the CallNode.- Introduce CallEffectKind to provide a more fine grained categorization of calls.- Introduce call_pure_extern and call_llvm_pure_intrin to  allow us to indicate pure calls in those cases.- Migrate existing usecases to the new API.	1
[Tutorial, QNN] Add tutorial for loading quantized PyTorch model (#5321)* add pytorch tutorial code and doc stub* add more docs* formatting, more docs* typo fix* try make sphinx happy* add performance section* type and nit fix* format fix	0
[ONNX] only broadcast matmul if the shape has changed (#10321)* [ONNX] only broadcast matmul if the shape has changed* fix copy-pasta mistake	0
[Frontend][Relay] Add Parser 2.0 (#5932)	1
[Relay] Fixed bug in attribute parsing for pool layers. (#5582)* Fixed pooling bug.* Added tests and fixed more cases.	0
[TOPI] conv2d avx (#883)* conv2d schedules for Intel CPU (AVX2 & AVX512)* fix lint* remove override register	4
[TENSORIR] Add `from_legacy_te_schdule` attr to TE PrimFuncs (#8641)* [TENSORIR] Add `from_legacy_te_schdule` attr to TE PrimFuncsThe `from_legacy_te_schedule` marks PrimFuncs created from TEscheduling. Passes that only operate on TE scheduling check this attrsand no op if it is not found. If `from_legacy_te_schedule` is false ornot set, then it is assumed that the PrimFunc is from TensorIR. Passesspecific to TensorIR now check for the absence of this attr.* formatting* enable passes regardless of te or not	4
Fixed temporary lock_guard instances. (#7199)	0
Migrate C Interface API Generation to C++ (#9106)Using the new name transformations added in #9088, the C interface API is now generated in C++ rather than in Python. This is intended to be a no-op for the actual users of this change and thus I've undone some of my overzealous sanitizing to match that expectation.Follow up PRs will clean up any remaining name transformation inconsistencies.Fixes #8792	0
[VTA][TSIM] add virtual memory support to tsim example (#3868)* [VTA][TSIM] add virtual memory support to tsim example* fix identation* remove USE_TSIM macro and use 32-bit addr instead	1
Adjust strategy plevel to achieve expected performance by default (#5118)	5
[TIR][TARGET] Refactor Target codegen to use IRModule and PrimFunc. (#5107)As part of the unified IR refactor.This PR refactors the target codegen to use IRModule containing tir::PrimFuncs.In order to break the refactor into several steps without breaking the codebase,we built an conversion pass to convert Array<LoweredFunc> into IRModule.The follow-up refactors will gradually move the passes covered by IRModule upuntil we cover all the passes. Then we can remove the additional redundantconcepts such as LoweredFunc.	1
[microTVM][Zephyr] Hot Fix Bad Merge (#8980)* fix bad merge* Fix test to reflect change of simd size for ARM	4
[FIX] Improve error messages and docs (#7064)- Better document tvm.relay.create_executor- Print what was provided in src/target/llvm/llvm_module.cc,  src/tir/transforms/arg_binder.cc,  src/tir/transforms/storage_rewrite.cc	1
[Relay] Convert Layout Pass. (#4335)	4
Graph executor: remove unnecessary unique_ptr, NFC (#8214)	4
Fix saveload json bug (#1831)	0
[PERF] Parallelize reduction for CPU (#4158)* [PERF] parallel reduction in cpu* fix* x* update* lint* fix	0
[RELAY] Non-recursive Graph Vistor and Rewriter (#4886)* First pass a defining a non-recursive Graph Vistor and Rewriterautoformatremove a currently empty test until testing is solidfied* Make CalcDep from Dead Code Elimination non-recursive* Partially working, not passing all tests yetpasses tests when disabling GetExprRefCount, I think I have a bug in visit countingfix GetExprRefCountFix a subtle bug with nested recursive/non-recursive scopes* Refactor* improve comments* respond to review comments on comments* Fix a problem with default recursion for dataflow nodesmark DataflowVisitor methods as override* implement ScopeMutator* convert forward_rewrite to ScopeMutator, remove DataflowMutator* rewrite ExprRewriter and convert fast_math to use it* switch BiasAddSimplifier to ExprRewriterfix a clang warningfix cpp lintfix doc param error* respond to review comments* fix a typo in the iterative looping* add a regression test for GetExprRefCount issue* Normalize naming* fix lint* First pass a defining a non-recursive Graph Vistor and Rewriterautoformatremove a currently empty test until testing is solidfied* Make CalcDep from Dead Code Elimination non-recursive* Partially working, not passing all tests yetpasses tests when disabling GetExprRefCount, I think I have a bug in visit countingfix GetExprRefCountFix a subtle bug with nested recursive/non-recursive scopes* Refactor* improve comments* respond to review comments on comments* Fix a problem with default recursion for dataflow nodesmark DataflowVisitor methods as override* implement ScopeMutator* convert forward_rewrite to ScopeMutator, remove DataflowMutator* rewrite ExprRewriter and convert fast_math to use it* switch BiasAddSimplifier to ExprRewriterfix a clang warningfix cpp lintfix doc param error* respond to review comments* fix a typo in the iterative looping* add a regression test for GetExprRefCount issue* Normalize naming* fix lint* respond to review comments	0
[ARITH] RewriteSimplifier: min/max, logical, select (#2768)	2
add tanh dispatch (#619)	1
[ci] Split hexagon into 2 steps (#11132)This shards up the hexagon build since after #11016 it's the longest test step. This also drops the max runtime per stage down to 2 hours from 4 hours to make these kind of increases more obvious in the future.Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[topi][relay] Add operation gather to relay. (#5716)	1
kill from tvm import te (#5007)Co-authored-by: Michal Jamroz <jamroz@chem.uw.edu.pl>	2
[MetaSchedule] Fix comments in instruction traits (#9614)	0
Enable bracket syntax sugar to get tensor element	1
[TIR] Restrict Buffer indices, only last index can be multi-lane (#10513)* [TIR] Restirct Buffer indices, only last index can be multi-lanePart of tracking issue https://github.com/apache/tvm/issues/10505,restrict multi-lane indexing to at most one index per bufferaccess. This removes ambiguity as an expression such as`A[T.ramp(i,1,2), T.ramp(j,1,2)]`, which could be interpreted eitheras `[A[i,j], A[i+1,j+1]]` or as `[A[i,j], A[i,j+1], A[i+1,j],A[i+1,j+1]]`, depending on whether the implied iterators of the tworamp nodes are shared.* Improved readability based on review suggestions.* Resolve lint error.	0
[microTVM] Zephyr: add B-U585I-IOT02A board support (#10416)	1
Set tvm.micro.project_api as a Python Module (#8963)* Add missing tvm.micro.project_api module file. The missing  __init__.py makes it impossible to import this module with  `import tvm.micro.project_api`.* This uncover 30-ish linting errors, which are also fixed here.	0
[POC][PatternLang]Remove constants from partitioned functions (#5663)* remove constants from partitioned functions* remove print statements	4
[tvm4j] provide error msg for failure function call (#2967)	1
[Profiler] Add significant VM instructions to profiling report (#9292)Added a hooks to the VM execution loop to record runtime of certaininstructions with significant runtimes. Now the profiling report willinclude data allocation and transfer times.	5
Fix android runtime error (#6575)codes for USE_RANDOM in tvm4j was missing.	1
Fix foldconstant involving dropout (#7550)Co-authored-by: masa <masa@pop-os.localdomain>	4
add two more device properties (#1124)	1
[CONTRIB] rocBLAS integration (#751)* rocblas integration* fix include* fix lint	0
[TIR] Expose Vector-related API in Python (#12571)This PR exposes the following TIR operation in python:- `vectorlow`: tested [here](https://github.com/apache/tvm/blob/592148abf6866a41eefa736efca067d42f5aea86/python/tvm/tir/tensor_intrin/arm_cpu.py#L62)- `vectorhigh`: tested [here](https://github.com/apache/tvm/blob/592148abf6866a41eefa736efca067d42f5aea86/python/tvm/tir/tensor_intrin/arm_cpu.py#L79)- `vectorcombine`: add new unittestCo-Authored-By: yongwww <yongcale@gmail.com>	3
[microNPU] Remove identity operations between non-compute operations (#10411)Builds upon the work in #10254 to remove identity operations sandwichedbetween two non-compute operations (reshape/strided slice - concatenateis handled differently), under certain conditions. Specifically, anidentity operation is not removed when the dimensionality between thetwo non-compute operations is reduced, due to non-congruent valuesbeing accessed incorrectly. For example,```strided_slice(dims=4) -> identity -> reshape(dims=4)```becomes...```strided_slice -> reshape```but,```strided_slice(dims=4) -> identity -> reshape(dims=2)```remains as...```strided_slice -> identity -> reshape```Change-Id: Ie28ba384fcb3230d6f4651c0c19e2b9526ebcc42	4
[microNPU] Setting a random seed for the codegen tests (#10640)Although a random seed is set while generating the input for the codegentests (`infra.py/generate_ref_data_tflite`), other sources of randomdata such as weight constants were not being deterministicallygenerated. This commit fixes that by setting a seed at the start of eachcodegen test.Change-Id: I25bb27a131cfc8aa318a8d808e78fb5ad628ad27	3
Disable pip cache when creating Docker images (#8575)* This is a good practice to save storage space in  the Docker images being created* Also sort pip package lists alphabetically	1
[RUNTIME] Minimum runtime module (#31)	1
Add more logging information to ReshapeLikeRel (#10125)* Update transform.cc* fix capitalize* fix lint* fix lint	0
[CI] Fix build.sh to propagate --network=host to the docker build command (#5336)* when passing --net=host to build.sh it needs to be also   sent as --network=host to "docker build", so that both   build and run will use the same network configuration	5
[RFC] [Contrib] Minimal runtime (~12kb .text on ARMv7/x86) for subset of TVM models (#3567)This is an alternative implementation of a subset of the TVM runtime API (andgraph runtime) that focuses entirely on reducing code size, at the expense offunctionality (no tvm.extern(..) calls via PackedFunc, CPU only, etc). It mightbe worth incrementally expanding the surface area if there's interest.The motivation for this work was seeing what the minimal useful subset of theTVM runtime is. This is relevant for e.g. super code-size constrainedapplications in e.g. embedded/mobile. The current runtime is more like O(100KiB)or so, so this might be compelling for some users.The smaller surface area for auditing might make this relevant forhttps://github.com/dmlc/tvm/issues/3159, or the usecases I was thinking about inhttps://github.com/dmlc/tvm/issues/2523#issuecomment-459165815 re: the Rustruntime.The symbols in the tvm::minimalruntime space (i.e. excluding std:: andpicojson::) are about 5KiB, so I think there's a bunch of room here (i.e. wecould replace picojson:: with [`jsmn`](https://zserge.com/jsmn.html) orsomething, and we could replace more of the `std::unordered_map` usage, etc withcustom primitives as well (similar to the `DynArray`).	5
register softmax (#16)	5
Run extract constants pass only for CMSIS-NN target (#9913)-Added checks for CMSIS-NN target in the pass extract_constant-Added a negative test with nested functions mapped to a combination of cmsis-nn and other compiler	1
[TOPI] Update parameter name of conv2d (#1380)	2
Bundled interpreter demo (#2297)	5
[PASS] Enhance gpu verify pass (#1660)	4
[TVMC] Switch profile flag to use new profiler (#8710)	2
Fix spelling in some comments (#7124)	0
[MetaSchedule][Test] Add unittests for DIL (#12077)	3
[Relay][Frontend] Add a bunch of ops in tf converter (#3270)	1
[PatternLang] Convert PatternGrouper to do pre-order, non-recursive analysis (#5653)* make the PatternGrouper iterate over the input Expr in a non-recursive pre-order fasion* add a comment	1
Add a 'rolling_buffer' scheduling primitive (#9444)* Add a 'rolling_buffer' scheduling primitiveCo-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>* Fix lint problemsChange-Id: I5e27a66105fccca84327e41b4c68836ac2515126* Remove designated initializersChange-Id: Ic148264239eac7df7d976a6a3e15236935232792Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>	5
[ci] Remove commit check on ci skipping logic (#10537)* [ci] Remove commit check on ci skipping logicThis makes it very hard to use an sometimes out of the submitter's control (e.g. when Jenkins decides to push a merge commit before running CI) for dubious benefit (the PR title is where people are looking after-the-fact anyways, so having it in the commit message doesn't make much sense). This removes the check for the commit message in order to make the process smoother.commit-id:dbd18808* Address commentscommit-id:ecd2be81Co-authored-by: driazati <driazati@users.noreply.github.com>	1
[RUNTIME] Add min_repeat_ms to time_evaluator (#2200)	1
[Collage] PartitionRule (though without CombinePartitionRule) (#11993)* [Collage] PartitionRule (though without CombinePartitionRule)See https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.(Special thanks to Matthew Barrett for authoring partition_rule_test.cc and suggesting a PRpartitioning strategy.)Collage uses a small 'combinator library' of PartitionRule to decribe how candidate partitionscan be extracted from a model for measurement and comparison. This introduces most of thatmachinery, however we defer the all important 'CombinerPartitionRule' for the next PR. Thusthe rules at this stage can only express the sorts of DFPattern-based rules we find in mostBYOC integrations, and cannot describe rules more traditionally associated with operator fusion.Based on #11981.* - Backport improvements to partiton_rule_test.cc* - Oops	3
[FIX,ROOFLINE] Only save tir functions for roofline (#12339)Only collect TIR PrimFuncs in roofline's SaveLoweredTIR. SaveLoweredTIRwas saving the full Relay main function leading which could beexcessively large. Also improve the logic to only save functions rightbefore MakePackedAPI.	1
[NNVM][KERAS] Add cropping support (#1636)	1
fix uint case (#11597)	0
[ARITH] Analyzer RewriteSimplifier: add/sub/mul/div/mod (#2722)	1
[TVMSCRIPT] Fix printing of rank 0 buffer access (#8215)* [TVMSCRIPT] Fix printing of rank 0 buffer accessAlso improve error messages and fix min/max/Select.* fixes* return fix* remove print	4
Add TupleGetItem to CSE (#5931)* Add TupleGetItem to CSE* rename a local variable	1
src/runtime/module.cc (#8496)	1
remove tabs (#3603)	4
[AutoTVM][Fix] Fix wrong axis names of data_vec (#12303)This PR is trying to fix the wrong axis names of data_vec. As the data_vec is nchwc format, the axis names should be batch, ic_chunk, ih, iw, ic_block, but not batch, ic_chunk, ih, ic_block, iw.Although the following code does not use these last two axises, so it does not cause some bugs for now. But I think we should fix this.	0
tightening bounding box for IntSet fused in PassUpDomain (#3073)Apply suggestions from code reviewCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>	4
fix docs of threefry_split and threefry_generate (#8035)	2
[UnitTest][Flaky] In test_report_serialization, compare csv. (#9275)* [UnitTest][Flaky] In test_report_serialization, compare csv.`str(report)` calls `ReportNode::AsTable()`, which includes aggregatevalues.  Otherwise negligible differences in the computed value can berounded differently after the round trip.  This was first [noticed inCI](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-9194/7/pipeline/#step-246-log-1217)for an unrelated PR.  Testing locally, this failure mode occurred 2times out of 3000 trials.Switching to `report.csv()` avoids this issue, as it does not includeaggregates.* Switched back to using AsTable(), but with column sums disabled.The .csv column headers are in arbitrary order, and do not testwhether the `device_metrics` field has been serialized/deserializedcorrectly.* Added explicit sorting of columns to Report::AsTable	1
[Hexagon] Add skip option for RPC server initialization (#12368)* add hardware parallelism in hexagon* fix name* lint issue* better description	1
Support creating Bool constants in the pattern_utils (#7507)	1
Add qemu build step to CI (#6644)	1
[MicroTVM] expose project options in autotuning (#12479)* expose project_options in autotuning* address comment* address commentCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>	1
[µTVM] Add --runtime=c, remove micro_dev target, enable LLVM backend (#6145)* need to fill address of globals in tvmfuncregistry* llvm func registry generator works!* lint fixes* rm hexdump include* bring bundle_deploy back to life and add to CI* revert gcda additions* git-clang-format* fix check for --system-lib and test_runtime_micro target* fixup compile flags for bundle_deploy CRT and improve robustness* git-clang-format* add debugging info* git-clang-format* initialize ret_values in PackedFunc_Call.* retrigger CI* fix log messages* git-clang-format* remove default for --runtime target opt* put backtrace behind a flag and enable it* simpify ReadString(), fixing bad instruction exception on os x.* git-clang-format* uncomment tests* reorder backtrace ldflags for linux gcc	3
Add params.* to Jenkins file parameters (#8771)* Prefix all parameters with params.* so that it checks   whether parameters exist before using them * This is a follow-up fix on #8721 so that existing PRs work   without being re-triggered manually twice	1
Add python installation script for Ubuntu 20.04 (#10841)	1
[AutoTVM] Support range in index based tuners (#4870)* Support range in index based tuners* Address comments* Remove __*state__* trigger CI	4
Add sort_by_time flag to debug_executor.run method (#12402)	0
[Relay] crossentropy_with_logits and its gradient (#4075)* save* lint	2
[relay][pass manager] Open transform namespace (#3226)	4
[Relay][Strategy] Allow cuda cross compilation without physical device. (#7063)* Allow cross compilation of cuda targets without physical device.* Formatting.* Add warning when architecture cant be found.* Use target instead of autotvm arch specification.* Change warning message.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>	2
Revert "Tighten split's extent (#4931)" (#5027)This reverts commit 585f9ce6e7bef7d0e8902b1c1e55dcb3bbe84eed.	4
fix docs (#9266)	2
[Error reporting] Replace runtime errors with LOG(FATAL) (#9311)* Replace runtime error with LOG(FATAL)* flaky test	3
gitignore build-* folders (#10168)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>	1
Update debugger.rst (#11231)Updating docs to reflect a way of using the debugger that works, see [TVM forum post](https://discuss.tvm.apache.org/t/runnig-a-model-with-tvm-debugger/9869/8?u=wheest)	0
[TVMC][ETHOSN] Improve target string to avoid duplication (#11272)* [TVMC][ETHOSN] Improve target string to avoid duplicationImproves the TVMC target string to avoid duplication of theNPU variant. The new target string will require the just the NPUname followed by -variant=n78. The old target string is deprecatedand will be removed in a subsequent version of TVM.Change-Id: I4638f36788df3f478435ac13d3531aad2b23f204* fix lintingChange-Id: I76a9da511899f24a163be669877605cd1a440022* fix make variant functions and update test error messageChange-Id: Iff553d4b255c0ce0b86bad42eaa94ee9b1c62508	4
Optimize the implmentation of swish operator (#10655)	1
Change Call with TIRCallAttrs to call_lowered op (#9312)* Introduce call_lowered opAdd op vm.call_tirChange from checking if CallNode has CallTIRAttrs to checking if the Op is vm.call_tirChange device_domains to use vm.call_tir op more explicitlyFixed issue in type checker, now have seg fault :(Fix typo -- most of VM tests pass nowInterpreter now deals with call_tir properlyFix typo in te_compilerUse InvokeTVMOp and CallTIRAdd some checks to graph_plan_memory.ccMake GetToken skip function typesC++ TESTS PASS WOOHOORemove printsformattingvm.call_tir -> call_tir and more comment removalscall_tir -> call_loweredfix lintclang formatRemove compute from non computational vm opsmissed some semicolons in prev commitFix warningMove call_lowered to relay/op/call/call.cc and rename util funcAdd helper fn that returns lowered_call opfix import orderclang formatAdd constraint to call_lowered type relclean up empty token vectorcommentMove CallTIRAttrs to include/tvm/relay/attrs/call.hRename TIRCallAttrs as CallLoweredAttrslintAdd helper for extracting func and args from call_loweredChange graph_executor_codegen to use helper functionUpdate interpreter to use helperFix device_domains.cc -- could still use cleanup, also I am not sure why there are still direct calls to primfns in DomainforCalleeClean up DeviceCopyProps and lintlintreturn CallLoweredAttrs with the extern funccommentnote in commentProgress & notes. Realized that I am not handling externs correctlynot sure why this ever worked before?Clean up CreateFuncCall signature, notescommentsFix extern function handlingextern_function -> extern_funcfix DeviceAwareVisitExpr_ -- now it handles both lowered and normal callsyay passes AOT tests!formatting and comment removalcleanupIntroduce call_lowered op* lint* Fix AOT to deal with externs* add const auto&* Fix aot crt test	3
[Hexagon][LLVM] Enable/test tensorized Hexagon DMA on 2d transformed layout (#10905)* [Hexagon][LLVM] Enable/test tensorized Hexagon DMA- In the `CodeGenLLVM::CreateIntrinsic` handler for  `builtin::address_of()`, pass N-d indices to  `CodeGenLLVM::CreateBufferPtr`.  The base class implementation still  asserts that there is a flat memory space, while the  `CodeGenHexagon::CreateBufferPtr` override allows 2-d memory.- Enable tensorization in `test_cache_read_write.py`, using  `tir.address_of` to pass the lowered value.Co-authored-by: Adam Straw <astraw@octoml.ai>* [TIR] Allow buffer_bind_scope of N-d buffersPreviously, any `buffer_bind_scope` attribute that provides a viewinto a non-flat buffer would result in an error.  After this commit,`buffer_bind_scope` may be used for non-flat buffers, but use of`arg_buffer->elem_offset` within the body of the bind statement isstill an error.The `BufferNode::elem_offset` field represents the offset between thepointer of the backing allocation and the first element of the buffer.This offset is only well-defined for flat memory spaces.* update test to tensorize cache_read `y` (works) and cache_write `z` (fails)* add `split` to allow for tensorization of cache_write of `z`* fix typo and cleanup comment* add back original 1d test_cache_read_write* update comments* format errorCo-authored-by: Adam Straw <astraw@octoml.ai>	5
[microtvm][Zephyr] Add project overlay to overwrite device tree configs (#12741)* add nucleo overlay	1
[Relay][AutoTVM] Relay op strategy (#4644)* relay op strategyfix lintbitpack strategybitserial_dense (#6)* update strategy* address commentsfix a few topi testDense strategy (#5)* dense* add biforst; remove comments* address commentRefactor x86 conv2d_NCHWc (#4)* Refactor x86 conv2d* Add x86 depthwise_conv2d_NCHWc* Add back topi x86 conv2d_nchw* Merge x86 conv2d_nchw and conv2d_NCHWc* Minor fix for x86 conv2dfix more strategyAdd x86 conv2d_NCHWc_int8 strategy (#8)* Add x86 conv2d_NCHWc_int8 strategy* Remove contrib_conv2d_nchwc_int8* Fix generic conv2d_NCHWc for int8* Fix topi arm_cpu conv2d_NCHWc_int8update x86 conv2denable specify relay ops to be tuned for autotvmadd cuda conv2d strategyadd conv2d strategy for rocmadd conv2d strategy for hlsadd conv2d strategy for arm cpuadd conv2d strategy for maliadd conv2d strategy for bifrostadd conv2d strategy for intel graphicsclean up and fix lintremove template keys from autotvmremove 2 in the func nameaddress commentsfix* fix bugs* lint* address comments* add name to op implement* Modify topi tests (#9)* Add pooling, reorg, softmax and vision* Add lrn* fix topi test* fix more topi test* lint* address comments* x* fix more tests & bugs* Modify more tests (#10)* Modify tests for bitserial_conv2d, bitserial_dense, bitserial_conv2d_rasp and bnn* Minor fix* More minor fix* fix more test* try to update vta using strategy* fix cpptest* x* fix rebase err* Fix two tests (#11)* change autotvm log format* lint* minor fix* try fix vta test* fix rebase err* tweak* tmp hack for vta pass* fix tutorial* fix* fix more tutorials* fix vta tutorial* minor* address comments* fix* address comments* fix cpptest* fix docs* change data structure name and api* address comments* lint* fix rebase err* updates* fix winograd test* fix doc* rebase* upgrade tophub version number* fix bug* re-enable vta tsim test after tophub is upgraded* fix vta test to use the correct args so the config can be found in tophubCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>	5
[RUNTIME] better parallel launcher and task distribution (#1026)	1
[TIR][UX] allow override when register TensorIntrin (#12439)* allow override when register TensorIntrin* lint	1
[TensorIR][M2a] Fuse, Split (#8467)* Fuse&split (#408)Co-authored-by: jinhongyi <323195289@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>	1
[Runtime] Android argsort support (#3472)* Add contrib sort functions to android rpc app.* replaced tab with spaces oops.	1
[ci][docker] fix the path of custom toolchain in ci_qemu for csinn2 (#11905)	0
[TE] Minor bugfix in message_passing.cc (#5254)	4
[MetaSchedule][Minor] Organize Testing Scripts (#11751)	3
[Relay][RFC][Fix] Rename RelayPrint to AsText (#2984)	0
[Relay] [TOPI] `{relay,topi}.nn.sparse_transpose` for **Square** CSR matrices (#3707)* add build gcn tutorial* add transpose operator for square sparse matrices* remove extra files* change loop tag* comply with lint* comply with lint -- line too long* comply with lint* lint check* lint check* lint check* apply marisa and theirry's reviews	4
[Runtime] Extend Graph Runtime To Support Cuda Graph Launch (#7616)* add graph runtime cuGraph poc* lint format* add unittest* fix review comments* Update CMakeLists.txtCo-authored-by: Cody Yu <comaniac0422@gmail.com>* build cuda graph runtime in gpu test* Revert "build cuda graph runtime in gpu test"This reverts commit f286711e4126c696860be3ec3d82400ca8542bd5.* rename cuGraph to CUDA Graph* rename cuda_graph* rename cuda_graph* lint format* Update src/runtime/graph/graph_runtime_factory.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/testing.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix lint error* remove unnecessary warn* add test, fix lint* fix lint W0223Co-authored-by: Cody Yu <comaniac0422@gmail.com>	0
[hexagon] 'add_hvx' test to explore HVX usage. (#10604)Add a unit test named 'add_hvx' to explore how variousscheduling choices, tensor sizes, etc. impact efficient usage of HexagonHVX units.	1
Fix TF resize for dynamic size models (#4510)	0
Update dmlc-core to latest (#5401)	3
[Fix] Refactor the roundtrip test. (#10592)This is a tiny fix on the roundtrip test, the case test I introduced in #10370 doesn't use `tvm.testing.parameter`.	3
More comment change on bench script (#197)	4
[ci] Delay pytest errors until all invocations have run (#10521)* [ci] Delay pytest errors until all invocations have runThis makes it a little easier to gather CI signal on a PR by ensuring that all pytest invocations run. Currently pytest runs through to completion for a single invocation so some failures are gathered, but not all. This is annoying for development since its hard to guage how a PR actually fared in CI without seeing the full picture. This will increase demands on CI since failures won't cause the skip the following pytests, but we can monitor CI to see if this has a big impact on queue times.This also also kind of a stop-gap since this wouldn't be an issue if we used a single pytest invocation, but that is difficult since we rely on loading `tvm` multiple times over the course of the test suite.* Don't use a file to stash info between runs* Fix exit code handlingCo-authored-by: driazati <driazati@users.noreply.github.com>	1
[FIX] Make HashCombine stable across platforms (#7801)* [FIX] Make HashCombine stable across platformsPR #7605 inadvertatly broke cross platform hashing when when it switchedsize_t to uint64_t. This cause a different specialization of HashCombineto be used. Unfortunately the new specialization used std::hash which isimplementation dependent. I've added tests to make sure this doesn'thappen again.* fix template specialization issues	0
[TIR][Schedule] Add Annotate/Unannotate primitive (#9742)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>	5
[FFI] Specifically check handle for recursion during shutdown (#8548)NOTE: previously slot may get overriden by child class and itis better to directly check for handle here.	0
[SCHEDULE] Remap the cached bind_scope. (#272)* [SCHEDULE] Remap the cached bind_scope.* more fix	0
Add type solver unit tests for unifying quantified funcs (one bug found) (#3947)	0
[DOCS] add benchmark log format doc (#4366)* add benchmark log format doc* code review changes* remove runtime_config, add md5 field* schema edits	2
[RELAY] Fix function call parsing for binary op (#2424)	1
fix Android build w/ threading_backend (#1059)	0
Implemented kDLCPUPinned (cudaMallocHost) (#4985)* implement kDLCPUPinned* Fix line endings* Fix whitespace for linter* cleanup up allocdataspace method	5
[AutoTVM] Minor bug fixes in AutoTVM for QNN graphs (#4797)* [AutoTVM] Minor bug fixes in AutoTVM for QNN graphs.* Bring back strided_slice.* Replace tvm.nd change.	4
[TIR][REFACTOR] Migrate low-level passes in tvm.lower to the Unified IR pass manager. (#5364)- Migrate BoundCheckers and Simplify- Migrate RewriteUnsafeSelect and RemoveNoOp- Migrate UnrollLoop and StorageRewrite- Migrate InjectDoubleBuffer and InjectVirtualThread- Migrate LoopPartition and Vectorize- Migrate CoProcSync, LiftAttrScope, InjectCopyIntrinWe still keep ir_pass registerations for now.Need a separate PR to refactor the parts before the StorageFlatten.	4
[QNN] Add nn.adaptive_avg_pool1d to FQ2I (#10541)* add adaptive avg pool 1d* clean up* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>	5
[Refactor] Reduced repetition in CodeGenLLVM's buffer access (#10567)* [Refactor] Reduced repetition in CodeGenLLVM's buffer accessPreviously, the majority of the BufferLoad and BufferStore visitorswere duplicate logic to handle the indexing.  After this commit, theshared logic is extracted out into a helper function.* Fixup, remove declaration of unused variable.* Bump to CI	1
Rev ci-qemu to 0.07 (#8698)	5
[Build][Windows] Fix Windows build by including cctype (#4319)* Fix build* dummy change to retrigger CI* dummy change to retrigger ci* dummy change to retrigger ci	4
add rocm codegen unittest for cross thread reduction (#4423)	3
[TIR] Add support for 0-dim buffer (#9224)	1
[REFACTOR][RUNTIME] Add LibraryModule that merges systemlib and dso. (#4481)Historically we have two variations of modules(DSOModule and SystemLibModule)that both exposes module via symbols.This PR creates a common implementation for both, and introduce a Librarybase class that allows us to have different implementations of GetSymbol.It paves ways for future library related module enhancements.	1
[BugFix][TIR] Fix narrower dtype of loop vars in CreatePrimFunc (#11030)This PR fixes a bug uncovered in end-to-end tests, where the dtype of loop variable could has fewer bits than the loop min/extent, which leads to a fatal error introduced by the recent #10595 which enforces more restrictive checks.	1
[SGX] Add ignored files to sgx example (#1852)	2
[TVMScript] fixing block attr printing bug (#9667)	0
fix install script (#4350)	0
Ignore invalid git tags when running "git describe" in version.py. (#8009)* When using version.py, the presence of tags not conforming   with vMAJOR.MINOR.REV can potentally cause version.py to   fallback to the default release tag (currently "0.8.dev0") * This change makes version.py ignore tags that do not conform   with vMAJOR.MINOR.REV by using "git describe --match ...".	1
Fix TVM compile without LLVM (#7621)* Fix TVM compile without LLVM* Fix formatting	0
[Onnx] Pow support for other types (#8933)* update pow* update pow* remove duplicateCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>	5
Enhanced simplification rules for Div by a positive constant (#2346)* Enhanced simplification rules for Div by a positive constant* Fixed my last commit to correctly interpret TVM's division as truncated division* Fixed implemenation of IntSet::can_prove_non_positive()* addressed comments by @yzhliu* addressed comments by @sgrechanik-h* addressed more comments by @yzhliu	1
[ETHOSN] Removed support for 20.08 version of the driver stack. (#7858)- Replaced capabilities header file with api calls introduced by the 20.11 ethosn driver stack release.  - Removed 20.08 driver stack support and updated all affected code.	5
add PATH to dll_path on windows (#479)	1
[Community] @AndrewZhaoLuo -> Reviewer (#9020)	3
[Relay, Topi][OP] Correlation (#5628)* [Relay,Topi] Correlation* fix* move* typo* Update test_topi_correlation.py	3
[DOCS] Reduce artifcats generated by sphinx gallery (#5208)	2
Restore License (#8779)	5
add missing nullptr check (#4773)	1
[PASS] InstrumentBoundCheckers pass (#2079)The pass which instruments checkers beforememory accesses (load/store).This allows to handle invalid memory accesses.The patch is related to issue:https://discuss.tvm.ai/t/array-bounds-checking/944	0
[RELAY,TOPI] Add scatter_nd op (#6854)* [RELAY,TOPI] Add scatter_nd opScatter_nd is the inverse of gather_nd and also happens to be itsgradient. The implementation here is not optimized. There are no cpu orgpu specific implementations.* formatting* Fix tests* formatting* specify types on test* Fix grad test* scatter_nd cuda impl* cuda impl* x86 impl* formatting* fix shape rel* fix tests* formatting	3
