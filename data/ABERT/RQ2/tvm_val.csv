commit_msg,labels
[TOPI] Improve dilate (#330),1
Update CI to use tlcpack/ci-cpu:v0.80 (#9865),1
INT8 conv operator implementation with NCHWc data layout for Intel machines (#1680)* Int8 implementation for convolution operator on Intel Skylake* Int8 implementation for convolution operator on Intel Skylake* PR changes* PR changes* PR changes* Fixing an error* Fixing an error* Minor typos fix* Minor typos fix* Removing the broadcast16 CPP code. Using astype feature instead* Replacing constant by variable name num_elements_intel* Name fixes and tensorize update rule updated* Fixing the bug about checking skylake* Replacing bitcast with reinterpret* Isolating INT8 and FP32 schedules to ease out future AutoTVM PR merge* Putting check_skylake function in the x86 directory* Added documentation and organizing files to better locations* Tensor intrin renaming. Avoid code duplication for intrin by kernel reshaping,1
"[Relay][Dyn] Add dynamic reshape grad (#6080)* add dynamic rehape grad* fix lint* fix unit tests, warning",2
"Use CTest for C++ tests (#8809)By using the `gtest_discover_tests` CMake macro the CPP and CRT tests can be configured to build binaries with a single test runner each. Once CTest has information about tests it can be used in IDE extensions such as [CMake Test Explorer](https://marketplace.visualstudio.com/items?itemName=fredericbonnet.cmake-test-adapter).`ctest` can also run tests in parallel using the `-j` flag, which could be interesting in future.",1
[Docker] Relax name check (#10011)Fix a issue that user name like aaa.bb can't be added to docker container,2
Fix incorrect doc in conv2d_nhwc_python (#1677),2
[TFLITE]TOP_K op parser support (#5051)* [TFLITE]TOP_K op parser support* Testcase updated,5
"[TIR] tir.transform.StorageFlatten refactor (#9091)* [TE] Improved flexibility of ArgBinder::BindDLTensorAllowed a compact DLTensor to bind to a Buffer object that definesstrides, if the strides defined correspond to a compact layout.* [TIR] Exposed ElemOffset as a member function of BufferNode.* [TE] Pulled shape determination out of StorageFlattenerPreviously, StorageFlattener would determine the shape of a physicalbuffer based on the extents of the BufferRealizeNode.  Pulled theseout into a separate BufferShapeLegalize pass.  After this pass, allbuffers have a shape that matches the buffer realization extents.* [TE] Refactor stride calculation out of StorageFlattenerPreviously, StorageFlattener would handle any attr::dim_alignannotations.  Now, this is pulled out into a separateBufferStrideLegalize pass.* [TE] Refactor thread scope propagation out of StorageFlattener.Previously, StorageFlattener would use the scope in IterVar to assigna scope to allocated buffers, where not otherwise defined.  This hasbeen pulled out into a separate ThreadScopePropagate pass.* [TE] Refactor buffer bind mapping out of StorageFlattener.Previously, StorageFlattener would look for `attr::buffer_bind_scope`to determine if a Buffer object is a view into another buffer, andwould apply that mapping while making the Allocate/Store/Load nodes.Now, the mapping of buffer binds is pulled out into a separateBufferStrideUnwrapper pass.This also resolves an issue in which BufferLoad/BufferStore nodes thatrefer to a Buffer defined through `attr::buffer_bind_scope` wouldgenerate Load/Store nodes that point to the linked buffer, rather thanthe actual buffer.* [TIR] Removed checks on buffer->shape.size()Even after BufferShapeLegalize, rank-zero tensors may have an emptyshape.* [TIR] Relaxed check on a bufferview's striding.Original refactoring requiring that a bufferview have no explicitstriding, and instead take the striding from the buffer that it isviewing.  Modified to allow bufferview to specify striding, so long asit is consistent with the viewed buffer's striding.  This reproducesthe behavior of StorageFlatten before the refactoring.* [TIR] Fixed StorageFlatten test for shape_legalize.AttrStmtNodes that contain rewritten Buffers need to be rewritten aswell.* [TIR] Assigned storage scopeThe earlier stage of the refactor left a buffer's storage scopeundefined if it's scope was not determined by the IterVar of a loopcontaining its allocation.  Now, these are explicitly set toStorageScope::kGlobal, to match the previous behavior ofStorageFlatten.* Updated ICHECK_EQ to CHECK_EQ for a test that depends on user-provideddata.* Added comments in storage_flatten.cc, indicating why buffer_bind_scopeneeds special handling.* Updated comment with a few examples of where compact buffers areassumed to have no strides defined.* Updated following @csullivan's comments.* Added fuzzy mapping to the BufferShapeLegalize.Maintains earlier behavior of StorageFlatten, which allows bufferviews to be mapped to higher dimension buffers, if the view extent is1 in each extra dimension.* Updated BufferShapeLegalize, asserts need to be inside the buffer_bind_scope.* Pulled all shape-dependent behavior into BufferShapeLegalize.Previously, BufferBindUnwrapper passed fuzzy_match=true toArgBinder::BindBuffer, which could change the number of dimensions.Now, all buffer dimensions should be updated prior toBufferBindUnwrapper, and it is an error to have mismatched dimensionsin BufferBindUnwrapper.* Added another pass to remove verifiable assert statements.ArgBinder::BindBuffer inserts these assert statements if they are notverifiable at the time of substitution.  Previously, with one giantsubstitution, the assertions were verifiable at that time.  After therefactor, with substitutions done in multiple stages forshape/stride/buffer_bind_scope, we need to clean up any assertionsthat are verifiable after all substitutions have occurred.* Minor cleanup- Removed StorageFlattener::BufferEntry::RelIndex, behavior already  handled by BufferShapeLegalize.- Improved comments and error messages.- Extracted duplicate behavior in BufferLoad/BufferStore handling in  BufferShapeLegalize.* Updated to handle BufferRealizeNode with no defined bounds.* Updated to be less aggressive when checking AssertStmtA true Assert statement can be removed, but a false Assert statementrequires CFA to give as a compile-time error.  Since we only need theremoval of true assert statements, skipping the CFA this time.",3
[Ansor][AutoTVM v2.0] Phase 1: Access Analyzer (#6103)* add access analyzer* add test cases* move header files and polish comments* fix lint* update* fix lint* address comments* fix lint,0
relay::StructuralHash to tvm::StructuralHash (#5166),5
Add safe destructor,1
"[Topi][UnitTests] Parameterize conv2d and depthwise_conv2d tests (#8433)* [UnitTests][Topi] Updated test_topi_conv2d_nchw.py to have parametrized tests.- Better error messages, displays which workloads/targets failed and why.- Fixed bug in topi.nn.conv2d._get_workload exposed by the  parametrized tests.  Incorrect padding if the ""SAME"" parameter is  used with dilation>1.- Fixed bug in tvm.topi.x86.group_conv2d._get_default_config, missing  dilation parameter in call to _get_conv2d_workload.* [UnitTests][Topi] Parametrized the tests in test_topi_depthwise_conv2d.pyIn preparation for parametrizing to test on float16 as well.- Single test_conv2d test with parameters for layout/input sizes.- Extended the support for NCHWc layouts, so that they could be  included in the parametrization.  (Implemented  topi.testing.depthwise_conv2d_python_nchwc and  topi.nn.scale_shift_nchwc, added layout argument to  topi.nn.depthwise_conv2d._get_workload).Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",5
Add thrust support for nms (#5116)* add argsort_nms_thrust* consider valid count in thrust nms sort* make thrust optional* typo* typo* fix pylint* address some of the comments* address more comments* fix lint* address more comments* address more comments,1
[hexagon][tests] re-enable maxpool hardware test (#12676)- Re-enable test_max_pool2d_slice.py when run on Hexagon  hardware (as opposed to hexagon-sim).  This is now safe because https://github.com/apache/tvm/issues/11928  has been fixed.,0
[TOPI] Fix softmax bug (#437),0
"[API] Prefetch schedule supported (#258)* prefetch interface added* prefetch python comments modified. prefetch info data structure maintained.* start injecting prefetches. first step (domain touch) implemented.* domain touch tested.* Prefetch ir_mutator and ir_visitor dispatch registered.* modify domain touched from passing a func_ref to passing a tensor* modify domain touched from passing a func_ref to passing a tensor* modify Tensor copy to Tensor ref* temp commit for rebase* debug info removed, typo fixed, ready to rebase* prefetch flatten test add!* roll back builtin functions to side effect functions* lint error fixed!* add cache line size to storage flatten argument* forgot modifications add* change code style to dmlc-like; get rid of can_prove, use manually compute instead* python lint error fixed* modify instrinsic name to pass tests* [TEST] get rid of str(), replace them by accessing attributes* change map to list comprehension* redundant numpy import removed",4
[DOC]Remove non-existent parameter doc (#2277),2
[TFLITE]Round op parsing support added (#5022),1
"[ONNX][Relay] Support ""tf_crop_and_resize"" in relay Resize op. (#9475)* add fallback to opset 11* Support tf_crop_and_resize in resize op* change api use in the rest of the codebasereally fix the tests* respond to review comments, improve doc strings* fix docstring indentation* remove N anc C from resize roi",4
Update stale link to new location (#6819),1
[FIX] Allow tokenizer to parse numbers greater than INT_MAX. (#8120),1
"[microNPU] Fix bug with re-reading in EncodeConstants (#9646)When a striping strategy that leads to weightsbeing re-read was deployed, the logic in EncodeConstantsfailed. This adds a test for that case and fixed thepass so it handles it correctly.Change-Id: I6f54cdb7be69428e49c3b4208271cd3e6c192e5d",5
[BUILD] add with_api_wrapper to lower (#95),1
Support sub warp reduction for CUDA target. (#10207)* upd* upd* upd* lint* fix* upd docstring* upd,2
Enable conv family fused with gelu (#12106),1
[Relay][Training] Small refactoring (#3893)* init* fix,0
add reviewer (#9430),1
"Add missing headers to llvm_module.cc/.h, NFC (#11925)",1
add a few gradients (#5899),1
fix #4670: add bias for fc layer (#4801),1
[TOPI] Fix flaky testcase for check round (#4211),3
"[Vulkan][Unittests] Add parametrization to vulkan unit tests. (#8348)This also switches to using `vulkan -from_device=0` by default, andmarks tests as `pytest.xfail` if the device does not support thefunctionality being tested.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",5
[skip ci][ci] Fix stale test in teams tagging (#10920)This is failing in `main` but as of 8e438683a4a815ae2d5b528360ae0f111501b607 it's not used anymoreCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[Refactor] Remove dead code from depthwise_conv2d for Intel graphics (#8381)After fix a66186b, I saw that it should be necessary to do the same fixfor depthwise_conv2d for intel graphics. I saw that we never used theremoved code and it is just the same code fromcuda/depthwise_conv2d.py. So we can use the cuda implementation when itwill be necessary.",1
"[BYOC][ACL] Improved pooling support (#6248)* [BYOC][ACL] Improved pooling supportAdds support in ACL for the following relay pooling operators and composite functions:  * nn.avg_pool2d (fp32), cast + nn.avg_pool2d(uint8) + cast => AVG pool  * nn.global_max_pool2d => Global MAX pool  * nn.global_avg_pool2d, cast + nn.global_avg_pool2d(uint8) + cast => Global AVG pool  * power(2) + nn.avg_pool2d + sqrt => L2 pooling (for fp32 only)Tests updated to reflect these changes.Change-Id: I1644b67b60ebb252344eb9695a521d2d958c724e* Address commentsChange-Id: Ibe8a61b4c42da246ce54701c89ea985b423c8f83* Fix not checking output saturationChange-Id: Ia6f3d9db31cfb8c417d8556d29961210fea418b2* Use defined set of trialsChange-Id: Ib180e3a0cbb84d6fa00c7e1994f58cb62662db15* Rebase masterChange-Id: I5c932751cd38da06d6f2b397be5d8ab7fdeb169f",4
[ci] Skip CI based on globs (#10456)Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[Relay][Quantization] KL-divergence-based per-layer calibration (#3538)* [Relay][Quantization] Support floating-point scale* [Relay][Quantization] KL-divergence calibration on dataset* Fix unhandled LeftShift case in QuantizeRealize* Fix lint* drop QBias* fix lint* address comments* address comments* Update comments* address comments* lint* kQIdentity = 0,1
[Frontend][ONNX] Support RandomNormal operator (#9493),1
"[Apps] Pin android_camera TensorFlow/Keras dependency version (#12710)At the moment, android camera is installing latest TF and Keraswhich is causing the following issue in CI:```  File "".../keras/dtensor/lazy_variable.py"", line 26, in <module>    from tensorflow.python.trackable import base as trackableModuleNotFoundError: No module named 'tensorflow.python.trackable'```This patch fixes the versions in the last known working versionsof both: TF 2.9.1 and Keras 2.9.",1
"[VTA][Chisel] TSIM VTA Source Refactor (#4163)* app init push* fix on readme* change name, add bit serial explanantion* rm serialLoadMM, change doc* syntax change for readme* add parallel test functionality* fix readme* add python doc* syntax* init commit* fix empty line* fix typo",2
fix dense tuning (#3768),0
[CI][Caffe Frontend] Change the caffe deps into SSD distribution (#9060)* Change the caffe deps into SSD distribution* update make flag* remove `rm -rf /var/lib/apt/lists/*`* install all python packages in one pip command* install latest package version* add caffe-frontend dependencies,1
Support quantized NEG operator in TFLite frontend (#9404),1
Fix comment of binary op 'elemwise_div' (#1712),0
[TOP] Support ceil_mode,1
Change cat image extension to png to match its download URL (#1800),4
[BACKPORT-0.6][Bugfix][Arith] keep div_mode during floordiv simplify (#5922),0
Also strip prefix from TVM_LOG_DEBUG specs. (#10755),2
"[Relay] Plumb external codegen target via Target.current() (#11432)* [Relay] Plumb external codegen target via Target.current() for all external codegen paths(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).We want both old-style (via relay.ext.$toolchain) and new-style (via ""RelayToTIR"" Passattribute on target kind) external codegen to be able to access the current 'external codegen'Target instance via Target.current(). - For old-style, plumb the true Target through TEComplier and push it on the context   stack before calling relay.ext.$toolchain. - For new-style, pass the CompilationConfig to the RelayToTIRTargetHook pass, make the jump from   ""Compiler"" attribute value to Target via the new CompilationConfig::FindPrimitiveTargetForKind   method, and push on the stack before invoking the custom ""RelayToTIR"" pass.While working on this discovered RelayToTIRTargetHook was incompatible with the VM's compilationflow since RelayToTIRTargetHook assumes all ""Compiler"" attributed functions are inlined. Generalizeit to support both inline and global function styles.Extend Target::IsExternalCodegen to recognize target kinds with ""RelayToTIR"" attributes asexternal.Update target hooks unit test to exercise new support for outline-style, picking up the current target,and compiling via the VM.* - A bit of polishing en passant.* - Add comment as per Josh's suggestionCan't repro tests/python/contrib/test_ethosu/cascader/test_scheduler.py::test_compute_cycles_annotation failure, flake?",0
"[skip ci][ci] Remove -i from lint scripts (#10469)This was changed in #8509 to run without checking the file formatting, which would lead to pylint errors like we saw on `main` in https://github.com/apache/tvm/commit/0c836b73ffd9669bcc416515dce6436cbd7d7ebe.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
[TVMC] tune: Use proper caps for AutoTVM and AutoScheduler (#10864)Use proper caps in help messages when mentioning AutoTVM andAutoScheduler tuners.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,1
[BUGFIX] Add cuda 11 to contrib.nvcc.find_libdevice_path() (#5902),1
[TFLite] Support for 'SAME' Padding option for TRANSPOSE_CONV operator of TFLite. (#6381)* [TFLite] Support for 'SAME' Padding option for TRANSPOSE_CONV operator of TFLite.* Added support for 'SAME' Padding option for TRANSPOSE_CONV operator for all  valid kernel sizes.* Added tests for 'SAME' Padding option for TRANSPOSE_CONV operator.* Minor Changes.,4
[VTA] [Chisel] Added Chisel Module Unit Test Infrastructure (#3698)* added wholething* changed build and makefile,2
[Refactor] Rename Datatype to ADT (#4156)We think it will reduce the confusion with the meaning.https://discuss.tvm.ai/t/discuss-consider-rename-vm-datatype/4339,5
"Created CSourceMetaData module for model metadata (#7002)* Created CSourceMetaData module for model metadata* Currently, there is a MetaData module to capture constants  conditionaly if the runtime modules implement const init  PackedFuncs. However, this one relies on a load process  in which the metadata is created on volatile memory that  may be not usable in uTVM environments.* There is a need for model level metadata that is valid  across all runtime modules such as the func registry  when creating a system-lib.* This commit implements a CSoureMetaData module to hold  func registry that collects function names from the  runtime module and generates a c source file to be  linked with final artifact.* Modified and added export_library for utvmChange-Id: Ie2e8e2aea1a66520f03fe8af7cc5bdf27339ea10* Created CSourceMetaData module for model metadata* fixed llvm_module to return null pfs for  get_symbol and get_const_varsChange-Id: I84810e0695d4d6fb314af2469117f965eed71b51* Created CSourceMetaData module for model metadata*fixed bundle_deploy testsChange-Id: I0d1332a4abbb6830531784c59264021bbbd7148a* Created CSourceMetaData module for model metadata*fixed export_library not to insert ""options"" when targeting tar*fixed unit testsChange-Id: Ia1686889498b71af66f1a0311a059154ad3c2c3e* Created CSourceMetaData module for model metadata* enable wasm to support csource metadata module* disabled non DSOExportables from using csource metadata moduleChange-Id: Ie09beaad35cbc2ef738d1d24d91e249b5e099569* Created CSourceMetaData module for model metadata* changed const pfs to be called only on external modules  or DSOExportable modulesChange-Id: I6ad28f166c0fc27a2548c851bf9287ec805550d1* Created CSourceMetaData module for model metadata* CSourceMetadata module wrapper is only created for c/llvm targetsChange-Id: I13cb4140c17e2e1f91d495b15a1ff7eeab9fb14d* Created CSourceMetaData module for model metadata*target should be defined to use csourcemetdata moduleChange-Id: Id8e55b23d0007a79c550334de2c0fec63d40171f* Created CSourceMetaData module for model metadata* reinstate llvm func registryChange-Id: I53e0754b6fb533637f08b25e98064d8c04092de4* Created CSourceMetaData module for model metadata* addressed comments and fixed bugsChange-Id: I26401685dc803aeaf7642c865df88d683419e859* Created CSourceMetaData module for model metadata* addressed a missed commentChange-Id: I65e65c30bc780a946f3f1b8372c40a49a5c20582* Created CSourceMetaData module for model metadata* te build interface should only include c-source metadata if  targetting ""c""Change-Id: Ie23cb8c6231c1f2de6d2827084774e3510288098* Created CSourceMetaData module for model metadata* c_source modules should be created only if they are  non-DSO exportableChange-Id: I53f2f8e9caa41f133446f8881b9dc541ebeee8cc* Created CSourceMetaData module for model metadata* documetation misalignment in source_module.ccChange-Id: I83e2c29b1f2980ca65a694304720dc58a5cb7879* Created CSourceMetaData module for model metadata* typo : same object file written as a dependency in the MakefileChange-Id: I8becc4196d286cfb6372768687b3c836799dcb78* Created CSourceMetaData module for model metadata* removed unused param from a briefChange-Id: Ie4db2aca3b7ea147bd8c65ef5d1cc2146f530e76* Created CSourceMetaData module for model metadata* made export library use c as the format for c source modulesChange-Id: Ie2fd6204414f0fa43988a8082d18af7a3225e237* Created CSourceMetaData module for model metadata*addressed a nitChange-Id: I6084b8c06ddfaaece295439dbab589e6e202b664",5
[TE COMPILER] Propagate structural hash from relay function to TIR function (#10475)The structural hash of each relay function is copied to the TIR functionso that users can associate relay functions with their lowered TIRversion.,1
[Pylint] Making hexagon tests pylint compliant Part 1 of N (#12082),3
[CONTRIB/BLAS] Add CBLAS Example to contrib (#120)* [CONTRIB/BLAS] Add CBLAS Example to contrib* Update makefile,2
[Relay/Frontend][TFLite] Change the output shape calculation based on keep_dim option in fully connected (#9840)* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Change the output shape calculation based on keep_dim option in fully connected* TODO : Need to construct a fc op with (keep_num_dims == True)* TODO : Need to construct a fc op with (keep_num_dims == True),2
[TRT] Add check to use setBindingDimensions in TRT 6.0.1+ (#11178),1
[TensorRT] Fix pad_value access (removed from PadAttrs) (#9858),5
[SGX] Improve edgeroutines (#1775),1
[VTA] Hotfix for padded load test in Chisel VTA (#4264)* Update TensorUtil.scala* Update test_vta_insn.py,3
Fix compile time and runtime errors of EdgeTPURuntime (#8133)* Fixed the destruction order tflite::Interpreter and EdgeTPUContext* Fixed include omission* Formatted,0
"[DOCKER] add ci tag, upgrade gpu-ci (#2438)",1
[cuDNN] Add cuDNN grouped convolutions support (#5319)Signed-off-by: Wei Pan <weip@nvidia.com>,1
[TESTS] add gpuonly tests for python unittests and integration (#6346),3
"[TESTS] Decrease test times by introducing testing model (#6235)Adds a new testing model `tvm.relay.testing.synthetic` which is a small,but representative model. Replaces resnet with this model in many tests.",3
"[TVMC] Treat invalid FILE arguments (#9213)Currently if an invalid FILE argument is passed to 'tvmc run' the userwill catch a traceback because exceptions are not treated, for instance:$ tvmc run /tmp/nonexistingfile will throw a FileNotFoundError trace.This commit catches the possible exceptions that can happen when aninvalid FILE argument is used and convert them to better messages forthe user so the invalid FILE can be easily spotted.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
Add phisiart to code owner of webgl module (#1070),1
[TE Schedule] Fix broken 2D softmax TE schedules when axis=0 (#11803)* Support arbitrary reduce axis in softmax schedule.* Fix lint.,0
[Keras] Support return_sequences in LSTM (#9303),1
Fix typo in Git Usage Tips (#9377),2
[TEST][TOPI] of depthwise_conv2d (#230)* test of depthwise_conv2d* fix nose test error* python3 fix,0
[RELAY] Move Layout to tvm Node system (#2125),5
[COMMUNITY] driazati -> Committer (#11525),3
[OP] Fix reduce op problem when axis=None (#2436),0
[Relay][Legalize][ARM_CPU] Handling NHWC layout for arm_cpu. (#3754),5
[TEST] Temporary disable test_mutate_parallel (#6572)* [TEST] Temporary disable test_mutate_parallel* Use skip,1
Redundant batch_flatten removed for 2D input matrix in Dense layer. (#9792)* Redundant batch_flatten removed for 2D input matrix in Dense layer.* Fix to follow code review. infer_type for input[0] is called once.,5
[LLVM] LLVM codegen debug utilities (#9857)* [LLVM] Set LLVM IR names to match TIR variable names* [LLVM] Added CreatePrintf and CreateLookupReturnAddress utilities.,1
[CI] Force doc build pass to mark success (#158),4
Use new onnx API to load model from file (#1874),2
[Frontend][PyTorch][Bugfix] Ignore Cuda in PyTorch version number when comparing versions (#11511)* Do not consider cuda in the PT version number* Add docstring,2
[CI] Install wasmtime for WebAssembly tests (#5494),3
"Revert ""Actually add Compute Library tests to the Jenkins File (#8394)"" (#8400)",2
"BUG: alloc_tensor offset and reshape shape should be on the CPU (#9421)* BUG: alloc_tensor offset and reshape shape should be on the CPUThe VM ManifestAlloc pass was allocating constants in a few places Iforgot to tag with on_device for the host/cpu. As a result the runtimewould (silently) do the x-device copy, which destroys perf.To make this easier to spot in the future added a 'constants' propertyto the VM Executable to dump the shape & device for all VM constants.This is CORE-102 in OctoML JIRA.* [checkpoint] Older compilers can't handle << overload* [checkpoint] Woops, forgot requires_cuda",1
"[CI] Update Hexagon image to install boost (#12613)The new image has xgboost installed, which I need for https://github.com/apache/tvm/pull/12587Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/279/pipeline",2
[docs] Missing documentation 'autodocsumm' (#6595),2
[RELAY][PASS] General OpFusion. (#2090),4
[RELAY][AUTOTVM] Extract tuning tasks from Relay programs (#2181),4
Fix make format (#11197),1
fix deprecation warning (#3446),2
Fix a multithreaded bug in llvm LazyInitJIT (#3158),5
checkin initial of itervar,5
[RELAY][OP] end to end support for pad op. (#2213),1
[CI] Install libc6-dev-i386 to compile wasm32 (#6886)* [CI] Pin wasmtime version to 0.16.0* Keep the wasmtime version to the latest,3
[relay][frontend] Enable ssd test by attaching schedules to multibox and ssd ops (#2322)* add ssd ops to mxnet.py* add ssd ops to mxnet.py* add result check for multibox and nms unit tests* add result check for multibox and nms unit tests* address @kevinthesun's comments* Disable cuda test for nms for now.,3
[RELAY]impose a max op limit to the op fusion pass (#4002)* impose a max op limit to op fusion* use cross platform data type,5
remove FLOP computation for 3rd party lib call (#4005),4
[Cmake] Add default value for option USE_DNNL_CODEGEN in the cmake (#6099),1
Update frontend.rst (#1881),5
[TOPI] disable test_shift with i8 datatype (#7597)https://github.com/apache/tvm/issues/7539Co-authored-by: guoweijun <guoweijun@baidu.com>,0
Version 0.5 (#2604)* Version 0.5* update version.py* update news* update news* update news,1
Fix onnx import bugs (#4750)* Fix onnx import bugsFix onnx attributes of string type incorrect handlingMerge symmetric padding of Conv to symmetric form* Only merge symmetric padding for conv2d,1
"Make Tensor comparator and hash to be aware of same op and index, init checkin of the ir generation",5
[TIR] Fix check for multiple axis separators (#10845)Fix check for more than 1 axis separators and add 3d test,3
[RELAY] Fix segfault in pretty print when ObjectRef is null (#5681)* [RELAY] Fix segfault in pretty print when ObjectRef is nullEncountered when pretty printing module with function attribute equal to NullValue<ObjectRef>().Change-Id: I2e7b304859f03038730ba9c3b9db41ebd3e1fbb5* Add test caseChange-Id: I579b20da3f5d49054823392be80aaf78a055f596,4
[SETUP] Always use relpath for setup (#421)* [SETUP] Always use relpath for setup* [CMAKE] Fix cmake llvm build,1
"[Target] Enable device querying for all targets. (#8602)- Move ""from_device"" argument definition from ""vulkan"" target to all  targets.- Add device querying to TargetInternal::FromConfig, using  ""from_device"" argument.  If present, these have lower priority than  explicitly-specified attributes, but higher priority than the  default attribute values.- Add default no-op DeviceAPI::GetTargetProperty.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",5
[docker][RVM][microtvm] Refactor CMSIS installation to add to RVM (#11148)* Refactor CMSIS installation for RVM* Fix `ethosu_dir` existing directory* Address Andrew comment,1
[ONNX][FRONTEND] Constantfill - #1539 (#1764),5
"[microTVM] Add support for AutoTVM (#8715)* Initial commit of API server impl.* initial commit of api client* Add TVM-side glue code to use Project API* Change tvm.micro.Session to use Project API* Rework how crt_config.h is used on the host. * use template crt_config.h for host test runtime; delete   src/runtime/crt/host/crt_config.h so that it doesn't diverge from   the template * bring template crt_config.h inline with the one actually in use  * rename to MAX_STRLEN_DLTYPE * Create a dedicated TVM-side host crt_config.h in src/runtime/micro* Modify Transport infrastructure to work with Project API* Add host microTVM API server* Zephyr implementation of microTVM API server * move all zephyr projects to apps/microtvm/zephyr/template_project* consolidate CcompilerAnnotator* Allow model library format with c backend, add test.* Update unit tests* fix incorrect doc* Delete old Zephyr build infrastructure* Delete old build abstractions* Delete old Transport implementations and simplify module* lint* ASF header* address gromero comments* final fixes?* fix is_shutdown* fix user-facing API* fix TempDirectory / operator* Update micro_tflite tutorial* lint* fix test_crt and test_link_params* undo global micro import, hopefully fix fixture* lint* fix more tests* Add session_constructor_args to tracker request() function. * Allows tracker clients to open non-traditional RPC sessions* Generate entry_func symbol in C host codegen. * Needed for AutoTVM.* print MeasureErrorNo enum value in MeasureResult repr* Add microTVM session constructor. * This constructor is to be called from the RPC driver to flash and   connect to the RPC server on the microcontroller.* add build_kwargs as a Builder constructor arg. * build_kwargs is derived from pre-configured args, the runner, and   now from the script. * user-supplied build kwargs override the other two, and a warning is   printed if any key is overridden.* Add do_fork option to Builder, to support stateful builders * When AutoTVM builder forks, any global state modified by the   build_func is lost between builds* Checkin module_loader used to build and flash microTVM for autotuning.* Import micro into top-level when enabled. * AutoTVM RPC server needs to load the micro session constructor.* Add tvm.contrib.random.random_fill to microTVM. * Allows autotuning with random data.* Move compilation to runner :O* Add a tutorial for AutoTVM with microcontrollers.* Fix si_prefix in autotuner callback* black format and git-clang-format* Switch tutorial back to qemu version* improve error reporting so CI will show test error* black format* autotvm is working* fix tutorial* fix dependencies* fix auto tune issue* lint* address comments* fix lint* test crt and zephyr added* fix func registery size* moved autotune test and fixed* fix crt test* address comments* change relay text* change relay in text_zephyr* class added* changed relay module in tutorial and cleanup* address comments* address TK comments* change fork* final comments* retrigger due to flahy test* fix tutorial* retrigger* fix changes due to mergeCo-authored-by: Andrew Reusch <areusch@octoml.ai>",5
Add gradient graph (#280)* Add creating gradient symbol* Fix lint* Address comments* Fix typo* Address comment,1
[QNN] Support different qnn params between in/out tensor in leaky_relu (#12116)* [QNN] Support different qnn params between in/out tensor in leaky_relu* format code* format code* fix bug* fix format* fix format* fix,0
"[Minor] fix redundant compute (#10580)we should bind axis in CS stage to threadIdx in each warp, otherwise awarp will compute all the tiles in a block.Co-authored-by: tom.hx <tom.hx@alibaba-inc.com>",0
[CI] Migrate Tensorflow and Tensorflow lite in CI to  2.1.0 (#5392)* Migrate Tensorflow and TFLite in the CI up to 1.15.2The latest stable version of Tensorflow and Tensorflow litein the 1.x series is 1.15.2. The tflite frontend is receivingsupport for versions of tflite > 1.14 but there is no consistenttesting.There are 2 failures already in the source base with tf 1.15and I'm concerned this will just get exacerbated over timeif we don't have CI picking this up and I view this as a steppingstone towards stepping CI to TF2.x.The test failures that I have commented will get issues raisedfor them as issues to be fixed.* Comment out run of qnn_mobilenet_v3_netThis is another test that fails with TFlite 1.15.2* Skip the qnn_mobilenet_v3 test in the pytest fashion.* Switch docker versions to support Tensorflow 2.1.0* Fix up pytest imports and usage.* Skip these tests currently for Tensorflow 2.1.0,3
[CODEGEN] Fix let expression (#1727),0
[PASS] Enhance scale fold axis (#424),4
Move flake8 to ci_lint (#8652)* Move flake8 to ci_lintThis fixes the scenario where you lint with ci_lint but it can stillfail in PR due to flake8 being injected only into the Mac build.* Disable flake8 until the docker changes have landed,4
"[ci] Add more details when showing node info (#10195)This adds some more information to help debug when there are infra problems with Jenkins, notably:* More Jenkins environment variables: https://www.jenkins.io/doc/book/pipeline/jenkinsfile/#using-environment-variables* EC2 metadata* System level information (disk space, CPUs, memory)Co-authored-by: driazati <driazati@users.noreply.github.com>",1
"[Relay, Quantization, TOPI] int8 dense on CUDA & Dense op quantization  (#2877)* Quantize dense layers* Add out_dtype arggument to dense; Add dense_int8 on CUDA* Add topi unittest of dense int8* Fix relay* Fix topi integration* Fix quantization* Update dense_rewrite* Triger CI* Change qconfig quantize_dense to quantize_op* Fix* Remove quantize_op from qconfig",5
"[Bugfix][TIR] compute-at/fuse/split dtype mismatch (#11582)The schedule primitives, including compute-at, fuse and split usuallygenerate loop variables with `dtype=int32` as default. However, in somemodels, there are usecases where int64 are part of tensor shapes, whichleads to unexpected behavior in scheduling. This PR brings the fix toexisting known issues.",0
Handle empty tuple (#95),0
"Dynamic Strided Slice (#6316)* Dynamic Strided Slice* fix clang-format lint* remove debug print* respond to review comments* respond to yongwww's comments* fix bad rebase* revert hybrid-script assert* reformat mxnet change* use new testing api* while getting test to work with the new testing API, refactor all of the tests iin the dyn directory",3
Update date in the NOTICE (#5942),5
[Fix] fix compilation error when setting USE_RELAY_DEBUG (#6380)* fix compilation error when setting USE_RELAY_DEBUG* awake github ci-test* remove unnecessary debug log,2
Add Arm DSP implementation of Depthwise Conv2D (#12448),1
[TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>,1
"Improve error messages with TVM_LOG_DEBUG and add docs (#11344)* Improve error messages with TVM_LOG_DEBUG and add docs.* Fix requirement to prepend ""src"" with /.",1
[TIR] Support affine expressions as indices in reverse compute inline (#11317)* [TIR] Support affine expressions as indices in reverse compute inline* fix trivial iterators,0
"[AlterLayout] NCHWc upsampling, fix depthwise conv (#2806)* [AlterLayout] NCHW upsampling* [Relay][Pass] Fix Depthwise AlterLayout",0
[Adreno] Define memory_info for global.texture* (#12647)There are now many warnings in the tuning process about undefined memory information when using textures. A definition is required as textures* are tagged.,1
"[Fix,Conda] update conda download url (#6760)Co-authored-by: Shibui Yusuke <yusuke.shibui@ShibuinoMacBook-Pro.local>",5
[microTVM][Zephyr] Enable -O2 optimization on build by default (#12718)* add spped optimization flag* trigger* add exception for qemu_riscv64,1
[TEST][FLAKY] Fix flaky test on topk and quantize pass (#3362)* fix flaky test* fix flaky quantize pass,4
update comment,5
Skeleton of bound inference passing rule,4
"[Torch, QNN] Support quantized mobilenet v3 from torch 1.8 (#7606)* [Torch] support hardsigmoid* qhswish first impl* add qhardsigmoid but the result is not correct* add qmv3 to test* comment fix",0
Update SGX example Cargo.toml (#6067),5
change project to NNVM,4
Replace learnt with learned (#3684),5
[LANG/PASS] InjectVirtualThread (#38),4
[RPC] clarify error message for unmatched context (#451)Clarify confusing error message for unmatched context,0
query rpc tracker - sort servers by key name (#10641)* query rpc tracker - sort servers by key name* fix black formatingCo-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,1
"[TOPI] Make cumsum IR reusable, add thrust scan (#7303)* import changes from scan branchcommit cf0d4fdf3bf8fa6e1d6abf631042de28176923c3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 10:12:01 2020 +0900    get valid count test workingcommit eb142d3ee9bb16ddf8d37fdec10c1bcda209deaaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 07:22:00 2020 +0900    integrate new cumsum changecommit f89684d73dad1f863b4fd291e8804b5c24eae94fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 06:56:46 2020 +0900    remove ceil_div from nmscommit a2ad4dea87d9a637745fb0a40ff9bbdde286194aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 20:36:34 2020 +0900    add api for returning reduction from ex scan outputcommit b7f4ef7006b722e365533bec53b1f104aa056da2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 19:49:07 2020 +0900    move ceil_div to utilscommit a9a57e34317b1f254165c3a88e465e33c7fda01bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 19:38:15 2020 +0900    rename prefix_scan.py to scan.pycommit 03ed43ff550a435a28740ce1fa62cea71b90cf2cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 19 06:12:55 2020 +0900    surpress cpplintcommit abceac980d8dfd94072acc228108d1fcd94a214cAuthor: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:36:24 2020 +0900    support more data typecommit 3e7d1f81821a1e221cbb1322ef5b23f273f51c42Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:09:51 2020 +0900    1d thrust scan workingcommit ac13b407e21a83ca57240cad205c32a5d000f999Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 19:49:25 2020 +0900    adding thrust scan supportcommit 65634e86c33786541485dc6461a96da833332297Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 19:01:11 2020 +0900    add thrust scan python stubcommit 9876c901ee8b406bc9d75ba91c4734d55f85811bAuthor: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:55:14 2020 +0900    introduce prefix_scan.py and move scan ir in nms.pycommit 667bdd3b135a03b53937fdb664915e07f1365ee1Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 15:06:18 2020 +0900    make the scan loop exclusivecommit 480787bc072bfc59dcc279038c772f8ad2ec03e9Author: mbrookhart <mbrookhart@octoml.ai>Date:   Thu Dec 17 10:01:11 2020 -0700    Parallelize cumsum in get_valid_counts* fix for 1d scan* rename* cast to out dtype* do not run return reduction for inclusive scan* remove another ceil_div definition* adding scan test* add scheduling for scan op, fixed scan 1d test* pylint fix* add doc string* add more thrust scan test* add dynamic get valid count test, including empty size tensor* fix hard coded gpu targets for cpu only env* try retunring early if scan_size is 0* another change for empty tensor and thrust pathCo-authored-by: masa <masa@pop-os.localdomain>",4
"[Bugfix] Handled TransformNode in PassUpBitMaskOr/PassDownBitMaskOr (#10620)Previously, a layout transformation applied to a te.compute whosecomputation used a reduction axis would fail.",0
[QNN] Replace nn.leaky_relu with qnn.leaky_relu (#11930)* [QNN] Replace nn.leaky_relu with qnn.leaky_relu* jostle ci* fix typo,2
Fix the potential index overflow (#3751),0
[Frontend|MXNet] SwapAxis operator support (#5246)* MXNet swap axis* MXNet swap axis* swap axis review comment* swap axis review comment,1
[ci][docker] Use sccache everywhere by default (#11267)This adds `/opt/sccache` to the PATH of each of the CI docker images so when cmake looks for a C compiler it will pick up the sccache wrapper by default. This fixes some issues where compiler invocations weren't being run though sccache. With this approach the invoker doesn't need to do anything specific to set up sccache.This will require a follow up PR to update the Docker images and remove some of the sccache logic in `task_build.py`Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[ARM] Fix int8 NCHWc compute and alter layout (#10839)This PR fixes a bug in TE ARM int8 compute for NCHWc conv2d, introduced in https://github.com/apache/tvm/pull/10310. The compute itself, not the schedule, is broken for the following reasons:* We are using `n_elems = 8` in https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_alter_op.py#L350. Thus, the innermost axis of the transformed kernel has extent 8: https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_alter_op.py#L375* In the TE compute, we iterate over the innermost axis `ic_s_inner` of the kernel at https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L577. `ic_s_inner` has extent `n_elems` according to https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L566. `n_elems` is 4 by default according to https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L478* The ARM code that calls this compute does not explicitly pass `n_elems`, according to https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_int8.py#L106-L108* Thus, even though the innermost axis of the kernel has extent 8, the TE compute only loops over `n_elems = 4` of the input channel dimension. Initially, I tried to keep `n_elems = 8` in alter layout and fix the intrinsic definition. But `n_elems = 8` breaks tensorization pattern matching, since now the compute is doing 4x8 innermost loop but this intrinsic is supposed to do 4x4 dot product, see https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/tensor_intrin.py#L467-L479. Setting `num_int8_elements = 8` there does fix the tensorize pattern matching, but the result was still incorrect.Rather than fixing the intrin implementation in https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/tensor_intrin.py#L492 to adapt for 4x8 dot product, I settled on setting `n_elems = 4` in alter layout. It turned out this change is enough to get the correct output. Moreover, `n_elems = 8` is simply wrong for the dot product path in https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/conv2d_int8.py#L154-L155 which computes 4x4 dot product in one instruction. @tkonolige I suggest doing perf benchmark again, since the numbers in https://github.com/apache/tvm/pull/10310 are invalid.cc @mbrookhart @Mousius  @junrushao1994 @vinx13",0
fix (#3417),0
Add functionality to optionally disable Select rewriting (#2385),1
use uint32_t (#478),1
[DOC] Add how to enable IR debug messages. (#7978),0
[Runtime][MISRA-C][Bundle] Bundle deployment with static linking (#5158)* test file for static link added* rename files* Fixed static linking issue* cleanup* changed to dynamic and static demo* MISRA-C static and dynamic test* cleanup* cleanup* Update README.md* cleanup headers* update readme,5
"[Hexagon] Fix VTCM allocation (#8954)Check if a buffer is in the `vtcm_buffers` list, before it's removedfrom it.",4
[Frontend][Torch] Check graph inputs match expected (#4992)* [Frontend][Torch] Check graph inputs match expected* error/warn when missing/unused graph inputs* Change to use get_graph_input_names,1
[microNPU] Update existing microNPU tutorial for CMSIS-NN (#11285)* [microNPU] Update existing microNPU tutorial for CMSIS-NN * Added instructions to existing microNPU tutorial indicating how to offload operators to CMSIS-NN.Change-Id: I9faef1d92a2107e04cfc21b7bfd1b72dc1bd5489* [microNPU] Update existing microNPU tutorial for CMSIS-NN  * Reformat micro_ethosu.py with blackChange-Id: Id189f333b5bd891232781d4eb58522a240146c95,4
[Community] @joshpoll -> Reviewer (#3412),3
[microTVM][tutorial] AOT host-driven tutorial with TFLite model (#12182)* Add aot tutorial,1
[ARITH] Bugfix min/max const canonicalize rule (#3386),0
[Fix] Fix conv2d HWNC type strategy (#8147)* fix conv2d strategy* fix style* fix styleCo-authored-by: wangyucheng <wangyucheng@sensetime.com>,1
[RUTNIME] Support C++ RPC (#4281),1
[Frontend][TFLite] Add parser support for 'square' operator (#4915)* [Frontend][TFLite] Add parser support for square operator* Add parser implementation* Add relevant tests* Note: 'square' is an unary elemwise operator but it's added separately  in the parser since there is no Relay 'square' op  and instead we have to use 'multiply'* Change relay operation from 'multiply' to 'power'* Remove a redundant line as requested,4
[AOT] Re-enable AOT output name test on AArch64 (#10868)This was fixed in https://github.com/apache/tvm/pull/10731 as it was the mismatch of tensorflow versions in use by the different CI containers.,1
Gitignore work items in jvm and android_rpc (#10253)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,1
[VERSION] Enhance version.py to support git-describe. (#6757)* [VERSION] Enhance version.py to support git-describe.This PR enhances version.py with a --git-descrbe optionwhich allows it to generate a git describe based versiontag for potential dev related nightly packaging during thedevelopment cycle.The behavior of the normal relase remains the same.Note that the version.py still modifies the files inplaceand we only recommend using it during a clean clone based workflow.The setup.py is also updated to take advantage of the version.Note that the git info is already captured by the c++ side in a previous PR.The tool is mainly used to create PEP compatible python wheels.* Update per comment,5
Fix missing te in the code example (#6569)- caused `AttributeError: module ‘tvm’ has no attribute ‘thread_axis’  error- Fix baesd on https://discuss.tvm.apache.org/t/attributeerror-module-tvm-has-no-attribute-thread-axis/6606,0
[Topi][Relay] Support for FP16 ERF on CPU. (#11413)* Functionality and tests implemented* Formatting and lint.* Typo fix.* Reduce strictness for fp16 tests.Co-authored-by: Ubuntu <ubuntu@ip-172-31-53-187.us-west-2.compute.internal>,3
Enable copy over behavior of attributes of automatic created vars (#34),1
[Relay][Frontend][ONNX] Allow importing models with malformed Loop nodes. (#8475)* Snapshot* Undo comments.* Add testing for malformed loop nodes.* Format oops.,3
Update layout.h (#1255),5
lower plevel of conv2d winograd on cuda (#4987),5
[OBJECT] Update types slots for baseexpr and primexpr (#6814),5
Update Jenkinsfile for external runtime (#4396),1
[API] expose dir,5
"If an expression has two branches, and the pattern ignores one with a wildcard, allow grouping via dominator analysis (#7355)",1
Remove zephyr installer file after installation (#9883),2
[TensorIR] Cross-Thread Reduction (#9360)* [TensorIR] Cross-Thread Reduction* Code revision on analysis and misc* Refactor TransformReductionBlock* Refactor code organization* Address comment* Use `std::make_tuple`Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
"[FRONTEND][TENSORFLOW] Add Pad and PadV2 support (#1545)* [FRONTEND][TENSORFLOW] Add Pad and PadV2 support* Add assettion to _pad, and fix testcase for pad.",3
[RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860),1
[DOC] Fix Some Broken Web Links (#6475),2
Fix bug in processing script (#6867)The argsort command returns a new array that is the sortedindex rather than a new sorted value array. This patchstores the sorted index in a new variable and uses it toreference the predicted values.,1
[microNPU] Remove unused code from testing infra (#10462)Removing some legacy code from infra.py that is not called by anything.,5
[RUNTIME][RPC] Enable remote linking of device code. (#444)* [RUNTIME][RPC] Enable remote linking of device code.* fix build,0
[TVMScript][FIX] Fix number of arguments for T.Buffer[...] (#9758)* fix number of arguments* make test clear* Update tests/python/unittest/test_tvmscript_syntax_sugar.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>* only tuple for nowCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>,3
"[build] Fix/simplify `ccache` logic (#11189)- Remove TVM's `USE_CCACHE` option in favor  of CMake's built-in `CMAKE_C_COMPILER_LAUNCHER`  and `CMAKE_CXX_COMPILER_LAUNCHER` variables.  This eliminates a significant source of  complexity, especially:  - TVM's CI scripts, which use `sccache`    instead of `ccache`, and  - calls to `ExternalProject_add` in TVM's CMake logic.- Ensure that `CMAKE_C[XX]_COMPILER_LAUNCHER` variables  are passed through in all `ExternalProject_add` calls.- Update user documentation.",2
[Relay][Frontend][TFlite] Add support for quantized LOGISTIC (#4696)* [Relay][Frontend][TFlite] Add support for quantized LOGISTIC * add qnn implementation * add qnn test case for qnn logistic* Helper functions for quantize and dequantize.,1
[PROFILER] Add CSV output to profiler (#7797)* [PROFILER] Add CSV output to profilerThis patch changes the profiler output from a string to a Report object.A Report can either output CSV or the usual human-readable table.* no spaces after commas* Update src/runtime/profiling.ccCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* fix gcc* fix test* overall percent fix* rename overall -> device_metricsCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>,0
allow module exits without del (#8063),1
Skip onnx test cases if no onnx (#9272)This was missing a guard which meant VS Code errored on test collection.,3
[Relay][VM] Clean up the VM and VM profiler code (#4391)* [VM] add a few more API to vm* [VM][Fix] fix vm convert args* [VM] a few fixes* rename fields* update* update vm profiler* x* add doc* lint* fix test* address comments,1
[SCHEDULE] Fuse support for 0 rank tensor (#1328),1
error info (#69),5
[FRONTEND] [HYBRID] Non-zero starting supported; Buffer AttrStmt add! (#1330),1
add 2D reduction into tutorials (#204),1
added vim temporary files to gitignore (#453),2
[Torch] Simplify contiguous (#7544),5
added int type axis for relay reduce ops (#2199),1
"[PYTORCH]Take, Topk op support (#5332)* [PYTORCH]take, topk op support* Ci Failure fix",0
[BUG_FIX][TOPI] Allow topi resize to accept more options (#7532)* Make topi more permissive* Remove testing stuff* lint* Downsampling tests,3
[TOPI] Bitserial dense operators for CPU (#3051),1
[RELAY][OP] roi_pool operator alter layout (#6516)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,1
"Better grouped convolution for CPU targets (#6137)* integrated with v0.8* Rebase, and undoing accidental removal of auto scheduler NHWC support* Added ASF license header* Minor bug fixes* Added asymmetric padding supportFixed linting* Improve linting* Better linting, disable final linting checks* Fixed final linting errors (figured out how to run lint tests locally)* fixing linter formatting part 1* fixing linter formatting part 2* fixing linter formatting part 3* Update conv2d.pyFixed merge issue* Rebase, and update responding to some comments* Fixed AutoScheduler bug for NHWC case* removed infer_pad from GSPC* Rebase, and undoing accidental removal of auto scheduler NHWC support* Added ASF license header* Minor bug fixes* Added asymmetric padding supportFixed linting* Improve linting* Better linting, disable final linting checks* Fixed final linting errors (figured out how to run lint tests locally)* Update conv2d.pyFixed merge issue* Rebase, and update responding to some comments* Fixed AutoScheduler bug for NHWC case* Minor fix* Fixed removal of infer_pad to no padding* Fixed unexpected linting errorCo-authored-by: Perry Gibson <Perry.Gibson@glasgow.ac.uk>",0
[Relay] Improve build error when no lowered funcs are produced (#4132)* Improve build error when no lowered funcs* Switch from fatal to warning,2
Allow libaray path to be configurable (#50)* Allow libaray path to be configurable* Enable partial shape inference result to be passed via shape* fix python3* disallow copy assign in index,1
[RUNTIME][OPENCL] set type_key even when platform is not available (#2741),1
"Remove unused variables in AOT tests (#8686)These were re-introduced in https://github.com/apache/tvm/pull/8380,noticed when I went to rebase https://github.com/apache/tvm/pull/8650.",3
fix compact_dataflow (#9747)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,5
[AutoScheduler] Make SearchTask and ComputeDAG serializable (#6842)* serialize task and dag* fix test* more tests* format* format* format* trigger ci,3
"[ROOFLINE] Roofline analysis over RPC (#11252)* [ROOFLINE] Roofline analysis over RPCRun roofline analysis on remote devices if requested. Peak flops andpeak bandwidth estimation are done on the remote device.* allocate testing arrays directly on device and randomly fill* forgot to include remote* lower flops ratio, machine may be using multiple threads* forgot fill",1
Make from_tensorflow.py more GPU memory friendly. (#8763)* Make from_tensorflow.py more GPU memory friendly.Sphinx-gallery runs everything in a single process. Theredoesn't appear to be any easy way to force Tensorflow toreturn memory other than terminating the process. This atleast gives us a little more wiggle room.* Also deploy_sparse.py. Should probably also be done to tensorflow.rst.,1
"[Pass] Check in infershape, move indexedgraph to graph.h (#15)",4
Prevent IRSbustitute to create new buffer when buffer var is unchanged (#11103)* Prevent IRSbustitute to create new buffer when buffer var is unchanged* typo,2
[TOPI] add extern schedule for cudnn and miopen (#724)* add extern schedule for miopen* fix comment* optionally dispatch to miopen from topi* fix lint* check if current target is None* use generic dispatch for rocm conv2d* fix lint* fix workspace bug* remove blank line* remove blank line* remove blank line,4
conv2d adjusted to fix different workloads (#511),1
[Relay][Pass] Count MAC for BatchMatMul (#4157)* count MAC for BatchMatMul* update doc,2
"Fix printing of schedule operations (#8949)- Add printing of factor/nparts in ""split"".- Print correct operation name in ""fuse"".",1
fix keeping trivial loop (#982),0
"[VTA][Relay] Extending Vision model coverage compilation for VTA (#3740)* adding support for graphpack over multiply op* increasing resnet model coverage* fix indentation* lint* moving recursion limit fix into graphpack pass* moving recursionlimit to relay init* pooling on NCHWnc format* adding more models* deploy_resnet_on_vta.py* trailing line* generalizing to vision models* merge conflicts* fix, apply quantization to VTA only* improving comments* trimming models that have runtime issues for the moment* lint* lint* lint",0
"[RELAY][PYTORCH]Resize3d, Upsample3d op support (#5633)",1
[Relay][Frontend][Onnx] Add RNN operation for ONNX frontend (#12213)* Add RNN operation for ONNX frontend.* link checks* rm test_rnn_batchwise in unsupported_onnx_tests* merge similar codes to class methods* implement opset 14 and refactor test_forward* reformat verify_rnn_helperCo-authored-by: 张亦驰 <zhangyichi1@corp.netease.com>,3
"[CODEGEN][OPENCL] Fix compile error about ternary expression. (#2821)Code like this can't be built with NV OpenCL, and it needs an explicit type  converison for ternary expression if return type is uchar.       uchar i = 0, j = 0;       uchar t = max((uchar)j, ((i > 0) ? (uchar)1 : (uchar)0));",0
[Relay][Frontend] Add Crop op converter (#3241)* Add Crop op converter* lint* x,1
[DRIVER] Specify name when build with primfunc (#9602),5
"Add pytest-xdist and pytest-profiling to the base installation packages. (#6736)For building and testing some small portions of the python testsuite,I've been playing off and on with xdist and pytest-profiling.We know it's not safe for the entirity of CI yet but this couldenable smaller parts of pipelines that folks use using thecommon scripts to be parallelized or indeed profiled for moreinsight into where time is spent in building and testing TVM",3
[Hexagon] moves conftest.py to tvm.contrib.hexagon so outside repos can access the testing fixtures (#11277)* adding pytest_plugin to python so other repos can access* import requires_hexagon_toolchain from tvm.contrib.hexagon.pytest_plugin,3
"[VTA][TSIM] Verilator compile report error for printf (#3438)[Symptom]after follow the tsim example readme, doing verilator install by 'sudo apt-get-install verilator'Once enable 'debug' or manually add 'printf' logic in chisel module, verilator would reportfollowing error.'syntax error, unexpected INTEGER NUMBER, expecting IDENTIFIER'[Fix]upgrade verilator to 4.012, issue fixed.[Solution]Link README.md verilator install steps with verilator home websiteinstall instruction.",2
[TOPI] [Hexagon] Uint8 Reshape and batch flatten slice ops (#12037)* [TOPI] [Hexagon] Uint8 Reshape and batch flatten slice ops* Fix documentation,2
[CONTRIB] cuBLAS integration (#744)* add cublas support* integrate cublas to topi dense* add cublas error check* minor fix* fix lint* remove topi import from contrib unittest,3
[Doc] Relay tutorial - Deploy the Pretrained Model on Raspberry Pi (#2693),2
"[cpptest] Use find_package to locate GTest files (#9208)* [cpptest] Use find_package to locate GTest filesThere is a standard CMake utility for finding GTest, use that insteadof doing a manual search.Thanks @tkonolige for the suggestion!* Use GTest:: targets instead of variable* Add USE_GTEST to cmake/config.cmake* Remove GTEST_INCLUDE_DIRS from target_include_directoriesCmake will automatically propagate the interface include directoriesto the dependends of (in this case) GTest.* Restart CI",3
fix int set analysis on negative scale (#9776),1
[TEST] Temporary disable fp16 type_as test for PyTorch Frontend (#5799),3
fix dcgan layer naming overlap (#2145),0
expose range,5
[RUNTIME] Finish GPU runtime and python interface (#16)* [RUNTIME] Finish GPU runtime and python interface* fix travis test* fix build,0
"[ETHOSN] Streamline Ethos(TM)-N cross-compile rpc usage (#9477)* When cross-compiling the runtime or rpc application, LLVM is not  required so don't insist on it being enabled for Ethos-N.",0
[Frontend][Tensorflow]add batch_dim support for gatherV2 (#7951)* add batch_dim support* fix lint* add check for num of arguments for topi.take* fix gpu test cases* add check for batch_dims in take_grad,1
Add CODEOWNERS (#545),1
Introduce NodePtr (#9),5
"[Runtime][PipelineExecutor]Add forwarding queue logic for set input. (#10990)* [Runtime][PipelineExecutor]Add forwarding queue logic for set input.When the set_input function get called, a runtime of pipeline may notyet finish the former computation work then the new set_input call wouldbreak the current computation logic, to avoid such issue, we add theforwarding queue logic to guarantee the order of input data consuming.* polish the documents.",2
[hotfix] missing include headers (#4204),0
"[TIR] Get read/write access precisely for opaque access. (#11110)* [TIR] Get read/write access precisely for opaque access.When the opaque access is wrapped with tvm_access_ptr, we can get the access_maskfrom tvm_access_ptr in BlockReadWriteDetector and put this opaque access to read_regionsor write_regions according to access_mask.* [TIR] Add parameter extent for access_ptr.Co-authored-by: sqing <qing.siqi@intellif.com>",2
Avoid crash when linear activation does not have alpha and beta defined (#306),5
[TOPI] Overload operators of Tensor when TOPI is imported (#1029),2
[RELAY] First pass at pretty printer (#1749),4
"[RVM] Fix AttributeError when action is not specified (#9683)Although an action (build, test, or release) is always required bybase-tool-box.py currently actions are not set as ""required"" in theparser, hence it doesn't complain when an action is missing and laterwhen 'args.platform' attribute (present in all actions) is referencedthe tool exits abruptly due to an AttributeError (because 'platform' isnot set), without giving any clue on what went wrong. For instance:$ python3 ./base-box-tool.py --provider virtualbox # no action is givenThis commit fixes it by setting action subparsers as required so theparser complains when no action is specified.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
Add autoscheduler support to tvmc (#7070)* Add autoscheduler support to tvmc- Add an autoschedule module to tvmc- Extract common tuning option between autotuner and autoscheduler- Add testing* Linting and small bug-fixing* Addressing comments and refactoring* Fix linting* rebasing* Addressing comments - 2* Addressing comments -3Change-Id: I207872757473210681d9db04bfdcd2c5e6deaa05* Addressing comments - 4Change-Id: I11f73c9b32e83c013cfb2224ccce06f60a128af7,4
[RELAY] Support concatenate. (#2298),1
Fix node.func to node.funcs on parser.py (#12053),0
[Hexagon] Guard UserDMA code with architecture check (#10770)UserDMA is only available in Hexagon V68 and later.This fixes issue https://github.com/apache/tvm/issues/10768.,0
[RELAY] Add sigmoid relay operator (#1836),1
[NNVM][CONVOLUTION] Group convolution generalization for NHWC (#1232),5
[LLVM] Vectorized load/store (#60),5
[Hexagon] Removes directory after stopping the server (#11212)* removes hexagon directory,4
Fix verilog testcase (#1047),3
"[Relay][Frontend][ONNX] operator support: DepthToSpace, SpaceToDepth (#4271)",1
[ci] Bump job timeout to 3 hours (#11350)This is intended to be temporary to avoid timeouts on jobs while we work on getting some things under control like artifact upload time and shards for various jobs.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[CODEGEN][SDACCEL] add support for specifying FPGA device name (#1448),1
[FFI] Improve error messages when array/map types do not match in function calls (#7330)* [FIX] Improve error messages when array/map types do not match in function calls* missed some places for renaming* Rename Mismatch to CheckAndGetMismatch. Add Check back in. Use Optional::defined.* Optional<String> -> String* formatting* move ObjectTypeChecker template specializations into where thier respective classes are defined so they will always be found correctly,4
"[microTVM] Add method to query template info without creating a project (#8950)Add info() method to TemplateProject class so it's possible to query allavailable options for a given template project without creating a newone. This is necessary because TVMC will query the available options fora given template project to show them to the user so the user can usethem to finally create a new project dir.That is also useful in general to query the available options for anyproject type. For example, one can query all boards available on theZephyr platform with:import tvm.micro.project as project_apitemplate = project_api.TemplateProject.from_directory(ZEPHYR_TEMPLATE_DIR)boards = template.info()[""project_options""][8][""choices""]where 8 element refers to the ""zephyr_board"" option.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",5
flaky off (#8972),5
init skeletons,5
[RPC] Allow back pressure from writer (#250)* [RPC] Allow backpressure from writer* fix* fix,0
Add issue template  (#1118),0
[MODULE] Enable OpenCL and CUDA Modules (#53),0
Fix gpu not found when running TVM docker (#4975),2
fix UTF (#8185),0
[APP] Android RPC README (#395),5
[RUNTIME] Add function to pack arguments (#452),1
[RELAY] Add broadcast_to operator (#2276),1
"[SCHEDULE][REFACTOR] Default Fuse to outer inner, consistent to split (#289)* [SCHEDULE] Fix fuse node order* Make fuse order consistent with split",1
[BYOC] Fix incorrect conv2d padding handling of `dnnl with c source runtime` (#9097)Co-authored-by: sunway <wei.sun@hexintek.com>,1
[Relay] DCGAN port (#2010),5
[CI] [Hexagon] Update docker tag in jenkins (#11588),2
[PTYTHON] Migrate VTA TIR passes to the new pass manager. (#5397),4
rev jenkins containers for #7995 (#8155),2
[YOLO]yolo op added in frontend and removed from topi (#1974),4
"[Relay, TOPI] Complete rewrite of where op to support broadcasting (#6759)* where type rel with broadcast* add tests for where with broadcast* clean up tests* uncomment other tests* add more tests* update doc* CHECK -> ICHECK* add where any test* fix format* remove useless detections for one* set manual seed* ported shape broadcast helper func to hybridscript* remove shape function helper from cppCo-authored-by: masa <masa@pop-os.localdomain>",1
[Relay/TOPI][OP] Add arange op in Relay and TOPI (#2621)* Add arange op* Update docs* Fix bug* add sanity check in relay and mxnet frontend mapping* lint* nits* pylint* don't allow empty output from arange* Remove empty test for arange* Fix bug and update doc,2
Generate correct output tensor names in C Interface API (#10191),5
[PASS] PrintGraphIR Join attributes when print ir (#20),4
"[LLVM] Use llvm::ElementCount with LLVM 11+ when creating vectors (#5265)LLVM 11 added support for scalable vectors, and now the number ofelements in a vector is represented by a llvm::ElementCount class,not just a number.",1
[Bugfix] Fix caffe2 nnvm frontend (#2996),0
[Relay] [Error] Fix error in partial evaluator (#3693)* fix* lint,0
[Tutorial] fix the link thing for the pass tutorial (#1700),4
MPS conv (#822),5
"[Hexagon] Handle TCP server binding to unknown port (#10945)The server IP address will be obtained from the RPC tracker, but multipleservers must be distinguishable. To enable this, set a unique key whenstarting a server, and use that key when starting a session.",1
Constant input attr added to fully connected operation in TFLite frontend (#6228)* Constant input attr added to fully connected operationAn ability to handle constant input attr added to fully connected operationUnit tests amended.* renamed wrap_input to const_input* removed extra spaces,4
[TOPI][DARKNET]Yolo op added  (#1372),1
[SUBMODULE] update dmlc-core and Test (#387)* [SUBMODULE] update dmlc-core* [TEST] Not keep ROCM link but only verifies it,2
Update TVM VTA (VTA Chisel Wide memory interface) (#8973)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* Update to latest VTA,3
fix keras install (#8391),0
[AutoSchedule] Sparse dense tuning support with custom sketch rule (#7313)* Add sparse dense tuning tutorial* Add sparse input fusion* Update the dag to support output fusion* Update* Add task input to search_task* Update* Add search_inputs to measure* Lint fix* Lint fix* Update* Update* Update* Update* Add file save load support* Update* Update* Update* Remove add_task_inputs API* Update* Update* Update* Lint fix* Lint fix* Lint fix* Lint fix* Update* Add example ci_log* Update* retrigger ci* Update* Update* Update* Lint fix* Lint fix* Lint fix,0
Fix GatherND attribute registration (#8269),0
fixed path in installation/setup guide,1
"[TIR] Support tensorization using ldmatrix + MMA (#11355)* [TIR] Support tensorization using ldmatrix + MMAcommit 3218facf100b0dfc55715acfd1cee156764129baAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 14:04:56 2022 +0900    some clean upcommit 7a235b69dc2023b3098ed44d591edb63b20a8f4eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 13:55:11 2022 +0900    parameterize over storage scope in mma store intrincommit 827ea4c434c35607b241f8e0ae2efe3214ac2458Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 13:37:38 2022 +0900    properly handle floordiv/mod in codegencommit 42d4c6f42182c9fd79566c0955f99cc82abd5144Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 09:53:57 2022 +0900    update tuned factors for fp16commit 328d0aa36b2ea9ea1b051970d612bff82d2d20e6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 08:43:30 2022 +0900    all tests workingcommit 5e086cf5fd1404ac38f85c4bfbe692687b45a16cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 07:48:43 2022 +0900    add doc for mma_fill and mma_store intrincommit 4f945c4116b6d3bdc965ecb2be2229bb46dc11abAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 06:39:01 2022 +0900    remove testscommit df7708f7f67761d9c18f9564bc15abd50c12ac69Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:52:14 2022 +0900    unified testcommit 754c83eeb8510b31fb9652b089177f9b8e642ec0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:36:24 2022 +0900    clean up LowerWarpmemorycommit 178c3dcee7bfa17d5d93fec02aa858dc62151670Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:15:04 2022 +0900    Use IndexMapcommit 07fb58910338c62847fd902b37801d09b8c673b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 17:51:44 2022 +0900    remove 16x8x8 testcommit 2b05b5a5470ac221d559f31a31a8e2ff753b2414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 17:31:35 2022 +0900    generate mma fill/storecommit bf23fc50f0ffa99e875d9247ca66acec0c36677fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 12:23:30 2022 +0900    mma intrin generation with meta programmingcommit 5afb5f00afd642cb1e39872edc7965f476dcdcb7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 05:26:14 2022 +0900    ldmatrix intrin generation with meta programmingcommit fb62abb3424b88ec48c697e306e05889a3ac306fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 20:30:49 2022 +0900    minorcommit 5a80adce24e84d3ec6bf931b60cb9c730d243394Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:55:57 2022 +0900    revert some changecommit e599a55078ee75f2480a721098341812db58cf6fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:54:18 2022 +0900    remove obsolete filescommit 4b13b85ff91d0d592a7e0c01924e0b49b82f35a8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:51:21 2022 +0900    wipcommit 848de63455539e25cd0d43e5a65fd048636ef0f7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:44:29 2022 +0900    wipcommit b35bff97ed10c22559e2164eb7538db0f711ce7eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:31:18 2022 +0900    update parse error msgcommit ad9b053ef865b1f91f03d7b15ed7aae3420ee213Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:26:51 2022 +0900    fix for avoiding Buffer.vload(...) casecommit 54c686443e370edbfae860d0809b1b6182d26414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:59:55 2022 +0900    wipcommit 078060fe28d22f1db5f07b1c382dee438f02df60Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:57:34 2022 +0900    wipcommit 576f8415e65e0e8a8a7808885e219b3b53867950Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:52:15 2022 +0900    wipcommit 12a376ae2f44aa6660121e64e0358f2866624f7fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 17:54:58 2022 +0900    Squashed commit of the following:    commit 48eef4981d1a55aaf3b0ac935f2a10347cb1ac2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 17:40:48 2022 +0900        more comment    commit 8f67fc87038834e9f7e2c5cd3dfe61fabf442206    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 17:11:27 2022 +0900        update test    commit ad85036621c005b733763e67ceffae39c356ec99    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 16:54:01 2022 +0900        add test    commit 4a5dc3ffd5d0bb4a1700e57897c9e0f26e3d2a88    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 16:40:47 2022 +0900        [TVMScript] Support function call to help construct ASTcommit 76c1bcf0ade45d7433a0066236add8372b1cc547Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 16:30:07 2022 +0900    simplify iterator in layout transformcommit 936280324ea2c91429a6a85a1b8ee89c7b825928Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 11:31:39 2022 +0900    remove obsolet filescommit 2e119b422d72d726d5f2bd20fe48a1e62fcb0510Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 10:43:59 2022 +0900    calculate mma store dst index using inverse affine mapcommit 9489434ee52b546e2abb2ab28173eefd51525ba4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 10:01:12 2022 +0900    simplify storecommit 1adcb77b8bba8e5d91080fe6cbfc7add7f4365c2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 09:43:40 2022 +0900    simplified fillcommit 7b13c736d23e0eac94137aa918101d788e60d4f3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 09:22:17 2022 +0900    simplify intrin desc using index map functioncommit bcf212dda0f94c51f55c48921f61d92fd3b83777Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 07:16:42 2022 +0900    seems to workcommit dd8ccf9ec2e48100158152e5d4590d141424e2e2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 07:11:57 2022 +0900    poking with the parsercommit 596582cbfbd08ebe23ea71aaf7a447472415ccd1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 20:04:59 2022 +0900    16x8x32 4k trans workingcommit 273f89a8a6ac34f7c79147563922d34d44bffd08Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:52:13 2022 +0900    add 16x8x16 fp16 transcommit 8e2066cc4c6e86616bc9751324e63ba81a3b02afAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:32:37 2022 +0900    16x8x16 4k trans workingcommit c2d0744051733e94f840d4517bcee9ca5d444c75Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:25:52 2022 +0900    16x8x16 trans workingcommit c2e314cdda1c3a931781e51a863901ea178dffecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 16:19:32 2022 +0900    tuned int8 4k, 91 TOPScommit 94d9d965f19ff1a2ebdd342079ef420fb537b16aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 15:59:33 2022 +0900    int8 4k tune workingcommit 3ca8ca02593aff7540c9655aa831348246171752Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 08:43:57 2022 +0900    mma 16x8x32 int8 working with ldmatrix b workaroundcommit 54f1cb731d4b42a6cbc08baf144e74646400eef5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 18:23:27 2022 +0900    wipcommit 9d2844db602dc65af4dbd06a73fdd815f486b8b9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 16:38:53 2022 +0900    test tensorize without layout transformcommit 86ee6dabc801aeb8d6917bec6de97b42025dbdd1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 15:15:34 2022 +0900    int8 4k tensorize workscommit 39f9e32c9a64222c91daba2c32969b27207a31d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 12:44:39 2022 +0900    begin int8 4k tunecommit 6fa91e55b5ab2ba0f901d0d35be1b2fb3ab092b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 18:53:20 2022 +0900    try fix ldmatrix b for int8commit 7a962cddc4799fa3df0c0fdf3c056146d3f2cbdfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 18:28:34 2022 +0900    fixed warp_coeffcommit a0afb5698f307382147a38819e004a2db7f554b1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 12:20:01 2022 +0900    wipcommit f70ccd09b07d5325454ffdc39a7619ea84aa7e06Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 12:09:57 2022 +0900    int8 tensorize workingcommit 20321fa4674dabc78fe55b5e0e2876c35b245d21Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 07:06:22 2022 +0900    starting 16x8x32 int8commit 441fd193c59cdc436d87ab35896cbb8c779ddf35Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 05:50:46 2022 +0900    adding fp16 accum casecommit c9d40b69b1b57bfaddffba09ea07624ae90ee465Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 17:04:29 2022 +0900    clean upcommit 5b2d48635e762c77c824d1c259ac8bcbcc949421Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 16:38:19 2022 +0900    16x8x16 4k tune workingcommit c3cb170d85600d03da5c3f4cda03552208ca0b8cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 16:20:27 2022 +0900    tensoriz fixedcommit 68039b081efcdd6aea1d132940b3745f50164974Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:55:25 2022 +0900    begin 16x8x16 4k tunecommit ced5d8d980cc267d4735957c25cb60d71ae977d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:50:11 2022 +0900    16x8x16 workedcommit 3d2c90d77c1bb2df2193e9af6cbaa2bd927a26d8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:47:26 2022 +0900    fixcommit 403050b03ad6b4f0ee8d45088ffb324727bbae48Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:45:10 2022 +0900    add 16x8x16 testcommit 18e8d73661c99cd1c83021063b41a457afcb1638Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 06:50:32 2022 +0900    fixed mma store codegen for 16x8x16commit ec81250561195705122bccb9a2372f71de68121fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 04:25:25 2022 +0900    add 16x8x16 mma store codegencommit e08df2a62a4809bcd39782949283c16e7703aa5cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 03:47:47 2022 +0900    tensorized C_warp initcommit ae0678918929c1ceec73f2039467040c5bb7823bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 03:06:06 2022 +0900    mma store codegen workingcommit deb4d6646cc93d4cdb4f2560ce723bee4d86e144Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 10 19:22:57 2022 +0900    update lower warp memorycommit 71fe5fe465300705fa94f9544a2e1a5070de6e0dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 10 09:01:42 2022 +0900    tensorizing mma storecommit e80a1f148c47f2a3fac2363a733d8d4e2a2631d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 19:54:08 2022 +0900    clean upcommit a9640f4b7c3c9f22b87ca74a61003438dfd8f992Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 19:40:55 2022 +0900    add tunable 4k test, 36 TFLOPScommit b9f7eae7041d1a9b3e434c331c874e8347e89dc4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 18:01:08 2022 +0900    fixed bug in LowerWarpMemory index splitting for ldmatrixcommit 00df30823f874910ed1ec1f74718100311764234Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 07:58:17 2022 +0900    fixed missing reverse_compute_atcommit 93f9fe7e5f7ad16c8d0e6240c16c0281a0e97decAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 06:55:12 2022 +0900    add 4k testcommit 3689ef712aa4b282a4818fa2fa2e7e349c3a5eecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 06:54:09 2022 +0900    temp disable high dim base indices check in tensorizecommit 0c859c4f385ba0b6f9477b569b80cee80b5b7282Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 19:18:23 2022 +0900    clean upcommit f6aadbfcfbd73c1667a6de7aedc5894232b8e750Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 19:13:09 2022 +0900    Add 16x8x8 MMA + LDMatrix testcommit 4cf6b20c6ca415e967ab58d80e4a77c701ad7255Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 18:04:17 2022 +0900    testing 16x8x8 ldmatrix tensoriation* set measure_perf to False* add requires_gpu decorator in tests, always test build on non-ampere* skip cuda compile on old gpu",3
[Relay][TOPI] Remove redundant cuda kernels caused by fusion of less & logical or (#8618)* [Fix] Remove redundant cuda kernels caused by fusion of less_less_logical_or* put the check function in reduction.py and add UT* fix CI issue* fix CI* fix CICo-authored-by: saury <saury@saurydeMacBook-Pro.local>Co-authored-by: saury <lifei59@meituan.com>,0
Update tflite tutorial to use TFLite r1.13 schema (#3271),1
[CUDA] Enable int64 (#683)* [CUDA] Enable int64* [PYTHON] Fix rpc tutorial with opencl* OK* update,5
[MetaSchedule] Update scripts for subgraph tuning (#10501),5
"[REFACTOR] copy DMLC headers, move operator to example (#20)",1
[skip ci] Fix for ping_reviewers wait time (#10149)This was set to 1 day instead of 1 weekcc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>,1
Fix load subgraph from json (#1980),5
[CUTLASS] Profile only the largest-possible alignment by default (#10036)* introduce profile_all_alignments option* add profile_all_alignment option to API* wip* fixed dynamic case* black* update gen_gemm too* minor improvement* fix* all tests work* add doc* fixed for sm = 75 case* fix typo* remove unused import* profile_all -> find_first_valid* fix,0
[RUNTIME] Enable injection of some core runtime functions to avoid dynamic lookup (#260),1
Update the tarball deployment. (#5120),5
Register all operators' Python attributes in Python so they can be easily accessed from Python code (#3175),1
"CI Failure Fix (#1682)The recent changes in tutorial is with PR # https://github.com/dmlc/tvm/pull/1501 broken the link for downloading the weights file, leading to this CI failure.",0
Add Python function to get type index by class (#12393),1
"[ONNX]ReduceL1, ReduceL2, ReduceSumSquare, ReduceLogSum ops added (#5721)",1
fix CorrectLayout for softmax & log_softmax (#1401),2
[DOCKER][GOLANG] fix golang version. (#2023),0
[TEST][FLAKY] test_arith_solve_linear_inequality.py::test_multi_equal (#6014),3
[Bugfix][Minor] Avoid re-inference for MetaSchedule layout (#11842),5
[microTVM][RVM] Fix clock skew on virtualbox (#8395)* fix virtualbox clock skew* address comment* trigger,1
Fix post-merge conflict between #7785 and #7945. (#7982),5
[ROCM] Make sure all bit code files exist (#2323),2
[ETHOSN] Add support for default Ethos-N78 configuration. (#6982)Note: 'ETHOSN_VARIANT_CONFIG' must be set to test against Ethos-N78 and this adds support for one configuration of Ethos-N78 in TVM.,5
[NNVM][TOPI] Add FTVMCompute for matmul (#1239),1
[Relay][OP] Add cast op (#2319)* Add cast op* Rename dtype_cast to cast* Add additional safety check for String2TVMType* Add missing relay op docs,2
"[FIX,TOPI] Fix issue when running conv2d in autoscheduler (#9900)conv_nchwc scheduling was missing a check on the type on inputs beforeit tried to apply pragmas to them.",1
Update contributor guide (#1010),5
Followup from #9312 (Introduce call_lowered op) (#9491)* followups from #9312* remove unneeded moves,4
[TFLite] add support for float16 (#7093)* [TFLite] add support for float16* add testi case* add test case* add comments,1
Fix runtime error on osx (#1449),0
[DOCS] Fix installation from source link some text (#9238)Fix install from source link(pointed to matplotlib).Updated some wording.Move description of tlcpack to just a link so it can be kept from tlcpack side.,2
fix unit8 in _convert_dtype_value (#11924),0
Recover web emscripten support (#2379),1
Update hexagon max_concurrency to be at most equal to the number of HVX units available. (#12394),5
"[TOPI, Relay] ROI Pool operator (#2811)",1
[Relay][Pass] Fold constant tuple (#2201),4
Adding annotations for tir.allocate (#9168)* Adding annotation for tir.allocateThis commit is adding annotations for tir.allocatenode to be used as hints for future transformations.Change-Id: I02a3a875c38c3edd449385da5b741ef4958bb47f* Adding annotation for tir.allocate* adding tvmscript support* adding tir text printing supportChange-Id: Id0b6725b2e79c23f6b8ff192772f1ea4125a27c2,4
[TFLite Runtime] Add TFLite Runtime dependencies to CI CPU docker build (#5437),2
[RUNTIME][SDACCEL] Add support for multiple kernels (#1424),1
ignore 'training_mode' tag from onnx in batch_norm op (#9575)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>,5
add resize op converter (#4838),1
[QNN] unary op for quantized resize2d and test (#10589)* unary op for resize2d and test* renamed test,3
[Build] Add cmake options into libinfo (#6286)* [Build] Add cmake options into libinfo* Address comments from @tqchen* Add LLVM version to libinfo,5
"[Hexagon] Add trivial conv2d operator to Hexagon relay strategy (#8915)This is just to enable relay codegen for conv2d on Hexagon, not forperformance.",0
[MetaSchedule] Fix Task Extraction (#11954),4
Make compiler more robust (#378),1
[VM] class Executable does not export symbols to dll (#11963)* class Executable of VM exports symbols to dll* restart CICo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,2
[Relay][Pass] Fix lambda lift pass for recursive call (#4432)* Fix lambda lift* clean up* lint* fix* remove unused import,2
"[COREML]multiple output support, reshape, split ops added (#6296)* [COREML]multiple output support, reshape, split ops added* Review comments addressed",1
Fixed build with metal on MacOS with case-sensitive FS (#601),0
[Relay][Pass] CanonicalizeCast (#3280),4
[RUNTIME][OPENCL] show correct device type name (#1441),1
[Docker][QEMU] Update gpg server (#8319)* update* trigger,5
[CODEGEN] Fix alignment generation (#955),0
add -mattr=+neon for all arm cpu target (#1612),1
"[FRONTEND][Keras] Add support for tf.Keras networks in Relay Keras frontend (#4630)* Make Relay Keras frontend support networks created using   Tensorflow (1.13) Keras implementation (tf.Keras) * Modify Keras frontend tests to run from a class rather than a   function based script * Adjust Keras frontend tests to run with both 'Keras' and 'tf.Keras' * Change ""TestKeras.test_forward_merge"" to validate instances by   class name rather than instance type",5
Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.,1
[CodeGen] Add build config option disable_assert to control whether to generate assert (#4340),3
[Adreno] Fix winograd schedule to support prime shapes > 4 (#12157),1
"[OpenCL] Avoid SelectNode ambiguous overloading (#11488)* [OpenCL] Avoid SelectNode ambiguous overloading* Revert ""[OpenCL] Avoid SelectNode ambiguous overloading""This reverts commit 60f68d2e7f750a0f8e62536da7b3327d1f5f29c1.* [OpenCL] Avoid SelectNode ambiguous codegen",4
[KERAS] conv3d frontend operator support (#5080)* [KERAS]Conv3d support added* Keras conv3d testcase added,1
[AutoSchedule] Fix misusage of an already-moved object (#12239),4
[relay] use time_evaluator for measurement (#4191),1
"[REFACTOR][IR] Unify IntImm and UIntImm (#4706)* [REFACTOR][IR] Unify IntImm and UIntImmThis PR unifies UIntImm and IntImm to simplify the codebase.Unsigned integer constants will also be stored as IntImm.For uint constant that does not fit into int64(rare case), we introducedan intrinsic tvm_big_uint_imm to construct such intgers by itslower and higher 32bits.* [REFACTOR][IR] Remove UIntImm to use IntImm* rename big->large",1
[Vulkan] Workaround for zero size allocation (#7691),1
"Add sliding_window operator (#9816)* Add windows operator* remove TODO* Convert ICHECKs to CHECKs* Report errors using diagnostic context* Use more readable CHECKs* Remove example; move comments to test* Revert ""Remove example; move comments to test""This is a partial revert.This reverts commit c810c2db7637ce9537adc49d1016caddd5093d3a.* Add newline to fix Sphinx error* windows -> sliding_window* whitespace* fmt",0
"[Vulkan][Codegen] Added spvValidate check after vulkan shader generation (#8098)spvValidate found the bug that was fixed in #7966, along with a fewother issues on missing capability/extension declarations.  Now thatall unit tests pass with it enabled, would like to enable by default.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",5
[Pass] Remove dead code (#4177),4
Add the new logical operators to the doc. (#2761),2
Protect child process enumeration in AutoTVM (#7887),5
[IR] Rename attr_key in AttrStmt (#83),5
[COMMUNITY] Denise Kutnick -> Reviewer (#11778),3
Update tensorflow.py (#3632),5
[skip ci][ci] Skip flaky test_auto_scheduler_layout_rewrite_networks test (#10709)See #10707Co-authored-by: driazati <driazati@users.noreply.github.com>,1
enhance tir signed-unsigned cast (#8706),5
[CI] Update the ci-gpu to the lastest build with the new vulkansdk. (#5571),1
[AutoScheduler] Bug fix for layout rewrite CI error in i386 (#6830),0
[Rust] More Rust bindings for Attrs (#7082),5
[MetaSchedule][Minor] Fix Median Number (#12273)The previous median number calculation util function used the wrong index which could cause out of bound issue as mentioned in #12199 . This PR fixed this issue.,0
add topK FInferCorrectLayout attr (#11849),5
Make sure there is no tie in scores in NMS test (#8335),3
"[Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci",3
[Ansor][AutoTVM v2.0] Phase 1: Add pragma/storage_align/rfactor steps (#6141)* Add pragma/storage_align/rfactor step* Update* Update* Update UT* Update,5
[Hexagon] Refactor to keep HexagonBuffer private to the device api (#10910)* No longer return HexagonBuffer from device api.* fixup! No longer return HexagonBuffer from device api.,0
Clarify error message for missing libraries (#9710)Ran into this when importing tvm without a buildCo-authored-by: driazati <driazati@users.noreply.github.com>,1
Allow linking against MKLML (#2902),2
Change name matching to SSA Variable and Function matching,1
[Arith] Fix DetectIterMap floordiv when IterSum only contains base expr (#11887),0
add query for shared memory size (#1083),1
[CMAKE] Improve FindLLVM to handle llvm-prefix with space. (#6466)* [CMAKE] Improve FindLLVM to handle llvm-prefix with space.Useful for windows settings where llvm can sits in Program Files.Also updated the windows compiler to use clang.* Additional updates,5
[Simplifier] Add printing of SplitExprNode and SumExprNode (#9262),1
add dnnl (#5936),1
[ARITH] Refactor: Remove un-necessary usage of ComputeExpr (#3503),4
[Relay] Fix ad for conditional expression (#3453)* save* fix,0
"[Hexagon] Pass extra parameters to link_params via Map (#10830)There is no way to pass kwargs dictionary from C++ code, so the previousway never worked. Use TVM's Map instead, and pass the target architectureversion to the linker to use libraries specific to the architecture.",1
Add operator `isnan` (#3979)* add expr `isnan`* move to intrinsic* doc & add to topi* fix error from ci,0
[MetaSchedule] Post Processor: Rewrite Reduction Block (#10013)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,1
"[ci] Check more events before pinging reviewers (#10208)* [ci] Check more events before pinging reviewersThis was missing some events before (reviews without comments, PR updated from a draft -> ready for review) so these were being ignored when finding the latest event. This PR adds them and restructures the code a bit to make it more clear what is happening for each PR. This addresses some of the issues from #9983* fix testsCo-authored-by: driazati <driazati@users.noreply.github.com>",1
"[Relay][Parser] simplify build script, remove python 2 support  (#3419)* simplify build script, remove python 2 support* remove py2 file* update py3",5
[RPC] Lazily import micro when starting an RPC server (#6505)* [RPC] Lazily import micro when starting an RPC serverSince #6334 the RPC server cannot be started unless USE_MICRO is enabled. I've tracked this down to an import in `python/tvn/exec/rpc_server.py`: `from tvm import micro` in the top level list of imports. This will mean that we try to import micro when it's not been built. Fix this by lazily importing micro when initializing an rpc server with micro enabled.Change-Id: I8f22d81e215cfe4ac0662b0a99bdf02a3e91f90c* fix lintChange-Id: I8b78b678374bc82b3b66a7b3595ed4f1684e7d90,4
[TIR] Well-Formed Verifier (#12166)* tir_well_formed_verifier* fix typo* lint* fix testcase,3
Add while node support in TVMScript (#9004)* support while* update synr version,5
"[EXECUTOR] Save/Load Params (#242)* [EXECUTOR] Save/Load Params* [EXECUTOR] Improve Save/Load, fix Makefile* [EXECUTOR] Make save independent with executor",1
"[CI][Docker]Update Hexagon docker image to Ubuntu 20.04 (#10932)* fix permission* update pip versionAdd ONNX, TFLite, Tensorflow and update SDK version",5
[Hybrid Script]Fix some syntax errors (#8116)* [RUNTIME] Add clear() function in tvm::Map class* [Hybrid Script]Fix some syntactic errorsCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,0
[BYOC] Enable bfloat16 in DNNL BYOC (#11111)* refine the code style (#10112)* support more data types in oneDNN BYOC* consider dtype when query layout* support more translation of blocked layout* refine log for invalid layout transform* reset N and C for the weights* support multi-blocking in TransDims2Plain()* add tests for bf16 oneDNN BYOC* unregister 'round' OP in oneDNN BYOC* restore the criteria for fp32 tests* disable test_prune_dnnl_subgraph for bf16* fix typo in dnnl.py* delete tag::format_tag_last* delete 'is_weight' in layout2tag()* reuse dtype_dl2dnnl()* fix lint errors* change to WARNING for invalid laytout transform* skip bf16 tests if AVX512 is unavailable,3
[COMMUNITY] @d-smirnov -> reviewer (#7510),3
[TFLITE] Match TFLite shape for SSD custom op (#5473)This patch ensures that the output shape from TVM'sDetection_PostProcess is the same as TFLite's andexpands the unit test to confirm this.Change-Id: If5db95741533f131241dfebbaa7708dbd528fe70,5
[MetaSchedule] Handle 'warp_execution' in RewriteCooperativeFetch (#11955)Updated `RewriteCooperativeFetch` to handle 'warp_execution' annotation when the extend of `threadIdx.x` is not specified,0
[COMMUNITY] Update contributor list to reflect new guideline. (#2138),1
Initial commit,5
Quit and clean when TVM is interrupted (#3640),4
[Support] Add parallel_for support to run a loop in parallel (#6275),1
[NNVM][KERAS] Support multiple outputs (#1648),1
"[microTVM] Remove prepare_options() (#9644)This commit removes prepare_options() helper function because it's nowsuperfluous since the default values for the project options are now setcorrectly at the server side, hence there is no need to force setttingthem at the client side.Also prepare_options() can incorrectly set the default value of Zephyr's'west_cmd' option when that option is not passed by the user. Hence byremoving that function this issue is resolved too.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[Relay] change device annotation from post DFS to recursive (#6124)* change device annotation from post DFS to recursive* add testcast for recursive device propogation,3
"Zephyr: Add support for FVP (#12125)adds corstone300 FVP to the platforms supported by the zephyr. We use the Iris debugger to communicate with the emulator via semihosting, due to the FVP serial port's faulty behavior.also changes the generated micro-projects build system from make to ninja.Co-authored-by: Andrew Reusch <areusch@gmail.com>",1
fix output_shape in conv2d_nchw (#1613),0
Fix ci-qemu Arduino install dir (#8766),0
"[CI] Revert #10181 / #11399, use non-versioned scipy intersphinx link (#11411)Follow-up from https://github.com/apache/tvm/pull/10181 andhttps://github.com/apache/tvm/pull/11399.  Thank you to @rgommers for[pointing out](https://github.com/apache/tvm/pull/11399#issuecomment-1133874138)that the non-versioned link is stable and working.  The use ofthe versioned link was only introduced to work around the breakage ofthe stable link, so this reverts to the pre-breakage behavior.",4
[Relay][Docs] Add parser dependency install instructions. (#3277)* [Relay][Docs] Add parser dependency install instructions.See https://discuss.tvm.ai/t/trouble-enabling-antlr/2783.* Add a word.* Update since the parser will now be committed to the repo.* revert b/c adding the parser doesn't fix this,0
Fixed a g++ explicit constructor compatibility error for unordered_set. (#935)* Fixed a g++ explicit constructor compatibility error for unordered_set.* Change std::unordered_set<std::basic_string<char>>() tostd::unordered_set<std::string>().,1
[SCHEDULE][PASS] support storage_align of certain axis (#400)* [SCHEDULE][PASS] support storage_align of certain axis* fix lint,0
[Relay] Fix name conflict in PartialEval (#3402),5
[VTA][Chisel] rename USE_TSIM macro with USE_VTA64 and cleanup runtime (#3872),1
"[ETHOSN] Implement tanh operator (#10486)Adding compiler support for TANH operator, which is based onan underlying pattern matching scheme.One negative test is included as well.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>",3
"[Diagnostics][Relay][InferType] Refactor InferType to work on whole module, and use new diagnostics. (#6274)* Refactor the type checker to use diagnosticsAlthough this patch is very large and seemingly disjoint thefixes are required to get it working for the entire stack.I started with first changing InferType to use the diagnostics,these weren't yet in the pass manager so this required changesto module and module pass. InferType wasn't actually writtencorrectly as a pass requring refactoring there, then in orderto add spans to AST it required turning on AnnotateSpans whichin term required changes to the parser, and module to makeit possible to use the errors. These changes to parse and modulerequired changes to diagnostics and InferType. Althought seeminglydisconnected there are hidden cycles between the components whichrequire simultaneous change in order to remove the old errorreporting.A huge change due to this patch is that the module no longerimplicitly type checks functions which are added.* Apply suggestions from code reviewCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* Apply suggestions from code reviewCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Clean up parser* CR feedback* Apply Bobs suggestions* Fix up Python interface for diagnostics* Fix test_ir_parser and formatting* Fix cpplint* Fix lint* Fix format* More lint* Fix format* Kill dead doc comment* Fix documentation comment* Rebase fixups* Add docs for type.h* Fix parser.cc* Fix unittests* Fix black* Skip previously typechecked functions* fix ACL* Fix numerous issues* Add repr method* Fix issue with Pytest, I am ready to cry* Fix the rest of tests* Kill dead code* Fix dignostic tests* Fix more tests* fix more tests (#11)* Fix diagnostic.py deinit bug* Fix deinit issue* Format* Tweak disabling of override* Format* Fix BYOC* Fix TensorArray stuff* Fix PyTorch* Format* FormatCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>",1
[CI][Contrib] Add Vitis-AI docker installation (#6342)* [CI][Contrib] Add Vitis-AI docker installation* rename ubuntu_install_vai_packages.sh to ubuntu_install_vitis_ai_packages_ci.sh* Add Dockerfile.demo_vitis_ai and environment scripts* Add comment to docker/bash describing Xilinx Vitis-AI specific setupCo-authored-by: anilm (generated by with_the_same_user script) <anilm@xhdabidk40.xilinx.com>Co-authored-by: Anil Martha <anil.martha@xilinx.com>Co-authored-by: Jorn Tuyls <jornt@xilinx.com>,1
[RELAY]take and transpose comp and schd (#2135),5
[FIX] Pass the attributes of master node (#304),4
[VTA] quant support for alu-only op (#6191),1
[AutoScheduler] Add custom build function (#7185)* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* cheduler] Add custom build function* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>,1
"Improved MLF to contain workspace info (#7938)* Improved MLF to contain workspace infoAdded functionality to calculate workspace, io and constantmemory required by each primfunc and main function. Moreover,the workspace information required by each primfunc and mainis reported in metadata.json in the Model Library Format(MLF).- added functionality to record tir and relay primfuncs- added tests for model_library_format changesChange-Id: Ib4a8b787345aa35f8a1645e8a648fad84de37bce* Improved MLF to contain workspace info* disable AoT for now* addressing commentsChange-Id: I5f041ec461b02dac6ea9c96ea50eb400d55eef53* Improved MLF to contain workspace info* addressed comments* added aot executor supportChange-Id: I9b54a7939d8ccb3c6ce0454f0fe62866ac66eb5c* Improved MLF to contain workspace info* removed redundant utils.pyChange-Id: I256dd88fab31a595bf9509bd1c4ab59b0c145b1e* Improved MLF to contain workspace info* removed redundant ffi apiChange-Id: I9ad6795aa839edfdfd05b902d4531fb0a20e894d",4
[CI] Update CPU and GPU image (#11369),5
update tutorial to new TARGET as micro_dev is no more (#6262)Signed-off-by: Tom Gall <tom.gall@linaro.org>,1
"Better host handling in CompilationConfig & debug printing (#9460)(This is a bit of a grab bag in preparation for #9326which I'm trying to minimize)While switching the device planner to use SEScopes I had a lotof trouble with Target's not matching up.- If no explicit host target is given but the given  TargetMap has targets with hosts, try to use those  to establish the host_target.- Make sure both the 'legacy' TargetMap representation  and the newer representation agree to pointer equality on  their targets.- Make sure the Interpreter uses the target from CompilationConfig  since it's been normalized.To debug the above:- When in pretty printing with show_meta_data_ false give as much  detail on SEScopes, Targets and call attributes as possible.  That needed some rework in the relay_text_printer.cc.- Ditto for critical 'target' attribute on PrimFuncs.- Also added a Target::ToDebugString so I could see the  host fields along with everything else since a lot of problems  were caused by a mismatch of 'the same' Target with and without  a host. (Tried using that for the ReprPrinter but broken unit  tests.)Note that the codebase assumes Targets are compared by ObjectPtrEquality,yet CheckAndUpdateHostConsistency (I count 65 call sites) changes the targets.Ultimately CompilationConfig or it's ultimate replacement should ensure we mungetargets only once at the 'main' entry points.",1
improve text summary (#1655),1
[CMSIS-NN] Moved test_cnn_small to the latest version (#9962),3
Rename gpu to cuda in java/rust/typescript (#8036)* rename gpu to cuda in java/rust/typescript* fix rpc test to call cuda,3
[NNVM][KERAS] Fixed padding in pooling (#1635),1
[ci] xfail failing ethosu codegen tests (#12508)This adds a testing utility so we can mark parameter combinations asxfail without having to manually match each parameter from the name intothe code. The param strings here come directly from CI logs as inhttps://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12389/5/pipelineCo-authored-by: driazati <driazati@users.noreply.github.com>,1
fix type bug about topi test unitest (#12285),3
[PyTorch] Fix all_any_common with no default input (#12395)* fix all_any_common with no default input* work around* better naming,1
[Runtime] Change default alignment to 64 bytes. (#12586)* Change default alignment to 64 bits.* Run dlpack test a few times.* Update alignment in tests.* Revert mma alignment change.* Change default printing of buffer.* Change crt runtime default allocation.,1
SparseReshape Op (#7477)* SparseReshape Inital Code* Done* Format* Add empty tests* Formatting* SanityCheck* formatting documentation* Documentation* Only Enable CPU* Add support for CUDA* Stuff* Add Dynamic Support* Parallelize GPU Impl* Documentation* Documentation* Import* Import* Remove unnecessary code* PR Comments* Schedules* Tests* Dtypes* Black* Parallelize CPU* CI errorCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
remove self-include in runtime/container.h (#8117),1
[microNPU] Add NHWC -> NHCWB16 layout transformation pass (#9561)Adds a layout optimization pass that modifies the ifm/ofm layoutof an operation to NHCWB16 where possible. This can occur when theproducer or consumer of a tensor is also an NPU operator.,1
"Arm(R) Ethos(TM)-U NPU Depthwise2d operator support (#9209)* Arm(R) Ethos(TM)-U NPU Depthwise2d operator supportThis commit adds support for Depthwise2d primitive operator throughoutthe TVM stack including Relay legalization pass, operator definition,TE, TIR passes and translation into the command stream.Change-Id: If82b85f5d3b23cd214fe38babd724451bf95ef5b* Change depthwise2d to depthwise_conv2dAnd respond to other review comments.Change-Id: I58a9f28723750970d386b4d0ba62fa399c5c6181* Make a line shorter and add a commentChange-Id: Idf4c078bf65e7ed31fe82a92bf334295a82b6ead* Change the order of importsChange-Id: Ic6c77af30a5b9cb68dcc0c173b95490965359481* Whitespace changeChange-Id: I7318bd8cfa5985b33fc7d020cc19057cc9498197",4
[TensorIR][M1c] Lower and build TensorIR (#8044),5
[VTA] Parameterization and bug fix in TensorLoad module (#3841),0
[BuildModule] Fix AlterLayout Pass (#3155),4
thread local handle for rocblas (#7851),0
"[RUNTIME] Enable auto conversion from str to runtime::String in PackedFunc, move dtype related handling to data_type.h (#5251)",5
Sync up,5
[AMP][Pass][Typing] Add faster type inference (#9735)* reuse checked types* analogous subgraph* brr go fast* clean up src logs* clean up PR more* more clean up* more documenetation* clean up* formatting* rename fast --> local* more ocmments* jostle ci* type inference* change comment for SameTypedSubgraphExtractor* get_analogous_expression -> GetAnalogousExpression* comment in GetAnaalogousExpression* add comment* replace infer tests* jostle,3
Jenkinsfile changes for #7333. (#7388),4
[TOPI][OP] Use Thrust sort for argsort and topk (#5097)* [TOPI][OP] Use Thrust sort for argsort and topkThe current GPU sort implementation (odd-even transposition sort) is too slowwhen the number of elements is large.  This PR introduces Thrust implementationof sort which is much faster.Note that this change requires CMake 3.8 or later since we have to use nvcc tocompile a thrust code.* cmake: make CUDA optional* allow .cu file to be into the repository* pylint fix and cleanup* require cmake 3.8 only when thrust is enabled* fix nvcc compiler error when passing -pthread* add missing include* add USE_THRUST option in config.cmake* retrigger CI* retrigger CI,5
[FIX] Fix android projects (#7764)src/runtime/logging.cc was missing from the runtime files list,2
[Bugfix][Arith] Fix TryFuseIter (#10427),1
Better gemm support for cublas and cpu (#1967),1
Improve non_max_suppression and get_valid_counts for CPU (#3305)* Improve non_max_suppression for CPU* Improve get_valid_counts* Minor change* Skip some unnecessary computes,4
[ETHOSN] Use partition_for_ function when running tests (#11945)Keeps the tests in parity with the partition_for_ function so anychanges are reflected in the tests.Change-Id: I580cc381d382c777484e8251c609867a69da8e67,3
[TEAM] Laurawly -> committer (#2141),5
[DOCKER] Cleanup docker image (#50),2
[Relay][DOC] Add tutorial for adding an operator to Relay (#1778),1
"[TVMScript] Support T.buffer_decl using data pointer from Let/Allocate (#10099)* [TVMScript] Added unit tests demonstrating desired functionality* [TVMScript] Implemented parsing of T.Ptr[...]These can be generated when exporting to TVMscript, but were notparsable after being generated.* [TVMScript] Updated buffer_var printingLetStmt and AllocateNode can both be used to generate handles that areused in Buffer objects.  In these cases, the Buffer declarations mustgo after the handle declaration, not in the function header.* Moved printing of var and buffer_decl into separate statements.* Updated following @shingjan's review comments.",5
[CUDA]batch_matmul tensorcore schedule (#7146)* add batch_matmul_tensorcore* add bmm cublas autotune* add bmm tests* out_shape for bmm_tensorcore* fix comments* code format* add todos for tensorcore datatype checking* fix lint* fix have_tensorcore* add dtype check for batch_matmul_tensorcore,1
"[RELAY] Re-wrote the Graph Partitioner to support multiple outputs (#5143)* [RELAY] Re-wrote the Graph Partitioner to support multiple outputsInput : A Relay module that have functions with disjoint annotated regions        using compiler_begin and compiler_end. There could be multiple outputs.Output : A Relay module with global functions for such disjoint annotated regions         with calls inserted at the respective locationDependencies : AnnotatedRegionSet Utility class.Methodology :      1) The AnnotatedRegionSet utility class is able to construct a collection of         nodes that are bound by a give annotation -- here we use compiler_begin         and compiler_end      2) Initially, for each function in the module AnnotatedRegionSets are populated.      3) Then, Vistor pass is traversed until a compiler_end node is encountered         that belongs to a ""region"".      4) When the first compiler_end of a given annotated region is found, a function is         formed and inserted.         a) if the region has multiple outputs, a Tuple node (capturing all outputs)            is returned.      5) Thereafter, if we encounter an another output of the same annotated region,         it is important to note that the function is already formed. Therefore, it will         lookup the function and add a TupleGetItemNode is inserted.          a) We will use the location index of ""rets"" of each ""Region"" of AnnotatedRegionSet             as TupleGetItemNode index.      6) Therefore, functions will be created for all annotated regions. The name for each         global function is created using ""Region"" id and the compiler name.Change-Id: I1372f02a845b6d3da03b561763e03a378dca263c* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *removed the expected use-case as we are taking broken-down PR approach    *code style fixes    *some trivial one liners* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *fixed an implicit copy to a move* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *code style changes for comments    *renamed test case multiple outputs --> mixed single multiple outputs        Since the existing test case checks for both single and multiple        output scenarios    *added a new test case with conv2d + batch_norm    *some var name changes in the test* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs*rebased",1
[Relay to Onnx Conversion test] Fixed relay.var initialization (#8322)* Fixed issue in bias variable initialization by passing arg name (shape),4
fix codegenc (#4597),0
[Relay][AlterOp] Minor refactor. (#4064),4
Bump prebuilt-image version in demo dockerfile (#4770),2
[BUILD] Make the core library compatible with msvc 13 (#167),1
[TOPI] GPU scatter_add using atomic (#7044)* use atomic add for faster 1d scatter add* update tests* run black* more pylint fix* remove fp64 bintcount testCo-authored-by: masa <masa@pop-os.localdomain>,3
[VitisAI] Update Vitis AI integration to 1.4 release (#8815)* Update Vitis AI to 1.4 release* Parameterize Vitis AI codegen tests* Update Dockerfile.demo_vitis_ai,2
"[TIR] Updated python docstring and parameter names for AllocateConst (#10602)The previous docstring referred to the non-existent `data` parameter,and passed the argument named `condition` in Python as the parameter`data_or_idx` in C++.  This commit matches the Python names anddocumentation to those in C++.",2
[TIR][SPIR-V] Fix computing clz on int64 input for vulkan (#7913)* Fix computing clz on int64 input for vulkan* rebase fixCo-authored-by: masa <masa@pop-os.localdomain>,0
[Relay][Dyn] Dynamic TopK Op (#6008)* add dynamic topk op* add topk to dynamic_to_static pass* fix TF test* fix pylint,0
"Eliminate some compiler warnings for microNPU demo app (#10549)* Eliminates all ""control reaches end of non-void function"" warnings * Eliminates all ""unused variable"" warnings * Eliminates some ""implicit declaration of function"" warningsChange-Id: Icba390f3e821e42f37066a1e4522c26d50a92380",4
[TIR] Introduce tir::PrimFunc (#5070)This PR introduces tir::PrimFunc which will be used as the TIR functioncontainer in the unified IR.Also streamlined the function attributes a bit further.- All common attributes are under tvm::attr- TIR specific attributes are under tvm::tir::attr and comes with a tir prefix- Use stl_style for attributes for now,1
added tests for quantized tflite sin operator (#9478)* added tests for quantized tflite sin operator* removing unnecessary rsqrt code_block* resolving linting error,0
hide psutil (#4013),5
[Object Detection] Gluoncv SSD support on CPU (#2353),1
[CONTRIB/NNPACK] Add NNPack Fully Connected Functions (#199)* Add NNPack Fully Connected Inference* Add NNPack fully_connected_output* Fix lint* Fix,0
[REFACTOR] Remove legacy nnvm folder (#10821)nnvm was the first generation IR that was maintained by TVM before the community moved to a newer generation.The community keeps it for a few more releases(v0.7 and v0.8).This PR removes the folder. The source code can still be found in past releases.,4
[COMMUNITY] @kparzysz-quic -> committer (#6290),3
[Frontend][TFLite] Add support for NonMaxSuppressionV5 op (#12003)* add nms_v5 op for TFLite* add a test for the TFLite nms_v5 op,3
[TFLite] Support PRelu (#4298),1
[SCHEDULE] Refactor bound inference logic (#41),2
export VirtualMachine for Windows (#11947),5
Add warning about nnpack installing googletest (#5185),3
"Refine LSTMBlockCell to support dynamic rnn (#5963)1. Refine conversion of `LSTMBlockCell`       1) Make its output follows definition in TensorFlow       2) Avoid introducing variables which doesn't match any placeholder nodes in TensorFlow graph    2. About change in test_forward_ptb       States nodes of LSTMBlockCell in this PB file  are actually Constant node.       TF can feed data to those Constant nodes but relay can't do that, so current conversion of LSTMBockCell introduces extra variables to solve this issue.       But this causes that relay IR doesn't match original TF graph. This PR solves this issue by convert those states node into placeholders.",0
[RPC] Allow RPCServer to run without decorator (#257),1
[unittests] Skip import of tvm.micro if micro-TVM was not enabled (#9301),0
Overload get() function for `Optional` type. (#9748)* upd* simplify* upd* fix* upd* fix docstring,2
[LINT] Fix pylint (#501),0
[microTVM] Fix AOT/ARMv7m tests on physical devices. (#9364)* init* test on hardware* moved to testing.py* add option to set workspace size* changed model* fix memory issue* fix tests* conv2d test* fix simd test* format* fix exception* refactor workspace helper function* address comments* revert to fix the bug* address comment,1
[SCHEDULE] Fix boundary check (#2126)* Fix boundary check* Add unittest,3
[VTA][TSIM] update app example (#3343)* add initial support to cycle counter to accelerator* remove prints from c* add event counter support to chisel tsim example* make it more readable* use a config class* update driver* add individual Makefile to chisel* add rule for installing vta package* add makefile for verilog backend* update drivers* update* rename* update README* put default sim back* set counter to zero,1
"Tf2 test fixups (#5391)* Fix oversight in importing tf.compat.v1 as tf.* Actually disable test for lstm in TF2.1Since the testing framework actually uses pytest, the versioncheck needs to be moved.",4
syntax fix (#3765),0
"[VTA] Performance optimize, remove unnecessary contigious memory use. (#4246)* [VTA] Performance optimize, remove unnecessary contigious memory use.Issue:Uop maintain a cache vector to copy uop data into contigious DRAM memory forFPGA/Simulator use, but this cache vector not get clear after FPGA/Simulatorcore run, in Resnet18 case, if we printf the cache size in UopQueue::ReadBarrierfunction, we can saw such cache size keep increase, this would causeno use data copy and unnecessary contigous DRAM memory malloc.Analysis:This issue caused by not clear cache_ vector when douop_queue_.Reset().Solution:Override BaseQueue Reset function in UopQueue and add cache_ clearlogic.* address review comments, remove spacing.",4
checkin tensor,5
fix a typo in topi key (#6502),2
Fix links and formatting in langref (#2440),2
save (#6338),5
[RPC][BUGFIX] Fix remote device sync (#5538),0
"[TIR] Refactor MakePackedAPI to target dependent stage. (#5326)Previously MakePackedAPI was in the target independent stage,but never the less requires the device_type information that will bebinded at a later target dependent stage.The previous implementation was due to the limitation of LoweredFuncwhich can not carry buffer_map info(so they have to be lowered right away).This is no longer the case after the unified IR refactor.This PR migrates MakePackedAPI to a target dependent stageand removes the un-necessary BindDevice pass.",4
[RELAY] Support recursive call syntax (#2352),1
[TIR][Transform]Block scope hoisting added (#6238)* Block scope hoisting added* lowering flow added with 2 variants* Fake commit to trigger ci with pass default enabled* CI Failure resolved* Optimize for if var list iteration* More test case added* Fake commit to disable failed test cases* Pass default value restored* [1] Review comment handled* [2] Review comments handled,0
"[Runtime][PipelineExecutor] Add Pipeline Executor Interface (#10010)Adding interfaces into Pipeline Executor to ""run"", ""stop"",""set input"",and ""get input"" from the pipeline executor,In this patch, we also implemented the ""BackendRuntime"" structure towrap the graph runtime interface in order to support  pipeline executorinterface and implement data copy method. This method is used totransfer data between two backend runtimes.",1
"[Relay][Frontend][ONNX] Broadcast condition, x, and y for Where op (#4774)* ONNX frontend broadcast condition* fix* fix styleCo-authored-by: Jon Soifer <jonso@microsoft.com>",0
"[Vulkan] Check at codegen if the shader is within shared memory limits. (#8746)Previously, shaders that do not respect device limits for sharedmemory could result in segfaults that occur during the call to`vkCreateComputePipelines`.",1
[Hexagon] Remove HexagonBuffer external constructor and support (#10978),1
[FIX] Fix for a specific case when loop partitioning with indivisble (#4243)factors and resulting nested loop is broken.This is due to the fact that we are creating zero extent loops whichare fixed afterwards. However unroll pass breaks due to the zero extentloop.,4
"[TFLITE][FRONTEND]Reduce_any op parsing support (#4926)* [TFLITE][FRONTEND]Reduce_any op parsing support* Testcase check added to run in tf version above 1.14.0 & review comments* Review comment, checked updated to 1.15",5
[Executor] Debug Graph Executor - Dumping output tensors include topographical ordering (#9557),0
[Fix] Fix precision issue in FFI converting `int/float` to `PrimExpr` (#12417),0
add generic home path (#1435),1
[COMMUNITY] An Wang -> Reviewer (#12517),3
[REFACTOR][PY] tvm._ffi (#4813)* [REFACTOR][PY] tvm._ffi- Remove from __future__ import absolute_import in the related files as they are no longer needed if the code only runs in python3- Remove reverse dependency of _ctypes _cython to object_generic.- function.py -> packed_func.py- Function -> PackedFunc- all registry related logics goes to tvm._ffi.registry- Use absolute references for FFI related calls.  - tvm._ffi.register_object  - tvm._ffi.register_func  - tvm._ffi.get_global_func* Move get global func to the ffi side,1
[RELAY][BACKEND] Enable PlanMemory in the graph runtime. (#2120),1
[ARM][Strategy] Fix is_int8_hw_support check function (#11193)* Fix hw schedule condition* add warning messages to unoptimized schedules,2
Make AutoScheduler handling of errors during measure consistent with AutoTvm (#6909)* Match ansor handling of 'too many errors' during measure to that of autoTVM and match default level of logging* Set correct level of verbosity for debug modeCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>* Lint* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>Co-authored-by: Taylor Zowtuk 84152750 <taylor.zowtuk@huawei.com>,0
[REFACTOR] util => utils for consistency in the project. (#6684)* [REFACTOR] util => utils for consistency in the project.* Update CMake,1
[microTVM][Cortex-R5] Add zephyr cortex-r5 board to Zephyr  (#8519)* cortex r5 added* add aot demo,1
[Relay] Nullable Type Alpha Equality (#1906),5
[RPC][IOS] Add random to ios_rpc (#8935)random_fill is used in measure.py,1
Add assert message (#11665)Change-Id: I88f19c7105cce048d2f52d50450a551fb12162dc,4
[skip ci][microTVM] Update Arduino RVM name and box version (#11743)* update* Fix version* readme* Update README.md,2
[QNN] Use Int16 upcast in Fallback Conv2D. Fix test names. (#4329),3
"[Runtime][Pipeline Executor] Add the map logic of global input and subgraph input. (#9751)* [Runtime][Pipeline Executor] Add the map logic of global input and subgraph input.User can use ""global input name"" to feed input data for pipeline runtime. The name like""data_a"" will be mapped into a input interface of subgraph. In this PR, wecreate the related logic to do the following things. 1. building the input map configuration 2. in runtime c++ module, parseing the input connection configuration then    creating related data structure to record the said connection map. 3. providing the function to return the map information for verification.* address review comments.* addres review comments.* address review comments.",1
[ci] Add local test re-run info (#11051),5
"[BYOC][ACL] Add support for dense (fully connected) layer (#6254)* [BYOC][ACL] Add support for dense (fully connected) layerThis patch adds the ability to offload dense (or fully connected) operators to ACL.For fp32 a single dense layer can be offloaded, or the composite variant: nn.dense, nn.bias_add? (ACL does not currently offer fused activation).For uint8: qnn.dense, nn.bias_add?, qnn.requantizeChange-Id: I83ea00b2aa6bdc5d9ef5cd6d54bbf981e523bd14* Don't offload dense layer with unsupported datatypeChange-Id: I856eb2298499fdf22c172ba7f85d21033d3cc920",5
Add SGX runtime (#963),1
"Add resource_handle to TVM_DLL_EXPORT_TYPED_FUNC. (#7338)* In #5921, resource_handle was added as a parameter to   TVMBackendPackedCFunc, which is the typedef for functions called by   LibraryModule's function lookup. * It appears TVM_DLL_EXPORT_TYPED_FUNC was overlooked in that PR,   although there don't seem to be any runtime affects known so   far. However, making this definition proper to avoid any compiler   warnings/debug tool problems. * See also https://discuss.tvm.apache.org/t/rfc-misra-c-changes-for-rpc-support/7098/5",4
[CONDA] Revamp conda recipe. (#6732)* [CONDA] Revamp conda recipe.- Combines two packages into a single recipe.- Enable windows build.- Better packaging hash tag (use git string).* Address comment,1
Update TVM ci-cpu docker image to v.079 (#9454)* Requested in #9296 * Validated at   https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/169/pipeline/,2
"[Hexagon] Pass stack size to simulator (#11046)Increase the default stack size to 256kB, since this is the minimummain thread stack size in QuRT on simulator.",5
"[QNN] Lookup operations for hard to implement operators (#10053)* initial tanh impl* smalls error* support uint and int lookup into tables* reinterpret cast, working tanh tests* refactor relay func creation* basic casting tests* explicitly say do not handle multi-channel lookups* add example funcs* fix silent fail* fix some bugs with floating point funcs not working* add TODO* add tood* canonicalizations* refactor integer lookup ops into own folder* fq2i stuff* clean up existing tests* flesh out todo* more tests* test on keeping shape good* lookup table fix* replace canonicalization for rsqrt* remove canonicalization of rsqrt* add asf headers* topi tests* gather supports unsigned integer tests* fix things* move to legalization* jostle ci* linting* use take instead of gather* remove gather changes* undo changes* undo changes* undo changes* move thing in range* initial tanh impl* smalls error* support uint and int lookup into tables* reinterpret cast, working tanh tests* refactor relay func creation* basic casting tests* explicitly say do not handle multi-channel lookups* add example funcs* fix silent fail* fix some bugs with floating point funcs not working* add TODO* add tood* canonicalizations* refactor integer lookup ops into own folder* fq2i stuff* clean up existing tests* flesh out todo* more tests* test on keeping shape good* lookup table fix* replace canonicalization for rsqrt* remove canonicalization of rsqrt* add asf headers* gather supports unsigned integer tests* fix things* move to legalization* jostle ci* linting* use take instead of gather* remove gather changes* undo changes* undo changes* undo changes* move thing in range* lint* remove unneeded line* jostleCo-authored-by: andrewzhaoluo (generated by with_the_same_user script) <andrewzhaoluo@system76-pc.localdomain>",5
Avoid using heavy API to query single attribution (#3179),1
[Relay][Backend] Fix interpreter argument conversion for tuples. (#3349)* Support taking a tuple as an argument* Add test,3
[DOCS] Mention incubating in readme (#4401),2
Add __float2half_rn for cuda compute capabilities less than 53 (#4489)* Fix* clean up,4
[CI] docker images build script cmd line args optional (#7776)* allow COMMAND to be empty when building a container* clarfiy the difference between build.sh and bash.sh* fix typo and highlight command as option in usage snippet,2
"[TIR] Affine utility support iter lowerbound and diagnostics (#9699)* Enable freevars, iter lowerbound and diagnostics in affine utility* fix lint issues and compare bug* update to use iter shift instead of itermark min for lowerbound* add testcase of fused iters sum with multiple lowerbounds* add more affine check testcases, fix bug for single iter and duplicate constraints on iter* add a newline to comment* forbidden predicate unmatchCo-authored-by: baoxinqi <wrongtest@intellif.com>",3
[CI] Update GPU image (#10992),5
[FIX] Only allow autoscheduler layout rewritting in conv2d_nhwc (#10522)Autoscheduler cannot handle layout rewritting for grouped convolutionsand non-nhwc layouts. Previously layout rewriting was enabled for allconvolutions causing errors where autoscheduler generated too largelayouts like `64N4n1n1n1H1W68C1n1h1w2c2n`. Autoscheduler is now onlyenabled on non-grouped conv2d_nhwc.,0
Updated runtime to run under FreeBSD. (#6600)* Updated runtime to run under FreeBSD.setenv CXX to proper binary - c++ or g++9 for FreeBSD 12.0.* Update python/tvm/runtime/module.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Update python/tvm/rpc/server.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Changed to use os.environ.get* Fixed format.* Yet another lint fix.Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
print import_llvm ir in tensorize tutorial (#2064),2
[RELAY] Inline scalar compute (#2335),5
Add AttrStmt,1
[tutorial] Relay pass infra tutorial (#4083)* Add pass manager tutorial* fix some examples* retrigger ci* Update tutorials/dev/relay_pass_infra.pyCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Add ToANormalForm link,2
"[Hexagon] Add support for Hexagon v69, deprecate v60 and v62 (#10623)* [Hexagon] Add support for Hexagon v69, deprecate v60 and v62- The tvm.target.hexagon() function now accepts v69.- Hexagon SDK 4.x does not support v60 and v62, so they have been removed.* Restart CICo-authored-by: Krzysztof Parzyszek <kparzysz@invalid>",4
Add logical_not shape registration. (#7820)Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,2
"[REFACTOR][IR] Initialize Unified IR Expr Data Structure (#4673)This PR moves a few base types from relay and low-level Expr into the ir sub-folder.These classes will serve as a common type system across the stack.Rationale:- PrimExpr for low-level expressions- RelayExpr for advanced features, including Function definition.- Introduce BaseFunc to host all functions, including future PrimFunc(low-level expr functions, subject to discussion).This is a minimum change we can do to unify the classes into a common hierarchy.The main data structure that are variant specific will still be kept in the sub-namespaces.We only include classes that is needed to allow a common Module class.- BaseFunc- GlobalVar- Type definition part of ADTWe will only need the BaseFunc and their checked_type to decide the calling conventionacross the function variants.",1
fix mali topi for python3 (#789),0
"[LINT] recover lint error, add asf header check (#3117)",1
[Relay][OP] Register topi schedule for Relay fast_exp and fast_tanh (#5131)* register for fast_exp and fast_tanh* Add unit test for fast math* Add unit test for op fast math* Add unit test for op fast math* Add unit tests to guard registering topi schedule for Relay fast_exp and fast_tanh* Fix ident* Fix the indent* Add fast_tanh in the test_fastmath of topi tests,3
[MetaSchedule] Fix autoinline for single const consumer block (#12668)fix autoinline and add test,3
added error checking to loading symbol json (#2301),5
Run cpplint in quiet mode. (#10292),1
[COMMUNITY] @junrushao1994 -> PMC (#8450),3
[CODEGEN] Make codegen registerable (#193)* [CODEGEN] Make codegen registerable* fix llvm disbaled,0
[Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.,2
[CI] turn on keras frontend test (#309)* [CI] turn on keras frontend test* fix* using tensorflow cpu version,1
GraphTuner supports relay.module as input (#3434),1
[CI] Update docker image ci_lint to obtain Python 3.6 from ppa:deadsnakes/ppa (#4505) (#4506),2
"[VTA] Support TLPP in function simulator. (#3555)* [VTA] Support TLPP in function simulator.Issue:currently vta function simulator just doing serialized instructionexecution, the dependency logic of runtime ISA which use for tasklevel pipe line parallelism can not get verified by function simulator.Solution:make the simulator driver to be multiple thread and support TLPP.Benefit:TLPP support VTA function simulator would make VTA logic testing/debug/change more easy.replace boost lockfree queueadd configure control for simulator tlpp enable or disable.change code tyle into google style.Wrap queue read/write and sync logic to make function call more simple.Add some comments.Remove MT logic, change into Single thread mode.address review comments.code style change to match google code style and add comments.add cmake macro to enable/disable simulator tlpp logic.submodule update.correct file name mentioned in comments.* remove USE_VTA_FSIM_TLPP.",1
[DOCS][REFACTOR] Reorganize the docs. (#6146)- Move most toctree to `:hiden:` so there can be top-level categorizations in the navigation bar.- Move frontend guide into design and developer guides- Move get started tutorials into its separate folder.Co-authored-by: Chris Hoge <chris@hogepodge.com>Co-authored-by: Chris Hoge <chris@hogepodge.com>,1
Fix GLOBAL_SCOPE Shallow copy bug (#9718),0
Change new concat (#11800)* changed x86/concat to use lists of ints instead of te.tensor.Tensor for loop extents and array offsets* typos fixed* removed unused import* fixed micro model test* fixed micro model test,3
Update license file to note libbacktrace (#9579)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>,1
[AutoScheduler] Fix task extraction (#6965)* [AutoScheduler] Fix task extraction* fix* fix* trigger CI,0
[RPC] Update build support for cross compiling apps/cpp_rpc with OpenCL (#6229)* Standardize support for building and cross compiling apps/cpp_rpc.* Add cmake coverage for building the C++ RPC server binary  and update documentation.* Add support for linking against custom OpenCL SDK employing  a custom find_opencl macro. This can be useful when cross  compiling with a custom OpenCL device driver.* Update OpenCL related documentation.* Add embedded linux build instructions to apps/cpp_rpc/README.md andensure pthread is linked against when OS=Linux is defined. Removeoutdated apps/cpp_rpc/Makefile.,2
[Relay][Legalize] Legalize conv2d_transpose for NHWC (#4399),5
"[NODE][IR] Introduce StructuralEqual Infra for the unified IR. (#5154)* [NODE][IR] Introduce StructuralEqual Infra for the Unified IR.This PR introduces a new way to handle structural equalityfor both TIR and relay nodes in an extensive way.- Each object can now register an optional SEqualReduce function, which  describes how to reduce its structural equality to another instance  into equality of the children.- Optionally, the object can choose to allow remapping of vars(e.g. function parameters)  by calling DefEqual- We implemented a non-recursive structural equality checker that  recursively traverses the objects and does the structural equality checking.This PR also fixes a few potential problems in previous relay's AlphaEqual.- In particular, the new structural equality relation will be communicative.- It is can be dangerous to use same_as relation to quickly check equality,  demonstrated by the following case. (%x, %y) are shared vars between two functions.- function0: fn (%x, %y) { %x + %y }- function1: fn (%y, %x) { %x + %y }The new structural equal is intented to supersede AlphaEqual and AttrsEqual.Follow-up PRs should be performed to redirect the existing usages, and removesthe corresponding implementation.* Update the rule to distinguish between graph node and non-graph nodes.* Refactor the test cases to use structural equal.* address comments* Mark more relay::Expr as graph node, fix a testcase issue(was bug that was not caught by previous alpha equal)* Remove unrelated comment* Fix file comment* Address review comment* Relax condition to fit flaky case",1
[CODEGEN] enable static handle cache (#723),0
[FQ2I] fix unary op output affine type in fq2i (#12224)* fix unary op output affine type in fq2i* better names* add option to force to positive values for ops that are undefined on negative values,1
RelayTextPrinter is now non-recursive. ExpandDataflow refactored (#7817)* RelayTextPrinter is now non-recursive. ExpandDataflow refactoredRelayTextPrinter is now non-recursive to allow printing largergraphs. ExpandDataflow is generalised to have separate node expander.Change-Id: Id5a3a470fbc8b90822502fbc8d24d534df1ea355* requested changesChange-Id: Iac69766428d5b9783279cb02a57064fd82842001* unit test addedChange-Id: Id20ae72f9f5f8dd92d4d182360b28156c035e667,4
[ci] Remove TensorCore node name (#11048),4
[microTVM][zephyr] Add support for host-driven AoT execution on zephyr (#11650)* - add support for host-driven AoT execution on zephyr;- add initial version of reference counting to prevent python code from inadvertently freeing tensors during garbage collection;- add support for numerical indices to host-drive AoT get_input();- add two initial tests for host-driven AoT execution on zephyr;- rename existing zephyr AoT exec. test;* address PR feedback* increase stack size to accommodate qemu_riscv64 stack usage,5
"[Rust] Improve the error reporting in build.rs files by using anyhow. (#6401)* Improve build.rs error handling.Instead of just unwrapping use Result on main function, and use anyhow to add error context.* Remove NDArray and Python changes* Format* Fix build.rs* Apply suggestions from code reviewCo-authored-by: Greg Hale <ImAlsoGreg@gmail.com>* Format* Fix build.rsCo-authored-by: Greg Hale <ImAlsoGreg@gmail.com>",0
[RELAY][RUNTIME] Refactor interpreter and graph_runtime into consistent interface. (#2042),1
[AutoScheduler] Fix task scheduler after 8478 (#8984),0
[ONNX] Add support for GatherElements conversion (#6446)* support onnx GatherElements* remove print* run blackCo-authored-by: masa <masa@pop-os.localdomain>,1
fix pydoc format (#1975),2
Avoid use of MemoryInfo when undefined in StorageRewrite (#11254)* Check if the requested memory info is defined before using it.* Address review comment to add warning when MemoryInfofor scope is undefined.,5
[TUTORIAL] Resnet-18 end to end tutorial example (#55),5
"[DOCS][REFACTOR] Organize Design and Architectures (#6097)* [DOCS][REFACTOR] Design and ArchitecturesThis PR refactors the design and architecture docs.Previously this part of documentation was quite unstructured, and lacks a globalview of the overall architecture.This PR takes a stab in resolving the problem- Provide a guided overview of the current TVM's overall design- Categorize the specific docs into architecture components or How tos.* Apply suggestions from code reviewCo-authored-by: Jared Roesch <roeschinc@gmail.com>* Apply suggestions from code reviewCo-authored-by: Jared Roesch <roeschinc@gmail.com>* Update per comment* More updates per feedbacks* clarify external codegen* Update per commentsCo-authored-by: Jared Roesch <roeschinc@gmail.com>",5
[PASS] Fix reuse small buffer in storage rewrite (#1012),1
[CI] Move ci-cpu to use llvm-11 (#7541)* [CI] Move ci-cpu to use llvm-11* Fix the testcase of x86 codegen by relax the register names.,3
[Runtime][Object] expose runtime::String to Python (#5212)* expose runtime::String to Python* retrigger ci,1
"[BYOC][TensorRT] Fixes for explicit batch mode, Support reduce to scalar, Support split op (#7967)",1
fix typo: anchor windoes should be anchor windows (#5706),2
"Fix function number datatype from char to uint16_t (#11365)* Fix function number datatype from char to uint16_trewrite the modified part to pass lint checkUse 2 bytes for func num in fun_registryFix errors in linterAdd the declaration of the helper functionsset 2 bytes for func num in func_registry test unitspass num_func by valueThis commit change the datatype of the number of the function from 1 Byte to 2 Bytes.Besides, I use some helper functions to access the number of function and the first function name.* Fix aot_executor_module to unbreak CI.* Fix GraphExecutorModule.* Remove graph_json_to_c_func_registry. * No longer needed and not called anywhere. * Superseded by emitting the FuncRegistry directly in codegen.Co-authored-by: 嚴中璟 <a1245967@gmail.com>",1
"[runtime] AOTExecutor implementation and c target code-generator (#10283)* Add memory pools to Metadata classes.* Move ShapeToJSON to utils.* Track returned TensorType from AOTExecutorCodegen.* Support calling Relay functions with Tuple.* Expand supported TIR calling conventions to work with C++ runtime.* Rename MetadataModule to ConstLoaderModule.* Add runtime AOT executor module.* Add AOT code-generation.* Add a runtime Module to mux between .text Metadata and live Metadata.* Move launch_param to namespace* Add test of c++ AOT.* Fix incongruity between kTvmRuntimeCrt constant* Expand ExecutorCodegenMetadata to include AOT runtime metadata.* commit cpp test* Make Metadata compile under C.* Ignore ephemeral metadata_module export_model_library_format. * This module does not need to be exported, since it is merely a C++   wrapper around get_c_metadata, and get_metadata is not used in C.* address manupa, kparszsyc, masahi comments.* further address comments* clang and python format* Fix broken test* Address lingering comments from masahi, kparszyzc",1
"[UnitTests][Ethos-N] Mark unit tests as requiring Ethos-N (#8873)- Adds the decorator `tvm.testing.requires_ethosn`- Marks all tests in `tests/python/contrib/test_ethosn` as requiring  ethosn instead of directly checking `ethosn_available()`.  This way,  they show up as skipped rather than passing.- Marks test_compile_tflite_module_with_external_codegen as requiring  ethosn.",1
[METAL] Switch to manual ref counting (#114),5
"[VTA] Fix FSIM Compile Error. (#6070)Issue:when set vta target into ""sim"", vta compile would get fail andshow error message ""fatal error: vta/driver.h: No such file or directory"".Solution:set VTA_HW include path correctly.",1
[QNN] Add support for per channel weight scale in dense op (#4880)* add test case for per channel dense* add unit arg in tflite frontend* update qnn legalize test* fix output dim index,0
[AlterLayout] Strided slice layout transform fix (disallow NCHW4c -> NCHW etc properly) (#9245)* prohibit propagating through packed to unpacked layout* add test,3
"[CUTLASS] More robust support for pattern matching and alignment (#9698)* bug fix in im2col encoding* skip legalize when batch size is dynamic* add sm75 kernels to sm80 profilings* add dtype and layout check in parttern match* use align1 kernel for unusual channel cases (IC = 3 etc)* test IC=3 convolution* fixed check functions for fused cases, run infer type before mergecomposite* check align on N dim* add comment on IC == 3 case* lint fix* do not offload depthwise conv2d* lint* trigger CI",0
"[TIR] Simplify final indices from transform_layout (#10761)The final indices returned from transform_layout when applied on a`te.compute` are not simplified. Thus the returned index ranges areharder understandEg: When applying NHWC to NCHWc transform_layout```python   iter_vars = s[B].transform_layout(lambda n,h,w,c: [n, c//4, h, w, c%4])   print(iter_vars)```iter_vars before simplification:```python[iter_var(axis0, range(min=0, ext=((w - 1) + 1))), iter_var(axis1,range(min=0, ext=(floordiv(((z_div*4) - 1), 4) + 1))), iter_var(axis2,range(min=0, ext=((x - 1) + 1))), iter_var(axis3, range(min=0, ext=((y -1) + 1))), iter_var(axis4, range(min=0, ext=4))]```iter_vars after simplification:```python[iter_var(axis0, range(min=0, ext=w)), iter_var(axis1, range(min=0, ext=z_div)), iter_var(axis2, range(min=0, ext=x)), iter_var(axis3, range(min=0, ext=y)), iter_var(axis4, range(min=0, ext=4))]```",5
[AutoScheduler] Add task.desc for its function name (#7794),1
[CI] Add setup-pytest-env.sh call to task_demo_microtvm.sh (#12260)task_demo_microtvm.sh fails when running on a container because it can't find TVM. Adding a regular `source ...task_demo_microtvm.sh` will make the proper test environment setup.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>,1
"[ARITH] DeduceBound (#40)* [PYTHON/API] Add compare and logic build-in op for Expr* remove 'and', 'or'* add deducer* [WIP] bound_deducer.cc* move IntervalSet and StrideSet into int_set_internal.h* add multiple failure for VariablePathFinder, add EvalSign* consider round in deduce, add success flag* remove Visit_(Div)* add comment, update HalideIR* expose intset to python* check the sign of every expr* set return type as ExprSignType* fine tune* add min & max python api for interval set* support for conditional expr* refactor test* add checker for BoundDeducer* add python check test* fix* fix* change range to interval; remove converter* remove converter declaration* remove int_set_internal.h",1
Upgrade Windows build to use windows-2019 runner (#10585)* Switch to windows-2019 build.* Use Visual Studio 2019 generator.,1
"[DOC] Improve ""Getting Started with TVM"" tutorials and fix warnings (#8221)* improve src/README.md* fix intro* fix more warnings* improve docs* update* update* update* update overview image",5
"[CODEGEN/EXEC] CUDA, NVRTC pipeline complete (#27)* [CODEGEN] CUDA/OPENCL pipeline complete* Hide TVMType by str in frontend",5
Fix typo in comment about kill() (#10863)Fix typo in comment about kill method in PopenWorker class used to killchild processes created by the worker.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,1
[Relay] Keep fixed dim when unifying dynamic shape (#5795),0
Added LLVM TargetIRAnalysis pass (#2386),4
Defined a common base class for TensorComputeOp and ComputeOp (#2587)* Defined a common base class for TensorComputeOp and ComputeOp* Made changes requested by @ZihengJiang* added a testcase to assert that `tensorize` does not have any effect on TensorComputeOp ops.,3
"Don't explicitly link libgcc.a into libtvm_runtime.so on Android (#10052)Setting Android toolchain via CMAKE_TOOLCHAIN_FILE also causes necessaryflags to be added. Also, newer versions of the Android NDK no longer shiplibgcc.a, so this takes care of that as well.",1
[Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI,1
Update README.md,2
"[PATCH] [ring_buffer.h] Improve commentary for RingBuffer (#5518)bytes_available refers to the number of bytes used in the ringbuffer. At the same time, fix a typo.",2
"Change function constructors to WithFields (#9690)* Change function constructors to WithFieldsGet rid of std::moves, they were causing problems* Fix bad rebase* flaky* try to trigger ci* try again",1
[Relay]Frontend][Onnx] Remove pop that interferes with nested loops. (#7781)* Remove popping that interferes with nested loops.* Only check user inputs in the outer-most graph scope.* Fix style.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,0
modify schedule_depthwise_conv2d_nchw (#350),5
"[ONNX] [Relay] Update unique operator to match ONNX output (1D only) (#8099)* Fix topi test case and docs (tvm was returning inverse_indices and claiming it was indices)* Passes on CPU, fix unique op test* more changes* mtrying to fix optional outputs in onnx importer* TupleGetItem is being passed a stringgit add python/tvm/relay/frontend/onnx.py debugging print statements* Unique is passing onnx unit tests* fix indices* change comment* fix return of compute unique* black* fix lint* Some stray .asnumpy()s got through my merge, fix)* fix lint* revert changed .numpys* missed a few* fix more .asnumpy* fix black* Fix op level 3 test* remove prints* Fix pytorch and tf importers* black* fix lint* fix indentation* fix topi test",3
[TOPI][ARM CPU] fuse bias to depthwise conv2d (#1631),1
[FRONTEND][TENSORFLOW] Support Unstack and Split (#2105),1
[RELAY] Add softmax (#1841),1
[MXNET]DepthToSpace & SpaceToDepth Operator (#5408),1
[ETHOSN] Add support for transpose convolution (#12674)Adds support for offloading transpose convolution with an optional biasto the NPU.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>Co-authored-by: Leo Blonk <leo.blonk@arm.com>,1
[Docs] Fix an irrelevant sentence in relay.reverse (#10331)It seems the sentence is from relay.repeat() and not related torelay.reverse().,0
add aten::randn (#11994),1
[RELAY] Add occurs check before unification (#2012),1
[ONNX] Make input shape immutable (#7844)Co-authored-by: Yanming Wang <yanmwang@amazon.com>,1
[TOPI] PReLU Support (#1008),1
"[CUBLAS] Add cuBLAS as a Relay partitioning target (BYOC) (#10820)* [CUBLAS] Add cuBLAS as a Relay partitioning target (BYOC)This PR adds a partitioning pass for cuBLAS so thatsupported Relay patterns can be offloaded to cuBLAS.This initial commit only adds offloading supportfor nn.matmul.Although cuBLAS is already enabled in TVM by usingstrategy selection in TE, by exposing it explicitlyas a Relay partitioning target we can more preciselydescribe how to execute a model in Relay. This isdesirable particularly in the Collage effort toimprove multi-backend graph partitioning.* Refactor to remove boilerplate",4
[TOPI] fix weight layout in conv2d_transpose (#616),0
[Codegen] fix bug on LLVM 10.0 (#4480),0
Bump the CMake version in ubuntu_install_cmake_source.sh to 3.14.7. (#9424)* This is required in platforms we need to build xgboost from source * Fixes #9414,0
fix warning (#1041),2
Extend tune_relay_x86 tutorial to measure default and kernel level tune (#8794),5
"[MetaSchedule] Resolve dependencies between header files (#11604)* [MetaSchedule] Resolve dependencies between header filesAfter PR11590 TVM stopped compiling with clang-14 and libc++. The problemswere caused by incomplete types used in contexts where complete types wererequired. To resolve this, some code had to be moved into .cc files. Alsothe MeasureCandidate classes needed to be added to their own include files(or otherwise there would be a circular dependency between headers).All headers from the meta_schedule directory were updated to include alltheir dependencies (forward declarations were left where appropriate).* Fix a typo: PySpaceGeneratorCode -> PySpaceGeneratorNode",2
"change a, n, l to A, N, L (#8027)",4
"BUG: Fix core-dump in crt graph_executor.c (#9155)The JSON loader was missing a BeginArray for the ""device_index"" attribute.That's a 1 line fix. The rest is to add a unit test and make it build.The crt JSON handling is perhaps not our finest code.",5
[Tutorial] Cache the test data in tutorial (#2923),5
Homepage URL to tvm.ai,5
[PASS] Export simplify and equal to python (#14)* [PASS] Export simplify and equal to python* fix naming convention,0
[PYTORCH] Support max_pool2d_with_indices (#5549)* Use real output name instead of node_name* Add pytorch max_pool2d_with_indices converter.* Add test for maxpool2d with indices* Add explicit assert for single output* Only consume output (not indices) from max pool 2d with indices* undo change,4
add a shape function and dynamic test for round (#7324),3
add gfx906 bc (#3808),1
[RUNTIME] Unify load params interface (#7559),2
[microTVM] [Fix] reboot include for Zephyr version >=2.6.0 (#11790)* Fix reboot include for Zephyr version >=2.6.0,0
[ARITH] Introduce base-class IRMutatorWithAnalyzer for scope dependent analysis (#3969),5
[Torch] Fix conv2d conversion for group conv (group > 1 but != in channels) (#5132)* Fix conv2d conversion for group conv* add more comment for clarification,1
Support colons inside of TVMC input shape name arguments (#9080),1
"[BYOC] Allow custom codegens to register their own constant updater (#6697)* [BYOC] Allow custom codegens to register their own constant updaterCurrently, all codegens using BYOC must make use of the defaultConstantUpdater pass. However, certain codegens, like Ethos-N,don't want to store any constants in metadata module. Thisprovides an interface (via a global) to register a customconstant updating method and assigns a 'null' updater for theEthos-N codegen.Change-Id: Ibd71d3091f992362eeede5d894eedb373b2dbc8f* Fix to use symbol in const nameChange-Id: I0ade81af9002d413c5b20a50488018e8cd8d8bad* Remove ;Change-Id: I61967bc4997efb87f87b49dad7e0a660c536ef35* Remove ccompiler constant updaterChange-Id: Iea9ee0f689683512fa114afeadeccb7fc9048e4f* Unregister updater after testChange-Id: I8009940bb2ac949f2c3f0d72c943a5b74afd6954* Create UpdateConstants utility functionChange-Id: I83c8c6f92cfe3be3a7e811e98a4eec17590186ff",4
[Hexagon] One more fix for concurrency count (#5589),0
fix typo (#8484)* fix typo* Fixed typos in documentation,2
[skip ci] Fail silently in ping_reviewers GitHub actions. (#10173),0
Update gitignore,5
"Revert ""[Relay][Frontend][ONNX] Fix reshape precompute, and type error (#3230)"" (#3385)This reverts commit df6957a5ea49806b3073bbb81e339ae379cbbb1c.",4
"Refactor bilinear and neighbour implementation in Tensorflow frontend (#4504)There is significant duplication between functions.Spotted while looking to move the tensorflow and tflite framework support to later than1.13.1. The tests barf around resize_nearest_neighbour not ignoring the attribute'helpful_pixel_centers'.That upgrade is a separate discussion while this can go inindependently.Thanks,Ramana",5
fix T.Ptr[T.void] for packed api roundtrip (#12118),0
A few typo fixes in the uTVM design doc. (#7291),2
[Relay][OP] Add reverse_reshape (#2503)* Enable reverse in reshape* Fix lint and typo* Put reverse_reshape into a separate op* Fix pylint,0
Add relay.where (#1869),1
[PYTHON][FFI] Skip numpy.ascontiguousarray if C_CONTIGUOUS == True (#9073),5
"Update C++ standard to C++17 (#12337)* Update C++ standard to C++17LLVM has switched to C++17 in its development branch. Follow suit to beable to compile LLVM headers.* Clang 8.0+ also supports -faligned-new* Make make verbose* Use CMAKE_OSX_DEPLOYMENT_TARGET to set minimum macOS version (10.12)* Update llvmdev version in conda to >= 11Something seems to be wrong with the llvmdev 10.0.0 packages, since theLLVM unit test fails on Windows. It works fine when LLVM 10 is compiledfrom sources.",1
"[Runtime][Vulkan] Add RGP support to TVM for vulkan device (#10953)RGP(Raedon GPU Profiler) is a tool used to analyze the applicationsrun on AMD GPU. RGP captures the data based on VKPresent and providesthe hardware specific information. Allowing the developer to optimizethe application. To add RGP support to TVM, debug labels ""AmdFrameBegin""and ""AmdFrameEnd"" need to be inserted into the vulkan queue.These Labelshelps the RGP tool to understand the start|end of frame when no presentis available. Thus enabling the RGP tool to capture and analyze the data.At runtime, set the envirnoment variable ""TVM_USE_AMD_RGP=1"" to startinserting the Debug Labels into the vulkan queue.Signed-off-by: Wilkin Chau <Wing-Ki.ChauWilkin@amd.com>Signed-off-by: Anurag Kumar Vulisha <AnuragKumar.Vulisha@amd.com>Co-authored-by: avulisha <avulisha@amd.com>",0
Update community.rst,5
[BugFix] fix nvptx not supported by device_enabled error (#9585)* [BugFix] fix nvptx not supported by device_enabled errorSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [BugFix] shared.dyn support for codegen_nvptxSigned-off-by: ZQPei <ziqiangpei@foxmail.com>,1
[microNPU] Add support for LeakyReLU (#10127)* [microNPU] Add support for LeakyReLUAdds support for offloading an int8 Leaky ReLU activation functionto the NPU by legalizing to a LUT.Change-Id: I63dd5b16a1a2a747b11f15a5b8124810e2ebf491* refactor LeakyReLUParams to inherit from LutActivationParamsChange-Id: I35b59200b16a7eff1915f771ab6b5d9181d4f3ab,4
"[Hexagon] Asynchronous DMA support (#12411)Adds adds asynchronous DMA support through the Hexagon User DMA engine with unit tests to validate basic functionality. Asynchronous DMA support here means the ability to ""kick off"" asynchronously a number of DMAs using the Copy API and then to Poll for or Wait on a number of ""in flight"" (not done) DMAs. Enables future testing and development for asynchronous memory copy on Hexagon. For now, Hexagon DMA support remains synchronous in nature through existing hexagon_user_dma_1d_sync interface which uses asynchronous capable HexagonUserDMA class in a synchronous way --- calling Copy and Wait back to back for each request.* use ring buffer to store DMA descriptors* add RingBuffer class; used by HexUserDMA to store descriptors* add test to overflow the HexagonUserDMA ring buffer",1
[DOC] Add intro to 'comm_reducer' in tutorial; fix doc (#108)* [DOC] Add intro to 'comm_reducer' in tutorial; fix doc* Fix* Fix,0
[Fix] Add missing expr visitor for any (#6082),1
[Frontend][MXNet] Fix default value for is_ascend in topk (#7568)* Use correct default value of False for is_ascend* Add unit test for default topk is_ascend value,3
Fix apt install (#11781),0
[FLAKY] A small bug fix on the CmakeLists (#8826),1
[RELAY]prelu op support (#2016),1
[CI] Add more guidelines about local setup (#6848),1
[NNVM][DARKNET]Yolo and Upsample frontend support (#1501)* Yolo and Upsample frontend support* Lint fix* Mac support added* Code clean and trigger CI,4
[PASS] Basic storage flatten (#13),4
[F2QI] Fix a rounding error on AvgPool when input and output affine scales differ (#12577)cc @sfvaroglu @AndrewZhaoLuo,0
Disable Rust CI (#7793),5
[CI] Update ci-cpu to latest (#4121),3
"[HARDWARE, TEST] Fixed hardware generation flow (#34)",0
[PRNG] Add check to PRNG to make sure that unsigned integer arithmetic is wrapping (#7287)* [PRNG] Add check to PRNG to make sure that unsigned integer arithmetic is wrapping* Add threefry_test_wrapping: a manual test for wrapping unsigned arithmetic.* fix test to actually run on the target* formatting* lint,1
te_compiler_cache: reduce name length without loss of information (#9787)This can mitigate issues described in #8953 without increasing memory requirements,1
[PyTorch][Relay] Add aten::cross_entropy_loss (#11935)* add cross entropy loss* fix cross entropy args* fix typo* add class indices* fix CI* fix naming* fix typo,2
change line seperator of tensorflow/test_forward from CRLF to LF (#1405),3
[microTVM] Remove microTVM RVM version suffix (#11629),0
[Relay][Frontend][Onnx] MaxUnpool Operator (#7036)* Added maxunpool test.* MaxUnpool implemented and tested.* Lint fix.* Add explicit output shape in tests.,3
"[FIX,STORAGE REWRITE] Rewrite buffers in let statements (#12349)Storage rewrite was missing a visitor for let statements so buffersadded in them would still refer to the pre-rewritten version. This errorwas originally noticed when using `global.vtcm` buffers which getchanged to let statements by LowerVtcmAlloc.Implementing the test for this change also required adding support forvectorized datatypes to tvmscript. The solution included is a littlehacky and involes adding the datatypes to the `global()` table of eachmodule they need to be defined in.",5
[PyTorch]Add PyTorchTVM: compile torchscript to tvm and export as pytorch_op (#8777)* add pt_op* add compile api* perf: support set_output_zero_copy* fix: cpu device_id mismatch* fix: pt_class test script* refactor: unify namespace to tvm.contrib.torch* add ASF header* build: set pt tvmdsoop default off* build: remove unset_log_macros.h* refactor: change header order* refactor: fix python code format* style: resolve pylint issues* style: add blank line* style: fix pylint invalid_name* trigger CI* test: add more test scripts* style: add empty lines* test: update test for trace tvm module* style: fix linting issues* style: remove single quote* style: disable pylint invalid-name* trigger CI* trigger CICo-authored-by: kongroo <imjcqt@gmail.com>,4
Add a python tutorial of deploying tvm module with tvm runtime only (#4094),1
[SPIR-V] Fix pushconstants offset calculation for 32 bit values (#7620)* Fix push constant offset for 32 bit value* add test* remove unused function from test* add dynamic cumsum test* skip if vulkan is not enabled* replace dynamic cumsum test with dynamic argsort for nowCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,3
[PyTorch] add var_mean support (#10233)* [PyTorch] add var_mean support* update mean_variance,5
"[UX][TVMSciprt] Use HTML formatter in notebook environments (#12240)Previously we use ANSI color sequences to highlight TVM script. In jupyter notebook environments, such color sequence will be recoginized and translated to corresponding HTML to display things. This works fine for most notebook environments (including the jupyter notebook and the VS Code plugin). Recently, thanks to @tqchen, we found that Google Colab does not well support ansi color sequence for 24-bit colors (`JupyterLight` and `VSCDark`) that all its displayed colors are unexpectedly black/gray/white. To also bring highlighting in Colab, in this PR, we directly render the highlighted code with HTML when a notebook environment is detected.",1
Address Christopher's comments from #8788 (#9197)We don't need the Optional<IRModule> on ToANormalForm and friends.,1
add .clang-format (#2395),1
[Meta Schedule][M3a] SpaceGenerator  (#9079)* Add meta shedule space generator.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Clean up.* Minor fix.* Move utils.h.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,1
[FastMath] Add fast_softmax support in fast_math pass (#8138)* Add fast_softmax support in fast_math pass* Lintfix* Update,5
[ONNX] Normalize axes for Slice (#9517),5
[FRONTEND][TENSORFLOW] Use input shapes directly instead of 1-element lists (#2242),1
[Quantization] Fix annotation for multiply op (#4458)* fix mul rewrite* register Realize Rewrite for global avg pool and add test* remove unnecessary check* improve the test case,3
[DOCKER] mark tmp as temp (#1327),2
"[DOCKER,CI] Add PAPI to docker images (#8016)",2
[CI] Update docker to latest (#6708),3
remove minimum 32-bit restriction (#621)Change minimum 32-bit restriction for floating point types to 8-bit.This change is to enable reduced precision types that may use vector operations underneath the hood (cases #lanes > 1 such as half4).,1
[AutoScheduler] Fix deserization of workload registry entry (#8662),1
[hexagon][testing] refactor benchmark-table code (#11400)Generalize the benchmark-table code to support arbitraryindependent values. This supports future changes to the benchmarkcode.,4
[CMake] Corrected warning message about USE_GRAPH_EXECUTOR_DEBUG (#9006)Warning message about deprecated cmake flag `USE_GRAPH_EXECUTOR_DEBUG`referred to `USE_GRAPH_EXECUTOR` instead.,1
[FRONTEND][COREML]MultiplyLayerParams L2NormalizeLayerParams and UpsampleLayerParams support … (#1511),1
"[TFLite] QNN support for TFLite 2.1.0 quantized models (#5848)* [TFLite] TFLite 2.x parser quantization support.* Address comments. Fix a bug for depthwise conv* Added tests for relu, conv, quantize. Address comments.* Using web-data. Minor refactoring.* Removing TF hub package* Trigger CI.* Handle TFLite input layer naming.* Addressing reviews.* Retrigger CI.",1
[TOP] fix weight layout in conv2d_transpose (#220)* update tvm* [TOP] fix weight layout in conv2d_transpose,0
[Torch] Run torch JIT pass lower_all_tuples before conversion. (#10186)* [Torch] Run torch JIT pass lower_all_tuples before conversion.* [Torch] Input containing tuples will disable lower_all_tuples.Co-authored-by: wenyuchi.wyc <wenyuchi.wyc@alibaba-inc.com>,5
"Make batch matrix multiplication on GPU tunable (#5752)This is primarily aimed at the AMD GPU backend and done as partof a project for AMD, but should work for all users of the GPUschedule.",1
[Frontend][PyTorch] support for quantized conv_transpose2d op (#9133)* [Frontend][PyTorch] support for quantized conv_transpose2d opPyTorch uses the same underlying function to pack andunpack the params for conv2d and conv_transpose2d ops.This change adds support for quantized conv_transpose2d opby reusing the ConvPackedParam and adding theoutput_padding param to it.This output_padding param will remain unused in case of conv2d.Also added test for above with specific condition fortorch v1.7.1 and below.* fix after merging main,0
[Refactor] move vm.py under runtime and adt to runtime.container.py (#4855),1
Add the acc16 intrinsic support (#3081),1
[RELAY][FRONTEND]Onnx to relay frontend (#2302),5
"Fix conv2_gemm after target structure update (#6037)After target structure changed in this RFC:https://discuss.tvm.ai/t/rfc-tvm-target-specification/6844/42The conv2d optimizations was broken for the following reasons:- ""target"" is now called mtriple (this changes how we test if the  architecture is AArch64)- when we invoke ""clang.create_llvm"" we still need to specify the  ""--target"" option (set to aarch64-linux-gnu)This submission reverts those changesChange-Id: I04c597b91ca5800ddf4471255e2a358c60bc048e",4
Update deploy.md (#219)* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.,1
[microNPU] Remove spurious prints and improve documentation (#11247),2
"[TIR][StorageRewrite] Allow in-place buffer reuse of non-flat memory (#12655)* [TIR][StorageRewrite] Allow in-place buffer reuse of non-flat memoryPreviously, shared buffer use was entirely disabled for non-flatmemory, since the existing checks for shared memory assume flat 1-dspaces.  This was enforced in `FindAlloc` and validated in`PrepareNewAlloc`.  The validation in `PrepareNewAlloc` could trigger,if the buffer sharing was due to an in-place operation, and notthrough the `FindAlloc` function.In-place operations do not require N-d packing, nor do they introduceambiguity in how different code generators may interpret non-flatphysical indices.  Therefore, this commit relaxes the validation in`PrepareNewAlloc`, allowing buffer reuse of non-flat buffers forin-place operations.* Update new StorageRewrite with correct allocate/buffer_decl usage",1
[RELAY][PASS] Check Positiveness in FoldScaleAxis (#2220),4
[TIR][Schedule] Annotate allows array as annotaton value (#9920)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,5
"[VTA] Refactor to increase platform coverage (Ultra96 etc.) (#3496)* hardware refactor for increased FPGA coverage, small optimizations* fix header* cleaning up parameters that won't be needed for now* streamlining makefile, and simplifying tcl scripts* moving parameter derivation into pkg_config.py, keeping tcl scripts lightweight* refactoring tcl script to avoid global variables* deriving AXI signals in pkg_config.py* unifying address map definition for hardware and software drivers* single channel design for ultra96 to simplify build* enable alu by default, no mul opcode for now* hardware fix* new bitstream; vta version* avoid error when env variable is not set* ultra96 cleanup* further cleaning up tcl script for bitstream generation* preliminary rpc server support on ultra96* rpc server tracker scripts* ultra96 ldflag* ultra96 support* ultra96 support* cleanup line* cmake support for ultra96* simplify memory instantiation* cleaning up IP parameter initialization* fix queue instantiation* 2019.1 transition* fix macro def* removing bus width from config* cleanup* fix* turning off testing for now* cleanup ultra96 ps insantiation* minor refactor* adding comments* upgrading to tophub v0.6* model used in TVM target now refers to a specific version of VTA for better autoTVM scheduling* revert change due to bug* rename driver files to be for zynq-type devices* streamlining address mapping* unifying register map offset values between driver and hardware generator* rely on cma library for cache flush/invalidation* coherence management* not make buffer packing depend on data types that can be wider than 64bits* refactor config derivation to minimize free parameters* fix environment/pkg config interaction* adding cfg dump property to pkgconfig:* fix rpc reconfig* fix spacing* cleanup* fix spacing* long line fix* fix spacing and lint* fix line length* cmake fix* environment fix* renaming after pynq since the driver stack relies on the pynq library - see pynq.io* update doc* adding parameterization to  name* space* removing reg width* vta RPC* update doc on how to edit vta_config.json* fix path* fix path",0
"[LANG] Introduce Scan, Bugfix Canonical (#43)",0
[AutoScheduler] Fix task extraction with TE compiler (#8560)* [AutoScheduler] Fix task extraction with TE compiler* fix* test* Update python/tvm/auto_scheduler/relay_integration.py,5
"[CI] Allow command-line argument or TVM_BUILD_PATH for C++ unittests (#12011)* [CI] Use command-line argument or TVM_BUILD_PATH for C++ unittestsPreviously, the `ci.py` script would execute all C++ unit tests in the`""build""` directory, regardless of the docker image being used.  Thischange allows a caller to specify the build directory to be used by`task_cpp_unittest.sh`, either by the command line or by using thesame `TVM_BUILD_PATH environment variable as used by the top-levelMakefile, and passes this argument from `ci.py`.  To preserve theexisting behavior for the pre-commit CI, if no argument is passed andif the `TVM_BUILD_PATH` is undefined, `task_cpp_unittest.sh` defaultsto the `""build""` directory.Python unit tests executed through `ci.py` used the `TVM_LIBRARY_PATH`environment variable, and were not similarly affected.* Remove `name=name` in format scriptCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Fix lint error* Use default expansion of TVM_BUILD_PATHOtherwise, `set -u` rightly errors out for it being undefined.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>",1
Handle empty LLVMModule in GetFunction (#5146),1
Fix attr (#62),0
[ETHOSN] Add support for Ethos-N 21.08 driver stack release. (#9596)Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>,1
[Object] Introduce POD-C Compliant tvm::Map (#5740),5
"[Bugfix][MetaSchedule] Fix over-simplification of Select (#10605)The feature extractor simplifies `Select` into a constant number, which overlooks the possibilitythat there could be buffer access inside Select.",4
TVM android camera demo (#5005),5
[RELAY] bugfix. (#2215),0
[TOPI] Fix mali conv2d performance regression (#3131)* [TOPI] fix mali conv* fix typo* address comments,1
Fix typo in travserse (#4469),2
Concatenation corner case fix. (#11907)* Concatenation corner case fix.* lint fixes.,0
Move Micro TVM top level page (#8249)The micro TVM page was moved during a recent docs update. Thispatch moved the top level index to the former location.,4
[CI] [ComputeLibrary] Use pre-built binaries instead of compiled (#8245)* [CI] [ComputeLibrary] Use pre-built binaries instead of compiledPre-built Compute Library binaries are now downloaded (credits to @leandorn)instead of on-site compilation.Change-Id: I9fd66ce02141813f02382b95351a382ccf775584* Added Apache 2.0 LicenseChange-Id: I3c2af1a86984f81c4ee9408925af9c51510a978f,4
[Relay testing] densenet implementation fix (#8704)* Fixed testing densenet bug* Fixed code format using black,1
[Relay][Op]BroadcastToLike CollapseSumLike (#1886),5
"[CodeGen][CUDA] Fix issues in cuda codegen (#4876)- Do not emit __shared__ etc. as part of type for casting- Fix fp16 reduction kernels with compiler errors:  ""no operator ""+"" matches these operands, volatile half + volatile half  This patch inserts casts to remove volatile type qualifier following  volatile loads (fp16 only). CUDA fp16 library headers should add  volatile member functions.- Update have_fp16 to include compute 6.1 GPUs, which do support fp16,  although their fp16 throughput is low. Updated tests.Signed-off-by: Wei Pan <weip@nvidia.com>",3
Fixed bug in ExprOp that caused bitwise operators to fail when a basic python type was on the left hand side of the expression. Added regression test for crashing cases. (#4852),3
[µTVM] Enable AutoTVM for ARM STM32F746XX Boards (#4274),0
[FRONTEND]minor bug fixes (#1632),0
Update deploy_model_on_rasp.py,5
[microNPU] Test averge pool partitioning (#11965)Follow up for #11469.Change-Id: I474b1d43d3abc6b66d35ebcf3ad6fea50becfb97,4
Fix issue mutating if expressions (#2601),0
[Fix] Update stale relay.Module API in docs/comments (#8411),2
"[FIX,CMAKE] Use set_property with append flag instead of (#6725)set_target_properties.set_target_properties does not append to existing properties. There werea couple place where previously set properties were overridden withdifferent properties. For example, the debug flags for relay were notset correctly because set_target_properties was called twice in a rowwith different options.",1
[ci] Specify permissions for tvm bot (#11937)This adjusts some error reporting for tvm-bot and manually specifies the permissions it should run with to hopefully alleviate the 403 errors when merging PRs,0
[Relay]Refine tensorflow frontend 1.x & 2.x compatibility (#6240)* [Relay]Refine tensorflow frontend 1.x & 2.x compatibility* fix lint error* revert gpu related changes,4
disable other rewrite to test CI (#7371),3
[VTA][Dockerfile] Chisel dependencies for TSIM CI (#3721),2
"[Torch] Pool ops, convert strides and pool_size to int (#7517)* Convert strides and pool_size to int* Make helper function, add test* Fix lint",0
[IR] Update new version of HalideIR (#116),1
[Torch] Add initial 3D op support and test on Resnet 3D (#5075)* fix minor lint issue* add conv3d and adaptive avg pool3d conversion with test* fix max pool handling* add batch norm 3d test* add resnet 3d test* add more conv3d test* clean up batch norm test* add note on disabling inception v3 test* add more tests* add more tests* fix names,0
[RELAY] MobileNet (#1997),5
[CUDA] [Codegen] Ensuring atleast one thread block for dynamism (#7273),5
Bump up dev version (#4941)* bump up dev version* update,5
adding ramana to reviewers list (#11311),1
[ARITH] Remove legacy const pattern functions (#5387),1
remove the pragma primitives for better performance when the threads are binded (#949),1
[ci][docker] Update images to include sccache changes (#11314),4
[Relay][Frontend] Adding ADD operator to tflite frontend for compiling the MobileNetV2 (#2919),1
Some docstring fixes. (#7367),0
[DNNL] Add TensorRequisite concept (#11345)Allow to use DNNL runtime in multi instance mode.Thread safe execution of Run() method.Signed-off-by: Alexander Peskov <peskovnn@gmail.com>,1
"[VTA] Add VTA PYNQ metal_test bitstream program logic and fix compile issue. (#3400)* [VTA] Add VTA PYNQ metal_test bitstream program logic and fix couple compile issue.Issue:VTAProgram not exist and cause compile error.No logic to program the bitstream into FPGA.metal test still use pynq 2.1 library which not support on latestpynq 2.4.Solution:remove old VTAProgram.when setting is pynq, program the bitstream during compile.change DMA link library to libcma.* Address review commends.",1
[TVM] Add importer for ONNX QLinearMatMul op (#8952)* adds importer code * enables `test_qlinearmatmul_2D` unit test,3
[Bugfix] Add nullptr checking for `AttrStmt` with `coproc_uop_scope` attr key (#9123),1
"In memory_plan, check if value is not None, instead of just checking value as boolean. (#5700)",2
[TOPI] add schedule for ARM Mali GPU (#786)* add schedule for ARM Mali GPU* fix lint* fix lint,0
"[MetaSchedule] Enable Adapative Training For XGBoost Cost Model (#11892)CostModel retraining is a time consuming part for MetaSchedule tuning, similar to AutoScheduler, we can alleviate it with an adapative way of increasing waiting period between each retraining. This PR introduced an argument called `adpative_training` in `TuneConfig` and the constructor of `XGBoostModel` to enable the capability. Testing tuning scripts are also updated.",5
"[QEMU] Add number of cores, target list for build (#8156)* num of cores* add target list* comments* cleanup* cleanup* trigger* address comment* comments",1
fix CO CI problem (#1641),0
[TOPI] Add einsum operator (#6370)* [TOPI] Einsum* Fix tuple* fix oshape* * test* * Fix lint* * Remove useless define* * Move to einsum header file* * Fix single value situation* * Fix CamelASE* * Print stride* * Fix single input bug* * fix lint* * Fix lint and add comments* * create test einsum* * Fix lint* * Fix comments,0
"[FRONTEND][ONNX]add Pad, ReduceMax, ReduceMin, ReduceMean and ReduceSum OP (#2061)* add Pad,ReduceMax,ReduceMin,ReduceMean,ReduceSum for onnx frontend* fixed pylint error and warning for frontend.onnx file* add implement v2 for Pad in onnx frontend* compatible with python 3.x* disable too-many-lines pylint check in frontend onnx* use random values instead in onnx frontend testing",3
[COMMUNITY] @konturn -> Reviewer (#12543)Co-authored-by: Leandro Nunes <leanun01@e123855.arm.com>,3
"[TIR] Propagate storage scope of undefined vars in SplitHostDevice. (#11255)* [TIR] Propogate storage scope of undefined vars in SplitHostDevice.* Test global.texture for input, output, and intermediate buffers.",3
Refactoring x86 conv2d_NCHWc (#3944),4
[Relay][Quantization] Fix duplicated simulated quantization (#2803),0
[Relay] Shape func fix for all_class_nms and where op (#7910)* fix missing cast to int64 in all_class_nms shape func* fix scalar in where shape func* add add test* update test* minor fix* add where scalar shape func test,3
[microNPU] Add support for SIGMOID (#9627)Add support for SIGMOID activation function using the lookuptable mechanism in the NPU.,1
[TVMScript] Add intrinsic to look up llvm intrinsic id (#10551)* [TVMScript] Add intrinsic to look up llvm intrinsic id* fix* fix,0
"[Relay] Use LowerTEPass in VM (#9483)We replace use of the TECompiler::{Lower,LowerShapeFunc} methods from the VM'scompiler.cc with LowerTEPass. This clears the way for performing post-loweringIRModule->IRModule transformations which combine Relay and TIR analysis. In particular,it will allow us to use the PlanDevices pass to propagate memory scope constraintsacross PrimFuncs.We run LowerTEPass fairly early in the pipeline, which required quite a few passesto become 'post-lowering friendly'. In particular, ManifestAlloc is now run afterrather than before lowering, and so must now work in a mixed Function/PrimFunc world.The ""vm.shape_func"" operator has been removed since a) lowering has already generatedthe necessary dynamic shape function, and b) the call to that function can berepresented by an 'ordinary' vm.invoke_tvm_op call.We worked our way through the following glitches: - Dynamic shape functions are now given their true type (rather than the type of   the primitive function they are paired with). - Lowering was choosing definitional GlobalVars which were not pointer-equal to the   referential GlobalVars left behind in the rewritten Calls. We fixed that in   te_compiler.cc, though better would be to push GlobalVars deeper into the   lowering machinery. - device_copy was rewritten to a call to @__copy without any definition. Though we   tried adding it as a global this (obviously in retrospect...) won't typecheck if   there are multiple device_copies in the program. Instead leave device_copy unchanged   during lowering and update each executor codegen to look for them specially. - Calls to already-compiled BYOC functions were indistinguishable from calls   to (non-primitive) Relay functions. We move them into the call_lowered calling   convention, and leave behind a Function tagged with ""ExternalSymbol"". Better would   be a first-class representatn for externals in the IRModule but one step at a time. - Functions with dynamic shapes tagged for BYOC compilation were not tracking their   connection to their dynamic shape function. We now use exactly the same attributes   as for non-BYOC primitives. - VerilatorRuntime can legitimately be deleted before initialized. - IRModule attributes must be preserved. In particular, since LowerTEPass can   be invoked more than once we need to be careful to preserve any existing external   modules and other attributes gatherd from an earlier LowerTEPass. - GetUniqueName accounts for existing definitions in the module, but is not used   for external functions since their intended names are communicated to the codegen   toolchain via the already fixed ""global_symbol"" attribute.",0
[Hexagon] Don't use alternative linker for non-x86 API binaries (#10854),2
"[BugFix] shapeOfAttrs should be registered before ""vm.shape_of"" used (#9669)* [BugFix] shapeOfAttrs should be registered before ""vm.shape_of"" used[BugFix] DialectRewriter should not tranform scaler to let expr* retry tests* retry tests again* optimzie and add unit test* retriger test* add comment* fix lint",0
Fix typo in word explicitly (#3376),2
[CODEGEN][CONTRIB] Various update for CoreML codegen (#5934)* [CODEGEN][CONTRIB] Various update for CoreML codegen* fix lint error,0
[ONNX] Update slice to infer attributes when not graph inputs (#6276)* Update ONNX Slice converter to infer slice attributes when necessary.* Linting,5
[RELAY] Copy subfunction arguments to output tuple field (#2537),1
Delete adding a invlid path from Dockerfile.gpu (#1160),2
[Relay][Frontend] Add a few mxnet ops in relay frontend (#2704),1
"Add beagleboard ai, thunderx and stm32mp1 to the arm_cpu target. (#6501)* Add beagleboard ai, thunderx and stm32mp1 to the arm_cpu target.Signed-off-by: Tom Gall <tom.gall@linaro.org>* updates from black on target.pySigned-off-by: Tom Gall <tom.gall@linaro.org>",1
"Add a `--context-path` for build.sh, allowing to test Dockerfiles (#8557)in different directories, and still get relative paths to work.* Add new option --context-path to build.sh* Keeps the default as the `dirname` of the Dockerfile, so  no changes expected in the current behaviour",4
[CodeGenC] Fix bugs when calling extern functions (#7911),1
[PASS] CombineContextCall (#255),4
[TRT] Minor fixes on TRT python interface (#10917)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,5
"[AutoScheduler] New layout rewrite option: Weight pre-transpose (#6750)* Add pre transpose support for layout rewrite* Update* Bug fix* Bug fix* Update* Bug fix* CI Fix* Update* Update* Re-trigger CI* Update* Update test_auto_scheduler_layout_rewrite.py* Update test_auto_scheduler_layout_rewrite.py* Update task_scheduler ut, re-trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>",5
Assert dont crash on null strides (#976),3
"Don't replace reduction init axis with new axis if bound to a thread. (#3408)* Don't replace reduction init axis with new axis if bound to a thread.* Linter.* Reduce bind test case.* Guard test on CUDA support.* [CUDA TE TESTS] Add rfactor predicate test, add global bx and tx.* [CUDA TE TESTS] Add loop partition test for simple rfactor case.",3
"TVMC: Add new text/relay frontend (#10941)* TVMC: Add new text/relay frontendThis feature enables passing a textural representation of a relay module to the tvmc command line.Example: `tvmc compile relay.txt --target c --runtime=crt --executor=aot --executor-aot-unpacked-api=1 --pass-config tir.disable_vectorize=1 -f mlf`Currently it is not possible to supply parameters as it is mainly intended to be used for testing certain relay functions or operators. In the future (with minor changes to the tvmc frontend api) params could be passed via an additional i.e. `params.bin` fileThis commit also adds minimal unit testing of the added feature.Resolve PR commentsTVMC: add warning if relay frontend is used* [TVMC] populate parameters with random values instead of ones* [TVMC] Relay frontend: do not populate input tensor buffers if --input-shapes is providedThis prevents that the constants inputs are used for Constant folding,thus changing the complexity of the model.If there would be a way, to distinguish between model inputs and parameter thisworkaround would not be required.* [TVMC] Relay frontend: check provided file contents before calling tvm.parser.fromtext()",2
Add LOGISTIC operator to relay tflite frontend (#3313),1
[Relay]Frontend][Onnx] Add a converter for ATen Nodes (#7747)* Add support for ATEN DLRM ops.* Fix bugs and test.* Force Aten mode and add extra aten ops.* CI torch version is too old for aten argument.* Add assert for embedding_bag node.* Use new Aten override argument.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,1
fix Relay build docstring (#7963),2
[BYOC][COREML] Handle one symbol for each runtime (#5989)* [BYOC][COREML] Handle one symbol for each runtime* LOG -> DLOG,2
"[LLVM] Remove redundant function CreateBufferVecPtr (#5982)The functions CreateBufferPtr and CreateBufferVecPtr do the exactsame thing, so there is no need for both of them to exist. Thelatter is only used in place, which further suggests that thedistinction is unnecessary.",1
fast tanh (#3255),5
Add dist to python/.gitignore (#1691),1
[Relay] Remove FTVMCompute from TNonComputational ops (#9334)* remove FTVMCompute from noncomputational ops* Remove injective schedule registration for on_device since it is non-computational* lint,4
Use single-threaded SGX parallel (#975),1
[VTA] HW sources refactor (#5188)* refactor* path udpate,4
Check for presence of LLVM configuration. (#8293)LLVM is a pre-requisite for configuring Compute Libraryand offloading to Ethos-N NPU.Better to get the error message at build time ratherthan debugging SEGVs in testsuites.,3
add bfloat16 typeflag support (#4525),1
[Relay][UnitTest] Removed redundant unit test. (#8993)test_op_level2.py::test_conv2d and test_any.py::test_any_reduce shouldhave been removed in the refactoring in #8947.  All functionalitytested by it is in test_op_level2.py::TestConv2D andtest_any::TestAnyReduce.,3
Fix an issue in ReplaceDataFlow for issue 1043 (#1062),0
[ARITH] Improve detect linear equation (#529)* [ARITH] Improve detect linear equation* fix doc,2
"[UnitTest][Flaky] Increased tolerance on onnx test_forward::test_aten (#8798)Default tolerance had about 2-3% failure rate (8/300 iterations), andcaused failures on unrelated PRs(e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-8784/1/pipeline#step-485-log-1156).New limit of `atol=5e-7` chosen to be above the maximum delta of3.5e-7 observed in 300 iterations.",2
"[TFLite runtime] Allow to set number of threads to TFLite interpreter (#6901)* Support for setting thread count in TFLite runtime,Co-authored-by: FrozenGene <zhaowu@apache.org>* fix lintCo-authored-by: FrozenGene <zhaowu@apache.org>",0
[FIX] fix the python script for building resnet (#6526) (#6527),0
SparseFillEmptyRows Op (#7442)* Initial Commit* Fix formats* Remove comments* Black* THreeops* Add Frontend Code* Add Default Value to feed dict* Add Frontend Code* New test Cases and new code to handle them* Add Python Implementation''* Remove stuff* Remove unused imports* Pylint* Pylint* PyLint Shape Func* Make tests cpu only* Add unsorted tests* Add frontend code* Row Major Sorting Only Test* Handle Dynamic Shapes* Add dynamic input shapes* Dynamic Shape Tests* Add documentation* Dtypes* PR Comments* Added comments and changed naming* Add comments* Comments to Shape Func* Documentation* PR Changes* PR Comments* Resolve input and output dtype compatCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
[HYBRID FRONTEND] Modify hybrid script to new interface; hybrid op supported; enable compilation_database in CMakeList.txt (#1757),5
[CI] Enable TOPI tests in ci_arm (#10564)As part of this I had to disable some of the Target specific tests which didn't run under CI correctly,1
Make cython compatible with python3 (#12),1
[YOLO]Add the probability to the image (#1910),1
[microTVM] Add support for Arduino Portenta H7 (#11636)* Add support for Portenta H7* Add Portenta H7 to supported boards in README* Rerun tests,3
"[ci] Don't update Jenkinsfile timestamp on image updates (#12621)The timestamp in the Jenkinsfile is there to prevent post-mergeconflicts from different PRs that edit the templates mergingnon-sequentially. This is not an issue when a line is edited in placethough, which is often the case when Docker image tags are updated. ThisPR makes it so the timestamp is not updated in these cases which shouldreduce merge conflicts on these types of PRs.",5
[Team] jwfromm -> reviewer (#5076),5
"[ci] Add auto-updating `last-successful` branch (#10056)This adds a script that runs on a cron to discover the last commit where CI all passed (every job was successful and `tvm-ci/branch` is included) and updates a git tag `green` to point to this commit on `main`. This can be used for checking out the latest unbroken TVM, which can be useful for developers wanting a good changeset to base their changes on or for infra needing a clean, up-to-date TVM.",5
Add parser support for SUM tflite operator (#4182),1
[Relay][Frontend] TF Tile Round Sign Pow Exp Reverse (#2960)* [Relay][Frontend] TF Round Sign Pow Exp Reverse* fix ci* fix comments,0
"[Onnx] Add SoftmaxCrossEntropyLoss (#8906)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger* [BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc* Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.* Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a* [microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger* [AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci* [Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to ""* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build* [Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0* update gpu and cpu (#8853)* VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* [FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* move rust lint script (#8726)* [AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality* [TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests* extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>* Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.* Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* [Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>* [Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>* [Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.* [UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.* Remove AOT Executor header from Arduino project (#8857)* [Community] @mdw-octoml -> Reviewer (#8868)* [TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* [Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci* [Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials* [Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.* [TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue* [RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860)* [Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI* [RPC] Fix ios_rpc build (#8864)* [Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. ""NVIDIA"", ""radv"", ""AMD open-source driver"") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `""unknown_driver_name""` if the driver does not supportquerying the driver name.* [ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs""* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>* refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling* Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>* [TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>* [CI] make pre-commit hooks to run on every push instead of every commit (#8888)* [TVMScript] Fix printing ForNode annotations (#8891)* [1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055* [microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests* sce loss example* add comments, remove other tests* lint* lint* jostle* lint up* jostle* uncomment some tests* proper return* clean up* lint* minor merge errorsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Michalis Papadimitriou <mikepapadim@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>",1
"[RELEASE] Update copyright message, change notice, remove cma kernel module for now (#4431)",4
[Relay][Frontend] Add reverse op to relay (#2800)* start adding reverse* reverse updated* reverse uses topi::flip* typo fixed* comment addressed* exp simplified,1
allow libbacktrace to be used when cross compiling the runtime (#7917),1
[RUNTIME][String] Overload string operators (#5806),1
[CUDA] BF16 support (#7014),1
[Arm] parallel batch axis (#3931)* support LLVM trunk* guard with USE_LLVM in if condition for c++14* GREATER_EQUAL -> GREATER* [Arm] parallel batch axis,1
"[TIR] Change the behavior of read/write region analysis for reduction blocks. (#10638)After discussion w/ @spectrometerHBH @Hzfengsy , we decide to exclude the buffer access from read regions if it's being written to inside a reduction block. In this way, the outer block would not find overlap between the region reads and writes simultaneously, thus solving the issue mentioned in #10420 .One tricky case is how to handle opaque memory access in `GetBlockReadWriteRegion`, where we have no hint about which buffer is being written to. And I keep the original behavior that the opaque access was added to both read and write regions of a block, no matter whether it's a reduction block or not.",1
[Core][Build] Move build module transformations and utilities to C++ (#9103)* Initial investigation* More progress!* More progress / notes* rewrite build_for_device mostly in c++* More progress* Initial split of transformations applied to device and host as post split action from mixed module* Combine duplicate passes after spliting mod on aot and vm flows* Minor cleanup* Move target mangling to driver_api.cc* Move more build utlities to cpp driver api* [Build][WIP] Moving build utilities to C++ from Python* [Build] Remove comments* [lint] Pass black* More formating* Move more build functionality into cpp* Remove comments* Remove unused defs and imports* Address PR comments* More PR comments* More comments* More comments* Add comments on the new split function* Fix PR comments on clarity* Test CI* Fix format* Refactor build* Expose splitted composite passes to python* Format files* Test fix* Fix for annotating entry funcs on code targeting CPU* Prevent entry funcs to be annotated when compiling for CPU with C runtime enabled* Guard for aot executor entry* Sphix format* Sanity fix* Sphinx fixCo-authored-by: electriclilies <lilyorthsmith@gmail.com>,0
[GCC] Fix GCC8.1 and GCC8.2 template dispatch compilation issue (#6893)* update* [GCC] Fix GCC8.1 and GCC8.2 template dispatch compilation issue,0
[CI] Disable ASF header checking on untracked files (#6975)* This patch will disable checking for ASF header in untracked  files that are never going to make its way into the repo.* That would help developers to have their untracked local files.Change-Id: Ie9f1aae28a474f10f52f22fe9e27a52afd95b4be,4
Added a helper function that dumps Node to stderr (#1703),1
[Debugger] Sorting op-time breakdown for quicker analysis. (#4352),4
[DOC][FIX] Fix some typos in git-clang-format.sh (#5786),2
[TOPI] Memoize winograd matrix (#3687)* [TOPI] Memoize winograd matrix* lint* Fix name,0
[SCHEDULE][PASS] Enable Warp memory and lower to shuffle (#1050)* [SCHEDULE][PASS] Enable Warp memory and lower to shuffle* OpenCL dispatches for now to intel shuffle,0
[RUST][FRONTEND] Fix resnet example (#3000)Due to the previous changes the frontend resnet example failed to build.  So this patch 1) fixes it 2) adds ~~a local `run_tests.sh` to remedy non-existence of MXNet CI (used in python build example)~~ the example build to CI with random weights and a flag for pretrained resnet weightsPlease review: @tqchen @nhynes @kazimuth,1
[Texture] Add 2d memory support into static memory planner (#11876)* [Texture] Add 2d memory support into static memory plannerCo-authored-by: Chris Sullivan <csullivan@octoml.ai>* Add test verifying GraphPlanMemory work for 2d memoryCo-authored-by: Chris Sullivan <csullivan@octoml.ai>,5
don't validate AttrInitEntry until a value has attempted to be set (#6672),1
"Add LowerTEPass, and convert calls to LowerTE to application of LowerTEPass (#8802)* Initial commitInitial stab at IRModule -> LoweredModule conversion func, notesAdd external_mods and main_func_info to conversion funcsMTest lowered module to ir modulefix problem with conversion funcs + print stmtsAdd LowerTE passAdd pLowerTEPassAAdd LowerTEPass to graph_executor_codegen.ccUse LowerTEPass instead of LowerTe in graph_executor_codegen.ccCode cleanupAdd docs, more cleanupFormatting* Fix bad rebase* Address 1st round of comments* Use tir kTarget instead of relay one* Change target string to Target obj* removing target string causing issues* Fix typos* Revert target str -> target obj changes* Don't use Update : IRModule because it is broken* Fix check* flaky test?* lint",3
[DOC][BUILD] Fix cmake and docs (#485),2
[CI] Move to use main as the default (#6665),1
"[PYTHON/API] Add compare and logic build-in op for Expr (#39)* [PYTHON/API] Add compare and logic build-in op for Expr* remove 'and', 'or'",4
[µTVM] Avoid use of builtin math functions (#6630),1
porting new upsample test case from nnvm to relay (#3115),3
[TEST] force openblas threads to be 1 (#1580),1
[3/6] Arm(R) Ethos(TM)-U NPU TIR compiler with conv2d support (#8806)* Arm(R) Ethos(TM)-U NPU TIR compiler with conv2d supportThis commit adds the lowering passes necessary to loweran NPU Relay module down to a TIR module that can becompiled for the NPU. Conv2d is supported as the firstNPU operator. An intermediate TE stage between Relay andTIR allows support for scheduling the operators.Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>* Fix Conv2D TIR type sensitivityChange-Id: I3741f9dd8bb5952590ff8c586f6b96e5c3a03795* Arm(R) Ethos(TM)-U NPU TIR passes and TE for Conv2D*fixing testsChange-Id: Id4a4c80f72ce29b98fc8b3954a1413c1c7fda500* Fix import guards for testsChange-Id: Iaee06017bd125d3040ce42182c4ccdb80d7fc946* Fix typing failures with ignoresChange-Id: I81513f112a42b93cfdd3bcaf8e8852dd60ffe9e9* Remove unused importChange-Id: I6596b62ab56e4ca8b31ef08293686f53f38454d2* Reintroduce get_target_accel_typeChange-Id: I0aaf83fe0204c0db435692e9b92dee6e9d6997feCo-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>,1
[Hexagon] Enable int8 vlut codegen for Relay take (LUT) operator (#11693)* Working 8 bit vlut for relay take operator* Formatting* More formatting* clang-format on codegen_hexagon.cc* Update for llvm api* Add return to VisitExpr(BufferLoadNode) function* different llvm api,1
[ci] Upload built Docker images to ECR (#10662)Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[CI] Bump to use the new cpu image (#4677),1
[Relay] Option to select which convolution layers are quantized. (#3173)* Stashing for later maybe.* Added new option to leave specific layers unquantized.* Better error checking.* remove unneeded import* tab to spaces* pylint fixes* more pylint fixes,0
"[AMP] Turn off accumulation data types for mixed precision pass (#8341)* don't use mixed precision accumulators* turn off fp32 accumulators for now, adjust passing test cases* Add TODO on cuda codegen for failures. Make test case pass on cuda for nowtest to mixed precisionmore testsadd internal func callbroadcast failuresmoreeeadd comment and change lstm unit test to pass on cuda* remove debug statements* to mixed precision* rebase main* rtol and atol adjustments* bump up tolerance again* jostle CI",0
[DOCS] Introduction to Relay IR. (#2185),2
Infer the value of shape expr to avoid dynamic (#12313),5
"[relay] add missing virtual d'tor (#11601)Add a default virtual destructor to`tvm::relay::transforms::GlobalSymbolCache`, so thatcorrect destructors run when destroyingsubclass instances.",1
[Frontend][Pytorch] add suppport for 'aten::upsample_bicubic2d' (#8648)* fix* lint,0
[Relay] Add gradient operator tutorial docs (#2751)* Add gradient operator tutorial docs* Incorporate Steven's and Ziheng's feedback* Remove TODO about `collapse_sum_like`* Add more examples,1
adding soiferj to the list of reviewers (#4108),1
[TARGET] Move target_host usage to new target style. (#9497)- Add deprecation warnings to functions with target_host parameters.- Update the build usage to new target style.,1
[PASS] Fix intrinsic lowering with fma and other intrin (#457)* [PASS] Fix intrinsic lowering with fma and other intrin* relax rtol for sqrt,0
"Add oneflow fronted tutorials (#11036)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* fix bug when upgrade oneflow to 0.7.0* add tutorials* add tutorials* try to fix* fix bug* add test* fix bug* fix flowvision bug* Update test_forward.py* Update test_forward.pyCo-authored-by: hhhfccz <hjk1938927583@163.com>",3
Update Jenkinsfile (#1893),2
add exclusive mode for rpc server (#941),1
docker/bash.sh: lookup docker image in Jenkinsfile. (#7453)* This PR makes it possible to type   `docker/bash.sh ci_cpu tests/scripts/task_config_build_cpu.sh`   and the same version of ci_cpu as is used in Jenkins will be   used to run the command.,1
"[TOPI, Cuda] Fix conv2d_transpose output padding (#6236)",1
[Hexagon] Add test for depthwise conv2d schedule (#11138)* Add test for registered scheduales - depthwise_conv2d,3
[skip ci] Fix onnx/models URLs (#10218)These are broken in CI: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2500/pipeline/275The upstream changed their default branch from `master` -> `main` whichbroke the links used in these tests. This pins to a specific commit (thelatest one at the time of filing this PR).Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[CRT] Create C-runtime-style metadata module for llvm builds (#7398)* Create C-runtime-style metadata module for llvm builds.* maybe address manupa's comment* lint* actually address manupa comments* comment and rename* git-clang-format* pylint* cpp warning* try to fix apps/bundle_deploy* black format* build correct file* Use save() for C++-runtime targeted artifacts.* fix build_module LLVM metadata module conditions* fix test comment* black format* further restrict CRT MetadataModule creation* Fix test_link_params* black format and address zhiics comments* fix test_link_params, i think?",3
Bump up CUDA log version in tophub.py (#4347),2
[GRAPHIR] Print attributes of input (#69),5
Allow implicit conversion in TVM FFI to tvm::Bool (#5907),1
[Int8] Support cublas on e2e int8 models (also tried cudnn but doesn't work) (#9898)* fixed int8 dense offload for cublas* support OHWI kernel layout in qnn.conv2d* fixed reduction axis* add cublas int8 qnn test* lint,3
"[Hexagon] Fix use of subprocess.run in _check_call_verbose (#11985)It uses parameters that are not present in Python 3.6, plus itcatches generic exception, which may not have `stdout` or `stderr`members.",2
[GRAPH] checkin the constructor of indexed graph,5
[REFACTOR][PY] Establish tvm.te and tvm.driver (#4900)- Move the related files to tvm.te- Move build_module.py to tvm.driver,4
[ONNX Parser] Add warning in case of opset mismatch (#8356)* add warning in case of opset mismatch* fix CICo-authored-by: elenaslavutina <elena.slavutina2013@gmail.com>,0
[ETHOSN] Add support for special indices of Reshape (#12556)This pr adds support for the special indices values of the reshape operator for the Arm(R) Ethos(TM)-N NPU.,1
"[TIR][USMP] Integrating USMP to AoT Executor (#9565)This commit integrates USMP with the AoT executor codegen. Additionally, this commit introduces two PassContext options to disable_usmp and disable_storage_rewrite.Moved PrintType from codegen_c.cc to codegen_source_base.cc to be accessible by source_module.ccMoved runtime::metadata to be ExecutorCodegeMetadata as it contains metadata produced by ExecutorCodegen for actual code generation (not a runtime component).",1
[TIR] Add schedule primitive SetAxisSeparator (#11225)* [TIR] Add schedule primitive SetAxisSeparator* remove unused include* Move ReplaceBufferMutator impl to cc file,2
"Revert ""[AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space (#7175)"" (#7236)This reverts commit f2ab977de0ac543cae77d3bef76af1b56dd61eed.",4
[RPC] Added native debug logging to Android RPC (#1432),2
Enable bool type as storage type (#1853),0
[PYTHON] Improve equal sugar (#564)* [PYTHON] Improve equal sugar* fix comment,0
[KERAS]Embedding layer (#5444),5
[Rust] Fixes for wasm32 target (#5489)* [Rust] Fixes for wasm32 target* [Rust] Add test for wasm32* allow cargo config to be into repo* Disable wasm tests in CI,3
"[Torch] Add cast to double, fix flatten conversion (#6357)* support cast to double and fix flatten conversion* also support batch flatten, add test* add flatten test* clean up",4
"Add tf parser wrapper, infer shape automatically",5
[CI] Update PyXIR version to 0.1.3 (#6769)Co-authored-by: Shibui Yusuke <yusuke.shibui@ShibuinoMacBook-Pro.local>,5
[Relay][Docs] Documentation for Algebraic Data Types (#2575),5
trigger travis (#37)* trigger travis* fix* chomo* make* squash,1
"BUG: Make sure FoldConstant can inline constants underneath on_device annotations (#9367)After device planning and conversion to ANF we can end up with:  let %x = on_device(constant, device_type=D)  ...  @f(..., %x, ...)where the device D is not the same as the device for the let-expressionitself. (eg D may be the CPU, %x a shape, and @f an allocation primitivethat requires shapes to reside on the CPU). That's all consistent with the conventionthe DeviceAware* visitors expect for recovering device information.However, it means folding constant into @f's call site must both 'see' theconstant underneath the on_device annotation and bring the on_device annotationalong for the ride:  @f(..., on_device(constant, device_type=D), ...)- Make FoldConstant be on_device aware- Clean things up a bit while I'm there.- Setup unit tests specifically for const folding with on_device annotations.- Replacing if __name__ == ""main"" drivers for units tests with official  incantation as I encounter them.- Don't create on_device(on_device(...))- Logging changes so A/B diffs can focus on just the pass of interest.- Revert Index->DLDeviceType changes in the vm in case they are the cause  of downstream problems.",0
"[microTVM] Temporarily remove mps2_an521 from CI (#8927)Temporarily removing the mps2_an512 board from CI due to issue 8728.Possibly all tests can fail on mps2_an512, so removing the boardinstead of xfailing specific tests.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",3
fix (#87),0
[Codegen][CUDA] Fix: cuda codegen vectorize cast (#7561)* fix: cuda codegen vectorize cast* style: fix python coding style* fix: missing break* refactor: directly split by factorCo-authored-by: jiangchengquan <jiangchengquan@bytedance.com>,4
Fix Web Build after CMake transition. (#2407),1
[Relay][Compile_engine] Int64 shape handling for outputs. (#4031),5
Pass mfloat-abi to LLVMModule::Init (#6150)Fix lint,0
Move SGX enclave signing key to gist (#1393),4
[TOPI] Move topi.nn.util to topi.util (#319)* [TOPI] Move topi.nn.util to topi.util* update the path,5
fix pattern topological order (#5612),2
[docs] typos in include/tvm/ir.h (#4493),2
[VM] Better error messages (#8218),0
[SETUP] Fix python setup (#380),1
"[RFC] Pytest environment improvements (#5421)* [RFC] Pass pytest options globally.In many places having a global pytest flag is useful . For me with thebuild and test of tvm , I would like to be able to globally pass inpytest options as part of development flow or CI flows where one wouldlike to measure other things regularly that need measurements includingpytest coverage data that I would like to experiment with across the stack.This has been achieved with an additional setup-pytest-env.sh file intests/scripts rather than putting in something in every single task testscript and something I would like to avoid.This now means the -v option to pytest is superfluous. I did considerhaving a pytest.ini file but that doesn't allow me to pass any oldenvironment variable in and this seems to be the compromise.* Improve other use case documentation* Rationalize pytest environment.* Remove the setting from docker/with_same_user.* Take the opportunity to migrate common PYTHONPATH andTVM_PATH into the common environment setting.* Fixup vta fsim* Be more explicit with common PYTHONPATH* Fix python path for task_python_vta_fsim.sh properly* Fix nit in documentation.",2
[Hexagon] Handle v69 in RPC launcher on simulator (#10829)There are a few tweaks that are needed related to directorystructure in the SDK.,0
TF argmax - handling int64 datatype (#6674)Co-authored-by: Ubuntu <ubuntu@ip-172-31-0-202.us-west-2.compute.internal>,5
"[Relay] Allow Primitive functions to carry virtual device annotations in PlanDevices (#12095)* [Relay] Allow Primitive function to carry virtual device annotations in PlanDevicesPreviously Primitive=1 functions not analyzed and calls to such were completelyunconstrained. With this change at least any virtual device annotation on the functionare respected and accounted for in calls, even though the body is not analyzed.This may help with piggy-backing on PlanDevices for doing memory scope analysis, sinceit is now possible to express cross-scope functions on Primitive functions. HoweverI believe there are other issues to deal with in addition to this one.* - comments* - also canonicalize targetsWhen including virtual device annotations in test relay programs theannotation will typically use a target which was used as an input tothe make_compilation_config helper, but due to various canonicalizationmake not be pointer equal to the final structurally equal target which endsup inside the constructed CompilationConfig. However VirtualDevices usepointer equality when comparing their target field.So make sure the notion of CanonicalVirtualDevice also accounts for canonicaltargets.* - update unit test to reflect the Ardreno example* - trivial cleanup",4
[Runtime] MISRA-C compliant TVM runtime (#3934)* implement of MISRA-C compliant TVM runtime;* working on bundle_deploy_c demo* move header files into include dir* fix compatibility issues* fix compatibility issues* resolve most of the warnings and errros* implement c_backend_api* introduce bridge* working well* move to header files and bundle.c into src/runtime/crt* clean up* satisfy linter* clean up* test with the cat image* remove synset* refactoring* refactoring* refactoring* initial crt_runtime_api.c* improved compatibility with g++* using exposed API in c_runtime_api.h* call from c_runtime_api.h* clean up* lint* merge into apps/bundle_deploy directoryChange-Id: I51904db81b8589e65d107d8ca77b47452e3812b5* make the demo runs in ciChange-Id: I2c24f8b592508833d3555311c2b24d1931f19385* address review commentsChange-Id: I027ddff15c31fb4da0bd0e461427dce619de1f93* releaseChange-Id: I5ad5bb8426468aac9fc8d074e56ddea358a7fd91* fix ci testingChange-Id: Ic2e82fb3051b6c254ef32a964f976b61e3e5fe4d* add test case for misra c runtimeChange-Id: Ie0dfd0ade6be4665b4384db7d260a6c69b35010f* fread files in testing to avoid calling xxdChange-Id: Ie7fbc16b4b0b9509918d986a841f443900813bef,4
[Backend][Verilator] regression tests (#7000)* add files* update tests* test this* test this case* update jenkins file* fix offload* update* update variables* rollback ci files,2
"Tensor Expression Debug Display (TEDD) (#4651)* Initial TEDD for publishing.* 1. Fix lint issues. 2. Print intrin.body instead of intrin.name in Schedule Tree.  3. Add examples to top level APIs' comments.  4. Top level APIs don't print Dot string by default, unless outputdotstring is True.* Fix more lint issues.* Update top level API argument names and use raw strings to avoid Python lint warnings in the tests.* Disable TEDD verification, but keep TE construction.* Stop importing tedd to avoid failure.* Separate data extraction and visualization. 1. Add API tedd.dump_json(schedule) to dump a json string for the schedule data for visualization.  2. Update tests.  3. Add a tutorial.  4. Add range information to IterVars.* Update TEDD about InferBound failure.  1. TEDD doesn't call inferbound for DFG. 2. Update tutorial about the InferBound failure.* 1. Import IPython only if SVG is requested.  This is required to fix a tutorial publishing faliure.  2. Fix test about IPython availability check.",3
[TEST] Temporary disable conv2d grad strided flaky test (#6183),3
[Hexagon] Add support for on-device unit testing using gtest (#11145)* link gtest to tvm runtime* first test running!* HexagonBuffer tests running in sim* move to new tests directory* use USE_HEXAGON_SDK* add python frontend for Hexagon unit tests* clean up after rebase* isolate cmake changes to Hexagon* add gtest init with arguments* add hexagon sources only if building for Hexagon; remove workaround* format & lint* fix Hexagon build error* remove x86 implementation and win32 code* check if hexagon gtest path exists before linking* make USE_HEXAGON_GTEST an optional cmake param* turn on Hexagon gtest in Hexagon CI* Hexagon unit tests should fail if run without proper gtest linkage* add tvm option; move Hexagon tests to test/cpp-runtime/hexagon* add libinfo* trigger ci,5
"[Relay] Bitserial ops (#3844)* Added arm_cpu NHWC schedules.* Fixed kernel shape legalization.* Added bitserial ops to relay.* Snapshot and more missing files.* Added dense testing.* Added tests* Added ASF header to new files.* cc lint* Pylint change.* pylint fixes.* Change arm legalize test.* Added assert check to arm legalize.* Added better documentation, fixed some bad style* Reverted arm conv2d nhwc changes.",4
Fix lint error missed during CI outrage (#1846),0
"[docs] Getting Started With TVM: Tensor Expressions (#7768)* [docs] Getting Started With TVM: Tensor ExpressionsUpdate of getting started with tensor expressions.Adds a matrix multiplication example to be used in later tutorials,makes CPU primary target and GPU optional for wider audience reach.* Fix linting",0
