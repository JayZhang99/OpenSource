commit_msg,labels
Initial commit,5
init skeletons,5
checkin basic expr,5
fix bug for rdiv,0
checkin basic var,5
basic var,5
"Move Var back to Expr, add format str test",1
add expr simplify and canonical,1
checkin tensor,5
checkin domain,5
finish tensor dom infer,5
Fold RTensor into tensor,5
checked split,5
Temp checkin c++ code.,5
Merge branch 'master' of ssh://github.com/tqchen/tvm,5
checkin basic cpp test,3
"Enable array, basic form of tensor",5
C++ API work with python,5
Add safe destructor,1
fix c api,0
[OP] enable binary op,5
[API] expose dir,5
Add domain,1
Tensor API,5
expose range,5
checked buffer / schedule / code gen,5
Expose array to python,5
Simplify for cxx,5
Change array to copy on write semnatics,4
Allow all enum int to be exposed,5
Check in basic schedule container,5
add var binding for expr,1
"[REFACTOR] Move Node always bebind NodeRef, expose ->",4
checkin stmt,5
check stmt in,5
"Switch to HalideIR, with C API compile",5
"promote C API lib to root, pass basic_test",3
Add most of IR constructors,1
"Add in Array, fix most of IR",0
Change name matching to SSA Variable and Function matching,4
"Add Tensor, cleanup test, all present tests pass",1
python3 compatibility,5
Check in Tensor API on python,5
Checkin reduce,5
change APIVariant to class,4
Add new functor,1
Merge branch 'master' of ssh://github.com/tqchen/tvm,5
Sync up,5
Enable reduction in front-end,5
Checkin Schedule and split construction in front-end,5
Adds cpp test for Halide IR cse pass.,1
Enable IRFunctor based IRMutator,5
update comment,1
Checkin IR Visitor and tests,3
check substitute,5
Refactor IR Pass,4
temp checkin,5
SSA Pass,4
Enable attribute key in LetStmt,5
Add Operation class,1
Check in inline and test,3
fix compilation,0
schedule over operation,5
Finalize tensor and operation,5
Add AttrStmt,1
temp checkin of schedule ops,5
Keep up with changes of NodeRef,4
checkin initial of itervar,5
Refactor to use iterVar,5
Enable bracket syntax sugar to get tensor element,5
"Fix Schedule structure, refactor compute to all rely on iter var",0
temp checkin of schedule,5
Finish schedule operation,5
Add tile operation,1
Merge pull request #5 from ZihengJiang/masterAdd tile operation,1
Skeleton of bound inference passing rule,4
"IntSet Evaluation, skeleton finish",5
"Rename bound to schedule, add graph related utils",1
Add map container and tests,1
"Expose testcase as bound inference to python, now push toward the goal!",3
Pass first basic case of bound inference,4
"Fix Tile, add a few more test cases on bound inference",0
"Rename dim_var to axis, update testcases",1
Stronger type checker during conversion,5
"Make Tensor comparator and hash to be aware of same op and index, init checkin of the ir generation",5
[PASS] Schedule Ops init working version (#6)* [PASS] Schedule Ops init working version* bugfix in PassUp,0
Setup Travis CI (#7)* Setup Travis CI* add source* fix source,0
"[LANG] Change Schedule->Stage, Use Schedule for global schedule (#8)* [LANG] Change Schedule->Stage, Use Schedule for global schedule* add numpy as dep* add numpy installation, temporary disable osx",1
fix ctypes bug for mac (#9)* fix c_api bug for mac* update travis,0
[LANG] Enable json load/save and pickle (#10),5
add simplify test (#12),1
"[LANG] Include buffer semnatics, introduce pylint (#11)* [LANG] Include buffer semnatics, introduce pylint* Refactor inline add support for buffer indexing* fix doc",0
[PASS] Basic storage flatten (#13),4
[RUNTIME] Add interface header of runtime (#15)* [RUNTIME] Add interface header of runtime* fix mac build,0
[PASS] Export simplify and equal to python (#14)* [PASS] Export simplify and equal to python* fix naming convention,0
[RUNTIME] Finish GPU runtime and python interface (#16)* [RUNTIME] Finish GPU runtime and python interface* fix travis test* fix build,0
[RUNTIME] Enable OpenCL (#17),5
[PASS] Assign unique names to variables in ConvertSSA pass (#18)* [PASS] Assign unique names to variables in ConvertSSA pass* revert change to ConverSSA pass,4
[TESTCASE] Add a mock test workflow of CUDA codegen (#19),1
[API] Move all RTTI related code to one place (#20)* [API] Move all RTTI related code to one place* add back rtti comment,1
[IR] Move AttrStmt to HalideIR (#21),4
[CODEGEN] Add CodeGenC (#22),1
"[CODEGEN] Add LoweredFunc, MakeAPI to build a C API function (#23)* [CODEGEN] Add LoweredFunc, MakeAPI and SplitHostDevice* update halideir",1
"[RUNTIME] Add Function, Unify TVMTypeCode and TVMArgTypeID (#24)",1
"[API/JIT] Enable registerable global function, introduce StackVM intepreter (#25)",5
[API/Refactor] Unified PackedFunc for API and Generated Functions (#26),5
"[CODEGEN/EXEC] CUDA, NVRTC pipeline complete (#27)* [CODEGEN] CUDA/OPENCL pipeline complete* Hide TVMType by str in frontend",5
"[TEST/PYTHON] Add unittest folder, add a build pipeline. Rename Buffer.ptr to Buffer.data to be consistent with Array. (#29)",1
"[SCHEDULE] Improve bound inference, support reduce codegen. (#30)",5
"[PASS] StorageFlatten and StorageSync, safe condition in schedul_ops, gemm example. (#31)",4
"[PASS] UnrollLoop, isolate arithmetic module. (#32)",4
fix Stage.fuse (#33),0
[PASS] Canonical form simplify (#34),4
[ADDON] Allow piggy back nvcc compiler and code (#35),1
"[FUSION] add 'void AutoFuseEwise(Schedule sch)' (#36)* [FUSION] add Fusion(Schedule)* [FUSION] rename to AutoFuseEwise, detect whether the stage has been scheduled* [FUSION] change to visitor pattern* [FUSION] rename filename* [FUSION] fine-tune the interface* [FUSION] typo* move elem_wise to schedule* rename test function",1
[LANG/PASS] Support Vectorize (#37),4
"[PYTHON/API] Add compare and logic build-in op for Expr (#39)* [PYTHON/API] Add compare and logic build-in op for Expr* remove 'and', 'or'",1
[LANG/PASS] InjectVirtualThread (#38),4
[PASS] Change IRVisitor interfaces to function override (#42)* [PASS] Change IRVisitor interfaces to function override* [PASS] Change IRMutator interfaces to overloadable function,4
[SCHEDULE] Refactor bound inference logic (#41),2
"[LANG] Introduce Scan, Bugfix Canonical (#43)",0
"[SCHEDULE] Mutate dataflow in schedule, refactor Stage (#44)",5
"[ARITH] DeduceBound (#40)* [PYTHON/API] Add compare and logic build-in op for Expr* remove 'and', 'or'* add deducer* [WIP] bound_deducer.cc* move IntervalSet and StrideSet into int_set_internal.h* add multiple failure for VariablePathFinder, add EvalSign* consider round in deduce, add success flag* remove Visit_(Div)* add comment, update HalideIR* expose intset to python* check the sign of every expr* set return type as ExprSignType* fine tune* add min & max python api for interval set* support for conditional expr* refactor test* add checker for BoundDeducer* add python check test* fix* fix* change range to interval; remove converter* remove converter declaration* remove int_set_internal.h",0
"[SCAN/Refactor] Refactor scan interface, enable fix point analysis. (#47)",0
[ARITH] Add CombineInterval<Div> in IntSet (#48)* [FIX] add CombineInterval<Div>* fix error message and add comment about rounding* fix comment,0
[CODEGEN/LLVM] Initial support for codegen LLVM. (#49)* [LLVM] Initial support for codegen LLVM.* Fix the naming issue of codegen,0
[MODULE/REFACTOR] Introduce Module for AOT and runtime linking. (#51),5
[MODULE] Enable OpenCL and CUDA Modules (#53),5
[LLVM/RUNTIME] Support Parallel for on CPU (#54),5
[BUILD] Add CMake for Windows build (#55),1
[BUILD] Windows build pass on LLVM/CUDA/OPENCL (#57),4
"[VISITOR] New ExprFunctor, StmtFunctor Interface. Modular analysis (#58)* [ARITH/VISITOR] Modular Analysis, ExprFunctor, StmtFunctor* retrigger* [IRFunctor] Migrated CodegenC* [IRFUNCTOR] Migrate CodeGenLLVM* [IRFunctor] Migrate canonical* [IRFunctor] Migrate vectorize* [IRFunctor] migrate CodeGenStackVM",1
Fix Travis on test (#59),0
[LLVM] Vectorized load/store (#60),5
[TEST] Add dot (#61),1
"[PASS]LoopPartition (#56)* loop_partition draft* divide loop variable into constant domain and variable domain & consider multiple partitions* process doubt interval* fix and refactor, add relax_map arg in BoundDeduce* fix testcase and comment* rebase to zero, convert to SSA* change the logic of generating loop code & fix issues* add a testcase for relax map in deducebound && fix issues* clean code* const auto&* add test_multi_if",0
"[REFACTOR] Add Types to IterVar, Isolate Operator (#62)* [IterVar/REFACTOR] Add types to IterVar* [ARITH/REFACTOR] Move IntSet to include* [REFACTOR/OP] Move Op detail to seperate folder.* fix test",0
[BUGFIX/TESTS] Bugfix of Tenso slicing. Union. (#66),0
[PASS] RemoveNoOp. (#68),4
"[OP/LANG] Support Extern Call, more regression tests (#69)* [OP/LANG] Support Extern Call, more regression tests* [TEST] Include pylintrc",3
[VERILOG] Basic Verilog Testflow (#70)* [VERILOG] Basic Verilog Testflow* fix build* fix the comment* fix lint in verilog,0
"[RUNTIME] Make rutnime DLPack compatible, allow new device plugin (#71)* [RUNTIME] Refactor runtime to be DLPack compatible. Enable plugin of new runtime.* fix mac compile* ok",0
[VERILOG] VPI Mem Interface/ VPI MMap (#73)* [VERILOG] VPI Mem Interface/ VPI MMap* fix test issues,0
"[CODEGEN] Refactor common codegen, Verilog Codegen (#74)* [CODEGEN] Refactor common codegen, Verilog Codegen* fix make* fix mk* update enable signal* change function name to at neg edge* Move test to correct place",0
add docs (#75),1
Update contribute.md,1
[VERILOG] Generalize Buffer and Tests (#76)* adding tvm_buffer and fifo testbench* minor edits* line buffer test bench* adding double buffer tests for the tvm_buffer* making variable consistent with  python style,1
"[LANG/SCHEDULE] Reduction factor, predicate in reduction. (#77)",5
[LANG/GPU] Cross Thread Reduction (#79)* [LANG/GPU] Cross Thread Reduction.* Fix doxygen error* Upgrade verilog testcase to new one,0
[SCHEDULE] Fix the scan schedule with rewriting (#80),0
"[SCHEDULE] Add group, refactor thread bind api. (#82)* [SCHEDULE] Add group, refactor thread bind api.* fix doc* fix g++-4.8* More testscase* Remove graph context from fix pt analysis",0
[IR] Rename attr_key in AttrStmt (#83),5
[PASS] Support for partition loops with thread_axis (#81)* [PASS] Support for partition loops with thread_axis* Add check for AttrStmt.attr_key,1
[SCHEDULE] More reliable bound inference on threading. (#84),5
[SCHEDULE] Fix cross thread schedule after refactor (#85),0
[BUGFIX] Thread related bound (#86),0
[PERF] Persitent kernel (#87)* [PERF] Persitent kernel* fix doc,0
[DOC] Initial doc system (#88)* [DOC] Initial doc system* Migrate API* Update docs,1
"[DOC] Add setup.py, fix comments (#89)* Add setup.py, fix comments* Update installation document",0
[DOC] API doc organization. (#90)* [DOC] API doc organization.* remove breathe for now,2
[DOC] move comments to file header (#91),2
Update README.txt,1
[SCHEDULE] Normalize returns a new schedule (#94),1
[BUILD] add with_api_wrapper to lower (#95),1
[DOC] Add schedule_computaion (#92)* [DOC] Add schedule_computaion* Finetune the doc* Finetune the doc* Finetune the doc* Set max_unroll_step=0 by default,1
fix Makefile ifdef & typo (#97),0
[DOC/PERF] Reduction Tutorial and GEMM (#96)* [PERF] Add gemm* [DOC] Reduction tutorial,1
[DOC] Scan tutorial (#98)* [DOC] Scan tutorial* Fix according to ziheng's comment,0
[LANG] Change namespace convention to dot (#100),4
[LANG/CODEGEN] Intrinsics and Extern Math (#101)* [LANG/CODEGEN] Intrinsics and Extern Math* fix lint,0
[DEV/IR] Python IRBuilder (#102),5
Update contribute.md,1
"[PASS] StorageRewrite, Memory optimization pass as in NNVM. (#104)* [PASS] StorageRewrite, reuse memory pass as in NNVM.* fix issue",0
[PYTHON] Rename the namespace (#105),5
[LANG] CommReducer (#103)* [LANG] CommReducer* Reorganize c_api* Remove InitValue and Combine; refactor Functor* Make CommReducer an Expr* Make comm_reducer type independent* Make CommReducerNode a Node* Small fix* Refine* Refine front api; add integration testcases for min/max* Fix python* Refine* Fix lint and add example,0
[PYTHON/FFI] Enable Cython FFI (#106)* [PYTHON/FFI] Enable Cython FFI* fix cython,0
[PYTHON] addon->contrib add docs (#107),1
[DOC] Add intro to 'comm_reducer' in tutorial; fix doc (#108)* [DOC] Add intro to 'comm_reducer' in tutorial; fix doc* Fix* Fix,0
"[CODEGEN/PASS] Improve callpacked lowering, allow pass array callback. (#110)* [CODEGEN/PASS] Improve callpacked lowering, allow pass array callback.* fix cython",0
"[CODEGEN/RUNTIME] Metal support, runtime improvement. (#111)* [CODEGEN/RUNTIME] Metal support, runtime improvement.* Fix case when no device is available",0
[PYTHON] Enable cython ndarray API (#113),5
[METAL] Switch to manual ref counting (#114),5
Fix comment (#115),0
[IR] Update new version of HalideIR (#116),1
License as BSD for now before we finish approval for apache (#118),5
[DOC] Doc redirection (#119)* [DOC] Doc redirection* fix setup url,0
[CONTRIB/BLAS] Add CBLAS Example to contrib (#120)* [CONTRIB/BLAS] Add CBLAS Example to contrib* Update makefile,1
[RUNTIME/CUDA] Allow save exit when driver unload (#122),5
[CMAKE] Windows support upgrade (#125)* [CMAKE] Windows support upgrade* Fix lint,0
[BUGFIX] Fix schedule dataflow rewrite with multiple scan states (#126),0
"[BUGFIX/REGRESSION] Complex inline call, regression test on lstm cell (#128)",0
Require explicit version for llvm_config. Update doc build requirements. (#129),1
[FIX] Miss kUInt in TypeCode2Str & dir method (#130)* [FIX] Miss kUInt in TypeCode2Str & dir method* [FIX] Add regression test,0
[SCHEDULE] Add store_predicate (#131),1
[MODULE/DSO] Support pack everything into one shared library. (#133)* [MODULE/DSO] Support pack everything into one shared library.* fix osx load,0
"[DOC/LICENSE] Make doc and license consistent, opensource repo when we get approval (#134)",5
Update CONTRIBUTORS.md,1
[PASS] Use likely tag & enable LoopPartition by default (#132)* [PASS] Use likely tag & enable LoopPartition by default* [PASS] Support thread_axis partition* Take IfThenElse branch method* [PASS] Insert branch at the innermost thread scope* [PASS] Select candidates before trying to partition & add test for select* [PASS] Clean code* Fix* Remove print & assert vectorize happens,0
[BUGFIX/PASS] Fix Vectorize with If condition (#135),0
[PYTHON] Support DLTensor compatible API (#136)* [PYTHON] Support DLTensor compatible API* optimize for common path,5
[Tutorial] External Tensor Op (#137),5
[FIX] Add CombineInternal<Mod> & Fix LoopPartition (#138)* Add CombineInternal<Mod> & Fix LoopPartition* Add check for path,0
check attach_stage & group in schedule.copy() (#139),5
"[PASS] Improve SSA conversion, add forbid list in loop-par (#142)",1
fix doc examples & easy install (#143),0
Fix build status (#145),0
fix TVMRetValue move constructor not clear old value  (#144)* fix TVMRetValue move constructor not clear old value lead to repeat delete* fix,0
use auto source_group (#146),5
[RUNTIME] RPC runtime that support run testing on remote device. (#147)* [RUNTIME] RPC runtime that support run testing on remote device.* Fix ctypes in OSX.* fix lint,0
add tvm.select (#148),1
[DLPACK] Upgrade to the latest version (#150),3
[TESTS] Jenkins test flow (#152),2
Remove linux from travis (#156),4
"[DOCS] Jenkins deployment of docs, add FAQ (#157)",1
[CI] Force doc build pass to mark success (#158),4
[TEST/CI] 32bit compatibility and CI. (#159),3
[CODEGEN/RUNTIME] Cross Compile Test (#160),3
"[BUILD] Enable RTTI of most part of library, example extension pkg. (#161)",5
"[EXAMPLE/PYTHON] Improve extension type, add operator lib (#162)",1
[BUILD] Clean the HalideIR submodule during the make clean (#163),5
[CODEGEN] Change default max_auto_unroll from 256 to 32 (#164),4
Improve makefile (#165)* Improve makefile* Fix,0
[ARITH] More aggressive CSE during canonical simplify (#166),5
"[PASS] Refactor build config, allow implicit unroll pragma (#167)",4
[PERF/TIMER] Add builtin timing logic (#168)* [PERF/TIMER] Add buildin timing logic* fix lint,0
[BUILD/CODEGEN] Allow combine multiple functions in build stage. (#169)* [BUILD/CODEGEN] Allow combine multiple functions in build stage.* Enhance code module* fix compile,0
Change Schedule Array constructor to static make method (#170)* Change Schedule Array constructor to static make method* Add CreateSchedule* Add doc* Change CreateSchedule to create_schedule at cpp side,1
"[MODULE/RUNTIME] Remove Precompile, simplify module (#174)",4
"Support for Tuple Inputs of Reducer and ComputeOp (#175)* Support for batch ComputeOp* Support for batch ComputeOp* Fix CrossThreadReduction* Fix lint* Add UpdateArray, remove support for batch reduce* Tuple input support for reduce* rfactor works with multiple reducer; support multiple reducers with different types* Small fix* Small fix* Change return type of rfactor to Array<Expr>* Fix lint* Improve* Add tutorial* Improve tutorial* Improve tutorial",0
[TUTORIAL] Update tvm.make.Select to tvm.select (#177),1
[Compile] Fix compile issue with LLVM 8.0 (#181),0
[NNVM] Example NNVM integration. (#182),5
[MODULE] support load back of .ll file into llvm module (#183),2
[TUTORIAL] Cross Compilation and RPC (#184)* [TUTORIAL] Add tutorial for RPC* [TUTORIAL] Update tutorial* [TUTORIAL] Update tutorial* trigger update* [TUTORIAL] Improve build,1
[RUNTIME] Move device_api to include (#185)* [RUNTIME] Move device_api to include* fix doxygen* fix device api* fx,0
[CODEGEN] More storage alignment info aware generation (#186)* [CODEGEN] More storage alignment info aware generation* fix* fix* fix warning,0
"[LLVM] More optimized option, allow emit assembly (#187)",5
[IR] Include PrefetchIR (#189),5
[CODEGEN] Make codegen registerable (#193)* [CODEGEN] Make codegen registerable* fix llvm disbaled,0
[LANG] Expose tvm.cast (#195)* [LANG] Expose tvm.cast* Update* Add unittest,1
[LANG] Add all and any in the python API (#196)* [LANG] Add all and any in the python API* compatible with python3,1
[OP] Initial Stucture of Op Library (#198)* [OP] Initial start of op library* add gtest,1
Fix] Avoid Directly Pass Python Context Object (#201),0
[INTRINSIC] Add sqrt (#202)* [INTRINSIC] Add sqrt* [INTRINSIC] Expose on cpp,1
[CONTRIB/NNPACK] Add NNPack Fully Connected Functions (#199)* Add NNPack Fully Connected Inference* Add NNPack fully_connected_output* Fix lint* Fix,0
"[LANG/BUFFER] Change buffer arguments to match DLPack order, add scope (#203)",1
add 2D reduction into tutorials (#204),1
[FIX] Fix allocate size^2 in graph_executor (#207),0
[CODEGEN] Concise typecast for threadIdx (#208),5
"[BUFFER/REFACTOR] Buffer byte_offset-> elem_offset, add buffer_bind_scope (#209)",1
[REFACTOR] examples->apps (#210),5
Switch off global barrier detection by default (#211),5
[DOCS] fix doc builder (#213)* [DOCS] fix doc builder* fix* fix* fix doc builder,0
[REFACTOR/PASS] Formalize argument bind and match util (#214)* [REFACTOR/PASS] Formalize argument bind and match util* grammar,4
[APP] Improve GraphExecutor (#216)* Remove 'final' in GraphExecutor for extension* Dynamic num of inputs/outputs for tvm_op,4
"[PASS/OP/REFACTOR] IRDeepCompare, isolate computeop part, allow fuzzy bind (#218)",4
[TAG] Add tvm.tag module for tagging operator (#217)* [TAG] Add op_tag module for tagging operator* Fix accroading to comments* Add example* Add into doc* Add --fix-missing for docker,0
[IR] Add body to AssertStmt (#220)* [IR] Add body to AssertStmt* fix lint,0
"[CODEGEN/PASS] add restricted, alignment option (#221)* [CODEGEN/PASS] add restricted, alignment option* fix lint* Fix the alloca",0
"[C API] Make DSL API registerable, add copy from/to raw bytes (#222)* [C API] Make DSL API registerable, add copy from/to raw bytes* fix cython",0
[SCHEDULE] tensorize (#223),5
"[TOPI] Example for depthwise convolution (#197)* first commit* move to topi/recipe* refactor, almost rewrite* 2-D sum reduction; implement SAME pad; improve schedule* add util.py; separate test script* conv + bn + relu fusion* auto fusion* separate declare and schedule; using op tag* divide large image into blocks* move to topi; improve blocking schedule* restructure* add doc* using time_evaluator",1
[TOPI/TEST] Add Testcase folder for TOPI (#225),1
[DOC/TOPI] Add API doc for topi (#226)* [DOC/TOPI] Add API doc for topi* fix lint,0
Small fix of the Depthwise Convolution example in python3 (#224)* fix for python3fix for python3* Update depthwise_conv2d_map_test.pyremove sys.append,0
[RUNTIME] Add System Lib (#227)* [RUNTIME] Add System Lib* lint* lint* fix compile,0
[TAG] Fix signature of decorated function (#228)* [TAG] Fix signature of decorated function* Add dep,0
[RUNTIME] Add workspace pool (#229)* [RUNTIME] Add workspace pool* fix doc* fix the free list* avoid zero size,0
[TEST] Add scipy to test dep (#231),1
[TEST][TOPI] of depthwise_conv2d (#230)* test of depthwise_conv2d* fix nose test error* python3 fix,0
[RUNTIME][ABI] Flat structure arguments (#232),5
"[RUNTIME][ABI] Remove TVMValue as argument, use address (#236)",1
[PASS] Layout transform pass (#233)* [PASS] Layout transform pass* Fix according to comment* Fix,0
[CI] Add JVM Env (#237)* [CI] Add JVM Env* add update,1
Enable identity layout (#238),5
[JS][WEB][BACKEND] Javascript(webassembly) backend. (#239),5
[DOC] Fix c++ doc build (#240),0
[RUNTIME] Fix Metal runtime compile (#241),0
"[EXECUTOR] Save/Load Params (#242)* [EXECUTOR] Save/Load Params* [EXECUTOR] Improve Save/Load, fix Makefile* [EXECUTOR] Make save independent with executor",0
[RUNTIME][RPC] Change RPCServer to Event Driven Code (#243)* [RUNTIME][RPC] Change RPCServer to Event Driven Code* fix,0
"[REFACTOR] collections->container, RPC returns func, time_evaluator r… (#244)* [REFACTOR] collections->container, RPC returns func, time_evaluator returns struct* fix executor",0
[INTRIN] prefetch support (#246)* [INTRIN] prefetch support* lint* add buildin,1
Update index.rst,1
[DOC] Fix doxygen comments (#247),0
"[WIP][Frontend] Scala/Java package (#176)* JVM package skeleton* [JVM] link libtvm.so and list function names* [JVM] Function & NDArray skeleton* [JVM] TVMFuncCall in JNI* [JVM] handle string arg in TVMFuncCall* [JVM] get module function* [JVM] entry function for Module* [JVM] construct Module from function return value* [JVM] TVMContext, TVMArray attributes* [JVM] NDArray from / to java array* [JVM] load so and compute on cpu* [JVM] move PackedFunc to individual modules* [JVM] assembly package & native library loader* [JVM] unit test & codestyle check settings* [JVM] NDArray from & to different dtypes* [JVM] NDArray from native double array. Add linux-cpu profile.* [JVM] modify Makefile* [JVM] add linux-x86_64-gpu profile* [tvm4j] delay load libtvm_runtime.so* [tvm4j] refactor to pure java* [tvm4j] remove scalastyle-config.xml* [tvm4j] remove link HalideIR, remove Shape, remove scala binary versions* [tvm4j] only allow convert from/to same type array* [tvm4j] make NDArray api more readable* [tvm4j] refactor for c api* [tvm4j] add Jenkins tests* [tvm4j] fix duplicate Dockerfile cmd* [tvm4j] fix ut script filename* [tvm4j] add module load tests* [tvm4j] add javadoc, remove types package* [tvm4j] fix test script* [tvm4j] remove ut temp dir* [tvm4j] fix missing package types* [tvm4j] java code style check* [tvm4j] fix java lint* [tvm4j] downgrade checkstyle plugin for JDK7* [tvm4j] add stylecheck in jenkins tests* [tvm4j] specify source file encoding* [tvm4j] lazy init function; add Function.call() api; allow manully release Module,NDArray,Function* [tvm4j] fix ModFree* [tvm4j] cache Function in API",0
"[RENAME] nvcc_compiler->nvcc, cc_compiler->cc, metal_compiler->xcode (#248)",5
[DOC] Make range related function consistent (#249),5
[EXECUTOR] Enable load executor remotely (#245)* [EXECUTOR] Enable load executor remotely* [EXECUTOR] Pipeline* Pass bytearray directly* Enable load dynamic library in rpc_server.py* Fix* lint* Return Module from remote side directly* Remove unused header file* Fix* fix,0
[RPC] Allow back pressure from writer (#250)* [RPC] Allow backpressure from writer* fix* fix,0
[CODEGEN] Generate main compute function separately with alias info (#253),5
[EXECUTOR] Fix bug and improve (#252)* [EXECUTOR] Fix bug and improve* [EXECUTOR] Enhance test case,0
[PASS] Add storage alignment info to heap allocated data (#254),1
[PASS] CombineContextCall (#255),4
fix build in windows (#256),0
[RPC] Allow RPCServer to run without decorator (#257),5
"[API] Prefetch schedule supported (#258)* prefetch interface added* prefetch python comments modified. prefetch info data structure maintained.* start injecting prefetches. first step (domain touch) implemented.* domain touch tested.* Prefetch ir_mutator and ir_visitor dispatch registered.* modify domain touched from passing a func_ref to passing a tensor* modify domain touched from passing a func_ref to passing a tensor* modify Tensor copy to Tensor ref* temp commit for rebase* debug info removed, typo fixed, ready to rebase* prefetch flatten test add!* roll back builtin functions to side effect functions* lint error fixed!* add cache line size to storage flatten argument* forgot modifications add* change code style to dmlc-like; get rid of can_prove, use manually compute instead* python lint error fixed* modify instrinsic name to pass tests* [TEST] get rid of str(), replace them by accessing attributes* change map to list comprehension* redundant numpy import removed",0
[TOPI] Example for convolution in GPU (#212)* [TOPI] Example for convolution* update conv ex* fix submodule HalideIR* update conv impl* python3* minor fix* fix pylint error* Add test code* x* fix* fix* move python helper function into topi.testing* fix pylint,0
[EXECUTOR] Move tvm_op and Handler<DLTensor> to graph_executor.cc (#259),4
[RUNTIME] Enable injection of some core runtime functions to avoid dynamic lookup (#260),5
[RPC] IOS RPC (#261),5
[DOC] More detailed installation instruction (#262)* [DOC] More detailed installation instruction* fix lang,0
[FIX] Fix bug and typo in rpc_server (#263)* [FIX] Fix bug and typo in rpc_server* [FIX] Remove unnecessary condition,0
[SCHEDULE] Detect useless bound early (#264)* [SCHEDULE] Detect useless bound early* fix,0
[tvm4j] register user-defined function (#251)* [tvm4j] register user-defined function* [tvm4j] define java function (pushArgToStack) to convert arguments to C TVMValue* [tvm4j] make Module & Function extends TVMValue* [tvm4j] make registered cb function return Object* [tvm4j] add cb finalizer; add TVMValueBytes* [tvm4j] support NDArrayBase cb arg* [tvm4j] register cb function unit tests* [tvm4j] pass Function.Callback to resource_handle* [tvm4j] fix type cast,0
[LANG] Add reflection routine to construct node (#265),1
[STORAGE][BUFFER] Support access ptr for clear access pattern. (#266)* [STORAGE][BUFFER] Support access ptr for clear access pattern.* fix lint,0
[BUGFIX] Fix CanonicalSimplify change type of int constant (#269),0
[BUILD] Fix osx compilation (#271),0
[SCHEDULE] Remap the cached bind_scope. (#272)* [SCHEDULE] Remap the cached bind_scope.* more fix,0
[EXECUTOR] Improve LayoutTransform pass (#273)* [EXECUTOR] Improve LayoutTransform pass* Remove offline params for now* Small fix,0
[TUTORIAL] Optimize gemm on CPU add! (#270)* [TUTORIAL] Optimize gemm add!* temp commitment* [TUTORIAL] python3 compatiblity made; doc generation updated!* [DOCS] gen_modules clean and ignore add!* [TUTORIAL] title modified!* [TUTORIAL] some rolled-back modification re-write* [TUTORIAL] title underscore extended!* CONTRIBUTORS add me!,1
[EXECUTOR] PruneGraph pass (#274),4
[TUTORIAL] gemm tutorial image add! (#276)* image add!* image path move to web-data,1
[TEST] Fix java compilation (#279),0
gitignore tags (#277)Signed-off-by: Edward Z. Yang <ezyang@fb.com>,5
[EXECUTOR] Add GraphHandle (#285)* [GRAPH] Add GraphHandle* Move to apps/graph_executor,1
[BUILD] rename build.py to avoid conflict name of build (#284)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed,0
Fix issue relating to serialization of reducer (#282),0
[DOC] Fix typos in tutorials (#287),0
"Build system and dynamic library fixes (#283)* Install rules and dynamic library loading fixes.A batch of fixes:- Added 'install' rule to cmake and make, which installs runtime  headers and library (libtvm_runtime).- Added 'installdev' rule to make, which also installs the compiler  infrastructure headers and library (libtvm)- Added 'INSTALL_DEV' option to cmake, for toggling installation  of compiler infrastructure headers and library- cmake no longer builds into lib/ directory; instead all build  products go in your build directory- New algorithm for dynamic library loading, as described in #281.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Emit dylib on OS XSigned-off-by: Edward Z. Yang <ezyang@fb.com>* Lint fixes.Signed-off-by: Edward Z. Yang <ezyang@fb.com>",0
"[SCHEDULE][REFACTOR] Default Fuse to outer inner, consistent to split (#289)* [SCHEDULE] Fix fuse node order* Make fuse order consistent with split",0
[PASS] Improve graph fusion (#286)* [PASS] Improve graph fusion* Change fusion center to segment head* Use 'master' to identity the schedule node* Make things compact* Fix,0
fix rpc server proxy connect (#290),0
[PASS] Simplify dependency of StorageRewrite (#291),4
[FIX] Fix doc_string of reducer (#292),0
Conda build recipe (#288)* Typofix.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Probe for nvrtc in lib directory as well.Signed-off-by: Edward Z. Yang <ezyang@fb.com>* Conda build recipe for TVM.Signed-off-by: Edward Z. Yang <ezyang@fb.com>,0
[ARITH] Refactor intset eval with functor (#295),5
[PASS] Refactor thread storage sync to a common visitor (#296)* [PASS] Refactor thread storage sync to a common visitor* Fix the sync scope check behavior,0
[PASS] Enhance LayoutTransform pass (#293)* [PASS] Enhance LayoutTransform pass* Fix* Fix Compilation* Refactor* Refactor* doc* fix* add file,0
[PASS] More storage sync. (#297),4
"[SCHEDULE][RUNIME] Introduce pragma for additional extension hint, threadpool runtime. (#299)",1
[EXECUTOR] Split graph_executor to header file and (runtime) source file (#300)* [EXECUTOR] Split graph_executor to header file and (runtime) source file* Fix,0
[NNPACK] Add nnpack.convolution (#301)* [NNPACK] Add nnpack.convolution* Add instrinsic* Fix lint,0
[RUNTIME][PASS] Allow declare vector type array (#302)* [RUNTIME][PASS] Allow declare vector type array* fix bcast* [BUFFER] Enable vload/store function in buffer* ok,0
[tvm4j] RPC Server (#268)* [tvm4j] RPC Server* [tvm4j] fix recursively function calling; connect to proxy server; osx rename .so to .dylib* [tvm4j] test case for proxy connection; thread pool for serving,0
[PASS][RUNTIME] Support attr scope lift and runonce (#303),4
[FIX] Pass the attributes of master node (#304),0
[PASS] Allow allocation in parallel scope (#305),4
[TEST] Upgrade gpu docker to cudnn7 (#306)* [TEST] Upgrade gpu docker to cudnn7* fx,3
[tvm4j] disable proxy test for now (#307),3
[RPC] Enable shutdown hook (#308),5
[PASS][FIX] Fix LiftAttrScope with if (#309)* [PASS][FIX] Fix LiftAttrScope with if* [PASS] Fix on proc sync* fix,0
minor fix (#313),0
[PASS] More improvement of canonical (#314),4
"[PASS] Memory barrier detection, storage access lower. (#317)",4
[PASS][PRAGMA] Allow pragma debug_skip_region to skip region of computation (#318),0
"[WIP] C++ topi contributions (#312)* [WIP] C++ topi contributionsSummary:This diff implements C++ topi contributions for:  - relu with parametrix threshold  - pad with generic padBefore / padAfter specification  - matmult with transposes  - conv2d_nchw, conv2d_hwcn with runtime constant padding and strides  - depthwise_conv2d_nchw with runtime constant padding and strides  - group_conv2d_ngchw with runtime constant padding and strides  - broadcast_to a broadcastable shape  - broadcast_bop where bop is an usual binary op (+ - * / %)Convolution padding is implemented using the pad operation.To avoid extra memory consumption, it is generally recommended to inline the padding with the autoinliner.Unfortunately in its current form the elemwise checks are too restrictive to allow inlining.So this diff also proposes an extension to LHS injective (i.e. no reduction axis in the current IR design)Test Plan:Tested in C++ testsuite in a separate repository, I am looking for suggestions to quickly spin up some tests for tvm.Reviewers: tqchenSubscribers:Tasks:Tags:Blame Revision:* Review + Lint + GSG C++",1
[TOPI] Move topi.nn.util to topi.util (#319)* [TOPI] Move topi.nn.util to topi.util* update the path,1
"[TOPI] conv2d nchw gpu scheduler (#315)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed* conv2d gpu scheduler for two layouts moved to tvm* changes made according to CR* conv2d_nchw formating updated, conv2d_hwcn tests updated* lint error fixed* element wise operator schedule fusing fixed for conv2d* conv2d_nchw topi test added, all resnet workloads now pass* conv compute lint error fixed* fixed python 3 compatibility problem* conv2d tensor input support added, test typo fixed, ir_pass.Simplify changed to util.get_const_int",0
[DOC] Include TOPI in doxygen (#321)* [DOC] Include TOPI in doxygen* update,1
[TOPI] add dilation operators (#316)* add dilation operators* fix pylint* dilate testcases success* n-D tensor dilation* support arbitrary dimension,0
[TOPI] C++ doc (#320),5
[BUILD] Simplify build process (#326),5
[TOPI] Add broadcast and reduce operators (#267)[TOPI] Add broadcast and reduce operators,1
[TOPI] Move ewise.h -> elemwise.h (#327)* [TOPI] Move ewise.h -> elemwise.h* fix test,0
update depthwise_conv2d schedule and testing (#328),1
[DOC] Document update (#329),1
[TOPI] Add ops compute (#323)* [TOPI] Add ops computeRemove 'compute' and add assert for safetyAdd documentfix lintfix softmax* fix batch norm,0
"[TOPI] Fix conv2d for small input channels (#331)* __init__ updated* pull request updated* build_module added* typo fixed* another typo fixed* conv2d gpu scheduler for two layouts moved to tvm* changes made according to CR* conv2d_nchw formating updated, conv2d_hwcn tests updated* lint error fixed* element wise operator schedule fusing fixed for conv2d* conv2d_nchw topi test added, all resnet workloads now pass* conv compute lint error fixed* fixed python 3 compatibility problem* conv2d tensor input support added, test typo fixed, ir_pass.Simplify changed to util.get_const_int* fixed channel numer < 4 error, also made sure other splitting factor woudn't be 0",0
[TOPI] Improve dilate (#330),5
"[TOPI] Isolate padding option, improve decl of depthwise/conv2d/pool (#332)",1
[BUILD] Enable cudnn in gpu build (#333),5
[Contrib] CuDNN v7 Support (#311)* [Contrib] CuDNN v7 Support* Add test,1
[WIP] [TOPI] Depth wise Conv for NHWC (#325)* rename the nchw and pass the unit test; going to do it for nhwc depthwise* bug with fusion* nchw works fine; nhwc float32 problem remains* still cannot bind them together* fusion works* syntax fix* all bugs fixed; test cases pass* minor fix on nn.h,0
[NNPack] Support for threadpool (#334)* [NNPack] Support for threadpool* fix lint* fix lint* Use static class function,0
[PASS] RewriteUnsafeSelect lowers unsafe select to condition expr (#335),4
Allow install-dev to include all necessary header files (#338),2
Fix CUDA library search (#339),0
[DOC] Release note (#340),5
[SUBMODULE] switch to https (#341),5
[DOC] Add link to release blog (#342),1
Add tutorial for convolution in CUDA (#343),1
update depthwise convolution api (#344),1
"conv_nchw parameter updated to the one generates mobilenet benchmarks, doc typo fixed (#345)* conv_nchw parameter updated to the one which generates mobilenet benchmarks, doc typo fixed* removed unused variables",0
modify schedule_depthwise_conv2d_nchw (#350),5
changed makefile to build rocm backend (#355),2
[Python] Dist wheel tools (#348),1
[BUILD][LLVM] Support LLVM mainline 5.0 6.0 (#356)* [BUILD][LLVM] Support LLVM mainline 5.0 6.0* Reduce parallelism,5
[DOC] Add install prerequisites (#358)Add install prerequisites of customized building,1
[iOS] Better RPC guide and bug fix (#357),0
[CODEGEN][LLVM] Refactor cpu runtime related code to CodeGenCPU (#361),5
Update installation guide of windows (#364)* update installation guide of windows* update installation doc of windows,1
[TOPI] update depthconv padding api; fix shared memory overflow (#365)* modify depthconv padding* fix shared memory overflow in depthconv schedule,0
[BUILD] Include rocm cross compile env (#367),5
Softmax operator migrated to topi (#366)* softmax migrated and test added* pylint error fixed* pylint error fixed,0
[API] Expose AutoInlineInjective (#368),5
[DOCS] Add some notes about LLVM (#373)Add some notes about LLVM before building TVM,1
[DOC] Add Build TVM Runtime on Device section (#372)* [DOC] Add Build TVM Runtime on Device sectionAdd Build TVM Runtime on Device section* Add echo USE_RPC=1>> config.mk in code-blockAdd echo USE_RPC=1>> config.mk in Build TVM Runtime on Device section* [DOC] Fix small problem,0
[SETUP] Fix python setup (#380),0
[TEST] Add rocm library to library path (#381),1
update dlpack (#382),1
[APP] Android RPC (#359)* [APP] Android RPC first version* [APP] Android RPC build jni automatically* [APP] Android OpenCL RPC tested on real devices* [APP] optimize android app interface. add ndk compile tool* add ndk compile tool* [APP] fix android app thread crash; add android test script* [APP] android app - show alert dialog and disconnect when error occurs* fix ndk build script code lint* fix ndk build default argument* ndk script build remove shell=True. disable android app screen orientation,0
[SUBMODULE] update dmlc-core and Test (#387)* [SUBMODULE] update dmlc-core* [TEST] Not keep ROCM link but only verifies it,1
[RUNTIME] v2: runtime support for rocm (#386)* v2: runtime support for rocm* fixed coding space errors* removed kROCM from c_runtime_api.h,0
[TEST] Jenkinsfile (#389)* [TEST] Jenkinsfile* Fix wheel setup,0
[NNPack] Fix automatically cast fail on some platforms (#388),0
[tvm4j] Java runtime README (#391),5
[TOPI] Relu and schedule elemwise (#390)* convolution compute typo fixed* relu activation migrated to topi* reviews addressed* elemwise schedule added* relu compute deleted,0
[CODEGEN] NVPTX backend. (#392)* [CODEGEN] NVPTX backend.* Fix pylint* use fix,0
[TOPI] improve elemwise schedule (#393)* [TOPI] improve elemwise schedule* modify fuse,5
[APP] Android RPC README (#395),5
[DOC] Reorganize docs (#397),2
[DOCS][APP] Add Example for C++ deployment (#398)* [DOCS][APP] Add Example for C++ deployment* fix lint,0
[CODEGEN] Multiple parallel in one launch (#399),5
[SCHEDULE][PASS] support storage_align of certain axis (#400)* [SCHEDULE][PASS] support storage_align of certain axis* fix lint,0
[PASS] IRTransform to enable IR pass proptype in python (#401),4
[submodule] update dlpack (#403),1
[BACKEND] Allow nvptx to pass ll ir to CUDAModule (#404),4
[PASS] InjectDoubleBuffer (#405),4
[BACKEND] Explicitly allow specialization of FMA in llvm (#407),5
[BUILD] Allow inject custom pass via phase (#408),4
[PASS] Check memory info bound to guard failure (#409),4
[PASS] Improve vthread injection. (#411),4
[PASS] Improve double buffer (#413),4
[GPU][TOPI] Fix cross thread reduction schedule (#414),0
[TOPI] Add topi.target; Schedule for raspberry pi (#406)* CPU Schedule for raspberry pi* Update* Update* Add topi.target* Refactor* Update* Make python3 happy* Improve* Improve* Improve* Use get_const_int,1
[FIX] Fix build error: call to 'make_const' is ambiguous (#415),0
[SETUP] Always use relpath for setup (#421)* [SETUP] Always use relpath for setup* [CMAKE] Fix cmake llvm build,0
[BUILD] Improve build instruction with llvm. (#422),2
[TEST] Add memoize to save test data (#424)* [TEST] Add memoize to save test data* Update comment* mark py version,1
[PYTHON] Allow general types (#425),5
[TOPI] Improve conv2d for resnet18 workload  (#427)* relu activation migrated to topi* reviews addressed* relu compute deleted* conv2d_nchw updated* resnet18 hand tuned schedule added* pylint error fixed* one more workload test for conv2d_nchw* conv2d schedule subfunctions added for different patterns* reviews addressed,0
[METAL][RUNTIME] Fix bug of memcpy into metal buffer (#428),0
[DOCS] Fix markdown syntax error (#430)Fix markdown syntax error (code shifts out of markdown-code box).,0
[SCHEDULE] Enhance cache_write to enable layout change. (#432)* [SCHEDULE] Enahance cache_write to enable layout change.* more tests,3
Conv2d updated  (#435)* improved conv2d for last group of workloads* conv2d_nchw improved on 14_256_256 and 56_64_128,1
[LLVM] Protect ll when emit pass (#436),4
[TOPI] Fix softmax bug (#437),0
[APP] enhance android ui (#441),5
disable fopen64 in dmlc-core (#443),5
[RUNTIME][RPC] Enable remote linking of device code. (#444)* [RUNTIME][RPC] Enable remote linking of device code.* fix build,0
"[DOCS] Add prerequisites about zlib1g-dev (#446)* [DOCS] Add prerequisites about zlib1g-devinAdd prerequisites about zlib1g-dev. It occurs `/usr/bin/ld: cannot find -lz` without zlib1g-dev.* Add prerequisites about python-setuptools Add prerequisites about python-setuptools. Otherwise, it will fail when executing `python setup install --user` command.* [DOCS] Add prerequisites about python-devAdd installation prerequisites about python-dev. Otherwise, it will fail with `SystemError: Cannot compile 'Python.h'. Perhaps you need to install python-dev|python-devel.` when executing `python setup install --user`.",0
[RUNTIME] Enable extension type to PackedFunc. (#447)* [RUNTIME] Enable extension type to PackedFunc.* More comments,5
"[TOPI] Depth wise convolution backward methods for NHWC (#434)* rename the nchw and pass the unit test; going to do it for nhwc depthwise* bug with fusion* nchw works fine; nhwc float32 problem remains* still cannot bind them together* fusion works* syntax fix* all bugs fixed; test cases pass* minor fix on nn.h* back wrt input* backward wrt input nhwc; only test case in recipe* test case for depthwise back wrt input* test case for depthwise backward wrt weight* tags* minor fixes* pylint test; add arch=3.7* modify scheduler* better backward depthwise w.r.t weight scheduler* updated scheduler* test_topi_depthwise_conv2d_back_input.py and test_topi_depthwise_conv2d_back_weight.py success* all test cases wrt input pass* update* new test cases and scheduler* not working 1 and 2* good wrt weight, bad wrt input* test cases added* remove tf lines* minor fix* compute arch changed* remove compile hook* minor change* pylint* fix the float for python case* fix cases for python3 case* except for memoize* fix most; memoize still wrong* memoize added* unexpected layout cases added for scheduler* error message layout other than NHWC added* improve padding* fix as pr requests* remove dilate in backward wrt weight",0
[RPC] clarify error message for unmatched context (#451)Clarify confusing error message for unmatched context,0
conv2d schedule fall back warning fixed (#450),0
[RUNTIME] Add function to pack arguments (#452),1
"[BACKEND] initial llvm codegen for amdgpu (#402)* added initial llvm codegen for amdgpu* fixed whitespace* fixed hsaco gen from ir* fixed targetmachine for rocm and added GetSource for rocm* fixed whitespace issues* changed statement to use less than 100 lines* added intrinsics for workgroup - rocm* whitespace - newline error fix* fixed error msg for workitem-workgroup intrinsics* added llvm ir dump for rocm codegen* [ROCM] changed codegen to emit proper amdgpu kernel header* fixed whitespace error* fixed whitespace error- 2* fixed AddFunction to not to use extra arg1. Changed AddFunctionInternal to not to take extra arg for target type2. Use Target from CodeGenLLVM to check for AMDGPU target* fixed whitespaces* fixed whitespaces 2* fixed codegen for AMDGPU - now generating valid IR* fixed codegen depending on code review* reviewed alignment for amd devices* added code to dump code object to file* fixed cpplint errors* print out IR after pass manager* added code to dump asm, obj to file and std string* fixed whitespaces* Update codegen_amdgpu.cc* used registry for amdgpu llvm* Fixed whitespaces* added code for calling linker* fixed formatting errors* added rocm link python interface* fixed pylint issues and added more body to the function* added doc string* added doc string for module* fixed python code after review, fixed llvm object codegen* fixed linker to generate code object* removed dumping to output file and debugging log out* fixed lint for python code* added fault check after running linker* removed print statement in rocm.py* changed rocm lld linker to raise runtimeerror than emitting error log to stderr* changed the way linker command line is pass to subprocess.popen* removed redundant code and reuse tvm utils* removed commented out code* removed cloning of unused modules, and put IR into string",0
added vim temporary files to gitignore (#453),1
[TOPI] add binary broadacst (#456)* add binary broadacst* fix testing* revise testing threshold,0
[PASS] Fix intrinsic lowering with fma and other intrin (#457)* [PASS] Fix intrinsic lowering with fma and other intrin* relax rtol for sqrt,0
[RPC] Include rpc session info into context (#458)* [RPC] Include rpc session info into context* add type checker in return converison,1
[RPC] Expose module handle (#459)* [RPC] Expose module handle* not include handle,5
Use ewise schedule for broadcasting (#460),5
[SUBMODULE] upgrade dmlc-core (#461),5
[METAL] use 32bit indexing for metal until we have a bound adapted pass (#462)* [METAL] use 32bit indexing for metal until we have a bound adapted pass* fix lint,0
[RUNTIME][PYTHON] More compatibility in ndarray (#463),5
Acknowledge related projects (#465)* [CODEGEN] Redo CodegenLLVM.* Add remarks about origin of the passProperly acknowledge related projects* Fix and expression,0
More on source reference (#466),5
[DOC][DEVGuide] Runtime system note (#467),5
fix cpp deploy (#468),0
[CODEGEN] Fix Metal codegen when storage scope is needed (#469),0
[TOPI] negation->negative to be consistent with numpy (#470),5
[INTRIN] Enable pow (#471)* [INTRIN] Enable pow* rename pow->power* fix,0
[TOPI] Formalize the tag system (#473),5
"[TEST] rfactor+ewise, cite rfactor paper (#474)* [TEST] rfactor+ewise, cite rfactor paper* include all authors via abbrv* [TOPI] Add transpose* fix lint",0
"[TOPI] migrate global_avg_pool, fully_connected (#472)* migrate global_avg_pool, fully_connected* fix pylint* enable fusion of pooling schedule* rename fc->dense, enable fusion* improve dense schedule* unified global pool",0
[TOPI] Fix reduction fusion with injective input (#475),0
[TOPI] dense API to remove redudant use_bias (#476),4
[TOPI] Fix reduce fusion with more levels (#477),0
add pool (#478),1
Packing and data layout change added to conv2d_nchw (#479)* conv2d layout change and packing added for the last workload* packing added for other workloads* conv2d added packing for first workload* fix pylint error,0
[BASE] Make macro namespace angostic (#480),5
fix fuse reduce_axis error in pooling schedule (#482),0
log_softmax added to topi (#483),1
[RUNTIME] Minimum graph runtime (#484)* [RUNTIME] Minimum graph runtime* update docs,1
[DOC][BUILD] Fix cmake and docs (#485),0
[RUNTIME] Remove parameter def from runtime (#486),4
[ARITH] More robust int set checking (#487),5
"[TOPI] add reshape, concatenate, split (#481)* [TOPI]add reshape* fix problems* fix lint* try to add concatenate* fix lint and error* fix doc* fix error* try to add split* fix lint* fix error* fix lint",0
fixed algorithm. padding should be on both sides (#489),0
[CUDA] auto detect compatibility when arch is not passed (#490),4
[RUNTIME] Fix graph runtime for gpu (#491),0
[TOPI] add squeeze (#494)* add squeeze* should be squeeze,1
[UTILS] Move target to tvm; rename convolution as conv2d (#492)* Move target to tvm; rename convolution as conv2d* Fix* Fix,0
[TOP] Initial Schedule of MobileNet on Rasp (#496)* [TOP] Initial Schedule of MobileNet on Rasp* Fix* Fix,0
[TOPI] Update conv schedule on rasp (#497),1
"fix squeeze to output (1,) if all axes are squeezed. E.g squeeze((1,1,1...), None) case (#498)",0
[DOCS] How to deploy TVM Modules (#499)* [DOCS] How to deploy TVM Modules* More comments,2
[TOPI] Update depthwise conv2d schedule on rasp (#500),1
[LINT] Fix pylint (#501),0
[LANG] Support for Bitwise Operation (#502)* [LANG] Support for Bitwise Operation* Add test,1
"[TOPI] Add left_shift, right_shift, clip, cast (#504)* [TOPI] Add left_shift, right_shift, clip, cast* [TOPI] Add test* [TOPI] Fix",0
[SCHEDULE] Fix inline with multiple outputs (#507),0
[SCHEDULE] Further fix of reduce inline with multiple outputs (#508),0
conv2d adjusted to fix different workloads (#511),0
[RUNTIME] Better error message in cuda launch (#513),0
conv2d data re-layout fix out of threads bug (#514)* conv2d layout change bug fixed* remove debug msg* misaligned error fixed,0
Conv2d modified for better performance (#516)* conv2d tweaked for better end-to-end performance* syntax changed,4
"[TOPI] add argmax, argmin (#515)* add argmax argmin* remove coder saver",1
"[CODEGEN] More robust llvm intrin handling, remove graph executor (#519)",4
[CMAKE] Fix cmake build (#520),0
[BUILD] Windows support of DLL exports (#522),5
Use LIB_SUFFIX (#526),0
[ARITH] Improve detect linear equation (#529)* [ARITH] Improve detect linear equation* fix doc,0
"Case-sensitive CMake module for OpenCL (#537)* Case-sensitive CMake module for OpenCLCase-sensitive CMake module for OpenCL, see https://github.com/Kitware/CMake/blob/master/Modules/FindOpenCL.cmake .* fix in case Metal locates OpenCL as wellfix in case Metal locates OpenCL as well",0
fixing typo in apps/README.md (#538),0
[PASS] copy intrin (#536)* [PASS] copy intrin* update comment thanks to derisavi,1
[RUNTIME] Enable ext_dev type for quick plugin of device (#542)* [RUNTIME] Enable ext_dev type for quick plugin of device* [TEST] Update testcase to cover all computation,1
fixed rocm runtime. set default gcn arch to be gfx803 (#544),0
[TEST] Use equal for equality comparison expression (#543)also improve comment and unit test,3
Add CODEOWNERS (#545),1
[TOPI] Fix declaration for different dtypes (#546),0
"[CODEGEN] Skip unrolled hint, export symbol on win32 (#547)",5
Add rocm target to topi tests (#548)* add masahi to contributors* enable rocm target in topi tests,1
add msvc in cc (#531),1
added support for rocm gpu autodetect (#549)* added support for rocm gpu autodetect* changed type casting from old style to static_cast* fixed code to generate gfx specific code object* fixed namespaces,0
"Add same_as to NodeBase (#550)* Add same_as to NodeBase1. Most class inherited from NodeBase(Schedule, Stage, etc) still havethe convenience of using '==' for object identity. And this is the rightbehavior for non-Expr classes.2. subclasses of ExprOp now create EQ expression when '==' is used.`__nonzero__` and `__bool__` in EQ and NE is a comprise that in some casesobject identity semantics is still useful, like in unit test. For instance:````assert a == b````""a == b"" will create EQ expression, assert then calls `__nonzero__` of theresult expression. `Expr.__nonzero__` throws exception since it prohibitsevaluating IR expression.More complex case like:````assert a in b # b is dict````it will call `__eq__` on a and all keys of b, then `__bool__` on the resultexpression. This could not easily be done by same_as.* Retain __hash__ from NodeBase in Python3",1
[CODEGEN] Detect broadcast(cast(x)) pattern in FMA (#551)* [CODEGEN] Detect broadcast(cast(x)) pattern in FMA* [CODEGEN] Improve* [CODEGEN] Fix,0
enable rocm target for topi/recipes. add timing util to gemm test. (#554),1
[Refactor] Introduce target generic dispatch system (#556)* [TVM] Introduce target generic dispatch system* fix target warning,0
[CODEGEN] Force not inline compute core for better debug (#557)* [CODEGEN] Force not inline compute core for better debug* also support llvm4,0
[CODEGEN] Bugfix multiple condition generation (#558),0
[CODEGEN] Allow link additional module (#559)* [CODEGEN] Allow link additional module* fix py3* add register back,0
[FIX] Fix target warning (#560)* [FIX] Fix target warning* [FIX] Deduplicate options* Fix* Fix,0
[ARITH] More caninical simplfy (#561)* [ARITH] More caninical simplfy* [DEBUG] Use HalideIR with trace logging,0
[CODEGEN] Use correct math intrin for metal (#562),5
[PYTHON] Improve equal sugar (#564)* [PYTHON] Improve equal sugar* fix comment,0
"[PYTHON] Improve equality wrapper (#567)use `object.__eq__`(default object identity comparison) as defaultimplementation of same_as. This should be OK since `EqualOp` and`NotEqualOp` are pure Python object, `object.__eq__` is sufficient.",5
"[ROCM] Working math function support for ROCm backend, a bug fix in LLVM based codegen (#570)* added math function support* bug fix extern func call in llvm based codegenlint fixfix buildbug fix extern func call in llvm based codegen* moved rocm bitcodes detection to python",0
[SCHEDULE] Detect duplicate IterVar in reorder (#575),5
add friendly tips when not found cl and link (#574)* add friendly tips when not found cl and link* fix lint,0
[PASS] More robust UnrollLoop configuratin (#576),4
Update topi/cuda schedules to use target.max_num_threads (#577)* update topi/cuda schedules to use target.max_num_threads* allow num_thread to be larger than cuda.max_num_threads* remove get_max_num_threads and make it inline,1
[DOCS] Fix tag_scope example (#581),0
[CODEGEN] Fix CPU compute attribute (#582),0
[PYTHON] Allow no de-allocation when exit (#583),5
[TOPI] add conv2d_transpose_nchw (#586),1
[BUFFER] Smarter slice to detect compactness (#587)* [BUFFER] Smarter slice to detect compactness* move simplify of begins early,2
[ROCM] View llvm ir and gcn asm with module.get_source(...) (#590)* view llvm ir and gcn asm with module.get_source(...)* fix lint,0
[ROCM] remove fma dispatch (#591)* removed fma dispatch* added comments to explain why remove fma* fix lint* use fmuladd intrin for fma dispatch,0
add helpful message to topi test (#592),1
[TOPI] Support ceil_mode in pooling (#593),5
vgg16 workload error fixed (#598),0
Fixed build with metal on MacOS with case-sensitive FS (#601),0
[INTRIN] Enable popcount (#606)* enable popcount intrin* fix lint* add test* fix python3,0
[TOPI] modify conv2d_transpose schedule (#613),5
[DLPack] Upgrade dlpack to 0.2 (#609),5
[TOPI] fix weight layout in conv2d_transpose (#616),0
add tanh dispatch (#619),1
remove minimum 32-bit restriction (#621)Change minimum 32-bit restriction for floating point types to 8-bit.This change is to enable reduced precision types that may use vector operations underneath the hood (cases #lanes > 1 such as half4).,4
conv2d_56_64_128 mark==1 bug fixed (#624),0
"Support vector operations for AMD (llvm IR) (#623)* Support vector operations for AMD (llvm IR)* fix whitespace* update comments, docstring",0
WIP: Add how_to readme to install tvm with nnpack support (#610)* feat(docs) add how_to for tvm install with nnpack support* feat(docs) change python package paragraph* feat(doc) remove unsure sentence* add comments on nnpack usage vs TVM* remove mxnet nnpack tips for nthread change,1
"inline AMD GPU functions (#625)* Support vector operations for AMD (llvm IR)* fix whitespace* update comments, docstring* inline AMD GPU functions",0
android gemm for topi/recipe (#628),5
[NNPACK] Add argument nthreads (#631),1
[PASS] Enhance LiftAttrScope (#632)* [PASS] Enhance LiftAttrScope* update vt,1
[TUTORIAL] use OpenCL on ARM board (#633),5
[PASS] Update coproc sync (#634),1
[CODEGEN] Enable closure with no argument (#635),5
[PASS] Fix vthread when extern access touching (#636),0
Fix conda packages (#642)* Make the tvm conda package build with in-place source and use cmake from conda.* Add a package for topi.,0
"conv2d perf improved for conv2d_56_64_128, super resolution workloads added (#643)* conv2d perf improved for conv2d_56_64_128, test name added to differentiate workloads* fix lint error",0
[APP] improve parameter pack (#645),5
[TOPI] Add out_dtype argument for conv2d; Add x86 schedules (#646)* [TOPI] Add out_dtype argument for conv2d; Add x86 schedules* Fix* Fix lint* Fix,0
[UNROLL] New unroll option (#647),1
Conv2d scheduler tweaked for super resolution perf (#652)* scheduler tweaked for super resolution perf* lint error fixed* lint error fixed* conv2d_transpose schedule error fixed,0
"Compat for opencl mode between cpu mode and gpu mode (#655)some host opencl runtime may at cpu mode, but remoteclient opencl runtime at gpu mode, compat it",5
[RUNTIME] support limited save without cross compile (#659),5
Fixed nnvm issue #239 (#660)* scheduler tweaked for super resolution perf* conv2d_transpose schedule error fixed* nnvm issue #239 fixed,0
[CONTRIB] MPS DNN Dense (#615)* mps* update,1
[PASS/SETUP] Fix minior issues (#663)* [PASS/SETUP] Fix minior issues* fix lint,0
Documentation correction (#665)Readability.,5
[PASS] Allow compact checking when strides is available (#669)* [PASS] Allow compact checking when strides is available* remove assert compact,4
[TOPI] Fix for pooling (#673),0
[ARITH] Upgrade CanonicalSimplify to Simplify Mod (#676),5
[ANDROID][RPC] Remove binary distro jar (#677)* [RPC][JVM] Remove binary dist gradle from repo* fix header,0
"fix parameter name in UnrollLoop (#679)In unroll_loop.cc the parameter name is ""auto_max_depth"", but in ir_pass.h the parameter name is ""auto_min_depth""",0
fix name bug in test_pass_inject_double_buffer (#678)Change the parameter 'C' name,0
[APP] fix gradle build for Android build (#685),0
[CUDA] Enable int64 (#683)* [CUDA] Enable int64* [PYTHON] Fix rpc tutorial with opencl* OK* update,0
Consider variable range information during simplification of tensorize expressions (#674),5
[RANDOM] Init contrib.random Library (#684)* [RANDOM] Init contrib.random library* [RANDOM] Add uniform* [RANDOM] Fix lint* [RANDOM] Add comments and tests* [RANDOM] Fix lint,0
Support rank-0 tensor (#687)* Support rank-0 tensor* fix lint,0
[CI] Enable llvm in CPU test (#688)* [CI] Enable llvm in CPU test* fix llvm,0
"Port build_module.py to C++ (#667)* Port build_module.py to C++* Fix lint errors* Fix more lint errors* Fix more lint errors* Fix more lint errors* Fix build error* Implemented style fixes* Fix lint errors* Added function to construct target from stringlower now returns array* Fix lint error* Implemented review changes - style & Target options -> std::vector* Fixed lint, argument alignment and added unit test* Changed test to target LLVM, fixed sign compare warnings* Reverted unit test to CUDA, changed Jenkinsfile to enable GPU for C++ tests* Slight change to Jenkinsfile* Changed build_module test from CUDA to LLVM* Added function var() to construct a Var instance.Changed implementation of LLVMEnabled()* Reverted Jenkinsfile",0
[CODEGEN] add callback post proc for opencl (#692),1
[CODEGEN] add fp16 and fp64 enable pragma for opencl (#697)* [CODEGEN] add fp16 and fp64 enable pragma for opencl* fix style,0
Fix long for windows in cuda (#700)* Use long long for platforms where long is 32 bits (like windows).* Make sure scalar chars are signed.* Re-add NOLINT marker.,0
"1) Make unroll code reusable 2) reduce non-determinisim in CanonicalSimplify (#701)* 1) Refactored some parts of the unrolling code into their own methods so we can reuse unrolling functionality in other parts of the code. E.g., to explicitly unroll loops with count of 1 when they are programmatically created.2) Reorder based on top operator before resorting to pointers, which causes non-determinism.* Fixed lint errors",0
Simplify expressions early on (#702)* Simplify expressions early on* fixed lint errors,0
Make duplicated function name checker working (#705),5
fix cudnn output shape (#708),0
Halide -> HalideIR (#698),5
"removed non-determinism from CanonicalSimplify (#704)* 1) removed non-determinism from CanonicalSimplify2) added couple of testcases for CanonicalSimplify* Use IRDeepCompare instead of comparison of string representation* Give a warning (instead of fatal error) when two ""ComExprEntry""s are equal",0
"During tensorize, call Simplify on algorithm and intrinsic definitions before CanonicalSimplify. This will prevent a number of false tensorize mismatches. (#718)thanks, this we can use this solution for now",2
Added a regression test for #696 (#720),1
Update metal_module.mm,1
Fix dependency problem of reducer condition (#712) (#721)* Make duplicated function name checker working* Fix dependency checking problem for reducer condition (#712); add test* Fix dependency checking problem for reducer condition (#712); add test* Specify R to be computed inlined,0
[CODEGEN] enable static handle cache (#723),5
"[CODEGEN] update codegen for vector operation (#711)* [CODEGEN] update codegen for vector operation* update comment, fix for metal",0
[ROCM] MIOpen contrib for convolution kernels (#722)* fist working miopen support* do FindFwdAlgo during build time* fix lint* update doc string* import topi after checking if rocm is enabled* add miopen namespace* fixed descriptor overwrite bug* add use_miopen option* fix lint* better miopen option handling* fix typo* fix options handling,0
[TOPI] 1bit dense operator on x86_64 (#629)* add x86_64 target* add binary dense operator* rebase* improve schedule* remove x86 target* improve schedule,1
update dmlc-core (#728),1
[TOPI] add extern schedule for cudnn and miopen (#724)* add extern schedule for miopen* fix comment* optionally dispatch to miopen from topi* fix lint* check if current target is None* use generic dispatch for rocm conv2d* fix lint* fix workspace bug* remove blank line* remove blank line* remove blank line,0
[TOPI] CUDNN integration (#730)* add target.libs to target str representation* integrate cudnn into topi cuda* append target.libs to target.options,1
[TOPI]Support dim-0 tensor in topi broadcast/reduce (#731)* support dim-0 tensor in topi opsrevert transform* revert,5
"[SCHEDULE] New Reduction Mode for Tensorize (#727)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.",0
"Re-organize the test cases for tensorize. (#736)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.* Re-organize the tensorize test cases, and add a new case for none-resetmode.* Fix a typo.* Delete the unit case because merged it into test_schedule_tensorize.py already.",0
Let CUDNN choose the best algo (#734)* use cudnn findalgo to choose the best algo* fix lint,0
"enable partition const loop with build flag (#732)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop",1
[WEB] update web runtime to latest emcc (#742),1
"Support automatically Name Loop Variable in IRBuilder (#716) (#741)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop* [IRbuild]Support automatically Name Loop Variable in IRBuilder (#719)    * add idx_num in class* using typical index [i, j, k] first, then i_suffix* keep inputs names* fix lint* improve comment of name* fix lint",0
[CONTRIB] cuBLAS integration (#744)* add cublas support* integrate cublas to topi dense* add cublas error check* minor fix* fix lint* remove topi import from contrib unittest,0
"[CODEGEN] fix & improments in codegen (#745)* [CODEGEN] update codegen for vector operation* update comment, fix for metal* fix some bugs in codegen* use 'restrict' in every argument* fix* fix",0
modified schedule_dataflow_rewrite.cc to fix Stale Tensor during Dataflow Rewrite #738 (#747)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op,0
[CONTRIB] rocBLAS integration (#751)* rocblas integration* fix include* fix lint,0
correct conv2d workload for resnet18 (#750),5
[CODEGEN] use charp for voidp (#753)* [CODEGEN] use charp for voidp* fx,5
"[SCHEDULE]Improve bound deduce for loop partition (#743) (#755)* [SCHEDULE]enable partition const loop with build flag (#719)    * enable partition loop with build flag    * add a testcase, and modify LoopPartition related cases*     * add document for split_const_loop* [IRbuild]Support automatically Name Loop Variable in IRBuilder (#719)    * add idx_num in class* using typical index [i, j, k] first, then i_suffix* keep inputs names* fix lint* improve comment of name* fix lint* [SCHEDULE]Improve bound deduce for loop partition (#743)    * add divided checking when deducing    * related testcase* fix* * transform LE and GE first* remove is_equal* modify testcase for edge cases checking* * fix comment* * fix lint* * apply transformation form LT -> LE, GT -> GE* * fix lint* simplify code and testcase* add negative co-efficient case* More complicated cases* add testcase* simplify testcase* comment case for now* fix testcase",0
[PASS] StorageRewrite Fold Inplace op storage when possible (#759)* [PASS] StorageRewrite Fold Inplace op storage when possible* update comment to fix typos,0
[TUTORIAL] Improve opt_gemm tutorial (#757)* Improve opt_gemm tutorial* Addressed comments,1
[PASS] Improve loop partition to remove un-necessary warning. (#766)* [PASS] Improve loop partition to remove un-necessary warning.* fix comment,0
small fixes on docs (#769)* small fixs on docs* add IR output after parallelization,0
[PASS] Fix storage rewrite merge rule for special tag memory (#770),0
"[INTRIN] enable popcount on cuda, opencl, metal (#774)",5
[TOPI] Upsampling op support (#772)* add upsampling cpu op* add upsampling gpu schedule* add doc for upsampling opadd more doc* cleanup upsampling test* add doc* fix lint* fix lint* fix lint* remove unused import* remove skimage dependency* remove skimage import* remove schedule_upsampling,0
[LLVM] Enable same target option in JITModule (#778)* [LLVM] Enable same target option in JITModule* not set mcpu explicitly,5
Fix lib64 not found error when building with cuda for OSX (#782),0
try to fix test (#784)try to fixfix,0
[CODEGEN] fix vector conversion for opencl (#783)* support more argument type in depthwise_conv2d* mark all pointer as 'restrict' & fix vector conversion for opencl,0
[TOPI] add schedule for ARM Mali GPU (#786)* add schedule for ARM Mali GPU* fix lint* fix lint,0
fix (#788),0
fix mali topi for python3 (#789),0
[TOPI] Basic x86 schedules (#775)* add basic x86 schedules* parallelize & vectorize batchnorm + relu* fuse conv into bn + relu* move rc loop to outer* add nhwc conv* change weight layout to hwcf* conv + bn + relu fusion for nhwc conv* fix conv_nhwc schedule when no fusion* clean up default parallel schedules* simplify elemwise parallel* fix elemwise parallel for batch == 1* update nhwc conv test* fix and add comment* fix lint* remove redundant import* remove default multithreading for some ops* remove default multithreading for global pool,0
fix the description of create_shared (#793)The type of parameter options should be a str list.,0
Additional mali target support (#794)* Add Mali target support to tvm.target.create* Add Mali target support in codegen,1
simplify expr in get_const_tuple (#795)* fix upsampling output shape* simplify expr in get_const_tuple,0
Support dump ir for each pass (#693) (#791)* Support dump ir for each pass(#693)* expose DumpIR* fix comments* fix comments,0
[WIP] WebGL Backend (#672)Basic WebGL Backend,5
Disable OpenGL test temporary (#801),3
temporarily disable opengl in test_runtime_ndarray.py (#804),3
Update inject_virtual_thread.cc (#806)This compilation warning is fixed.src/pass/inject_virtual_thread.cc:43:19: warning: ‘rw_mask’ may be used uninitialized in this function [-Wmaybe-uninitialized]       if (rw_mask & 2) {           ~~~~~~~~^~~,0
Update setup.py (#803)fix errors when running `python3 setup.py sdist bdist_wheel`,0
[DOC] Generalize the get_started script for beginners with different environments. (#798),5
[Compilation Warning Fix] comparison between signed and unsigned integer expressions (#807)The compilation warning is fixed. src/runtime/graph/graph_runtime.cc:392:24: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   CHECK(data_byte_size == size)         ~~~~~~~~~~~~~~~^~~~/mnt/D_DRIVE/work/nnvm_22_Jan/nnvm_latest/tvm/dmlc-core/include/dmlc/logging.h:109:9: note: in definition of macro ‘CHECK’   if (!(x))                                                \         ^,0
[OPENCL] Fix 32bit pointer size in OpenCL runtime (#809),0
"fix #802, create cache based on sugar tensor (#808)",0
Improve gemm tutorial (#800),5
fix gemm tutorial for env that may not have right instruction (#810),0
"[PASS] enhance storage_rewrite to support different dtypes for unified buffer (#805)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op* repare address offset for different kinds of dtype* bc* aaa* aaaaa* repare address for different dtypes* remove nonsense files* add whitespace of line 581* use base alloc elem_type* enhance the testcast of basic buffer is 64bits,32bits,16bits,8bits* use extends[0]->type() as dtype of offset* clear program writes",0
[CODE COMMENT] Comment BindBufferScope (#815),5
Update cross_compilation_and_rpc.py (#816),1
fix rpc tutorial (#818),0
[OpenGL] Let OpenGL texture always be 1024 x nrows. (#817)* OpenGL texture is always 1024 x nrows.* Address review comments.,1
minor tweak of the runtime doc to fix some grammatical and expression issues (#828),0
"support using pointer with an original offset (#826)* when there is no intrin func, using body for initialization. For issue 714.* Refine code per review comments, and add a test case.* Fix lint issues.* Re-organize the tensorize test cases, and add a new case for none-resetmode.* Fix a typo.* Delete the unit case because merged it into test_schedule_tensorize.py already.* always use new tensor in its stage when rewrite for cache read* revert previous changes to sync up with master* support using the ptr with an original offset* update test case and fix CI error",0
add the link for how to setup rk3399 opencl driver (#827),1
[TIMER] Enhance time evaluator to create multiple results (#830),5
[DEBUG] get_node_output : To retrieve out put of any node - for debug purpose. (#820),0
Add type code and bits to AllocWorkspace. (#831),1
"Porting schedules (except convolutions) to C++ (#763)* Ported injective schedules to C++. Added some elementwise ops.* Fix lint errors* Added reduction ops and schedules* Fix lint errors* Fix lint errors* Fix lint errors* Added transform ops* Fix lint errors* Fix lint errors* Added softmax, log_softmax, leaky_relu and flatten ops.Fixed issue where TVM_DECLARE_INTRIN_UNARY used the PureExtern flaginstead of PureIntrinsic.Added softmax CUDA schedule.* Fix lint* Fix lint* Added binary_dense, batch_norm_inference, dense, dilate, scale_shift_*,global_pool and pool ops.Extended pad to allow specifying pad_value.Fixed issue where pad would throw if padding was zero in all dimensions.* Fix lint* Fix lint* Added CUDA schedules for dense, pool and global_pool* Added extern schedules for generic and CUDA* Fix lint* Added x86 binary schedules* Fix lint* Added rocm dense schedule. Added rocBLAS and cuBLAS support to dense ops* Added pow ops. Added x86 default and injective schedules* Fix lint* Fix lint* Fix lint* Fix lint* Fix lint* Fix indent* Removed schedules directory* Changed left_shift, right_shift to operators. Changed pad_value in pad() to remove pointer usage* Fixed usage of pad in nn/pooling.h. Fixed declaration of operator>>* Fixed comments for shift operators* Added comments to utility functions* Added TOPI C++ library, exporting broadcast_add op* Fix lint* Share libinfo.py with TVM* Fix lint* Add other broadcast ops* Fix lint* Fix imports in topi* Fix lib names* Fixed build issue where windows builds don't apply correct definitions* Removed TVM_EXPORTS from topi library* Attempted CI build fix* Add topi lib to tvm_multilib* Fix Jenkinsfile* Added TOPI build target to Makefile* Fix nn op namespaces.* Fix lint* Renamed TOPI lib to libtvm_topi* Removed _ffi/base.py* Remove _ffi from topi, now shared with tvm.* Make libtvm_topi loading optional* Fix compiler warnings* Fix lint* Fix lint* Fix lint* Fix build error by making new libs argument to Target optional* Added C++ Target type interop. Added registration of remaining C++ ops and schedules. Added test of broadcast ops* Fix lint* Fix lint* Fix compile error* Fix compiler warnings* Fix compiler warnings* Fixed int vector interop. Fixed argmin incorrectly invoking argmax. Fixed corner case in default schedules of attempting to fuse 0 length axes. Added tests for reduce ops.* Refactored reduce builders* Fixed typos in topi.cc. Added basic test.* Fixed padding size error. Added dense, dilate, pooling tests* Fixed issue where clip would output a different dtype to the input. Added split_sections op to cover the other mode of the python split op. Added tests.* Changed extension type numbers to avoid clash with NNVM* Fix lint* Fix compiler warnings* Removed use of std::vector from the public TOPI API* Fix lint* Add TOPI C++ tests to CI* Fixed detail namespacing. Improved comments.",0
Fix Jenkins pipeline (#835),0
[TOPI] Fix compiler warning in topi cpp (#837),0
fix opengl runtime to use OpenGL/gl3.h for macOS (#833)* fix opengl to OpenGL/gl3.h for APPLE* use glfw3 to include gl.h header,0
fixed #841 (#845)* Update workspace_pool.cc* Update workspace_pool.cc,0
"[PASS] Improve storage rewrite(#846) (#847)* fix #802, create cache based on sugar tensor* [Pass] Improve storage rewrite* fix ci* fix comment* fix comment",0
[TOPI] Fix cpp library dependency on MAC (#852),0
[RELEASE] Release note for 0.2 (#853),5
copy intrinsic now can include typecast (#855),2
Update HalideIR submodule to include TVM_STATIC_IR_FUNCTOR_REGISTER (#857)* Update HalideIR commit to include TVM_STATIC_IR_FUNCTOR_REGISTER* Fix HalideIR to point to the right commit* Add missing using to C++ TOPI nn.h* Update HalideIR to include compiler error fix* Fixed error where broadcast_to fails if shape is tuple of IntImm* Change get_const_int to support int as input,0
[BACKEND] Vulkan Runtime and SPIRV Codegen (#861)* [BACKEND] Vulkan Runtime and SPIRV Codegen* fix doc,0
Fix boolean type (#862),0
TOPI C++ bugfixes (#864)* TOPI C++ bugfixes* Fix lint,0
Change HalideIR back to most recent commit (#865),4
[TOPI] Add compute for more operators (#849)* [TOPI] Add compute for more operators* Remove device except llvm* Address comments* Remove matmul compute* Add outtype to boolean operator* Address coments,1
Note about CodeGenC's Range of Use (#866),5
"Fix Vulkan Build, add tanh to llvm instrinsic, fix halideIR (#868)* Fix Vulkan Build, add tanh to llvm instrinsic, fix halideIR* fix llvm tanh",0
Fix bugs with C++ TOPI flatten and relu (#869)* Fix bugs with C++ TOPI flatten and relu* Added regression tests. Fixed typo in CMakeLists.txt. Fixed topi cpp import removed.,0
"enhance pragma to support single point copy  (#863)* modified schedule_dataflow_rewrite.cc to fix losing tensor problem* modified schedule_dataflow_rewrite.cc for lint scan* modified schedule_dataflow_rewrite.cc for lint scan* using tensor's value_index to index output of stage op* repare address offset for different kinds of dtype* bc* aaa* aaaaa* repare address for different dtypes* remove nonsense files* add whitespace of line 581* use base alloc elem_type* enhance the testcast of basic buffer is 64bits,32bits,16bits,8bits* use extends[0]->type() as dtype of offset* clear program writes* enhance inject_copy_intin to support of pragma stmt with no loops* fix cpplint errors* fix cpplint error of !* enhance detectLinearEquation to support with no loop vars* fix cpplint errors",0
Run C++ TOPI tests in Jenkins build (#870)* Added +x permission to task_cpp_topi.sh. Added C++ topi tests to Jenkinsfile* Fixed test_topi_math.py* Minor style fix,0
Try fix cpp topi test (#871)* Try fix cpp topi test* move cpp test to another stage* update,0
[CODEGEN] Fix vector element access in metal (#872),0
Fixed namespacing issues in schedules (#873)* Fixed namespacing issues in schedules* Fixed compile error,0
support to keep trivial loops with extent of 1 (#877),5
[CONTRIB] add peak test (#878)* add peak test* fix error for lanes=16* update doc* fix names* fix names,0
[WIP] Add OpenGL topi. (#836)[TOPI][GL] OpenGL topi.,1
[PASS] Enable StorageRewrite before virtual thread lowering (#880)* [PASS] Enable StorageRewrite before virtual thread lowering* update* fix testcase,0
[TOPI] conv2d avx (#883)* conv2d schedules for Intel CPU (AVX2 & AVX512)* fix lint* remove override register,0
[PASS] Prepare storage rewrite for unified buffer (#885)* [PASS] Prepare storage rewrite for unified buffer* more comments,4
allow fallback path to non imagenet workloads (#886),5
Update jenkins (#890),1
ignore model option in target (#889),5
"[TOPI] Initial NHWC layout support (#882)* add 4 dim softmax* update for NHWC layout* remove layout param from softmax* fix typo* minor fix to poolsupport axis=1 ndims=5 softmax.add softmax axis* few fix for softmax* fix typo* add more doc* minor doc fix* fix upsampling output shape* fix lint* cleanup softmax* minor fix* raise exception instead of assert, handles negative axis* check axis after axis transformation",0
"Convert BuildModule to use TVM node system (#879)* Make python BuildConfig serializable/deserializable to/from string* Make C++ BuildConfig serializable/deserializable to/from string* Revert ""Make python BuildConfig serializable/deserializable to/from string""This reverts commit a5e1fb3ff63a161cc0d63475d2a32816cc4c3666.* Revert ""Make C++ BuildConfig serializable/deserializable to/from string""This reverts commit ec0c2c54543050fe6f264d06eebff33dee70370b.* Converted BuildConfig to use TVM node system* Fix lint* Fix lint* Added code to set node attributes through the C API* Fixed bug in build_config()* Fix lint* Fix lint* Fix test errors* Reduced scope of node __setattr__ to apply only to BuildConfig* Fix lint* Fix lint* Changed python BuildConfig to be immutable, with values set once on construction.* Fix lint* Fix C++ test* Fixed BuildConfig setting python-side args* Fix lint* Removed dependency on reflection.cc to construct BuildConfig (allow use in runtime library)* Fix lint* Revert ""Fix lint""This reverts commit 16ed6d7a1ca5e551b035bad46e8361ea487cd45b.* Revert ""Removed dependency on reflection.cc to construct BuildConfig (allow use in runtime library)""This reverts commit 43817c97a2ee045791e0c031d962fa97636ce8f6.* Avoid accessing BuildConfig when using runtime lib* Fix missing import* Fix error running under cython (root cause: node handle is not valid until after __init__ has returned, so cannot call __dir__ during __init__* Fix error where BuildConfig._node_defaults was not copied in build_config()* Fix lint* Fix lint* Fix lint* Fix lint* Add comments to python BuildConfig",0
"[Documentation Changes] Parameter description change for reduction apis (#881)* [Documentation Changes] Parameter description change for reduction apiThe parameter description is updated for max, min, argmax and argmin* Lint changes on patch-3 of reduction.py",1
[TOPI] Add winograd for mali (#898)* add winograd for mali* fix lint* add padding* fix comment,0
[Graph Debug] bug fix (#897)Need to break after executing intended operation (not before).,0
Change CodeGenCPU::GetPackedFuncHandle to generate global variable with InternalLinkage. (#901)Emscripten seems to not have done initialization properly.,4
Add dense base scheduler (#887)* Add basic dense scheduler* Revert to put back cpp dense registration* Fix lint,0
Set total memory of emcc module to 1GB (#906),5
[TOPI] update c++ pool and softmax (#905)* update c++ pool and softmax* clean up reduce axis,1
Add UIntImm to select rewrite (#909),1
add missing inline (#910),1
[RUNTIME] Support nop (#913),5
[RUNTIME] More reliable runtime only detection (#914)* [RUNTIME] More reliable runtime only detection* fix lint,0
[SCHEDULE] Add factor_axis to rfactor (#895),1
[PASS] Support buffer reuse for different types (#891)[PASS] Support buffer reuse for different types,4
"[RUNTIME] Refactor extension type handling, now it is header only (#924)* [RUNTIME] Refactor extension type handling, now it is header only",5
"Revert ""[RUNTIME] Refactor extension type handling, now it is header only (#924)"" (#925)This reverts commit 12d15704d7f5d30cff7540f1fd16be64c6baca68.",5
[EXT] Allow easy extraction of extern module (#926),5
Fix a typo for function registration (#927),0
MXNet NDArray bridge. (#930)* MXNet NDArray bridge.Support convert a tvm Function as MXNet's async NDArray function.* fix lint* update comment,0
Append null terminator when converting JS string to c string. (#931),5
TVM SGX (#919)* Update DMLC core most recent version* Modify runtime to minimize io when building for SGX* Add SGX example app* Prefer streaming versions of packed_func function,1
Fixed a g++ explicit constructor compatibility error for unordered_set. (#935)* Fixed a g++ explicit constructor compatibility error for unordered_set.* Change std::unordered_set<std::basic_string<char>>() tostd::unordered_set<std::string>().,0
Name all the lock guards. (#938),5
Add test case: Create a static WebGL library and run it in the browser. (#932)* Add test case: Create a static WebGL library and run it in the browser.* Add documentation for loadModuleFromFile* Modify emscripten.createjs,1
add exclusive mode for rpc server (#941),1
Fix compiler warnings (#939),0
Spelling mistake corrected (#945),5
Better error message handling for contrib (#946)* Better error message handling for contrib* fix lint* fix testcase* fix test,0
remove the pragma primitives for better performance when the threads are binded (#949),4
Fix a typo for Target class (#951),0
Add the equivalence of graph_runtime.py in tvm_runtime.js (#950),1
[RUNTIME] Stream API  (#953),5
[CODEGEN] Fix alignment generation (#955),0
explicit import testing (#956)* explicit import testing* Enable init api for extension modules,3
Update function.py,1
Small refactor for clarity in arraycopyfromto (#960),5
SGXify graph runtime (#937),5
prevent starting of RPC server w/o RPC support (#962)* prevent starting of RPC server w/o RPC support* fix indent,0
Add SGX runtime (#963),1
MPS conv (#822),5
enhance access_ptr that args can support Expr (#970),5
Yolo2 operators (#911),5
Use single-threaded SGX parallel (#975),5
Assert dont crash on null strides (#976),5
[RUNTIME] Better scalability for multi-thread parallelization of CPUs (#971),5
[IOS] Improve the iOS RPC with exclusive filesys lock (#981),2
fix keeping trivial loop (#982),0
prevent aggressive unrolling in vthread (#983),5
Bug fix for Android platforms (https://github.com/dmlc/tvm/pull/971) (#986),0
[FFI] Fix global free destruction (#985),0
[CYTHON] Correct backtrace print for python3 (#989),5
"[RUNTIME] Update graph runtime to rely on smarter planner, add get_input (#990)",1
[BUILD] Windows build with cuDNN support (#999),5
[PASS] Add VerifyMemory pass and test cases (#410) (#993),1
Updated documentation error (#1001),0
Pluggable Thread Launching Mechanism (#991),2
[CONTRIB] windows compatiblity (#1009),5
[LANGUAGE] Verify Compute with respect to Reduce operations (#1006),5
Update contributor guide (#1010),1
[PASS] Fix reuse small buffer in storage rewrite (#1012),0
Simplify enclave lifecycle management (#1013),5
file include/tvm/logging.h from AutoTensorize work (#1015),2
Documentation issues fixed (#1016),0
[RUNTIME] More reliable thread enumeration (#1017),5
Implement C++ registry to back Python target.generic_func (#892),5
JVM NDArray fatal exception fix (#1022),0
[RUNTIME] better parallel launcher and task distribution (#1026),5
"Fix the issue #1033 (#1037)* Fix the issue #1033fix the issue #1033 ""converting to ‘const std::unordered_set<std::basic_string<char> >’from initializer""* Fix the issue #1033fix the issue #1033 ""converting to ‘const std::unordered_set<std::basic_string >’from initializer"".",0
Fix the issue #1036 (#1040),0
Fix fatal error <CL/opencl.h: No such file> when building with CMake (#1045),0
[CYTHON] Fix exception propagation for cython3 (#1046),0
Fix verilog testcase (#1047),0
LLVM 7.0 support (#1048),5
[CONTRIB] Patch nnvcc to generate error when build the empty result (#1049),0
[SCHEDULE][PASS] Enable Warp memory and lower to shuffle (#1050)* [SCHEDULE][PASS] Enable Warp memory and lower to shuffle* OpenCL dispatches for now to intel shuffle,4
Update softmax.h (#1057),1
[TOPI] Overload operators of Tensor when TOPI is imported (#1029),5
Only warn when unable to find a graph input (#1052),5
[TOPI] PReLU Support (#1008),5
fix Android build w/ threading_backend (#1059),0
fix warning (#1041),0
Add Community Page (#1063),1
delete init part when keeping trivial loop (#1031),5
[DOCS] Add logo and tracker to docs (#1064),1
[WIP]    Linux/Android native deploy (#980),5
[PASS] More reliable error message for lower warp (#1065),0
[DOCS] Try upgrade build (#1066),2
[DOCS] Code quality comment (#1069),2
[TOPI] Enhance Conv2D for More Data Type (#922),5
Fix an issue in ReplaceDataFlow for issue 1043 (#1062),0
preserve input option order (#1068),5
Add phisiart to code owner of webgl module (#1070),1
support string option when create cuda/rocm/... target (#1071),5
[TOPI] Fix x86 schedule for conv out_dtype (#1072),0
Move BuildConfig context stack to C++ (#1025),4
mix fix (#1079),0
[TOPI] fix bug of leaky_relu type issue (#1076)Signed-off-by: Janboe Ye <yeyuanbo@xiaomi.com>,0
"[RPC] Refactor, introduce tracker (#1080)* [RPC] Refactor, introduce tracker* [RPC] Change RPC hand shake convention, always get remote key.* fix lint",0
[RPC] Tracker status query (#1081),5
[RPC] support tracker in proxy (#1082),5
add query for shared memory size (#1083),1
[RPC] More robust tracker protocol (#1085)* [RPC] More robust tracker protocol* fix normal rpc,0
"Intel target added, sub group sync added (#1084)",1
[RPC] safe destructor when remote server get killed (#1086),5
"Update halideIR, add more device query for shared memory (#1087)",1
Make target and build module more pythonic (#1089),5
[RPC] callback option rpc server starts (#1092),5
add device name to context attribute (#1090)* add device name to context attribute* update for other backends,1
Add SGXModule (#1019),1
Fix device name and Issue #874 (#1096),0
[PASS] More simplifier for mod and div (#1100)* [PASS] More simplifier for mod and div* fix testcase,0
Generate Lower Bound Conditions for issue 1014 (#1091),5
RPC session connection request information persist app locally (#1098),5
[RPC] LocalSession to provide RPCSession back by local env (#1102),5
[TOPI] LRN & L2norm Operator (#1051),5
Fix metal backward compatibility (#1105),0
Improve SGXModule (#1104)* Improve SGXModule* Address code review comments,1
[RUNTIME] Fault tolerant vulkan init error (#1107),0
register depthwise conv2d as generic function (#1108),5
[RPC] Fix tracker fault handling (#1109),0
Remove cython init messaging. (#1110),4
pickle memoize no longer print message (#1111),5
"[IR] Change pragma  convention, enable pass unroll option via pragma (#1112)* [IR] Change pragma scope convention, enable pass unroll option via pragma* add coverage test* add explicit unroll as option",1
Add issue template  (#1118),1
Update ISSUE_TEMPLATE.md,1
Update ISSUE_TEMPLATE.md,1
Update ISSUE_TEMPLATE.md,1
Add SGX random engine (#1113),1
[PASS] Remap thread axis. (#1122),4
Generalize pooling to support arbitrary layout (#1103)* generalize pool2d to arbitrary layout* explain more the layout support for pool* allow missing factor size for pooling* explain what factor size is used for* fix typo* name idx -> axis,0
"Expose tvm.ndarray.empty in doc. (#1125)Expose tvm.ndarray.empty which has already been implemented, just not yet documented.",5
enhance cache write to support multiple tensors generated by ONE computeOp (#1042),5
add two more device properties (#1124),1
[PASS] Revert the change of intel gpu warp index (#1127),4
[MAINTAINER] Add Nick Hynes to CodeOwner of SGX (#1128),1
[TOPI] parallel schedule improve for x86 & layout_transform support (#1130)* add layout_transform. add schedule support for ndim>=5 for x86* fix lint,0
"[TOPI] support dilated conv2d and depthwise_conv2d (#1129)* support dilation in conv2d and depthwise_conv2d* handle dilated conv in extern libs (cudnn, miopen)",5
Update README.md,1
fix x86 pooling topi schedule (#1132),0
fix default out type of depthwise conv2d (#1134),0
[DOCS] Integration guideline (#1135),2
[DOCS] API doc update (#1136),1
register conv2d_transpose as generic function (#1137),5
[Metal] Correct handle warning info (#1140),5
CPP support for region and reorg operators (#1115),2
Update jenkins file first to use sort (#1147),1
fix relu tests (#1149),0
register loadbinary_hip (#1151),5
"[CODEGEN] Enable cross compile of AMDGPU without rocm, update rpc  (#1154)",1
[APP] ROCM RPC (#1155),5
Update README.md,1
make ThreadPool thread local (#1152)* make ThreadPool thread local* add return,1
Add ssd op with ir builder (#1095),1
AVX schedule for conv_NCHW[x]c (#1143)* add conv2d_NCHWc compute template* add conv_NCHWc compute decl and schedules* allow default avx schedule* fix lint* remove unused schedule* remove import nnvm.reg* pass schedule object to compute and schedule,0
Delete adding a invlid path from Dockerfile.gpu (#1160),1
Fix error during running on nvptx with cuda9 (#1162),0
"Add count_include_pad support to AvgPool (#1163)* Add count_include_pad support to AvgPool* Fix python_cpp/test_topi_pooling.py* Change auto to explicitly type, and fix format.",0
[llvm] fixed issue with llvm 5 vs 6 (#1167),0
#1159 bug fixed (#1164)* #1159 bug fixed* #1159 #1164 fixed,0
[TOPI] flip (#1161),5
[TOPI] add take (#1158),1
"fix wrong type declaration of float64 ""log"" in intrin_math.py (#1169)",0
Release 0.3 (#1171),5
barrier fence added for warp mem (#1174),1
change version number (#1175),4
Added equality check and upgraded concatenate op (#1172),1
Issue in nnvm is fixed (#1176),0
axis bounds checking in split (#1178)* Add kazum to contributers* Add axis bounds checking in split,1
Fix build break on Windows. (#1179),0
ok,5
Initial commit,5
Update README.md,1
Update README.md,1
Update README.md first init,1
[OP] Finalize Op registry,5
Add TShape and Tuple to nngraph,1
Add unittest,1
checkin dfs visit from min,5
[GRAPH] checkin the constructor of indexed graph,5
check in pass,4
change project to NNVM,4
[PASS] Add save/load json (#1),1
[SYMBOLIC] Add symbolic API (#2)* [SYMBOLIC] Add symbolic API* Update Testcase to nnvm,1
[C API] add in C API for symbolic (#3),1
"[PYTHON] Check in a symbolic construction interface in python, start … (#4)* [PYTHON] Check in a symbolic construction interface in python, start add graph API* Graph API",1
Enable use json for graph attr exchange (#5),4
"Change op function pointer to std::function, enable mutation (#6)",4
[PASS] Add order mutation (#7)* [PASS] Add order mutation* A few benchmarks on compose speed,1
Rename shared_ptr<Node> to NodePtr (#8),5
Introduce NodePtr (#9),5
Check in a experimental cython API (#10),5
Create a ctypes cython optional compatible package (#11),5
Make cython compatible with python3 (#12),5
[CYTHON] Make speedup component minimum (#13),5
Updates (#14)* Remove outstanding cython functions* Add in operator overload* Enable JSON to save version,1
"[Pass] Check in infershape, move indexedgraph to graph.h (#15)",4
[Pass] Finish infershape testcase (#16),3
[Pass] enable infer type (#17),4
[Pass] Enable BackwardOp,4
[PASS] Add place device (#18),1
[PASS] add plan memory (#19),1
"[REFACTOR] copy DMLC headers, move operator to example (#20)",4
cleanup (#21),5
"Update Symbol and C API (#22)* Update tuple to be compatible with mshadow* Move set error message to C API* simplify with using* updates to shape inference* Add unnamed namespace to the implementations* [SYMBOL] Enable inference of Auxiliary data, rename list_arguments to list_inputs",0
Update mutate function (#23),1
Enable aux data (#24),5
add scalars,1
updates (#25)* [FIX] Remove extra move* [MEMORY] Add inplace index,0
update (#26)* updates (#1)* add scalars* change format* change inferattr interface* remove scalar* remove warning,1
Change function def to Node ref for more flexiblity (#27)* Remove warning in g++5* Change function def to Node ref for more flexiblity,4
"Revert ""Change function def to Node ref for more flexiblity"" (#29)",4
[PASS] Add gradient pass (#28),1
[NODE] Move op inside node attribute (#30),2
Enable copy on write in graph attrs (#31)* [INFER] Enhance backward op policy* [SYMBOL] add list inputs* relax graph attr to enable copy-on-write,1
"Bugfix plan memory, fully support mxnet executor (#32)* [PASS] include knullop info in plan memory* Bugfix plan memory, fully support mxnet",0
Place device now compatible and tested (#33),3
Enable copy over behavior of attributes of automatic created vars (#34),5
Add travis test scripts (#35),1
overview (#36)* overview* fix,0
Update README.md,1
trigger travis (#37)* trigger travis* fix* chomo* make* squash,0
travis mac (#38)* chomo* try mac options,5
Update symbolic.cc,1
add cmake with windows (#40),1
"Enable Alias, refactor C API to reflect Op Semantics (#41)* Enable Alias, refactor C API to reflect Op Semantics* add alias example",1
Fix comments while reading the codes. (#42),0
Strict gradient boundary check (#44),5
ApplyPass -> ApplyPasses; Refactored infer pass; (#43)* ApplyPass -> ApplyPasses; Refactored infer pass;* lint fix,0
Fix inplace. Only permit shared when shape and dtype is the same (#45),0
[API] Change attr to explicit name set_attr (#46),4
update doc (#47),1
update test (#48),1
[Compatiblity] backward compatible to mxnet json (#49),5
Allow libaray path to be configurable (#50)* Allow libaray path to be configurable* Enable partial shape inference result to be passed via shape* fix python3* disallow copy assign in index,0
"[Infer] More robust inference, support backward inference (#54)",5
Enable control deps in API (#55)* [SYMBOL] support control deps in API* enable more generic tuple list* fix* fix,0
fix typo (#56),0
[OP] Enable register via match tag (#57)* [OP] Enable register via match tag* more docs on usage,2
Add shape backward inference (#58),1
"[SYMBOL] Change list_input->list_input_names, add list_input_variables (#59)* [SYMBOL] Change list_input->list_input_names, add list_input_variables* fix",0
Fix attr (#62),0
Update README.md,1
Fix Plan memory when multiple inplace option is available (#67),0
[PlanMemory]Avoid inplace for kNullop requests (#68),5
error info (#69),0
Change license file to be detectable by github (#72),2
add rpow & fix 'Number' to 'Number_' (#76),0
also save graph attributes (#78),5
Enable optional dependency memory and attr hint (#79)* Enable optional dependency memory and attr hint* fix travis,0
add make/config.mk; add nnvm-fusion into plugin as a submodule (#80),1
do hint insertion after aggregation (#81),2
update nnvm-fusion (#82),1
enable shape inference with hint func (#84),5
Fix json parsing behavior (#83),0
"Remove backward index, use gradient guessing instead (#85)* Remove backward index, use gradient guessing instead* minor fix* bugfix",0
sync logging improvement from dmlc (#86)* fix glog* sync logging improvement from dmlc,0
fix (#87),0
[PASS] Make placedevice compatible with backward op (#88),4
Add list attr recursive (#89)* Add list attr recursive* fix,0
fix error message (#90)* fix error message* fix,0
add amalgamation support (#91)* add amalgamation* update target name,1
"Fix warnings from Logging, enable Plan memory to take external memory (#93)* Fix warnings from Logging, enable Plan memory to take external memory* fix external memory id* fix graph",0
add FSetInputVariableAttrs (#92)* add FSetInputVariableAttrs* rename,1
enhance shape inference. allow in complete shape (#94),5
Handle empty tuple (#95),5
fix mxnet amalgamation (#96),0
Fix the build to generate .d file (#99),0
simplify makefile,2
"Remove dmlc headers, add optional DMLC_CORE_PATH variable for build path (#102)* Remove dmlc headers, add optional DMLC_CORE_PATH variable for build path* travis.yml doesn't need to clone dmlc-core anymore",1
Update overview.md (#103)misstype fix,0
Update Makefile,1
Update amalgamation.py,1
fix,0
remove limit on simple type,4
add symbol::GetChildren (#104),1
LSTM Memory Allocator Fix #5035 (#105)* Imbalance version of shared pool during plan memory* Bug fix for no shared_pool case* Auto search and updated shared mem pool* Cleanup unused code* Cleanup logging code* Add unit test for shared storage* Remove shared pool in PlanMemory. Fix lint warnings* Fix lint warnings* Use reference instead of ptrs,0
Enable shape hints during infer_shape pass (#107)* enable shape hints during infer_shape pass* fix comment,0
change to match by both op and name (#109)* change to match by both op and name* add zero prop for non differentiable ops* Update infer_shape_type.cc,1
CMake/make adjustments and warning fix (#106)* CMake/make adjustments and warning fix* Fix warnings,0
fix (#111),0
Rename TShape::index_t to dim_t and change to int64_t (#114)* Change TShape::index_t to int64_t* Add comment* Make Tuple::Save&Load dtype generic* trigger update* Fix lint* Fix comment* Change index_t to dim_t* Remove legacy index_t,0
Fix for build (#117),0
create symbol from nodeattr (#116),5
Handling duplicate NodeEntries on the edge of the gradient graph (#122)* Handling duplicate NodeEntries on the edge of the graph* Fix docs and segfault* Suggestions from review* Added attr_parser check,0
delete dependency node (#121),5
fix plan memory add inplace_identity option (#124)* fix plan memory add inplace_identity option* comment,0
enable negative values in TShape (#125),5
Fixed make no_optimization runs indefinitely when run the memcost example provided  (#126)* fixed make no_optimization runs indefinitely when run the memcost example provided* format plan_memory.cc,0
Fix a spelling mistake (#128),0
Fix 2 spelling mistakes (#129),0
add kDynamicStorageID to plan mem (#127),1
Fix error during loading library in python3 (#130),0
remove unused variable (#131),4
[ATTR/SYMBOL] Expose op_name attr to python (#132)* [ATTR/SYMBOL] Expose op_name attr to python* fix xcode,0
allow variable composition (#133),5
fix (#134),0
Fix a spelling mistake (#135),0
fix (#138),0
add hash function for tuple (#141),1
rm unused variable (#143),5
[LIBINFO] Enable load .dylib on darwin (#142)* [LIBINFO] Enable load .dylib on darwin* Fix makefile,0
Fix a spelling mistake (#136),0
add additional field into node for autograd (#145)* add additional field into node for autograd* check inputs reachable* fix,0
add range to plan memory (#147),1
fix symbol.attr('op_name') (#149),0
Fix infer shape bug (#148),0
declare a type name for each tuple type (#151)* declare a type name for each tuple type* generic way to declare type names for each tuple type* fix lint error* update submodule dmlc-core,0
"[JSON] attr->attrs in node attribute, keep backward read compatible (#152)",5
Address execution hot spots (#154)* Address execution hot spots* extend the reserve logic a bit* FIx more hot spots* Performance: use resize(0) instead of clear() in order to avoid a free/malloc,0
[OP] Initial checkin of nnvm core op folders,5
[TOP] Init dense,5
"[TOP] concat, sigmoid",5
[TOP] Level1 complete (#3),5
improve infer shape/type error message (#4)* improve infer shape/type error message* fix dense infer shape,0
add level2 ops (#6),1
[TOP] Level 3 complete (#7),5
[TOP] complete level2 (#8)* [TOP] complete level2* [TOP] add split,1
[TOP] Rename conv pool parameter back to 2d,5
"[TOP] level4 except argmax/min, correct split (#9)* [TOP] level4 except argmax/min, correct split* [DOCS] Add doc generator for top",1
[TOP] Add comments about optimizable ops (#10),1
[TOP] GraphExecutor (#11),5
[COMPILER] Initial compiler infra (#12),5
"[DLL] Use local dll, not de-allocate function in shutdown",5
"[PASS] PrecomputePrune, add testcase (#14)* [PASS] PrecomputePrune, add testcase* update comment",1
fix segfault when op is unset (#15),0
register softmax (#16),5
"register depthconv, elemwise (#17)* register depthconv, elemwise* use global elemwise schedule for relu",5
"[PASS] PrintGraphIR, SimplifyBatchNormInference (#19)",4
[PASS] PrintGraphIR Join attributes when print ir (#20),4
"[RUNTIME][COMPILER] Formal compiler pipeline, runtime wrapper module (#21)* [RUNTIME][COMPILER] Formal compiler pipeline, runtime wrapper module* more detailed comments",5
"[TOP] Add dense, batchnorm (#22)* [TOP] Add dense, batchnorm* update tvm",1
"[PASS] SimplifyBatchNorm->SimplifyInference, remove dropout (#24)",4
"[TOP][COMPILER] Add expand_dims, change graph_compare to not compare input optionally (#25)",1
[PASS] Improve GraphFuse to include five patterns (#26),4
init mxnet converter (#27)graphbackupupdatefinish mxnet converterfixfix variousadd testsfixadd multi networksuses model_zoofix testsminor fixfix graphfix,0
"[TOP][COMPILER] sum, min, max, transpose, fix dense (#28)",0
fix lint (#29),0
"[COMPILER] GraphHash based cache system, allow dump and query duplicated functions. (#30)",5
[RUNTIME] Minimum runtime module (#31),5
[ATTR] More robust attr parsing (#33),5
"[TOP][Example] register pool, global_pool; add mobilenet example (#32)* register pool, global_pool; add mobilenet example* tests of pool and global_pool* use new API of runtime module* small fix",0
[DOCS] Revamp the docs (#34),2
"[TUTORIAL] Move mobilenet to tutorial, fix precompute_prune (#35)* [TUTORIAL] Move mobilenet to tutorial, fix precompute_prune* Some language improvements",0
"[DOCS][FRONTEND] Modify from_mxnet to also return params, update docs (#36)",1
"[RUNTIME] Remove runtime, rely on tvm only (#38)",4
[FIX] Make master compile,0
update nnvm.runtime to tvm.contrib.graph_runtime (#41),1
"[DOCS] Add save_param_dict, readme (#42)",1
"[TOP] split, reshape, concatenate (#43)",5
[DOCS] Add intro tutorial (#45),1
"[Frontend] Onnx (#40)* init onnxfinish onnx frontendadd onnx testsfix variousbackupuse transformer[Frontend] graph passedadd test forwardtest forwardfix doc and lintfix test graph tuplefrom_onnx now take 2 args, output (sym, params)fix renamefix input namesfix multiplefix lintfix lint check* better doc",0
Merge with pull request #44 (#46)* resnet example merged to imagenet* merge with master,5
[Tutorial] mxnet (#47)* [Tutorial] mxnetupdateadd from_gluonadd to __init__fix tutorial and from_gluonfix doc lintmerge from_mxnetfixfixfix tutorialfixfix header* fix tutorial* fix data* fix,0
[TEST] Checkin test docker and scripts (#48),3
[TUTORIAL] Onnx tutorial (#50)* add onnx* fix,0
[TUTORIAL] use resnet v2 (#51)* use resnet v2* fix,0
revert v1 (#53),5
[TEST] Xavie initialization for benchmarks (#54)* [TEST] Xavie initialization for benchmarks* remove additional line,1
add squeeze (#52)* add transform* fix* update doc* Update tvm,0
[PASS] FoldScaleAxis (#55)* [PASS] FoldScaleAxis* Move FoldAxis to O3* Set unroll to 0 when ready,4
"[DOCS] Add install docs, fix Jenkins (#57)",0
use shorter title (#58),5
Fix docs issue (#59),0
[OP] Conv2d and Depthwise Conv2d for Raspberry Pi (#49)* [TUTORIAL] ImageNet Inference on Raspberry Pi* Update tvm,1
[TUTORIAL] Onnx tutorial (#60)* update* back to color,1
[DOC] add json spec intro (#62)* add json spec intro* fix error* fix name attrs* rename and move to top* address boolean values* update title,0
[TUTORIAL] Deploy the Pretrained Model on Raspberry Pi (#61)* [TUTORIAL] Deploy the Pretrained Model on Raspberry Pi* [TUTORIAL] Improve* [TUTORIAL] Improve* [TUTORIAL] Improve* [TUTORIAL] Improve,5
[COMPILE] More debug message when compile error (#66),0
[DOCS] update docs (#67),1
[GRAPHIR] Print attributes of input (#69),5
[BENCH] Add Benchmark for Rasp (#68)* [BENCH] Add Benchmark for Rasp* [BENCH] Add arg opt-level* [BENCH] Add model choices* [BENCH] Improve,1
[TEST] include coreml tools in docker (#71)* [TEST] include coreml tools in docker* upgrade tvm,1
fix coreml tools (#72),0
Add cuda imagenet inference benchmark script  (#73)* resnet example merged to imagenet* resnet example merged to imagenet* merge with master* merge with master* cuda imagenet benchmakr added* benchmark syntax fixed,0
"[FRONTEND] CoreML (#63)* add coreml* fix bool* fix flatten in fullyconnected* fix duplicate flatten* fix syntax* add tutorial* fix mxnet flatten, fix tutorials* fix flatten issue* fix lint",0
Update README.md,1
Update README.md,1
cuda imagenet inference benchmark timing updated (#158),1
Install nnvm lib and haders. Offer choice to build static or shared lib. (#146),2
[CMAKE] Windows build instruction (#161),2
[RUNTIME] Remove header dependency on private header (#163),4
[FIX] Fix Typo (#164),0
Fix Cmake error on Windows (#160),0
[Frontend] Onnx improvement (#165)* fix recently released layers* fix fc layers with partial infer_shape,0
fix symbol output compose (#166),0
[BUILD] Make the core library compatible with msvc 13 (#167),5
"Fix lint, temporary add amaga back (#170)",0
[TVM] upgrade to generic schedule (#173),5
[FRONTEND] Composed operators (#175)* fix for composed symbol* fix* clean up* fix exception type,0
[OPT] Improve PreComputePrune When Output Is Pruned (#178),5
[FRONTEND] Correct the use of `concatenate` operator (#181)* Correct the use of `concatenate` operator* Optimize,5
Fixed attribute parameter (#184),0
"Change the way to import ONNX model. (#186, #187) (#189)* Change the way to import ONNX model.* Decide ONNX version and change the processing* Check whether onnx has the attribute '__version__'",4
Change as graph.input can be either ValueInfoProto or string (#186) (#192)* graph.input can be either ValueInfoProto or string* pylint,4
[TVM] Update tvm and benchmark script (#196),1
More comment change on bench script (#197),4
fix version comparison (#200),0
[TOP] Support ceil_mode,5
[Frontend] ONNX frontend v0.2 support (#202)* update* fix* use generated onnx model* fix tests* fix lint* remove log filter* add vgg* fix tests* update tests* fix download* fix ci* fix tutorial url* clean cache,0
add softmaxoutput (#207),1
[TOP] add conv2d_transpose (#217)* [TOP] add conv2d_transpose* update tvm* fix pylint,0
Update deploy.md (#219)* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.,1
[TOP] fix weight layout in conv2d_transpose (#220)* update tvm* [TOP] fix weight layout in conv2d_transpose,0
Update deploy.md (#223)* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdAdd an example in c++.* Update deploy.mdUpdate dlpack device and type enumerations.,1
[FIX] Update rasp benchmark example (#225),0
[CMAKE] Update cmake to support OSX/Linux (#228),1
Add conda package (#231)* Make sure to install the nnvm_compiler library.* Add conda packge.,1
unroll condition changed for cuda (#232),4
Correction in C++sample code. (#238)Variable declaration correction.,5
[BUILD] add target_host to compiler.build (#240),1
[FIX] Fix from_mxnet for multiple outputs symbol (#247),0
fix onnx conv2d_transpose loading (#245),0
[CMPL] Add Support for Other Data Types (#252)* [CMPL] Add Support for Other Data Types* [CMPL] Add test* [CMPL] Fix,0
Added support for CoreML Permute layers (#262)https://apple.github.io/coremltools/generated/coremltools.models.neural_network.html?highlight=permute#coremltools.models.neural_network.NeuralNetworkBuilder.add_permute,1
[FRONTEND] Fix mxnet multi outputs (#271)* fix mxnet multi outputs* add test case for multi outputs* fix* fix* fix* fix index* use json hack* fix test cases* fix test cases* fix test cases* fix test cases* fix test cases* fix test cases,0
"API call to get symbol output count (#270)* Symbol __getitem__ using list_outputs() is too expensive, when it only cares about the output count in most cases* Add cython cmake* GetNumOutputs() and __len__ changes per PR comments* set commit for tvm",1
[SYMBOL] Add __iter__ and GetChildren for symbol (#268)* [SYMBOL] Add __iter__ and GetChildren for symbol* [SYMBOL] Fix lint,0
Add support for missing uint types. (#272),1
"Keras Frontend (#273)* vgg16 success* remove six.PY2, use sys.version_info;convert_activation() accepts activation type name(str, e.g. 'relu') as input;* add convert_merge* fix convert_batchnorm;improve tests* fix lint* add numpy-style pad operator* deal with asymmetry padding* resnet50 success* fix pool_convert; xception passes test* update tvm* fix bias error; all tests pass* use > >, not >>",0
keras frontend tutorial (#278)* keras frontend tutorial* fix,0
Add compile library tutorial (#277)* Add compile library tutorial* Clean output* Refactor with sphinx gallery* Refactor* Change title and other minor fixes,0
Update deploy_model_on_rasp.py,1
[GRADIENT] Add backward operator to enable backward graph  (#276)* Update docs* Add backward operator to enable backward graph* Fix testing* Refactor top level1 test code* Fix format* Test* Added zeros ones op* Register fill_like operator* Fix unit test,0
[FIX] make keras frontend py3 compatible (#283),0
Add gradient graph (#280)* Add creating gradient symbol* Fix lint* Address comments* Fix typo* Address comment,0
[SYMBOL] Enable get index from symbol (#286),5
"Make choice of archiver configurable (#288)At present, the build system assumes that the Unix archiv utility 'ar'will always be present in PATH, but this might not be true if a separatetoolchain is being used. This patch makes the build system a bit morecross-compile capable.",5
Add missing cmath header (#287)f9ed337552a26cc55c7c0fb22cd54839ee13f19c added a call to std::logwithout including cmath,1
typo in c_api.h (#290),5
dump lowered IR when debug logging (#292)* dump lowered ir when debug log* avoid calling tvm.lower() twice when not debug,0
"Add __declspec(dllexport) to class Node and class Op if build on Wind… (#296)* Add __declspec(dllexport) to class Node and class Op if build on Windows, so that it can be referenced in some test code in MXNet* correct typo* reuse the macro in c_api.h* revert changes on base.h* one more place to revert* use enclosing extern c block* revert a change caused by copy* remove a space",1
[WIP] Onnx1.0 (#294)* add more op for onnx 1.0* fix syntax* fix lint* fix* update 1.0* fix* update model,0
Upsampling op support (#298)* add nnvm upsampling symbol* add upsampling mxnet frontend* add doc for upsampling op* cleanup upsampling test* minor fix* use schedule_injective for upsampling* upgrade tvm,0
Add support for keras upsampling (#305)* Add support for keras upsampling* Fix code formatting* Fix indentation* Fix indentation round 2* Hide unused parameter* Only enable UpSampling2D since the others are not currently supported by TVM* Improve error messages and code layout,0
remove dtype in model symbol (#310),4
Avoid crash when linear activation does not have alpha and beta defined (#306),5
make shape inference of BatchNorm layout neutral (#301)* make shape inference of BatchNorm layout neutral* refactor to use the axis variable to do BatchNorm shape inference* refactor to use the axis variable to do BatchNorm shape inference* add unittest to the axis param for batch norm shape inference,1
[CI] turn on keras frontend test (#309)* [CI] turn on keras frontend test* fix* using tensorflow cpu version,0
Update frontend for keras 2.1.3 compatibility (#314)* Keras keeps renaming properties. Update frontend for keras 2.1.3 compatibility* Add error message when inbound_nodes is not found,0
[Keras] fix convert_pooling with same pad (#322),0
"[GRADIENT] Register more gradient operators (#300)* Add conv2d max_pool backward op* Added tests* Fix testing* Address comments* Change dot to matmul* Address comments* Break down indicator function* Make greater, less numpy compatible",0
Explain how to generate module library in Quick Start tutorial (#323)* Explain how to generate module library* Small fix,0
[NNVM] : Function header corrections. (#324),5
[TUTORIAL] add tutorial for mali gpu (#313)* add tutorial for mali gpu* update submodule version* fix doc error* fix server error* fix server in test,0
Fix keras frontend elementwise-ops for lists with len>2  (fixes issue #325) (#326)* Added elementwise-add test* Fix typo* Fixed elem-wise ops for lists with len>2,0
fix upsampling fuse pattern (#330),0
Add test for issue #327 (#328)* Added test for adding down-/up-sampled layers* Enabled test for adding down-/up-sampled layers* Normalize whitespace,1
[TUTORIAL] add extern lib tutorial (#331)* add extern lib tutorial* do fix,0
Add CacheItem2Schedule Extension (#338)* add CacheItem2Schedule extension* fix lint* move function position* make cache item visible to frontend,0
explain the lowering process in nnvm.compiler.build (#339),5
[Convolution] Error while importing onnx model with weights. (#345)* [Convolution] Error while imported onnx model has weights.Fix the use_bias based on the input parameters to use bias (Ex: sqeezenet). Ref #336* Review corrections.,0
[FIX] Fix target in tutorial (#347),0
improved empty set definition (#346)fix similar problem like https://github.com/apache/incubator-mxnet/pull/4589/files .,0
"image tranform runtime wrong on python (#354)x = np.transpose(img, (2, 0, 1))[np.newaxis, :]TypeError: an integer is required",0
"[PYLINT FIX]pylint issue fixes (#348)Before Fix************* ************* Module nnvm._baseR: 21, 0: Disallow trailing comma tuple (trailing-comma-tuple)R: 27, 0: Disallow trailing comma tuple (trailing-comma-tuple)-----------------------------------Your code has been rated at 9.99/After Fix************* siju@siju-DAEMON:~/D/Community/nnvm$ make lintpylint python/nnvm --rcfile=/media/siju/DATA/Community/nnvm/tests/lint/pylintrc-------------------------------------------------------------------Your code has been rated at 10.00/10 (previous run: 9.99/10, +0.01)",0
[Compilation Warning Fix] in matrix_op.cc (#356)The below compilation warning issue has been fixed.src/top/tensor/matrix_op.cc:37:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   for (int i = 0; i < lshape.ndim() - 1; i++) oshape[i] = lshape[i];                     ^src/top/tensor/matrix_op.cc:38:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]   for (int i = 1; i < rshape.ndim(); i++) oshape[i + lshape.ndim() - 1] = rshap,0
Change TOPI ops to use C++ implementation where applicable (#357)* Updated TVM version. Implemented fix for nnvm_compiler crash on exit on windows. Changed TOPI ops from using python to using C++ where applicable.* Fix lint* Fix lint* Fix macro* Fix reshape* Update TVM to fix test fails,0
Move dense compute back to python (#364),4
"[Compilation Warning Fix]Enum constant in boolean context (#365)The below compilation warning is fixed.In file included from /mnt/D_DRIVE/work/nnvm_8_feb/dmlc-core/include/dmlc/any.h:16:0,                 from include/nnvm/./base.h:11,                 from include/nnvm/graph.h:15,                 from src/compiler/fold_scale_axis.cc:6:src/compiler/fold_scale_axis.cc: In function ‘nnvm::Graph nnvm::compiler::FoldScaleAxis(nnvm::Graph)’:src/compiler/fold_scale_axis.cc:155:39: warning: enum constant in boolean context [-Wint-in-bool-context]     CHECK(kind == kPassTroughFirst || kMulConsumer);                                       ^/mnt/D_DRIVE/work/nnvm_8_feb/dmlc-core/include/dmlc/./logging.h:110:9: note: in definition of macro ‘CHECK’   if (!(x))",0
[NHWC] InferShape Layout conversion fix. (#372),0
Initial NHWC layout support (#376)* initial NHWC layout support* remove layout param from softmax* more nhwc support* fix typo* add nhwc layout test* fix lint* update tvm* update for c++ topi* fix lint* update tvm,0
Make compiler more robust (#378),5
[FRONTEND] fix same var used in single op (#383)* fix same var used in single op* revert to older version,0
WebGL end-to-end example (#369),5
Sum operator for ONNX. (#386),5
[DOCS] add a solution to installation error because of the old submodule (#384)* [DOCS] add a solution to installation error* Fix the command for updating submodules,0
Add end-to-end SGX ResNet inference example (#388),1
[OP] Experimental assign op (#389),5
fix tutorial build (#390),0
Documentation errors updated (#392),0
[FIX] several bugs found when using NNVM (#391),0
[Keras] fix dropout bug (#399),0
fix onnx frontend softplus bug (#413),0
use caffe2.python.onnx.backend instead of onnx-caffe2 (#418),2
Add optimizer (#334),1
Onnx opset support (#416),5
"Update tvm, fix lint due to pylint update (#423)",0
[PASS] Enhance scale fold axis (#424),4
declare type name for optional<TShape> (#429)* declare type for optional tshape* add doc* move code to another place,1
[FRONTEND] DarkNet Yolo2 Frontend Support  (#377),5
Update TVM to latest (#432)* Update TVM to latest* remove darknet from testing due to cffi,1
"Issue fix for tiny yolo (#436)* Issue fix for tiny yoloThere was an issue in maxpool frontend for tiny-yolo. It is fixed in PR. https://github.com/dmlc/nnvm/issues/431* updated review comment, python 3 compatabilty* Update darknet.py* Updated review comment",0
[OP] PReLU Support (#394),5
fix mxnet model import (#449),0
Support imagescalar operator for onnx (#448),5
Fix license URL (#451),0
Some minor documentation issues rectified (#450),5
General Layout Support (#447),5
Rename FInferLayout -> FCorrectLayout (#453)* rename FInferLayout -> FCorrectLayout* correct stupid IDE* update submodule tvm,1
support dilation in conv2d (#439),5
[DOCS] improve document of symbol; (#456),2
"allow missing FCorrectLayout (#457)* allow missing FCorrectLayout* misunderstood OpMap[], fix",0
fix restore layout in AlterOpLayout (#460)* fix restore layout in AlterOpLayout* lint test case,0
AlterOpLayout with tvm.target (#463)* AlterOpLayout with tvm.target* fix test,0
raise error when cannot get shape in reshape (#467)* raise error when cannot get shape in reshape* fix pylint,0
add sanity check to input shape type (#469),1
fix lint error (#475),0
"dilation fixed for (1, 1) case (#477)",0
use uint32_t (#478),5
add PATH to dll_path on windows (#479),1
[Keras] ReLU6 support (#481),5
enable AlterOpLayout to keep OP unchanged (#471),4
"add use_mt option, default to use md as mt won't work with current c++ api (#484)",1
Better to check Infer result with topi results at build time instead of leaving to a runtime error. (#476),0
"Fixed issue #483, removing enum dependancy (#485)",0
don't rename onnx input tensors (#491),5
Cleanup of '-Wsign-compare' warnnigs. (#504),5
Increase depfiles lookup. (#509)Not able link build/src/top/tensor/*.d.       Hence don't compile nnvm for a change in tvm/topi headers.,2
[FRONTEND][Keras] Fix softmax axis (#503),0
Add count_include_pad support (#498)* update tvm submodule* Add count_include_pad support to avg_pool,1
Squeeze bug fix. (#506),0
fix doc (#510),0
[FRONTEND][Keras] fix reshape (#493),0
[FRONTEND][Keras] Add test for MobileNet (#513),1
Consider version too #508 (#514),5
Expose clip to frontend mxnet (#512),5
Flip operator (#505),5
[DOCS] documentation merge,2
"[DOCS] Improve docs naming, fix docs warnings",0
Update gitignore,1
Homepage URL to tvm.ai,5
[BUILD] correction in topi dependencies        Change at 'topi/include/topi/' not recognozed by make.,4
Gitignore updated as NNVM untracked files not detected,1
[IO] Support cross-endian,5
[RUNTIME] update to make runtime copy type aware,1
fix opengl runtime,0
Fix compilation failure with latest LLVM (#1208)* fix problem with the latest LLVM* add if-defs to support older LLVMs,0
"[LINT] Expected, hence disable for a while. (#1212)************* Module nnvm.top.nnE:141, 8: Assigning to function call which doesn't return (assignment-from-no-return)",5
Add some tips for building on macOS. (#1215),1
ROCm installation guide (#1216),2
[Metal] Fix block launching parameter (#1219),0
leaky_relu bug fix (#1218),0
"[DOCS] Detailed contributor guide, doc refactor (#1220)",2
[DOC] topi intro tutorial (#1217),5
[DOCS] Fix topi tutorial (#1222),0
[NNVM] Elu support added in darknet frontend (#1199),1
add test to irbuilder for gpu execution (#1228),1
[WIP] [NNVM] Fix softmax gradient (#1201)[NNVM] Fix softmax gradient,0
[TOPI] Slice operator (#1165),5
Update ISSUE_TEMPLATE.md,1
update template (#1233),1
[NNVM][FRONTEND][Keras] Support for reusing layers (#1192),5
Remove SGX demo private key from repo (#1237),4
fix extern naming (#1238),0
[RPC] default use spawn for fork safety (#1240),5
[NNVM]Activations support added in Keras Frontend (#1210)* [NNVM]Activations support added in Keras Frontend* Helper for ELU added* All activations test cases clubbed to one,1
Add nnvm target to Makefile (#1245),1
Add -Bsymbolic-functions to linker option (#1244),1
support custom IP address from rpc server to tracker (PUT) (#1243),1
Add PReLU support to mxnet frontend (#1249),1
Unite cmake builds (#1248),3
Update layout.h (#1255),1
[BUILD] Switch to CMake only Infra (#1254),5
[BUILD] Upgrade build system to default python3 (#1260),5
Fix the gemm conversion in onnx frontend (#1241),0
Community guideline in effect (#1261),3
Update README.md,1
[DOCS] Fix links (#1263),0
Add Item list to community guide (#1264),1
Update code_review.rst,1
Fix the building error in android_deploy (#1262)* Fix a link in android_deploy/README.md and a error while building android_deploy.* revert and change APP_STL in Application.mk,0
[MAINTAINER] Add Pariksheet Pinjari as reviewer (#1266),1
[INTRIN] Add support for floor and ceil (#1267),1
[CODEGEN] ARM Popcount lowering rule and codegen updates (#1235),1
add support for subgraphs. (#1221)* add support for subgraphs.* fix.* fix.* Fix compilation error* Fix compilation error* add comments.* update comments.* Sanity check on subgraphs when creating IndexedGraph* avoid the overhead of sanity check.* Stop using non-recursive DFS* Trigger CI* trigger CI,0
Add silent mode to rpc server and rpc tracker (#1268),1
"[BUILD] Enable path option for ROCM, CUDA, Vulkan, simplify optional build (#1270)",5
[NNVM][FRONTEND] Tensorflow frontend support (#1188),5
"[BUILD] Add clang to build matrix, -Werror (#1273)",0
fix copro_sync.cc errors of ctx (#1274),0
"fix missing std::to_string during Android build, android rpc test script (#1279)",0
[MAINTAINER] add masahi as reviewer (#1277),1
[TOPI][IMAGE][RESIZE] Bilinear interpolation for resize and upsampling. (#1181),5
Fix missing cublas libraries (#1281),0
Use CMake for make clean (#1280),5
Remove incorrect extension registration of tvm::Target (#1272),4
Link system library needed for LLVM (#1282),5
fix proxy registration with tracker (#1283),0
fix lint (#1284),0
SSD support in NNVM (#1214),5
[MAINTAINER] Add zhreshold as reviewer (#1287),1
[CONTAINER] Introduce StrMap (#1292),5
[OP] Introduces auxiliary attrs into compute (#1293),5
"[DOCS] Improve review guide, improve cmake llvm build (#1295)",2
[NNVM][TESTING] Add two testing symbols: dqn and dcgan (#1294),1
Check common subdirs for vulkan/spirv headers (#1298),5
Add MyPy to lint (#1301),1
[FRONTEND][MXNET] Add squeeze_axis support to split operator (#1288),1
[BUILD] Fix reflection build for gcc-8 (#1304),0
[MAINTAINER] add srkreddy1238 as reviewer (#1305),1
"[MATH][TOPI][NNVM] introduce trunc, round (#1310)",5
[CONTRIB] TVM download utility based on urllib2/urlib.request (#1313)moving nnvm/testing/download.py to python/tvm/contrib/download.py to be used as a general TVM download utility,3
Fix a bug in Symbol::Compose when using subgraphs as input (#1314),0
CPP implementation of L2Norm and LRN ops (#1157),5
[TOPI] Conv2d Added and Optimized for Intel HD Graphics (#1290),1
moving module import inside of download function (#1319),2
[FRONTEND] A Python hybrid frontend (#1251),5
[TOPI] Numpy consistency: always broadcast binary op. (#1321),5
Import hybrid module in __init__.py (#1323),5
[FRONTEND][MXNET] Add expand_dims supoort (#1317)* [FRONTEND][MXNET] Add expand_dims supoort* fix lint,0
[PASS] Add GPU IR verifier (#1296),1
[NNVM] Introduce const shift ops (#1325),5
[DOCKER] update to include deps (#1326),1
[DOCKER] mark tmp as temp (#1327),5
Update ci_build.sh,1
[SCHEDULE] Fuse support for 0 rank tensor (#1328),5
Windows VS2015 Compile error - 'conversion from 'double' to 'float' requires a narrowing conversion' (#1329),0
[NNVM][CONVOLUTION] Group convolution generalization for NHWC (#1232),5
Documentation error fixed (#1337)Updated documentation error,0
Contrib: add mkl blas support (#1336),1
[FRONTEND] [HYBRID] Non-zero starting supported; Buffer AttrStmt add! (#1330),1
[NNVM][TOPI] Add FTVMCompute for matmul (#1239),1
support t attr in onnx (#1300),5
Add support for Xilinx FPGA board with SDAccel (#1278),1
[NNVM][TENSORFLOW] Mobilenet support. (#1335),5
[NNVM][ONNX] Shape operator support (limited/differed) - #1297 (#1333),5
[NNVM][ONNX] Squeeze and Unsqueese operators. (#1339),5
[SCHEDULE] Fix schedule for big array (#1340),0
[NNVM][TOPI] Add gradients for broadcast_* ops (#1234),1
[TOPI] Add C++ implementation of elementwise operators (#1306),1
[FRONTEND] [HYBIRD] [TEST] Add GPU shared memory test! (#1338),1
"Improve schedule load, add slice_like (#1299)",1
Fix cmake search for vulkan on Windows (#1343),0
[RUNTIME] keep opencl runtime deps free from node (#1349),5
[PYTHON] Make decorator optional for runtime (#1350),5
update include regex to work for path with symbol (#1354)change regex to match all symbol except for space.,1
Strided_slice added in NNVM (#1318),1
[NNVM] Initial mixed precision support of conv2d (#1356),5
[NNVM] Move FTVMCompute registration of the elementwise operator to c++ (#1351),4
Prelu bug fix (#1358),0
JNI Crash fix (#1357),0
Minor bug: causes undefined symbol error in libtvm from NNPACK (#1368),0
Fix tutorial to follow a change of elemwise_sum (#1374),0
fix narrow conversion on gcc-8 (#1376),0
Transpose core dump resolved (#1355),5
"[NVPTX] libdevice support, enable NVPTX backend in topi tests (#1365)",3
[TEAM] Add alex-weaver as reviewer (#1377),1
"[NNVM] Move FTVMCompute registration of cast, greter, less to C++. (#1370)",4
[NNVM][TENSORFLOW] Sigmoid op support #1367 (#1369),5
add take frontend (#1307),1
[AVG POOL] Asymmetric padding (SAME) support. (#1346),1
[TOPI] Update parameter name of conv2d (#1380),1
support equal and not_equal in topi (#1373),5
"[NNVM][ONNX] Slice, Floor, Ceil, Clip and MatMul support for frontend  #1297 (#1371)",5
[NNVM][TENSORFLOW] Fixed variable ops shape parsing issue (#1381),0
Add normal distribution to random engines (#1352),1
Fix tutorial to follow the conv2d change (#1390),0
Move SGX enclave signing key to gist (#1393),4
Fixing issue #1395: Undefined symbol: cudaGetDevice (#1396),0
[RPC] android process isolation/watchdog (#1387),5
"[NNVM][TOP] broadcast versions corresponding to topi: mod, max, min, pow, left_shift, right_shift greater, less, equal, not_equal, greater_equal and less_equal. (#1383)",5
[TOPI] Add GPU SSD (#1397),1
[DOCKER] Fix dependency for autotvm (#1398),0
[NNVM][Keras] allow only tensorflow backend (#1392),5
[DOCKER] Start docker infratructure (#1402),5
[DARKNET FRONTEND]Batchnorm added as part of Dense op for running rnn model for next wo… (#1385),1
[TOPI][DARKNET]Yolo op added  (#1372),1
[DOCKER] Add docker demo image (#1404),1
Add support for multiple OpenCL platforms (#1345),1
change line seperator of tensorflow/test_forward from CRLF to LF (#1405),3
[DOCKER] Add demo-gpu image (#1407),1
fix CorrectLayout for softmax & log_softmax (#1401),0
[RUNTIME][OPENCL] Create program lazily when the program is built (#1408),5
[TVM] Fixed SPIR-V codegen incorrectly not declaring the interface for the entry point (#1400),0
[RPC] graduate tvm.contrib.rpc -> tvm.rpc (#1410),5
[TVM] Fixed SPIR-V codegen for OpControlBarrier (#1409),0
[DOCS] Improve documents on deployment (#1412)* [DOCS] Improve documents on deployment* minor updates,1
[OP] Improve bitwise op type checks (#1415),5
[TEAM] New reviewer: kazum (#1417),1
[RUNTIME] Simple NDArray container API in c++ (#1418),5
Initial commit,5
"hardware compilation flow, and driver tests",3
doxygen path update,1
[DOCS] Initial docs (#4)* [DOCS] Initial docs* update instruction,1
[REFACTOR] Code base refactoring (#5),5
"[REFACTOR] Macro standardization, lint tests (#7)* code refactoring* code refactoring* code refactoring* code refactoring* fixing macro* refactoring, tests, makefile* style - making sure lint test pass* prefixed macros with VTA, fixed bugs",0
"[PYTHON, TVM] Python TVM library, unit tests and end to end example* VTA python library* Python unit tests* End to end example with Resnet18* README instructions* Bug fixes",0
fixing URL; adding () to print (#17),0
Upgrade TVM to latest version,3
[RPC][RUNTIME] Support dynamic reload of runtime API according to config (#19),5
"[SCHEDULER, HW] Auto scheduler for conv2d, hardware generation (#20)* Hardware generation fixes/sweep, auto scheduling for VTA conv2d* Hardware generation fixes/sweep, auto scheduling for VTA conv2d* derive hw spec from config file* up to date hardware spec",0
[COMPILER] Refactor compiler to enable configuration (#21),5
[DRIVER][RUNTIME] Make runtime fully device agnostic (#23),5
"[DRIVER] Add simulator, unify testcase to unittest (#25)",1
[RUNTIME] Simplify dynamic library and code path. (#27)* [RUNTIME] Simplify dynamic library and code path.* reword the readme,5
[INFRASTRUCTURE] Migrate to json based config. Move gemm test to integration. (#28)* Migrate to json based config. Move gemm test to integration.* temp checkin* checkin  example json,3
"Refactor, refactor code structure, fix pynq rpc (#29)",0
[TEST] CI infrastructure (#30),3
fix jenkins (#31),0
[COMPILER] Upgrade to meet latest TVM IR pragma convention (#32),3
[PYTHON] Enable environment scoping (#33),5
"[HARDWARE, TEST] Fixed hardware generation flow (#34)",0
"[TOPI] Automated schedule in conv2d TOPI lib, moving to GEMM intrinsic (#35)* removing programming out of end to end example for now* updating TOPI library to use gemm tensor intrinsic* bug fix, autoschedule in TOPI conv lib* removing the deprecated GEVM intrinsic* refactoring, fixed lint test* fix for integer division bug* python3 bug fix for non matching types due to float division* comment",0
"Update Graph Support for Batching, Fix Swapping (#37)* fix graph transform for batch dimension* fix* fix",0
[BITSTREAM SERVER] Bitstream server integration (#38),5
[INIT] Allow proper throw in compiler (#39),5
[EXAMPLE] Fix example for simulator (#40),0
"[DOC, EXAMPLE] Updated READMEs, tests, etc. (#41)* bug fix for new drivers in new PYNQ image v2.1* updating instructions for resnet inference* updated the instructions for starting the RPC server* deriving host/port from env for unit tests",0
"VTA and TVM on PYTHONPATH, also pass to sudo (#42)",4
Update TVM Version and CI scripts (#46),1
Update Jenkinsfile,1
[DOC] VTA installation & basic tutorials (#47),2
"[UTILS, DOC] Use TVM file downloading utility, conv2d tutorial (#48)",2
[DOCKER] Cleanup docker image (#50),5
"[DOC, TVM] ResNet tutorial, updated TVM (#51)",1
[TVM] Upgrade TVM Support,5
[NNVM] Make param file python version agnostic,2
[TUTORIAL] Resnet-18 end to end tutorial example (#55),5
[TOPI] Fix the CPU op perf (#56),0
"[BUILD][DOCS] Migrate VTA CI, test, build, docs",2
Update index.rst,1
Added option to build android rpc app with vulkan support,1
fixed path in installation/setup guide,0
minor style edit,5
[RUNTIME] Support setting CPU affinity (#1403),5
[AUTOTVM] Core part of auto-tuning module (#1312),5
[TOPI]Add where operator (#1416),1
[DOCS] VTA installation guide (#1428),2
Add support for absolute opeartion (#1406),1
fix android packed runtime (#1430),0
[Darknet] softmax temperature in frontend (#1429),5
[TVM][RUNTIME] dependencied update for ndarray. (#1431),1
[RUNTIME][SDACCEL] Add support for multiple kernels (#1424),1
[BUILD] enhance vta build (#1434),5
[NNVM] Add symbol squeezenet (#1436),1
add generic home path (#1435),1
[CI] Switch to use prebuilt docker instead of build from scratch (#1442),2
Update README.md,1
Fix cmake for building with cuda in msvc (#1437),0
[RUNTIME][OPENCL] show correct device type name (#1441),5
[BUILD] restrict runtime include headers (#1444),5
Fix conda package builds (#1445),0
[DOCS][SDACCEL] Update AWS F1 deployment (#1447),1
[RPC] Added native debug logging to Android RPC (#1432),0
[NNVM][TENSORFLOW] bug fix on bilinear and resize op integration in frontend. (#1440),0
[CODEGEN][SDACCEL] add support for specifying FPGA device name (#1448),1
Fix inceptionv3 (#1446),0
use SetAffinity when logical cores > physical cores (hyperthreading) (#1453),2
[NODE][REFLECTION] Support NDArray as field (#1452),5
Fix runtime error on osx (#1449),0
Tutorial enhancement to keep it clean on docs.tvm.ai (#1450),2
[DARKNET]RNN Support for darknet (#1443),5
Fix conda 2 (#1456),0
Minor doc fixes (#1458),0
[CUDA] FP16 support  (#1413),5
"[RPC] Android RPC Performance Regression Fix, Update Android RPC to use Tracker (#1457)",0
Fix GetReduceAces (#1460),0
Add deps for Relay (#1463),1
Update VTA schedule (#1464),1
[FRONTEND] [HYBRID] Augmented assign operator supported! (#1459),5
"Revert ""[FRONTEND] [HYBRID] Augmented assign operator supported! (#1459)"" (#1466)This reverts commit 33245b8c09009fa2c080e1f42e786ede745de963.",5
[AUTOTVM] Misc bug fix (#1467),0
[tvm4j] fix java build (#1471),0
"[DOC, HARDWARE] Hardware developer guide, migrating to use Vivado 2018.2 (#1473)",5
[TOPI] Bitserial low-precision convolution (#1332),5
[BUILD] Fix LLVM static/dynamic link issue (#1461)This patch prevents libtvm.so from both containing static parts of LLVMand link with libLLVM.so by removing the latter from link list.,0
[IR] support general type annotation. (#1480),5
update android rpc docs (#1479),1
[tvm4j] add GraphRuntime (#1472),1
Update Application.mk (#1483)Fix application crash problems on armv7a architectures,0
[DOC] Update VTA readme files to avoid stale information (#1484)* updated readme files to avoid stale instructions* bsp generation turned off by default,1
[NNVM][DARKNET]Logistic activation added (#1477),1
[CODEGEN] Enable inline llvm asm code (#1486),5
[NNVM][TENSORFLOW] LSTM operator and PTB word prediction frontend (#1389),5
timing closure fix for default VTA config (#1489),0
[NNVM] Add argmax and argmin operations from topi (#1462),1
add orderedset to Ubuntu python package list (#1491),1
Fix more type annotation (#1490),0
Fixed bugs for conv2d (#1465),0
[DOCS] Reword community guide (#1494),2
update dependency (#1495),1
"Revert ""update dependency (#1495)"" (#1499)This reverts commit f1f30c4c4ab5e8a47a0d76c8cd2c5344c0a9096a.",1
perfom full rpc tracker handshake (#1500),5
[NNVM] Fix grads for sum and expand_like (#1455),0
[BUILD] warning fix: new does not have an alignment parameter     (#1478),0
[NNVM] Fix check in layout parsing (#1502)* [NNVM] Fix check in layout parsing* add one workload,0
"[TOPI, x86] Adapt AVX schedules for SSE target (#1504)* adapt avx schedule for sse* fixed output type when mixed precision mode",0
Activations for coreml added (#1508),1
C-RNN layer support is added (#1492),1
[TEAM] yzhliu -> committer (#1509),5
[tvm4j] support kNDArrayContainer (#1510),5
Add initial support for Intel FPGA SDK for OpenCL (AOCL) (#1474),1
update halideIR (#1515),1
[NNVM] Fix gradients for broadcast_div (#1512),0
[VTA] bugfix parameter derivation (#1521),0
[TVM][CUDA] NVIDIA GPU Int8 Support (#1503),5
[NNVM] remove keepdims from expand_like arguments (#1517),4
Add some missing operators (#1524),1
Onnx Gather operator added (#1513),1
Fix a testcase name in test_codegen_cuda (#1526),0
keras.layers.ReLU added (#1530),1
[AUTOTVM] TOPI integration for ARM CPU (#1487),5
[NNVM][TENSORFLOW]Local Response Normalization added for tensorflow (#1522),1
[MXNET] LRN support in MXNET frontend (#1520),5
[NNVM] Support argmax/argmin in tensorflow frontend (#1514),5
Update test_rpc_exec.py,1
Fix rpc testcase (#1538),0
Fix RPC (#1542),0
Update ci to new location (#1552),1
fix dependenci and improve doc (#1535),0
[CI] Simplify labeling rules (#1554),5
[AUTOTVM] Improve tutorial and logging (#1544),2
[NNVM][TENSORFLOW] Cleanup redundant code. (#1551),5
update topi schedules (#1556),1
adds antlr4 to python3 package list (#1560),1
"[FRONTEND][TENSORFLOW] Add Pad and PadV2 support (#1545)* [FRONTEND][TENSORFLOW] Add Pad and PadV2 support* Add assettion to _pad, and fix testcase for pad.",0
[FRONTEND][ONNX]LRN support for ONNX (#1518)* LRN support for ONNX* [ONNX] Updated lrn testcases,1
[NNVM] Enhance operator fusion for more element wise patterns (#1548),5
add tvm external registry entrypoint (#1562),1
[ONNX]onnx gather bug fix (#1543),0
l2normalization operator support for tensorflow (#1528),5
"Separate fusion and Compilation (#1564)* Separate fusion and compilation* fix description of graph_fuse.h* fix lint* fix @masahi 's comments, move fusion out of target* fix graph passing and make fused_entries singula in graph attr* fix typo* fix some comments* run test again* remove rvalue for graphfuse and graphfindfusiablegroups",0
[TOPI] add injective scheduler for HLS backends (#1553)* [TOPI] add injective scheduler for HLS backends* Introduced PrintBinaryExpr,1
Use int for int8x4 due to performance overhead of char4 (#1569)* Use int for int8x4 due to performance overhead of char4* Add a comment about using int* Remove invalid test,1
[DOCS] Neural network Deployment Guide with System Module Mode #1523 (#1533),2
add conv2d transpose and fix bugs (#1566),0
[TEST] force openblas threads to be 1 (#1580),3
Vulkan TVM Android Support (#1571),5
[TEAM] merrymercy->code owner (#1581),5
[AUTOTVM] API change (#1583),4
update dmlc-core for security reason (#1584),1
DLPack Conversion API (#1573),5
use phone EditText for numerical fields (#1587),5
[RUNTIME] Refactor to enable stackvm in runtime. (#1588),5
[DLPACK] Enable cython support (#1589),5
Fixed bugs for SSD sorting and multbox detection (#1578),0
Split_indices negative axis added (#1595),1
[FRONTEND][TENSORFLOW] Optimized tensorflow testcases (#1546)* [NNVM][TENSORFLOW] Optimized tensorflow testcases* Replace Constants with Placeholder* Review comment fix,0
[NNVM][DOC] Update NNVM symbol documentation to latest. Ref. 1591 (#1599),1
[NNVM][POOL] bug fix. Remove the hardcode. (#1600),0
[FRONTEND][DARKNET]LSTM and GRU support (#1576),5
[AUTOTVM] Fix GATuner and improve error message (#1605),0
[NNVM] Add symbol for inception v3 (#1604),1
[TEAM] New reviewer: kevinthesun (#1606),1
[RUNTIME] Enable return NDArray in RPC (#1610),5
[NNVM] Add ONNX upsample converter (#1591),1
add -mattr=+neon for all arm cpu target (#1612),1
fix output_shape in conv2d_nchw (#1613),0
[NNVM] Bug fix Prevent fusing convolution with injective op  (#1608),0
[NNVM] TF: Add Pack operation (#1570),1
#1592 [PASS] Fix missing mem CHECK in storage_rewrite (#1616),0
[FRONTEND][COREML]MultiplyLayerParams L2NormalizeLayerParams and UpsampleLayerParams support … (#1511),5
fix import (#1621),0
Add missing check when deciding conv op and injective op are in the same group (#1622),1
[NODEREF] Introduce named attribute system. (#1618),5
[NNVM][DARKNET]Yolo and Upsample frontend support (#1501)* Yolo and Upsample frontend support* Lint fix* Mac support added* Code clean and trigger CI,0
Improve x86 Inception (#1506)* Improve x86 pooling and concat* Fix* Fix test concatenate correct layout* Add conditional vectorize* Fix lint* Modify schedule for global pooling* Fix* Fix warning* Fix alter layout test* Remove vectorization for pooling when using 4D layout* Remove vectorization for 4D concat* Fix concatenate layout* Fix concatenate schedule* Fix concat* Fix lint* Fix concat* Simplify pooling logic* Update docstring* Fix test topi pooling* Small changes,0
[VERSION] Update to 0.5.dev (#1623)* [VERSION] Update to 0.5.dev* Update the docs to include all intrins,1
Add int8 gemm recipe (#1614),1
[RUNTIME] Add TypedPackedFunc (#1626),1
check in (#1629),5
[AUTOTVM] Allow fallback for template & Fix bugs in tuners (#1615)* support fallback & fix bugs in tuners & clean topi test* update task extraction* update task extraction* fix arm tutorial* Update tune_nnvm_arm.py,0
[TOPI][ARM CPU] fuse bias to depthwise conv2d (#1631),5
[FRONTEND]minor bug fixes (#1632),0
[ATTRS] change AttrFiledInfo->Node (#1634),2
[NNVM][KERAS] Add cropping support (#1636),1
[NNVM][KERAS] Fixed padding in pooling (#1635),0
[TEAM] New reviewer: nishi-t (#1637),1
[FRONTEND][COREML]More ops are added (#1619),1
trigger ci (#1620),5
[AUTOTVM] Simplify TopHub (#1630),5
"Remove leading ""./"" from include paths (#1640)",4
[RUNTIME][PYTHON] Switch to use __new__ for constructing node. (#1644),1
fix CO CI problem (#1641),0
[RUNTIME] [OPENCL] Fix access modifiers (#1643),0
[FIX] Fix issue with TypedPackedFunc template instatition (#1649),0
[NNVM][KERAS] Support multiple outputs (#1648),5
[PYTHON] Enable constructors in Node (#1647),5
[NNVM][TEST] Test against numerical grad (#1505)* [NNVM][TEST] Numerical gradient testing* [NNVM][TEST] Make some tests a little faster* Fix the failing test_top_level3* Target exclusion for the check_function* Try to ignore singularities* grad_input_vars now can't contain shapes* Don't pass unnecessary grad_input_vars to check_function* Multiple outputs; fixes; testing of check_function* Use numerical_grads_params to pass parameters to numgrad checker* Fail when no action is requested excplicitly* Pass additional params to functions* Silence the linter issue* Simplified numgrad checking* Improved docs for check_function* Fixed the error message when no dtype is provided* Several fixes* Tests with shape/dtype inference for inputs* Don't check dense's grads on cuda* Raise an error if output dtypes haven't been inferred* Moved shape/dtype inference into a separate function; use float32 as fallback* Remove redundant dtype=float32* Fix multiple outputs* Use check_function in the rest of the test_top_level1,0
[AUTOTVM] Fix local executor (#1651)The old queue size is too small. It will stall the executor due to race condition.,0
"[NODE] Enable global singleton object, allow set_body_typed in function registry, default fallback of IRPrinter. (#1652)",5
improve text summary (#1655),5
[LANG] Improve serializer (#1658),5
[RUNTIME][OPENCL] delay device check (#1657),5
[CODEGEN][AOCL] Add math intrinsic rules (#1653)* [CODEGEN][AOCL] Add math intrinsic rules* introduce aocl_emu target for AOCL emulation* rename aocl_emu with aocl_sw_emu* update docs,1
"[FRONTEND][ONNX]HardSigmoid, min, max, mean ops support (#1645)",5
[FRONTEND][TENSORFLOW] fix the convertion of sum and add testcase for it (#1654)* [TENSORFLOW] fix the convertion of sum and add testcase for it* delete checking tyoe of axis and divide reduce test,0
add docstring skip in hybrid script (#1668)* add docstring skip in hybrid script* fix lint,0
[DOCS][NNVM] Delete duplicated tensor operators from list (#1669),2
Fix incorrect stride in conv2d_nhwc_python (#1670),0
[AUTOTVM] Decouple build and run in measurement (#1661),5
[TOPI] add nn schedulers for HLS backends (#1663)* [TOPI] add nn schedulers for HLS backends* fix pylint* fix topi transform test,0
Fix incorrect doc in conv2d_nhwc_python (#1677),0
[PASS] Enhance gpu verify pass (#1660),4
Support FoldScaleAxis for depthwise convolution (#1664),5
[FRONTEND][TENSORFLOW] Add Transpose support. (#1665)* [FRONTEND][TENSORFLOW] Add Transpose support.* [FRONTEND][TENSORFLOW] Get parameter from inputs and fix document style.* [FRONTEND][TENSORFLOW] Handle the case that perm is not specified.* [FRONTEND][TENSORFLOW] Convert Rank and Range to param.* [FRONTEND][TENSORFLOW] Fix a pylint issue.* [FRONTEND][TENSORFLOW] Implement Rank and Range as normal op.,0
[CUDA][TVM] fix constructing invalid command line string for nvcc (#1674),0
"CI Failure Fix (#1682)The recent changes in tutorial is with PR # https://github.com/dmlc/tvm/pull/1501 broken the link for downloading the weights file, leading to this CI failure.",0
Rename axis parameter in onnx squeeze (#1683)* Rename axis parameter in onnx squeeze* Add test,1
expose SaveToFile symbol on windows (#1685),2
[Tutorial] tutorial to writing a costumized pass (#1671),4
Allow log_softmax on explicit trailing dim (#1684),2
[Sparse] add sparse tensor computation support (#1289),1
[AUTOTVM][TOPI] Use tunable templates for GPU (CUDA/OpenCL/ROCm/Mali) (#1638),5
Add dist to python/.gitignore (#1691),1
Documentation issues (#1702),5
[TEST][Keras] use pretrained model to avoid small error caused by random weights (#1701),0
[Tutorial] fix the link thing for the pass tutorial (#1700),0
[RUNTIME][API] Graph runtime API enahncement to support NDArray (#1659),5
[TEAM] were -> Reviewer (#1705),5
Added a helper function that dumps Node to stderr (#1703),1
Bugfix #1692. Constant folding and result comparision allowance. (#1708),0
avoid flaky testing errors in test_topi_sparse (#1706),0
Fix comment of binary op 'elemwise_div' (#1712),0
intel graphics conv2d schedule fixed for input shapes (300*300) and (512 * 512) (#1709),0
[NNVM]Tensorflow and Onnx basic ops (#1666),5
[NNVM][KERAS] Fix keras model converter and improve tutorial (#1716),0
[TOPI] Add dp4a intrinsic to CUDA (#1707),1
Add demo_android Dockerfile (#1646),1
[FRONTEND][TENSORFLOW] Helper function to add shapes into the graph. Use tmp folder for model files and clean it. (#1697),1
[RUNTIME] Support TVMContext (#1720),5
Allow inplace memory optimization for different data type (#1696),5
[NODE] Enable EnvFunc to serialize global function as node (#1721),5
[Keras] fix weight shape in dilated conv (#1715),0
[NNVM][TOPI] Add mean and product operators (#1628)* Add mean and product operators* Fix typo* Fix lint* fix test* Fix gpu schedule* Update doc* remove mean from topi* Add nnvm test* Fix cuda schedule* Remove cuda schedule,0
[CODEGEN] Fix let expression (#1727),0
[NNVM][KERAS]LSTMCell support (#1686),5
[COMPILER][BUG] Fix out of bound access. (#1723)* [COMPILER][BUG] Fix out of bound access.* * Review comments.,0
[SUBMODULE] update submodule to latest (#1728),1
Fix VTA Tutorial for more strict graphrt check (#1737),0
[TOPI] Fix reduce behavior to be consistent to numpy (#1738)[TOPI] Fix reduce behavior to be consistent with numpy,0
"[NNVM] Recover reduction behavir, fix CI (#1740)",0
[High level OPT][RFC] NNVMv2 IR - Relay (#1672),5
[RUNTIME] Improve memory usage for RPC (#1741),5
[NODE] Node base system refactor (#1739),5
[FRONTEND][TENSORFLOW] GPU support for tensorflow models. (#1718),5
[CI] always rebuild sphinx-gallery docs from scratch (#1742),2
[TEST][KERAS] convert tvm output to channels_last format (#1733),3
[TEAM] jroesch -> Reviewer (#1746),5
[TEAM] siju-samuel -> Reviewer (#1745),5
[NODE][RELAY] Move most of the reference related code to node (#1747),4
Fix Softmax in onnx frontend (#1642),0
[TVM] Fix operator!= for Tensor (#1753),0
[RELAY] IR Wellform Checker (#1748),5
Heterogeneous Runtime (#1695),5
[NNVM]Keras SimpleRnn and GRU support (#1729),5
[DOCKER] Golang CI recipe. (#1759),5
fix buffer elem_offset calculation (#1762),0
[FRONTEND][TENSORFLOW] NCHW layout support (Resnet V1/V2). (#1743),5
Update SGX cmake (#1763),1
[DOC] Argument name correction. (#1765),5
[DOC]Errors corrected (#1767),0
[NNVM] Bugfix operator fusion for residual block with layout transform (#1760)* Bugfix operator fusion for residual block with layout transform* add a test case* update error message,0
[DOCKER] CUDA upgrade to 9.0 to acommodate tensorflow-gpu (1.10.0). (#1761),5
[RUNTIME] Add fp16/fp32 conversion functions (#1766),1
INT8 conv operator implementation with NCHWc data layout for Intel machines (#1680)* Int8 implementation for convolution operator on Intel Skylake* Int8 implementation for convolution operator on Intel Skylake* PR changes* PR changes* PR changes* Fixing an error* Fixing an error* Minor typos fix* Minor typos fix* Removing the broadcast16 CPP code. Using astype feature instead* Replacing constant by variable name num_elements_intel* Name fixes and tensorize update rule updated* Fixing the bug about checking skylake* Replacing bitcast with reinterpret* Isolating INT8 and FP32 schedules to ease out future AutoTVM PR merge* Putting check_skylake function in the x86 directory* Added documentation and organizing files to better locations* Tensor intrin renaming. Avoid code duplication for intrin by kernel reshaping,0
[RUNTIME] Use weak link for fp16 functions  (#1769),5
[Relay] Restore kind checking (#1758),5
[TOPI] Access topi::matmul from Python (#1744),5
[ONNX][FRONTEND] Constantfill - #1539 (#1764),5
Adding source types to C++ reduce functions (#1771),1
[SGX] Improve edgeroutines (#1775),5
Added Dockerfile demonstrating OpenCL & OpenGL installation (#1770),1
[RELAY] Refactor type inference to use type solver (#1779),5
[Relay][DOC] Add tutorial for adding an operator to Relay (#1778),1
[NNVM][TENSORFLOW]Fix lstm testcase to support get_output without size input (#1731)* [NNVM][TENSORFLOW]Fix lstm testcase issue to support get_output without size input* removed redundant* Enabled inceptionV1 testcase,0
support of multiple devices for tvm.build (#1773),5
Implement tensorflow relational operators and related tests (#1714),3
[BUG] Fix incorrect libcuda.so found by cmake when multiple versions of CUDA exist (#1788),0
[Relay] Free Variables (#1786),5
Add docs/dev/relay_add_op.rst to docs/dev/index.rst (#1790),1
Add atol=1e-5 to test_topi_matmul.test_matmul (#1791),1
[Relay] Incorporate TypeRelations into more tests (#1792),3
[IR]  eager constant folding in operator overloading (#1789),5
[RELAY] First pass at pretty printer (#1749),4
[NODE] Keep base node system in HalideIR (#1793),5
[TOPI] Add conv2d int8 template (#1735),1
[NNVM][TEST] Numgrad: fix nan and multioutput (#1754),0
Change error.h path in doc.h (#1794),0
[TOPI] Update TopHub and benchmark (#1796),1
Fix vulkan build with homebrew install of vulkan-sdk (#1802),0
Use ctx instead of tvm.gpu(0) in nnvm_quick_start tutorial (#1801),2
Change cat image extension to png to match its download URL (#1800),4
[AUTOTVM] Support multiple targets load in tophub (#1803),5
[Tutorial] tutorial for tensorize (#1774),5
"[RELAY][OP] conv2d, ShapeExpr->IndexExpr (#1798)",5
Correction in documentation (#1810),5
[TVM] Fix negating undefined in DetectLinearEquation (#1816),0
[DEBUG]Support a debug framework for TVM Runtime (#1378),0
[RUNTIME] Fix debug runtime i386 build (#1818),0
[RELAY][DOCS] Core Operator docs (#1821),2
Add SGX to docker (#1822),1
[DOCKER] Fix CI script (#1826),0
Fix dmlc-core path in nnvm Makefile (#1829),0
[RELAY][OP] expand_dims (#1819),5
[RELAY][OP] comparison (#1824),5
[OP] right_shift (#1832),5
"[Relay] Add Let list, a helper datastructure to relay (#1827)",1
[DOCS] Add docs of logical snd right shift (#1834),1
[Relay][Doc] Correct bad formatting and typos in Relay operator addition doc (#1833),1
Fix saveload json bug (#1831),0
[Relay] More type alpha equality test coverage (#1823),3
[LANG] Generalize compute to tensor region (#1476),5
[RELAY] Add sigmoid relay operator (#1836),1
[RELAY][OP] Left shift operator (#1839),5
maximum relay op V2 (#1838),5
Fix lint error missed during CI outrage (#1846),0
[Relay] [Op] zeros_like and ones_like (#1835),5
[RELAY][OP] Add relay minimum op (#1840),1
[DOCKER][GOLANG] Fix golang compiler version to 0.10 (#1848),0
Add rust runtime (#1597),1
Update SGX example (#1825),1
[RELAY][PASS] Dead Code Elimination (#1776),4
"[RELAY] reorg testcase, make checked_type property, fix constructor error handling (#1850)",0
[SGX] Add ignored files to sgx example (#1852),1
[Rust] Update rust install in dockerfile (#1855)* Update rust docker* minor edit for consistency,1
[RELAY] Add softmax (#1841),1
"[Relay][Op] concatenate, reshape, transpose, copy (#1847)",5
Update test_op_level1.py,1
Enable bool type as storage type (#1853),5
Install rust for all users (#1856),2
"[RELAY][OP] Operators.   pool2d, global_pool2d, batch_flatten, tanh, sigmoid, floor, ceil, trunc, abs, negative, multiply, mod, pow,  resize (#1813)",5
[RELAY][OP]log_softmax op (#1857),2
[Relay][Op] Clip (#1844),5
add relu (#1849),1
[RELAY][OP] take (#1863),5
Allow override gtest library search path (#1867),3
[Relay] GetItem (#1861),5
Add instructions to run tests locally (#1868),1
[RELAY][OPS]LRN and L2_Normalize (#1860),5
[Relay][Op] Add operators full and full_like (#1845),1
Use new onnx API to load model from file (#1874),1
Update SGX example (#1879),1
[RELAY][OP] conv2d_transpose (#1862),5
Update frontend.rst (#1881),1
[Relay] Alpha equality tests for Relay exprs (#1871),3
Add relay.where (#1869),1
[Relay][Op] Pad operator (#1843),5
[Relay] add python doc for function in ir_pass (#1877),1
[Rust] Add rust runtime to CI (#1851),1
Update HalideIR (#1890),1
[TOPI] Update pre-tuned parameters for TX2 and fp16 on Mali (#1892),1
Fixes for tensorize in Windows build to expose TensorIntrin::make and search clang.exe (#1896),0
"[Relay] [Op] Zeros, Ones (#1885)",5
Add test to confirm that we forbid allocate statement referencing undefined variable (#1899),1
support double buffer to use in ir builder DSL(#1897) (#1898),5
"[RELAY][IR] Move type_annotation to Var, remove Param (#1900)",4
[RUNTIME][DEBUG]Support remote debugging (#1866),0
[Relay] [Op] Squeeze (#1858),5
[Relay][OP] MultiboxPrior (#1882)* Relay MultiboxPrior Operator* Fix lint* Fix build* Add test for default args,0
[LANG][ATTRS] Enable deep equality comparison and hash of Attrs (#1903),5
[Relay] remove redundant test cases in test_op_level4.py (#1905),3
[Relay][Op] Dropout and batch_norm (#1870),5
[TVM] Eagerer const folding for logic ops (#1907),2
Add dtype option to verify_mxnet_frontend_impl (#1908),1
[FRONTEND][DARKNET] YOLO V3 model support (#1734),5
"[RELAY]Ops Dense, leaky_relu (#1828)",5
Add javadoc build into Jenkins workflow (#1909),1
[DOC]Clear javadoc directory everytime (#1917),5
fix linting command instruction (#1919),0
[NNVM/TOPI][OP] Split : default axis to 0 and allow negative values - nump… (#1883),5
Fix docstring of ConcatRel (#1912),0
Update Jenkinsfile (#1893),1
AutoTVM x86 (#1772)* AutoTVM for x86 conv2d* Add ApplyGraphBest dispatch context* Fix tutorial* Fix conv2d* Improve tutorial* Fix default schedule* Fix 1x1 default schedule loading* Fix workload type* Change gridsearch to random* Add reference to autotvm arm* Merge conv2d common and 1x1 decl* Fix lint* Minor fix,0
[DOCS]Update debugger in docs.tvm.ai (#1924)Update debugger in index to reflect in docs.tvm.ai under the Design and Developer Guide,0
Update submodule dmlc-core (#1920),1
"[RELAY][OP]Reduction operator framework, argmax, argmin (#1865)",5
fix build issue for MSVC 2017 15.8.0 and above (#1928),0
"[RELAY][Refactor] TextPrinter, move ret_type after body in Function. (#1918)",4
adding Liangfu Chen as reviewer (#1926),1
[YOLO]Add the probability to the image (#1910),1
Check iter_type in vectorize (#1921),5
Update SGX example (#1933),1
[Relay] Nullable Type Alpha Equality (#1906),5
Fix x86 Conv tuning tutorial (#1932),0
"[RELAY] IR builder stablize refactor, clean pass (#1934)",4
Fix master CI due to stale push (#1943),0
Fix typo in module.py line 90 (#1947),0
[VTA] pynq v2.1 -> v2.3 (#1945),5
[Relay] format text_printer.cc (#1946),5
[TOPI] Specify non-zero absolute tolerance in tests (#1925),3
[Frontend][MXNet] ones zeros ones_like zeros_like ops support (#1814),5
Fix non-zero extent of access_ptr out of range (#1937) (#1939),0
[RELAY]Reduce ops sum/max/min/mean/prod (#1927),5
up (#1940),5
[Relay] fix format in ty.py (#1948),0
[Relay] fix doc in ty.py (#1949),0
[Relay][Op]BroadcastToLike CollapseSumLike (#1886),5
Add tophub for x86 (#1955),1
[AUTOTVM] Fix measurement for CPU (#1956),0
[Relay] Fix format (#1957)* save* fix format,0
Add link to the reviewers,1
[RELAY] Refactor AlphaEqual to support deep comparison of Attrs. (#1958),5
[Relay] Parser CI dependencies and build rules (#1965),5
[Relay] Serialization round-trip tests (#1968),3
[RELAY][TypeSystem] Add support for populating type args (#1962),1
Fix int8x4 broadcast value codegen in cuda (#1959),0
fix pydoc format (#1975),0
"[RELAY] BiasAdd, MLP, Resnet testing (#1969)* [RELAY] BiasAdd, MLP, Resnet testing* fix review comments",0
add Xiaoqiang Dan as reviewer (#1976),1
[Frontend][Darknet] L2 normalization support in darknet (#1916)* l2 normalization* retrigger CI,5
[Relay] visit the span (#1990),5
[DOCS] Fix C++ example:graph_runtime.cc:151: Check failed: data->ndim == data_out->ndim (2 vs. 1) (#1987),0
Fix load subgraph from json (#1980),0
"[Relay] fix small typekey issue (#1992)It might cause TupleTypeNode to be printed incorrectly.it doesnt show in http://ci.tvm.ai:8080/blue/organizations/jenkins/tvm/detail/PR-1989/1/pipeline/141, but if you run it on local machine you will see what get compared being NodeBase and TupleType.Also as a side thought can we write a giant macro that make sure everything get did right (all field get visited, typekey match, declare_node_type_info match, etc?) I can do some macro metaprogramming, so I can take up the work.",0
Cleanup deadcode (#1991),5
[ATTR] Introduce Integer container (#1994),5
[RELAY] Add structural hashing for Relay (#1977),1
fix typo in resnet definition (#1995),0
[RELAY] Fix compilation under clang-4.0 (#1998),0
[RELAY][OP] Split (#1876),5
initialize base class in copy constructors (#2006)GCC issues warnings with -Wextra if we don't explicitly initializebase class in copy constructors. This commit fixed the issue.,0
[RELAY] Add occurs check before unification (#2012),1
[RELAY]reshape_like (#1950),5
[TOPI][CUDA] batched int8 conv2d (#1961),5
[RELAY][OP] Fix conv2d NHWC type inference. (#2019),0
[OPENCL][RUNTIME] Fix race condition of modules (#2018),0
[DOCKER] temporary revert cuda version to cuda8 (#2021),5
save (#2015),5
[TF] ignore Truncate in cast (#2022),5
[DOCKER][GOLANG] fix golang version. (#2023),0
[RELAY][PASS] FoldScaleAxis Forward (#2020)* [RELAY][PASS] FoldScaleAxis Forward* Introduce helper function type_as* Update per review comment* Fix according to comments,0
Add attrs package (#2025),1
[intrin]support fmod for cuda (#1964),5
Do not mutate GlobalVar's checked_type field. (#2026),5
[Relay] DQN Port (#2009),5
[PASS]unroll loops with extent=1 (#2027),4
[Relay] DCGAN port (#2010),5
[RELAY]prelu op support (#2016),5
Refine porting x86 NCHWc conv to AutoTVM (#1993),5
[PASS] add a pass for the specific hardware accelarator when it is not binded (#1999),1
[Frontend][MXNet] Change mxnet graph traversal from recursion to iteration (#2007),4
[RELAY][PASS] FoldScaleAxis Backward (#2024),4
Conditional Loop Partitioning - Extending to remove if conditions (#1797),4
[YOLO]yolo op added in frontend and removed from topi (#1974),1
Fix a bug in inject-virtual-thread (#2039),0
[TOPI][AUTOTVM] Improve style (#2034)* [TOPI] Improve the style of using autotvm* fix,0
[RELAY][OP]  Maketuple to be resolved when containing incompleteType (#2031),5
[RELAY][RUNTIME] Add Relay interpreter and compiler for TVM runtime system. (#1954),1
[TEAM] Add Zhi Chen as a reviewer. (#2040),1
[AUTOTVM] Misc fix to document and style (#2035),0
Better gemm support for cublas and cpu (#1967),5
[RELAY/PASS] Simplify inference. (#2033),4
[RELAY] MobileNet (#1997),5
[TOPI] Add dilation argument to conv2d and depthwise_conv2d (#1970),1
[NNVM/TOPI][OP] gather_nd (#2041),5
[Cleanliness] [Easy] Make TVM leak-sanitizer and Wnon-virtual-dtor clean. (#2046),5
[RELAY][RUNTIME] Refactor interpreter and graph_runtime into consistent interface. (#2042),5
Refine CMakeLists.txt (#2049),5
[TOPI] Fix adding dilation arguments (#2047),0
[FRONTEND][TENSORFLOW] Enhancements. (#1923)* [FRONTEND][TENSORFLOW] Enhancements.* Generalize the shape with explicite argument.* Supported entire range of mobilenet_v2 models.* Cast op updated to latest tensorflow.* Documentation updates.* CheckNumerics op handling without exception.* Test data from tensorflow official releases.* * CI error.* * self review* * Enhanced reshape handling.* * docs.* * tutorials* * review comments.* * review.,0
[NNVM][OP] Allow two input tensors with different type in reshape_like op  (#2052),5
Rename relay::Environment to relay::Module (#2054),5
[RELAY][RUNTIME] Add compute and schedule attributes for all ops in relay/op/tensor.py (#2050),1
[RELAY][BACKEND] CompileEngine refactor. (#2059),5
print import_llvm ir in tensorize tutorial (#2064),5
Add a testcase of dilated conv2d int8 (#2065),1
[FRONTEND][ONNX] fixed operator converter for Split in onnx frontend (#2038),0
"[CODEGEN][LLVM] Cache packed func ptr, lift alloca (#2070)",5
Allow to use negative index of array in python (#2069)* Allow to use negative index of array in python* Support negative index in array slice* Print index and array size in IndexError* Fix style,0
fix asan check heap-use-after-free (#2071),0
Fix a crash in android_deploy demo. (#2073),0
"[Frontend][MXNet] argmax, argmin ops support (#2048)",5
Fix conv2d int8 schedule on CUDA (#2074),0
[OPENCL] Make use of cpu device when gpu device doesn't exist. (#2076),5
[TEAM] vinx13 -> Reviewer (#2083),5
"[RELAY] CompileEngine update, nn conv2d, fix dense, pool. (#2082)",0
[TVM] [NNPACK] Modernize and improve NNPACK bindings (#2084),5
Add NNPACK to CI (#2085),1
[TOPI][CUDA] int8 group conv2d  (#2075),5
"[VTA] Improved RPC for VTA (#2043)* assign default port to 9091 as the documented* bug fix in printing RuntimeError and add an additional search path* pretty print rebuild runtime args* PRC => RPC* replace vta_config.json file path`build/vta_config.json` => `vta/config/vta_config.json`* undo the change in adding lib_search path* search vta_config.py file in vta/config* avoid exposing driver function calls, and use predefined `VTAMemGetPhyAddr` instead.* rename `tests/hardware/pynq` => `metal_test`* set config path back to `build` dir",0
[TOPI] depthwise-conv2d in NCHW[x]c layout for x86 (#2045),5
"[FRONTEND][ONNX]add Pad, ReduceMax, ReduceMin, ReduceMean and ReduceSum OP (#2061)* add Pad,ReduceMax,ReduceMin,ReduceMean,ReduceSum for onnx frontend* fixed pylint error and warning for frontend.onnx file* add implement v2 for Pad in onnx frontend* compatible with python 3.x* disable too-many-lines pylint check in frontend onnx* use random values instead in onnx frontend testing",0
[RELAY] Fix type info after mutation in simplify inference (#2093),0
[RELAY][PASS] General OpFusion. (#2090),4
[RELAY][OP] strided_slice (#2094),5
[Relay][OP]NMS (#1929),5
[Jenkinsfile] Build NNPACK and run tests in `ci-cpu` (#2095),2
Fix error in fuse_ops.cc (#2098),0
Update fuse_ops.cc (#2102),1
"[RELAY][PASS] Bind, FoldConstant (#2100)",4
"[RELAY][PASS] FuseOps, fix input fusion rule for conv2d (#2110)",0
[Bugfix] Recover original layout when alter_layout function return None (#2101),0
[RELAY] bugfix type functor caching (#2113),0
[RELAY][PASS] Make FoldConst context and target invariant (#2114),4
[NNPACK] temporary disable nnpack test (#2115),3
Docs: Fix links (#2118),0
Fix doc of strided_slice (#2103),0
[NNPACK] Add check for NNPACK being available (`nnp_initialize()` succeeding) (#2119)This fixes issues with failing tests on PowerPC.,0
clarify NNVM’s LLVM requirement (#2117),5
[RELAY][[PASS] Consolidate ForwardRewrite pass. (#2124),4
[TOPI] Improve performance for dilated convolution (#2107),5
[nnvm] Add caffe2 frontend (#1981),1
"[Relay] compute & schedule for relu, softmax (#2127)",5
[SCHEDULE] Fix boundary check (#2126)* Fix boundary check* Add unittest,0
[TOPHUB] fix x86 backend after introducing dilation (#2129),0
[HYBRID FRONTEND] Modify hybrid script to new interface; hybrid op supported; enable compilation_database in CMakeList.txt (#1757),1
Update README.md typo (#2132),1
Relay Op sprint (part 2) - Level 1 - log_softmax (#2128),2
[COMMUNITY] new community guideline (#2077),1
[TOPI] Minor fix in the LSTM recipe (#2131),0
[WIP] [RPC] clean up uploaded modules (#2121) [RPC] clean up uploaded modules,5
[RELAY]sch & comp for ops in nn.py (#2092),5
[RELAY][BACKEND] Enable PlanMemory in the graph runtime. (#2120),5
[Relay][Op] Add test for batch_flatten (#2134)* Add tests for batch_flatten and softmax* Softmax is already tested elsewhere,1
[RELAY]Slice_like support (#2014),5
[COMMUNITY] Update contributor list to reflect new guideline. (#2138),1
Update CONTRIBUTORS.mdmake name alphabetical,1
[TEAM] Huyuwei -> committer (#2139),5
[TEAM] adityaatluri -> committer (#2140),5
[TEAM] Laurawly -> committer (#2141),5
[TEAM] Lianmin Zheng -> committer (#2142),5
Add nick to committer (#2143),1
Update CONTRIBUTORS.md,1
fix dcgan layer naming overlap (#2145),0
Fix relative import in x86 conv2d (#2149),0
[RELAY] Move Layout to tvm Node system (#2125),4
tensorflow frontend supports user given outputs (#1913),5
[FRONTEND][TENSORFLOW] Enable strided_slice with fix. (#2002),0
[FRONTEND][TENSORFLOW] Fix a typo in _matmul (#2152),0
[Relay] Port LSTM to Relay for testing (#2011),3
Alter op layout for group_conv2d on CUDA (#2148),5
[TOPI] Fix atlest1d for reduce and squeeze (#2147),0
Reverse shape dims of weight type (#2155),5
[DOCS] fix link (#2157)* fix fname in comment* fix,0
[APPS] add an external dll call example (#2156),1
[RELAY][PASS] CombineParallelConv2D (#2089),4
"[RELAY]Testing Inception, Squeezenet, VGG port (#2013)",3
"[Relay][Op] Add compute, schedule, and tests for expand_dims and squeeze (#2133)",1
Compare relay and numpy outputs in graph runtime test (#2164),3
Relay reshape reshape_like compute and schedule (#2159),5
[RELAY][FRONTEND] Initial MXNet frontend support. (#2163),5
Fix str decoding error on non-English Windows (#2158),0
[COMMUNITY] @phisiart -> Committer (#2165),3
"[RELAY][OP] Move computes to cxx, enable concat as injective (#2166)",4
[RELAY]sch and compute for reduce ops (#2091),5
[PASS] PostOrderVisit (#2169),4
[RELAY] Add multiref trigger to ForwardRewrite (#2168),1
"[Relay] Register compute and schedule for upsampling, with miscellaneous fixes (#2171)",0
[RELAY]take and transpose comp and schd (#2135),5
[Relay] Densenet benchmark (#2154)* Port densenet to Relay* Invoke densenet test in __main()__* Even the spacing in the IR text format tests* Forgot to import densenet in init* Correct reference to densenet in test,2
[Relay][Pass] Fix CombineParallelConv2D (#2167),0
"[RELAY]full, full_like compute and schedule (#2170)",5
[RELAY][IR] Introduce IdNode to preserve var id across rewriting (#2178),5
[Relay]resize op compute and schedule (#2172),5
fixing nnvm tutorial typo (#2188),0
[Relay]where compute and schedule (#2179),5
[BACKEND][CODEGEN] C codegen with tests (#2161)* Implement C code generation with tests* Code cleanup* Implement C code generation with tests* Code cleanup* tabs to spaces* make lint compliant* update export_library and reserve unique C keywords* move ReserveKeywordsAsUnique to codegen_c* some documentation and code cleanup* use tvm.contrib.util for tempdir in testcases,1
[Tutorial]NLP Sequence to sequence model for translation (#1815)* [Tutorial]NLP Sequence to sequence model for translation* Review comments* Review comments updated,1
[FRONTEND][TENSORFLOW] Support AttrValue that has different types of value in a list (#2177),5
[TOPI] Add tensor multiplication. (#2106),1
Update comments for the API tvm.lower (#2193)tvm.Schedule ==> tvm.schedule.Schedule,1
[COMMUNITY] @grwlf -> Reviewer (#2190),3
dockerfile cpu changes (#2191),2
[DOCS] Introduction to Relay IR. (#2185),2
[Relay]collapse_sum and broadcast_to compute & schedule (#2180),5
[RELAY]missing schedules updated (#2196),1
[TVM] Fix segfault for CanonicalSimplify(x % -1) (#2194),0
Remove redundant item from langref/relay_op.rst (#2192)Remove redundant item `tvm.relay.sigmoid` from langref/relay_op.rst,4
Fix logging in autotvm record (#2195),0
fix llvm dependency bug (#2198),0
[Relay] Alter Op Layout (#2150)* [RELAY] Finish alter op pass* [RELAY] AlterOpLayout Pass* fix broadcast operators* fix broadcast operators* fix broadcast operators* Support concatenate* address comments* address comments* add comments* rebase,0
[PASS] InstrumentBoundCheckers pass (#2079)The pass which instruments checkers beforememory accesses (load/store).This allows to handle invalid memory accesses.The patch is related to issue:https://discuss.tvm.ai/t/array-bounds-checking/944,2
[Relay] Add support for tuple node in operator fusion (#2187),1
NOTICE (#2203),5
[TVM] Fix llvm codegen (div by power of 2) (#2204),0
[Relay][Pass] Fold constant tuple (#2201),4
added int type axis for relay reduce ops (#2199),1
[SCHEDULE] Fix code lowering when loop condition depends on outer axis. (#2208),0
Python security issue about mktemp() and abspath() (#2202),5
[DOCKER] inheritate javahome (#2210),5
[RELAY][PASS] Memorize FoldScaleAxis backward transform result (#2214),4
Run verifier during LLVM code generation (#2211),5
[RELAY][OP] end to end support for pad op. (#2213),5
[DOC][Relay]: Add API docs for Relay. (#1750),1
[RELAY] bugfix. (#2215),0
[Relay][RFC] Relay IR Text Format (#1781),5
[Relay] Parser Tests (#2209),3
Fix misprint (#2223),0
[RELAY][PASS] Fix expr subst and CombineParallelConv2D (#2218),0
Port from_nnvm to NNVM as to_relay (#2144),5
[DEBUG]Fix debugger message mess in display_debug_result (#2228)Signed-off-by: Zhebin Jin <zhebin.jzb@alibaba-inc.com>,0
[RELAY][PASS] Check Positiveness in FoldScaleAxis (#2220),4
[TOPHUB] Set vulkan as alias for opencl (#2230),5
[contrib][nnpack] remove training-optimized ops (#2224),4
[typo] fucntion ==> function (#2239)fucntion ==> function,5
[typo] sin ==> in (#2238)sin ==> in,5
fix dump ir (#2235),0
Fix misprint (#2243),0
Add test case of argmax for detecting out of bound access (#2234),1
[typo] fucn => func (#2240),5
[FRONTEND][TENSORFLOW]Add Split and realdiv op support (#2123)* Add Split and realdiv op support* Fix the pad calculation in the case of dilated convolution,0
Use unsafe_get in nnvm (#2247),5
[COMMUNITY] @masahi -> Committer (#2252),3
"GetChar() in base64.h should return int, not char (#2255)",5
add c backend to CreateTarget (#2256),1
Fix missing sigmoid intrinsic in C++ (#2231),0
allows constant param in op construct (#2257),5
Generate predicates for non-root iteration variables as well (#2258),5
[COMMUNITY] @ajtulloch -> Reviewer (#2236),3
[RUNTIME][GOLANG] TVM runtime for golang v0.1 (#1470),5
"Improve CanonicalSimplify to handle Min, Max(#2248) (#2261)Also enable Mul caching for more cases",5
[Hybrid Script] Support logical and/or; support 0 < a < 5 clause (#2264),2
Fix serialization issue (#2263),0
Allow long type values in shape list (#1806)* Allow long type values in shape list* Update build_module.py,1
Fix misprint (#2272),0
"correct mistake in muladd function logic (#2269)Doesn't make sense to have %1 = mul(%x, %y) computed but never use the result %1",1
[DOC]Remove non-existent parameter doc (#2277),4
Testcases of onnx (#2274),3
Fix a issue when running with graph_runtime_debug in python (#2271)* fix a issue when running with graph_runtime_debug in python;* add support to `debug_get_output` in python;* comply with the linter;,0
[FRONTEND][TENSORFLOW] Bugfix (#2267),0
[AUTOTVM] Use range in AnnotateSpace to fix JSON serialization (#2278),0
typo: Xlinx => Xilinx (#2283)typo: Xlinx => Xilinx,5
[BUGFIX] [Hybrid Script] fix in-correct value index in hybrid script (#2268),0
[FRONTEND][TENSORFLOW] Support Unstack and Split (#2105),5
[DOC]Update documentation (#2286),1
[DOC] fix installation doc (#2290),0
[RELAY] Fix alter_op_layout (#2289),0
"[TOPI] NCHWc added input shape 4 condition, intel graphics conv2d schedule debugged for inception_v3 workloads (#2265)",0
[CI] Golang unit test trigger for Jenkins (#2266),2
[RELAY] Support concatenate. (#2298),5
[RELAY] Add broadcast_to operator (#2276),1
added error checking to loading symbol json (#2301),0
[Relay][doc] Update the description of returns in mxnet.py (#2309),1
[PASS] Avoid recursion in FoldScaleAxis (#2299)* [PASS] Avoid recursion in FoldScaleAxis* remove GetForwardScale,4
add relay and autotvm in readme (#2312),1
Bundled interpreter demo (#2297),5
[Hybrid Script] Inter-function call supported! (#2287),5
[DOC] Codebase walkthrough with vector add example (#2273),1
[TVM] Move check_numerical_grads to tvm.testing_ (#2314),3
[relay][op] multibox_transform_loc (#2315),5
[COMMUNITY] @eqy -> Committer (#2311)* Add Eddie to committer* Fix order,0
[Relay][Frontend] Add MXNet test example for relay (#2316)* Add MXNet test example for relay* Fix a bug in BiasAddSimplifier,0
[BUGFIX] Seg fault in memory planing for symbolic shape (#2317),0
Small refactors and bug fixes. (#2281),0
"[NNVM] Fix dtype of output of pad. (#2331)Dtype of output of pad should follows input, but if dtype of input is not float,  output will still be float becase pad_value is float.",0
[ROCM] Make sure all bit code files exist (#2323),2
[Relay][docs] Details on comp. graphs in Relay dev intro (#2324),2
[RELAY] Add missing arg in vgg (#2329),1
[Relay][Docs] Fix broken bullet points in Relay operator addition tutorial (#2325),0
[RELAY][AUTOTVM] Extract tuning tasks from Relay programs (#2181),5
[FRONTEND][TENSORFLOW] Bugfix (#2326),0
"[DOCS] typo ""@func myfunc"" => ""func @myfunc"" (#2333)typo ""@func myfunc"" => ""func @myfunc""",2
[relay][frontend] Enable ssd test by attaching schedules to multibox and ssd ops (#2322)* add ssd ops to mxnet.py* add ssd ops to mxnet.py* add result check for multibox and nms unit tests* add result check for multibox and nms unit tests* address @kevinthesun's comments* Disable cuda test for nms for now.,1
Add a the ability to trigger debugging in the interpreter without recompiling (#2219),0
[TOPI][CUDA] Add reorder option in int8 conv2d (#2327),1
[RELAY] Inline scalar compute (#2335),5
"[NNVM] Fix dtype of output of mean. (#2334)dtype of count is the same as dtype of inputs[0] when created, but its type may  change when multiplied by inputs[0]->shape[i]. Which causes dtype of  output is not same as dtype of input.",0
[Relay][OP] Add cast op (#2319)* Add cast op* Rename dtype_cast to cast* Add additional safety check for String2TVMType* Add missing relay op docs,1
[COMMUNITY] @srkreddy1238 -> Committer (#2339),3
[TEST] Setup frontend stage (#2347),3
[RELAY][EXPR] Make const numpy consistent (#2349),5
[FRONTEND][TENSORFLOW] Use input shapes directly instead of 1-element lists (#2242),2
Update cuda softmax schedule for spatial inputs (#2338),1
[TOPI] Add roi align (#2350)* [TOPI] Add roi align* Refactor bilinear in image resize* Rename to roi_align_nchw* Fix,0
Mutate free variables in CommReducer in cache_write (#2354),5
[RUNTIME] Add min_repeat_ms to time_evaluator (#2200),1
[Relay][Docs] Relay Language Reference (#2232),2
Fix clock type in rpc_session timer (#2362)* Fix clock type in rpc_session timer* Fix lint error,0
Fix typos (#2367),0
[Hybrid Script] Unify the symbol tables to one; support `tvm.container.Array` (#2366),2
Remove duplicated functions in relay_integration (#2370),4
Enhanced simplification rules for Div by a positive constant (#2346)* Enhanced simplification rules for Div by a positive constant* Fixed my last commit to correctly interpret TVM's division as truncated division* Fixed implemenation of IntSet::can_prove_non_positive()* addressed comments by @yzhliu* addressed comments by @sgrechanik-h* addressed more comments by @yzhliu,0
[nnpack] Preallocate workspace buffer (#2369),5
[Relay][Frontend] Keras Support (#2336),5
[Relay][Keras] force const dtype to be float32 (#2376)* [Relay][Keras] force const dtype to be float32* fix pylint,0
Recover web emscripten support (#2379),5
[TOPI][CUDA] Fix nms block extent and type mismatch in multibox (#2320),0
[RELAY] Port winograd ops to relay (#2356),5
Improve if_parser_enabled (#2372),5
Fix test-test-set (#2384),0
Add functionality to optionally disable Select rewriting (#2385),1
[LLVM CodeGen] Solve LLVM CodeGen br instruction accept not-i1 type issue (#2381),2
Added LLVM TargetIRAnalysis pass (#2386),1
Remove redundant FoldConstant pass (#2387),4
"[RUNTIME][TRACE] Support trace primitive. (#1973)Add trace call expr to allow trace Tensor dataat the runtime. By default the default handleris enabled which prints a tracing data to stdout,otherwise user should specify a call_back asglobal_function (aka @tvm.register_func).The issue is related to:https://discuss.tvm.ai/t/idea-trace-expression/945",1
[PASS] not vectorize if_then_else (#2389),4
add .clang-format (#2395),1
Fix LLVM initialization again (#2399),0
[RUST][CI] Add rust frontend tests in Jenkins (#2375),1
relax rtol/atol checks on some onnx tests (#2403)relax the error constraints on these tests due to likelyFP compuation accuracy issues.,0
relax rtol atol for div/log operators (#2400),2
"Revert ""[RUST][CI] Add rust frontend tests in Jenkins"" (#2406)",1
[PASS]Treat Halide call_type as pure expression (#2404),4
Fix Web Build after CMake transition. (#2407),0
"Add support for passing arguments by args and kwargs when using executor (#2402)* Add support for passing arguments by args and kwargs when using executor* Fix linting* Update comment, and add arity checking* Small tweak to error message",0
[TFLite] CI recipe. (#2371)* [TFLite] CI recipe.* * Custom bake tflite package and install.,2
[ARM][Performance] Improve ARM CPU depthwise convolution performance (#2345)* Add sptialpack schedule for arm cpu depthwise convolution* Supply comments.,1
[LLVM CodeGen] Partially disable unsafe fp math (#2422),5
allow const_range allocation; preprove if-then-else (#2419),5
[PASS][TENSOR] Use correct select semantics (#2394),4
[TVM] Reduction simplification improvements (#2284),5
Add simplify pass in the Relay interpreter (#2417),1
[RELAY][FRONTEND]Onnx to relay frontend (#2302),5
[relay][pass] Annotation for heterogeneous compilation (#2361),4
[FRONTEND][TEST] Remove duplicated frontend tests (#2414)* Remove duplicated frontend tests* Add test for nnvm to relay* Add test to script* Remove dcgan in nnvm_to_relay test due to unsupported op in cuda* Fix bug in converting conv2d from nnvm to relay* Fix dropout,0
[Hybrid Script] allow const_range allocation; allow const_range lazy compilation (#2423),5
[X86][TOPI] Add AutoTVM template for dense (#2392)* Add GEMM autotvm template for x86* Fix tophub link* Disable RPC server logging file delete* Update dense autotvm template* Fix tests* Fix lint* tweak* Register two templates with different tags,0
[Tensorflow] Support for Crop (#2285)fixesfixes,0
[RELAY] Fix function call parsing for binary op (#2424),0
[Relay] First order reverse mode Automatic Differentiation (#2321)* initstaging onfinal save before stagingsaveinit verinit verll stuffsaveadd failing testadd filepass testfix errorhuh?saveAdd test changesFix fusion with nested tuplesFix reverse mode testMore hackingClean up ADHacking on reverse modeFix issue in reversesavefix lintfix lintfix lintsavesavefixsupport negaddress some commentadd back filesavesave* save* save* save* lint* fix lint* save* fix,0
[RELAY] Filter PlaceholderOp from schedule. (#2412),5
[RELAY] Support recursive call syntax (#2352),5
[Relay][ONNX] fix bug in from_onnx (#2430),0
[Hybrid Script] Supporting scheduling hybrid script (#2416)* on the way to enable hybrid schedule* I think I am done with imperfect loop split?* copyright watermark* loop annotation* fix lint* fix lint 1* shit!* loop reorder supported* support bind to add some tests* fused tested* imperfect loop testcase* fix lint* add bind testcase* fix doc* fix online edit typo* resolve @mercymercy review* fix indent* i should convince myself it is not flaky test first* fix test hybrid* how many flaky test are you expecting; i ball ball u to let me pass* rebase halide...,0
[Doc][Tutorial] Add the instructions how to use contrib_spatial_pack (#2427)* [Doc][Tutorial] Add the instructions how to use contrib_spatial_pack* Update the code according suggestions,1
[Relay] Expand type unification and other utilities (#2189),5
fix handling a tuple node in op fusion (#2433),0
"Revert ""[Relay] Expand type unification and other utilities"" (#2434)",5
[Relay][Docs] Fix local variable code example. (#2435),0
[OP] Fix reduce op problem when axis=None (#2436),0
"[DOCKER] add ci tag, upgrade gpu-ci (#2438)",1
Fix links and formatting in langref (#2440),0
[Relay] Fixes to sum (#2439),0
[NNVM][TENSORFLOW] bugfix. (#2444),0
[Relay] Unifier hotfix (#2437),0
[Runtime] Make runtime compatible with android ndk api 15 (#2446),5
[RELAY][PASS] Support Negative Scale in FoldScaleAxis (#2426)* [RELAY][PASS] Support Negative Scale in FoldScaleAxis* Fix comment,0
Avoid runtime exception when file doesn't exist (#2441)* Avoid runtime exception when file doesn't exist* Update the check based on feedback* Revert the old fix,0
"[Relay][Parser] Improve Relay parser and pretty printing, including CMAKE (#2377)",5
Update docs for some new modules (#2454),1
move fallback out of the build interface (#2456),4
[TUTORIAL] Introduce frontend folder (#2457),5
[DOCS][COMMUNITY] Improve code review guideline on API designs (#2459),2
[COMMUNITY] @junrushao1994 -> Reviewer (#2463),3
Add gluoncv installation (#2464),1
Fix broadcast add and subtract grad (#2465),0
Fix typo (#2467),0
Frontend before tensor expression,5
[RPC] Add the IPV6 support for server side auto tuning (#2462)* use IPV6 instead of IPV4* backward compatible* add error report* fix linter* more linter* fix the python2 api,0
Fix rust tests (#2482),0
[TFLite] Support TFLite FP32 Relay frontend. (#2365)* Support TFLite FP32 Relay frontend.* Fix lint issue* Remove unnecessary variables and packages* Add method doc string* Fix capital letter of method doc string* Add TFLite FP32 Relay frontend based on latest code* Merge the latest code* Solve cython issue with relay type inference* Modify code based on suggestions,0
fix deploy_model_on_rasp.py spell error. (#2491)* Update deploy_model_on_rasp.pyspelling fix.* Update deploy_model_on_rasp.pytrigger CI,0
[RELAY] Fix ops in packed layout (#2472)* [RELAY] Fix ops in packed layout* Fix style,0
[AUTOTVM] typo (#2478)* [AUTOTVM] typo* trigger CI,5
Update rust contributors (#2500),1
[DOCS][COMMUNITY] Committer guide and tips (#2468),2
check in (#2484),5
[TEST] Remove script that references previously removed content. (#2481),3
[Relay] A Normal Form Canonicalization (#2251),5
1) fixed constant folding for mod operation in CanonicalSimplify 2) added a unit test (#2487),0
Fix Relay docs formatting and grammar (#2471),0
[TOPI] Fix uint8 resize_bilinear issue. (#2490)Signed-off-by: Zhebin Jin <zhebin.jzb@alibaba-inc.com>,0
[Relay] Add generic & informative Relay error reporting (#2408),0
[DOC]Update doc in _api_internal.py and ir_pass.py (#2514),1
Optimize Linux shared library modules (*.so files) (#2445),2
[Doc] TFLite frontend tutorial (#2508)* TFLite frontend tutorial* Modify as suggestion,5
[BugFix] Copy intermediate result in debug runtime (#2520),0
[Relay][Frontend] CoreML Support (#2476)* [Relay][Frontend] Add CoreML Support* pip install six in CI* remove triggering nnvm coreml test* set opt_level=2 for nnvm coreml test case,1
"[BugFix] SSD fully supported on GPUs, updated deploy_ssd tutorial (#2510)* nms fixed for gpu, tested on cuda and opencl devices, ssd now can run fully on the gpu* sort updated to use virtual thread* typo fixed* fix lint* fix lint* add support when batch_size > 1* intel graphics conv2d bugs fixed for inception_v3* intel conv2d api updated, nn input size 4 condition added* review addressed* move conv_tags to attributes* opencl ctx fixed* nms_ir index simplified",0
Remove an obsolete comment (#2527),4
add min_repeat_ms to other CUDA tutorials (#2526),1
[TFLite][Python 2] Solve TFLite frontend python 2 compatibility and Modify TFLite whl files path (#2529),2
[OPT] Low-bit Quantization (#2116)* [QUANTIZE] Quantization implementation.* Update.* Update.* Update.* Update.,1
Update rustc (#2524),1
Add fallback for ApplyGraphBest (#2485),1
[RELAY] Copy subfunction arguments to output tuple field (#2537),5
Fix typo in Evaluate inference time cost code (#2542),0
[DOCS][RELAY] Sync up ops with code base (#2532),2
[COMMUNITY] @FrozenGene -> Reviewer (#2544)* [COMMUNITY] @FrozenGene -> Reviewer* Fix,0
[CI] Enable ANTLR in CPU env (#2548),5
[Relay][Frontend] Caffe2 Support (#2507)* [Relay][Frontend] Add Caffe2 Support* [Relay][Frontend] Add Caffe2 Support (fix unsed import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix model install and reflect code reviews)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 frontend import)* [Relay][Frontend] Add Caffe2 Support (rename function name in test_forward)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Relay][Frontend] Add Caffe2 Support (fix caffe2 model import)* [Doc] Caffe2 frontend tutorial* [Doc] Caffe2 frontend tutorial* [Doc] Caffe2 frontend tutorial* [Relay][Frontend] Add Caffe2 Support (remove unsed file),0
[CI] Update rust format version (#2550),1
[RUST][FRONTEND] Add rust frontend v0.1 (#2292),1
[RELAY] TextPrinter: Use Map Format (#2553),5
print ast w/o metadata (#2533),5
[VERSION] Move version script to the project root (#2556),4
Remove duplicate as Checks and CHECK value (#2531),4
"Misc refactor on graph runtime, layout node (#2557)",5
[Relay][OP] Add reverse_reshape (#2503)* Enable reverse in reshape* Fix lint and typo* Put reverse_reshape into a separate op* Fix pylint,0
"[RELAY][FRONTEND] Tensorflow frontend. (#2216)* [RELAY][FRONTEND] Tensorflow frontend support.* * LSTM removed for a while.* * basic ops are good.* * nn wip* * wip* * python2.7 corrections.* * NN ops are good.* * e2e models working good* * all good except LSTM* * rebase, tutorials and CI trigger.* * CI errors.* * enable opt_level=3* * Docstrings cleanup. testing.tf utils moved to relay from nnvm.* * tutorials update.* * LSTM work good now.* * Rebase* * CI error* * enable PTB.* * rebase.* * tutorials* Update python/tvm/relay/frontend/tensorflow.pyCo-Authored-By: srkreddy1238 <sivar.b@huawei.com>* * review comments.* CI fix.* * review comments.",0
[Python dep] Add missing dep pkg for relay (#2568),1
[Golang] bugfix #2517 (#2558),0
[TVM][BUGFIX] Fix missing reduction init predicates (#2495)* [TVM][BUGFIX] Fix reductions in split axes* A test case for the problem* Fix the fix: skip loops that are related to reduction AND are unrelated to axis,0
[BUGFIX] Fix for quantize. (#2573),0
Update plan_memory.cc (#2574),1
Optimize move semantics of NodeEntry reducing copies of shared_ptr which causes atomic contention (#2576),4
"Add tf parser wrapper, infer shape automatically",1
Support TensorFlow saved modelTF parser: return the consistent error message to error handler,0
Get tags of saved model automaticallyRemove exception trail in tf parser error messageFix lintFix comments,0
[Team] @merrymercy -> PMC (#2578),5
Conda packages with cuda support (#2577),5
[IR] Update HalideIR (#2582),1
[TUTORIAL] Fix downloaded file path (#2590),0
A couple of fixes for GEN (#2593),0
Tighten buffer bound for TensorComputeOp by improving EvalSet on ranges (#2565),5
"Fix typo (#2595)thanks @icemelon9, this is merged",0
[AUTOTVM][RELAY][DOCS] relay ports of `tune_nnvm_*` autotvm tutorials (#2594),2
[TEST] Remove script that references previously removed content. (#2596),3
"[Hybrid script] Backend support (#2477)* a preliminary version is done?* we no longer need the redundant hybrid/api.py* support assert stmt* cast supported* intrin -> runtime; util is mainly in charge of compilation time* assert statement* fix python lint* fix cpp lint* on the way to module* rollback .cc* fix typo, no direct expose then* @vinx13 ceil is added i guess?* wip...* temp commit* fix import* i preliminary version is done?* on the way to build hybrid module* nearly fixed...* dumped python are equiv as original python* on the way to bootstrap* cpu bootstrap done* bootstrap!* fix lint* fix doc* resolve some review concerns* support load/save* fix lint* thanks to xqdan fixed my typo* fix build, make dump non-optional* add vthread* jesus why i added this",0
[RUNTIME] Enable NDArray type extension (#2598),5
[TOPI][CUDA] Add faster-rcnn proposal op (#2420)* [TOPI][CUDA] Add faster-rcnn proposal op* Fix doc* Add global barrier* Use vthread in argsort* Update sort and nms ir* Fix lint* Update sort ir in ssd nms,0
[TVM][Bugfix] fix storage_rewrite bug when input is big (#2580)* fix storage_rewrite bug when input is big* cast when necessary* simplification* simplification* int64->uint32* revert uint32->int64,0
[DOCS] update titles to reflect tutorial content (nnvm vs. relay) (#2597)* update titles to reflect tutorial content (nnvm vs. relay)* move things around* fix typo,0
[Relay] Reference (#2489)* movefix testfix lintfix testadd more codefix lintbetter type infer ability* fix build* address comment,0
Version 0.5 (#2604)* Version 0.5* update version.py* update news* update news* update news,1
fix get layout in to_relay (#2610),0
"[Relay] Algebraic data types (#2442)* First pass on ADTs* Add doc string for tag field* Visit constructors in TypeVisitor for TypeData* Add to description of type call* Add type call to type solving and unification* Make type mutator for typecall consistent with others (only create new node if there's a change)* Ensure kindchecking can handle type calls and typedata* Fix bad nesting in module constructor* Correctly construct call in typecall test* Add call override for ordinary vars (do we want this?)* Remove generalization hack from type inference because it was breaking ADT constructors* Check that there are no free type vars in exprs after inferring type* Free var checks need module because of ADT constructors* Typecall test can't have unbound type var, make it global* Uncomment tmap test and remove comments about failing to infer ret type; those work now* Put in dummy visits for ADTs in graph runtime codegen to placate pylint* Fix Relay type infer test module constructor* Mark override for TypeCallNode in type solver* Ensure free vars check treats patern vars as bound* Run interpreter in more ADT test cases* Refactor kind check to return the kind, like typechecking* Fix invalid typecall in test* Add kind check to type inference, do not use nulls in func_type_annotation()!* Redundant whitespace* Make TypeData a separate kind* Make ADT handles a separate kind too, document calling convention better* Remove nats and tree from prelude, move to test, document prelude* Restore and document nat and tree to prelude, add more tree tests* Add alpha equality tests for match cases, fix variable binding bug* Add more kind check tests for ADTs* Add more tests for finding free or bound vars in match exprs* Add unification tests for type call* Update main() for alpha equality tests* Add simple type inference test cases for match exprs and ADT constructors* Add more ADT interpreter tests* Allow incomplete types when typechecking match cases* Type inference for pattern vars should use the type annotation if it's there* Two more specific test cases for ADT matching* Add option ADT to prelude* Fix broken reference to kind enum* Fix rebase snags* Do not attach checked types to constructors* More docstrings for module fields* Use proper wrapper for indexing into module type data* checked_type for constructors is not populated* Expand type call docstring* Rename PatternConstructor con field* Use error reporter for pattern constructor case* Condense error reporting in kind check, use error reporter* Expand docstrings and rename ADT fields* Rename 'option' ADT to 'optional' for consistency with Python* Add various list iterators and utility functions to prelude* Add smoke tests for new iterators in prelude* Add concat to prelude* Add smoke test for concat* Correct docstrings in prelude* Ensure that type defs are written in module initialization* Various requested renamings* Correct rebase snags* Add kind check tests for ref types* Update the main() for kind checking tests",0
[Quantize] Skip for same input-output domain scale. (#2611),5
[RELAY][DOCS] Port from_mxnet tutorial to relay (#2608)* check in* update build and run,1
[RELAY][TOPI] `alter_op_layout` for x86 (#2602)* alter_op_layout for x86* cleanup* cleanup* fix lint* fix lint* fix lint* fix lint* change support level* change other support levels,0
[Bugfix] Nms_ir data_race solved (#2600)* nms data race solved* tst_topi_vision reference results are gonna be updated in PR #2353* proposal nms_ir updated,0
Fix the FInplaceIdentity (#2572),0
[Relay][Docs] Documentation for Algebraic Data Types (#2575),2
Fix issue mutating if expressions (#2601),0
[EXPR] Expression-template based pattern matching. (#2589),5
[TVM][LANG] Add eager simplification for operations with FloatImm (#2615)* Add eager simplication for FloatImm* fix* fix lint* Fix gcc warning* fix* Add test case,0
remove batch_norm_inference (#2626),4
[RELAY][OP] ROI Align (#2618),5
[RELAY] Stop_fusion annotation (#2624),5
[Relay]fix heterogenous annotation bug (#2622),0
Update task_python_vta.sh,1
"[Bugfix] Missing #include <sstream> (#2629)Thanks @hlu1 and @tqchen, this is now merged.",0
[autotvm] fix #2617 (#2619),0
[RELAY][PASS] add a relay pass to count #macs of a model (#2609)* add a relay pass to count #macs of a model* remove unnecessary includes* fix end-of-file issues* address review comments* remove test resnet* address more review comments* use data layout string to locate the input channel* fix bug in conv 2d output tensor checking* C must exist,0
check in (#2627),5
Add test case for expr_functor.py (#2632),1
[Tutorial][Frontend] move from_keras tutorial to frontend (#2479)* [Tutorial][Frontend] move from_keras tutorial to frontend* remove tutorial/nnvm/from_keras.py,4
Fix fusion bug when call symbol that is not an operator. (#2630),0
[RUNTIME][NDArray] Allowing External Libraries to Subclass NDArrays (#2613),5
Fix pylint 2.2.2 gripes. (#2642),0
add MXNet converter for where operator for both NNVM and Relay (#2647),1
[Quantization][RELAY] Add check against NCHWc ops in the quantization pass (#2646)* check in* fix typo* fix typo* change message* change message* typo* lint,0
Stop pylint complaining about useless import alias. (#2655)Recent pylint warngs about import renames with no effect.  Removethem.,4
Explicitly disable pylint warning subprocess-popen-preexec-fn (#2656),5
[RELAY][PASS]use attribute registration style in the mac count pass (#2645),4
[Relay] fix anf for reference and pattern matching (#2637),0
fix lint (#2649),0
[RELAY/OP] Gradient of relay level1 ops (#2633),5
Update community.rst,1
[Relay] GNF (#2492),5
add committer (#2661),1
[Relay/TOPI][OP] Add arange op in Relay and TOPI (#2621)* Add arange op* Update docs* Fix bug* add sanity check in relay and mxnet frontend mapping* lint* nits* pylint* don't allow empty output from arange* Remove empty test for arange* Fix bug and update doc,0
Fix -Wreturn-std-move and -Wself-assign-overloaded (#2669),0
[Relay] add more function to prelude (#2660),1
[BUILD] Simplify after bind device type (#2670),5
"[Hybrid Script] Add `max_num_threads` (#2672)* i think it works for now?* fix lint* fix 2/3 compat* fix py2 again* fine, i gave up",0
fix (#2674),0
[Relay] fix error in ANF (too agressively inline atomic expression and create free variable). (#2665),0
"Add CONCATENATION to tflite frontend, support Inception V3 (#2643)* Add CONCATENATION to tflite frontend* fix typo* Fix codestyle* Fix code style* simplify convert map* Update",0
[AUTOTVM][Bugfix] Fix history loader for heterogeneous execution,0
[Graph Runtime] Run_individual for benchmarking individual layers (#2569),5
REGION op removed from topi and added in darkent frontend (#2275),1
yolo reorg op for relay (#1941),2
[Relay] Ensure nested higher-order functions are treated correctly (#2676),5
[Relay] add more descriptive error for checked_type (#2652),0
[Relay] Port param dict save/load from NNVM (#2620),5
add converter for MXNet slice in nnvm and relay (#2662),1
[PYLINT] Disable consider-using-get (#2654),5
[DOC] CoreML frontend tutorial (#2667)* [DOC] CoreML frontend tutorial* Update tutorials/frontend/from_coreml.pyCo-Authored-By: kazum <morita.kazutaka@lab.ntt.co.jp>* Update tutorials/frontend/from_coreml.pyCo-Authored-By: kazum <morita.kazutaka@lab.ntt.co.jp>* Addressed comments and added the original author,1
Support mean in NNVM to Relay converter. (#2680),5
Stop pylint complaining about unnecessary return statement. (#2684)Recent pylint introduced support for the useless-return diagnostic.This patch remove the useless returns.,4
[RUST] Fix typo (#2681),0
Handle Select in IntSetEvaluator (#2687),5
[CODEGEN LLVM GPU] Initialize llvm before lookup for the target (#2683),5
"[RELAY] Fix get_int_tuple for shape like '(1001,)' (#2691)tshape.strip('()[]').split(',') will make a list ['1001',''] but the empty one isn't needed.",0
[AUTOTVM] tweak `sample_int` implementation (#2677)* check in* lint* cleanup* Update util.py,1
"[Lang] Layout in TVM node system (#2509)* move layout.h & layout.cc from relay to tvm* change ConvertLayout in relay to bijectiveLayout->Forward/backward* add first test case* add LayoutAxis* add LayoutAxis struct and compiles* simplify BijectiveLayout rule consturct* polish func name for Layout, move impl to .cc, remove Layout::defined(), add defined() checker* partially add layout py support* add layout test cases* add doc for tvm.layout & tvm.bijective_layout* fix lint* fix lint* fix layout name generation bug* fix layout typo* address comments and add topi.layout_transform* layout.h->data_layout.h, test_lang_layout.py->test_lang_data_layout.py",0
[DOC] Using External Libraries in Relay (#2694)* added relay quick start* added relay/using_external_lib.py* update using_external_lib* Update using_external_lib.py* update tvm/make/config.mk -> cmake/config.cmake* Fixed: result mismatched when lowering relay with cudnn support at opt level 2* setting opt_level=2 and out_channels=16 for consistency of original tutorial* Fixed some typos,0
[RELAY][PASS] Enable switching CanonicalizeOps in pass_enabled (#2696),4
Docker updates (#2702)* [DOCKER] Switch from yes|apt-get to apt-get -yThe yes | apt-get idom guarantees that the 'yes' process always existswith exit code 141 (pipe broken).  This is fine while the scriptgenerally ignores failures but won't work when the script behaviour istightened to robustly catch errors.* [DOCKER] Turn down the wget/curl volume,0
[Relay][Doc] Separate arguments types formatting with comma (#2690),5
[DOC] MXNet frontend tutorial (#2688),5
Few docs fixes (#2703),0
Pin pylint version 2.2.2 (#2698),5
[Relay] fix checkwellform (#2705)* do* address comment,0
support MXNet _minimum and _maximum (#2709),5
[TOPI][Relay] Fix default `out_dtype` for `conv2d_NCHWc` and Relay (#2707),0
"Improve task_lint.sh robustness (#2711)* [SCRIPT] Refactor grep for multiple patternsTidy up the use of grep.  Use -E rather than run multiple grepinstances.* [SCRIPT] Refactor grep use in pipeline.Prefer to use stdin redirection rather than create a pipeline.* [SCRIPT] Refactor placement and cleanup of temporary files.Place temporary files in the conventional /tmp location. Avoidpoisoning file name space by using $$. Ensure the temporary files getcleaned up, even when the script fails / exits early.* [SCRIPT] Improve robustness of task_lint.sh error handling.Ensure script failures are caught and propagated.  Rather than tryingto explicitly catch and propagate failures with explicit ""|| exit""annotations, use the ""set -e"" idom from docker/install scripts andhave the shell catch and propagate errors in the general case andspecial case the grep instances where non zero exit is permitted andshould be ignored.",0
"Docker build script robustness (#2710)* [DOCKER] Make all install .sh scripts directly executable.* [DOCKER] Use curl -L consistently.Make the use of the curl -L option in docker build scripts consistent.* [DOCKER] Drop use of --force-yesThe --force-yes option is generally not recommend, it can leavesystems in an undefined state.  The use of --allow-* options ispreferred.  In this particular case the --force-yes option appears toserve no purpose.  Dropping it.* [DOCKER] Drop superflous repeated apt-get update.The ""apt-get update && apt-get install"" idiom is necessary andspecific to Dockerfile.  In shell the repeated apt-get update issuperflous.  Drop the duplicates.* [DOCKER] Robustness -e -u -o pipefailThe install scripts used to construct docker environments do not, ingeneral, propagate errors.  Some of the scripts use adhoc &&directives to chain together short sequences of commands but there arenumerous failure modes which are silently ignored.  This patch puts inplace some consistent, basic, shell error trapping across all of theinstall scripts.Note this is a step forward towards more robust scripts but it is nota complete solution.* [DOCKER] Shallow clone.Use shallow clone to reduce bandwidth requirements of repeated docker(re)-builds.* [DOCKER] Use clone --branch rather than clone then checkoutUse the git clone --branch idiom rather than git clone && gitcheckout.  This paves the way for using --depth=1",0
[Doc] Relay tutorial - Deploy the Pretrained Model on Raspberry Pi (#2693),5
Defined a common base class for TensorComputeOp and ComputeOp (#2587)* Defined a common base class for TensorComputeOp and ComputeOp* Made changes requested by @ZihengJiang* added a testcase to assert that `tensorize` does not have any effect on TensorComputeOp ops.,1
[Relay/TOPI][Op] Add batch_matmul in relay and TOPI (#2561)* Add batch_dot and cpu schedule* Add relay support for batch_dot* Rename batch_dot to batch_matmul* nits* Add missing file* Put batch_matmul and dense x86 schedule in separate files* Fix pylint* Remove unused import* Add cuda schedule for batch_matmul* Add test case with larger batch size* Add batch_matmul in api doc* Fix quantize pass rounding error* Fix pylint and minor change* bug fix,0
"[ARITH]  Analyzer Infra, ConstIntBound, Modular (#2668)",5
[EXPR] ir_operator.h->expr_operator.h Centralize const folder logic (#2719),2
[RELAY][PASS] Common subexpression elimination (#2639),4
"[Tensorflow, NNVM, TOPI] Support for logical operators (#2453)",2
[Relay][Frontend] Add a few mxnet ops in relay frontend (#2704),1
[Relay][Frontend] Add slice axis op in mxnet converter (#2706)* Add slice axis op in mxnet converter* Fix lint,0
[DOCS] Fix tutorial (#2724)* fix docments* delete e,0
[Relay] Higher order reverse mode automatic differentiation that work with control flow (#2496)add testremove dead codestashdo itadd more test,1
Fix compilation on XCode 10 (#2731),0
[DOCKER] Pin pylint==1.9.4 (#2727),5
Docs: pip dependencies for testing (#2728),2
[COMMUNITY] @sgrechanik-h -> Reviewer (#2732),3
use LLVM linker (#2713)* use LLVM linker* error message improved in case of filenotfound* linting error fixed,0
[RELAY][OP] Faster-RCNN Proposal OP (#2725)* [RELAY][OP] Proposal* Fix* Fix test,0
[Relay][Frontend][Bugfix] Fix bug in converting slice_axis when axis is negative (#2739)* bug fix* trigger ci,0
[VERSION] Update to 0.6.dev (#2736),1
"[Relay][TOPI][OP] intel_graphics conv2d alterlayout support relay, added stack op (#2729)* add stack op frontend* concate moved* topi stack added* stack added* fix stack bugs and tested* conv2d alterlayout udpated for relay* fix pylint* fix cmake warnings* cmake warnings fixed",0
[RUNTIME][OPENCL] clFinish before releasing memory (#2737),5
[Bugfix][Relay][Frontend] Fix bug in mxnet converter for slick_like (#2744)* Fix bug in mxnet converter for slick_like* More tolerance for topi_conv2d_NCHWc,0
"Improve NNVM to Relay conversion (#2734)* Improve NNVM to Relay conversion* fix pylint* support __lshift_scalar__, abs, ceil, floor, and trunc to pass CI",0
[Relay] Add logical operators (#2743),1
Fix vmlal.s16 code generation for int8 x int8 -> int32 (#2748),0
revert PR#2420 nms changes (#2747),4
[Relay][Quantization] Speed-aware quantization scheme improvement (#2723)* [Relay][Quantization] Speed-aware quantization scheme improvement* Add comment* Add use_stop_fusion to qconfig* Update comment,1
[RUNTIME][OPENCL] set type_key even when platform is not available (#2741),5
[DLPACK] fix flaky ctypes support (#2759),0
Improvements to the conda build (#2742),5
[COMMUNITY] @kevinthesun -> committer (#2760),3
[WIN] Fix a bug in find_llvm when specify llvm-config (#2758),0
fix typo in backend interpreter (#2752),0
[ARITH] Analyzer RewriteSimplifier: add/sub/mul/div/mod (#2722),1
Add the new logical operators to the doc. (#2761),1
update relay python api doc (#2766),1
[Relay/TOPI][Frontend] Add tile and repeat operators in Relay and TOPI (#2720)* tile and repeat operator added in rely* fix pylint* fix make warnings* comments addressed* fix lint error* comment addressed,0
[relay][frontend] TensorFlow saved model support (#2586)* [relay][frontend] TensorFlow saved model support* Add Examples section* keep one copy of tensorflow_parser in relay,1
[Object Detection] Gluoncv SSD support on CPU (#2353),5
Implement flop support for int8 models (#2776),5
[Relay] Improve more operator mxnet frontend importer (#2772),5
[TEST] Hotfix CI outrage after TF in docker update (#2781),0
"[Relay] Pass manager (#2546)* initial commit* add python frontend and module tests* add unit tests for function pass and optimize interface* add ExprPass* remove PassState and pass context for run* add required_passes* return module* remove move* fix minor reviews* remove optimizer, optimizer->pass_manager, make pass a the base class of all* remove deleted files* move resolvedependency to sequential pass, use ir_pass namespace* add todo* add disabled passes in sequetialpass* fix minor* fix currying doc* remove pass_kind from passnode* remove pass kind from test* fix doc* fix per @tqchen's comments* remove pass_manager.py create separate classes* simplify pass_func* inline using passfunc* update doc* disable test_quantize_pass for now* create PassInfo class to contain the meta data* flatten passinfo for interface* retrigger ci* remove required method* make Pass python class lighter* create pass -> decorator* make the api consistent for all classes",0
[TEST] recover tflite test (#2788),3
Make topi cuda nms_gpu method signature similar to non_max_suppression (#2780),5
[DOCS] Cleanup the relay docs location (#2785),2
[DOCS] Phase out nnvm tutorials (#2783),2
[COMMUNITY] @wweic -> Reviewer (#2789),3
[DOCKER] Update docker protocol (#2793),1
Make cpptest build on Ubuntu (#2798),3
Ensure loop count is a constant before trying to unroll. (#2797),5
[Relay][Frontend] Add reverse op to relay (#2800)* start adding reverse* reverse updated* reverse uses topi::flip* typo fixed* comment addressed* exp simplified,0
[Relay/TOPI][Op] Add shape op in Relay and TOPI (#2749)* Add shapeof op in topi* Add relay shape_of op* Add constant folding for shape_of* Allow shape op to specify dtype* Add mxnet converter for shape_array* lint* lint* Add doc,1
[COMMUNITY] @sxjscience -> Reviewer (#2807),3
Update HalideIR and dmlc-core to the latest (#2809),1
Support for sign (#2775),5
Fix a bug in nnvm to relay converter. (#2756),0
Fix caffe2 relay frontend (#2733),0
"[ARITH] RewriteSimplifier: min/max, logical, select (#2768)",2
"[Bugfix] Repeat and tile bug fixed, relay tests added (#2804)",0
[Relay][Quantization] Fix duplicated simulated quantization (#2803),0
[GRAPH] Include default metadata description in graph. (#2770),5
"[Relay] Add hd,tl,nth for list in Prelude (#2771)",1
"[TOPI, Relay] ROI Pool operator (#2811)",5
upgrade java style-check due to CVE-2019-9658 (#2817),5
"[DOCKER] Fix git clone failure. (#2816)git clone --branch=xxx won't take a hash, switch from the hash to thetag that represents that hash.",0
Fix init_proj.py: Team ID expected (#2824),0
tvmrpc: Fix includes (#2825),0
[COMMUNITY] kazum -> committer (#2831),3
xcode.py: Decode bytes before output (#2833),5
[Relay] Adding _contrib_BilinearResize2D op from mxnet (#2777)* adding _contrib_BilinearResize2D op from mxnet* error fixed* use resize instead of upsample,0
Fix Xcode 10 metal compile error (#2836),0
Fix typo (#2839),0
"[CODEGEN][OPENCL] Fix compile error about ternary expression. (#2821)Code like this can't be built with NV OpenCL, and it needs an explicit type  converison for ternary expression if return type is uchar.       uchar i = 0, j = 0;       uchar t = max((uchar)j, ((i > 0) ? (uchar)1 : (uchar)0));",0
[TOPI][ARM] Improve injective schedule (#2801),5
"[DOCKER] Revert git shallow clone change. (#2841)This patch reverts one of my earlier patches (squashed in #2710) toreduce bandwidth requirements of git clone, in this particular case weare checking out a specific hash rather than a tag or branch name. The--branch option to git clone permits tags or branches but does notpermit a specific hash.",4
[DOCKER] Update ci-gpu to include NNPack (#2846),1
[RUNTIME] Scaffold structured error handling. (#2838),0
[FRONTEND][TENSORFLOW] Enhance with left over patches from NNVM. (#2757)* [FRONTEND][TENSORFLOW] Enhance with left over patches from NNVM.commit 76188a4Author: Siva sivar.b@huawei.com[NNVM][TENSORFLOW] bugfix. (#2444)commit 6737739Author: Ashutosh Parkhi ashutosh.parkhi@imgtec.com[Tensorflow] Support for Crop (#2285)commit f6c3f99Author: Alexey Romanov alexey.v.romanov@gmail.com[FRONTEND][TENSORFLOW] Use input shapes directly instead of 1-element lists (#2242)commit e5d92e1Author: Dominic Symes 36929632+dominicsymes@users.noreply.github.com[FRONTEND][TENSORFLOW] Bugfix (#2326)commit 00d509dAuthor: Alexey Romanov alexey.v.romanov@gmail.com[FRONTEND][TENSORFLOW] Support Unstack and Split (#2105)commit df9d3adAuthor: Siva sivar.b@huawei.com[FRONTEND][TENSORFLOW] Bugfix (#2267)commit d1a0c90Author: Zhebin Jin zhebin.jzb@alibaba-inc.com[FRONTEND][TENSORFLOW]Add Split and realdiv op support (#2123)* Add Split and realdiv op support* Fix the pad calculation in the case of dilated convolution* * review comments* * resnet fix.* * review comments,0
Update iOS RPC README (#2847),1
Pin rust version used to compile tools (#2852),1
[Relay][Frontend][keras] added interpolation method of Upsampling2D (#2854)* [Relay][Frontend][keras] added interpolation method of Upsampling2D.* added testcase* small fixes,0
[Docker] Enable NNPACK for ci_gpu (#2856),5
"[AlterLayout] NCHWc upsampling, fix depthwise conv (#2806)* [AlterLayout] NCHW upsampling* [Relay][Pass] Fix Depthwise AlterLayout",0
[Relay][Frontend] Add ops in mxnet converter (#2844)* Add ops in mxnet converter* trigger ci,1
"Fix an OrderDict initilization bug. (#2862)The dict which is used to initilize OrderDict is not ordered, so  metadata may not be at the end.",0
[Relay] Improved `multiply_rewrite` for gluoncv_ssd quantization (#2848)* improved `multiply_rewrite` for gluoncv_ssd quantization* add a check to attach_simulated_quantize below to prevent quantizing lhs to INPUT if lhs_kind is INPUT;,1
"fix typos: 4x ""disbale"" to ""disable"" (#2865)",0
Truncate function name (#2863),5
[Relay][Text Format] Text Printer Refactor and Debug Printing (#2605),0
Fix error reporting for missing axis (#2835)* Fix error reporting for missing axis* Update data_layout.cc,0
[DOCKER] Pin flatbuffers checkout to the last release tag (#2823). (#2879),5
[Relay][Text Format] Reverse CallNode Print Order (#2882),5
[NNPACK] Modernize test (#2868),3
[Relay] Add list update to prelude (#2866),1
Add missing sgx includes (#2878),1
Fix setting up hints for getaddrinfo (#2872),0
[ARITH] RewriteSimplifier: improved cmp simplification (#2851),5
do (#2883),5
[RELAY][Frontend][TF] decompile tf control flow (#2830)* decompile tf control flow* Add docs* remove import relay* move tests under tensorflow frontend* minor fix,0
Enhance upsample operator to adapt onnx opset version 9 (#2840),5
Use version invariant rustfmt (#2886),5
[Relay][Op] Add group conv2d dispatch to topi function (#2870)* [Relay][Op] Add group conv2d dispatch to topi function* Rerun tests,1
[Apps] [howto_deploy] fix cxx-flags order and build directory (#2888),0
"fix prelu, now can use on 2d input and add one test (#2875)",0
Add dense schedules to __init__ for cpu (#2855)* Add dense schedules to __init__ for cpu* Add documentation for topi::shape* Add additional imports to topi CPU __init__.,1
"[TESTS] Improve script robustness (#2893)A number of test scripts use the '|| exit 1' idiom.  This has twoissues, first process exit codes are defined to be in the range 0-255.Second, more importantly, the idiom is fragile because it requiresthat every possible failure point be explicitly coded.  This patchremoves the idiom in favour of ""set -e"" as used in the docker scriptsas a more robust mechanism to ensure that script failures are alwayscaught and propagated by default.",3
[Relay] Fix name of bias in testing.mlp (#2892),0
winograd_nnpack (#2721),5
[Relay] Fix Relay ARM CPU depthwise spatial pack schedule alter op layout issue. (#2861)* Fix Relay ARM CPU spatial pack depthwise alter op layout issue.* Update tune_relay_arm.py,0
"[TESTS] Import script robustness (set -u) (#2896)Adopt the ""set -u"" idiom from the docker scripts as a mechanism toimprove future robustness.",3
[DOCKER] Upgrade ci-cpu to latest v0.50 (#2901),3
Allow linking against MKLML (#2902),2
[COMMUNITY] ASF mentors (#2906),3
[Relay] Allow converting keras.layers.Sequential (#2842)* Allow converting keras.layers.Sequential* Use existing new_var function* Only update expr when missing* Add test,1
"[Relay] clean up hd, change tl (#2917)",4
Turn on USE_SORT by default (#2916),5
[TEST] Cache test data (#2921),3
Unified error handling in NNVM and Relay frontends (#2828),0
add support for mxnet smooth_l1 (#2905),1
[Relay] Add support for TupleGetItem in op fusion (#2914),1
"[Relay, TOPI]  Deformable conv2d (#2908)* [Relay, TOPI] Add deformable conv2d* Moved to op level2* Fix lint* Moved to level2 & bug fix* Update comments* Disabled flaky test of conv2d",0
TVM debugresult dump to Chrome Tracing (#2922),0
[Relay] add test for second order ad (#2754)* do second order* add comment* better name* use tvm assert all close* refire ci,1
"Revert ""[Relay] add test for second order ad (#2754)"" (#2926)This reverts commit f5ca9915ab163364c885de0b103579e4d85460eb.",1
[Tutorial] Cache the test data in tutorial (#2923),3
[AUTOTVM] Refactor measure build func (#2927),5
Fix intersect of modular set (#2904)Fix comment bugs and code style,0
"[Relay, OpFusion] Fix handling TupleGetItem for nested tuples (#2929)",0
Consistent result of DetectLinearEquation() when an empy vars is passed (#2860),4
[FRONTEND][ONNX] Some bug fixes and Shape operator fixed for relay. (#2850)* [FRONTEND][ONNX] Some bug fixes and Shape operator fixed for relay.* * test cases* * ci error,0
Outdated renaming for flatten in ONNX converter (#2843),5
[FRONTEND][TENSORFLOW] bug fix for tensorflow official slim models. (#2864)* [FRONTEND][TENSORFLOW] bug fix for tensorflow official slim models.* * review comments,0
Fix vcvtph2ps codegen (#2925),0
[ARITH] Analyzer CanonicalSimplifier (#2891),5
Update schedule_dataflow_rewrite.cc (#2934),1
[TEXPR][PASS] Fix thread all reduce to avoid write after read hazzard (#2937),0
Fix PRC typo (#2939),0
[DOCKER][FRONTEND] Run DarkNet tests (#2673)* [DOCKER][FRONTEND] Run DarkNet tests* update tests to pass CI,1
[Relay] Add foldr1 (#2928),1
[Relay/TOPI][OP] Add clip and wrap mode support in take (#2858)* Update take* Add special case for canonical simplify and fix test cases* Use lower case for wrap and clip* remove unnecssary lower* Fix mxnet converter for take* fix,0
"[Relay, Quantization] Quantize all fields of concatenate (#2913)",5
Fix makedirs() condition in contrib. (#2942),0
[Relay][OP] Gather_nd exposed to relay (#2945)* gather_nd added* gather_nd test added* more test added* fix lint* fix build error* fix lint* comments addressed,0
Add missing #!/bin/bash directive. (#2951),1
[Bugfix] Bilinear resize bug fix from PR #2777 (#2857)* error fixed* rename* solve conlicts with master* more test added* fix error* remove test* comment addressed,0
[Relay][OP] Fix bias_add default axis (#2829)* Fix bias add default axis* update* Fix canonicalize ops for bias_add,0
[Rust] Unify types between bindings and pure Rust impl (#2616),5
[Relay][Frontend] Support TF Gather (#2935)* [Relay][Frontend] Support TF Gather* fix comments,0
"[RUNTIME][OPENCL] Make OpenCL runtime Compatible with OpenCL2.0 #2897 (#2950)There are many OpenCL platforms that do not yet support OpenCL 2.0,hence we use 1.2 APIs, some of which are now deprecated.  In orderto turn off the deprecation warnings (elevated to errors by-Werror) we explicitly disable the 1.2 deprecation warnings.At the point TVM supports minimum version 2.0, this commit can bereverted.",0
[Relay][Frontend] Support tf.where (#2936)* [Relay][Frontend] Support tf.where* fix comments,0
[Relay][Frontend] Adding ADD operator to tflite frontend for compiling the MobileNetV2 (#2919),1
[RUST] Remove empty ty.rs (#2958),4
"fix undefined reference to dlopen, etc (#2957)",0
[TOPI] bitserial_conv2d move to autotvm template and updates (#2819),1
Removed std::unary_function because it is deprecated and removed in newer c++(https://en.cppreference.com/w/cpp/utility/functional/unary_function) (#2962),1
[REFACTOR] Remove stale verilog generator (#2964),2
[tvm4j] provide error msg for failure function call (#2967),0
[TVM][Bugfix] Fix missing runtime:: (#2966),0
[WIP][AUTOTVM][TOPI] Port x86 NCHWc to AutoTVM for Task Extraction (#2664)[AUTOTVM][TOPI] Port x86 NCHWc to AutoTVM for Task Extraction,5
Rustify PackedFunc & Friends (#2969),5
[HEADER] Add Header to Comply with ASF Release Policy (#2982)* [HEADER] ASF header dir=include* [HEADER] ASF Header dir=src* [HEADER] ASF Header -dir=python* [HEADER] ASF header dir=topi* [HEADER] ASF Header dir=nnvm* [HEADER] ASF Header -dir=tutorials* [HEADER] ASF Header dir=tests* [HEADER] ASF Header -dir=docker* fix whitespace* [HEADER] ASF Header -dir=jvm* [HEADER] ASF Header -dir=web* [HEADER] ASF Header --dir=apps* [HEADER] ASF Header --dir=vta* [HEADER] ASF Header -dir=go* temp* [HEADER] ASF Header --dir=rust* [HEADER] Add ASF Header --dir=cmake* [HEADER] ASF Header --dir=docs* [HEADER] Header for Jenkinsfile* [HEADER] ASF Header to toml and md* [HEADER] ASF Header to gradle* Finalize rat cleanup* Fix permission* Fix java test* temporary remove nnvm onnx test,0
[Relay][RFC][Fix] Rename RelayPrint to AsText (#2984),0
[Relay] InferCorrectLayout for strided_slice & min_num_branches option in CombineParallelConv2D (#2961)* [Relay] InferCorrectLayout for strided_slice* Add min_num_branches option to CombineParallelConv2D* Return undef if original layout contains splitted axes,1
"[Relay] Add expr_visitor, fix expr_functor exponential blowup problem (#2988)* save* lint",0
"add document (#2714)lintlintsavesaveadd more casesaveerrorlintlintcommitdolintsavefix lintwrap it back as funclintsaveremove dead commentfix stylefix lintUpdate src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>Update src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>address review feedbackpe now handle freevar. as a result preserving function is now trivial.testadd basic test, implement pretty printing for generic functiontestlintfix segfaultsavesavedotestfix another erroraddress commentcommitsaveaddress review feedbackadd test for invalidate, fix error in lookuprename cont to boduyfix error and add regression testfix error, add test caseUpdate src/relay/pass/partial_eval.ccCo-Authored-By: MarisaKirisame <lolisa@marisa.moe>fix lintremove extra linesavesave",0
Update let_list.h (#2987),1
Expose backtrace symbols in Debug mode (#3001),0
add output format to ndk build func (#2999),1
fix java checkstyle version (#2998),0
"[REFACTOR] Use more TypedPackedFuncs (#2981)* Add `set_body_simple` to Registry, refactor a lot of code to use it* Add more types to Relay PackedFuncs* Add Registry::set_body_method to easily make Node methods intoPackedFuncs* Add set_body_method, set_body_node_method; start typing api_lang* Add some docs, remove unused script* Fix mysterious linter problem* Touch up api_ir.cc* Fix some issues with TOPI argument counts* Revert changes to topi.cc to avoid problems with optional arguments* A little more cleanup* Type more of the api _ functions* Whitespace* Finalize names and docs for new registry helpers* Update docs",0
[AutoTVM] fix argument type for curve feature (#3004),0
Support SpaceToBatchND/BatchToSpaceND in Tensorflow frontend (#2943)Thanks @alexeyr . This is now merged.,5
[AutoTVM] Fix typos (#3014)Signed-off-by: Ce Gao <gaoce@caicloud.io>,0
"[NIT] fix relay invariant error message (#3011)* [NIT] fix common error messageExtremely minor issue, but this is one of the most common error messages people see...* Update type_solver.cctrigger CI",0
[Relay] Add gradient operator tutorial docs (#2751)* Add gradient operator tutorial docs* Incorporate Steven's and Ziheng's feedback* Remove TODO about `collapse_sum_like`* Add more examples,1
"[Relay] C++ GraphRuntimeCodegen, Deprecate Python2 (#2986)* [Relay] C++ GraphRuntimeCodegen* [Test] Deprecate Python2* [Python3] Add Py2 check* Update _pyversion.py* [Python3] Update test",1
[Bugfix] Fix caffe2 nnvm frontend (#2996),0
[Relay][Text Format] Pretty Printer Smart Inlining (#2881),5
[Heterogeneous][Bugfix] Fix bug of wrongly generated device_map (#2990)* fix bug of device_index* cpplint* nose* Update test_pass_annotation.py* fix name of testcase* delete comment,0
[COMMUNITY] @hlu1 -> Reviewer (#3021),3
[RUST][FRONTEND] Fix resnet example (#3000)Due to the previous changes the frontend resnet example failed to build.  So this patch 1) fixes it 2) adds ~~a local `run_tests.sh` to remedy non-existence of MXNet CI (used in python build example)~~ the example build to CI with random weights and a flag for pretrained resnet weightsPlease review: @tqchen @nhynes @kazimuth,0
[Relay] use unordered_map instead of map in ANF (#3024),2
[Relay] Add compiler pass tutorial docs (#2746)* Add Relay compiler pass tutorial docs* Add Python API hook wrapping step* Incorporate feedback* More doc iteration* Mooooore iteration* Rewrite `runtime.md` in rst,1
[DOC] Add Android Tutorial (#2977)* fix APP_STL for latest android ndk* add vulkan sdk for tutorial* add android tutorial* fix of invalid input layer name* update relay build opt_level 1 -> 3,0
[ARITH] Fix x||!x for comparisons in rewrite simplifier (#3029),0
"[Relay] Fix BatchMatMulRel typerelation (#3032)return false mean retry in the future, and in the case of error, it should be report ASAP, not retry.",0
Simplify TF get_output_names (#3025),5
Update expr.h (#3031),1
Add caffe2 nnvm frontend to CI (#3018),1
Ensure interpreted functions can take values that are not TensorValues (#3015),5
"Update dmlc-core, fix default ctors of NodeEntry  (#3017)",0
[Relay] Fix Fuse (#3035)* save* fix* Update fuse_ops.cc,0
Support Deriving channels when it is not provided in AlterLayout. (#2972),5
Implement relay nn.bias_add compute in C++ (#3027)* Implement nn.bias_add compute in C++* Address comments* Remove unnecessary check,1
[Relay] Add printing for ADT Type (#3030)* Update pretty_printer.cc* Update pretty_printer.cc,1
Additional fix for PR#2972 (#3044),0
Bugfix for path issues (#3038),0
[Relay][Frontend] TF Tile Round Sign Pow Exp Reverse (#2960)* [Relay][Frontend] TF Round Sign Pow Exp Reverse* fix ci* fix comments,0
[RELAY] Avoid unnecessarily reconstructing FunctionNode. (#3047),5
fix PostOrderVisit signature (#3048),0
[Bugfix] Fix winograd nnpack fp16 (#3046),0
[TOPI] Rename output tensors for better readability (#3006),5
[Frontend][TF] Fix Placeholder issue (#2834)* [Frontend][TF] Fix Placeholder issue* Add test cases,0
Fix code comment and typos. (#3063),0
[Relay] fix target string (#3071),0
Enhance upsample operator to adapt onnx opset version 9 for nnvm comp… (#2968)* Enhance upsample operator to adapt onnx opset version 9 for nnvm compiler* Add upsample test case for newer opset in nnvm* re-trigger the CI,1
Fix UnboundLocalError: local variable 'tensor' referenced before assignment (#3074),0
check in (#3089),5
Use bridge network and expose port on macOS when launch docker image (#3086),5
[Relay][Text Format] Fix Pretty Printing Annotations (#3041),0
[Relay][TOPI] Add rsqrt operator (#2949),1
Add VSCode directories to gitignore (#3095),1
[Relay][TensorFlow] Remove 'input_0d_mismatch' special handling (#3087)* [Relay][TensorFlow] Remove 'input_0d_mismatch' special handling* Add more tests.* Cover the case that strided_slice outputs a scalar,1
[TEST][FLAKY] fix for #3099 (#3101),0
[COMMUNITY] @vinx13 -> committer (#3100),3
"[Relay, Quantization, TOPI] int8 dense on CUDA & Dense op quantization  (#2877)* Quantize dense layers* Add out_dtype arggument to dense; Add dense_int8 on CUDA* Add topi unittest of dense int8* Fix relay* Fix topi integration* Fix quantization* Update dense_rewrite* Triger CI* Change qconfig quantize_dense to quantize_op* Fix* Remove quantize_op from qconfig",0
[TVM][ARITH] Teach BoundDeduce to handle the case in which target var can appear in rhs of expression (#2795)* target variable can now appear in either lhs or rhs of the expression to be analyzed* removed extra spaces,4
1) fixed a functional bug in loop partitioning algorithm that is exposed when double splitting with indivisible factors 2) added a testcase (#2956),0
Fixed issue #3069 by checking op tag (#3070)* Fixed issue #3069 by adding in_channels* Registerd group_conv2d_nchw as topi compute* Improved by checking tag value* Removed group_conv2d_nchw topi registration* Added test for relay group_conv2d_nchw* Added assertions to forbid small group size* Removed hard-coded oc_block_factor* Added explanatory comments to group_conv2d_nchw_cuda* Updated group_conv2d_nchw_cuda scheduleRemoved 'direct' CUDA tests* Reverted an accidental change in a conv2d test* Fixed indentation problems* Fixed a mis-commented line* Reverted change in group_conv2d_nchw tag* Removed commented int8 group_conv2d test* Fixed group size assertions in group_conv2d_nchw_cuda,0
[ROCM] Fix conv2d (#3107),0
[TOPI] Bitserial dense operators for CPU (#3051),5
"Check that the node is not null, add contains to OpMap (#3037)",1
fixed some typos (#3112),0
[TOPI] Fix group_conv2d unit test (#3113),0
[CI] Add file type check (#3116),1
"[LINT] recover lint error, add asf header check (#3117)",0
"[Relay, OpFusion] Better tuple fusion implementation  (#3092)",5
porting new upsample test case from nnvm to relay (#3115),1
[Lang] Fix undef BijectiveLayout and add scalar layout support (#3105),0
"[Relay][TOPI] Gluncv SSD support on the GPU (#2784)* ssd gluoncv gpu op updated* ssd gluoncv gpu op updated* tutorials and testes modified* tutorials and testes modified* fix lint* fix lint* address comment* multibox bug fixed* space line added* use less threads per block* use less threads per block* less threads per block for get valid count* less threads per block for get valid count* merge with master* Revert ""less threads per block for get valid count""This reverts commit 08896cfccc34b0b2a1646d01d01ea4cad73941c4.* Revert ""less threads per block for get valid count""This reverts commit 08896cfccc34b0b2a1646d01d01ea4cad73941c4.* typo fixed* elem length made to a variable* fix lint error* fix lint error* lint fixed* bug fixed* bug fixed* lint fixed* error fixed* error fixed* test ci* test ci* seperate argsort to be an independent op* seperate argsort to be an independent op* fix lint* fix lint* remove unsupported models* typo fixed* argsort added to realy* solve conflicts with master* fix lint* fix lint* test push* Revert ""test push""This reverts commit 6db00883fab6cc06bddf564c926bb27c874397d8.* fix lint error* fix more lint* cpu test_sort udpated* debug ci* nms fixed* expose argsort to relay frontend* test ci* fix lint* sort register error fixed* fix nnvm* nms type fixed* adaptive pooling added to relay* Revert ""adaptive pooling added to relay""This reverts commit 1119f1f2c055753e0cc5611627597749134c5c8c.* fix lint* expose argsort op* fix lint* fix lint* fix lint* sort test updated* sort bug fixed* nnvm error fixed* fix argsort default data type returned to be float insteaf of int* fix lint* fix lint* test fixed* fix valid count* fix titanx bug* tutorial add both targets* titanx error fixed* try to fix CI old gpu error* try to solve CI GPU error* get_valid_count added* reverse get_valid_count* get valid count optimized* address comments* fix ci error* remove unessesary block sync* add back one sync* address comments* address more comments* more comments* move sort to be indepent algorithm* typo fixed* more typos* comments addressed* doc updated* fix pylint* address final comments* apache license added",0
Fix bug in ONNX importer (#3084),0
"Fixing a doc nit (#3123)URLs to the authors repo for these tutorials had an extra`https://`, this patch removes that.",0
[Bugfix] Fix type code error for StringImm (#3050),0
[RELAY][FUSION] Enhance fusion rule that starts from elemwise and broadcast (#2932)* [relay][bugfix] fuse injective to elemwise and broadcast* enhance fusion for prarllel injectiveOD* check if tensor in schedule* fix codegen* fix lint* update* lint,0
[Relay][Tensorflow] Allow an op as loop var. (#3056),5
"[FRONTEND][TFLITE] Add FULLY_CONNECTED op into tflite frontend, support Inception V4 (#3019)* Add FULLY_CONNECTED op into tflite frontend, support Inception V4* Fix comment style in TF Lite tests.",0
"[DOC] various assorted grammar fixes (#3127)* Correct spelling of 'inavlid'* [DOC] correct spelling of 'schdule'.* [DOC] clean up use of abbreviation ""interop""* [DOC] capitalize API abbreviation consistently* [DOC] correct spelling of 'peformed'.* [DOC] correct spelling of 'intermidiate'* Remove trailing white space.* Correct spelling of 'parametrization'.* [DOC] minor improvements to Range documentation.",0
Fix PRelu layout in Relay (#3013)* Fix PRelu layout in Relay* Fix cpplint* Add PRelu test case,0
Minor addition to graph runtime debug (#3129)* print op names in graph runtime debug* fix lint,0
[DOC] Add missing targets to target_name documentation. (#3128),1
Update CONTRIBUTORS.md (#3130),1
[Relay][Runtime] Add support for virtual machine Objects (#3120),1
[LINT] Add more allowed file type,1
Add MXNet converter for RNN layer ops (#3125),1
[Relay][Runtime] Add memory manager for NDArray (#3121)* Add support for custom NDArray memory managementCredit to @icemelon9 and @wweic* Fix copy-paste issue* Fix naive allocator.h* Remove buffer field* Apply Wei's suggestions.Co-Authored-By: jroesch <roeschinc@gmail.com>* Fix Wei's suggestion* Fix go rts* Break MM dependency* Add docs and clean up diff* Add more docs* Move to VM folder* Fix lint* Remove Go dep.* Rename to Empty* Address Haichen's comments,0
[DOC] Various documentation improvements (#3133),5
[DOC] Developer documentation for InferBound pass. (#3126)* Developer documentation for InferBound pass.,4
"[ARITH] Constraint-aware ConstIntBound, Enhance CanonicalSimplify (#3132)",5
[TOPI] Fix mali conv2d performance regression (#3131)* [TOPI] fix mali conv* fix typo* address comments,0
[Relay][Frontend] add log op in tf frontend (#3111)* [Relay][Frontend] add log op in tf frontend* address comment,1
[ROCm] Fix dense autotvm template registration (#3136)* Fix rocm dense autotvm template* suppres lint warning,0
Handle vectorize for LE statement (#3137)* Handle vectorize for LE statementFix a new cases introduced by commit 7afbca5691fdb599cd90b043d5a5036e55cae2d6* Add test,0
[DOC] fix :code: markup syntax (#3140),0
[Bugfix][TOPI] conv2d_transpose bugfix (#3138)* deconv tests* deconv bug fixed for certain cases tests added,0
Relay C++ Build Module (#3082)* [Relay] C++ Build module* asdf,5
[CI] Always run cpptest during build to ensure library correctness (#3147),3
fix python lint warnings (#3145),0
"[RFC] [VTA] [TSIM] Enabling Cycle-Accurate Hardware Simulation for VTA #3009 (#3010)* merge files* move verilator to the right place* change name to tsim* add default rule to be build and run* add README for tsim* Update README.md* add some structural feedback* change name of VTASim to VTADPISim* more renaming* update comment* add license* fix indentation* add switch for vta-tsim* add more licenses* update readme* address some of the new feedback* add some feedback from cpplint* add one more whitespace* pass pointer so linter is happy* pass pointer so linter is happy* README moved to vta documentation* create types for dpi functions, so they can be handle easily* fix pointer style* add feedback from docs* parametrize width data and pointers* fix comments* fix comment* add comment to class* add missing parameters* move README back to tsim example* add feedback* add more comments and remove un-necessary argument in finish* update comments* fix cpplint* fix doc",0
[Relay][Op] Adaptive pooling (#3085)* Add topi adaptive_pool* Use adaptive_pool to compute global_pool* Add relay adaptive pool2d* Fix lint* Fix typo* Minor change* Change support level to 10* Add contrib* Remove global pool schedule* Add contrib module* Fix lint* Update doc* Update doc,0
[BuildModule] Fix AlterLayout Pass (#3155),0
[Relay][Runtime] Implementation of Relay VM (#2889)* Implement the virtual machineCo-Authored-By: wweic <ipondering.weic@gmail.com>* Fix rebase build issues* Reorganize vm.py and fix allocator bug* Remove compiler* Remove tests* Remove backend/vm/vm.cc too* Fix docs* Fix doc* Fix doc* Add vm docs* Remove change to dead_code.cc* Remove Relay logging* Remove reduce* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Reformat* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Address feedback* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Apply suggestions from code reviewCo-Authored-By: jroesch <roeschinc@gmail.com>* Fix a couple outstanding comments* Last couple comments* Update include/tvm/runtime/vm.hCo-Authored-By: jroesch <roeschinc@gmail.com>* Address code review feedback* Fix final comment* Address comments* Error reporting and example* add Const* Explicitly delete copy assignment operator* Fix rebase* Pass 3rd arg to fusion,0
add more syncs (#3151),1
Fix a multithreaded bug in llvm LazyInitJIT (#3158),0
[codegen] heterogeneous build for c++ (#3144)* heterogeneous build for c++* merge relay buildmodule to codegen build* use module split* use target_host* remove sse3* retrigger ci,4
Fix a tensorflow test bug. (#3165)Length of input_shape isn't always 4.,0
[HybridScript] Capture constant external python variables (#3157),5
Register all operators' Python attributes in Python so they can be easily accessed from Python code (#3175),5
[Relay][TensorFlow] Support tf.math.reduce_prod (#3166),5
[Bugfix] Check file exists before removing it (#3178),0
[Relay][Runtime] Add VM compiler.  (#3139)* Implement the VM compiler* Fix issues* Fix ASF headers* Fix test issue* Apply typo fixes.* Update src/relay/backend/vm/compiler.ccCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Refactor compiler* Fix* Fix* Fix in benchmark* Fix* Address comments,0
[GOLANG] Some fixes for golang latest version compiler. #3119 (#3182),0
Fix a bug of flatten in ONNX to Relay converter (#3180)* fix onnx frontend flatten bug* Update onnx.py* Update onnx.py* Update onnx.py,0
cleanup: removed a piece of code that is redundant now given updates to HalideIR submodule (#3169),1
add onnx elemwise greater/less (#3186),1
[RELAY][PASS] detect depthwise conv2d in mac_count pass (#3083)* check in* use groups* CHECK_EQ* trigger CI* Update mac_count.cc* trigger CI* trigger CI,1
Avoid using heavy API to query single attribution (#3179),5
[Relay][TensorFlow Frontend] SoftPlus Sqrt (#3187),5
"[Datatypes] Custom datatypes (#2900)* Register and use custom datatypes in TVMThis patch adds the ability to register and use a custom datatype from Python,using the `register_datatype` call. The datatype can then be passed as the`dtype` parameter using the syntax `dtype=""custom[<type_name>]bitsxlanes""`.* Removes extra file* Register custom datatypes with TVM; specify Cast and Add loweringThis commit adds functionality for registering custom datatypes with TVM, andfurthermore adding custom lowering functions to lower those custom datatypes.This commit only adds lowering for the Cast and Add ops; more ops will be addedsoon.Check out some custom datatype samples in my repository of samples:https://github.com/gussmith23/tvm-custom-datatype-samples* Register and lower casts from Python* Formatting* Fix include; was including too much* Add comment* Add DatatypeRegistered* Add storage size field to custom datatypesThis field indicates the bitwidth of the opaque block of data into whichinstances of the datatype will be stored, when TVM compiles. For example, if Icreate a datatype with a storage size of 16, then- Constants of that datatype will be created as unsigned 16-bit ints- Calls to external functions taking that datatype will pass the data as  unsigned 16-bit ints- External functions returning that datatype will be assumed to return unsigned  16-bit ints.* Change how lowering funcs (Cast and other ops) are named in registrytvm.datatypes.lower.<target>.cast.<dst-type>.<src-type>becomestvm.datatypes.lower.<target>.Cast.<dst-type>.<src-type>And fixes some sloppy code around how the other ops were being formatted.* Update Python register_datatype to accept storage size* Oops, left out one cast->Cast change* Look up storage size when parsing `custom[typename]`When we encounter this type string in Python, it will be parsed into a Halidetype object in C++. Some of my original code supported this parsing, but we nowhave to attach the storage type to the type (by setting the bits field).* Change how external calls for casting/other ops are doneFirstly, we now use the storage size of the custom type when determininginput/output types; e.g. a cast to a custom type with storage size 16 is seen asa call to an external function returning an opaque uint of size 16.Secondly, write a macro to handle the other ops. Originally I thought I couldhandle these at runtime, with a single `_register_op` global. I transitionedinstead to using individual `_register_Add` etc. calls generated with a macro,but I don't remember why.* When encountering a custom type immediate, generate UIntImm* Translate custom types to LLVM type* Generate correct return type in CastsOriginally I was assuming that the result type from casts was always a customdatatype, and so I was making the Call return a UInt type.* Use TVM-idiomatic recursion style in DatatypesLowererThis was actually a bug, I'm pretty sure; we wouldn't have recursed deep on anycomplex programs. As a result of making this change, I also uncovered anotherpotential bug, where the datatypes lowering pass would attempt to lower a Loadof a custom type. By commenting out the `Mutate_` for Load, I was able to stopthe error from cropping up, but frankly, I'm not satisfied with the solution;how is it that we are able to run codegen when Loads of custom datatypes arepresent in the IR? I have not written any code, to my knowledge, that willsupport this. Perhaps Load does not care about the underlying datatype?* Use CHECK* Add comment about which Mutate_s are needed* Add comments* Add GetCustomDatatypeRegistered as an extern C function* Formatting, comments, casting* Change how datatype string is formatted* Use bits() instead of GetStorageSizeUse bits() instead of GetStorageSize* Change comment* Add datatype.py* Change registered function name (datatypes->datatype)* Remove GetStorageSize* Format custom datatypes like any other datatypeSpecifically, we now print the bits and lanes after the `custom[...]` string.* Correctly implement datatype lowering in Python* Remove unneeded include* Make function naming consistent* Use CHECK instead of internal_assert* Rename macro* Formatting* Rename functions* Implement Cast lowering`_datatype_register_op` is now able to lower both binary ops and Casts.* Formatting* Formatting* Clang format, google style* Fix std::string/extern ""C"" warnings* Formatting* Formatting* Lower Allocates and Loads during datatype loweringThis should ensure that there are no custom datatypes remaining once datatypelowering is done. This will allow us to remove the code in the LLVM codegenwhich deals with custom datatypes.* Revert additions to codegen_llvm.cc which are now unneeded* Pass cpplint on lower_datatypes.cc* Add clarifying comment* Remove datatype lowering registration funcs from C++* Add CHECKs* Remove TODO* Remove all references to storage size* Move and rename function* Rename function* Remove done TODOs and other handled comments* Remove irrelevant Load code and comments* Comment out the IR node types I'm not sure about yet* Add bfloat16 datatype unittest* Fix MakeConstScalarMakeConstScalar for a custom datatype will now call out to a function which canbe registered on a per-datatype basis. The function will take a double andreturn the equivalent value in the custom datatype format.Note that these code paths are not actually used or tested at the moment. I havenot yet written an example which uses const scalars of a custom datatype.* Formatting* Change pass name* Allow users to register whatever lowering function they wantTianqi pointed out that users should be able to register whatever loweringfunction they want, and should not be constrained to registering loweringfunctions which just call out to external libraries.I still provide a function for making lowering functions which call out toexternal libraries, for convenience.* Add clarifying comment* Remove unneeded comment* Remove unneeded function* Rename file* Undo unnecessary change* Undo unnecessary change* Make naming consistentRename ""datatypes"" to ""custom datatypes"" in most contexts.* Revert an artifact of old code* Fix build warnings, add TODO* Lint* Remove unnecessary use of extern C by separating decl and impl* Error checking* Remove TODO* Missed a name change* Lint* Python lint* Correctly format datatype* Move bfloat16 to 3rdparty* ""custom_datatypes"" --> ""datatype"" in most placesI left the pass as ""LowerCustomDatatypes"" to indicate that we're not loweringanything other than custom datatypes. Otherwise, everything else has beenchanged.* Upgrade datatype unittestI used a float calculator to generate some real testcases for the unittest.* Separate public includes and private implementationSpecifically, create cleaner decoupling between datatypes stuff in packed_funcand the datatype registry implementation.* Formatting* Limit custom datatype codes to >128* Add TODOs* Fix comment* Formatting* Clean up datatype unittest* Remove un-exported functions in public headers; UIntImm->FloatImmMore places where I accidentally was using implementation-only functions inpublic headers.Additionally, store custom datatype immediates as FloatImms. A later change willadd new lowering logic to lower these FloatImms to UIntImms.Plus formatting change.* Lint* Use FloatImm (not UIntImm) to hold immediates of custom datatypesThis change switches from using UIntImm to FloatImm for storing immediates ofcustom datatypes. The value of the number is stored in a double, which should beenough precision for now, for most custom types we will explore in the immediatefuture.In line with this change, we change the datatype lowering so that FloatImms arelowered to UInts of the appropriate size. Originally, this was going to be doneby allowing the user to register a double->uint_<storage size>_t conversionwhich would be called at compile time to convert the value from the FloatImm toa UInt and store it in a UIntImm. After discussions with Tianqi, we decided totake the simpler route, and lower FloatImms just as we lower all other ops: byreplacing them with Call nodes. In this case, presumably the user will Call outto a conversion function in their datatype library.The justification for this decision is due to the functionality added in #1486.This pull request adds the ability to load LLVM bytecode in at compile time.This applies in our case as follows: 1. The user writes their custom datatype programs and registers their lowering    functions in the same way we've been doing it so far. All operations over    custom datatypes are lowered to Calls to the datatype library. 2. The user compiles their datatype library to LLVM bytecode. 3. At TVM compile time, the user loads the LLVM bytecode. Depending on how the    datatype library is written, Clang should be able to perform constant    folding over the custom datatype immediates, even if their conversions are    done with calls to the library.Additionally adds test to test the FloatImm codepath.* Re-add a change I removed accidentally during rebase* Cleanup* Remove unnecessary TVM_DLLs* Add custom datatype utilities source file to Go runtime pack* Revert ""Remove unnecessary TVM_DLLs""This reverts commit 4b742b99557fd3bf0ce6617f033c8b444b74eda4.* Mark bfloat code as TVM_DLL* Moves custom datatype runtime utilities to c_runtime_api.cc* Revert ""Add custom datatype utilities source file to Go runtime pack""This reverts commit aecbcde0b2cc09a2693955b77037fe20f93b5bfd.* Move datatype parsing to its own function* Change comments* Remove unneeded function* Formatting* Formatting* Documentation* Add kCustomBegin, use it for checking for custom types* Documentation* Formatting* Move static definition to implementation* Remove comment* Decide toBeLowered before lowering arguments of ExprIn the past, e.g. when lowering custom datatypes for an Add, we would lower aand b first, and then decide whether the resulting new Add needed to be loweredbased on the (new) types of a and b. Now, instead, we need to check the types ofa and b first (to see if they're custom types), and then lower them (so they'llbecome non-custom types), and then lower the new Add.* Revert ""Move datatype parsing to its own function""This reverts commit d554a5881afcf69af1c070d882a7651022703a09.This broke parsing. Will figure this out later. There isn't a really clean wayto separate this out given how the rest of the function is written.* Replace comment* Documentation* Remove comment and TVM_DLL* Better error messages* Remove artifact of rebase* Separate datatypes parsing to its own function* Add \returns* Comment changes; add TODO* Refactor tests",0
[Relay][Compilation] replace relay.build_module with C++ BuildModule (#3174),5
[Relay] Option to select which convolution layers are quantized. (#3173)* Stashing for later maybe.* Added new option to leave specific layers unquantized.* Better error checking.* remove unneeded import* tab to spaces* pylint fixes* more pylint fixes,0
Add the acc16 intrinsic support (#3081),1
Get list of unsupported ONNX operators (#2995),5
[TENSORLFOW] PlaceholderWithDefault (limited) implementation. (#3184),5
[TOPI] Raise exception group_conv2d_nchw not supported (#3195),5
Quick fix of VTA FPGA Toolchain Installation documentation (#3196),0
Update .gitignore (#3199),1
[RELAY] Hotfix build_module creation (#3198),0
[Relay] Better shape inference in TensorFlow Frontend. (#3176)* Some bug fixes in tensorflow graph converter and added DepthToSpace operator.* Made DepthToSpace better comply with other function syntax.* Added better shape inference for unusual situations.* Lint fixes.* Added depthtospace test.* Added test cases for value inference and depthtospace.* Added fill testing.* Made comment changes and added BroadcastTo op and tests.* Fixed underlining and unneeded opt_level forcing.* Added _infer_value assertion that all values to infer are available in passed parameters.,0
[CODEGEN][CUDA][OPENCL] Handle INF and NAN (#3194),5
[ARM] Fix concat (#3061),0
"[BugFix][VTA] Fix bug in vta runtime DepPop function. (#3208)Issue:    One of existing illegal dependency check's condition always true,    the correct logic actually should be such check for store and load.Solution:    Fix the said logic issue.",0
[VTA] [TSIM] Improve tsim example (#3206),5
[BugFix] Fix bug in cast to bool (#3207),0
[Relay][ONNX] fix #3134 converter where initializers were not registered as nodes (#3143),0
[Relay][TOPI] operator All (#3124)* [Relay][TOPI] operator All* Update tests/python/frontend/tensorflow/test_forward.pyCo-Authored-By: yongwww <55wuyong@163.com>* fix comments* change to level 4,0
Add bing to reviewer (#3214),1
[relay][vm] remove throw in destructor (#3215),4
[Relay][heterogeneous pass] remove on_device op after annotation (#3204)* remove on_device op after annotation* Update src/relay/pass/device_annotation.ccCo-Authored-By: MORINAGA <34588258+imorinaga@users.noreply.github.com>,1
[Contrib] cblas batch_matmul (#3210),5
Add `SkipVectorize` pass (#3222),1
[TFLite] Convert TFLite NCHW to NHWC (#3141)* Convert TFLite NCHW to NHWC* Minor comment fix,0
[Team] Eddie -> PMC (#3220),5
Add packing for int8 1x1 convolution and support the int8 group convolution on X86 (#2991)* Support the 1x1 int8 conv with NHWC layout and weight packingfix linter* fix the memoize issue* fix the failed nhwc test* add the schedule for pack to unbreak other tests* skip avx512 compile* Support the 1x1 int8 conv with NHWC layout and weight packingfix linter* fix the memoize issue* fix the failed nhwc test* add the schedule for pack to unbreak other tests* skip avx512 compile* Unify the data_layout and kernel_layout relation* add asf header* fix the comment* retrigger the build/test,0
[Bugfix] Fix sort changing original input data issue (#3212)* sort bugfix for not rearranging input data* separate sort schedule* fix lint* use identity op instead* fix lint* remove redundent code,0
[WIP] [Relay] [NNVM] [Frontend] implement MaxPool-8 and MaxPool-10 (#3114),5
[relay][pass manager] Open transform namespace (#3226),4
[Relay][Prelude] Remove Peano nats from the prelude (#3045),4
Register SkipVectorize (#3228),5
[3rdparty] sync submodules (#3229),5
[GraphRuntime] Debug graph runtime (#3232),0
"[NODE] Macro to define NodeRef methods, constructor style example (#3224)",5
Modified pick best to accumulate the best configurations from both the input and output file. (#3225),2
[LINT] handle more file types in ASF header (#3235)* Update add_asf_header.py* Update add_asf_header.py,1
[C++][API] Consistent RAII scoping API. (#3231),5
[Relay][Transform] merge PassContext and BuildConfig (#3234),4
[RELAY]Frontend darknet (#2773)* [RELAY]Frontend darknet* CI test file updated & CI error fixed* avg_pool pad fix* Changed repo_url and doc formatting,0
[Relay] remove unneeded VisitExpr (#3239),4
[Relay] Start porting pass to the pass manager (#3191),4
Fixed a typo (#3218)* Fixed a typo* Remove outdated url link.,0
[Relay][Frontend] Add Crop op converter (#3241)* Add Crop op converter* lint* x,1
[ARITH] Improve div/mod in rewrite simplifier (#3149)* [ARITH] Improve div/mod in rewrite simplifier* Fix lint error* Fuller file name in src/arithmetic/modular_set.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>* Generalize some rules* Replace gcd factoring with specialized rules* Mark rules that don't work for non-truncated division* More tests,0
[Doc][Relay] Add VM doc (#3188)* [Doc][Relay] Add VM doc* Add Apache header* Apply suggestions from code reviewCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>Co-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>Co-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>Co-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Junru's comment* More fix* More fix* More fix* last fix* Apply suggestions from code reviewCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Apply suggestions from code reviewCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Add code links* Remove unused bp* Update docs/dev/virtual_machine.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Explain TODO* Yong's commentCo-Authored-By: Yong Wu <55wuyong@163.com>* Comment,0
[VTA][TSIM] Use Module instead of RawModule for testbench by creating an empty bundle for the IO (#3242)* use Module instead of RawModule for testbench by creating an empty bundle for the IO* change default back to verilog,2
Move CombineParallelConv2D to opt level 4 (#3248),4
kCustomBegin overlapped with kExtEnd; incr by 1 (#3250)This was a typo in the original custom datatypes PR.,5
Typo: Tensorflow --> TensorFlow (#3249),5
[RUST] Rust DSO module (#2976),5
[TOPI] Fix resize nearest with fractional scaling (#3244),0
[C++] Cleanup transform API nits (#3253),5
"[BugFix][VTA] Fix vta_conv2d crash issue after change vta_config.json configuration. (#3213)Issue:Once change LOG_BLOCK_IN or LOG_BLOCK_OUT into > 4 value, when run vta“Simple Matrix Multiply” or load vta, vta would crash at vta_conv2d.py.Analysis:This issue caused by resnet18 logic of vta_conv2d.py which havein_filter minmum size that is 16. > 4 value would cause such in_filtercheck failed then make xfer_size be empty and find_schedules functionreturn a empty list finally cause crash.Solution:add the empty list check.",0
[AutoTVM]Core functionality for Graph tuner (#2184)* Add graph tuning* Add tests* Fix tests* Fix pylint* Small fix for docstring* Minor fix* Support fetching workload from relay expr* Simplify benchmark layout transformation* Add relay support* Fix infer layout func name* Refactor internal data representation* Fix issues* Add PBQP solver* Fix layout transform check* Add PBQPTuner test* Fix lint* Update tutorial* Fix tutorial* Fix lint* Add relay test* Remove nnvm since nnvm graph can be converted to relay function* Modify benchmark layout wrt new layout_transform api* Fix lint* Update docstring for DP tuner* Refactor traverse graph* Support graph tuning for multiple target operators* Fix fetching workloads* Add x86 depthwise_conv2d infer_layout* Fix x86 depthwise_conv2d autotvm* Fix PBQP tuner* Fix DP tuner* Generate dummy layout transform record* Update tutorial* Modify layout records name* Add ASF header* Add ASF header for testing files* Fix test* Fix topi fetching* Some refactors* Fix lint* Fix tutorial* Rename test files* Fix doc typo* Add test case note link,0
[Relay] Handle float16 constants & fix BatchNorm (#3260),0
[Bugfix] Fix a memory leak in OpManager (#3263),0
Jekyll (#3262),5
"[Relay][Hashing] Structural hash - incorporate the var type into its hash (#3267)Currently, the BindVar function does not take Var type into account. This causestwo same graph structures with different var shapes to have same hash.Structural hash is used for keeping track of which operators we havealready compiled. Because of this, two operators with different shapes end uppointing to same compiled code. The failure is encountered at runtime, where theexpected input shape asserts are not met.",5
Enable uTVM in Jenkinsfile (#3269),2
"[Bugfix][VTA] PkgConfig cause crash in PYNQ board due to link library (#3257)* [Bugfix][VTA] PkgConfig cause crash in PYNQ board due to link librarynot exist.Symptom:When run vta_get_started.py with pynq board, host crash andcomplain ""cannot find -lsds_lib"" and ""cannot find -l:libdma.so""Reproduce:At pynq board, delete the ./build/vta_config.json, then run rpcserver.In host machine run vta_get_started.py, issue would reproduce.Analysis:This issue caused by 'PkgConfig' function  still using pynq2.1library which not exist in pynq2.4 anymore, when a ""reconfig_runtime""logic of rpc_server.py get triggered , the compile would failed due tolink library not exist.Solution:change the link library to libcma.so.* [Document Change][VTA] Change pynq version from 2.3 into 2.4.Issue:pynq 2.3 image not available anymore from pynq download page and pynq2.4 is the current latest image which available in the said website, afterverification, currently VTA work good with pynq 2.4 image, hence updaterelated document from pynq 2.3 to 2.4.",0
[relay][heterogeneous] annotate using visitor (#3261)* annotate using visitor* retrigger CI,5
Update tflite tutorial to use TFLite r1.13 schema (#3271),1
[ARITH] Bugfix: check arg positiveness for mod rules (#3279),0
[RELAY][TRANSFORM] Migrate buildmodule to transform (#3251),5
[ARITH] Bugfix: int bound analysis for mod (#3288),0
Bump ONNX version (#3286),5
"[Bugfix] [VTA] VTA DRAM Have A Logic Issue May Cause GEMM Output Wrong. (#3278)* [Bugfix] [VTA] VTA DRAM Have A Logic Issue May Cause GEMM Output Wrong.Symptom:after change “LOG_BLOCK_IN” and “LOG_BLOCK_OUT” from vta_config.jsoninto 7, run vta ""Simple Matrix Multiply"" in ""simulator"", the vtacalculate result for GEMM is wrong.Sometime VTA crash with error “Check failed: phy_addr != 0 (0 vs. 0) :trying to get address that is nullptr”Analysis:Simulator hardcode kPageSize into 1<<12 and physical address calculatebased on this size, when doing “insn->dram_base” calculation , becauseGetElemBytes(dst_memory_type) larger than page size, different physcialaddress may get same dram_base, than caused logic issue and finallytrigger GEMM out put is wrong.Solution:add logic to check if PAGE SIZE larger then ""GetElemBytes"" return value.* address review comments.",0
[Relay][Docs] Add parser dependency install instructions. (#3277)* [Relay][Docs] Add parser dependency install instructions.See https://discuss.tvm.ai/t/trouble-enabling-antlr/2783.* Add a word.* Update since the parser will now be committed to the repo.* revert b/c adding the parser doesn't fix this,0
[Relay/TOPI][Op] Add TopK operator (#3256)* init impl for topk* Fix cpu for topk* init cuda impl for topk* Add cuda for topk* fix* Add doc* update doc* lint* lint* lint* x* fix warning* [Relay] Add TopK in tf converter* Add frontend converter* fix,0
[LANG] Comparison operators support for Imm expressions (#3283),5
[IR] Try to improve nms and get_valid_count (#3282)* improve nms* add back get_valid_count syncs,1
[Relay][VM] Fix code generation for packed functions + tuples  (#3287),0
Improve error message for custom tflite operators (#3284),0
More fixes and tweaks to the cuda conda packages (#3281),0
[VTA] [Hardware] Chisel implementation (#3258),5
Add support for overloading comparison operations in relay (#2910) (#3168),1
fast tanh (#3255),5
Ghost nodes in NNVM graph (#3290),5
Improve x86 roi align (#3296)* Improve roi_align performance for x86* Change test,3
[VTA] [APPS] [TSIM] small naming fix  (#3293)* make off lowercase* update README,0
[Relay][Frontend] Simplify parameter handling in Tensorflow frontend (#2993),5
Fix x86 depthwise conv2d alter_op_layout (#3264)* Fix x86 depthwise conv2d alter_op_layout* Small fix* Add test case* Fix test* Assert kernel layout* Minor fix* Add get_shape function* Minor change,0
Minor improve to assertion (#3295),5
[VTA] add doc to tsim-example driver and update verilator env variable (#3302)* add documentation and check for extension* add env variable for verilator include* fix typo* this will test if path exist otherwise it won't buid* check if verilator path and binary is set properly* add ?* remove export* no longer needed,0
Fix some typos in api docs (#3309),0
[DOC] Capitalize TVM consistently (#3316),5
[LINT] Improve robustness in task_lint.sh logic (#3315)The existing RAT ASF license auditing logic ignores any failure in theshell pipeline rather than just the exit code of the final grep.Adjust the logic such that failure of the various tools in thepipeline are not elided away.,1
[DOC] minor language use improvements (#3317),5
[CI] Ensure rat ignores rust cargo lock files [CI] Ensure rat ignores emacs backup files [CI] Ensure rat ignores .egg-info (#3314),2
[PASS][RELAY] polish pass infra (#3319),1
Make the behavior of data nullptr check of pooling layer same as others. (#3322),5
[VTA] [APPS] [TSIM] update documentation (README) (#3318)* update README* update README* update README* update README* fix typo,0
[Rust] Static syslib (#3274),5
Improve non_max_suppression and get_valid_counts for CPU (#3305)* Improve non_max_suppression for CPU* Improve get_valid_counts* Minor change* Skip some unnecessary computes,4
Add MUL operator to relay tflite frontend (#3304),1
add another default location to verilator (#3324),1
[DOC] minor gramatical improvements to tensor_expr_get_started (#3330),5
Drop trailing whitespace (#3331),5
[CI] Fix shell script exit codes (#3329)The exist code of a posix compilant shell is 0..255.  Attempting toreturn -1 will error in some shells and implicitly cast to 255 inothers.  Fix it by returning a legal return value.,0
Add all parameters to from_tensorflow docs (#3321),1
Fix Error messages in tflite.py (#3320),0
Support x86 dilation conv2d and improve multi-batch conv2d (#3308)* Support x86 dilation conv2d and improve multi-batch conv2d* Fix lint,0
[Autotvm] Support override (#3292),5
[Relay][heterogeneous] Fix tuple annotation (#3311)* [Relay][heterogeneous] Fix TupleGetItem* retrigger ci* retrigger ci,0
Add PAD operator to relay tflite frontend (#3310),1
[relay][vm] move vm opt passes to pass manager (#3323),4
"[Relay][Prelude] Use the Relay parser to define the Relay prelude (#3043)* Add ability to load Prelude from disk* Port over id* Define compose* Linting errors and style changes* Eliminate unnecessary parens* Rename identType to typeIdent (makes more sense)* Another unnecessary paren* Bump the version number for the text format* Ensure .rly (Relay text files) are permitted* Correct release number and simplify grammar rule* Correct load_prelude docstring* Corrections to _parser* Add Apache headers to prelude source file* Remove test_prelude (redundant)* Correct misleading error message* Add check that parser is enabled in Prelude* Commit pre-generated parser, ensure generated files are treated as binaries, and have parser tests always fire* Permit parser files and git attributes files* Exclude gitattributes and parser files from apache check* Another attempt at appeasing Apache audit checker* Corrections to rat-excludes* Apache should be truly appeased now* Ignore Relay parser files by name* Mark parser files as generated so they don't show up on Github* Add parsing helper function for tests* Mark parser files as not detectable",0
Add LOGISTIC operator to relay tflite frontend (#3313),1
[CI] Clarify RAT exclude patterns. (#3328),5
[RELAY] Pass infra cleanup (#3336),4
[CI] separate out legacy as a stage (#3337),5
[Topi] Fast mode in take op (#3325),5
[VTA][TSIM] update app example (#3343)* add initial support to cycle counter to accelerator* remove prints from c* add event counter support to chisel tsim example* make it more readable* use a config class* update driver* add individual Makefile to chisel* add rule for installing vta package* add makefile for verilog backend* update drivers* update* rename* update README* put default sim back* set counter to zero,1
Non_maximum_suppression and get_valid_counts add new parameters (#3335),1
[DOC] minor grammatical improvements (#3341),5
[DOC] clarfiy explanation (#3340),5
[Relay][Backend] Fix interpreter argument conversion for tuples. (#3349)* Support taking a tuple as an argument* Add test,0
[Relay][Frontend] Fix MxNet RNN without providing state initialization as input (#3326),0
[Relay] add ClipByValue and Neg in tf frontend converter (#3211),1
Support export ADT value in Python (#3299)* Support export ADT value in Python* Cache original functions* Cleanup* Cleanup,5
[Team] Jian Weng -> Committer (#3359),5
Update tflite schema version to 1.13 (#3356),1
[Relay][Transform] quantize opt passes to pass manager (#3289),4
[Relay] Check match expressions for completeness (#3203),5
"[Relay] Add Elemwise operator Sub, Divide, Power, Max, Min to tflite frontend. (#3357)",1
[Relay][Frontend] Add a bunch of ops in tf converter (#3270),1
[ARITH] Revamp IntSet (#3272),5
"[Relay] tflite frontend, keep underline with comments in same length. (#3363)",5
Update CMakeLists.txt to be more flexible (#3354),1
[VTA] add support to event counters (#3347)* add support to event counters in VTA* fix comment* fix event-counter interface parameter* no longer needed* add sim back* add docs to event counters* fix docs* add more details about event counting* make dpi-module docs more accurate,0
[TEST][FLAKY] Fix flaky test on topk and quantize pass (#3362)* fix flaky test* fix flaky quantize pass,0
"fix hardware-makefile for osx, bugfix chisel-RegFile, and rename driver (#3371)",0
[BUILD] Enable more visible symbols by default (#3365),5
Add test_forward_ssd_mobilenet_v1 to tflite/test_forward (#3350),1
[Relay][VM] Add AllocTensor instruction and better instruction printer (#3306)* Update vm print & add AllocTensor instruction* patch* fix invoke packed* update cmake* tweak move* update invoke_closure* lint* add doc* tweak,0
Fix typo in word explicitly (#3376),0
save (#3033)savesavesaveupstreamlintremove bad changesfix buildsavesaveplease the ci godUpdate src/relay/pass/partial_eval.ccCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>savefix testci is ANGRYfix rebase problemfix rebaseadd testsavesavecomment,0
add favicon in rtd (#3379),1
[RELAY][PASS] Enable decorating python class as Pass (#3364),4
[relay][frontend] Return module from frontend parsers (#3353),5
[Relay][Pass] CanonicalizeCast (#3280),4
[nnvm] fix nnvm compiler build module error (#3378),0
"[Relay][Frontend][ONNX] Fix reshape precompute, and type error (#3230)",0
"TFLite: Add fused_activation_function for ADD, SUB, MUL, DIV (#3372)",1
"Revert ""[Relay][Frontend][ONNX] Fix reshape precompute, and type error (#3230)"" (#3385)This reverts commit df6957a5ea49806b3073bbb81e339ae379cbbb1c.",0
hotfix for onnx (#3387),0
[ARITH] Bugfix min/max const canonicalize rule (#3386),0
Add RESIZE operators to realy TFLite frontend (#3370),1
[CI] Update ci-gpu to v0.52 (#3374)* [CI] Update ci-gpu to v0.52* update nodejs,1
[TEST][TENSORFLOW] clean up code (#3342),3
[Bugfix] Missing headers (#3392),0
"[VTA] Fix VTA function Vivado Compile Error. (#3375)Issue:when using vivado compile vta.cc with top function 'vta', vivadoreport deadlock error like '...with default size is used in a non -dataflowregion, which may result in deadlock Please consider to resize thestream using the directive ‘set_directive_stream’ or the ‘HL S stream’pragma.'Solution:give the queue a default size as 8.",0
return mod from frontend for autotvm (#3401),5
[Relay] Fix name conflict in PartialEval (#3402),0
to fix issue Target llvm is not enabled[followup] (#3404),0
Add EtaExpand to transform API (#3406)* Add EtaExpand to transform API* Add test case,1
[Community] @joshpoll -> Reviewer (#3412),3
[VTA] [APPS] Update README on tsim example (#3409)* update README* fix typo,0
Extend TensorComputeOp to allow scalar inputs (#2606). (#3300),5
Create closure object for GlobalVar (#3411),5
Fix global var in prelude (#3405),0
[QUANTIZE] Memorizing the quantize node mapping (#3233)* [QUANTIZE] Support for clip operator* [QUANTIZE] Memorizing the quantize node mapping.* [QUANTIZE] Remove use_stop_fusion and skip_k_conv in qconfig* update* update* update* update,1
Add nix to gitignore (#3418),1
[Frontend][MxNet] Support bidirectional RNN layer (#3397)* Support bidirectional RNN layer* tweak* tweak,5
Add Reduce operators to TFLite (#3421),1
fix (#3417),0
Fixing package path in tflite test (#3427),0
"[Runtime] Allow for parameter sharing in GraphRuntime (#3384)Summary:In multi-threaded applications where we have multiple inferences on thesame model in parallel (consider e.g. a TTS system handling multiplerequests), it can be useful to share the parameters of a model amongstthese multiple instances. This improves the cache utilization behaviourof the system, as multiple cores can use the same set of weights insteadof evicting the identical copies of weights in a shared cache.As the underlying `NDArray` instances in `data_entry_` implement aref-counted based sharing system, this is a simple modification of the`GraphRuntime::LoadParams` logic to instead copy parameters from anexisting GraphRuntime instance. This is a little ugly in that we needboth the pre-existing GraphRuntime instance, as well as the 'serialized'params (since we need to know the set of names we should copy), butwithout imposing additional assumptions (i.e. storing the set of paramnames in GraphRuntime, and enforcing that shared param names areidentical to the parameters set in the preceding `LoadParams` call),this seems unavoidable.Test Plan:Unit test added.",1
"[VTA] Add VTA PYNQ metal_test bitstream program logic and fix compile issue. (#3400)* [VTA] Add VTA PYNQ metal_test bitstream program logic and fix couple compile issue.Issue:VTAProgram not exist and cause compile error.No logic to program the bitstream into FPGA.metal test still use pynq 2.1 library which not support on latestpynq 2.4.Solution:remove old VTAProgram.when setting is pynq, program the bitstream during compile.change DMA link library to libcma.* Address review commends.",0
[Relay] Add ResizeNearestNeighbor and CropAndResize in tf converter (#3393),1
[AutoTVM] Fix a bug in simulated annealing (#3413)* [AutoTVM] Fix a bug in simulated annealing* Update sa_model_optimizer.py,0
Fix Windows build (#3429),0
"Undefined name: Typo in variable name sotrage_order --> storage_order (#3439)Discovered via: __flake8 . --count --select=E9,F63,F72,F82 --show-source --statistics__",5
Add mod supoort in relay.build (#3424),1
"[VTA][TSIM] Verilator compile report error for printf (#3438)[Symptom]after follow the tsim example readme, doing verilator install by 'sudo apt-get-install verilator'Once enable 'debug' or manually add 'printf' logic in chisel module, verilator would reportfollowing error.'syntax error, unexpected INTEGER NUMBER, expecting IDENTIFIER'[Fix]upgrade verilator to 4.012, issue fixed.[Solution]Link README.md verilator install steps with verilator home websiteinstall instruction.",0
"Use print() function in both Python 2 and Python 3 (#3440)Discovered via: __flake8 . --count --select=E9,F63,F72,F82 --show-source --statistics__Legacy __print__ statements are syntax errors in Python 3 but __print()__ function works as expected in both Python 2 and Python 3.",0
GraphTuner supports relay.module as input (#3434),5
[Relay][Frontend] Fix tensorflow frontend lstm forget bias adding order (#3410),0
[Relay] Fix reduce axis bug (#3422)* fix relay reduce axis bug* add tests for reduce bug,0
"[Relay][Parser] simplify build script, remove python 2 support  (#3419)* simplify build script, remove python 2 support* remove py2 file* update py3",1
Memory leak in the relay interpreter (#3448),5
fix deprecation warning (#3446),0
"[Relay] Register abs gradient: grad * (select(x < 0, -1, 1)) (#3447)",5
[RELAY] [OP] [MXNet Frontend] Add sequence_mask (#3437)* Add sequence_maskuse exactly the same arguments as mxnetfix* fix lint* fix lint* add mxnet conversion + relay* update* update doc* fix pylint* fix doc* address comment* try to address comments* try to enable shape check for valid_length* fix* try to fix* fix bug* try to fix* address comment* address comment,0
"Nested rfactor fix, update predicates as well as source. (#3382)* Nested rfactor fix, update predicates as well as source.* Linter* Syntax fix.",0
[Relay] Fix ad for conditional expression (#3453)* save* fix,0
[Relay] Feature Detection (#3238)* initinitlintrenamecifixaddadd some docsaveadd some testadd some testlintlintlint* fix build,0
[VTA][Relay] Relay Compilation + AutoTVM compatible operator libraries for VTA (#3135),5
"[CI] Fix windows build, add azure pipeline (#3458)",0
Migrate badge to new job (#3459),1
Update README.md,1
[Bugfix] Fix AutoTVM bug (#3462)* fix autotvm* fix bug when heap_items is empty,0
"Partition fix with rfactor, simplify and likely predicates. (#3444)",0
[ARITH] Improve min/max/div cases in RewriteSimplify (#3463)[PASS] Use new infra for lower warp memory[ARITH] EvalSet recursively evaluates set in case dom_map contains set that need to be relaxed.,1
"[ARITH] CanonicalSimplifier, better folding, eliminate store. (#3464)",5
Update tflite wheel version to 1.13.1 (#3435),1
[ARITH][SCHEDULE] Update schedule to use the new analyzer (#3466),1
[ARITH] Canonicalize comparison to move constant to one side (#3467),4
[Relay][Pass] Only allow Module -> Module for opts managed by pass infra (#3430)* [Relay][Pass] Only allow Module -> Module for opts managed by pass infra* revert gradient pass,4
Migrate simplifier to new infra. (#3368),1
[ANALYSIS] Mac count deconv (#3469)* add mac count for conv 2d transpose* add the explanation of missing parameter in docstring* typo* fix pylint,0
[RUNTIME] Only checks the custom data type if it is bigger than the specified range (#3471),5
[Relay] fix 'please use input parameter mod warning' triggered in build_module (#3452),0
[Runtime] Android argsort support (#3472)* Add contrib sort functions to android rpc app.* replaced tab with spaces oops.,1
[Codegen] Support broadcast op with symbolic shape (#3389)* [Codegen] Support broadcast op with symbolic shape* fix case where last dim = 1* use enum; simplify stride calculation; improve doc* fix lint* improve py doc,0
Clean up pass.h (#3312),4
Delete _ir_pass.pyi,4
Add dockerfiles for the conda package builds (#3344)* First shot* Add dockerfile for CPU too* Finish the build infrastructure* Remove extra file* Comment out the Jenkinsfile section since it is not ready* Add missing license headers* Update to newer cudnn that anaconda packaged* Bump the build numbers for the newer cudnn* Bring back the toolchain option with a tweak for cuda* Cache some large packages in the docker and update to llvm 7.0.0* Merge all the python packages together* First fix for the conda cuda builds (again)* Use the tarball version of cudnn since tvm has trouble detecting the other one* Use llvm 8.0 from the numba packages* Also use llvm 8.0 for the cpu builds* Don't use the anaconda compiler for OS X* Enable Metal on OS X builds* Make sure to detect undefined variables in scripts* Fix build when not using cuda,0
[Relay] Continuation Passing Style (#3456)* saveaddme find type checker problemsavesavelintdolintreset tiadd some docadd failed test caseadd recursion for cpsadd recursion for cpsfix pytestlintsavefix test errorlintsavefix error* fix rebase* fix* fix test* lint* lint* restore rewriteannotationops* do,0
producing simulation statistics instead of time to get useful information out of simulation runs (#3481),2
[Relay] use transform instead of ir_pass for CPS (#3485),2
Pre-allocate buffer for x86 roi_align (#3475)* Pre-allocate buffer for x86 roi_align* Fix typo,0
[Relay] Fix PE (#3482),0
Docker: Install Python packages 'requests' and 'Pillow' (#3495)Needed for:- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/tutorials/frontend/deploy_model_on_android.py#L30- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/tutorials/frontend/deploy_model_on_android.py#L37- https://github.com/dmlc/tvm/blob/287078c33db85d4f312d8d2457a064442d9d18c3/python/tvm/contrib/download.py#L58,2
Tutorial: Use Python 3 (#3498),5
[VTA][Hotfix] Avoiding error when environment variable is not set (#3497)* avoid error when env var is not set* extra content,0
"[Relay][Module] Make tags for ADT constructors and ConstructorValues more robust (#3369)* Use hash of ADT name and constructor idx to generate tag, add reverse mapping to module and use where appropriate* Lint and build fixes* Add round-tripping test for getting constructors by tag* Use int64_t everywhere for tags* Add additional identity check* Bring out _arg_to_ast again* Use 8-bit hash of GTV name as MSB of tag, index as LSB for more readable tags* Use int32 instead of int64 for tag",0
Android demo: Docker improvements (#3499)- Install OpenCL headers- Set ANDROID_HOME environment variable,2
"[relay][frontend] Return Module from get_workload (#3483)* [relay][frontend] Return Module from get_workload* pass entry_func to autotvm* disable tune* add property to module* mod.entry_func to main* .main -> mod[""main""]* fix",0
Android RPC README improvements (#3500)- Fix APK path- Add ADB install/uninstall instructions,0
[ARITH] Refactor: Remove un-necessary usage of ComputeExpr (#3503),4
[TOPI] add basic scheduling for conv2d_transpose on x86 (#3491)* initialize cond 2d transpose scheduling on x86* refine the scheduler a bit* fix for lint* address review comments; remove duplicate code* fix lint,0
[ARITH] Bugfix div subtract rewrite rule (#3504),0
"[ARITH] More recursive rewrite rule, cleanup simplify tests (#3502)",3
[VTA] TSIM improvements and fixes (#3505)* add tsim init function* add sim device* test wait and resume* launch simulation thread from DPILoader* add VTASimDPI module to handle all simulation related stuff* test tsim init* move exit to simdpi module* update vta driver* add chisel DPI module* get back simshell* update vta to support dpi sim* update unittests* add tsim to integration-conv2d test* run resnet on tsim* remove max-cycles* match tsim counters with sim counters* use env in simulator to switch between sim and tsim* update unittest* rollback conv2d test* update resnet* add stats to matrix multiply* add stats* print stats after assert* update other tests* add stats to gemm* add return and remove unused libs* add missing arg* return lib* update comments for linter* add more comments to VTASimDPI module* remove trailing spaces* remove trailing spaces,0
[Relay][Transform] Support Dumping IR to help debugging (#3493)* [Relay][Transform] Support Dumping IR to help debugging* debugprint->printir,0
Passing dilation argument to account for API change. (#3510),4
[Relay] Roundtrip part of pretty printer and parser (#3460)* initfix rebaselintfix cmaketry againfix ci* add gitignore* fix format* do not include .interp and .tokens,0
[Vulkan] Added conversion from bool to float. (#3513)* Added bool to float conversion support to spirv ir builder.* Added unittest for vulkan bool conversion.* Typo fix.,0
"Relaxing convolution infer checks. (#3511)- Weight dtype can be different than idtype. So, using the weight tensor to setthe dtype of weight.- For conv2d NCHWc operator, the weight can be of any dimension. For int8computation on Intel, it can be 7D. Relaxing the weight type checking.",5
"[Relay] Clip gradient: grad * (select(x < min || max < x, 0, 1)) (#3509)",5
"[Relay][VM]Compiling pattern matching (#3470)* [Relay][VM]Compiling pattern matching* Fix lint* Remove debug code* Move TreeNode definition* merge ifi and selecti, todo: remove them* fix lint* remove ifi and selecti* rename GetTagi to GetTag* fix dltype* fix more dltype* Generalize If and select, and rename to Ifi and Selecti* Fix lint* Rename Ifi to If* Change register default to match value* Remove bad specialization for Move* Stop use Select* Remove Select* TreeNode refactor* Change entry_func name* Remove Cmp due to rebase issue",0
"[Relay][Testing] Relay-to-Python compilation (#3156)* First pass at Relay-to-Python converter testing utility* Indicate astor as a dependency* Add astor dep to host as well* Typos and small bugs* Handle ADTs and matching in Python conversion* Remove any dependency on ast.parse* Eliminate unnecessary type var field in Python version of ConstructorValue (already gone on C++ side)* Update constructor value, fix syntax errors* Don't forget keywords arg on Call nodes* Fix some incorrect calls to ast nodes* Fix more calls, a little more cleaning up* Missing cases in attr conversion* Lower op calls instead of running them through interpreter, as in @MarisaKirisame's AoT compiler* We do still need the module* Remove changes to op attrs: Will PR separately* Smoke test and corrections* More tests and fixes* Ensure imports are properly global in generated Python code* Add unit tests for refs* Add unit test for tuple indexing* Add unit test for if expression* Remove astor dependency* Remove astor from meta.yaml too* Fix if test and add basic local function test* Add global function test, refactor earlier tests* Correct 'clause' field in ADT so Python and C++ field names match* More fixes and tests for matching and constructors* Dramatically simplify matching: no need for a thunk* Improve ref writing test* Ensure local recursion works* cleanup* Add test for global recursion* Add test for higher-order calls* Get ops working, add basic tests* Remove accidentally duplicated test* More docstrings to appease pylint* Forgot to fix a test using constructor values* Reduce optimization level in fusion and fix tuple input to operators* Test op with tuple output, fix tuple output code* Add unit test for batch norm* Add a couple more tricky test cases* Correct nat constructor to drop unnecessary field* Fix the op attrs file (accidentally reduced it)* Address review comments* Adapt to new ConstructorValue representation (no more runtime dep on module)* Use pass manager and updated interfaces. Extend module.from_expr to accommodate necessary demands* Use sequential return value* Lift out nested conditionals* Replace triple single quotes with triple double quotes* Use main variable instead of entry_func",0
[CPP] Refactor remove tvm/tvm.h (#3523),4
[Relay][Doc] Docs for new op code (#3522),1
fix test (#3525),0
Fix broken rat install (#3527)http://www.trieuvan.com/apache//creadur/apache-rat-0.12/apache-rat-0.12-bin.tar.gzgives a 404 so point the installer at archive.apache.org hopefull thatis more reliable.,0
"Fix pylint issue in vta/python/vta/top/graphpack.py (#3519)This appears in linting using the docker scripts. I'm not surewhy this isn't failing in the standard CI for TVM and it mightbe that the docker images haven't been updated in the CI system.python3 -m pylint vta/python/vta --rcfile=/workspace/tests/lint/pylintrcUsing config file /workspace/tests/lint/pylintrc************* Module vta.top.graphpackC:131, 4: Missing method docstring (missing-docstring)",0
"[Relay][RFC] Implement type checking for Any (#3221)* Implement type checking for AnyRemove code generation related changesRemove compile changesRemove moreRemove unification hackAdd some code back that was needed, and clean up testRefactor test casesWIPImplement TypeHint ASTAdd test case which should failRemove unification changes, and fix bug with let recRestore unification for shapesImprove error reporting while debuggingAll examples type checkAll examples type checkWIPFirst version that works with hints, needs clean upRemove dead codeTweaksRemove type hintRemove unecessary type hint stuffRemove more type hintsClean upExpose Any expression nodeAddress CRFixFix solverKill unecessary codeFixPyLintFixRelocate loopsFix license and testLint againLint againFix loopsFix docstringFix template errorFix compiler issueFix compile errRemove more runtime changesRestore bufferFix segfaultFixFix arange* Address feedback* Fix typo* Fix arange* Fix op level3* Fix issue with Python wrapper",0
[bugfix] fix the bug caused by test_any (#3528),0
init (#3476)lintupdateaddress commentcomment out breaking test,1
Add Pack operator to TFLite (#3521),1
"posix_memalign appears in API 17, not 16 (#3532)",5
[INFA][IR] Build and Evolve Low-level IR. Remove HalideIR dep. (#3533)* [INFA][IR] Build and Evolve Low-level IR. Remove dep from HalideIR.* Update include/tvm/node/ir_functor.hCo-Authored-By: Jared Roesch <roeschinc@gmail.com>* Update include/tvm/node/ir_functor.hCo-Authored-By: Jared Roesch <roeschinc@gmail.com>,1
[DEP] Remove HalideIR from submodule (#3535),4
[Relay][Quantization] Fix add_rewrite and UnifyDTypeScale (#3534)* [Relay][Quantization] Fix issue introduced in #3135* Recover StopFusion* Fix fmultiref* Fix lint,0
"[ARITH][IR] Introduce FloorDiv/Mod (#3479)* [ARITH][IR] Introduce FloorDiv/Mod* Address review comments* address review comments, fix div sub rule",0
[ARITH][BOUND] Fix bound inference to avoid allocating too much (#3526)* [TVM] Fix bound inference to avoid allocating too much* [ARITH][BOUND] Pass analyzer to PropBoundToInputs,0
[Runtime] Enable set_input_zero_copy in GraphRuntime (#3416)* Enable set_input_zero_copy in GraphRuntime* Fix LoadParams* Fix* lint* Fix remote context issue* Fix* Remove LOG* Remove unused variables* Add tests* works* More test scenarios* make it simpler* Remove unnecessary changes* Address comments* More comments* Address comments* Fix build,0
"[Relay][VM] Port VM, VM compiler, and Object into python (#3391)* tmp* Port vm and object to python* clean up* update vm build module* update* x* tweak* cleanup* update* fix rebase* Rename to VMCompiler* fix",0
[FRONTEND][TENSORFLOW] Some bug fixes for tensorflow NCHW data_format (#3514),0
fix (#3550),0
fix js test load module example (#3556),0
Fix build error (#3552)* Fix build error* comments,0
fix pynq 32-bit address pointers (#3558),0
[Relay][VM]Fix debug statement (#3565)* [Relay][VM]Fix debug statement* Change debug statement,0
[docs] Add a tutorial for the pass manager (#3515)* [docs] Add a tutorial for the pass manager* address comment* address more comments* retrigger ci* address steven's comments* address comments* retrigger ci* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>* Update docs/dev/relay_pass_infra.rstCo-Authored-By: Logan Weber <36520469+weberlo@users.noreply.github.com>,1
tightening bounding box for IntSet fused in PassUpDomain (#3073)Apply suggestions from code reviewCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>,4
[Community] Zhi Chen -> Committer (#3572)Let's welcome Zhi as a new Apache TVM Committer!,1
Disable MicroTVM on i386 CI (#3569),5
Support additional architectures beyond x86_64 in ubuntu_install_java (#3546)* Support additional architectures beyond x86_64 in ubuntu_install_javaWhile attempting to get a development environment going for TVMon my AArch64 desktop I ran into some hardcoding of relevant architectures.,1
Emit DWARF debug information (#3420),0
[ARITH] Simplify let (#3568),5
[Relay] parser/pretty printer roundtripping (#3536),5
fix topi c++ conv2d_nchw lambda expr issue (#3570),0
avoiding cast None to int errors (#3578),0
Mention minimum version of python features one should stick to (#3588),5
Add printer for Layout/BijectiveLayout (#3582),1
[RPC] Better handle tempdir if subprocess killed. (#3574),5
[AutoTVM]Improve graph tuner for multiple subgraphs (#3490)* Improve boundary nodes in graph tuner* Limit output node number* Fix test* Improve warning.* Fix test,0
[TOPI][RELAY] Add op Size (#3094),1
[Relay] add some check for the ad algorithm (#3585)* do* fix test,0
bugfix function args order in alu instruction generation (#3592),0
"add coherent, length, and user bits option to Shell Config (#3593)",1
[CI] Upgrade LLVM envs (#3590),5
[VTA] Runtime refactor to allow for non-shared memory FPGAs (e.g. F1) (#3554)* updated runtime to support non-shared memory FPGAs for instruction and micro-op kernels* adding driver-defined memcpy function to handle F1 cases* refactor to include flush/invalidate in memcpy driver function* update tsim driver* bug fixes* cleanup* pre-allocate fpga readable buffers to improve perf* fix* remove instruction stream address rewrite pass for micro op kernels* fix:* white spaces* fix lint* avoid signed/unsigned compilation warning* avoid signed/unsigned compilation warning* fix* fix* addressing comments* whitespace* moving flush/invalidate out of memmove* clearnup* fix* cosmetic* rename API* comment fix,0
Update Jenkinsfile,1
"Add support for Tflite operator SPLIT (#3520)* [RFC] Initial support for Tflite operator SPLITThis patch adds initial support for the tflite operator split. HoweverI am not yet sure how to handle the axis parameter for the splitoperator and support it in the test infrastructure. Putting this up foran initial review and comment.The split operator in tflite according tohttps://www.tensorflow.org/lite/guide/ops_compatibilityappears to take num_or_size_split as a 0D tensor.I also note that tflite.split is one of the few operators that returnsmultiple outputs and thus the helper routines in the tests needed somemassaging to make this work.@apivarov , could you please review this ?Thanks,Ramana* Fix the axis parameterAdd more tests* Address review comments* Try out frozen_gene's suggestion* Handle split of 1 element* int32 is only supported in tflite 1.14, let's check that version here.* Keep this at python3.5* Add packaging as a python package to be installed",0
"[Runtime] [ThreadPool] Make SpscTaskQueue::Pop(..) spin_count configurable (#3577)In cases where we have multiple models or threadpools active, spinning around`sched_yield()` may not be desirable, as it prevents the OS from effectivelyscheduling other threads.Thus, allow users to conditionally disable this behaviour (via an environmentvariable `TVM_THREAD_POOL_SPIN_COUNT`, similar to existing environment flags forthe thread pool such as `TVM_BIND_THREADS`, etc).This substantially improves tail latencies in some of our multi-tenantworkloads in practice.Unit tests have been added - on my laptop, running:```TVM_THREAD_POOL_SPIN_COUNT=0 ./build/threading_backend_test;TVM_THREAD_POOL_SPIN_COUNT=1 ./build/threading_backend_test;./build/threading_backend_test;```gives https://gist.github.com/ajtulloch/1805ca6cbaa27f5d442d23f9d0021ce6 (i.e.97ms -> <1ms after this change)",1
[Relay] [Training] Allow gradient to return a tuple (#3600),5
Checking the correct dtypes for choosing the Intel int8 instructions. (#3516),2
[Relay][Pass][Docs] Update the doc for adding a Relay pass to mention the pass infra (#3583)* Update the Relay adding pass doc to reference the new pass infrastructure* Correct pass nameCo-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Align header equals signs,1
remove tabs (#3603),4
"{relay,topi}.reinterpret support (#3599)= MotivationIt's useful to expose the tvm::reinterpret functionality to Relay/TOPI users, asthis allows them to build (fused) operators leveraging the bitwisereinterpretation of an operator. An example is approximate transcendentalfunctions, which can be implemented similar to:```.py    def C(x):        return relay.expr.const(x, ""float32"")    def approx_exp(x):        x = relay.minimum(relay.maximum(x, C(-88.0)), C(88.0))        x = C(127.0) + x * C(1.44269504)        xf = relay.floor(x)        i = relay.cast(xf, ""int32"")        x = x - xf        Y = C(0.99992522) + x * (C(0.69583354) + x * (C(0.22606716) + x * C(0.078024523)))        exponent = relay.left_shift(i, relay.expr.const(23, ""int32""))        exponent = relay.reinterpret(exponent, ""float32"")        return exponent * Y    def approx_sigmoid(x):        # <2.0e-5 absolute error over [-5, 5]        y = approx_exp(x)        return y / (y + C(1.0))    def approx_tanh(x):        # <4.0e-5 absolute error over [-5, 5]        x = x * C(2.0)        y = approx_exp(x)        return (y - C(1.0)) / (y + C(1.0))```See unit tests for implementations of these approximate transendentals.",0
"We observe multiple groups across a range of domains (ASR, NMT, LM, etc), (#3566)internally and externally, interested in replacing standard dense layers withblock-sparse matrix multiplication layers. The motivations are generally: higherperformance (due to reduction in FLOPs, memory bandwidth/cache footprint),enabling larger models (e.g. fitting more layers in a given memory budget).Some public work along these lines:* https://openai.com/blog/block-sparse-gpu-kernels/* https://openai.com/blog/sparse-transformer/* https://arxiv.org/abs/1802.08435* https://arxiv.org/abs/1711.02782Various groups have been able to successfully train models with reasonablelevels of sparsity (90%+) with marginal accuracy changes, which suggestssubstantial speedups are possible (as this implies a >10x reduction in FLOPs).It is fairly straightforward to realize these theoretical speedups, see e.g. TVMbenchmarks for Intel CPUs inhttps://gist.github.com/ajtulloch/e65f90487bceb8848128e8db582fe902, and CUDAresults in https://github.com/openai/blocksparse, etc.* https://github.com/openai/blocksparse (CUDA)* https://software.intel.com/en-us/mkl-developer-reference-c-mkl-bsrmm (MKL BSRM)* https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html (SCIPY BSR representation)This is extracted from an internal patch we've been using internally. There arevarious extensions possible (int8/fp16/bf16, CUDA/other GPU architectures), butthis is a reasonable starting point. This needs more thorough unit test coveragehowever.We follow the conventions established by scipy.sparse.bsr_matrix and otherlibraries, see the unit tests for details.For folks interested in experimenting with scheduling/AutoTVM etc,https://gist.github.com/ajtulloch/e65f90487bceb8848128e8db582fe902 is a usefulstarting point.",2
[Relay][vm] Small bug fix for DataTypeObject (#3604)* small bug fix for DataTypeObject* retrigger ci,0
[TOPI][Relay] max_pool2d & avg_pool2d gradient (#3601),5
init (#3571)quickfix,0
[TEST] Fix testcase to make them more compatible to zero-rank (#3612),0
Hotfix pylint (#3615),0
Remove prints in `generic_op_impl.py` (#3616),4
[TOPI] Average Pool2D Bug. (#3607)* [TOPI] Average Pool2D Bug.Issue - https://github.com/dmlc/tvm/issues/3581* Add uint16 test.,0
fix typo (#3611),0
"[Relay][Keras] Permute, Softmax support (#3618)",5
Add a missing header in cuda_device_api.cc (#3621),1
"Implementation of uTVM (#3227)* uTVM interfaces (#14)* some minor interface changes* implemented HostLowLevelDevice* added MicroDeviceAPI* implemented micro_common and added Python interfaces* current status, semi implemented micro session* added micro_common implementation and python interfaces (#18)* added micro_common implementation and python interfaces (#18)* current status, semi implemented* host test working* updated interfaces for MicroSession arguments allocation* make somewhat lint compatible* fix based on comments* added rounding macro* fix minor bug* improvements based on comments* Clean up `binutil.py` and make Python-3-compatible* Change argument allocation design* Address feedback and lint errors* Improve binutil tests* Simplify allocator (per @tqchen's suggestions)* Doc/style fixes* farts* mcgee* rodata section werks(and so does `test_runtime_micro_workspace.py`)* simple graph runtime werk* TEMP* ResNet works, yo* First round of cleanup* More cleanup* runs a dyson over the code* Another pass* Fix `make lint` issues* ready to pr... probably* final* Undo change* Fix rebase resolution* Minor fixes* Undo changes to C codegen tests* Add `obj_path` in `create_micro_lib`* TEMP* Address feedback* Add missing TODO* Partially address feedback* Fix headers* Switch to enum class for `SectionKind`* Add missing ASF header* Fix lint* Fix lint again* Fix lint* Kill lint warnings* Address feedback* Change Python interface to MicroTVMAll interaction with the device is now through `Session` objects, whichare used through Python's `with` blocks.* Reorder LowLevelDevice interface* Store shared ptr to session in all alloced objects* Move helper functions out of `tvm.micro`* Switch static char arr to vector* Improve general infra and code qualityDoes not yet address all of tqchen's feedback* Forgot a rename* Fix lint* Add ASF header* Fix lint* Partially address MarisaKirisame's feedback* Lint* Expose `MicroSession` as a node to Python* Revert to using `Session` constructor* Fix compiler error* (Maybe) fix CI error* Debugging* Remove* Quell lint* Switch to stack-based session contexts* Make uTVM less intrusive to host codegenAnd use SSA for operands of generated ternary operators* Inline UTVMArgs into UTVMTask struct* Remove `HostLowLevelDevice` header* Remove `BaseAddr` class* Address feedback* Add ""utvm"" prefix to global vars in runtime* Fix lint* Fix CI* Fix `test_binutil.py`* Fix submodules* Remove ResNet tests* Make `test_binutil.py` work with nose* Fix CI* I swear this actually fixes the binutil tests* lint* lint* Add fcompile-compatible cross-compile func* Add docs for uTVM runtime files* Move pointer patching into `MicroSession`* Fix lint* First attempt at unifying cross-compile APIs* Fix lint* Rename `cross_compile` back to `cc`* Address feedback* Remove commented code* Lint* Figure out failing function* Remove debugging code* Change ""micro_dev"" target to ""micro""* Add checks in tests for whether uTVM is enabled* Add TODO for 32-bit support* Rename more ""micro_dev"" to ""micro""* Undo renameWe already have `tvm.micro` as a namespace.  Can't have it as a methodas well.* Fix failing CIThanks to @tqchen for finding this bug.  Emitting ternary operators for`min` and `max` causes concurrency bugs in CUDA, so we're moving theternary op emissions from `CodeGenC` to `CodeGenCHost`.* Address feedback* Fix lint",0
Add Winograd matrices computation. (#3553),1
[IR] Make iterators compatible with constructors of STL containers (#3624),5
"[VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity (#3605)* support for different inp/wgt bits, rewrote dot for clarity* [VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity* [VTA] [Chisel] support for different inp/wgt bits, rewrote DotProduct for clarity* change back to sim* fix index* fix index* fix indent* fix indent* fix indent* fix trailing spaces* fix trailing spaces* change to more descriptive name* matric->matrix* fix spacing* fix spacing & added generic name for dot* better parameter flow* spacing* spacing* spacing* update requirement (tested) for dot, spacing* function call convention* small edit",0
[Relay] [Training] Add numerical gradient check. (#3630)* add check_grad* finish* what does the fox say?* lint lint lint lint lint lint lint lint lint,1
[TOPI][CUDA] Schedule for pool_grad (#3622)* [TOPI][CUDA] Schedule for pool_grad* Relay test* Fix fused op* doc* Remove set scope local,0
[TensorFlow] Fix a bug output index is ignored (#3631)Enhance test to cover this case,0
Make Google Test usage configurable in CMake files (#3628)* Add USE_GTEST as a CMake variable* Add GTest section in installation docs* Incorporate feedback,1
Update tensorflow.py (#3632),1
Improve the x86 auto-tune tutorial (#3609),5
[Relay][TF] add BatchMatMul (#3634),1
[VTA] [Chisel] fix tensor issue/commit in gemm (#3637)* fix tensor issue/commit in gemm* remove trailing spaces,0
fix case when offset is odd and size is even (#3643),0
Hotfix for issue #3641. (#3644),0
fix comment/doc in TensorLoad (#3646),0
"[VTA] Refactor to increase platform coverage (Ultra96 etc.) (#3496)* hardware refactor for increased FPGA coverage, small optimizations* fix header* cleaning up parameters that won't be needed for now* streamlining makefile, and simplifying tcl scripts* moving parameter derivation into pkg_config.py, keeping tcl scripts lightweight* refactoring tcl script to avoid global variables* deriving AXI signals in pkg_config.py* unifying address map definition for hardware and software drivers* single channel design for ultra96 to simplify build* enable alu by default, no mul opcode for now* hardware fix* new bitstream; vta version* avoid error when env variable is not set* ultra96 cleanup* further cleaning up tcl script for bitstream generation* preliminary rpc server support on ultra96* rpc server tracker scripts* ultra96 ldflag* ultra96 support* ultra96 support* cleanup line* cmake support for ultra96* simplify memory instantiation* cleaning up IP parameter initialization* fix queue instantiation* 2019.1 transition* fix macro def* removing bus width from config* cleanup* fix* turning off testing for now* cleanup ultra96 ps insantiation* minor refactor* adding comments* upgrading to tophub v0.6* model used in TVM target now refers to a specific version of VTA for better autoTVM scheduling* revert change due to bug* rename driver files to be for zynq-type devices* streamlining address mapping* unifying register map offset values between driver and hardware generator* rely on cma library for cache flush/invalidation* coherence management* not make buffer packing depend on data types that can be wider than 64bits* refactor config derivation to minimize free parameters* fix environment/pkg config interaction* adding cfg dump property to pkgconfig:* fix rpc reconfig* fix spacing* cleanup* fix spacing* long line fix* fix spacing and lint* fix line length* cmake fix* environment fix* renaming after pynq since the driver stack relies on the pynq library - see pynq.io* update doc* adding parameterization to  name* space* removing reg width* vta RPC* update doc on how to edit vta_config.json* fix path* fix path",0
[VTA] [CMake] hotfix tsim rules (#3650),0
[VTA] [Chisel] make dram offset configurable for uops different than 4-bytes (#3654),5
[Relay][VTA] Add ChangeBatch pass  (#3656)* init* lint* lint,1
[Relay] Fix typo in ChangeBatch (#3660),0
Print llvm source by default in ROCMModuleNode::GetSource (#3662),5
tvm/contrib/rocm: improve finding of ld.lld (#3664)This refines the detection of ld.lld matching the neighbouring clangfile. This is particularly helpful on Ubuntu/Debian when either thedefault ld.lld is not installed or the versioned one is preferable forconsistency.@tqchen I think you last touched the clang equivalent in #3590 .,2
ROCm: Add SaveToFile and LoadFile (#3665)...and add rocm module_save to the tests.,1
[TOPI] Fix traverse function not inline zero-input op (#3623)* Fix traverse_inline not inline zero input op properly* Add where to python and set tag to broadcast* Fix inline* test* fix test target* fix,0
[TOPI] Enable standalone wheel build (#3657)* Fixed topi bdist_wheel build to include libraries.* Removed unneeded imports,0
removing deprecated script (#3667),5
[VTA] Support for batched inference (#3661)* fix in IR pass to support padding on 6-d tensors* support for both N>1 and N==1 for padding* batch size > 1 tuning and base config* output formatting* batch conv2d* print all category results* revert to single-batch config* pick record best* fix conv test* improving reporting* address batching bug in fast simulator* fix,0
[RPC] Terminate worker's childs first. (#3669),5
add reviewer - slyubomirsky (#3673),1
Add yolov3-tiny to the tutorial. (#3674),1
"[VTA] VTA Compilation Script for Intel FPGA (#3494)* initial compilation script for chisel-vta;* replace tabs with spaces;* compile script for de10-nano;* remove generated verilog source code;* remove `altsource_probe`, `debounce`, `edge_detect` ip;* replace quartus project files with a single tcl script;* Update install.md* improved makefile-based compilation script;* complete makefile-based compilation of chisel-vta for de10-nano;* install quartus;* conversion to .rbf file;* document chisel-vta compilation process for de10-nano;* rename generated bitstream file;* download and extract custom ip for de10-nano;* minor change* minor change* fix indentation;* bug fix;* improved robustness in makefile;* clean up;* add `.sdc .ipx .qsys` allowance in jenkins;* add ASF header;* add ASF header;* remove IntelShell.scala, update vta_hw.tcl, clean up Makefile & soc_system.qsys;* add ASF header;* keep sources compact;* keep sources compact;* it's not necessary now* AXI4LiteClient -> AXI3Client for IntelShell* remove connection to fpga_only_master;* a few important bug fix: wire reset pin, and set host_r_last to high* remove intel specific interface definition;* add NO_DSP option in Makefile;* AXI4Lite is not used in IntelShell;* minor fix: disable dsp and use logic instead;* quartus version change: 18.0 -> 18.1* remove altera related statement;* compose compile_design.tcl* initial tcl script for soc_system generation;* remove .qsys file;* remove unused;* .qsys can be generated by tcl script;* remove hps_io and shrink size of soc_system;* integrate into makefile;* version change: 18.0 -> 18.1* add sample config file for de10-nano;* parameterize DEVICE and PROJECT_NAME* remove extra lines;* brief description on flashing sd card image for de10-nano* docs on building additional components* parameterize DEVICE and DEVICE_FAMILY* parameterize DEVICE and DEVICE_FAMILY* parameterize DEVICE and DEVICE_FAMILY* de10-nano -> de10nano* minor change* add comment in code and document in order to address review comments;",0
[TOPI][CUDA] schedule for group_conv2d (#3663)* [TOPI][CUDA] schedule for group_conv2d* Fix #flops,0
[TEST] Comptiable with python3.5 (#3675),3
"[Relay][VM] Relay VM serialization (#3647)* relay vm serialization* fix lint* load params, fix stream* lint* fix typo",0
"[DOC] Update ssd doc to avoid confusion. (#3677)* intel graphics conv2d bugs fixed for inception_v3* intel conv2d api updated, nn input size 4 condition added* review addressed* move conv_tags to attributes* ssd doc updated* address comment",0
Replace learnt with learned (#3684),5
"Make tests multi-process friendly. (#3683)This side effect at module import time has a race condition between the ""exists"" check and the ""mkdir"" call.  The safer thing is to just call mkdir and catch the ""already exists"" error which is what makedirs does.",0
[Relay][Frontend] Fix typo names in frontend (#3685)Fix typo names in caffe2 and onnx frontend:* sotrage_order -> storage_order* OpNotInplemented -> OpNotImplemented,0
[Relay] Strict mode in pattern matching (#3620)* add fatallintlintlintdomake completeness check an errorlintremove fatal* fix test* reset parser file* remove unneeded import* Update python/tvm/relay/adt.pyCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Update include/tvm/relay/adt.hCo-Authored-By: Steven S. Lyubomirsky <slyubomirsky@gmail.com>* Eliminate trailing whitespace (my fault),0
"Add support for Tensorflow operators log1p, cos, sin (#3614)The patch adds support for Tensorflow operators log1p and cosTensorflow log1p is described at https://www.tensorflow.org/api_docs/python/tf/math/log1pTensorflow cos is described at https://www.tensorflow.org/api_docs/python/tf/math/cosTensorflow sin is described at https://www.tensorflow.org/api_docs/python/tf/math/sin",1
Enable the sparse schedule (#3651),5
Add shuffle support to TVM (#3633),1
[Relay][VM] Support execution on devices (#3678)* [Relay][VM] Support execution on devices* Reduce Copy calls* Cleanup* Lint* CR comments* Merge test into test_vm.py,3
[Relay][Quantization] KL-divergence-based per-layer calibration (#3538)* [Relay][Quantization] Support floating-point scale* [Relay][Quantization] KL-divergence calibration on dataset* Fix unhandled LeftShift case in QuantizeRealize* Fix lint* drop QBias* fix lint* address comments* address comments* Update comments* address comments* lint* kQIdentity = 0,0
[TOPI] Memoize winograd matrix (#3687)* [TOPI] Memoize winograd matrix* lint* Fix name,0
"[DOCKER] Add DGL to {ci_gpu, demo_cpu, demo_gpu} docker images (#3692)* add dgl to docker file* add dgl to docker file",1
Align the naming rule for OpAttributeUnImplemented (#3695),5
[AutoTVM] Fix hang/crash issues on feature extraction (#3689)* [AutoTVM] Fix hang/crash issues on feature extraction* Update xgboost_cost_model.py* fix lint,0
[Relay] [Error] Fix error in partial evaluator (#3693)* fix* lint,0
Add an option to build with -pthread (ON by default) (#3671),1
[VTA] [Chisel] Added Chisel Module Unit Test Infrastructure (#3698)* added wholething* changed build and makefile,1
Fix gather_nd in Relay (#3442)* Fix gather_nd in Relay* Add test cases for gather_nd.,0
[TOPI] Update softmax compute and CPU schedule (#3680)* Update Softmax compute and CPU schedule* Add C++ compute* Fix schedule* Update CUDA and OpenGL schedules* Fix log_softmax* Fix hls and opengl schedules* Fix CUDA schedule,0
"[Relay] Partial Evaluator do concatenate, and has better termination checker for scalar. (#3703)* savelint somelintlintadd charrnnsavesavesaveremove debugremove debugremove spacerefactorsaverewrite dce* reset files* join -> meet* lint* address review comment* wordsmith",0
Metal reinterpret fix (#3706),0
Quit and clean when TVM is interrupted (#3640),5
[CI] Update GPU docker (#3709),1
Export tvm::relay::OpRegistry::OpRegistry (#3711),5
"[Relay] [TOPI] `{relay,topi}.nn.sparse_transpose` for **Square** CSR matrices (#3707)* add build gcn tutorial* add transpose operator for square sparse matrices* remove extra files* change loop tag* comply with lint* comply with lint -- line too long* comply with lint* lint check* lint check* lint check* apply marisa and theirry's reviews",1
[Bugfix] Fix the issue that function pass modifies original module (#3712)* fix* fix interpreter,0
safe to remove thread related headers? (#3713),4
[relay][frontend] clean up tf frontend (#3710)* clean up tf frontend* fix get_relay_op,0
Update dmlc-core to the latest commit (#3716)This includes changes to build TVM runtime for Hexagon.,1
Fix (2/2) [TOPI] conv2d schedule code (#3648) (#3717)* Fix the tile_rx and tile_ry issue.    Note that this patch depends on pull request #9 in tvm-distro.,0
"[Relay] Legalize pass (#3672)* [Relay] Rewrite pass.This pass transforms an expression to other expression.This pass has many usecases * Replace a expr to another expr, if the other expr has faster performance. * For ASICs, we might want to modify the inputs to adapt to the HW support. * Alter op layout can work in conjunction with this pass.The supporting usecase is the Intel i8 x i8 conv. Intel HW supports u8 x i8 convin HW. Using this pass, we can replace an i8 x i8 conv to a sequence ofoperators where one of the operators is now u8 x i8 conv. This will also helpautomatic quantizaion performance.* Better API name.* Removing the conv2d legalization for x86. Will send a separate PR.* Test name changes.* Registering one funtion to register FTVMLegalize.* Better comments.",3
fix name (#3719),0
[Frontend][MXNet] Fix mxnet converter for hybridblock and add div_sqrt_dim (#3701)* Fix mxnet converter for hybrid block* tweak* fix rebase* fix* add test,0
[Relay/TOPI][Op] Add variance and layer norm op (#3700)* Add LayerNorm op* update* fix* Add mean_std and mean_variance* add std and update doc* add license* x* lint* x* fix* fix doc,0
Take zero extent loops as NoOp and remove it and add unittest for the same (#3724),1
[VTA][Dockerfile] Chisel dependencies for TSIM CI (#3721),2
"Tutorial: Build a Graph Convolutional Network on TVM (#3681)* add build gcn tutorial* add dgl to docker file* add dgl to docker file* Apply suggestions from code reviewCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* add dgl to docker file* rerun checks* Revert ""add build gcn tutorial""This reverts commit dbe8b5f0e02a13fdd586a9faa58fd1326653afb0.* resolve git issue* resolve git issue* resolve git issue* apply marisa's comment",1
Remove sccache from Rust install (#3728),2
[DOCKER] Fix missing apt https transport support (#3735)* [DOCKER] Fix missing apt https transport support* [DOCKER] Drop superflous explicit sudo's,0
"[QNN] Requantize operator (#3531)* [Relay] [Quantization] WIP - Common files for the qauntization work.* [Relay] [Quantization] WIP - Prototyping requantize op.* Requantize operator implementation.Requantize converts one quantized tensor representation to another quantizedrepresentation. The PR has following implementation features- Requantize operator defined in qnn namespace - relay.qnn.requantize- Lowering of the requantize to exisiting Relay operators- Integer fixed point implementation of requantize    - Two rounding modes - FE_UPWARDS (round towards infinity) and    FE_AWAY_FROM_ZERO (std::round behavior)- Floating point implementation as well, that can act as reference or can beused for devices when FP32 computation is not used.- Unit test casesRelevant Issue - https://github.com/dmlc/tvm/issues/2351Credit to TFLite and GemmLowp to provide reference implementations.* Typo and lint fixes.* Doc fix.* Uncommenting the lint script (fixing mistake).* Modifying the unit tests.* Moving C++ files into src/relay/qnn* Moving python files to python/tvm/relay/qnn. Some minor fixes.* Moving the attrs.h inside the include directory.* Pushing files that I forgot earlier. Changing util location.* Incorporating comments. API change. Lint fixes.* Modifying the GetFixedPointMultiplierShift API as per comments.* Forgot the dialect change.* Changing rewrite to qnn_lower.* Renaming Quantize to Qnn for clarity.* Remove use_int_domain.* Incorportaing review comments.* Adding API doc for QNN dialect.* Move the qnn_lower pass to transform namespace.* Moving from expr to module. Adding namespace in C++.* Minor sentence rewrites. Added qnn namespace.* Added the API doc.* Chanding default out_dtype to int8. Adding a test with in/out_dtype as uint8.* Style fixes. Better error messages.* Adding documentation.* More documentation fixes.* Adding out dtype check for requantize.* Adding corner case for FP32 to fixed point conversion.* Adding extra line.* Documentation fix.* Adding static inline.* Incorporating jackwish comment. Removed idtype from requantize lowering.* Removing Quantize/Dequantize code. Restricting Requantize to (u)int8/int32.* Style fixes.* Fix the docs.* Move to Legalize API.",0
"[CI] Update docker image ci_cpu,i386 to include verilator (#3738)",1
[VTA] [Chisel] Bug fix for VME Shell (#3737)* fix* fixes,0
Fix typo in ir_pass.h  (#3741),0
[Relay] [Training] Fix ad for concatenate (#3729)* reproduce error* fix* lint* lint,0
use pip3 for python3 (#3742)* use pip3 for python3* make python3 as default,5
"[Relay] Fix Partial Evaluator, Add stricter checking for CheckWellFormed (#3749)* aot* save* save* fix test* remove vta changes* lint",0
[TOPI] Update tophub according to the fix in schedule (opencl and rocm) (#3752),0
Improve graph tuner dealing with Tuple (#3649)* Improve graph tuner dealing with Tuple* Add test case* Move some data out of _base.py* Fix lint,0
add reviewer (#3755),1
Revert compile_cmd kwarg name change (#3746)* Revert compile_cmd kwarg name change* Fix binutil tests,0
Fix the potential index overflow (#3751),0
[Bugfix] tvm.scan follow by tvm.compute segfault (#3723)* [bugfix] tvm.scan follow by tvm.compute segfault* more strict bound condition check* access k + 1 -> k* fix scan test,0
"Don't replace reduction init axis with new axis if bound to a thread. (#3408)* Don't replace reduction init axis with new axis if bound to a thread.* Linter.* Reduce bind test case.* Guard test on CUDA support.* [CUDA TE TESTS] Add rfactor predicate test, add global bx and tx.* [CUDA TE TESTS] Add loop partition test for simple rfactor case.",1
[Relay] SpaceToDepth and MirrorPad Operators (#3718)* Added relay and topi mirror_pad operator.* Added mirror_padding to tensorflow frontend.* Added mirrorpad testing in tensorflow frontent.* Added space_to_depth in tf frontend.* Added tests for spacetodepth.* spacetodepth bug fix.* Lint fix* Added mirror pad python attrs.* Pad code formatting.* Syntax improvement* Hopefully last lint fix,0
"[ARITH] Simplify casts of constants 0 and 1 (#3758)* [ARITH] Simplify casts of constants 0 and 1* [EXPR] is_const_value to check whether non-ints are consts* Revert ""[EXPR] is_const_value to check whether non-ints are consts""This reverts commit 7e1b3462e3f74fd0afb1541d72978107cfa23c30.* Use tvm::cast",5
fix mistype (#3763),0
"[VTA] [Chisel] Improved Data Gen, Added ALU Test (#3743)* added alutest* fix indent* name change for cycle* improved data gen and infra* added alutest* fix indent* name change for cycle* improved data gen and infra* fix space* fix indent* fixes* aluRef* fix randomarary* add* Revert ""add""This reverts commit 87077daebbe055dee11f80e37da3a6291138e0f0.* Revert ""fix randomarary""This reverts commit df386c1e660eb6ebcff1a1f905610573676f1589.* Revert ""aluRef""This reverts commit 8665f0d4a7b12b796b2cb1ca6bf9cfe5613ee389.* should fix dlmc-core",0
[VTA][TSIM][Build] Towards TSIM CI testing (#3704)* building TSIM specific library along with fast simulator to quickly switch between dlls* cmake controlled TSIM libraries* always build tsim driver in either simulation modes* build DLLs based on CMAKE flags* updating the jenkinsfile* small restructuring* reducing the cmake flags* update instructions* reverting to 3 flags* update Jenkinsfile* adding new line* enabling TSIM unit and integration tests* fix description* temporarily disabling task_python_vta tests in CPU Build stage* move CPU tests in unit test stage* stage  reorg* better make* disabling TSIM tests for now* reverting some restructuring* fix,0
fix some pass docs (#3767),0
[VTA][Chisel] run all unittests by default (#3766)* [VTA][Chisel] run all unittests by default* better naming* add generated unittest folder to clean rule,1
syntax fix (#3765),0
[Relay][Frontend][TensorFlow] Support BatchMatMul with input dimensions larger than 3 (#3732)* Support BatchMatMul with shapes greater than length 3* Fixes* Add tests* Remove dependency on Python3* Clean up* Merge with master* Resolve comments,0
fix (#3769),0
fix dense tuning (#3768),0
[VTA][Chisel] scale dram base address in hardware instead of runtime (#3772)* [VTA][Chisel] scale dram base address in hardware instead of runtime* remove trailing spaces,1
[Relay][Legalize][ARM_CPU] Handling NHWC layout for arm_cpu. (#3754),5
[QNN] Concatenate operator (#3730),5
[QNN] InferType changes that missed CI. (#3779),4
[QUANTIZE] Refactor quantization codebase and fix model accuracy (#3543)* Refactor.* update* update* update* update* update* update,0
[Relay][Frontend][ONNX] Add Sign and Equal operators to ONNX frontend (#3760)* [Relay][Frontend][ONNX] Add Sign and Equal operators to ONNX frontend* Dummy change to retrigger integration test,1
QNN quantize and dequantize operators. (#3745)* QNN quantize and dequantize operators.* addressing review comments.* addressing review comments.* Adding new line at the end of the file.* Adhering to styling guidelines.* Adding name to contributors.* Fixing lint issue.* Fixing file name.* Removing unnecessary code.,0
[Relay][Quantization] Fix out-of-date realize (#3790),0
[BUGFIX] Fix for NoneType Target (#3792),0
Fix ArgBinder assert order (#3794),0
[Community] Hao Lu -> Committer (#3789),3
[VTA][TSIM] parallel TSIM hardware compilation with macOS and debug support (#3797)* [VTA][TSIM] parallel hardware compilation with macOS and debug support* simplify,0
"[TOPI, CUDA] Improve conv2d_transpose schedule template (#3796)",5
[CoreML] Solve CoreML frontend issue of image scaler and padding so that Mobilenet mlmodel can work correctly. (#3800),1
Fixed onnx test failures when run on a cpu backend (#3764)* Fixed onnx test failures when run on a cpu backend* Updated check_torch_conversion function to include output comparison,0
[Relay][Frontend][TFLite] transpose implementation for tflite.py (#3705)* transpose implementation for tflite.py* add TRANSPOSE to convert_map* Fix Unexpected keyword argument 'axis' in function call* add test for transpose oprator* Add the parameter 'axes' handling* add test for transpose oprator* solve conflict within CONTRIBUTORS.md* Improve the if condition for empty tuple* Add one unit test to cover empty tuple* solve conflict within CONTRIBUTORS.md,0
[CI] Solve occasional CI issue when pad value is all 0 (#3801),5
[CI] Temporary disable rust test (#3809),3
[dep] psutil (#3780),5
add gfx906 bc (#3808),1
[Relay][VM]VM Profiler (#3727)* [Relay][VM]VM debugger* Report mean/min/max for op duration* Typos* Lint* Lint* Lint* Support build debug VM in CMake* Lint* Enable VM debug in unit test* Disable debug vm test until new docker image is built* Add device sync code* Fix qnn unit test* Disable vm debug by default* Rename files* Rename classes* Fix comment* Fix comment,0
"[TOPI] Use cblas for dense and batch_matmul when ""cblas"" is in the target libraries (#3787)* Support cblas library in dense* start to add support for generic batch_matmul compute* Add x86 override for batch_matmul* Fix linting* reset file* Fix typos* dummy change to re-trigger CI",0
Changed topi cc resize to python implementation with new features. (#3788),1
"[TOPI][Relay][TensorFlow] Add OneHot operator (#3781)* Add one-hot to Relay* topi implementation* Working* add topi test* Add TF test* Fix check* fix linting issues* fix documentation* Fix documentation* Add support for on_value, off_value, axis, dtype* Add full support for axis* Fix compute and update test_forward* Move on_value and off_value to inputs* Add topi test* Update tests* Update docs* Fix style* re-enable tests* Add one_hot to mxnet converter",0
[TVM] Fix warnings (#3817)transform.h:118:3: warning: 'const' type qualifier on return type has noeffectattrs.h:68:3: note: expanded from macro 'TVM_DECLARE_ATTRS'node.h:244:3: note: expanded from macro 'TVM_DECLARE_NODE_TYPE_INFO'transform.h:95:3: warning: extra ';' after member function definitionattrs.h:68:62: note: expanded from macro 'TVM_DECLARE_ATTRS',0
[Relay] Fix typo in parser (#3785),0
[CODE] Halide attributions (#3824),5
[Legalize][QNN] Pass out_types to Legalize. Update QNN requantize to read from out_types. (#3782),1
Update hybrid_script.rst (#3799),1
Fixed repo change for llvm-9 to resolve missing dependency issue when building images with llvm enabled (#3826),0
Fix code comment of operators (#3830),0
[VTA][TSIM] Introduce Virtual Memory for TSIM Driver (#3686)* initial virtual memory;* initial integration;* include the header file in cmake;* implement allocation with virtual to logical address mapping;* virtual memory for tsim_driver;* implement the missing memory release function;* readability improvement;* readability improvement;* address review comments;* improved robustness in virtual memory allocation;* remove VTA_TSIM_USE_VIRTUAL_MEMORY macro and use virtual memory for tsim by default;* link tvm against vta library;* merge with master* build virtual memory system without linking tvm against vta;* minor change;* reuse VTA_PAGE_BYTES;* using DRAM class from sim_driver as VirtualMemoryManager;* satisfy linter;* add comments in code;* undo changes to Makefile* undo changes to Makefile* retrigger ci;* retrigger ci;* directly call into VirtualMemoryManager::Global(),1
"Fix inconsistent python/cpp API behavior for if_then_else, power (#3829)* fix inconsistent python/cpp APIs for if_then_else* fix error message* fix power consistency* fix* fix bug* add test",0
update docs for installation for CUDA (#3832),1
[VTA] Parameterization and bug fix in TensorLoad module (#3841),0
[Bugfix][Keras] axis of softmax (#3834),0
[Relay][Keras] Dot (#3668)* [Relay][Keras] Dot* fix reshape* fix comments,0
[AutoTVM] Fix database APIs (#3821)* [AutoTVM] Fix database APIs* Refactor the byte conversion,0
Support MKL on Windows (#3837),5
[TensorFlow] Fix limitation that depth_mult can only be 1 for DepthwiseConv2dNative (#3676)* [TensorFlow] Fix limitation that depth_mult can only be 1 for DepthwiseConv2dNative* Improve code readability,0
[runtime] reduce set_input and set_input_zero_copy overhead (#3805),5
[TEST] Not assuming HOME in tvm/download.py (#3803)* Not assuming HOME in tvm/download.py* Trigger notification,3
Improvements in conda recipe (#3791),5
Add build_create_shared_func to tvm/contrib/cc.py (#3840),1
[Relay] Conv2d grad (#3636)* [Relay] Conv2d grad* Fix test* Fix first order gradient,0
"[VTA] Fix RewriteForceSerial Function logic issue. (#3854)Issue:RewriteForceSerial is a debug function to force instructionsto be serialize instead of parrallel running, by doing so wecan isolate some parallel problem or do performance comparebetween parallel and serialize. But this function have someproblem, once get enabled by set debug flag, vta would stuckwhen running on pynq board.Analysis:once enable RewriteForceSerial, the dependency logic is differentwith default one, but we still use same logic to generate FINISHand other logic, this would cause dead lock.Solution:give a different dependency settings when enable RewriteForceSerial.",0
"[VTA] Infinite recursive device_api.ext_dev call fix. (#3843)Issuewhen try vta on fpga board, would see a Infinite recursivedevice_api.ext_dev issue that cause stack overflow and vtafailed.Analysis:device_api.ext_dev function in rpc_server.py is use to loadvta library, once vta library get load, device_api.ext_dev wouldget replaced with vta function by vta library, vta device_api.ccdid such work, but because a logic issue in VTA.cmake, the said filenot get compiled, then vta would keep failing on rpc_server.py.Solution:fix the logic issue in VTA.cmake.",0
codegen_spirv support Call::reinterpret (#3795),5
[Relay][QNN] QNNtoRelay & QNNLegalize Pass utility using Relay Legalize API. (#3838),4
"[Relay][QNN] Moving Conv, Dense, Concatenate InferTypes to header for sharing. (#3783)",5
[QNN] Concat - Refactoring to C++ (#3819),5
Add more cases to keras _convert_reshape (#3846),1
Improve numerical gradient check (#3856),5
[VTA][TSIM] add virtual memory support to tsim example (#3868)* [VTA][TSIM] add virtual memory support to tsim example* fix identation* remove USE_TSIM macro and use 32-bit addr instead,0
Add not operator for the frontend/onnx.py (#3836),1
"[Relay] Bitserial ops (#3844)* Added arm_cpu NHWC schedules.* Fixed kernel shape legalization.* Added bitserial ops to relay.* Snapshot and more missing files.* Added dense testing.* Added tests* Added ASF header to new files.* cc lint* Pylint change.* pylint fixes.* Change arm legalize test.* Added assert check to arm legalize.* Added better documentation, fixed some bad style* Reverted arm conv2d nhwc changes.",0
[NNVM][FRONTEND][ONNX] Fix PReLU conversion (#3813),0
[Relay][Any] Add shape func for dynamic shape (#3606)* init shape func in interpreter and vm compiler* Update interpreter* fix* lint* lint* fix* remove hack* update* fix* fix* update* address comments & update for shape_of* fix lint* update* fix hybrid* lint* fix bug & add take shape func* lint* lint* update* fix flaky test* add todo,0
Implementation of tile for TFLite (#3814),5
[QNN] Requantize - Optimize lowering for some corner cases. (#3864),5
[WIP][µTVM] Add OpenOCD Low-Level Device (RISC-V Support) (#3756),1
[Relay] [Parser] fix parser for cast. (#3873)* fix* lint,0
[VTA][Chisel] rename USE_TSIM macro with USE_VTA64 and cleanup runtime (#3872),5
Remove extern C warpper for cuBLAS (#3877),4
"[Runtime] Allow parameter sharing between modules (#3489)As GraphRuntime does not provide control-flow logics, we have to splitour model to two parts. While we need to share parameters between themto save memory usage.Solution:1) add ""lazy_init_input"" in graph's attributes   ""attrs"": {     ... ...     ""lazy_init_input"": [       ""list_str"",       [         ""p0""       ]     ]    }2) allow un-allocated NDArray entry in SetupStorage3) utilize ""set_input_zero_copy"" function to set parameters",1
ONNX frontend operator support: And (#3878),5
"Revert ""[Runtime] Allow parameter sharing between modules (#3489)"" (#3884)This reverts commit 224cc243b4e54a77d011644fe7d81bdee8e8116b.",5
[VTA] Fix TSIM compile error in Linux (add missing -fPIC flag) (#3876)* [VTA] Fix TSIM compile error in Linux (add missing -fPIC flag);* [VTA] Fix TSIM compile error in Linux (add missing -fPIC flag);* fix indentation problem;,0
[VTA][Chisel] add scalafmt and format existing scala codebase (#3880)* [VTA][Chisel] add scalafmt and format existing scala codebase* change column width to 100* add scalafmt conf file as a valid file type* add asf header to scalafmt conf file and rerun formatter,1
[Relay][Frontend][darknet] Solve tvm parsing darknet resnext failure bug (#3778)* test_darkent_bug* test_darkent* add resnext tests,0
[Relay] Add grads (#3857)* Add gradient implementations* Add docstrings to fix lint errors,0
[TENSORFLOW] Convert scalar Const into tvm.relay.const (#3885)* [TENSORFLOW] Convert scalar Const into tvm.relay.const* use _get_num_param() and _get_list_param(),5
[QNN] Convolution 2D Implementation. (#3580)Rebasing. Empty commit.Clang-format styling.,5
[VTA][Chisel] add ISA BitPat generation (#3891),1
Reveal hidden code snippets by inserting newline (#3892),1
[VTA] de10-nano driver (#3394)* rework;* `de10-nano` -> `de10nano`;* fix compilation error;* bug fix;* Update install.md* Update install.md* Update install.md* update with current runtime;* add debug messages;* bug fix in cma kernel module;,0
[QNN] Add - Refactoring to C++ (#3736),1
[Relay][Training] Small refactoring (#3893)* init* fix,0
"[VTA][Relay] Extending Vision model coverage compilation for VTA (#3740)* adding support for graphpack over multiply op* increasing resnet model coverage* fix indentation* lint* moving recursion limit fix into graphpack pass* moving recursionlimit to relay init* pooling on NCHWnc format* adding more models* deploy_resnet_on_vta.py* trailing line* generalizing to vision models* merge conflicts* fix, apply quantization to VTA only* improving comments* trimming models that have runtime issues for the moment* lint* lint* lint",0
"[VTA][TOPI] Conv2d transpose (deconvolution) operator support (#3777)* initial conv2d_transpose* correct select operator* cleanup* fix* fix correcness check* conv2d transpose declaration fix* autotvm conv2d_transpose tuning script* ir pass fix* fix tuning script* deriving params from env, adding bias* removing bias comp from deconvolution* lint* fix* lint* lint* turning off cpu* lint, ops* lint* import fix* removing hard coded values* lint",0
"[Test] enable NHWC of `relay.testing.mobilenet` (#3886)* [Relay] enable NHWC of `relay.testing.mobilenet`In this way, we can play around NHWC inside TVM regardless ofthe frontends.* [Test] test for NHWC of relay.testing.mobilenet",2
[DOC] Fix doc rendering  (#3897)* Update from_source.rst* Update deploy_ssd_gluoncv.py,0
[Relay] Fix operator fusion for multiple output (#3871)* save* add test* refactor* fix indent* save* refactor,0
Fix int32 range overflow by using int64 (#3870),0
[Relay] add Tuple pattern (#3596)* implement tuple pattern* add tuple pattern* lint;* lint* lint* fix error* fix* add test,0
[PYTHON/FFI] Search PATH for DLLs (#3888)* Search PATH for DLLs* Fix lint issue,0
[schedule] Improve ceil_divide in tile/split (#3842),5
Add another MKL name alias for MKL (#3853)Installed through pypi,1
[bugfix] remove duplicate resize (#3902),0
[Relay] Add ADTs to text format (#3863)* Getting closer to having ADT defs* ADT defs working probly* Match parsing basipally done* came to earth in a silver chrome UFO* match finished?* All tests but newest are passing* ADT constructors worknow cleanup?* Cleanup round 1* Cleanup round 2* Cleanup round 3* Cleanup round 4* Cleanup round 6* Cleanup round 7* Lil grammar fix* Remove ANTLR Java files* Lint roller* Lint roller* Address feedback* Test completeness in match test* Remove unused imports* Lint roller* Switch to Rust-style ADT syntax* Lil fix* Add dummy `extern type` handler* Add type arg to test* Update prelude semantic version* Repair test* Fix graph var handling in match* Revert 's/graph_equal/is_unifiable' change,0
[Relay][Op] Make Type Relation catch more errors (#3899)* save* init* move type_relations,0
save (#3901),5
[TOPI] Intel graphics conv2d autotvm template added (#3839)* update lint* lint fixed* lint updated* lint fixed* lint fixed* lint fixed* updates* add intel graphics as a package* remove print info* depthwise conv2d schedule added for intel graphics* asdf* fix lint* fix lint* fix ci* add channels,0
"[VTA] Support TLPP in function simulator. (#3555)* [VTA] Support TLPP in function simulator.Issue:currently vta function simulator just doing serialized instructionexecution, the dependency logic of runtime ISA which use for tasklevel pipe line parallelism can not get verified by function simulator.Solution:make the simulator driver to be multiple thread and support TLPP.Benefit:TLPP support VTA function simulator would make VTA logic testing/debug/change more easy.replace boost lockfree queueadd configure control for simulator tlpp enable or disable.change code tyle into google style.Wrap queue read/write and sync logic to make function call more simple.Add some comments.Remove MT logic, change into Single thread mode.address review comments.code style change to match google code style and add comments.add cmake macro to enable/disable simulator tlpp logic.submodule update.correct file name mentioned in comments.* remove USE_VTA_FSIM_TLPP.",0
add luis as reviewer (#3909),1
Add .hsaco save/load for ROCm target (#3852)fix lld,0
Fix a typo (#3913),0
Support LLVM trunk (#3907)* support LLVM trunk* guard with USE_LLVM in if condition for c++14* GREATER_EQUAL -> GREATER,5
[Fix] Fix blas cmake for mac os (#3898)* fix cmake for mac os* rename,0
change docker install script (#3524),2
[Relay][Training] Add gradient for cast (#3894)savefixfix grad,0
[Relay/TOPI][Op] Add erf intrinsic and op (#3702)* add more ops* stop vectorization for erf* x* cleanup* fix* add whitelist for vectorizable intrin* add tf converter* fix dense* fix* add missing intrin* fix mxnet frontend* fix nvptx,0
Numpy compatible dtype inference for `tvm.convert` and `tvm.const` (#3861)* numpy compatible type inference* update* try to fix* fix* try to fix* fix lint* Update nn.h* cast to int32* try to fix* fix again* retrigger ci,0
[VTA][Config] hotfix denano10 (#3918),0
[Relay][Training] Add gradient for max. (#3915)* save* save,1
[CODEGEN] Remove incorrect check for LLVM in C codegen test (#3921),3
[Relay][Frontend][Keras] Fix ReLU in Keras Converter missed the case (#3917)* [Relay][Frontend][Keras] Fix ReLU in Keras Converter missed the case* [Relay][Frontend][Keras] Add test case for ReLU in Keras Converter missed the case* [Relay][Frontend][Keras] Add test case for ReLU in Keras Converter missed the case,0
[Relay] fix exponential blowup in interpreter (#3559),0
[TFLite] Support depthwise convolution multiplier greater than 1 (#3922),5
[Arm] parallel batch axis (#3931)* support LLVM trunk* guard with USE_LLVM in if condition for c++14* GREATER_EQUAL -> GREATER* [Arm] parallel batch axis,5
[Community] Add reviewer Balint Cristian (#3935),1
[Relay][Module] Refactor the way we interface between different modules of Relay. (#3906)* Module refactor* Add load module* Add support for idempotent import* Tweak load paths* Move path around* Expose C++ import functions in Python* Fix import* Add doc string* Fix* Fix lint* Fix lint* Fix test failure* Add type solver* Fix lint,0
"[RFC] [Contrib] Minimal runtime (~12kb .text on ARMv7/x86) for subset of TVM models (#3567)This is an alternative implementation of a subset of the TVM runtime API (andgraph runtime) that focuses entirely on reducing code size, at the expense offunctionality (no tvm.extern(..) calls via PackedFunc, CPU only, etc). It mightbe worth incrementally expanding the surface area if there's interest.The motivation for this work was seeing what the minimal useful subset of theTVM runtime is. This is relevant for e.g. super code-size constrainedapplications in e.g. embedded/mobile. The current runtime is more like O(100KiB)or so, so this might be compelling for some users.The smaller surface area for auditing might make this relevant forhttps://github.com/dmlc/tvm/issues/3159, or the usecases I was thinking about inhttps://github.com/dmlc/tvm/issues/2523#issuecomment-459165815 re: the Rustruntime.The symbols in the tvm::minimalruntime space (i.e. excluding std:: andpicojson::) are about 5KiB, so I think there's a bunch of room here (i.e. wecould replace picojson:: with [`jsmn`](https://zserge.com/jsmn.html) orsomething, and we could replace more of the `std::unordered_map` usage, etc withcustom primitives as well (similar to the `DynArray`).",5
[TOPI][CUDA] Support cuBLAS BatchMatMul (#3936)* Support cuBLAS BatchMatMul* Add test and check target name,1
Do type checking for the input and kernel in the qnn conv2d (#3904)* [QNN] Convolution 2D Implementation.Rebasing. Empty commit.Clang-format styling.* Reformatting code.* Fixing lint issues.,0
Fix CUDA int8x4 vectorize (#3928)* Fix int8x4 vectorize* Fix gpu shared/local memory accumulate* Add test_shared_memory for int8x4* Adjust test format* Fix cpplint,0
Refactoring x86 conv2d_NCHWc (#3944),5
Add AVX512VNNI support for TVM (#3388),1
"[VTA] RPC path update. (#3924)Issue:RPC path get changed into ""vta_rpc"" from ""pynq_rpc"", but relateddocument still use old informaiton.Solution:Update RPC path information.",1
Vulkan2 Runtime API (#3849),5
1) Add EQ op to the deduce_bound and add unittests for the same (#3775)2) Add EQ support in the loop partition and add test for the same3) Change typo truc to trunc,1
trivial (#3954),5
"[AutoTVM] Enhance tuning space of split (#3949)* Refine policies for define_split- Rename policy ""all"" to ""factors""- Add policy ""verbose"" and ""power2""* Refine search space* add doc",1
[Relay][TensorFlow] Add support for SquaredDifference (#3930)* Add support for SquaredDifference and StopGradient; minor fix in BatchMatMul* Remove stopgradient change* Resolve PR comment* Dummy change to retrigger CI* dummy change to retrigger CI,0
Enable miopen transpose convolution and fp16 support (#3952)* Enable miopen transpose convolution and fp16 support* linter,5
[QNN] Legalization for Intel x86 QNN Conv2D (#3896)* QNNLegalize for conv2d* [QNN] Legalization for Intel x86 QNN Conv2D,5
"[TOPI] operator support: logical_and, logical_or, logical_not (#3929)* [TOPI] operator support: logical_and, logical_or, logical_not* [TOPI] operator support: logical_and, logical_or, logical_not* [TOPI] fix test cases for operator support: logical_and, logical_or, logical_not* [TOPI] fix test cases for operator support: logical_not",0
[tvm][codegen] Make buffer auto broadcast independent to the order of input args (#3956)* [tvm][codegen] Make buffer auto broadcast independent to the order of the input arg* fix indent,0
[Graph Tuner] Fix benchmark layout in graph tuner (#3926)* Fix graph tuner benchmarking layout transform* Add test,0
[TOPI] Improve conv2d_transpose schedule on X86 and CUDA (#3948)* improve conv2d_transpose x86 performance by reusing conv2d schedule* parallelize across batches to make large-batch conv2d and conv2d_transpose faster* improve doc for autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log* add fallback schedule for schedule_conv2d_transpose_nchw_cuda* fix pylint* fix pylint* unify conv2d_transpose declaration in topi.nn and topi.x86,0
[TOPI] Setting up AutoTVM template for Intel Int8 conv2D (#3955),5
More friendly error msg; Fix Android Demo LLVM ver (#3962),0
[Vulkan] Minor optimization for deferred token lookups. (#3960)Use a hash map keyed on the descriptor set to avoid bad asymptotic behaviour.,5
Adding support to check if an attribute is present or not without having to get the value (#3957)* Adding support to check if an attribute is present or not without having to get the value.* - Renaming the method to more appropriate name.,1
[Relay] Keras frontend upsample and 1 channel conv2d fixes (#3937)* Fix upsample layout in keras frontend.* Fixed group conv being used instead of conv when channels=1* Add new conv2d test to catch bugs when channels=1.,0
[TVM][AutoTVM] cast filepath arguments to string (#3968),2
[Relay] Add shape check for ConcatenateRel and StackRel (#3699)* [Relay] add shape check for concat* [Relay] add shape check for stack* add test case for shape mismatch* [typo] add the missing assert* fix lint errors.* replace int with size_t.* statically cast param->axis to size_t.* switch to run_infer_type.* fix checking for negative index* add static_cast for param->axis* merge to latest tvm* fix lint error* Fix an error with negative index.* Update transform.h* Update transform.cc,0
[ARITH] Introduce base-class IRMutatorWithAnalyzer for scope dependent analysis (#3969),5
[Relay] Legalize and AlterOpLayout for Int8 Intel. (#3961),5
adjust pylint output (#3973)adjust pylint output to show file location to make it possible to locate errors,0
Remove GTest cmake flag from install docs (#3953),2
[TOPI] Add proper scheduling for dense on CUDA (#3923)* add proper scheduling for dense on CUDA* add fallback config and fix unit test* fix corner cases* refactoring* fix bias and add testcase* let fusion happen,0
[QNN] Renaming tests to follow the Relay nomenclature. (#3975),3
"Add support for MXNet pad operator. (#3739)MXNet pad is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.padAdd support for parameter 'None' in MXNet slice operator.MXNet 'slice' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.sliceAdd support for MXNet cos, sin, arctanMXNet 'cos' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.cosMXNet 'sin' is described athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.sinMXNet arctan is descirbed athttps://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.arctanAdd support for MXNet 1D Convolution and 1D DeconvolutionMXNet convolution is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.ConvolutionMXNet Deconvolution is described at:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Deconvolution",1
[ARITH] Add Lowering rule for FloorDiv/Mod (#3976)* [ARITH] Add Lowering rule for FloorDiv/Mod* add comment about constant folding,1
[Relay][Frontend][ONNX] operator support: Tile (#3941)* [Relay][Frontend][ONNX] operator support: Tile* Trigger notification,5
"[Relay][Frontend][TFLite] frontend operator support: batch_to_space_nd, space_to_batch_nd (#3850)* Fix unittest* Fix pylint error: Line 915 too long* Fix the conflicting files* frontend operator support: space_to_batch_nd* add test case for frontend operator support: space_to_batch_nd* add test case for frontend operator support: space_to_batch_nd* frontend operator support: space_to_batch_nd* Fix ValueError: don't know how to convert type <class 'numpy.ndarray'> to node",0
add bc for gfx1010 (#3984),1
Enable miopen Group Convolution (#3987)* enable group conv through miopen* linter fix,0
Add docs for analysis namespace (#3985),1
Add operator `isnan` (#3979)* add expr `isnan`* move to intrinsic* doc & add to topi* fix error from ci,0
Qnn fully connected (#3910)* Qnn Dense layer.* Reformatting code.* Reformatting code and making the test case more readable.* Fixing lint issues.* Fixing test method names to pass the nose related configurations.* Aligning the code for code style.,0
"[Rust] Fixes ""common"" sub crate using nightly and master (#3965)",0
[QNN] Fix padding changes due to #3739 (#3989),0
[DOC] Add test script starter command to document (#3993),1
[Relay][Frontend][ONNX] Add Erf to ONNX frontend (#3988)* Add Erf to ONNX frontend* dummy change to retrigger CI,1
Add type solver unit tests for unifying quantified funcs (one bug found) (#3947),0
[Relay] Add new IR pass CombineParallelDense (#3862)* Refactor to create abstract ParallelOpCombiner* First draft of CombineParallelDense* Begin to work on tests* Test* Refactor to move out more common code* Clean up* Fix* Remove statics* fix wording* Start to add combine_parallel_op_batch* Resolve PR comments* Resolve PR comments* dummy change to retrigger CI* Change special case from bias_add to add* Revert special case change* Ignore units check* dummy change to retrigger CI* dummy change to re-trigger CI* Improve docs* Update docs* Update docs,0
add parser support for TANH tflite operator (#3996),1
[ARITH] Explicitly state truncdiv/mod in pattern matching. (#3986)* [ARITH] Explicitly state truncdiv/mod in pattern matching.* Fix the dependent cpp test,0
"Changes to make tensorize work. These changes also fix the previously broken test. (#3981)* Changes to make tensorize work. These changes also fix the previouslybroken test.Summary:Tensorize was breaking  for a few reasons.1)Assert at: src/op/tensorize.cc:234 CHECK(is_one(e.region[j]->extent))In some cases this cannot be proven, e.g.:expected shape=[16, 4], given region=[range(min=((ax1.outer*16)/16), ext=(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer)), range(min=((k.outer*4)/4), ext=(((((k.outer*4) + 3)/4) + 1) - k.outer)), range(min=0, ext=16), range(min=0, ext=4)]The unprovable one is: ext=(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer)).This can be simplified but it is not because to simplify divide, it mustprove ax1.outer > 0 and since it is var it cannot. The fix for this tojust find all the vars in expr in relace them with some const value.2) Equivalence between tensorized expr and one being asked to tensorize. For example,the error would be.TVMError: Check failed: Equal(lhs, rhs):Failed to match the compute with TensorIntrin tensor_intrin's declarationprovided= reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[(int16)0]), source=[(int16(data(k))*int16(kernel(((((((((k.outer.outer*64) + (k.outer.inner*2)) + k)/2)*128) + i) - (k.outer.inner*128)) - (k.outer.outer*4096)), ((((k.outer.outer*64) + (k.outer.inner*2)) + k) % 2))))], axis=[iter_var(k, range(min=0, ext=2))], where=(bool)1, value_index=0),intrin=  reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[(int16)0]), source=[(int16(data(k))*int16(kernel(i, k)))], axis=[iter_var(k, range(min=0, ext=2))], where=(bool)1, value_index=0)Difference is mainly in the source part:source=[(int16(data(k))*int16(kernel(((((((((k.outer.outer*64) + (k.outer.inner*2)) + k)/2)*128) + i) - (k.outer.inner*128)) - (k.outer.outer*4096)), ((((k.outer.outer*64) + (k.outer.inner*2)) + k) % 2))))]source=[(int16(data(k))*int16(kernel(i, k)))], axis=[iter_var(k, range(min=0, ext=2))]This was not being simpifiled due to compute_intrin_iter_space (map foriter var to range) not containing leaf iter vars.3) Here it fails with:Check failed: is_one(Simplify(value->shape[i])): Argument b_buffer shape mismatch[16, 4] vs [(((((ax1.outer*16) + 15)/16) + 1) - ax1.outer), (((((k.outer*4) + 3)/4) + 1) - k.outer), 16, 4]This is in buffer binding where it thinks expected and buffer boundshape is different. Although if we could simplify expr, this would notbe the case.Test Plan:On skylake avx512 machine:python tests/python/contrib/test_gemm_acc16.pyReviewers:Subscribers:Tasks:Tags:* Implemented bounded analyzer which traverses tree and for reduce/forstatements binds the bound of the analyzer. Later this is used tosimplify expressions. Inspired from ir_mutator_with_analyzerSummary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Addressed comments.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Added ASF header + define macro for the header file: TVM_ARITHMETIC_IR_VISITOR_WITH_ANALYZER_H_Some lint fixes as well.* Relax the assumption that dom_map must always contain all leaf itervars.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Disable copy constructor and move to raw ptr.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:",0
add parser support for GREATER tflite operator (#3963)add test for GREATER,1
[COMMUNITY] @yongwww-> reviewer (#3997),3
Added tesnorizeation for avx2 based gemm. (#3982)* Added tesnorizeation for avx2 based gemm.Summary:Tensorized the same region as avx512. Names produce 16x1 int32 results.Does by doing two sets of AVX2 instructions to do reduction on 8x4 int8kernel with 1x4 data.Test Plan:on avx2 machine:python tests/python/contrib/test_gemm_avx2_acc32.pyReviewers:Subscribers:Tasks:Tags:* Fix lint errors. Removed commented out code.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:,0
Change Vivado install instructions to version 2018.3 (#4003),2
Expose llvm.nearbyint intrinsic. This is a faster alternate to rounding. (#4001)* Expose llvm.nearbyint intrinsic. This is a faster alternate to rounding.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:* Added python binding. Added test.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:,1
[ARITH] Refactor to use explicit div/mod functions instead of operators. (#4000)* [ARITH] Use explicit div/mod functions instead of operators.* fix pooling case,0
remove FLOP computation for 3rd party lib call (#4005),4
"Revert ""Added tesnorizeation for avx2 based gemm. (#3982)"" (#4007)This reverts commit 23727eb49ea71609fc29963b996a68a14fddf79c.",1
"[TOPI] Move conv2d spatial pack schedule to dedicated file (#3972)More schedules are making the conv2d.py file too large, sowe'd like to move the spatial pack schedule to dedicated filebefore introducing NHWC schedule. No logic change in this patch.",2
[RELAY]impose a max op limit to the op fusion pass (#4002)* impose a max op limit to op fusion* use cross platform data type,4
[TOPI][x86] Introduce schedule_injective_from_existing and unify external schedules for all targets (#3983)* Fix extern schedule for x86* Register x86::schedule_extern* Fix* Fix* Replace extern.py with extern.h* Introduce new generic function schedule_injective_from_existing* Fix* Fix* Add back to C++* Fix style* Injective schedule calls local schedule_injective_from_existing* Fix* Remove target arg from schedule_injective_from_existing* Fix docs* Try to fix unit test* Fix test* Fix other tests* Fix bug,0
[QNN][Conv2D] Optimize lowering. (#4006),5
hide psutil (#4013),5
"Exposed lowered func to c++ API. (#4012)So that you can use: `build_mod_.GetFunction(""get_lowered_funcs"", false);`to get lowered_funcs.Summary:Test Plan:Reviewers:Subscribers:Tasks:Tags:",3
[ARITH] Use explicit div mode in python. (#4014),5
[Fix]use a more intuitive way to limit the #ops in a group (#4018)* use a more intuitive way to limit the #ops in a group* format,0
[DOCKER] make demo images consistent with ci images when possible. (#4024),5
[Rust] Fix issue with CPP enums. (#4019),0
docs: minor spelling tweaks (#4027),2
Additional MXNet Convolution and Deconvolution tests (#4026)Add different batch sizes and channel numbers toMXNet Convolution and Deconvolution tests.,1
Add parser support for ReLU tflite operator (#4022),1
[Fix] Add more pad_mode support for onnx converter (#4029)* [Fix] Add more pad_mode support for onnx converter* robustness fix,0
[ARITH] cleanup the indexmod/div on python side (#4028),5
[AUTOTVM][DOCS] Add a link to the defining network description of auto-tuning tutorial (#4023)* [AUTOTVM][DOCS] Add a link to autoTVM tutorial to direct the details of building NN with relay* [AUTOTVM][DOCS] Add a link to autoTVM tutorial to direct the details of building NN with relay,1
make tvm compilable by gcc 4.9.2 (#4032)please see https://stackoverflow.com/a/26949099,5
"[Relay] Move prelude to text format (#3939)* Fix parser* Doc fix* Add module utility functions necessary for prelude* Implement prelude in text format* Remove programmatically constructed prelude defs* Fix 0-arity type conses in pretty printer and test* Make prelude loading backwards-compatible* Fix patterns* Improve some prelude defs* Fix `ImportFromStd`It needs to also follow the ""add unchecked, add checked"" pattern* Lint roller* Woops* Address feedback* Fix `test_list_constructor` VM test* Fix `test_adt.py` failures",0
[ARITH] migrate indexdiv/mod to floordiv/mod (#4008),5
Add dmlc-core to the list of installed header directories. (#4035)There are dependencies on dmlc-core in TVM public API headers(e.g. some headers include dmlc/logging.h) so it needs to be installedas part of TVM for TVM headers to be actually usable.,1
[Relay][Compile_engine] Int64 shape handling for outputs. (#4031),5
[QNN] Renaming dense operator. (#4033),5
[COMMUNITY] anijain2305 -> reviewer (#4036),3
"[topi] add ARM v8.2 udot (uint8) support (#3978)* [topi] add ARM v8.2 udot (uint8) support* fix test case* fix common conv2d schedule* add back fp32_time in test* fix lint* fix doc, add support for int32_lanes=4, signed int* fix lint* add ic_bn % 4 checker in schedule",0
[TOPI]Add op argwhere (#3994)* Add op argwhere* Move shape func to _algorithm.py* Add lint rule* Raise exception if rank is not supportted* move argwhere to transform* Add argwhere example* Fix lint* Add 1-d support* cleanup* Add more dtype support* CR comment* Improve error message* Docs* raise exception,0
[COMMUNITY] ajtulloch -> committer (#4043),3
Fix split's last factor issue (#4044),0
[TF][Op] Op where (#4045)* [TF][Op] Add TF op Where* improve tests* add tests for vm,1
[RELAY/PASS] Fix the extent for the post_stmt in the loop partition (#3734),0
[QNN][Relay] Calling Dialect passes from inside Relay Build API. (#3971),2
[Relay][Op] Add instance norm op (#4004)* [Relay][Op] Add instance norm op* mend[Relay][Op] Add instance norm op,1
[Relay][TopHub] Add switch to disable TopHub download (#4015),1
[llvm] switch to use Align for llvm trunk (#4051),5
[Relay][Training] Add gradient for Crossentropy (#3925)* savesaveredo max testsaveaddress commentfix* address comment* increase rtol* address review comment,0
[Bugfix] Fix target host for vm compiler (#4057)* fix* tweak,0
[Relay][VM] Add autotvm context when compile (#4062),1
[Relay][VM] Add more passes to VMCompiler (#4058)* [Relay][VM] Add more passes to VMCompiler* Check build config* Add todo,1
[Bugfix][TF] reset graph after getting tag of savedmodel (#4055)@zhiics @icemelon9,0
"Add parses support for zeros_like tflite operator (#4042)The tensorflow zeros_like operation provided in array_ops.py produces directly a tensor with zeroswithout a graph, using only the shape and type of the input. This imposes the use of gen_array_ops.pythat produces both a tensor and a graph so a comparison between tflite and tvm can be done.",1
[Relay][AlterOp] Improving support for broadcast layout alteration. (#4040),5
[Relay][AlterOp] Minor refactor. (#4064),5
dicrease the complexity of CalcDep from exponential to linear (#4053),5
[DOC] Fix typos in tutorials (#4066)fix some typos,0
Add gradient for log-softmax (#4069),1
"Hide symbols from dependent libraries if HIDE_PRIVATE_SYMBOLS is ON. (#4041)In current implementation HIDE_PRIVATE_SYMBOLS hides symbols from TVMitself but not from its dependent libraries. This is problematic whenother third-party libraries with the same symbols are linked to thesame executable.One example is using TVM with Mesa OpenCL drivers: they depend on LLVMand load its shared libraries with RTLD_GLOBAL flag, which results inconflicts with LLVM symbols that TVM uses. Arguably this particularissue belongs to Mesa (here's their tracking bug:https://gitlab.freedesktop.org/mesa/mesa/issues/236) but in generalthat's the right thing to do regardless of this particular bug.Note that I'm not enabling this functionality for Darwin as in myearlier tests their linker didn't seem to understand ""--exclude-libs""(but I don't have test platform ATM to double-check).",0
Fix match case in Python-side expr functor (#4037),0
[QNN] Refactor fixed point multiplication in requantize (#4073),0
[Fix][VM] Fix VM invoke with set_params (#4079)* Fix VM invoke with set_params* add test* tweak,0
[AlterOpLayout][x86] NHWC to NCHWc conv support. (#4080),5
[CodeGen] Disable -mfloat-abi hard option for LLVM < 6.0 (#4071)The -mfloat-abi hard option does not work for LLVM < 6.0 as it is ignored.This adds a fatal error when using unsupported LLVM versions so that the failureis not silent.,0
"[VTA] hotfix for de10-nano driver (#4081)Issue:git clone latest TVM/VTA and run VTA on xilinx FPGA board, applicationcrashed due to the ""call stack overflow"" which caused by a infinite recursivefunction call. this issue ever happen before and get addressed by PR 3843.Analysis:seems like de10-nano driver PR  used old code base then the  logic changeof 3843 get eliminated.Solution:add the logic back.",0
Fix wrong n_trial number in autotvm tutorials' progress bar (#4070)if n_trial is larger then config space.,0
[ARITH] Add floordiv for the deduce bound (#4025)Use fdiv in the tests for the deduce_bound,1
[topi] enable fp16 sort for arm (#4084),5
[TOPI][X86] Pool operator parallel support. (#4090),5
[relay] Small refactor for context (#4091),5
"[TVM] Rewrite simplification rule to eliminate unnecessary conditionals. (#4076)The current bounds checking infrastructure inserts checks like:```for (i, 0, bounds[n]) {  if (likely(i < bounds[n]) {     ...  }}```into the TVM IR which is currently not removed by simplification infrastructure.This is a little unclean, as these are trivially true since for a loop var `i`with a given min and extent, we are guaranteed that `i >= min` and `i < min +extent`. Thus, we can insert these checks into the IR and use them to eliminatetrivial bounds checks early on.",2
[TOPI] Add valid auto tvm for Intel Graphics (#4078)* add valid autotune* fix pylint,0
[Relay][VM] Fix constant folding issue in VM compiler (#4077)* [Relay][VM] Fix constant folding issue in VM compiler1. allow pass params when compile a module2. enhance profiler robustness* remove dead code* fix lint* add get_params* fix test* don't pass params back* remove get_params* docs* move compile function to api* compile clashes with builtin name* fix compilation error* remove dead code,0
"[DOCKER] torch install depends on future package (#4098)The torch package depends on the future package but the torch wheeldoes not expose that dependency resulting in an inconsitent install.Ideally the wheel should declare all of its dependencies, I'm not surewhy the packagers have choosen not to do this, for now the simple workaround is to explicitly install the future package.Change-Id: Ic9f0f4bb4c78ab65706fc1b20c1b4fd287856a9e",2
Fixing tensor not found issue in bitserial operator (#4095),0
[Fix] Fix the logic of the number of nodes checking in op fusion (#4074)* move the number of nodes constraint in op fusion up to the dom tree level* add test case of limiting the max number of ops to be fused* uncomment other test cases,0
- Adding support for Mxnet flavored dequantization for both default and using MKLDNN. User can choose between the two at runtime. (#3945)- Added tests for new methods added.,1
correct error (#4093),0
Add a python tutorial of deploying tvm module with tvm runtime only (#4094),1
"[VTA][TSIM] Serial GEMM Application Added (#4082)* app init push* fix on readme* change name, add bit serial explanantion* rm serialLoadMM, change doc* syntax change for readme* add parallel test functionality* fix readme* add python doc* syntax",0
"[TOPI] FIFO buffer op, to accelerate sequence modeling with dilated convolutions (#4039)* Add FIFO buffer op to enable explicit computation re-use in convolution* Add a test* Add end-to-end test with 1D convolution* Add a stub in MXNet frontend* Address reviewer comments* Add back stub for MXNet frontend",1
"[Relay][AlterOp] NHWC to NCHWc support for Pool, pad, concatenate, sum. (#4059)",5
Tutorial: update Building a Graph Convolutional Network tutorial (#4060)* update build_gcn.py tutorialupdates* support bias in GCN layer* download pretrained gcn model* verify model accuracy* use time_evaluator to measure runtime* fix adding bias in gcn layer* remove printing output* fix small bug* add DGL-PyTorch comparison into the build_gcn tutorial* add accuracy testing* adjust import order* handle different dgl versions* update number for dgl version checking,0
force code object v2 for amd gpu backend (#4099),5
[tvm][any] broadcast with values other than one (#3967)* [tvm][any] broadcast with values other than 1* Add test for incompatible runtime values* Remove hybrid script compact buffer binding* retrigger ci,1
[Fix] Fix a few bugs when dtype is fp16 (#4088)* Fix layer norm for fp16* [Fix] Fix arange for fp16* [Fix] Fix mxnet frontend for fp16* [Fix] Fix arange for fp16* remove comments* x* fix nnvm,0
[codegen] Add multiple operands and function support when using fp16 compilation (#4056)* overload half operators for cuda codegen* add float16 te test_op_level1* fix test_op_level1.py* fix lint* disable fp16 test if gpu does not support* disable fp16 test if gpu does not support* bypass float16 test if gpu does not support float16,0
adding soiferj to the list of reviewers (#4108),1
Add parser support for CAST tflite operator (#4096)This implementation provides cast to limited number of dtypesthat tflite currently supports for placeholder op. Add INT64 in thepossible dtypes as it appears to be supported accrording to tlfite schema.,1
add dependency of compilation with LLVM (#4117),1
[CI] Update ci-cpu to latest (#4121),1
[QNN][TFLite] Parsing TFLite quantized models. (#3900),5
Revert ci-cpu due to nnpack issue (#4124),5
Update task_cpp_unittest.sh,1
[Relay][Topi] Disable conv NHWC pack int8. (#4038),5
"[RFC][RUNTIME] Introduce new object protocol. (#4115)* [RUNTIME] Introduce new object protocol.This PR introduces a new object protocol to unify the node and object.We also updated the existing runtime::vm code to make use of the new system.Update to the node will be done in a follow up PR.Other changes:- Remove object related code in json serializer as that code logic was not complete  and we have a separate serializer for VM, can revisit later.* address review  comment* Fix the child slot logic",0
"[ARITH] Fix lowering of floormod(x, y) != 0 (#4127)",0
[Relay][AlterOpLayout] NHWC to NCHWc pad operator. (#4103)* [Relay][AlterOpLayout] NHWC to NCHWc pad operator.* Fixing culprit.* Flaky test 1.* Flaky test 2.,0
Fix infer type of kernel in dense. (#4125)* Fix infer type of kernel in dense.* - Moving the check of weight being nullptr up as it is needed in both the branches now.- Adding test case for validating that data dtype and kernel dtypes can be different.* - Fix the dtype check for weight. If the weight is not present then we will use the data dtype.,0
[QNN] Change default rouning to UPWARD. (#4131),4
[Relay][Training] Add and fix gradients (#4126)* add and fix gradients* fix linter issues,0
Adding support for dequantizing from int32 to float32. (#4130),1
Update PULL_REQUEST_TEMPLATE.md,1
"[RUNTIME] Refactor object python FFI to new protocol. (#4128)* [RUNTIME] Refactor object python FFI to new protocol.This is a pre-req to bring the Node system under object protocol.Most of the code reflects the current code in the Node system.- Use new instead of init so subclass can define their own constructors- Allow register via name, besides type idnex- Introduce necessary runtime C API functions- Refactored Tensor and Datatype to directly use constructor.* address review comments",1
[Relay] Improve build error when no lowered funcs are produced (#4132)* Improve build error when no lowered funcs* Switch from fatal to warning,0
[TOPI][x86] Cascade lake support. (#4123)* [TOPI][x86] Cascade lake support.* Jenkins test debug 1.* Testing cascade lake alone.,0
"[DOCKER] Pin torchvision==0.4.1 (#4140)The existing sequence of pip install commands fetches and installstorch==1.0.1.post2 then fetches an unpinned version of torchvision,recent torchvision packages hardwire the specific torch version theydepend on, the overall effect is that we install a pinned torchversion then replace it with whatever version the torchvision packagedepends on.The most recent torchvision==0.4.1 package results in some test casefailures.This patch pins torchvision back to 0.4.0, the most recent versionthat the test suite worked.  Removing the explicit torch installbecause it is implied and pinned as dependency of torchvision.Change-Id: Ib30bf6aed79ff130ea15ef5134fefb0508790574",2
"[PATCH] Fix undefined __floatdihf in libtvmruntime.so on aarch64. (#4119)Arm architecture provides optional FP16 floating point support in two alternative formats, IEEE and an an alternative Arm format.The ACLE (Arm C Language Extension) defined preprocessor symbol __ARM_FP16_FORMAT_IEEE can be used to distinguish between implementations providing IEEE and the Arm alternative format, but cannot, on its own, be used to determined if FP16 HW support is actually present.Testing this preprocessor symbol can lead to undefined __floatdihf at runtime on an aarch64 target where no FP16 HW is present.The relevant preprocessor symbol to determine whether FP16 HW support is present in the target is __ARM_FEATURE_FP16_SCALAR_ARITHMETIC, this symbol implies  __ARM_FP16_FORMAT_IEEE.The relevant preprocessor symbols are defined by the ACLE standard, section 5.5.21 16-bit floating-point data processing operations, https://static.docs.arm.com/101028/0008/Q2-ACLE_2019Q2_release-0008.pdf",0
[relay][vm] Separate VM runtime with executable (#4100)* [relay][vm] Separate VM runtime with executable* Address comments* move ctx back to vm* make only vm related fields and methods protected* integrate seriliaztion/deserialization to executable* create stream,1
[Relay][Frontend][TF] Add tensor array ops (#3798)* [Relay][Frontend][TF] Add tensor array ops* rename* delete test* Move utility function* Refactor* fix tensor array ops* fix test* fix rebase* Fix serializer bug* Improve tf convert name lookup to use prelude api* Fix lint* Fix test,0
Fix typo (#4144),0
[CI] Pin NNPack pthreadtools version (#4152),1
[QNN][TFLite] Parsing QNN Add op. Adding MobilenetV2. (#4142),1
Add lift_if_then_else pass (#3865)* Add LiftIfThenElse pass* Add more comments* Rename and refactor* Add description for internal data structure* Rename a test* Minor change* Address comments* Improve update_for,1
[CI] Update cpu docker (#4153),1
[Refactor] Rename Datatype to ADT (#4156)We think it will reduce the confusion with the meaning.https://discuss.tvm.ai/t/discuss-consider-rename-vm-datatype/4339,5
[Runtime] Enable option to use OpenMP thread pool (#4089),5
"[REFACTOR][NODE][RUNTIME] Move Node to the new Object protocol. (#4161)* [REFACTOR][NODE][RUNTIME] Move Node to the new Object protocol.This PR removes the original node system, and make node as a subclass of Object.This is a major refactor towards a better unified runtime object system.List of changes in the refactor:- We now hide data_ field, use Downcast explicitly to get a sub-class object.- Removed the node system FFI in python.- Removed the node C API, instead use PackedFunc for list and get attrs.- Change relay::Op::set_attr_type_key(attr_key_name) to relay::Op::set_attr_type<AttrType>().  - This change was necessary because of the new Object registration mechanism.  - Subsequent changes to the op registrations  - The change revealed a few previous problems that is now fixed.- Patched up a few missing node type registration.  - Now we will raise an error if we register object that is not registered.- The original node.h and container.h are kept in the same location.- Calling convention: kObjectHandle now equals the old kNodeHandle, kNodeHandle is removed.- IRFunctor now dispatches on ObjectRef.- Update to the new type checking API: is_type, derived_from are replaced by IsInstance.- Removed .hash member function, instead use C++ convention hasher functors.* Address review comments",0
[CI] Move golang tests to the end (#4164),3
Add support for quantized multiply to Relay (#4141)This patch adds multiply operator for quantized tensors.The details of the quantized multiplication are outlinedin the code.This builds on pull request 3927 and includes the changesAnimesh mentions in the comments on that request.Change-Id: I555715b53d0266a91d5c03dc3dfe8fc31e7ce4e1,1
"Fix missspelling (#4166)FIX ""After connecting he usb"" with ""After connecting the usb""",0
[Relay][Pass] Count MAC for BatchMatMul (#4157)* count MAC for BatchMatMul* update doc,1
[Relay][QNN] Add unit test for int8 (#4159)* [bugfix][codegen] fix casting bug in llvm codegen* update example* retrigger ci* check llvm version,0
[relay][vm] Reuse allocated device memory (#4170),5
add missing gradient check to gradient pass (#4169),1
merge extract_from_program and extract_from_multiple_progam (#4173),5
[TOPI] Added support for Mali Bifrost target (#4047),1
[Relay][Frontend][TF] Fix Size operator (#4175)* [Relay][Frontend][TF] Fix Size operator* Uncomment tests,0
[Pass] Remove dead code (#4177),4
[rpc] use callback func to do send & recv (#4147)* [rpc] use callback func to do send & recv. don't get fd from sock as it is deprecated in java* fix java build* fix min/max macro define in windows* keep the old rpc setup for py* add doc for CallbackChannel,0
Add support and testing for tf.assert (as no-op) and tf.no_op to TF Relay frontend. (#4172),1
[DOCS] Add TensorFlow frontend docs (#4154)* Start to update TF frontend docs* Add rst* Remove markdown* Update wording* Resolve comments,1
"Revert ""[Relay][QNN] Add unit test for int8 (#4159)"" (#4192)This reverts commit 6f9d028b80f9e41fd577b5c6a7229cafcfc72173.",1
[cmake][ANTLR] Support setting path to ANTLR jar (#4176)* Support setting path to ANTLR jar* Update comment,1
Split adaptive_pool2d_avg into sum and div (#4186),5
[Documentation]Fix example code in comment of tvm.build_module.build() (#4195)* Fix example code in comment of tvm.build_module.build()* Update build_module.py,0
[relay] use time_evaluator for measurement (#4191),5
Add parser support for SUM tflite operator (#4182),1
[Relay] Fix memory leak in the interpreter (#4155)* savelint* address reviewer comment,0
[TOPI] Tunable Template for Conv2D HWCN on CUDA (#4168)* support conv2d HWCN in AutoTVM and Relay* fix lint* fix comments and unit tests,0
TensorCore Support using Intrinsic (#4136)* add tensor core support* avoid memory bank conflict* fix thread sync & better performance* better performance* add schedule test for conv2d* extend into BatchMatMul* support config fragment shape and layout using intrinsic* add TensorCore tutorial* add int support and fix lint* address comment* add 32*16*8 TensorCore test* fix wmma include logic,0
"[NODE][REFACTOR] Refactor reflection system in node. (#4189)* [NODE][REFACTOR] Refactor reflection system in node.- Removed the old Node, Node is now just an alias of runtime::Object- Introduce ReflectionVTable, a new columnar dispatcher to support reflection  - This allows us to remove vtable from most node objects  - The VisitAttrs are registered via TVM_RESGITER_NODE_TYPE,    they are no longer virtual.- Consolidated serialization and reflection features into node.* Explicit type qualification when calling destructor.* Fix SPIRV, more comments",0
hotfix the ci (#4199),0
[TOPI][x86] Legalize - Support int8xint8 convolution to use VNNI instructions. (#4196),2
[Relay] crossentropy_with_logits and its gradient (#4075)* save* lint,2
[hotfix] missing include headers (#4204),0
"[Relay][Training] Add checkpoint annotation for checkpointing memory optimization (#4146)* add checkpoint annotation for checkpointing memory optimization* add alpha-equivalence checkpoint test and fix gradient type issue* fix build issues* ignore checkpoint annotation when checking missing gradients* refactor, fix checkpoint compute for tuple and add tests",0
[Relay][Params] Add APIs for storing and retrieving parameters from individual functions. (#4194)* Add support for attaching params* Fix types* Fix test,0
[Relay][Frontend][ONNX] Add support for op Where (#4184)* Add support for op Where* Update impl version,1
"[VTA][Chisel] TSIM VTA Source Refactor (#4163)* app init push* fix on readme* change name, add bit serial explanantion* rm serialLoadMM, change doc* syntax change for readme* add parallel test functionality* fix readme* add python doc* syntax* init commit* fix empty line* fix typo",0
[RUNTIME] Separate runtime related contrib into runtime/contrib (#4207),5
Fix type var docs (#4208),0
[Relay] Setting Legalize opt_level to 1. (#4198),5
[TOPI] Fix flaky testcase for check round (#4211),0
[Relay][Op] Enhance Upsample Operator to support float scales   (#4206)* :add scale2 for upsample* update unit test for upsampling* support latest upsample op for multiple frontend* fix lint* fix lint* fix lint* fix lint* update scale description and rebase,0
[Relay][Quantize] Use fixed point mulplications (#4160),0
Update have_int8 condition to run on compute capability 7.x devices (#4214),1
Optimizing autotvm task extraction speed (#4138)* Optimize task extraction speed* correct pylint errors* Delete unused function* remove unnecessary argument* resolve code review comments* corrent cpp lint errors* remove one more graph_json return value* fix test bugs,0
[Relay] Add Python type functor and tests (#4209)* Add Python type functor and tests* Lint roller,1
Fix typo in packed_func.h (#4219),0
Improve the lowering of Qnn Dense (#4213)* [QNN] Improving Dense lowering.* - Moving get_shape method to util- Finalizing the test cases and the code structure for optimized dense computation.* - Fixing cpplint.* - Addressing review comments.* - Renaming the variables correctly.* - Renaming the variables correctly.,0
[ARITH] Fix the rule y < x && x <= y (#4220),0
[PYTHON] Add __init__ to the generated grammar so that it can be installed properly (#4223),1
"[Relay][Frontend][ONNX] New Operators and Opsets to Support BERT (#4197)* Added slice v10* Added constantofshape operation and small refactor.* Finished one_hot implementation.* Reshape working across all bert layers.* Fixed constantofshape and removed code duplication.* onnx model fully ingested.* Working on improving onnx tests.* Changed onnx testing to use onnxruntime instead of caffe2, also formatted.* Add arbitrary output nodes to onnx frontend.* Added v6 tiling for bert squad 8 support.* Small syntax fixes* Reduced code duplication in split opset versions.* Added batch matmul test* Added unstack split testing.* Adde onehot test, needs a little cleanup probably.* Replaced deprecated constant fill with constantofshape and updated tests accordingly.* Added tests for new opset version of slice and tile.* lint clean up* Lint fixes* Changed onnx dependency* Went back to caffe2 runtime for CI integration.* Rebase and small typo/syntax changes.* Added hard casting of onehot attributes to int.",0
[Relay][Topi][TensorFlow][ONNX][Lang] Add support for Any op (#4205)* Add support for Any op* Support ONNX frontend* Add doc* Add to relay docs* Dummy change to retrigger CI,1
[CI] use llvm9 for the gpu tests (#4224)* [CI] use llvm9 for the gpu tests* Update Docker script to support new nvidia docker,1
[Relay] Install Relay Prelude program in package install (#4227),2
[Doc] Update ANTLR instruction (#4231)* [Doc] Update ANTLR instruction* Update docs/install/from_source.rst,1
[CI] Move gpu docker binary to cuda10 (#4229)* [CI] Move gpu docker binary to cuda10* Fix the gcn tutorial,0
Fix typo in get_output doc-string (#4237),0
[CI] Update GPU docker to cuda10 (#4228)* [CI] Update the ci-gpu to use cuda10* [CI] Enforce tensorcore gpu for unittest,1
"[CUDA] Fix fp16 intrin, disable bad fp16 vecadd test for now (#4239)",0
[BUILD] Disable utvm standalone runtime by default (#4240),5
Fix the problem that android_rpc compilation failed. (#4244)Signed-off-by: qinqiuping <autumnqin@126.com>,0
[ARITH] Fix lowering of FloorMod (#4236),0
[ Relay ][ Frontend ][ Tensorflow ]add op add_n to relay/frontend/tensorflow.py (#4181),1
[Relay][Pass] Avoid FoldConstant folding some ops (#4245)* [Relay][Pass] Avoid FoldConstant folding some ops* rename,4
[Relay][Prelude] Add more dtypes to tensor_t (#4233),1
Implement explicit IR representation of memory alloction (#3560),5
"[NODE][REFACTOR] Rename IRFunctor->NodeFunctor, use func pointer (#4247)* [NODE][REFACTOR] Rename IRFunctor->NodeFunctor, use function pointer for dispatching.Previously we used std::function for the functor dispatching.It introduces additional overhead and problems during dll destruction(of std::function).This PR changes the std::function to function pointers.This change a bit restrictions around the set_dispatch that we can get around,but will improve the general efficiency by reducing one level of indirection in the std::function.We also no longer need special marcos to register functions to the Functor.",1
Support reshape for dynamic shape in tf converter (#4185)* Support reshape for dynamic shape in tf converter* Only allow reshape directly after shape function for symbolic input shape* Fix lint,0
"[VTA] Performance optimize, remove unnecessary contigious memory use. (#4246)* [VTA] Performance optimize, remove unnecessary contigious memory use.Issue:Uop maintain a cache vector to copy uop data into contigious DRAM memory forFPGA/Simulator use, but this cache vector not get clear after FPGA/Simulatorcore run, in Resnet18 case, if we printf the cache size in UopQueue::ReadBarrierfunction, we can saw such cache size keep increase, this would causeno use data copy and unnecessary contigous DRAM memory malloc.Analysis:This issue caused by not clear cache_ vector when douop_queue_.Reset().Solution:Override BaseQueue Reset function in UopQueue and add cache_ clearlogic.* address review comments, remove spacing.",1
Fix typo in err msg (#4251),0
remove PEP498  f-string new feature for support  python3.5 (#4250),1
"[Relay][Frontend][Tensorflow] Fix GatherV2, Add StopGradient (#4238)* Add StopGradient. Add batch_dims attr to ignore list for GatherV2* Trigger CI",0
CI trigger after repo move (#4252),4
Require LLVM >= 9 for AMDGPU backend (#4253)LLVM 8 will crash when loading the bitcodesThis is a runtime check as the file will be compiled in even whenUSE_ROCM OFF is used in the configuration if ROCM is installedin the default location.Fixes: #4087,0
workaround typing.Deque import error for Python 3.5 (#4254),0
[DOCS] Update link loc (#4257),1
[VTA] Hotfix for padded load test in Chisel VTA (#4264)* Update TensorUtil.scala* Update test_vta_insn.py,0
[Contrib] Fix error message at callback_get_section_size() (#4221)* [Contrib] Fix error message at callback_get_section_size()* Trigger notification,0
[TOPI] Fix bug in Winograd on CUDA (#4260)* fix winograd* move get padding after kernel transform,0
[AutoTVM] Add batch_matmul to tunable operations (#4242)* Batch matmul tuning running but with errors.* Default x86 schedule as good as before.* Code Cleanup* Remove unused argument.* improved template documentation.* Silly lint fix* Removed leftover comment.* Moved cfg declaration to schedule for batch_matmul* Moved x86 dense cfg declaration to schedule.* lint fix* Removed duplicate cfg declaration in dense.* Reverted changes to dense.,0
[Relay][Frontend][ONNX] Add support for broadcasting to Where and MatMul (#4267),1
[TOPI][CUDA] Fix Winograd Kernel Size Support (#4276)* fix_winograd_cuda_kernel_size* add unit test,0
Update tvm_runtime.h (#4278)fix the problem that android_rpc compilation failed,0
Auto TensorCore CodeGen (#4234)* Add Auto TensorCore TensorCore Unit Test* Rebase to tvm master branch & Add auto tensor core* Code Refine* Add tensor core switch by pragma* Add pragma in tensor core example code* Get real tile size to replace hard coded 16* support more than 2 dimensions (e.g. batchmatmul) for buffer bind scope* support batch matmul* Move cuda env check to tensor_core.cc* Coderefine for tensor_core.cc* Refine comments* Some refinements of code and comment* Update TensorCore UT to pass the CPU test* remove redundant code* matmul's storage align for different layout* Add support for differenct position of type cast* Add formal tutorial for auto tensorcore codegen* move tensorcore check up to tutorial code* code and doc refine* comment out tune_and_evaluate in tutorial* fix cpplint error,0
Rename ml.dmlc.tvm to org.apache.tvm (#4290),2
[Codegen][cuda-fp16] fallback to fp32 simulation when cuda arch < sm53 (#4268),5
[Test][TF][Relay] Fix argument preparation for vm test mode (#4296),0
[TFLite] Support PRelu (#4298),5
[RUTNIME] Support C++ RPC (#4281),5
[TOPI][AlterOpLayout][ARM] Enabling NHWC to NCHW layout transformation. (#4249),5
[tutorial] Relay pass infra tutorial (#4083)* Add pass manager tutorial* fix some examples* retrigger ci* Update tutorials/dev/relay_pass_infra.pyCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* Add ToANormalForm link,0
Fix tf reshape (#4285)* Fix tf reshape* Fix test* Fix pylint* Fix pylint,0
[TF][TEST] add test_forward_reduce_any back (#4301)the test case was removed in #4181 for some reason@tqchen @soiferj @zhiics,1
[RUNTIME][REFACTOR] Use object protocol to support runtime::Module (#4289)Previously runtime::Module was supported using shared_ptr.This PR refactors the codebase to use the Object protocol.It will open doors to allow easier interpolation betweenObject containers and module in the future.,5
[TF][Relay][Op] Pass module when infer shape (#4287)* [TF][Relay][Op] Pass module when infer shape* Fix lint* Improve style* Add test,0
Add More Shape Functions (#4179)* Add shape functions* Fix get_const_tuple* Fix cpplint* Fix pylint* Fix pylint* rebase and fix* Check Any for infer type* Fix expand_dim shape func for zero rank input* Fix pooling infer type* Address comment* Register layout transform attr,0
[Relay][Frontend][Tensorflow] Fix type assignment for operator 'tf.range' (#4294),0
Fix incorrect call to Unicode Win32 InetPton (#4306)* Fix incorrect call to Unicode Win32* Removed inet_pton call. Win32 already has it.,0
[Relay][Frontend][Keras] batch_norm op params not handling well (#4310)* Relay Keras frontent batch_norm op params not handeling well* add unit test for Relay Frontend Keras batch_norm,1
add (#4311),1
Add test for the qnn_add operator (#4282)* Add test for the qnn_add operatorThe tests use fake quant approach so until the tf session tensors remain in float32.The test data has to be passed in uint8 because of how the tflite/tvm comparison works.Abs tolerance up to 1 is allowed for the qnn results. For now input_stats are hardcodedassuming the tests for the other qnn ops will pass the input data in the same range.* Separate qnn uint8 test function from the fp32 elemwise testsIsolate qnn uint8 elemwise testsRemove blank lines,1
[Relay][Op][TF] Complete tensor array unstack with all ranks support (#4309),5
"Fix the TF tutorial to run against TF2.0 and TF1.x (#4104)* WIP Run the TF tutorial on TF2* Remove debugger statement.* Complete the support for TF2.0's `resize`.TF2.0 adds a `half_pixel_centers` attribute to the `resize` function inthe image API. This commit completes the hooks in Relay's TF frontend.At the point of this commit, no new test yet. Also, this commitaddresses solely the `resize` change. Other commits address otherchanges in TF2.0.* Support TF2.0 in the tutorial by using the compat API.This looks cleaner than trying to detect the TF version.* Use the TF compat API, so as to support TF2.0.This is a direct change, relying on the compat API provided by the TFteam.This code will last as long as the compat API exists, so a""proper"" support for TF1.x and 2.x will require more work in somefuture.* Partial support for EXPLICIT padding introduced in TF2.0.Explicit padding is a special case in TF2.0 (see reference linkedbelow). Some models are serialized with that mode, and break TF supportin TVM.Support is *partial* as EXPLICIT falls back to set padding on theRelay op, which only supports 2 values. At some point, padding may needto be extended to support 4 values, but that is out of scope of thissupport commit.Reference on EXPLICIT padding: https://github.com/tensorflow/tensorflow/commit/ec81825aaf7e848d9f8ddffdf1e0d20aebe9172c#diff-1d1c0bb0a880f85b6164f71dbb2f446e* Guard on checking for optional TF2.0 attribute.* Do not expect Relay to implement TF-specific attributes.The `half_pixel_centers` attribute is a new feature in TF2.0. Earliercommits of mine mistakenly introduce them in the Relay API. This isprobably not what Relay is expected to support, and the semantics of`half_pixel_centers` is unclear (to me, at least) at this point.* Remove unclear comment.CR https://github.com/dmlc/tvm/pull/4104#discussion_r338705742Addresses #4104* Changes after review.Complying without understanding the rationale for now.* Fix the arguments set mistakenly.An argument ignored for the wrong operation.",0
[TOPI][OP] Support Faster-RCNN Proposal OP on CPU (#4297)* Support Proposal operator on CPU.* PyLint space issue* PyLint space issue* Pylint singleton-comparison issue,5
[QNN][Legalize] Specialize for Platforms without any fast Int8 arithmetic units. (#4307),3
fix error when memory_id is VTA_MEM_ID_OUT (#4330),0
[CI][DOCKER] Add ONNX runtime dep (#4314)* [DOCKER] Add ONNX runtime dep* Improve ci script,1
[QNN] Quantize - Fixing the sequence of lowering. (#4316),0
[QNN] Use Int16 upcast in Fallback Conv2D. Fix test names. (#4329),0
[doc][fix] fix sphinx parsing for pass infra tutorial (#4337),0
change ci image version (#4313),4
[Codegen] remove fp16 function override for cuda  (#4331)* add volatile override back* [codegen] remove fp16 function override for cuda,1
[CI] Set workspace to be per executor (#4336),5
[Build][Windows] Fix Windows build by including cctype (#4319)* Fix build* dummy change to retrigger CI* dummy change to retrigger ci* dummy change to retrigger ci,0
Enable hipModuleGetGlobal() (#4321),5
[Relay][Pass] Add pass to remove unused functions in relay module (#4334)* [Relay][Pass] Add pass to remove unused functions in relay module* Add tests* Fix lint* Fix visit order* Add pass argument* Fix,0
Add support for quant. mul operator in tflite frontend (#4283)A test for qnn_mul has to be added when the qnn elemwise tests (#4282) get merged.,1
Add topi.nn.fifo_buffer to TVM doc (#4343),1
Solve custom model of prelu (#4326),5
Deprecate NNVM warning msg (#4333),5
[Contrib] Add MKL DNN option (#4323)* [Contrib] Add MKL DNN* update* update,1
[Relay][Frontend][TF] Fix transpose when axes is not a param (#4327)* [Relay][Frontend][TF] Use _infer_value_simulated when axes is not a const to Transpose* uncomment tests* dummy change to retrigger ci,0
[RUNTIME] Add device query for AMD GcnArch (#4341)* add gcnArch query* kGcnArch query for cuda is a no-op,1
[Test][Relay][Pass] Add test case for lambda lift (#4317),1
"[Relay][Frontend][ONNX] operator support: DepthToSpace, SpaceToDepth (#4271)",5
imp module is deprecated (#4275),5
[VTA] Bug fix for padded load with large inputs (#4293)* bug fix for padded load with large inputs* Update TensorLoad.scala* Update test_vta_insn.py,0
fix inconsistent tag name (#4134),0
[CodeGen] Add build config option disable_assert to control whether to generate assert (#4340),1
Bump up CUDA log version in tophub.py (#4347),2
Add check to ensure input file was successfully opened in NNVM deploy code demo (#4315),1
"[COMMUNITY] Add DISCLAIMER, KEYS for ASF release (#4345)* [COMMUNITY] Add DISCLAIMER, KEYS for ASF release* Add file name spec",1
[Relay][VM][Interpreter] Enable first-class constructors in VM and interpreter via eta expansion (#4218)* Fix constructor pretty printing* Make Module::HasDef name consistent with API* Add VM constructor compilation via eta expansion* Lint* Fix CI* Fix failing test* Address comment* Retrigger CI* Retrigger CI,0
[FIX] Fix for a specific case when loop partitioning with indivisble (#4243)factors and resulting nested loop is broken.This is due to the fact that we are creating zero extent loops whichare fixed afterwards. However unroll pass breaks due to the zero extentloop.,0
"Add workgroup size attribute to AMDGPU functions in codegen (#4342)When we did not set the workgroup size, LLVM will use too many registersfor kernel launches with many threads. This resulted in ""invalid ISA""errors. Here we set the maximum workgroup size to the maximum threadsper block from the device API.Of course, one might look into allowing configurations with fewerthreads at runtime to use more registers.",0
AutoTVM: selecting tuning templates when extracting task (#4338)* AutoTVM: selecting tuning templates when extracting taskMake the procedure of trying new templates easier.Test: tests/python/relay/test_autotvm_task_extraction.py* Use dict to match key for topi ops* fix lint issue* be more pythonic :),0
fix install script (#4350),0
proper device query through rocm api (#4305),5
[Debugger] Sorting op-time breakdown for quicker analysis. (#4352),0
"Retain qnn input kernel scales (#4292)* Add qnn conv2d attributes for input_tensor_scale andkernel_tensor_scale.The lowering in the tflite frontend loses the input_tensor_scaleand the kernel_tensor_scale by multiplying it and putting it intothe Requantize operation. This means that any graph partitioningpasses or other passes that need to access this information no longerhave it available in the qnn dialect.regardsRamana* Store input tensor scale and Weight tensor scale for Dense as wellAs for conv2d, the tflite frontend drops the input tensorscale and the weight tensor scale from the relay op. Storeit as separate fields in there.* Fix unintentional tab* Rename input_tensor_scale to input_scale and kernel_tensor_scaleto kernel_scale for conv2d.* input_tensor_scale -> input_scale weight_tensor_scale->weight_scale* Rework dense testcaseAnd use input_scale and kernel_scale* Be consistent in use of input_scale and kernel_scale values* Fixup qnn conv2d tests for input_scale and kernel_scale* Make pydoc identical between conv2d and dense for weight_tensor* Fix up conv2d parameters to be in the same order between C++ and python* Fix ordering of parameters for dense.* Add input_scale and output_scale to try and satisfy ci gods* Delete input_scale and kernel_scale.nn.conv2d does not contain input_scale and kernel_scale. We needto delete it when lowering it to nn.conv2d.* Add input_scale and kernel_scale for qnn.conv2d",0
Fix docstring in topi.nn.fifo_buffer (#4349),0
"Send list as argument to schedule_conv2d (#4358)When getting cuda schedule passing single tensor seem to work but after changing target to ""llvm"" causes assert.Sending list on other hand makes both cuda and llvm targets happy.See https://discuss.tvm.ai/t/solved-simple-example-error-attributeerror-tensorslice-object-has-no-attribute-op/2245/3",0
[Relay][Frontend][Tensorflow]Add conv2d_transpose (#4300)* [Relay][Frontend][Tensorflow]Add conv2d_transpose* add transformation from NHWC to NCHW to compatible with TVM conv2d_transpose implementation* remove 'dilations' paramater to compitable with TF1.3,1
[Frontend]Add TensorFlow FloorMod (#4308)* Add tf FloorMod* Add floor_div/mod into topi and relay* Add to rst* Fix test,0
[SOURCE] Add ASF header to __init__.py files (#4359),1
fix Android and OpenCL docker install (#4363),0
reminding message for TVM_REGISTER_NODE_TYPE (#4365),5
"add rule for clean (#4364)* add rule for clean* Update clean ruleSeems like lib/ directory is not made by the makefileSo don't delete directory, just the contents of it.",1
[Relay tests] AlterOpLayout - Temporary attr update (#4357),1
Fix TFLite RESHAPE assert (#4320),0
[tutorial][benchmark] nnvm -> relay (#4368)* [tutorial] nnvm -> relay* use relay workload* delete movbilenetv2 option,5
[PERF] Parallelize reduction for CPU (#4158)* [PERF] parallel reduction in cpu* fix* x* update* lint* fix,0
[Relay][Quantize] Integrate data-aware calibration into quantization (#4295)* [Relay][Quantize] Integrate data-aware calibration into quantization* Update _calibrate.py* trigger ci* Address comments* address comments,1
[nvcc] enable multiple arch in one fatbin (#4377),5
[CI] Avoid content-length request in test data download (#4375),3
"[doc] fix typo, codege to codegen (#4383)",0
fix build with llvm trunk (#4386),0
[team] add Yizhi's pgp key (#4380),1
Compare all outputs in TFLite test_forward_ssd_mobilenet_v1 (#4373),3
[ThreadPool] Solve thread transitions issue (#4344)* [ThreadPool] Solve thread transitions issue* Use pthread_atfork to avoid master thread affinity be derived by child.* Code Format* comment of exclude_worker0_* set full cpu affinity* Redundant blank line* CPPLint* CPPLint namespace* CPPLint* Fix the wrong logic of bind master thread.,0
"[CI] Add more info, per exec ws isolation (#4388)",1
[fix][pass] Save the function when it is used as a call arg (#4389),0
[QNN] Lowering for Depthwise Convolution. (#4351),5
"add GPU checking before compilation for rocm (#4394)Previously, we would rely on the later phases to error out(often for using too much shared memory). This enables thechecks on the IR that already exist for CUDA and OpenCL alsofor ROCm.",0
[Relay][Frontend][TF] Fix slice when begin or size is not Const (#4372)* fix slice bug when input is param* use _infer_value rather than _infer_value_simulated,0
Update compile_engine.py (#4393),1
Add Logan to reviewer (#4390),1
[TOPI] Fix flaky testcase for floor div (#4382)* [TOPI] Fix flaky testcase for floor div* avoid check at 0.0,0
[Relay][VM] Clean up the VM and VM profiler code (#4391)* [VM] add a few more API to vm* [VM][Fix] fix vm convert args* [VM] a few fixes* rename fields* update* update vm profiler* x* add doc* lint* fix test* address comments,0
Update Jenkinsfile for external runtime (#4396),1
update_document_after_repository_renamed (#4398),1
[Golang][Doc] improve the samples and doc (#4385)* [Golang][Doc] improve the samples and doc* [Golang][Doc] add asf header* [Golang][Doc] Improve the end to end example* [Golang][Doc] Improve the end to end example,1
[DOCS] Mention incubating in readme (#4401),2
Added tflite frontend support for quantized mean. (#4339),1
[LICENSE] add 3rdparty licenses (#4402)* [LICENSE] add 3rdparty licenses* rename license files to .txt,1
[TVM][RUNTIME] A minimum example to generate external library wrappers for DSOModule (#4280),5
[RUNTIME] Move module export to the function level. (#4405),4
[Relay][Legalize] Legalize conv2d_transpose for NHWC (#4399),5
[Release] resolve license issues (#4408),5
"[LINT] Remove unnecessary copyright message for files with ASF header (#4409)* [LINT] Improve the check tool to handle ASF copyright message.* [LINT] Remove unnecessary copyright message as per ASF requirement.* Fix codegen hybrid* [LINT] Broaden license checks to include html, xml* [LINT] Fix rest of the files* Fix notice* [LINT] Improve check file type error message",0
[License] move cma_api to 3rdparty. separate BSD 2-clause and 3-clause (#4410)* [License] move cma_api to 3rdparty. separate BSD 2-clause and 3-clause* add zlib license for blockingconcurrentqueue.h,1
"[LICENSE] clarify the blockingqueue license, update version to 0.6.0 (#4414)",1
Fix compilaton of bfloat16 on Windows (#4415),0
[RUNTIME] rename allocator.make -> allocator.make_object for term consistency (#4416),5
[Perf] Enhance cudnn and cublas backend and enable TensorCore (#4353)* add half and mix precision support to cublas backend* add TensorCore support in CuDNN* enhance CuDNN support* address comments and fix lint* fix* add fp16 test,0
add rocm codegen unittest for cross thread reduction (#4423),1
removing nnvm dep from VTA sources (#4419),5
[Fix][Relay] Remove schedule register for nonexisting log1p op (#4425),0
[SETUP] Add optional dependencies to extras_require (#4428),1
[AutoTVM] select model with the most tuned schedules (#4404)* select model with the most tuned schedules* change detect empty map method* modify model description for load_reference_log,2
Tweak debugger result (#4426),0
Allow Array/Map store objects that are not NodeRef (#4430),5
[DOCS] Update main website to tvm.apache.org (#4429)* [DOCS] Update main website to tvm.apache.org* Update jvm pom repo loc* Change the org to asf* Update ci addr to new one,1
"[RELEASE] Update copyright message, change notice, remove cma kernel module for now (#4431)",1
[VTA][HotFix] Relay->VTA quantization fix (#4433)* relay -> vta fix* setting optlevel to 3 for quantization to fold batchnorm,0
[ARM CPU] Fix infer shape error of depthwise (#4384)* [ARM CPU] Fix contrib_spatial_pack error* PyLint error fix* diable no-else-return as other files* Change the test case split OC not be 1 to cover 5D weight layout,0
add DeviceName to ROCm api (#4437),1
[VTA] Enable streamlined GEMM execution (#4392)* disable pipelined adder and enable streamlined gemm execution* pipeline first layer of adder* explain difference between pipeadder and adder* add comment for explaining the hard-coded latency,1
[Doc] Fix broken link (#4438)* [Doc] Fix broken link* [Doc] Fix broken link* [Doc] Fix broken link,0
fix multiple transfer issue in loaduop (#4442),0
rpi4b target (#4445),5
[Relay][Frontend][TFlite] Add test for qnn_mul operator (#4395)* Add a function to set the qnn output range wrt each elemwise operation.* Add comments warning for nonsense clamped output in the tflite/tvm results comparison.,1
[Relay][Pass] Fix lambda lift pass for recursive call (#4432)* Fix lambda lift* clean up* lint* fix* remove unused import,0
[Runtime] Make ADTObject POD container type (#4346),5
[TFLite] Add transpose_conv to TFLite parser (#4440),1
a tiny typo (#4452),5
[µTVM] Enable AutoTVM for ARM STM32F746XX Boards (#4274),5
"[DOCS] add benchmark log format doc (#4366)* add benchmark log format doc* code review changes* remove runtime_config, add md5 field* schema edits",1
"[Relay] shape func for zeros, zeros_like, ones, ones_like (#4448)",5
[TOPI][Relay][OP] Add a strided_set operation. (#4303),1
Fix MSVC build error with container.h (#4455),0
Fix the Makefile for howto_deploy (#4457),0
[MEMORY] Fix gcc 4.8 compact (#4461),0
"[RUNTIME] Add cudnn conv3d (#4418)* [RUNTIME] Add cudnn conv3d* add output checking to test_cudnn.verify()* fix tests failure* revised per as review comments* unify conv_output_shape, conv_find_algo and conv_forward* convert python list to tvm.array in conv_forward* revise per as comments* 'pass as reference' for vector args* add back con2d/3d seperated implementation* remove unused included header* remove extra std::vectors* remove unused header",0
[RUNTIME][RPC] Update RPC runtime to allow remote module as arg (#4462),1
"implement conv3d op (#4400)* implement conv3d op* add back missed conv2d_output_shape by mistake* fix typo and docs, add topi test* rebase to master and merge 2d/3d unification* use cudnn.conv_forward",0
[doc] fix typo (#4463),0
lldb pretty printers for relay (#4453)* lldb pretty printers for relayA set of lldb debugger pretty printers that use the relayPrettyPrinter functionality to display data structures inthe lldb debugger.* lldb pretty printers for relayA set of lldb debugger pretty printers that use the relayPrettyPrinter functionality to display data structures inthe lldb debugger.- Put the dot.lldbinit file in your home directory as .lldbinit.- Update the file to point to the pretty printer script tvm.py- Restart lldb,0
[CONTRIB] TFLite Runtime (#4439),5
[BUGFIX] Fix search path for libtvm_topi.so (#4467),0
[relay][op] Add shape func to tile (#4441)* [relay][op] Add shape func to tile* retrigger ci* check dynamic axes* retrigger ci,1
Fix typo in travserse (#4469),0
Workaround to make conv2d_transpose compilation for CUDA work (#4472),5
[Codegen] fix bug on LLVM 10.0 (#4480),0
Check function attr for alpha equal (#4479),5
[VTA] Bringing group convolution support  (#4421)* group conv operator support for VTA* autotvm tuning script for group conv2d* lint fix* lint fix* lint fix* addressing comments,0
[Relay][Frontend][TFlite] Add parses support for UNPACK tflite operator (#4447)* use SPLIT & SQUEEZE = UNPACK as implemented in tensorflow parser  Relay doesn't support UNPACK* tflite 1.13: UNPACK doesn't work as exepcted -> copies the values from  1st unpacked tensor to the other unpacks* tflite 1.13: doesn't accept negative axis,1
"[REFACTOR][RUNTIME] Add LibraryModule that merges systemlib and dso. (#4481)Historically we have two variations of modules(DSOModule and SystemLibModule)that both exposes module via symbols.This PR creates a common implementation for both, and introduce a Librarybase class that allows us to have different implementations of GetSymbol.It paves ways for future library related module enhancements.",1
[docs] typos in include/tvm/ir.h (#4493),2
[Team] Jared Roesch -> PPMC (#4488),5
[Relay][Fix] Fix alter op layout when calling a global var (#4454)* [Relay][Fix] Fix alter op layout when calling a global var* add test case,0
Add __float2half_rn for cuda compute capabilities less than 53 (#4489)* Fix* clean up,0
[VTA] Speedup TSIM by Multi-threading (#4491)This PR tries to increase TSIM performance by introducing multi-threading support.,5
update rocm intrin rule (#4499),1
[RUNTIME] Fix compile errors of OpenCL FPGA backend (#4492),0
[codegen][Build] it's more readable to move the if condition out of the loop (#4501),4
"Refactor bilinear and neighbour implementation in Tensorflow frontend (#4504)There is significant duplication between functions.Spotted while looking to move the tensorflow and tflite framework support to later than1.13.1. The tests barf around resize_nearest_neighbour not ignoring the attribute'helpful_pixel_centers'.That upgrade is a separate discussion while this can go inindependently.Thanks,Ramana",3
Add AMD codeGen unit tests (#4509),1
add rocm schedules to topi C++ (#4507)This imports the CUDA schedules to rocm.,1
"[NODE][Serialization]fix serialization precision loss in float (#4503)* fix serialization precision loss in floatWhen we want to serialize a tvm.tensor object(like pickle)， we will get a precision loss cause by std::to_string()。For example, a2.value will be 0.0 while a.value=0.00000001 in the following:     import tvm    import pickle    a = tvm.const(0.00000001, 'float32')    a2 = pickle.loads(pickle.dumps(a))* remove line end spaces",0
[TOPI] implement pool3d op (#4478)* [TOPI] implement pool3d op* use PoolInferCorrectLayout for both 2d and 3d pooling* unify MakeMaxPool and MakeAvgPool,5
Fix build for llvm newer than 9.0 (#4515),0
[Hybrid][Fix] Fix hybrid script to support array of tensors (#4494)* [Fix][Hybrid] Fix hybrid script to support array of tensors* add test case* clean up* trigger ci,0
[Quantization] Fix annotation for multiply op (#4458)* fix mul rewrite* register Realize Rewrite for global avg pool and add test* remove unnecessary check* improve the test case,0
[CI] Update docker image ci_lint to obtain Python 3.6 from ppa:deadsnakes/ppa (#4505) (#4506),1
Fix TF resize for dynamic size models (#4510),0
"Fix bias_add gradient (#4516)* Fix bias_add gradientA change caused collapse_sum_like to reject implicit dimensionbroadcasting for bias_add gradient, so switch to explicit sum reductionon the non-bias axis dimensions.* Lint fix",0
[Bugfix][Frontend][TFlite] Fix wrong function call in TANH tests (#4517)* Replace sigmoid() with tanh() in tests for TANH,0
Fixed extra reshape parameter bug. (#4524),0
Use the best tuner possible (#4397)* Use the best tuner possible* Add comment denoting availability of better tuners* Fix typos and wording,0
[ir] use DataType instead of Type for readability because Type has been deprecated (#4513),2
add bfloat16 typeflag support (#4525),1
fix empty config caused KeyError (#4520),0
fix onnx shape dtype (#4528),0
fix crash issue in tsim backend (#4527),0
PIL is depreciated and should be replaced with pillow (a fork of PIL) (#4533)Change-Id: If2075df5475505f2da87dae7145af5a7ab83d8a4,4
[Relay] External codegen (#4482),5
Update legacy places from nnvm to relay. (#4535)* Update legacy places from nnvm to relay.This PR prepares the current mainline to remove nnvm compiler dep.* remove legacy stage,1
Implement 1d deconvolution (#4476),5
[relay][op] add expand op (from ONNX) to relay frontend (#4483)* Add Expand to onnx.py* add test function for expand* Fix a onnx frontend test* Add tests for the value itself instead of shape only on test_expand* Cleaned up some unnecessary modifications.,0
[TOPI] Allow batch matmul to be fused into injective ops (#4537),5
[TOPI] Fixed nms max_output_size loop (#4541)One of the loops in hybrid_nms used forperforming the max_output_size reorderingwas incorrectly designated as parallelresulting in incorrect behaviour. This patchchanges that loop to a serial loop.Change-Id: I97184f5887f5f028d8ab339fa2808eb7630a4017,0
[DOCS] Mention Ninja build system in install/from_source.rst (#4554)* [DOCS] Mention Ninja build system in install/from_source.rst* Address comments,1
[PYTHON][FFI] Cythonize NDArray.copyto (#4549)* [PYTHON][FFI] Cythonize NDArray.copyto* Cythonize the shape property,5
vm external codegen (#4544),5
[COMMUNITY] @cchung100m -> reviewer (#4557),3
[VTA] improved virtual memory mapping (#4545)* [VTA] improved virtual memory mapping* Update virtual_memory.cc,1
[IR] fix style in ir_mutator and ir_visitor (#4561),0
[RUNTIME][VULKAN] Fix compiler warning (#4559),0
"[REFACTOR][DTYPE] Isolate dtype to runtime (#4560)dtype.h -> runtime/data_type.hChanges:- Rename all old reference of tvm::Type to DataType- ExprNode.type -> ExprNode.dtype- Expr.type() -> Expr.dtype()- Change Expr related functions to expr_operator.  - DataType::min() -> min_value(DataType)  - DataType::max() -> max_value(DataType)- Move type constructor Int, UInt, Float, Handle, Bool into DataType.  - Int(bits) -> DataType::Int(bits)  - UInt(bits) -> DataType::UInt(bits)",4
Support standardize runtime module (#4532),5
[Relay][Frontend][ONNX] Support auto_pad in Conv and ConvTranspose (#4563),5
[TEST] Remove nnvm related code in topi and test script (#4562)* [TEST] Remove nnvm related code in topi and test script* Remove docs dep,2
[Relay] add max_pool3d in relay and TF converter (#4551)* [Relay] add max_pool3d in relay and TF converter* fix comments,0
Remove nnvm (#4565),4
[VTA][Chisel] End-to-end Inference with Chisel VTA (#4574)* [VTA][Chisel] End-to-end Inference with Chisel VTA* Update TensorAlu.scala,1
remove unnecessary cast to int32 (#4573),4
Fix llvm-enabled build by adding missing intrinsics headers (#4575),0
[DEPRECATION] Remove NNVM compiler (#4571)* Remove NNVM compiler,4
[Relay/Topi][Op] Added native DepthToSpace and SpaceToDepth Operators (#4566)* Added tvm function stencil for subpixel operations to topi.* Topi subpixel operators added and tested.* Added subpixel attrs.* Added depth_to_space relay attributes.* depth_to_space fully working.* Fixed NHWC shape bug.* SpaceToDepth in and all tests passing.* lint fixes.* Added string include* Fixed topi formatting.* Added DCR/CDR mode to depthtospace operator.,0
[DOC] fix doc in api.py (#4580),0
[DEPRECATION] Cleanup legacy verilog support (#4576)This PR cleans up the left over code for legacy verilog support which was experimental.The new hardware backend path is now support by VTA via TSIM.,1
"[RUNTIME] Remove Extension VTable in favor of Unified Object system. (#4578)Before the unified object protocol, we support passadditional extension objects around by declaring a type as an extension type.The old extension mechanism requires the types to register theirconstructor and deleter to a VTable and does not enjoy the benefit of theself-contained deletion property of the new Object system.This PR upgrades the extension example to make use of the new object systemand removed the old Extension VTable.Note that the register_extension funtion in the python side continues to workwhen the passed argument does not require explicit container copy/deletion,which covers the current usecases of the extension mechanism.",1
Some Windows and MSVC fixes (#4569)* fix python exception creation in Windows* better string conversion for msvc* fix cpp style issue,0
[NEWS] add v0.6 release (#4558)* [NEWS] add v0.6 release* remove link prefix* fix issue number,0
[DOCS]fix typos in autotvm tutorial (#4585),0
"[Quantization, Calibrate] Fix context creation when current_target is explicity set (#4582)",0
[Container] Fix NDArray SaveDLTensor declaration and implementation signature different (#4586),0
"[TOPI][AutoTVM] NHWC conv2d templates for ARM (#3859)* [AutoTVM][TOPI] NHWC conv2d templates (spatial pack) for ARMAs some frontends (tflite for example) are using NHWC as the defaultlayout, we are enabling NHWC schedule templates in TOPI and AutoTVM.* some comments fix",0
[FIX][TOPI][X86] schedule dense pack (#4539),0
[Relay] Convert Layout Pass. (#4335),4
[Relay][AlterLayout] Broadcast with scalar shape (#4577),5
[TOPI] add 3D upsampling Op. (#4584)* [TOPI] add 3D upsampling Op.* fix lint issues* change align_corners to coordinate_transformation_mode* fix resize3d half_pixel* make a simple function and clean up trilinear_resize3d_python* fix doc,0
[Runtime] add necessary const qualifier for NDArray container of parameters (#4590),1
[autotvm] fix typos in comment (#4591),0
fix tf.compat.v1 issue for tf verison <=1.12 (#4593),0
[FRONTEND][TF] conv2d_transpose 'SAME' support kernel more than 1x1 (#4484)* [FRONTEND][TF] conv3d_transpose 'SAME' support kernel more than 1x1* revised per as review comments* add more fallback wolkaround to make all tests pass,1
[GraphRuntime] Support parameter out in the graph runtime debug (#4598)* [GraphRuntime] Support parameter out in the graph runtime debug* Dummy commit to trigger build,0
[Perf] Add CublasLt extern support for better Igemm performance (#4550)* cublaslt added* fix lint* address comments* address more comments* Trigger CI* Trigger CI,0
fix codegenc (#4597),0
"[REFACTOR][RUNTIME] Update NDArray use the Unified Object System (#4581)* [REFACTOR][RUNTIME] Move NDArray to Object System.Previously NDArray has its own object reference counting mechanism.This PR migrates NDArray to the unified object protocol.The calling convention of NDArray remained intact.That means NDArray still has its own type_code andits handle is still DLTensor compatible.In order to do so, this PR added a few minimum runtime typedetection in TVMArgValue and RetValue only when the correspondingtype is a base type(ObjectRef) that could also refer to NDArray.This means that even if we return a base reference object ObjectRefwhich refers to the NDArray. The type_code will still be translatedcorrectly as kNDArrayContainer.If we assign a non-base type(say Expr) that we know is not compatiblewith NDArray during compile time, no runtime type detection will be performed.This PR also adopts the object protocol for NDArray sub-classing andremoved the legacy NDArray subclass protocol.Examples in apps/extension are now updated to reflect that.Making NDArray as an Object brings all the benefits of the object system.For example, we can now use the Array container to store NDArrays.* Address review comments",1
[Relay][Convert Layout] Handling batch norm layout change. (#4600),4
[relay][refactor] Cache Op::Get in passes to reduce lookup overhead (#4594)* Refactor to use IsOp utility* retrigger CI,4
"[REFACTOR][OBJECT] Consoldiate NodePtr/Ref/Hash/Equal  to Object (#4603)* [REFACTOR][OBJECT] Consoldiate NodePtr/Ref/Hash/Equal and macros to Object.Historically, we have classes like NodePtr/Ref/HashEqual.After unified object protocol, these names are just alias of the object counterpart.Moreover, there are helper macros defined over the places for defining these object.This PR consoldiate the terminologies into the corresponding onesin the Object system so we have a clean and consistent API moving forward.* Update include/tvm/attrs.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>* fix compilationCo-authored-by: Wei Chen <ipondering.weic@gmail.com>",0
Sort VM stats by time (#4601),5
make adt tag signed (#4605),5
[FRONTEND][TF] Add conv3d (#4604)* [FRONTEND][TF] Add conv3d* fix high rtol,0
"[IR] Unify approach to Visitor/Mutator under Functor (#4606)IRMutator and IRVisitor were the main data structures for doing low level IR visiting.As the project evolves, we start to introduce more powerful variants such as StmtFunctor and ExprFunctor.This PR brings new classes that allows us to migrate the visitor mutator to be sub-class of these functors.List of changes:- Create separate class for ExprMutator and StmtMutator, following convention used in relay.- Introduce copy-on-write to StmtMutator that can later benefit the statement mutations  if we use move semantics and keep a single copy of stmt.- Move two generic visit mutate util to use the new classes.We will send followup PRs to migrate the existing passes that use the legacy visitorsto the new one.",1
Bugfix StmtMutator IfThenElse (#4609),0
[REFACTOR] Migrate Low-level IR Passes into the New Stmt/Expr Mutator (#4607)* CombineContextCall* Migrate BoundChecker* Migrate CoprocSync* Migrate detect_device* Migrate loop_partition* Migrate infer_fragement* Migrate inject_copy_intrin* Migrate inject double buffer* Migrate lower_intrin and simplify* Migrate storage flatten* Migrate inject prefetch* Migrate inject_virtual_thread* migrate inline* Migrate lift attr scope* Migrate custom datatypes* migrate lower_thread_all_reduce* Migrate lower_tvm_builtin* migrate lower_warp memory* Migrate make_api.cc* Migrate remap_thread_axis* Migrate remove_no_op* migrate rewrite_unsafe_select* Migrate skip_assert simple_passes* Migrate split_host_device* Migrate ssa* Migrate storage_access* Migrate storage_rewrite* Migrate tensor_core* Migrate unroll_loop* Migrate vectorize* Migrate verify compact_buffer gpu_code* Migrate verify_memory* Migrate storage_sync* Remove unused refs to mutator* Migrate hybrid_op* Migrate tensorize* Migrate schedule ops* Migrate schedule_dataflow_rewrite* Migrate auto_inline_elemwise* Remove unecessary ref to visitor* remove unecessary ref* Migrate bound_deducer* Migrate domain_touched* Migrate autotvm feature touch extractor* Add annotations,1
"[TOPI, Relay] Add half_pixel option to Resize op (#4610)* add onnx resize converter* update frontends* updating topi* adding onnx resize tests* fixed NHWC test by casting size dtype to int32* fix tests* fix lint* update existing test cases* fix tensorflow frontend* fix lint* remove NHWC stuff* update topi resize test for half_pixel* update doc* fix doc* remove onnx resize bits",0
[REFACTOR] Remove old Low-level Visitor/Mutator (#4612),4
[Quantization] Make calibration faster and more memory usage friendly (#4589)* Use memory efficient calibrate* Fixed indexing* add cpp kl stub* ported KL cpp from mxnet* Fixed std::distance arguments order* remove python implementation* fix lint and indent* fix indent* refactoring* fix lint* fix for i386,0
{QNN] Making scale/zero_points as expr instead of attrs. (#4611),2
[VTA] Throw exception on mis-formatted files and avoid overwrite Scala code (#4555),2
[CMAKE] Remove unecessary rdynamic (#4613),4
skip example json runtime test when config is not set (#4614),3
[relay][tensor_array] test tensor_array in vm (#4608)* [relay] test tensor_array in vm* add tensor_array scatter test,1
[Relay][Pass]Improve memory_allocation pass to support multiple i/o dynamic kernels (#4595)* Add more shape funcs* Fix test* Enhance test_any_concat* Fix pylint* Minor fix test* Fix pylint* Minor refactor* Add test any for elemwise,0
"[REFACTOR][TYPE] Remove un-necessary var sub-field in GlobalTypeVar and TypeVar (#4615)Currently, we use a tvm::Var to represent a placeholder for shapes in generic types.This is not necessary for GlobalTypeVar(as we never parameterize by shape var),and is a bit twisted for TypeVar.As we move to a unified type system, we want to break the dependencyfrom the base TypeVar(which is shared across the languages) from the expression.Note that it is fine for TensorType to depend on Expr.One alternative solution to embed the Var would be to introduce a TypeVarExpr,which can wrap a TypeVar as Expr. However, this new alternative won't benatural until we migrate the type to the global scope.Lucikly, we have not yet start to depend on the shape parameterization heavily yet.This PR removes the tvm::Var from the typevars. We will follow up with anotherPR to migrate the types to a base location. After that, we should be able touse the more elegant approach via TypeVarExpr.",1
"[REFACTOR] Unified IR base types. (#4616)This PR moves a few base types from relay to the ir sub-folder.These types will serve as a common type system across the stack.Notably, we want to be able to use the same FuncType for all function signatures.I tried to make a minimum move to bring the necessary dependencies for a FuncType.We can discuss what additional things we want to move as a follow-up.Notably, because the TensorType will have a dependency on low-level Expr,we will need to break the type.h into two files and introduce atensor_type.h(or leave them in relay for now).",1
"[REFACTOR] TVM_REGISTER_API -> TVM_REGISTER_GLOBAL (#4621)TVM_REGSISTER_API is an alias of TVM_REGISTER_GLOBAL.In the spirit of simplify redirections, this PR removesthe original TVM_REGISTER_API macro and directly use TVM_REGISTER_GLOBAL.This type of refactor will also simplify the IDE navigation toolssuch as FFI navigator to provide better code reading experiences.Move EnvFunc's definition to node.",1
"[REFACTOR] IRPrinter->NodePrinter, move to node/printer.h (#4622)Rationale: printer is a common infra that is shared across all nodes.",4
tensor_array split test (#4619),3
Added declare of aluBits for TensorAlu (#4624),1
Get around limitation of g++-4.8 (#4626),5
"[REFACTOR] Automatically deduce function type signature in Registry.set_body_typed (#4623)Previously we support a limited case of function type deduction and in many placeswe have to supply the type twice during set_body_typed (one in the template parameter, another in the lambda signature).This PR improves the deduce function by enablng automatic function signature deduction.```TVM_REGISTER_GLOBAL(""sub"").set_body_typed([](int x, int y) -> int { return x - y; });```Unfortunately, because of template conflict, we can not support the original casewhere both type signature and lambda are supplied through set_body_typed.This PR refactors the existing regsitration to the new style.",1
"[Topi]Allow empty tensor for reshape, tile and strided_slice (#4618)* Support empty tensor* Fix schedule* Refactor* Minor fix* Fix pylint* Merge cpp and python is_empty_shape",0
"[CONV] Asymmetric padding (#4511)* [CONV] Asymmetic padding* fix lint error* update for legalize, rocm and cudnn* add more test cases* change more symmetric padding* change conv2d winograd tests according orginal cases* remove 'alter_op_layout.h' header in bitserial.cc",0
"[REFACTOR][IR] Introduce SeqStmt to replace ir::Block (#4627)* [REFACTOR][IR] Introduce SeqStmt to replace Blockir::Block was used to represent a sequence of Stmts in the original low-level IR.The nested ir::Block structure is not really friendly for recursive visits,especially when the statements are unrolled.This PR introduce a SeqStmt that directly stores a sequence of statements in an Array container.The new SeqStmt will be used as a replacement of the original Block structure.* [REFACTOR] Migrate use of Block to SeqStmt.* [REFACTOR] Remove Block* Add more comments per yizhi's comment",1
Improve comments (#4633)* Improve commentary for operator fusion.* Attempt to clarify what well formed checker is doing,5
"Pin python pillow to ""<7"" due to torchvision 1.2.0 dependency issue (#4632)* As a result of backwards incompatible changes released in pillow 7.0,   torchvision crashes if you just ""pip install pillow"", as we do in   a few places. * This patch sets pillow<7 to be installed in Dockerfiles and support   material as tutorials and documentation.",2
Update image version tags in Dockerfile comments (#4631)* Fix typos on Docker image versions that we are currently running   as part of CI * Add version comment in the same pattern for ci_lint image,0
"[FRONTEND][Keras] Add support for tf.Keras networks in Relay Keras frontend (#4630)* Make Relay Keras frontend support networks created using   Tensorflow (1.13) Keras implementation (tf.Keras) * Modify Keras frontend tests to run from a class rather than a   function based script * Adjust Keras frontend tests to run with both 'Keras' and 'tf.Keras' * Change ""TestKeras.test_forward_merge"" to validate instances by   class name rather than instance type",1
[COMMUNITY] @wweic -> committer (#4636),3
[CI] better deletion script for pycache (#4635),5
Resolve constexpr related link error in debug mode (#4641),0
[QNN] Channel wise quantization - Quantize & Requantize (#4629),5
"[RUNTIME][DSO] Improve TVMBackendPackedCFunc to allow return val (#4637)* [RUNTIME][DSO] Improve TVMBackendPackedCFunc to allow return value.Previously the signature of LibraryModule's PackedFunc does not support return value.This wasn't a limitation for our current usecase but could become oneas we start to generate more interesting functions.This feature also start to get interesting as we move towards unifiedobject protocol and start to pass object around.This PR enhances the function signature to allow return values.We also created two macros TVM_DLL_EXPORT_PACKED_FUNC and TVM_DLL_EXPORT_TYPED_FUNCto allow manual creation of functions that can be loaded by a LibraryModule.Examples are added in apps/dso_plugin_module.The change to TVMBackendPackedCFunc is backward compatible,as previous function will simply ignore the return value field.* address review comments",1
[COMMUNITY] @MarisaKirisame -> committer (#4645),3
reduce input size to fix oom (#4653),0
"[REFACTOR][IR] Add Node suffix to low-level IR nodes (#4649)* [REFACTOR][IR] Variable -> VarNode* [REFACTOR][IR] Add/Sub/Mul/Div -> AddNode/SubNode etc.* [REFACTOR][IR] Min/Max/FloorDiv/FloorMod -> MinNode/MaxNode etc.* [REFACTOR][IR] EQ/NE/LT/LE/GT/GE/Select -> EQNode/NENode etc.* [REFACTOR][IR] Add Node suffix to Select/Call/Load/Ramp/Shuffle/Let* [REFACTOR][IR] Add node suffix to IntImm/UIntImm/FloatImm/StringImm* [REFACTOR][IR] Add Node suffix to Any, AttrStmt, AssertStmt* [REFACTOR][IR] Add Node suffix to Store/Provide/Allocate/Free* [REFACTOR][IR] Add Node suffix to ProducerConsumer* Fix lint* style updates, test fixes",0
[CONV] Reduce data size of asymmetric padding testcase (#4658)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>,1
[CI] Recover Windows Mac Build CI via Github Actions (#4662)* [RUNTIME] Fix windows build after the latest dso module change.Switch to shared_ptr to get around a problem in latest MSVC.* [CI] Add github action for win mac build.,0
[Autotvm] Use VM compile to extract autotvm tasks (#4328)* [AutoTVM] Use vm compile in extracting task from relay* update* restructure vm compiler to reduce task extraction time* x* fix* update doc* udpate doc* lint,0
[Relay/Topi][Op] 1D Pooling (#4663)* Added 1D pooling to Topi* Added 1D pooling relay op and tests.* Added onnx parsing and tests for maxpool1d and averagepool1d* formatting* moved partial import.* Fixed typo.,0
[REFACTOR] relay::Module Def -> TypeDef (#4665)* [REFACTOR] relay::Module Def -> TypeDefThe term Def was not very clear about what is the object of interest(could be function def or type def).Changes the term to TypeDef to be more explicit.* Update include/tvm/relay/module.hCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>Co-authored-by: Wei Chen <ipondering.weic@gmail.com>,1
"[Relay][Frontend][TFlite] Add parses support for unary elemwise ops (#4634)* [Relay][Frontend][Tflite] Add parses support for unary elemwise ops* Add generic method to convert unary functions: abs, exp, ceil, floor  log, sin, cos, sqrt, rsqrt, neg* Add relevant tests* Delete excessive underscores as requested in PR review* Change parameter name as suggested in PR review",1
Use int for endch to fix portability issues regarding signed/unsigned char (#4668),0
"[REFACTOR][IR] tvm::Expr -> PrimExpr(Primitive Expr) (#4669)* [REFACTOR][IR] tvm::Expr -> PrimExpr(Primitive Expr)As part of unified IR, we will need to unify relay::Exprand the current tvm::Expr under the same base type.From the techinical point of view. tvm::Expr is a ""primitive""expression that only contains POD types and handles and doesnot do life-cycle management.This PR renames Expr->PrimExpr to clarify that.We will send a subsequent PR to introduce the base expr class.* Remove legacy VarExpr and ExprHash/Equal",2
download fallback config file for search from tophub if it does not exist (#4671),2
Added pool autopadding and simplified parsers. (#4672),1
[VTA] Update docker for TSIM based simulation (#4674),1
[CodeGen] Generate blob use LLVM directly (#4657),5
[CI] Update deps for chisel (#4675),1
"fix topi.nn.global_pool layout=""NHWC"" (#4656)* Update topi.ccfix topi.nn.global_pool layout=""NHWC""* add topi.nn.global_pool layout=NHWC test",0
[CI] Bump to use the new cpu image (#4677),1
Also package core.rly (#4679),5
GitHub actions/checkout@v1 --> v2 (#4680)https://github.com/actions/checkout/releases,5
os.path --> osp to match the import (#4681),5
[Relay][Frontend][TFlite] Add parses support for SLICE (#4502)* [Relay][Frontend][TFlite] Add parses support for SLICE* TFlite 1.13: convertor gives nonsense output when size[i]==-1* TF parser: SLICE need fixing for size[i]==-1 -> gives wrong output  bcs of indices* Set end[i] = input_tensor_shape[i] as suggested in PR review* Add another test to cover size=-1 case,0
"Fix Python syntax error in start_rpc_server_to_tracker.py (#4682)[flake8](http://flake8.pycqa.org) testing of https://github.com/apache/incubator-tvm on Python 3.8.0$ __flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics__```./apps/vta_rpc/start_rpc_server_to_tracker.py:18:18: E999 SyntaxError: invalid syntaxPROJROOT=""$( cd ""$( dirname ""${BASH_SOURCE[0]}"" )/../../"" && pwd )""                 ^```",0
"[Bugfix] fskip of EliminateCommonSubexpr cannot always return false (#4620)* 'fskip' will not always return falsefskip returns false at the end of PackedFunc, discards return true in 'cast' case* Update build_module.cc",0
[Relay][TOPI]Fix meaning of conv2d_transpose output_padding parameter (#4318)* Add output_padding to generic* Add output_padding to the reference impl* Add output_padding to arm_cpu* Add output_padding to the test* Add output_padding for cuda* Add output_padding for x86* Make use of the new output_padding argument in Relay* Adjust conv2d_transpose Relay test* Fix lint errors* Fix the VTA declaration of conv2d_transpose* support for output padding in conv2d transpose* some output padding will break IR pass* Fix new conv2d_transpose test* Update tophub* Fix conv1d output_padding too.* Fix the conv1d_transpose reference function.* Fix the cuda impl* fix the topi test for conv1d* Update the versions in tophub.pyCo-authored-by: Thierry Moreau <tmoreau@octoml.ai>,0
[REFACTOR] Replace TensorObj and TensorValue with NDArray (#4643)* replace TensorObj and TensorValue with NDArray* NodeBase to Object in Python* rebase,5
"[REFACTOR][IR] Initialize Unified IR Expr Data Structure (#4673)This PR moves a few base types from relay and low-level Expr into the ir sub-folder.These classes will serve as a common type system across the stack.Rationale:- PrimExpr for low-level expressions- RelayExpr for advanced features, including Function definition.- Introduce BaseFunc to host all functions, including future PrimFunc(low-level expr functions, subject to discussion).This is a minimum change we can do to unify the classes into a common hierarchy.The main data structure that are variant specific will still be kept in the sub-namespaces.We only include classes that is needed to allow a common Module class.- BaseFunc- GlobalVar- Type definition part of ADTWe will only need the BaseFunc and their checked_type to decide the calling conventionacross the function variants.",4
[TOPI][RELAY][OP] add op crop_and_resize (#4417)* [TOPI][RELAY][OP] add op crop_and_resize* fix pylint* incorporate comments* fix ci,0
Fix Python syntax error AGAIN in start_rpc_server_to_tracker.py (#4685)#4682 Tried to fix a Python syntax error but did not go far enough because there are _three sets_ of embedded quotes.This PR solves the syntax error by using Python's triple quoted strings on the outside and then double quotes in the middle and then single quotes on the inside.,0
"Use ==/!= to compare str, bytes, and int literals (#4686)Identity is not the same thing as equality in Python so use ==/!= to compare str, bytes, and int literals. In Python >= 3.8, these instances will raise __SyntaxWarnings__ so it is best to fix them now. https://docs.python.org/3.8/whatsnew/3.8.html#porting-to-python-3-8% __python__```>>> dtype = ""float"">>> dtype += ""16"">>> dtype == ""float16""True>>> dtype is ""float16""False>>> 0 == 0.0True>>> 0 is 0.0False```",0
"[REFACTOR][IR] Allow Module to store BaseFunc. (#4678)Under the unified IR. We will allow a single IRModuleto store different function variants, such as relay::Function,ExternFunc, and low-level function.This PR changes relay::Function -> BaseFunc in the module fileto support multiple function variants.",2
"Update and rename start_rpc_server_to_tracker.py to start_rpc_server_to_tracker.sh (#4689)This is a shell file, not a Python file.",1
[Tutorial] Deploy Quantized Model on CUDA (#4667)* [Tutorial] Deploy Quantized Model on CUDA* update* update* address comments,1
"[REFACTOR][IR] Unified IR Primitive Op and Registry (#4687)This PR migrates relay's Op into the ir folder.Op and its registry provides an useful mechanism tostore any attribute meta-data of an operator includefunction signatures, lowering rules, side effect etc.These features are not only useful for Relay, but also needed in the low-level IR.At the current moment, intrinsic functions in the low-level IR are simplyrepresented by a string. This means we cannot type-check the low-level IRwhen the type does not meet the constraint, nor can we obtain furtherinformation such as side-effect and read write relation of these intrinsicswrt to arguments.Op will be used as the way to handle primitive ops(in DL terminology)(builtin intrinsics or in compiler terminology).We will perform follow-up refactors to make low-level CallNodetake Op as the function argument.",2
[Relay/Topi][Op] Conv1D (#4639)* added conv1d operators to topi.* Started to add python testing.* Added python conv1d implementation for testing.* Wrote test but need to add cuda schedule :(* Cuda schedules working for both conv1d layouts.* All topi tests passing.* Formatting topi.* Removed pad_method option as its probably overkill.* Added relay op definition of conv1d.* End2end conv1d working with onnx.* Lint fixes.* Formatting fixes.* Rebase fix.* Switched to array based attributes for consistency across convs.* Improved onnx parsing and testing for convolutions.* lint fix* Tiny tweak.* Bug fix* Rebase fix.* Add group ignore to onnx conv1d frontend.* Unified MakeConv and fixed documentation.* improved autopadding* Addressed feedback and simplified onnx frontend.* Format fix.* Basic X86 NCW schedule working.* Added nwc schedule.* fixed name* Added more tests and basic x86 schedules.* Format fix.* Added non power of two shape tests.,0
"GitHub Action lint Python code for syntax errors (#4688)* GitHub Action lint Python code for syntax errorshttps://flake8.pycqa.org/en/latest/user/error-codes.htmlOn the flake8 test selection, this PR does _not_ focus on ""_style violations_"" (the majority of flake8 error codes that [__psf/black__](https://github.com/psf/black) can autocorrect).  Instead these tests are focus on runtime safety and correctness:* E9 tests are about Python syntax errors usually raised because flake8 can not build an Abstract Syntax Tree (AST).  Often these issues are a sign of unused code or code that has not been ported to Python 3.  These would be compile-time errors in a compiled language but in a dynamic language like Python they result in the script halting/crashing on the user.* F63 tests are usually about the confusion between identity and equality in Python.  Use ==/!= to compare str, bytes, and int literals is the classic case.  These are areas where __a == b__ is True but __a is b__ is False (or vice versa).  Python >= 3.8 will raise SyntaxWarnings on these instances.* F7 tests logic errors and syntax errors in type hints* F82 tests are almost always _undefined names_ which are usually a sign of a typo, missing imports, or code that has not been ported to Python 3.  These also would be compile-time errors in a compiled language but in Python a __NameError__ is raised which will halt/crash the script on the user.* Force a retest* Rename start_rpc_server_to_tracker.py to start_rpc_server_to_tracker.shThis is a bash file, not a Python file.",0
[REFACTOR][IR] Unified IR IRModule structure. (#4699)This PR brings relay::Module as the unified IRModule structure.IRModule will be used as the basic unit for transformationsthrough out the stack.- Rename relay::Module -> IRModule- Move relay/module.h -> ir/module.h- ModuleNode::FromExpr -> IRModule::FromExpr- FromText -> IRModule::FromText,3
[VTA] Fix an issue in updating uop_idx in the TensorGemm module (#4694),0
fix RemoveUnusedFunctions pass,0
[REFACTOR][IR] Move error.h into ir (#4701)We will use a single ErrorReporter to report errors duringprogram transformations.,0
[REFACTOR][IR] Initialize Unified IR Pass Infra. (#4702)Move the relay's pass Infra to ir.Keep FunctionPass in relay as it is local to the dialect.,4
[relay] Relay annotation and partitioning for external compilers (#4570)* [relay] Relay annotation and partitioning for codegen* Add fusion unit test* fix comments* Update include/tvm/relay/attrs/annotation.hCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>* rebase* remove annotation helper* rebase againCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: 雾雨魔理沙 <lolisa@marisa.moe>,0
[REFACTOR][IR] Polish ir/type (#4705)- Use consistent constructor style to construct objects.- Move env_func to ir as it is mainly used to construct IRs.- Make docs consistent.,1
"[REFACTOR][IR] Unify IntImm and UIntImm (#4706)* [REFACTOR][IR] Unify IntImm and UIntImmThis PR unifies UIntImm and IntImm to simplify the codebase.Unsigned integer constants will also be stored as IntImm.For uint constant that does not fit into int64(rare case), we introducedan intrinsic tvm_big_uint_imm to construct such intgers by itslower and higher 32bits.* [REFACTOR][IR] Remove UIntImm to use IntImm* rename big->large",2
use packed func macro for external codegen (#4710),5
"Revert ""[Relay][TOPI]Fix meaning of conv2d_transpose output_padding parameter (#4318)"" (#4708)This reverts commit dcf7fbf1f962569e78c624755b2d612fffa81ada.",0
link the math library by default (#4713),5
[Relay][Frontend][TF] fix _parse_param bug (#4711),0
[Relay][Frontend][TFLite] Add constant input support for elemwise ops (#4666)* [Relay][Frontend][TFLite] Add constant input support for elemwise ops* modify in tflite.py,1
[REFACTOR][IR] attrs.h -> ir (#4709)This PR moves attrs.h into the ir folder as itcan serve as a common infra for building ir dats structures.We also moved common container(FloatImm) into ir/expr.h,4
"[REFACTOR] Move support related code to include/tvm/support (#4716)* [REFACTOR] Move support related code to include/tvm/support- tvm/logging.h -> tvm/support/logging.h- remove tvm/base.h, move with into tvm/support/with.h* src/common -> src/support",2
[REFACTOR][FFI] Make more clear naming for C API Type codes. (#4715)This PR introduces more clear naming prefix for C API type codesto avoid conflict with other packages.We also removed TVMArray and TVMType to directly use DLTensor and DLDataType.,0
[VERSION] Update mainline version to 0.7.dev0 (#4720),1
[REFACTOR][IR] Introduce include/tvm/target (#4721)As part of Unified IR infra.Introduce target folder to store all the compilation target related information.,5
[Arith] add SizeVar representing non-neg valued variable in a tensor shape (#4684)* [arith] add ShapeVar representing non-neg valued variable in a tensor shape* bounder remover; deal with div in int_set differently* fix bounder_remover* migrate unittest to use shape_var* use tvm.shape_var in integration & relay tests* add test case; fix Var register* fix lint* fix lint again* add default ShapeVar visitor in Relay* fix override* fix ShapeVar visit bug* revert IntervalSet for shape_var* remove bound_remover* remove is_var; use constructor for shapevar/var instead* ShapeVar -> SizeVar; add constructor comments* shape_var -> size_var in doc* tindex -> size,0
[COMMUNITY] @FrozenGene -> committer (#4719),3
[Relay][Frontend][TFLite] Add parser support for squared difference (#4652)* [Relay][Frontend][TFLite] Add parser support for squared difference* fix some error* fix exp_type* add comment,0
[CPP RPC] Fix the compile problem of cpp_rpc (#4725),0
[Relay][Op] Add type check to dense (#4724),1
"[REFACTOR][ARITH] Unified IR, introduce arith subfolder. (#4722)Spread the arithmetic.h into several components and moveinto arith subfolder.The arith namespace will be used for arithmetic expressionpattern detections and simplifications.",4
[Runtime] EdgeTPU runtime for Coral Boards (#4698),5
[Docs] Bring Your Own Codegen Guide -- Part 1 (#4602)* BYOC tutorial: codegen C* Address comments* Address comments* Add build option* Address comments* Use TVM_DLL_EXPORT_TYPED_FUNC,1
"[REFACTOR]  top - namespace for Tensor Operation DSL (#4727)* [REFACTOR] introduce top - Tensor Operation DSL.Historically we put Tensor, Schedule and compute under the root tvm namespace.This is no longer a good idea as the project's scope grows largerthan the tensor operation DSL.This PR introduces top -- a namespace for tensor operationalDSL concepts such as schedule, tensor, compute.We moved the related files to the new top subfolder.* Move relevant files into include/tvm/top and src/top",1
[Docs] Convert Layout pass. (#4664)* [Docs] Convert Layout pass.* Address comments. Section 3 massaging.* Address comments.,1
"[REFACTOR] Polish runtime (#4729)- Remove operator bool from base object ref macro  - Raitionale: operator bool can be dangerous for sub-classes    that also overloads other operators(e.g. ==).  - If bool is still needed, use explicit operator bool.- Use absolute include when necessary- Move type related util to data_type- Isolate stackvm code from compiler",1
[Relay] Invoke tvm::build from relay compile_engine and interpreter (#4723),5
export builtin_fp16 on Windows (#4731),5
[VTA] Update Jenkinsfile for VTA test with TSIM (#4734)* [VTA] Update Jenkinsfile for VTA test with TSIM* duplicate task_python_vta.sh multiple copies for now,1
[QNN] Conv2D type checking for kernel per-channel scales. (#4732)* [QNN] Conv2D type checking for kernel per-channel scales.* Address commments.* Address comments.* - Adding safety checks for downcasts.Co-authored-by: shoubhik <shoubhikbhatti@gmail.com>,1
[TOOLS] JSON upgrader to upgrade serialized json. (#4730)During Unified IR refactor we will change the structure of IRs.This will cause certain historical modules stored via json no longerable to be loaded by the current version.This PR introduces a backward compatible layer to try its best effortto upgrade json from previous version(this case 0.6) to the current version.We mainly aim to support update of high-level ir(relay).,1
[x86 schedule] Fallback schedule for Int8 depthwise. (#4733),5
[REFACTOR] Get rid of packed_func_ext. (#4735)Move the conversion extensions to the specific class definitionsso that we longer need to include packed_func_ext.,4
[VTA][TSIM] Enable TSIM CI Testing (#4407)* Update task_python_vta.sh* install sbt=1.1.1 with apt-get* update verilator_opt* install verilator with major version 4.0* disable multi-threading for now* bug fix for correcting uop fetch address in LoadUop module* bug fix for correcting uop fetch address in LoadUop module* adjustment to read from dram_offset* enable USE_THREADS with verilator 4.x* DEBUG: try avoid core dump with verilator 4.x* bug fix in LoadUop module* log mega cycles in tsim* download cat.png to avoid fetching in each run* bug fix in LoadUop module* solve dram_even/sram_even issue* bug fix* introduce scalalint in ci* speedup tsim in ci* bug fix* lint scala code before building* disable multi-threading* split fsim/tsim script* update Jenkins settings* duplicate task_python_vta_fsim.sh as task_python_vta.sh for nowCo-authored-by: Thierry Moreau <tmoreau@octoml.ai>,0
[CodeGen][CUDA] Improve CUDA vectorizer (#4736)- Fixes issues to enable fp16 vectorizer. Now correct packing and  unpacking CUDA code will be emitted. Enabled more unit tests.- Do not emit code to read the first lane from an undef variable  int _3;  _3 = _3 & ~(0x000000ff << 0) | ...  and emit the following code instead:  _3 = (((0x000000ff & (_1 >> 0))+(0x000000ff & (_2 >> 0))) << 0);  Note that nvcc 10.2 is forgiving and emits the same code for both cases.  A warning appears in test_codegen_cuda.py.Signed-off-by: Wei Pan <weip@nvidia.com>,0
[runtime][refactor] Unify vm and interpreter objects (#4693)* unify vm and interpreter objects* move closure back vm* adt/closure back to vm.adt/vm.closure* closure base,4
Fix dense (#4728),0
"[REFACTOR] Establish tir (#4740)TIR is the new namespace for low-level IRfor tensor-level optimizations and loop transformations.This PR establishes the namespace and files.- lowered_func.h,buffer.h,data_layout.h -> tir/buffer.h,tir/data_layout.h,tir/lowered_func.h- ir.h -> tir/expr.h, tir/stmt.h- ir_functor_ext.h -> tir/expr_functor.h, tir/stmt_functor.h",1
Fix demo dockerfile build failed (#4744),0
"[REFACTOR][CODEGEN] codegen->target, build_module->driver (#4742)This PR moves the codegen related code into the target folder,as they are target specific functionalities.We also adopt the term ""compiler driver"" in common compiler infrasuch as rust, GHC and clang.As a result, build_module is moved into the driver folder.",4
Add CUDA conv2d for NHWC layout (#4737),1
[REFACTOR][TYPE] Finish move all types to IR. (#4746)* [REFACTOR][TYPE] Finish move all types to IR.- Move definition of Ref and TensorType to ir- Move type_functor.h to public header.- Rename RefType -> RelayRefType for clarity.* Add atol,1
Expose relay BindParamsByName to Python (#4751)* expose BindParamByName to python* fixed alpha equal test,0
"[REFACTOR] Establish printer in the source folder (#4752)* [REFACTOR] Establish printer in the source folder.As we move towards the unified IR, we will eventually want to build a unifiedprinters for both relay and TIR.This PR isolate the printer component into a separate folder in src as a first step.- Refactored the Doc DSL using Object, clean up APIs.- Isolate out the meta data into a header.- move printer into relay_text_printer, add comments about further TODos.* Rename NodePrinter -> ReprPrinter to distinguish it from other printers",1
[REFACTOR] top->te (#4759)Bring up namespace te -- Tensor expression language DSL.,5
[INFO] Add .asf.yaml for github info (#4761),1
[Docs] Bring Your Own Codegen Guide -- Part 2 (#4718)* BYOC Tutorial -- part 2* Fix comments* Address comments,0
"[REFACTOR] driver.h -> driver_api.h (#4760)""driver"" normally refers to the ""main"" function.Rationale: the header exposes set of APIs to drive compilationand should be named as driver api to best reflect its usage.",5
Fix padding in pooling op (#4738),0
Remove run_infer_type duplicates (#4766),4
Improve CUDA conv2d_transpose_nchw (#4762)- combine pad and dilate;- fix for the issue https://discuss.tvm.ai/t/compile-error-for-cuda-target/4164- fix for the issue https://github.com/apache/incubator-tvm/pull/4472,0
pooling.cc improvements (#4767),5
"[VTA] Support network which have no unique operator as start/stop name for graph pack. (#4703)* [VTA] Support network which have no unique operator as start/stop namefor graph pack.[Issue]  Current vta use 'start' and 'stop' name to define the pack start point  and end point, but this method not work for these network which have  no 2 unique operator as  start point and stop point.[Solution]  In this solution we give 2 addtional parameters start_name_indx and  stop_name_indx to make vta pack logic work with the said network,  for exampl for following networks which have no unique operator,  %0 = nn.add  %1 = nn.conv2d  %2 = nn.batch_norm  %3 = nn.leaky_relu  %4 = nn.add  %5 = nn.conv2d  %6 = nn.batch_norm  %7 = nn.leaky_relu  %8 = nn.add  with this solution we can use following parameter format to make  vta work on it.  relay_prog = graph_pack(                //....                start_name=""nn.add"",                stop_name=""nn.add"",                start_name_idx=0,                stop_name_idx=4)  to apply on new network, by printing the network we can get index information like following.  print(mod.astext(show_meta_data=False))  relay_prog = graph_pack(mod                          ...                          start_name=""nn.add"",                          stop_name=""nn.add"",                          start_name_idx=0,                          stop_name_idx=4)* address review comments and fix index count bugissue:when do print(mod), the output not only the Call is also have other typelike Var, need add logic to count all except meta.solution:add related logic* address review comments.* address review comments* add more detail comments.",0
[Doc] TVM_REGISTER_API -> TVM_REGISTER_GLOBAL (#4768),5
"Fix Tensorflow conv3d pad bug, add non-cubic data and kernel tests (#4772)",0
[TOPI] Remove cpp upsampling and resize op (#4769)* remove cpp upsampling* remove cpp resize,4
add missing nullptr check (#4773),1
[Bugfix][Frontend][TF] Fix incorrect calculations in tf SLICE (#4518)* fix formula for calculating end indices when size[i] == -1* add a test case for size[i] == -1* discard expanding dimension of begin_value & end_value since  it is needed only if you pass them as scalars not as tensors.* discard 'slice_tensor' variable so that implementation matches  the tf parser pattern,0
Bump prebuilt-image version in demo dockerfile (#4770),2
Update tune_simple_template.py (#4778)fixed a spelling mistake.,0
[Build] Explicitly link to cublasLt if it exists (#4776)* Explicitly link to cublasLt* Only link cublasLt if it's foundCo-authored-by: Jon Soifer <jonso@microsoft.com>,5
properly extract error type from windows error message (#4780)Co-authored-by: Jon Soifer <jonso@microsoft.com>,0
"[Relay][Frontend][ONNX] Broadcast condition, x, and y for Where op (#4774)* ONNX frontend broadcast condition* fix* fix styleCo-authored-by: Jon Soifer <jonso@microsoft.com>",0
Safe remove tmpdir (#4781),4
[PassManager] Implement pass manager tracing API (#4782)* Implement pass tracing API* Set is_before correctly* Add docs for trace function* Fix lint* Remove PDB* Ensure trace_func is set before calling* Fix conditional,0
[Python] Replace os.path.exists with try...except...else (#4784),5
[AUTOTVM] Fix a bug in generating the search space (#4779)- Do not use numpy.prod which ignores integer (64 bits) overflows.  This leads to an incorrect number of points in the search space.,0
Make sure to visit the arguments of inlined functions (#4783),5
"[Relay][Frontend][TFlite] Add add parser support for relational ops (#4695)Add support for: greater_equal, less, less_equal, equal, not_equalAdd tests for the elemwise relational ops",1
Fix parsing of different exception string formats (#4785),0
Dedup BindParamByName function in VM compiler (#4793),5
[Relay][Topi] Use SimplifyInference for L2 Normazlization. (#4795),5
Add schedule for conv3d NDHWC layout (#4775),1
[Relay] Expose vm OptimizeModule to Python (#4800)* Expose VM OptimizeModule to python* added missing imports* fix import,0
fix #4670: add bias for fc layer (#4801),0
[QNN] Doc fix on convolution and dequantize (#4799)* QNN doc fix on conv and dequantize* fix param name in tflite frontend* make different fix,0
[QNN] Conv2D with dilation support. (#4796),5
Change color channel from BGR to RGB for darknet preprocessing (#4794),4
[ThreadPool] Solve ARM BIG.LITTLE heterogeneous multicores (#4747),5
"[TIR] Create a StringImm reference type (#4806)This is motivated by the want to send anarray of strings across the python/C++boundary. Arrays only support ObjectRef typesand so can't carry StringImmNodes. This createsa string reference type, StringImm, which canbe used with tvm::Arrays.Change-Id: I598a44536c156b97dbfe3e9518e0a1f705da850c",4
"[TOPI] upsample operator 'NCHWinic' format support. (#4791)* [TOPI] upsample operator 'NCHWinic' format support.some hardware accelerator ask packed format data like NCHWinic to fit thehardware resource, here add upsample NCHWinic format support to helpsuch requirement.* address review comments, add assert for 'else must be NCHWxc' logic.",1
[LINT] Fix -Wextra (#4804)* [LINT] Fix -Wextra* Fix virtual-dtor,0
[DOCS] Fix vta tutorial (#4809),0
[AutoTVM] Minor bug fixes in AutoTVM for QNN graphs (#4797)* [AutoTVM] Minor bug fixes in AutoTVM for QNN graphs.* Bring back strided_slice.* Replace tvm.nd change.,0
fix memory leak (#4811),0
[TOPI][x86] Injective schedule improvement (#4786)* [TOPI][x86] Injective Schedule Improvement.* Add tiling.* Vectorize when there is an axis.,1
[REFACTOR][PY] tvm._ffi (#4813)* [REFACTOR][PY] tvm._ffi- Remove from __future__ import absolute_import in the related files as they are no longer needed if the code only runs in python3- Remove reverse dependency of _ctypes _cython to object_generic.- function.py -> packed_func.py- Function -> PackedFunc- all registry related logics goes to tvm._ffi.registry- Use absolute references for FFI related calls.  - tvm._ffi.register_object  - tvm._ffi.register_func  - tvm._ffi.get_global_func* Move get global func to the ffi side,2
allow customize mkldnn library location (#4814),5
"Mxnet parser for Qnn dialect (#4714)* - Additional util methods needed for mxnet frontend for qnn dialect.* - Fixing call to quantize.* [QNN] MxNet-MKLDNN parser support for QNN* [QNN] Relax conv check.* - Merge from origin* [QNN] Channel wise changes* [QNN] Dense changes* Dense fix for QNN ops.* - Removed non-mkl code from utils.- Small refactoring- Remove ""with_sum"" from conv- Simplified code* - Fixing ring buffer name.* - Fixing pylint issues.* - Fixing lint- Removing redundant commented code.* - Adding test cases- Removing unused methods.* [WIP] end to end test case for mxnet qnn parser* Changes to parse large CV models.* Pylint issues.* Fix Conv2D with sum and quantized pooling.* Reverting the changes made for mxnet-mkldnn test cases. Because of #4753, mxnet could not be updated to mxnet-mkldnn.Co-authored-by: Animesh Jain <anijain@umich.edu>",0
[REFACTOR][PY] Establish tvm.runtime (#4818)* [REFACTOR][PY] Establish tvm.runtimeThis PR establishes the tvm.runtime namespace that contains the core runtime data structures.The top-level API are kept inact for now via re-exporting.We will followup later to cleanup some of the top-level APIs.* Fix ndarray name,0
Fixed subprocess creation under windows (#4820)* fixed subprocess creation under windows this addresses  the issue #4819* Update server.py,0
"[Frontend][TFLite] Dynamically calculate input_stats of any fake_quant range (#4789)* [TFLite] Dynamically calculate input_stats of any fake_quant range* pass the input range to the convertor and caclulate (mean, scale) there* change the range of the second tensor in elemwise operations  so that we test inputs with different quant params* change the possible output range for elemwise ops wrt the updated ranges* update the comments for (m, s) calculations* add input range dict to reduce_mean op* Apply requested changes* add exception handling for zero division in input_stats* fix range of the input tensor in elemwsie",0
[QNN] Optimize lowering for requantize and FixedPointMultiply. (#4798)* [QNN] Optimize lowering for requantize and FixedPointMultiply.* Add check for requantize scale gt 1.* Added test case.,0
"[Relay][Frontend][TFLite] Add parser support for logical operators (#4642)* [Relay][Frontend][TFLite] Add parser support for logical operators* Add parser support for logical_and, logical_or* Add boolean dtype as a valid tensor type* BOOLEAN dtype is supported only from tf 1.15  so logical ops work only in that and newer versions* Logical_not is ommited since tflite can't convert it -->  throws errors for addv2* Add TFLite vesion check in tests for logical ops* Check is added because of boolean dtype lack of support",0
[Relay] Conv2D padding representation (#4787)* enforce 4-way padding* add util with get_pad_tuple* delete unnecessary arguments* fix lint* add container.Array case* fix cudnn conv2d asymmetric padding logic* rename get_pad_tuple to get_pad_tuple2d* revert change for topi/python/topi/nn/conv2d.py* add get_pad_tuple2d for several contrib conv2d ops* add get_pad_tuple2d for all conv2d ops,0
[CONTRIB][CC] Enhance cc.cross_compiler (#4817)* [CONTRIB][CC] Enhance cc.cross_compiler- Enhance cc.cross_compiler to take str argument.- Remove cc.build_create_shared_func as it is dupilicated with cross_compiler- Add examples to cc.cross_compiler* address review comments,1
[TOPI][Relay] Add bitwise ops (#4815)* Add bitwise ops to topi* Add the bitwise ops to relay.,1
It's gpu not cpu. (#4832),5
[CI][DOCKER] Update ci-gpu to v0.60 (#4827),1
[CI][DOCKER] Update ci-gpu torch1.4 and onnx1.6 (#4826),1
Fix doc after moving to unified IR (#4835),0
[Doc] Introduction to module serialization (#4564),5
[Frontend][ONNX] LSTM Support (#4825)* Initial version working and passing tests.* WIP on supporting other activations.* add support for multiple activation functions in lstm* All tests working and code cleaned up.* Undo import swap to avoid conflict with masahi.* Added new tests and related bug fixes.Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>,0
Improve tol to resolve flaky case (#4836),5
[Doc] ConvertLayout - Call RemoveUnunsedFunctions.,4
[Relay][Frontend][TFlite] Add support for quantized LOGISTIC (#4696)* [Relay][Frontend][TFlite] Add support for quantized LOGISTIC * add qnn implementation * add qnn test case for qnn logistic* Helper functions for quantize and dequantize.,1
[Frontend][TFLite] Add MIRROR_PAD operator (#4822),1
"[REFACTOR][PY][API-Change] Polish tvm.runtime, tvm.runtime.module API update (#4837)* [REFACTOR][PY-API] Polish tvm.runtime, tvm.runtime.module API updateThis PR updates the tvm.runtime to use the new FFI style.- Remove top-level tvm.module to avoid confusion between runtime.Module and IRModule- API changes wrt to runtime.Module  - tvm.module.load -> tvm.runtime.load_module  - tvm.module.enabled -> tvm.runtime.enabled  - tvm.module.system_lib -> tvm.runtime.system_lib- Remove dep on api_internal from runtime.* Update module.load in the latest API",1
[COMMUNITY] comaniac -> reviewer (#4841),3
[Doc][AutoTVM] Fix bugs that override n_trials (#4842),0
Fixed process termination routine in windows (#4844)* Fixed process termination routine in windowsaddresses and Fixes AttributeError: module 'os' has no attribute 'killpg' error in #4821* Update server.py,0
[TEST] test_cuddn flaky (#4846),3
[LINT][PY] Fixes for pylint==2.4.4 (#4849),0
[CI] Update ci-lint to v0.60 (#4850),1
[CI][DOCKER] Update ci-lint to pylint2.4.4 (#4851),1
[Frontend][TFlite] use qnn helper function in softmax (#4840),5
Fixed bug in ExprOp that caused bitwise operators to fail when a basic python type was on the left hand side of the expression. Added regression test for crashing cases. (#4852),0
"[Relay] Added Merge Composite pass (#4771)* [Relay] Added MergeComposite passThis pass allows for patterns to be wrappedin a function marked with 'Composite' and acomposite function name. This is intended to beused with the external codegen for the cases wherean external operator maps to multiple Relayoperators. In that case, the mapping can be expressedas a pattern and assigned a name.For more information on this pass and its motivation,see the RFC:https://discuss.tvm.ai/t/rfc-external-codegen-defining-composite-relay-operators/5470Change-Id: Icb1b803a9f0ac57c529143200228f3bb5793afc0* [Relay] Merge composite testsAdded tests for the merge_composite pass.Change-Id: I1728b4a05b0c1c36140a40f1afe028fde62185dd* Merge composite additional testChange-Id: I9bc7d6053c575e9468ac5abc31214c6ad8507e46* Support priority order in merge_compositeThe order in which the patterns are matchedwas currently random as an unordered_map wasused to store the pattern table. This usesarrays instead so that a distinct priorityorder of matching can be defined. Additionaltests have also been added to verify thisbehaviour.Change-Id: Ief347df4262639138d5d9d7c8cee7ef233af7b56* Improved merge composite docsChange-Id: Ie3a72045ecc3f13ad3c302fbdf192b7296a306a8* Removed unused variableChange-Id: I7814d5fde368ffaf1b3d6d806060c774c7720364* Remove unnecessary op checkChange-Id: I38e78d2acd5b86cb8e837be72ff9d72cd10bcf33* Improve styling on composite function creationChange-Id: I37add1c3134e0b5d5085fe1eb9daf8e06890fa8c* Comment rewordChange-Id: Ie05872dcbbe0c3e1190b0597083b9a64e6b66c66* Stylistic changes to avoid std::moveChange-Id: I43a93995bbf10530399900c992aa99dd4ae4575f* Relax a check in ExtractPatternChange-Id: I0faef77a66c55f83f09e6e47c561ffaea63dedfa* Remove new lineChange-Id: Ifdd02c12087a7e1a0a9b54825669bc0de8f13c3d* Removed MatchPattern from MergeCompositeThis is not necessary now that ExtractPatterncan fulfill the same purpose.Change-Id: I14dc020afa8e50f2df4c0a2efb88a011987f8196* Removed a new lineChange-Id: I8b50f0c9069aa1bcaccbe68eb421031f01a64842* Improved docs for merge compositeChange-Id: Ib1959a35c856e7ea5639de2e4ef314a54f44caf5* Fixed free vars in testChange-Id: I2b7f273db275964ec0e9820560663f0808adee79* Handle case where root arg might not be a callChange-Id: I4eeea3ce723d3ba337d110dcc690377daebe8626* Removed blank lineChange-Id: I07f5392c0e95cfe3cfa5c333703cc6f82d6034fb* Change to CHECK_EQChange-Id: I5c5d62d3cd57f72508b30b926f72091ae6f0d1cc* Revised a conditionalChange-Id: I23a7897ca15a7cd076db5039dc653a4b8c27e803* Improved doc stylingChange-Id: I377f0a1c1ac70f3b8d7584b0c49bddc8c6c134ef* Fail extraction if vars conflictChange-Id: I78e36d805e8ed6b55e61d490212a967c857554a4* Added further merge composite testsChange-Id: Ib1d800409fca4c1834c7fe0cab5a26ab99a26820Co-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>",0
reverse changes in pr #4849 (#4853),4
[TFLite] Using real image for QNN testing. (#4816)* [TFLite] Using real image for QNN testing.* Setting seed for SSD mobilenet for fixed input.* Support quantized Pad op.* Remove unnnecessary line.* Ina comments.,0
[RUNTIME] Fix memory leakage of TVMByteArray (#4856),0
[LLVM] Explicit llvm::StringRef to std::string conversion (#4859),5
[TVM] const auto p -> const auto &p (#4861),5
add resize op converter (#4838),1
[Refactor] move vm.py under runtime and adt to runtime.container.py (#4855),4
Fix onnx import bugs (#4750)* Fix onnx import bugsFix onnx attributes of string type incorrect handlingMerge symmetric padding of Conv to symmetric form* Only merge symmetric padding for conv2d,0
[Topi] Missing header (#4865),5
"[REFACTOR][PY][API-CHANGE] establish tvm.ir, migrate corresponding files (#4862)* [REFACTOR][PY][API-CHANGE] establish tvm.ir, migrate corresponding relay files.This PR establishes tvm.ir and migrates the corresponding relayfiles into the new folder.API Change:- relay.Module -> tvm.IRModule* Update with ADT* Migrate transform* address comments* Migrate module* Migrate json_compact* Migrate attrs* Move LoweredFunc to stmt temporarily* temp migrate container* Finish migrate container",1
[DOCS][PY] Sphinx docs about tvm.ir,2
Fix optimize,0
[JVM] Update the runtime PackedFunc for module,1
[REFACTOR][PY][API-CHANGE] Establish tvm.targetMove the related target modules into tvm.target.API change:- tvm.target.current_target -> tvm.target.Target.current- tvm.datatype -> tvm.target.datatype,4
"[FRONTEND][TFLITE] Add support for TFLite_Detection_PostProcess (#4543)* [FRONTEND][TFLITE] Add support for TFLite_Detection_PostProcessThis adds support for the custom operatorTFLite_Detection_PostProcess which is commonly used inobject detection networks such as SSD Mobilenet. Itonly adds support for when use_regular_nms = False.Change-Id: I819b253c0eb6f0fa55da65d2634e09359b888828* Added a test for the tflite custom opChange-Id: Ie5baa092deae9a8bcffd2ebd9f6d346b90e58afd* Removed trailing commaChange-Id: Ib08f02b5f1a59a883048bfb36e4321152cd2e7f2* Added spaces between divideChange-Id: If1171fc03d211a809cedeb800804394972af4060* Formatted commentChange-Id: I3ce7e69b8d2c73aec57369c1c64ea1eec07f087b* Reduced line length in testChange-Id: I49eaafc3369070f8f3e85fbb965ad20972096c68* Set random seed for testChange-Id: I542a787d11422ea83c52147b2cb1144fcef0dd77* Fixes to styleChange-Id: I2971b8ecebe08c882b2481a99f67cfbe515e0b1f* Assert for incorrect number of inputsChange-Id: I393f3b3b62be73e427498d98456fb1d5a214e0af* Change comparison to pass lintingThe linter was updated, so I needed to fixa small style issue as a result.Change-Id: Ia3c954565a00de92e7fb1912eae9ed9875d60c7c",0
Optimize x86 conv3d_ndhwc  using data packing approach. (#4866)Add tuneable conv3d_ndhwc schedule,1
fix vm doc,0
Update docs/dev/virtual_machine.rstCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>,1
Update docs/dev/virtual_machine.rstCo-Authored-By: Wei Chen <ipondering.weic@gmail.com>,1
[REFACTOR][PY] Establish tvm.tir- Move related files into the corresponding location as in C++- Keep the top-level TVM API backward compatible to make minimum changes in topi,2
"[TOPI][CUDA] Enable vectorization on fp16 type (#4867)- This allows to better utilize the memory bandwidth- Note that not all cases are vectorized for fp16 datatype. For  instance, when the size is not a multiple of 1024, the inner loop  may be an expression that cannot be vectorized. In this case, a  small inner loop is still benefical for latency hidding.Signed-off-by: Wei Pan <weip@nvidia.com>",2
[QNN] More doc fix on quantize and convolution (#4874)* [QNN] Doc fix on quantize and convolution* update test,0
[QNN] Add support for per channel weight scale in dense op (#4880)* add test case for per channel dense* add unit arg in tflite frontend* update qnn legalize test* fix output dim index,0
[AutoTVM] Support range in index based tuners (#4870)* Support range in index based tuners* Address comments* Remove __*state__* trigger CI,1
improve antlr import error message (#4888),0
"[CodeGen][CUDA] Fix issues in cuda codegen (#4876)- Do not emit __shared__ etc. as part of type for casting- Fix fp16 reduction kernels with compiler errors:  ""no operator ""+"" matches these operands, volatile half + volatile half  This patch inserts casts to remove volatile type qualifier following  volatile loads (fp16 only). CUDA fp16 library headers should add  volatile member functions.- Update have_fp16 to include compute 6.1 GPUs, which do support fp16,  although their fp16 throughput is low. Updated tests.Signed-off-by: Wei Pan <weip@nvidia.com>",0
[Relay] Fix VM compiler for while loop with free vars  (#4889)* add additional switch to handle nested call node* Fix VM compiler for while loop with free var,0
[CI] Cleanup logfile before tutorial runs (#4896),2
Fix alpha_equal bug (#4897),0
"Update faq.md (#4893)various minor editorial updates - style, grammar, typos.",1
Fast exponent (#4790),5
[DOCS] Introduce how to add hardware backend to FAQ (#4898),1
[Relay][Pass] Fix bug in re-processing call node in MergeComposite pass (#4879)* Fix bug in re-processing call node* Add test* Add to main* temp changes to work from another machine* fix rest of tests* fix test_reuse_call_merge* fix mergeCo-authored-by: Jon Soifer <jonso@microsoft.com>,0
[REFACTOR][PY] Establish tvm.te and tvm.driver (#4900)- Move the related files to tvm.te- Move build_module.py to tvm.driver,2
"Fixed bugs that occured when using bitwise operators on floating point type expressions. Further crash when using ops <<, >>, %. Finally added regression tests for both types of bug. (#4892)",0
[CI] Update ci docker to add autodocsumm (#4903),1
[CI] Add autodocsum as dep (#4902),1
[REFACTOR][PY] Establish tvm.arith (#4904),5
[Relay][Frontend][Keras] NHWC import support. (#4899)* Basic test working* Almost all tests working.* all tests passing.* Fixed lint.* Improved Style.,0
[Relay] Expose FunctionGetAttr to Python (#4905)* [Relay] Expose FunctionGetAttr to Python* add testCo-authored-by: Jon Soifer <jonso@microsoft.com>,1
[DOCS] Update API docs to reflect the status after the refactor. (#4907),1
Fix tvm.target.generic_func runtime detection (#4910),0
[RELAY][FRONTEND][TF] Fix FuseBatchNorm output cast error if need_cast is True (#4894),0
"[REFACTOR] Polish ffi convention. (#4912)* [REFACTOR] Polish ffi convention.- Remove the src/api, keep registration local to the c++ function.- Remove the api_internal as it is no longer needed.* Update the codebase walk through",1
[DOCS] Fix sphinx warnings (#4917)* Fix Python docstrings* More fixes* Fix lint,0
[Relay] Fix an assertion exposed by loop vectorizer (#4916)- Allows uniform conditions for select expressions (the same as halide)  exposed by the loop vectorizer.Signed-off-by: Wei Pan <weip@nvidia.com>,0
"[DOCS] Fix Sphinx Warnings (RST indent, cross-ref, and image scale) (#4920)* fix indents* Fix image scale and cross-ref",0
[CODEGEN] Support cuda tensorcore subbyte int data type in auto tensorcore (#4546)* support cuda tensorcore subbyte int data type in auto tensorcore* add lisence* pass cpplint* fix code review comments* merge the int4/int1 codegen tutorial into the existing auto tensorcore tutorial* using master's new API* disable tuning when cuda is not enabled* address cr comment* do not run the tuning* fix test failure* fix cpplint error* fix bool type reduction bug* 1. fix a index bug 2. fix returned bytes value of int1/int4/uint4* fix typo,0
Fix tests for tflite unary elemwise operations (#4913)* add TFLite version check for 'ceil' and 'cos'* fix name check of test_op for positive inputs* add error message for operator not found in the installed fbs schema,0
[COMMUNITY] @anijain2305 -> Committer (#4921),3
[TEST][FLAKY] topi/tests/python/test_topi_sort.py::test_argsort (#4891)* [TEST][FLAKY] topi/tests/python/test_topi_sort.py::test_argsort* upadate test function of argsort like topk* Shuffle index and get data from shuffled index* Replace the random.uniform with np.arange,3
[Fix] Fix get_valid_count flaky test for cuda (#4901)* get_valid_count accuracy issue fixed for individual tests but not for all tests running together* minor fix* initialize valid_count and PrefixSum buffers* test updated* udpate relay test as well* update document* fix lint* address comment* fix lint* correct atomicAdd identifier name,0
"[Relay][AutoTVM] Relay op strategy (#4644)* relay op strategyfix lintbitpack strategybitserial_dense (#6)* update strategy* address commentsfix a few topi testDense strategy (#5)* dense* add biforst; remove comments* address commentRefactor x86 conv2d_NCHWc (#4)* Refactor x86 conv2d* Add x86 depthwise_conv2d_NCHWc* Add back topi x86 conv2d_nchw* Merge x86 conv2d_nchw and conv2d_NCHWc* Minor fix for x86 conv2dfix more strategyAdd x86 conv2d_NCHWc_int8 strategy (#8)* Add x86 conv2d_NCHWc_int8 strategy* Remove contrib_conv2d_nchwc_int8* Fix generic conv2d_NCHWc for int8* Fix topi arm_cpu conv2d_NCHWc_int8update x86 conv2denable specify relay ops to be tuned for autotvmadd cuda conv2d strategyadd conv2d strategy for rocmadd conv2d strategy for hlsadd conv2d strategy for arm cpuadd conv2d strategy for maliadd conv2d strategy for bifrostadd conv2d strategy for intel graphicsclean up and fix lintremove template keys from autotvmremove 2 in the func nameaddress commentsfix* fix bugs* lint* address comments* add name to op implement* Modify topi tests (#9)* Add pooling, reorg, softmax and vision* Add lrn* fix topi test* fix more topi test* lint* address comments* x* fix more tests & bugs* Modify more tests (#10)* Modify tests for bitserial_conv2d, bitserial_dense, bitserial_conv2d_rasp and bnn* Minor fix* More minor fix* fix more test* try to update vta using strategy* fix cpptest* x* fix rebase err* Fix two tests (#11)* change autotvm log format* lint* minor fix* try fix vta test* fix rebase err* tweak* tmp hack for vta pass* fix tutorial* fix* fix more tutorials* fix vta tutorial* minor* address comments* fix* address comments* fix cpptest* fix docs* change data structure name and api* address comments* lint* fix rebase err* updates* fix winograd test* fix doc* rebase* upgrade tophub version number* fix bug* re-enable vta tsim test after tophub is upgraded* fix vta test to use the correct args so the config can be found in tophubCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>",0
[FRONTEND][KERAS]GaussianDropout/Noise parsing support (#4928)GaussianDropout & GaussianNoise are active only during training time. This can be skipped during inference.,5
Use opencv reisze method for preprocessing of image in darknet (#4883)* Use opencv reisze method for preprocessing of image in darknet* Use opencv reisze method for preprocessing of image in darknet* Fix pylint issues,0
"[Relay] Add a PyTorch to Relay Parser (#4497)* Add a PyTorch to Relay parser* Add alexnet, googlenet, mnasnet, shufflenet wip* Fix lint* Remove fix for shufflenet* Lower check* Pull changes from neo-ai/tvm changes* Remove commented out section* Use infer_shape everywhere* Change back to using trace instead of path in from_pytorch* Parse state_dict to add param names* Umbrella single_op under test_forwards* Remove print and cleanup call* Check if update to test broke CI* Retrigger CI* Add back in updated tests* Try splitting up tests* First pass at flexible typing, implemented for ones* Add int32 for all ops* Remove print statements* Fix lint* Broad except* Add other tensor types* Temporarily use old tests* Retrigger CI* Lower type names* Use numpy to convert in dense op* Fix lint* Remove print* Need to cleanup but verify int32 works for add* Rough tests for different types, a lot of types are not supported on CPU* Probably doesn't build, need to save work as I have to switch branches (constantly)* Parse param type* Remove print stmt in parser* Clean up some code* Working on flaot32 for bn* Add resnet18 double type* Fix lint* Temporarily move PT tests first* Temporarily add back refactored tests to fix mem issue* Add more type test and temp remove some tests* Comment out tests, hopefully CI prints a trace* Get stack trace* Remove operator dict key, rename op_name to node_id, remove dead code* Make relay map a list* Remove some hacky string stuff* Move to PyTorch 1.4* Remove input_type as param* Remove _get_fill_value, fix full ops* Remove unused code and combine ops for identity and none* Remove fn_param* Clean up main loop* Remove useless if/else for outputs* Remove ir_names, only used once* Remove some string hacking* Remove string parsing to get output name* Fix bug with output sizes of nodes* Use attributeNames in parse ops* Remove continue and add_op in parse_op* Do this everywhere, use assert instead of explciitly type casting* Remove unnecessary swap* Slight refactor for elemwise input parse* Use a copy of graph everywhere* Rename nid_to_node_name* Refactor parse import prereqs* Clean up input node kind check* Clean up conditionals* Clean up add_op* Cleanup type for ones and zeros op* Fix lint* Add torch install to CI* Actually use torch* Try moving import torch to only where it's needed* Import torch for CI* Use take op for select* Temporarily add ignore for jit inline pass for CI* Use CompleteTensorType, might be a PT 1.2 only thing* Use different types in elemwise op* Use float16 ones* Fix float16 test* Remove the temp docker changes* Remove temp test* Temporarily comment out original tests* Remove file* Empty cache after each test* Add some prints and lower input sizes* Try using no grad* Trying to globally set grad off* Use no grad for torchvision* Remove xfail tests* Remove VGG and AlexNet due to some issues* Combine pooling tests* Remove extra test file* Remove single op, remove larger pooling tests* Remove maxpool3* Remove debug prints* Remove inference call and add no_grad in measure latency* Use standard string start char* Remove redundant infer_shape in slice* Convert most to checks to just expr* Remove extra paren* More refactor of isinstance* Add helper for creating typed constants* Assert instead of return when no matching type* Remove network variants* Add no_grad when forward, remove deatch, fix lint* Change isinstance to expr in transpose* Use opnotimplemented, refactor* Fix full ops, remove duplicate tests* Never use shape field unless we know the type* Remove comma, retrigger CI* Add paren, retrigger CI* Use inline if-else for flags* Throw exception instead of assert* Remove version check for CI* Check version when doing inline pass* Fix lint* Lower more input sizes* Add new line, conv2d only accepts weight as expr* Use tvm.runtime.ndarray* Remove change to torch version install* Try no grad for mobilenet* Fix lint* Fix lint again* Revert to last passing* Delete test files* Ignore lint* Revert back* Comment out mobilenet* Clean up compare compiled and baseline outputs* Use IRModule* Add todos* Refactor use_bias* Add todo for fix conv op channels* Change input to data type* Remove todo* Handle channel multiplier > 1",0
[Relay][External Codegen] Support data types for CSourceModuleCodegen args and output (#4934)* Support int args and no extra buffers* Fixes* remove testing code* fix style* more style* use const args* styleCo-authored-by: Jon Soifer <jonso@microsoft.com>,0
[LLVM] Fix build breaks from StringRef changes (#4923)- llvm::StringRef to std::string conversion is explicit now.Signed-off-by: Wei Pan <wpan11nv@nvidia.com>,0
[Fix] remove unnecessary spliting in the cached chunk (#4935)* remove unnecessary spliting in the cached chunk* remove unnecessary spliting in the cached chunk,0
[WIP] Fixing an Infinite Loop case in UnmatchedChecker. (#4881)* save* save* remove* remove cerr,0
"Tensor Expression Debug Display (TEDD) (#4651)* Initial TEDD for publishing.* 1. Fix lint issues. 2. Print intrin.body instead of intrin.name in Schedule Tree.  3. Add examples to top level APIs' comments.  4. Top level APIs don't print Dot string by default, unless outputdotstring is True.* Fix more lint issues.* Update top level API argument names and use raw strings to avoid Python lint warnings in the tests.* Disable TEDD verification, but keep TE construction.* Stop importing tedd to avoid failure.* Separate data extraction and visualization. 1. Add API tedd.dump_json(schedule) to dump a json string for the schedule data for visualization.  2. Update tests.  3. Add a tutorial.  4. Add range information to IterVars.* Update TEDD about InferBound failure.  1. TEDD doesn't call inferbound for DFG. 2. Update tutorial about the InferBound failure.* 1. Import IPython only if SVG is requested.  This is required to fix a tutorial publishing faliure.  2. Fix test about IPython availability check.",0
[DOCS] Fix Sphinx Warning: the target found for cross-reference (#4925)* [DOCS] Fix Sphinx Warnings: the target found for cross-reference warnings* Fix the warning: undefined label,0
Bump up dev version (#4941)* bump up dev version* update,1
"[Tutorial] Add a tutorial for PyTorch (#4936)* Add a tutorial for PyTorch* Fix sphinx formatting, add version support* Remove space* Remove version check* Some refactoring* Use no grad* Rename input* Update cat img source",0
"[Relay][pass] call graph for relay (#4922)* call graph for relay* CallGraphEntryNode->CallGraphEntry, __getitem__->print_var* fix typos",0
Remove SGX toolchain installation from CI Dockerfile (#4948),2
[Frontend][TFLite] Add parser support for 'square' operator (#4915)* [Frontend][TFLite] Add parser support for square operator* Add parser implementation* Add relevant tests* Note: 'square' is an unary elemwise operator but it's added separately  in the parser since there is no Relay 'square' op  and instead we have to use 'multiply'* Change relay operation from 'multiply' to 'power'* Remove a redundant line as requested,1
"[VTA] YoloV3 Support (#4887)* [VTA] YoloV3 SupportIssue:YoloV3 use some operator and logic that not get good support byexisting vta logic, like nn.pad, upsample, and 255 output channel.Solution:add related logic to let darknet YoloV3 can running on VTA* Fix small(0, or 1 heigh/width) detect frame issue.* add yolov3-tiny turtorial* add os import* address review comments.* rename tutorial file with a short name.* rename deploy_vision_on_vta.py into deploy_classification.py.* address review comment, fix plint eror in deploy_detection.py",0
"[TUTORIAL] Fix tedd tutorial after strategy change (#4947)* [TUTORIAL] Fix tedd tutorial after strategy change* Remove scale, remove link to external gdoc",0
[REFACTOR][PY][API-CHANGE] Remove legacy python files. (#4943)* [REFACTOR][PY][API-CHANGE] Remove legacy python files.Remove legacy python files.Use the te namespace for most of the tensor expression primitives.- tvm.create_schedule -> tvm.te.create_schedule- tvm.placeholder -> tvm.te.placeholder- tvm.compute -> tvm.te.compute* Remove top-level exposures.,2
[RELAY] fix error message (#4945),0
[Frontend] [MXNet] make_loss operator support (#4930)* make_loss test case* mxnet frontend make_loss support* added comment for make_loss* pylint fix* Update mxnet.py,0
Move Ops in relay.op.contrib.* (#4942)* move contrib* lint* address comment* address comment,1
[Runtime] Fix TVM_DLL_EXPORT_TYPED_FUNC to work on Windows (#4955)* [Runtime] Fixed TVM_DLL_EXPORT_TYPED_FUNC to work on Windows* fix styleCo-authored-by: Jon Soifer <jonso@microsoft.com>,0
"[DOCS] Sphinx -- Introduce alias detection. (#4954)* [DOCS] Sphinx -- Introduce alias detection.Background: some of our namespaces import function from anothernamespace. For example tvm.te imports most of the operators from tvm.tir.Previously we manually exclude these aliases from the doc.However that means we can not link them by the alias name.This PR adds a sphinx callback plugin to detect such aliases, and create a rubric blockon the button of its current docstring `Alias of the original class`.It is done in a way so that we can refer to the generated docs.We also fixed a few docs errors.* Fix most of the issues",0
fix doc warning (#4959),0
"[CI] Add pre-check script to check sphinx doc build. (#4956)Introduce the check stage to the unittest stage for nowso we don't have to rebuild CI images.As we make additional CPU images to make use of the sphinx,consider move it to an earlier stage.",1
"[Relay, Torch] Clean up and refactor PyTorch frontend (#4944)* The initial import of refactored implementation, all tests passed* enable mobilenet v2 test* minor cleanup* reorg* fix lint* use input names that come with torch IR* fix typo* introduce parse_operators* fix lint* add _ prefix",0
[DOCS] Fix sphinx precheck (#4967)* [DOCS] Fix sphinx precheck* ignore keras warnings* Remove more warnings,0
[Frontend][TFLite] Add parser support for l2_normalization (#4966)* [Frontend][TFLite] Add parser support for l2_normalization* TF doesn't provide uint8 support* TFL does the normalization only if it's over the last axis* TFL uses only the default value for expilon* Change error message,0
Added CopyFromBytes and CopyToBytes convenience methods to NDArray.  Fixed typos. (#4970)* Added CopyFromBytes and CopyToBytes convenience methods.  Fixed typos.* Removed unneed argument check* Use TVMArrayCopyFrom/ToBytes methods* Moved CopyFrom/ToBytes to ndarray.cc* CopyToBytes impl was using CopyFromBytes.  Fixed* changed inline to TVM_DLL* Used impl from TVMArrayCopyTo/FromBytes into NDArray CopyTo/FromBytes* Move implementation of all CopyFrom/ToBytes into a common impls* make arg const* simplify method impl,0
"[Torch] Upsampling op support and enable registering a user defined op conversion map (#4961)* add custom conversion map* add roi align test using custom convert map* refactor test* add support for upsampling op and test on segmentation models* remove redundant no_grad* add upsampling test case* make the default custom map None, instead of empty dict* updated tests, remove packaging and drop PT 1.2 support* add better support for aten::to and tests* add a note on dilation in x86",1
[TOPI] fix docs errors (#4973),0
[Relay][FastMath] Relay pass to use fast exp/tanh (#4873)* [Relay][FastMath] Relay pass to use fast exp/tanh* Adding required_pass to the tests.* FastMath test changes.,1
[TFLITE]FLOOR_MOD & FLOOR_DIV support (#4971)* TFLite Floor_div & floor_mod parsing code* Review comment updated,1
[Doc]refine the example description of max/min/sum/tag_scope (#4974),5
[Relay][Pass] Add inline pass (#4927)* add inline pass* IsInline -> IsMarkedInlined* fix comment,0
[Frontend] [Tensorflow] ReadVariableOp operator support (#4952)* tf frontend read variable op* pylint fix* tf frontend freezed graph pruned ops,0
"Pin xgboost dependency version to 0.90 (#4965)* Sets xgboost dependency to be 0.90, preventing   segfaults during TVM python unit tests execution * This is discussed in issue #4953",3
[Relay] Target annotation for external codegen (#4933)* op based external compiler annotation* Use TVM register directly* Small fix* test graphCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[Torch] fix unordered dictionary problem for python version under 3.6 (#4982)* fix unordered dictionary problem for python version 3.5* modify style,0
"Tighten split's extent (#4931)* Set split node's range to minimum of ext and split factor or split nparts, but only when PassDownDomain is called with allow_missing == false, i.e. by InferBound.  Add a helper PassUpThreadBinding() to get a map telling whether an IterVar has at least one leaf IterVar deriving from it binding to a thread. Add two unit tests.* Enhance LoopVectorizer for vectorizing by 0.  Found at least one case from testtopi/tests/python/test_topi_transform.py::test_tile.* Revert changes vectorize_loop.cc; when parent's ext is zero, set split's range to the factor or nparts.* Update with comments.* Refactor the ext tightening predicate.* Fix reference types.* Integrate tvm.te changes.* Trivial comment change to trigger CI.* Trivial comment correction to trigger testing.",0
"[Torch, QNN] Add support for quantized models via QNN (#4977)* qnn support initial import* fix upsampling num input* imagenet tests added* add qunatized module tests* quantized module tests working* imagenet test working* fix lint* remove top level torch import to fix ci error* disable lint warning on outside toplevel import* revert parse -> convert change* add comments to qnn translation* address comments, add sample outputs* add more comments* refactor bias add and requantize step",0
Fix gpu not found when running TVM docker (#4975),0
Conditions updated to cover better user scenarios (#4951)* Conditions updated to cover better user scenarios* [1] New test case added* [2] New test case added* [3] Proper variable name used* [4] Review Comments handled* [5] Review comments handled* [6] Review comments handled,1
refactor build module to take IRModule (#4988),5
hotfix gcn tutorial fail (#4994),0
Adding Hua Jiang as reviewer. (#4993),1
"[topi][relay] add operation tan to TVM (#4938)* Add relay operation relay.op.tan.* Update tan implementation in TVM.* Update tests.* Add shape function for tan.* Add missing main test to python/frontend/tensorflow/test_forward.* Revert, back to sin/cos.* Revert ""Revert, back to sin/cos.""This reverts commit 4da5b503b921585ba9d80944b29136142b575c40.* Fix implementation of tan in cuda. Do not support tan for float16.Simplify topi/tests/python/test_topi_math. Add testing for tan with float32 and float64.Try again to implement tan as sin/cos in llvm.",0
[Runtime] Export GraphRuntime in tvm_runtime.dll (#5002)Co-authored-by: Jon Soifer <jonso@microsoft.com>,5
Fix stride default value None in torch.nn.functional.avg_pool (#4984)* fix unordered dictionary problem for python version 3.5* modify style* default value of stride in torch.nn.functional.avg_pool is None* delete prev modifications* add testcase for nn.functional.avg_pool2d,0
[Frontend][Torch] Check graph inputs match expected (#4992)* [Frontend][Torch] Check graph inputs match expected* error/warn when missing/unused graph inputs* Change to use get_graph_input_names,0
fix ROCm strategy for winograd conv selection (#5001),0
[relay][external codegen] outline and inline lifted functions for external codegen (#4996)* outline and inline lifted functions for external codegen* add batch_norm test* test batch_norm inline,1
[COMMUNITY] @optima2005 -> reviewer (#5004),3
[VTA][Chisel] Change Scala Linter scalafmt => scalastyle (#4998)* scalafmt => scalastyleChange-Id: Ifc590e7cb63585f35dfdc9efcf3c6287b1afb1dd* scalafmt => scalastyleChange-Id: I8aff2632dadda05d2896e28bdaf6f780a160a15a* add indentation constraintChange-Id: Ibeb00c11a5718ea47322ea2b82e757828af8af91* trigger ci again,1
Add BN support with run-time mean and variance calculation (#4990),1
[FRONTEND][TENSORFLOW] support multiply outputs (#4980)* [FRONTEND][TENSORFLOW] support multiply outputs* [TENSORFLOW][TEST] add tf_testing.AddShapesToGraphDef test* update frontend test* retrigger CI,1
kill from tvm import te (#5007)Co-authored-by: Michal Jamroz <jamroz@chem.uw.edu.pl>,5
lower plevel of conv2d winograd on cuda (#4987),5
Docs and Readme updated as per new namespace change (#4989),1
"[VTA][Chisel,de10nano] Chisel fixes and de10nano support (#4986)* [VTA][de10nano] Enable user defined target frequency.Issue:The VTA target frequency on the DE10-Nano is hardcoded to 50MHzunnecessarily limiting performance.Solution:Add a PLL to the FPGA sub-system along with support for theselection of a user specified frequency at build time. The boardsuccessfully builds and runs at 100MHz.* Added a PLL in the soc_system.tcl platform designer generator  script.* Modified the Makefile to automatically set the target frequency  from that specified in the pkg_config.py file.* Modified the Makefile to generate a bitstream with an RBF  format that enables programming of the FPGA directly from  the on-board processor. Specifically, the RBF is generated in  FastParallel32 mode with compression, which corresponds to the  default MSEL switch setting on the board, i.e. 01010.* Added a false path override to file set_clocks.sdc to turn off  unconstrained path warnings on the VTA pulse LED.* [VTA][TSIM] Add more debug and tracing options.* Modified Makefile to change default config to DafaultDe10Config.* Added option in Makefile to produce more detailed tracing  for extra observability in debugging complex scenarios.* Added option in Makefile to produce traces in FST format which  are 2 orders of magnitude smaller, although much slower to  generate.* Added option in Makefile to build the simulator with GCC address  sanitizer.* Modified Makefile to not lint the scala code by default avoiding  unintended wrong indentation. Linting should be better performed  manually on a per-need basis.* [VTA][de10nano] Enable remote programming of FPGA.Issue:The Cyclone V FPGA on board of the DE10-Nano can only be programmedusing the JTAG port, which is a limiting option for users.Solution:Add support for the remote programming of the FPGA implementingthe FPGA programming manager protocol published in the Cyclone Vuser manual.* Added file de10nano_mgr.h implementing an FPGA manager class  that supports handling of control and status registers as well  as a push-button option to program the FPGA. The class can be  easily extended to include more registers if needed.* Used an instance of the FPGA manager to implement function  VTAProgram also warning users when incompatible bitstream  files are used.* Registered VTAProgram as a global function and modified  the program_bitstream python class to use it.* [VTA][de10nano] Enhance de10nano runtime support.Issue:The de10nano target has incomplete, non-working supportfor runtime reconfiguration, bitstream programming, andexamples of usage.Solution:Complete runtime support for the de10nano target.* Modified VTA.cmake to comment out a default override for  VTA_MAX_XFER to 21 bit wide.* Modified VTA.cmake to add needed de10nano include dirs.* Modified relevant files to support de10nano same way as  other targets for VTA runtime reconfiguration and FPGA  programming.* Added test_program_rpc.py example as a runtime FPGA  programming example. Note that unlike the pynq target  no bitstream is either downloaded or programmed when  the bitstream argument is set to None.* Cosmetic changes to vta config files.* [VTA][Chisel] LoadUop FSM bug fix.Issue:The LoadUop FSM incorrectly advances the address of the nextuop to read from DRAM when the DRAM data valid bit is deassertedand asserted at the end of a read. This is caused by a mismatchin the logic of the state and output portions of the FSM.This is one of two issues that was gating the correct operationof VTA on the DE10-Nano target.Solution:Modify the logic of the output section of the FSM to includea check on the DRAM read valid bit or fold the output assignemntinto the state section.* Folded the assignemnt of the next uop address in the state  section of the FSM.* [VTA][Chisel] Dynamically adjust DMA tranfer size.Issue:In the DE10-Nano target and possibly in others, DMA transfers thatcross the boundaries of memory pages result in incorrect reads andwrites from and to DRAM. When this happens depending on differentinput values, VTA loads and stores exhibit incorrect results forDMA pulses at the end of a transfer. This is one of two issues thatwere gating the DE10-Nano target from functioning correctly, but mayaffect other Chisel based targets.Solution:Add support for dynamically adjustble DMA transfer sizes in loadand store operations. For a more elegant and modular implementationthe feature can be enabled at compile time with a static constantthat can be passed as a configuration option.* Modified the load and store finite state machines to dynamically  adjust the size of initial and stride DMA transfers. The feature  is enabled by default by virtue of the static constant  ADAPTIVE_DMA_XFER_ENABLE.* [VTA][Chisel] Improve FSIM/TSIM/FPGA xref debug.Issue:Cross reference between FSIM, TSIM, and Chisel based FPGA tracesis an invaluable instrument that enables fast analysis on FSIM,and analysis/debug on TSIM and FPGA, especially for complex flowslike conv2d or full inferences. Currently this cannot be doneeasily since a suitable reference is missing. The clock cycleevent counter cannot be used since it is undefined in FSIM andnot reliable between TSIM and FPGA because of different latencies.Solution:Introduce a new event counter that preserves a program order acrossFSIM, TSIM, FPGA. We propose adding the accumulator write eventcounter in the Chisel EventCounter class and a simple instrumentationin the FSIM runtime code. Note that this technique enabled finding theChisel issues reportes in the PR, which would have been otherwisefar more difficult.* Added the acc_wr_count event counter and changed interfaces  accordingly.* [VTA][de10nano] Comply with linting rules.* [VTA] Appease make lint.* [VTA] Disable pylint import not top level error.* [VTA][Chisel,de10nano] Linting changes.* Use CamelCase class names.* Use C++ style C include header files.* Add comments to Chisel makefile.* [VTA][de10nano]* Reorder C and C++ includes in de10nano_mgr.h.* Restore lint as default target in Chisel Makefile.* [VTA][de10nano] Do not use f string in pkg_config.py.* [VTA][de10nano] Remove overlooked f strings in pkg_config.py.* [VTA][de10nano] Fixed typo.* [VTA][TSIM] Check if gcc has align-new.* [VTA][Chisel] Make adaptive DMA transfer default.* [VTA][RPC] Renamed VTA_PYNQ_RPC_* to VTA_RPC_*.Issue:With more FPGA targets coming online the initial method ofusing individual environment variables to specify target IP and portdoes not scale well.Solution:Use a single VTA_RPC_HOST, VTA_RPC_PORT pair to be changedevery time a different target is used. For instance in a scriptused to benchmark all targets.* Replaced every instance of VTA_PYNQ_RPC_HOST and VTA_PYNQ_RPC_PORT  with VTA_RPC_HOST and VTA_RPC_PORT, respectively.* [VTA][Chisel] Comply with new linter.",0
[Runtime] MISRA-C compliant TVM runtime (#3934)* implement of MISRA-C compliant TVM runtime;* working on bundle_deploy_c demo* move header files into include dir* fix compatibility issues* fix compatibility issues* resolve most of the warnings and errros* implement c_backend_api* introduce bridge* working well* move to header files and bundle.c into src/runtime/crt* clean up* satisfy linter* clean up* test with the cat image* remove synset* refactoring* refactoring* refactoring* initial crt_runtime_api.c* improved compatibility with g++* using exposed API in c_runtime_api.h* call from c_runtime_api.h* clean up* lint* merge into apps/bundle_deploy directoryChange-Id: I51904db81b8589e65d107d8ca77b47452e3812b5* make the demo runs in ciChange-Id: I2c24f8b592508833d3555311c2b24d1931f19385* address review commentsChange-Id: I027ddff15c31fb4da0bd0e461427dce619de1f93* releaseChange-Id: I5ad5bb8426468aac9fc8d074e56ddea358a7fd91* fix ci testingChange-Id: Ic2e82fb3051b6c254ef32a964f976b61e3e5fe4d* add test case for misra c runtimeChange-Id: Ie0dfd0ade6be4665b4384db7d260a6c69b35010f* fread files in testing to avoid calling xxdChange-Id: Ie7fbc16b4b0b9509918d986a841f443900813bef,0
typo (#5008),5
"Revert ""[Torch, QNN] Add support for quantized models via QNN (#4977)"" (#5013)This reverts commit fc7f0783940c362bf48cd46817956381196201e2.",1
[REDO AFTER GH BUG] Add support for quantized models via QNN (#5016)This reverts commit f346c60287b50950275e20db9e6d84b3fc568a00.,0
Revive the Rust + SGX refactor (#4976)* Add Nick's changes's squashed* Fix frontend compilation* Re-enable Rust CI* Add changes with conflicted badly* Restructure import_module! macro in order to avoid unstable features* Kill old unstable feature enablement* Refactor common to use new APIs* Move the code to stable* Fix warningCo-authored-by: Nick Hynes <nhynes@oasislabs.com>,0
Implemented kDLCPUPinned (cudaMallocHost) (#4985)* implement kDLCPUPinned* Fix line endings* Fix whitespace for linter* cleanup up allocdataspace method,0
Early checking added and new test cases added for schedule fuse (#5010)* [1] New test case added for fuse* [2] New test case added for fuse* [3] New test case added for fuse* [4] New test case added for fuse* [5] Early check added,1
[RELAY] Remove primitive attribute from composite function (#5014)* A composite function should not be primitive since we still may need to perform passes on it.Change-Id: If62d06d265234861a6ec0df7749dc1c339c1055c,4
"Revert ""[topi][relay] add operation tan to TVM (#4938)"" (#5017)This reverts commit d992468d80af816f0413fc43c2ee1c02f7fe19c3.",1
"[Torch] Add initial control flow support  (#4964)* Add support for prim::If and prim::Loop with test cases* rebase and fix tests* add some comments* simplifying, fix float cast* parse -> convert* recursivly retrive ops in get_all_op_names* use multiple return values from block correctly, simplify loop convert* choose dtype properly for zeros and ones* simplifying, replace convert_inputs with _get_relay_input_vars* fix for while loop with non input dependent init cond* add assert on loop var update* move the condition around* better testing for seg models* rebase fix, disable inception v3 in quant test as it is too slow toload with torch-1.4 + torchvision 0.5* simplify and add more comparison op converter",0
[CI] Temporary disable rust test (#5029),3
"[topi][relay] new PR to re-add tan to TVM (#5025)* Add relay operation relay.op.tan.* Update tan implementation in TVM.* Update tests.* Add shape function for tan.* Add missing main test to python/frontend/tensorflow/test_forward.* Revert, back to sin/cos.* Revert ""Revert, back to sin/cos.""This reverts commit 4da5b503b921585ba9d80944b29136142b575c40.* Fix implementation of tan in cuda. Do not support tan for float16.Simplify topi/tests/python/test_topi_math. Add testing for tan with float32 and float64.Finally implement tan as sin/cos in llvm.",0
"[CodeGen][CUDA] Enhance CUDA codegen for SelectNode (#4983)- This patch allows CUDA backend to emit correct code for  selects with vector conditions, which may be produced  by floordiv op lowering etc..- This already works for llvm BE, as llvm select instruction  supports vector conditions.Signed-off-by: Wei Pan <weip@nvidia.com>",2
"[TFLITE]Activation functions support (#4978)* [TFLITE]elu, leaky_relu, lrn, log_softmax activation functions* removed ops present in pr 4805* review_comments updated",1
[QNN] Support 4D padding. (#5036)* [QNN] Support 4D padding.* Empty commit.Co-authored-by: Ubuntu <ubuntu@ip-172-31-38-96.us-west-2.compute.internal>,1
"Revert ""Conditions updated to cover better user scenarios (#4951)"" (#5032)This reverts commit fe74b37ab578e6d3c540b0f6ac187a220ccc028a.",1
"Revert ""Tighten split's extent (#4931)"" (#5027)This reverts commit 585f9ce6e7bef7d0e8902b1c1e55dcb3bbe84eed.",5
[Intrin] Adding a few missing math intrin  (#5011)* [intrin] exp2* [intrin] exp10* [intrin] log2/10* [intrins] exp10* [test] math intrin,1
Conv3D ONNX support and conv3D_ncdhw x86 schedules (#4949)* Support 3d Convolution with the ONNX frontend* add unit tests for conv3d in onnx frontendrespond to PR formatting requestsadd x86 schedules to conv3d ncdhw testfix a doc string format issuerefactor for changed upsream API* first attempt at conv3d autotuningadd default schedule for conv3d_ncdhwfill in autotvm integrationadd a fallback for invalid schedulesfix fallbackfix reduction order to get simd working correctly,0
[Relay][VM] Fix compilation of If-Elses (#5040),0
Conditions updated to cover better user scenarios[Re-raised] (#5043)* Conditions updated to cover better user scenarios* [1] New test case added* [2] New test case added* [3] Proper variable name used* [4] Review Comments handled* [5] Review comments handled* [6] Review comments handled,1
[Object] Add String container (#4628),1
[refactor][relay pass] Separate analysis and transform passes (#5035)* [refactor][relay pass] Separate analysis and transform passes into different subfolders* remove pass folder,4
[VTA] VTA hardware/software codebase re-org (#5037),2
"Set split node's range to minimum of ext and split factor or split nparts, but only when PassDownDomain is called with allow_missing == false, i.e. by InferBound. Add a helper PassUpThreadBinding() to get a map telling whether an IterVar has at least one leaf IterVar deriving from it binding to a thread. Add two unit tests. (#5044)",1
Support for AddV2 in Relay Tensorflow frontend converter. (#5046),1
[TFLITE]Round op parsing support added (#5022),1
"[TFLITE][FRONTEND]Reduce_any op parsing support (#4926)* [TFLITE][FRONTEND]Reduce_any op parsing support* Testcase check added to run in tf version above 1.14.0 & review comments* Review comment, checked updated to 1.15",1
[REFACTOR] Streamline Function Attr interface. (#5045)* [REFACTOR] Streamline Function Attr interface.There has been quite a few recent changes that depends heavily onthe function attr interface. This PR streamlines that interface by introducingtwo APIs that covers most of the usages.- GetAttr which gets a typed object for a given key  - HasNonzeroAttr is a quick helper that calls GetAttr to quickly check an attribute- WithAttr that creates a new function object with the given attr  - The API comes with copy on write optimization to avoid multiple copies  - We deliberately pick the prefix With(instead of Set) to indicate this    function does not mutate the original input.On the python side:- We allow read access via func.attrs (which is a DictAttr)- func.with_attrs to create a new instance with updated attrs.We also get rid of the small wrapper functions and make sure the API centered aroundthe GetAttr and HasNonzeroAttr interface.This PR also changes the function construction to follow the new convention.* Address review comments* Address review comments* Fix doxygen path,0
Fixed div by zero core dump. Fixed rounding intrinsics on int crash (#5026),0
[1] Test case modified for int type (#5012),3
"[Strategy] Support for Int8 schedules - CUDA/x86 (#5031)* [CUDA] Op strategy changes for Int8 schedules.* Applying Haichen's suggestions.* Make 4D output work for task extraction.* Make x86 work.* Fix lint.* Lint fixes.* Tests, comments, out channel a multiple of 4.* Topi test.Co-authored-by: Ubuntu <ubuntu@ip-172-31-38-96.us-west-2.compute.internal>",0
[Autotvm] Fix autotvm customized template (#5034)* init* fix template* tweak naming,0
CI: Install apt-transport-https (#5053)The ubuntu_install_llvm.sh script started failing because of a http tohttps redirect.  This patch adds the package that allows apt to handlehttps transport.Change-Id: I70bcba32a9fc75d02c54f4f21f288b2f46226689,1
[C++] Require c++14 by default (#5056),5
"[Bugfix][IR][ATTRS] Fix AttrEqual for Array and StrMap, double (#5054)- Use fuzzy comparison for double.- Removed the hack for BatchNormAttrs and DictAttr.Also removed a warning from text printer printing.",0
maintenance (#5058),5
[PY] Require python3.6 (#5057),5
"[DOCS] Move git_howto to rst, add Stage documents to te (#5055)",1
[Graph tuner]Add opt out operator for has_multiple_inputs for graph tuner (#5000)* consider layout_transform in has_multiple_inputs* refactor code* remove debug info* remove subclass assignment* refactoring a little bit* remove default value* remove trailing whitespace* modify test for has_multiple_inputsCo-authored-by: Ubuntu <ubuntu@ip-172-31-40-194.us-west-2.compute.internal>,0
[Relay][Pass] Add submodule extraction pass (#4960)* rebased* fix lint,0
[Relay][AutoTVM] Bug Fix for ARM CPUs. Lower strict assumption. (#5063),0
"[Torch, QNN] Remove FP32 piggy back and use QNN add/mul/concatenate (#5061)* use qnn add/mul/concatenate* remove logging",1
[COMMUNITY] Add @abergeron -> reviewer (#5064),1
[TIR] Introduce tir::PrimFunc (#5070)This PR introduces tir::PrimFunc which will be used as the TIR functioncontainer in the unified IR.Also streamlined the function attributes a bit further.- All common attributes are under tvm::attr- TIR specific attributes are under tvm::tir::attr and comes with a tir prefix- Use stl_style for attributes for now,0
[TESTS] Triage the testcases to fit the the new namespaces (#5071)* [TESTS] Triage the testcases to fit the naming convention of the new namespaces* Remove multiple usage of system lib to avoid test problems,1
Add support for FusedBatchNormV3 (#5065)No changes seem to be needed to _fused_batch_norm. It just works.,1
"[Relay, TOPI] Refactor Adaptive pool and add 3d support (#5049)* add stub for nd impl* refactored indices compute* refactored divide step* remove unused variables, add doc* fix lint* add relay op def* add python registration* refactor topi test* update relay tests, but test result is weird* workaround for weird bug* add relay adaptive pool 3d test* add topi tests* update doc for 3d* typo fix* fix lint* add more tests including NDHWC",0
[IR] Update the type_keys to reflect the code-org (#5074),1
[Team] jwfromm -> reviewer (#5076),5
Return empty CSourceModule when no lowered_funcs exists in Relay mod (#4847)* Use dummy func when no lowered_funcs exists in Relay mod* Dummy func -> CSourceModule with empty code str* Added comments describing the empty CSouceModule* Always import external modules w/o assertions* Use CSourceModule as a fallback for LLVMModule* Changed cond for target == llvm* Create an empty LLVM module w/o using dummy func* Avoid using IR str concat to create LLVM module* Improved comments for codegen.LLVMModuleCreate* Satisfy the linter for LLVMModuleCreate,1
"Tensorflow script upgrade from 1.13.1 to 2.0.0, so that it can run in both versionsw (#4963)",5
[TFLITE]DepthToSpace and SpaceToDepth support (#5041)* [TFLITE]DepthToSpace and SpaceToDepth op parser support* DepthToSpace and SpaceToDepth testcases* Review comments fixed,0
"[Relay, TF Frontend] Dilation2D operator support (#5033)* update docs for dilation 2d* dilation2d compute* dilation2d register* dilation2d rel compute* dilation2d strategy* dilation2d attrs* dilation2d generic schedule* dilation2d tf frontend support* dilation2d tf frontend test case* dilation2d test cases* pylint fixes* add exception for cuda target* Update docstring* Update docstring* change rates to dilations* removed unused param* merge master* Update nn.py* Update nn.py",0
[Refactor][Relay] Refactor Relay Python to use new FFI (#5077)* refactor relay python* revert relay/ir/*.py to relay* Address comments* remove direct access to analysis and transform namespace,1
[RELAY][PY] Fix relay node registration after refactor (#5083),0
Change Azure pipeline badge to GH actions (#5081),4
Replace UseDefaultCompiler with GetAttr (#5088),5
create function.py (#5087),5
[CODEGEN][OPENCL] Explicitly cast min/max operands (#5090)* [CODEGEN][OPENCL] Explicitly cast min/max operands* retrigger CI,5
Description updated for pooling attributes (#5091),1
[RELAY] Codegen_c.h should include relay.function (#5093)Change-Id: I015b2c66a50b64d0eb2e9efe336f6c18ea1fdc67,4
"[Torch, QNN] Add missing upcast to uint8 avg_pool conversion  (#5089)* add missing upcast to avgpool* add avg pool test",1
[TUTORIAL] Fix vta tutorial after relay function refactor (#5095),0
[ConvertLayout] Support QNN ops. (#5066)* [ConvertLayout] Support QNN ops.* Changing layouts to C.* Fixing dilation.* Empty commit.Co-authored-by: Ubuntu <ubuntu@ip-172-31-53-55.us-west-2.compute.internal>,0
[Relay][Frontend][ONNX] operator support NonZero (#5073)* [Relay][Frontend][ONNX] operator support: NonZero* update* Solve the build fail* solve the build fail* Replace ctx_list with tvm.cpu(),1
[DOC] Add doc for Relay op strategy (#5078)* [DOC] Add doc for Relay op strategy* update* address more comments* update* update,1
[Torch] Add initial 3D op support and test on Resnet 3D (#5075)* fix minor lint issue* add conv3d and adaptive avg pool3d conversion with test* fix max pool handling* add batch norm 3d test* add resnet 3d test* add more conv3d test* clean up batch norm test* add note on disabling inception v3 test* add more tests* add more tests* fix names,0
"[TIR][TARGET] Refactor Target codegen to use IRModule and PrimFunc. (#5107)As part of the unified IR refactor.This PR refactors the target codegen to use IRModule containing tir::PrimFuncs.In order to break the refactor into several steps without breaking the codebase,we built an conversion pass to convert Array<LoweredFunc> into IRModule.The follow-up refactors will gradually move the passes covered by IRModule upuntil we cover all the passes. Then we can remove the additional redundantconcepts such as LoweredFunc.",1
Add colors to compute_at edges and thread/block indices. (#5111),1
[AutoTVM] Temporary fix to the stack overflow issue in autotvm task extraction (#5019)* Temporary fix to the stack overflow issue in autotvm task extraction* fix lint* fix graph tuner test,0
[TOPI][OP] Use Thrust sort for argsort and topk (#5097)* [TOPI][OP] Use Thrust sort for argsort and topkThe current GPU sort implementation (odd-even transposition sort) is too slowwhen the number of elements is large.  This PR introduces Thrust implementationof sort which is much faster.Note that this change requires CMake 3.8 or later since we have to use nvcc tocompile a thrust code.* cmake: make CUDA optional* allow .cu file to be into the repository* pylint fix and cleanup* require cmake 3.8 only when thrust is enabled* fix nvcc compiler error when passing -pthread* add missing include* add USE_THRUST option in config.cmake* retrigger CI* retrigger CI,0
[Fix] Fix CompilerAttrs (#5109)* fix CompilerAttrs* retrigger ci,0
[Relay][BYOCG] Propagate constant to subgraphs (#5094)* bind constant to subgraphs* con -> constant,5
"[TOPI, Relay refactor] Move Dilation2d from nn to image namespace (#5110)",4
[Relay][TF] Support for Atan/Atan2 in Relay Tensorflow frontend converter. (#5104)* add Atan/Atan2 op* fix bug and testing,0
TVM android camera demo (#5005),5
[docs] Update relay docs (#5112)* Update relay docs* any -> py:func* make clean,1
[KERAS] conv3d frontend operator support (#5080)* [KERAS]Conv3d support added* Keras conv3d testcase added,1
"[DOCS] include a tarball of docs, add a security faq (#5119)* [DOCS] include a tarball of docs during deployment* [DOCS] Add a short security faq",1
Update the tarball deployment. (#5120),1
[Rust] Fix the existing test cases before refactoring.  (#5122)* Fix up the final pieces* Tweak build.rs,0
Adjust strategy plevel to achieve expected performance by default (#5118),5
[CodeGen][CUDA] Vectorization for intrinsics (#5101)- This allows to emit vectorized loads/stores  for CUDA math intrinsics.- A few intrinsics should be lowered as CUDAMath not CUDAFastMath ones.- Fixed the code block identation.,0
[DOC][TUTORIAL] Fix typo for deploy_model_on_android.py (#5123),0
[DOCS] Cleanup docs before rebuild (#5127)* [DOCS] Cleanup docs before rebuild* Ask doxygen to generate svg to minimize the file size,2
"[Relay, Topi] [TF, MXNet] Unravel Index operator (#5082)* first cut unravel_index* merge fixes* change rates to dilations* unravel_index op relay, topi, mxnet, tf* doc changes* small changes* remove empty unravel and argwhere attrs* remove empty unravel and argwhere attrs",0
"[Relay, Topi, TF Frontend] Isfinite operator (#4981)* isfinite doc update* isfinit expr* isfinit expr* isfinite schedule reg* isfinite python binding* isfinite python binding* relay register isfinite* isfinite type relation* intrin isfinite* topi isfinite* testcase topi isfinite* tf frontend isfinite* tf frontend isfinite testcase* test case relay isfinite* small fixes* test forward tf isfinite* test cases injective for cuda* remove float16 test case* add support for isinf* remove unwanted import* fix conflict",0
[Bugfix] Fixed bug where shifting by out-of-bounds value results in no compute code being emitted. (#5115)* Fixed bug where shifting by out-of-bounds RHS values results in LLVM to codegen nothing. Added regression testcase* Updated testcase to be more precise.* Fixed testcase,0
[Frontend][TensorFlow]TensorFlow Parser Control Flow Enhancement (#5020)* Improve TF control flow major logic* Pass mod into operator convert function* Fix LoopBound* Add more control flow tests* Add two test cases for stridedslice* Fix docstring* Fix lint* Fix import* Fix test assert* Minor fix conv3d* Add more comments* Fix for dilation2d* Change newly added atan* Change newly added unravel,0
[DOCS] Minimize necessary doc change (#5129),2
[Refactor] Relay Node::make to constructor (#5128)* relay Node::make to constructor* patternwildcard* Address comments,1
Add thrust support for nms (#5116)* add argsort_nms_thrust* consider valid count in thrust nms sort* make thrust optional* typo* typo* fix pylint* address some of the comments* address more comments* fix lint* address more comments* address more comments,0
Fix for issue #4831. The data_min_idx and data_max_idx were flipped. (#5136)Existing test cases cover this fix. In addition I have added an assert to make sure that the data_min is always less than equal to data_max.,0
Change Rust version to stable in Docker (#5138),2
[Relay] GradientCell Relay Pass (#5039)* save* gradient.rly* fix* NOT WORKING: gradient cell pass* test gradient pass* fixed basic call ops* more tests* fix bug* transform calls to one ones_like zero zero_like* maintenance stuff* fix linting* linting* linting* throw default* remove unrelated changes* import gradent.rly in pass* comment* linting* remove changes to test files* move gradient_cell.cc to transforms* revert change* update files with new commits* type* wrapper function to main outermost function type* fix linting* fix unsigned and signed int comparison* review* GetConstructor definition in module and change op comparison* update node instantiations* increase code readabilityCo-authored-by: Marisa Kirisame <lolisa@marisa.moe>,0
[CI] Update rust docker (#5141),1
[RUNTIME]fix unused-value warning (#5140),0
[REFACTOR][TIR] Introduce PrimFuncPass. (#5139)* [REFACTOR][TIR] Introduce PrimFuncPass.- Introduce PrimFuncPass- Convert one pass to the unified Pass API.* Address comments* Fix comments,0
[Torch] Fix conv2d conversion for group conv (group > 1 but != in channels) (#5132)* Fix conv2d conversion for group conv* add more comment for clarification,0
[Torch] Add support for max_pool1d (#5142)* [Torch] Add support for max_pool1d* add test* fix line-too-long* remove wrapper class,0
Duplicate likely nodes added when loop axis split unevenly (#5084)* [TE][Schedule] Duplicate likely nodes removed* [1] Test case added* [2] Lint error fixed* [3] Review comments handled* [4] Review comments handled,0
[Tutorial][Quantization] Fix incorrect name of calibration mode (#5150),0
Handle empty LLVMModule in GetFunction (#5146),5
[Strategy][ARM CPU] Remove contrib spatial pack schedule of depthwise convolution (#5148)* [Strategy][ARM CPU] Low the plevel of contrib spatial pack of depthwise convolution* address comments,1
"[RELAY] Added a AnnotatedRegion utility class (#5030)* [RELAY] Added an AnnotatedRegionSet utility classIn many of the passes involved in graph partitioning,we need to extract and manipulate annotated regions.This class simplifies the extraction of regions from a relayexpression containing region begin and end annotationsas well as providing utility functions to query theseregions and merge them.Co-authored-by: Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>Change-Id: Ia912fea0b99f64b6a7197aa6da2347e58f469fbb* Rename fix* Update MakeRegions* Fix __init__* Indentation* Code style* Remove 'Region' from docs* Overload [] to get region* Use src/dest for MergeRegions* Simplify merge* Tidy const loop vars",0
[Doc] TVM release process (#5151)* [Doc] TVM release process* fix tag* remove things not apply,0
[Relay][OP] Register topi schedule for Relay fast_exp and fast_tanh (#5131)* register for fast_exp and fast_tanh* Add unit test for fast math* Add unit test for op fast math* Add unit test for op fast math* Add unit tests to guard registering topi schedule for Relay fast_exp and fast_tanh* Fix ident* Fix the indent* Add fast_tanh in the test_fastmath of topi tests,0
[RUNTIME]crt error handling (#5147)* crt error handling* Review comments fixed,0
[Relay][MergeComposite] Support TupleGetItem in body of pattern (#5106)* Support TupleGetItemNode in body of pattern only* Add bn_relu test case for MergeComposite with TupleGetItem* formatting* TupleGetItemNode::make -> TupleGetItem(),1
"[External Codegen] Fix annotate pass static variable (#5023)'fannotate' in the annotate_target pass was designated asstatic. This meant that if you use the pass to annotatemore than one codegen, its value is not updated when thetarget changes resulting in incorrect annotation.Change-Id: Ib4f3af5cfbef44f29771818219755198ac313a0e",0
"[TOPI][Tensor Core] Conv2d and Dense ops support on Tensor Core (#5099)* [TOPI][Tensor Core] Optimization of CNNs on Tensor Core #6004* update conv2d test* # pylint: dense_tensorcore.py* modify* modify conv2d* modify the unclear comment,add shape assertion in conv2d compute,combine general gemm intrinsic* add shape assertion in conv2d compute, combine general gemm intrinsicCo-authored-by: libaihong <libaihong@inspur.com>Co-authored-by: libaihong <61525430+libaihong@users.noreply.github.com>",1
Adding support for QNN subtract op (#5153)* Adding support for QNN subtract op* Fixing typo.* Fixing typo.* Fixing lint.* Addressing review comments.* Renaming variables as per convention and renamed QnnBinaryOpTypes -> QnnBinaryOpType* Renaming QnnBinaryOpType to QnnBinaryOpTensorType which now takes the index you want to extract to make the code more readable.* Fixing lint.* Moving common code to macro.* Fixing alignment.* Fixing typo.* Fixing lint.* Renaming method to pass CI.,0
[Relay][Frontend][Pytorch] Fixed ConvTranspose2D parsing (#5157)* Fixed conv transpose parsing.* small format change.* Chage test module names.* Simplified test syntax.,0
"[NODE][IR] Introduce StructuralEqual Infra for the unified IR. (#5154)* [NODE][IR] Introduce StructuralEqual Infra for the Unified IR.This PR introduces a new way to handle structural equalityfor both TIR and relay nodes in an extensive way.- Each object can now register an optional SEqualReduce function, which  describes how to reduce its structural equality to another instance  into equality of the children.- Optionally, the object can choose to allow remapping of vars(e.g. function parameters)  by calling DefEqual- We implemented a non-recursive structural equality checker that  recursively traverses the objects and does the structural equality checking.This PR also fixes a few potential problems in previous relay's AlphaEqual.- In particular, the new structural equality relation will be communicative.- It is can be dangerous to use same_as relation to quickly check equality,  demonstrated by the following case. (%x, %y) are shared vars between two functions.- function0: fn (%x, %y) { %x + %y }- function1: fn (%y, %x) { %x + %y }The new structural equal is intented to supersede AlphaEqual and AttrsEqual.Follow-up PRs should be performed to redirect the existing usages, and removesthe corresponding implementation.* Update the rule to distinguish between graph node and non-graph nodes.* Refactor the test cases to use structural equal.* address comments* Mark more relay::Expr as graph node, fix a testcase issue(was bug that was not caught by previous alpha equal)* Remove unrelated comment* Fix file comment* Address review comment* Relax condition to fit flaky case",0
"[NODE][IR] Introduce StructuralHash for the Unified IR. (#5160)* [NODE][IR] Introduce StructuralHash for the Unified IR.This PR introduces a new way to handle structural hash for the unified IR.- Each object can now register an optional SEqualHash function, which  describes how to reduce its structural equality to sequence of hash values.- Optionally, the object can choose to allow labeling of vars(e.g. function parameters)  by calling DefHash- We implemented a non-recursive structural hasher that maintains its own stack  to traverse te IR.This PR also improves the hash value property from the previous relay's hash utility.In particular, the graph node mode hashs a DAG differently from a treeby attaching an unique occurence index to each graph node.In all of the test cases so far, structural_hash is consistent with structural_equal.- if structrual(x, y) then structural_hash(x) == structural_hash(y)- if structural_hash(x) == structural_hash(y) then highly likely structural_equal(x, y)  - hash no collison is found in our testcases.Ideally we should work on automatically generating these functions in the future.* Fix cases for EnvFunc and Array dims* fix testcase* Update src/node/structural_hash.ccCo-Authored-By: 雾雨魔理沙 <lolisa@marisa.moe>Co-authored-by: 雾雨魔理沙 <lolisa@marisa.moe>",0
[VTA][Refactor] Introducing VTA_HW_PATH for easier migration (#5163),5
[BUILD] Fix VTA build in CI (#5165),0
[CI] Move build configuration to shell scripts (#5164)* [BUILD] Fix VTA build in CI* [CI] Move build configuration to shell scripts,0
[REFACTOR][IR] alpha_equal to structural_equal (#5161),5
relay::StructuralHash to tvm::StructuralHash (#5166),5
remove AttrsEqual and AttrsHash related code (#5169),4
[DOCS] Various sphinx related fix. (#5168)* [DOCS] Various sphinx related fix.- Use :ref: for reference.- Use :py:class: to refer to API docs.- Update installation guide to also refer to the download page.- Only move html contents in doxygen.* Address review comments* Update wording,0
Add support for sharing params of operators in tensorflow frontend (#5042),1
[CI] Improve VTA build message and scripts. (#5170)* [CI] Improve VTA build message and scripts.* Use absolute path to set the env var,5
"Create a new parameter --cache-from in tvm/docker/build.sh, so that we can point to an image to be used as cache, from an external (#5173)script. * Adjusts documentation to provide information about new optional   parameter ""--cache-from"" * Includes --cache-from in the underlying ""docker build"" command   triggered by build.sh, when required",1
[Runtime][MISRA-C][Bundle] Bundle deployment with static linking (#5158)* test file for static link added* rename files* Fixed static linking issue* cleanup* changed to dynamic and static demo* MISRA-C static and dynamic test* cleanup* cleanup* Update README.md* cleanup headers* update readme,0
[Dataflow]: nullptr check (#5176),5
[TFLITE]TOP_K op parser support (#5051)* [TFLITE]TOP_K op parser support* Testcase updated,1
"[RELAY] Add MergeCompilerRegions pass (#5134)* [RELAY] Add MergeCompilerRegions passThis pass is part of the flow to support creating compilerregions with multiple outputs. It should be called afterAnnotateTarget and will merge together regions that sharethe same target to create larger compiler regions that canbe off-loaded to external codegens.This pass implements an algorithm to ensure that during themerging, no data dependency issues are created. See the testsfor an example of this case.Co-authored-by: Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>Co-authored-by: Manupa Karunaratne    <manupa.karunaratne@arm.com>Change-Id: Ibd99083564608d888482f57c5080109f3eefec88* [RELAY] Annotate compiler_ends on each edgeThis alters the behaviour of the AnnotateTargetpass to enforce the property that all compilerannotations exist along a single data flow edge.Specifically, this means they should have exactlyone parent and one child.Change-Id: I0e74803a77767f4f377d17755a13a74a30909797* Fix comment* Rebase *Node::make* Moved block outside for loop* Code style* Update make API* Remove comment* Remove redundant 'else's* Make one line* Fix comment* RefWrite* Fix merge ordering* Add the RFC example as a test* [FIX] Fixed merging behaviour in AnnotateRegionSetDeleting items from a list while iterating it seems toresult in undefined behaviour which sometimes segfaults.This makes sure all the item deletion happens separately.* Added checks* Move comment* Update comments",0
"[DOCS] Point docs to the ASF site. (#5178)* [DOCS] Point docs to the ASF site.We have migrated the main docs to the ASF site,which will be periodically updated using the docs generated by the CI.Points the docs to the ASF version.* [CI] Improve the docs generation script",1
rocm: fix miopen convolutions (#5179)* fix miopen convolutions* fix overly long lines,0
[TEST] Various CI fixes for the VTA and Relay  (#5181)* [VTA] Set the correct type for synchronize* Fix the legacy API* Temporary remove the structural equal,0
[TOPI] Setting workload correctly for Depthwise conv ARM. (#5182),5
[TE] reverse-mode autodiff without any optimization (#5121)* [TE] reverse-mode autodiff without any optimizationCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* address review comments* add comments and retrigger CI* move unittest to debug ci* move test back and add seedCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>,0
[TVM] ref_counter -> ref_counter_ (#5184),5
Add warning about nnpack installing googletest (#5185),1
[VTA] HW sources refactor (#5188)* refactor* path udpate,5
[FRONTEND][KERAS]Max_pool3d and Averagepool3d operator support  (#5085)* [KERAS]Pool3d support added* Keras pool3d testcase added,1
[Torch] Add support for split (#5174)* [Torch] Add support for split* fix* fix test class,0
"rocm: fix dense_rocblas in strategy, topi (#5191)",0
"[RELAY] Re-wrote the Graph Partitioner to support multiple outputs (#5143)* [RELAY] Re-wrote the Graph Partitioner to support multiple outputsInput : A Relay module that have functions with disjoint annotated regions        using compiler_begin and compiler_end. There could be multiple outputs.Output : A Relay module with global functions for such disjoint annotated regions         with calls inserted at the respective locationDependencies : AnnotatedRegionSet Utility class.Methodology :      1) The AnnotatedRegionSet utility class is able to construct a collection of         nodes that are bound by a give annotation -- here we use compiler_begin         and compiler_end      2) Initially, for each function in the module AnnotatedRegionSets are populated.      3) Then, Vistor pass is traversed until a compiler_end node is encountered         that belongs to a ""region"".      4) When the first compiler_end of a given annotated region is found, a function is         formed and inserted.         a) if the region has multiple outputs, a Tuple node (capturing all outputs)            is returned.      5) Thereafter, if we encounter an another output of the same annotated region,         it is important to note that the function is already formed. Therefore, it will         lookup the function and add a TupleGetItemNode is inserted.          a) We will use the location index of ""rets"" of each ""Region"" of AnnotatedRegionSet             as TupleGetItemNode index.      6) Therefore, functions will be created for all annotated regions. The name for each         global function is created using ""Region"" id and the compiler name.Change-Id: I1372f02a845b6d3da03b561763e03a378dca263c* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *removed the expected use-case as we are taking broken-down PR approach    *code style fixes    *some trivial one liners* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *fixed an implicit copy to a move* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs    *code style changes for comments    *renamed test case multiple outputs --> mixed single multiple outputs        Since the existing test case checks for both single and multiple        output scenarios    *added a new test case with conv2d + batch_norm    *some var name changes in the test* [RELAY] Re-wrote the Graph Partitioner to support multiple outputs*rebased",0
[Topi x86] Missing vectorize for depthwise conv2d. (#5196),5
[REFACTOR][TIR] Migrate Low-level Passes to Pass Manager (#5198)* [TIR][TRANSFORM] Migrate LowerIntrin* LowerDeviceStorageAccessInfo* Migrate LowerWarpMemory,4
[PYTORCH]Activations for pytorch (#5194)* [PYTORCH]Activations for pytorch* Review comments updated,1
[RELAY] Partition graph codestyle fixes (#5202)* [RELAY] Codestyle fixes for Graph Partitioner*ran through clang-format* *formatting comments* *further codestyle changes (after clang-format),0
[DOCS] Use https link (#5183)* [DOCS] Use https link* use http for sphinx,2
[BUGFIX]bugfix in tensorflow space_to_batch_nd (#5175)* [BUGFIX]bugfix in tensorflow space_to_batch_nd* Test case added,0
[PYTORCH]Dropouts And InstanceNorm support added (#5203)* [PYTORCH]Dropouts And InstanceNorm support added* Review comments fixed,0
[FRONTEND][MXNET] Use leaky by default for LeakyReLU (#5192),5
"[RELAY] Fixes to MergeCompilerRegions (#5195)* [RELAY] Fixed issues with MergeCompilerRegionsThis PR addresses a few outstanding issues withthe implementation of MergeCompilerRegions. Inparticular, it now handles TupleGetItem nodes properlyand other minor bugs related to region merging havebeen fixed.Change-Id: I07783afc56183a6f798a510209f23b0a5f252255* Fixed issue using pre-merged regionsChange-Id: I0a844ac59bda1089ae0c67cef52f0b0c7ab2cbd7* Removed some debugging logicChange-Id: Ib6f2eede6f38bbb270073eb8d4c4dc19f60832c6* Remove default annotationsChange-Id: I9b7696a51c95871491cbea33c40f92ec327e417f* Annotate default 'if'sChange-Id: I0098bd1bf6788dd6366810dcefa84f1ebbffaab0* Clang formatChange-Id: I944365cd3080a97a9261f643a8f1efa5a63cf82b* Use src/dest in mergeChange-Id: Ie43113492bda8f1ce63eaf9615cb645bb9e2ee86* Fixed partition testChange-Id: I46f9e349b1a813a9140f7e4f8a2241687e2df73b* Removed commentsChange-Id: I309afdd1951d7e796e41d13788aa487707e0ac4c",0
"[REFACTOR][TIR] Introduce ExprDeepEqual, Remove IRDeepCompare (#5206)* [REFACTOR][TIR] Introduce ExprDeepEqual, Remove IRDeepCompareThis PR introduces ExprDeepEqual which reuses the StructuralEqual infra.We migrated the usecases of ir_pass::Equal to ExprDeepEqual and StructuralEqual.* Address comments",1
[DOCS] Reduce artifcats generated by sphinx gallery (#5208),2
[Debug] Add Dump function for Object type (NFC) (#5207)Signed-off-by: Wei Pan <weip@nvidia.com>,0
[Frontend][Torch] Fix up graph input handling (#5204)* [Frontend][Torch] Simplify operator input handling* [Frontend][Torch] Allow user supplied input names to override graph inputs* Fix pylint issues* Updates from code review feedback* Fix tutorial to use shape list input* Disable intermittent test failure in topi vision test,0
[REFACTOR][IR] kExternalSymbol -> kGlobalSymbol (#5211)* expose runtime::String to Python* kExternalSymbol -> kGlobalSymbol,5
[Runtime][Object] expose runtime::String to Python (#5212)* expose runtime::String to Python* retrigger ci,5
[TIR][PASS] dtype rewrite for indexing variables (#5092),4
[TIR] Introduce BufferLoad/Store (#5205)Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>This PR introduces BufferLoad/Store to TIR. The new nodes will replaceProvide and Call with Tensor arguments in the subsequent refactors.,1
"[REFACTOR][TIR] Migrate low-level pass functions to Pass Manager, (#5213)- Migrate LowerTVMBultin- Migrate inferFragment, LowerThreadAllreduce- Migrate ThreadSync- Refactor target::Build to directly take IRModule.- Remove un-used legacy functions.",4
"[PYTORCH]AvgPool3d, MaxPool3d and Squeeze op support (#5220)* [PYTORCH]AvgPool3d, MaxPool3d and Squeeze op support* Testcases added* review comments",1
[DOCS] Misc docs improvements (#5222)- Reduce CI docs task log size.- Update the relation to halide to the latest state.,1
[REFACTOR] tvm.hybrid -> te.hybrid (#5223)Rationale: The current hybrid module is more aligned with the te part.We might consider add a new varient of hybrid script that support the unified IR later.This refactor paves for the potential later changes.,1
"[CodeGen][CUDA] Fix bugs (#5209)- Support vectorized casts- It is incorrect to extract elements from int8x4 with   0x000000ff & (x >> i * 8)  as this value is of type int in C/C++. If this expression  is used for sign extensions, the sign bit will be wrong.  Simply use C style casts instead and sign bits will just work.Signed-off-by: Wei Pan <weip@nvidia.com>",0
[DOCSTRING]missing function parameters updated (#5228),1
[KERAS]Upsample3d & ZeroPadding3d op (#5125)* [KERAS]upsampling3d and zeropadding3d op* [KERAS]upsampling3d and zeropadding3d test case* Review comments updated,1
"[RELAY][FIX] Fix hang in MergeCompilerRegions (#5227)For certain network topologies, MCR could hang.This patch fixes that case.Change-Id: I3edd8a8a6b452b2b838b777720adea22a3b995b4",0
[TOPI x86] Adding unroll_kw config option for depthwise conv2d. (#5197),1
"[RELAY] Non-recursive Graph Vistor and Rewriter (#4886)* First pass a defining a non-recursive Graph Vistor and Rewriterautoformatremove a currently empty test until testing is solidfied* Make CalcDep from Dead Code Elimination non-recursive* Partially working, not passing all tests yetpasses tests when disabling GetExprRefCount, I think I have a bug in visit countingfix GetExprRefCountFix a subtle bug with nested recursive/non-recursive scopes* Refactor* improve comments* respond to review comments on comments* Fix a problem with default recursion for dataflow nodesmark DataflowVisitor methods as override* implement ScopeMutator* convert forward_rewrite to ScopeMutator, remove DataflowMutator* rewrite ExprRewriter and convert fast_math to use it* switch BiasAddSimplifier to ExprRewriterfix a clang warningfix cpp lintfix doc param error* respond to review comments* fix a typo in the iterative looping* add a regression test for GetExprRefCount issue* Normalize naming* fix lint* First pass a defining a non-recursive Graph Vistor and Rewriterautoformatremove a currently empty test until testing is solidfied* Make CalcDep from Dead Code Elimination non-recursive* Partially working, not passing all tests yetpasses tests when disabling GetExprRefCount, I think I have a bug in visit countingfix GetExprRefCountFix a subtle bug with nested recursive/non-recursive scopes* Refactor* improve comments* respond to review comments on comments* Fix a problem with default recursion for dataflow nodesmark DataflowVisitor methods as override* implement ScopeMutator* convert forward_rewrite to ScopeMutator, remove DataflowMutator* rewrite ExprRewriter and convert fast_math to use it* switch BiasAddSimplifier to ExprRewriterfix a clang warningfix cpp lintfix doc param error* respond to review comments* fix a typo in the iterative looping* add a regression test for GetExprRefCount issue* Normalize naming* fix lint* respond to review comments",0
[PYTHON] Make IntImm more like an integer (#5232),5
"[REFACTOR][TIR] Migrate most of low-level build to use the Pass Manager. (#5225)* [REFACTOR][TIR] Migrate most of low-level build to use the Pass Manager.- SplitHostDevice- ThreadSync- BindDevice- LowerThreadAllreduce- Provide a temp fix for printing IRModule with PrimFunc before the formal text printer.* Address comments, fix tests.* Fix relay tests* Explicit move",0
[TE] Support mixing normal and cross-thread reduction (#5193)* Support mixing normal and cross-thread reduction* minor improvements,5
Fix intel conv2d auto tune (#5200)* Fix x86 conv2d and depthwise conv2d auto tuning* Fix depthwise conv2d infer layout* Use random data instead of empty data for autotvm* Fix pylint* Keep empty array for now for autotvm,0
[ONNX]Pool3d & upsample3d op support (#5135)* [ONNX]Pool3d and Upsample3d op updated* Pool3d and Upsample3d testcase* Review comments fixed* Review comments,0
[REFACTOR][TIR] Migrate all low-level passes to the Pass Manager. (#5233)* [REFACTOR][TIR] Migrate all low-level passes to the Pass Manager.This PR migrates the tvm.lower to return IRModule of PrimFuncsinstead of the LoweredFuncs.* Remove LoweredFunc.,2
[Relay][ADT]Static Tensor Array (#5103)* Add other static tensor array ops* Add tensor array get data* Minor refactor* Fix pylint* Update docstring* Make get data more generic* Improve test* Improve split test* Improve get data* Minor fix* Further improvement for static shape* Improve shape parsing* Unify get_static_name,0
[Fix][VM] Fix copy constructor (#5237),0
[Relay][Topi][AutoTVM] Winograd support for Conv3D (#5186)* Functional conv3d winograd working.* Formatted python code.* registered conv3d winograd compute and started adding relay without_weight_transform operator.* Add topi testing for conv3d winograd.* Format file.* small tweak to unrolling to prevent build sticking.* Refactoring convolution ops in relay.* Refactored relay convolutions.* Bug fixes.* Fixed static bug in convolution.* Added conv3d alter op layout and related support.* Bug fixes and testing done.* Fix a few autotvm bugs.* Drop silly debug print.* Removed debug_skip_region.* Add variant of conv3d_winograd that doesn't transform depth.* initial infrastructure done for depthless conv.* Fix no_depth schedule bugs.* automatic topi switching between depth and depthless winograd.* Fixed bug in schedule.* lint fixes.* Removed indents in convolution.cc* missed a few indents oops.* fixed flop count.* One more small tweak.* Change kernel pack inner axes order.* Style changes.* Comment fixes.,0
[Runtime][Contrib] Support cudnn softmax (#5214),5
[CI] Update MxNet to 1.6.0 with MKL (#5240),1
fix to skip node not in graph. (#5238)fix to skip node not in graph because some network cannot be hybridized with some var unused.,0
fix lower_warp_memory (#5247),0
"[RUNTIME] Enable auto conversion from str to runtime::String in PackedFunc, move dtype related handling to data_type.h (#5251)",4
[PYTORCH]LayerNorm support added (#5249),1
[Topi] Breakdown topi.cc into smaller files (#5253)* [Topi] Breakdown topi.cc into smaller files* add missing file,1
[TE] Minor bugfix in message_passing.cc (#5254),0
[TFLITE]Hard Swish & MobilnetV3 model testing (#5239)* [TFLITE]Hard Swish & MobilnetV3 model testing* CI Failure addressed,1
[Pytorch]layernorm bug fix and testcase updated (#5257),0
Fixed typo and type mismatch (#5259)Co-authored-by: Adrian Muresan <muresan.adrian.bn@gmail.com>,0
[TIR] Fix perf regression of tir refactor (#5258),0
[Relay][OP] Add fast_erf implementation (#5241)* add fast erf* doc* lint* fix* fix indent,0
[uTVM][Runtime] Introduce Virtual Memory Allocator to CRT (#5124)* initial crt_memory and memory leak fix in graph_runtimeChange-Id: I0f79f909a04d1c677aabb80f202f0612c5ce7f2a* fix memory leakChange-Id: I37104c09e28112b1974fa2b064c809d0a8d686c3* clean upChange-Id: I039b12015a1d56c8f4120867cd5a5292da34f3e3* implement vreallocChange-Id: I35800470bcbfcf96652494f359711cb4c2d34398* allocate from stack memory for most of the variablesChange-Id: I72071289843fff4031c0df8796868a0b9fbc57ee* allocate from stack memory for all of the variablesChange-Id: I32dba85ac1660c77f51c2d0d8ab6436ed0c01c74* lintChange-Id: If12cd240685d7791fc60bc0cfb66389cdc186b73* lintChange-Id: I7c9d90c11b60b8edda2427ebd189ebe535af2100* facilitate the growth of TVM_CRT_MAX_NDIMChange-Id: I939fa43027a5c7529c5c7c6bd8d6e6beb91b7581* extend test coverage of vmallocChange-Id: Ie4ff6b64fdfe6810836cf8fd44dace82a20c4581* lintChange-Id: Ibf3c06619ef296df5c49f3945cb6428777781d69* move logging.h to src* fix an error in macOS* remove logging.h* use cflags for gcc* fix compilation error,0
"[LLVM] Use llvm::Align with LLVM 11+ to avoid warnings (#5264)LLVM 11 is introducing a separate class to represent alignment.The functions in IRBuilder that create aligned loads and stores,and which accept the alignment as an unsigned value have beendeprecated (and now cause warnings to be emitted).",5
"[LLVM] Use llvm::ElementCount with LLVM 11+ when creating vectors (#5265)LLVM 11 added support for scalable vectors, and now the number ofelements in a vector is represented by a llvm::ElementCount class,not just a number.",1
[RUNTIME] Quick fix PackedFunc String passing (#5266),0
[LLVM] Do not use x86_vcvtph2ps_256 intrinsic with LLVM 11+ (#5267)This intrinsic was removed in LLVM 11.,2
[RUNTIME] Implement TVMDSOOp(TensorFlow custom op) for TVM runtime (#4459)* Add implementation of TVMDSOOp* feat: Update cmake script to work with c++11 and in-repo build* feat: Use libtvm as oplib dependency* fix: Add missing link dependency to libtvm* feat: Update tf tvmdso op by review comments* fix: Update with pr comments* fix: Fix lint* feat: Add test script and fix gpu shape* feat: Add test script and fix gpu shape* fix: Conditional build tftvm op for gpu* fix: Conditional build tftvm op for gpu* fix: Fix pylint of tf_op module.py* fix: Fix pylint of tf_op module.py* feat: Conditional enable gpu test for tftvm op* feat: Conditional enable gpu test for tftvm op* feat: Add tf_tvmdsoop test script as an app test* fix: Fix gpu/cpu enabled check on tvm in test script* fix: Make tf tvmdso op test script runnable with pytest* remove unused test script test_tfop_module.py* fix: Remove pushd & popd in tfdsoop test script* fix: Upgrade tftvmop use python3 to find TensorFlow* fix: Upgrade tftvmop use python3 to find TensorFlow* fix: Change target_link_options to target_link_libraries* fix: Add tftvmop build script's c++ option* fix: Add tvm library path to tf op test library path* fix: Debug ci build for tftvm dso op* fix: Fix cmake error and skip tfop test* fix: Fix typo and indentation issues* feat: Use TF list input op def* fix: Fix style and unexpected changesCo-authored-by: baoxinqi <baoxinqi@4paradigm.com>Co-authored-by: Chen Dihao <chendihao@4paradigm.com>Co-authored-by: wrongtest <wrongtest@4paradigm.com>,0
"[RELAY][BYOC] Add support for composite functions in BYOC (#5261)* [RELAY] Add 'check' functions to MergeCompositeCurrently, MergeComposite can only perform structuralmatches. This patch introduces the ability to specifya 'check' function alongside the pattern which can includecustom logic to determine whether an extracted patternshould be merged.For example, if you only want to merge 'NHWC' convolutions,you can specify a 'check' function which queries thedata_layout value of the extracted pattern (see the test).Change-Id: I9337ce39f10997051a286d888be38ed0d410d340* [RELAY] Reformat merge_composite.ccRun clang-format on merge_composite.ccChange-Id: I1736bff798cc6d93e57519b08ab3362869098779* [RELAY][BYOC] Support composite functions in AnnotateTargetThis patch introduces support to annotate composite functionsin the AnnotateTarget pass. In order for a composite functionto be annotated, you should name it according to the style:{codegen}.{name}eg. dnnl.add_reluChange-Id: I74d6c0b506153d866f6d1feb203b32dad59f2871",1
"[PYTORCH]celu, gelu, selu activations (#5263)",5
"[LLVM] Include Support/Host.h for declaration of getDefaultTargetTriple (#5268)In newer versions of LLVM, this header is no longer included by one ofthe already included headers in llvm_common.h, so include it explicitly.",1
[LINT] Remove scalalint from lint deps (#5269),4
update compiler version in docs (#5281),1
[BUGFIX][IR] Fix String SEqual (#5275)* fix String SEqual* retrigger ci,0
[BUGFIX] Fix CRT static test bug (#5293)* [CI][DOCS] Make sure to refresh the cython part* [BUGFIX] Fix CRT static test bug* Fix demo_static* resolve review comment,0
[CI] Temporary disable CRT test (#5297),3
Create loops according to storage scope and thread hierarchies (#5190)* Set IterVar index to 0 for local thread bound IterVars.* Lint fix* Use rank instead of scope name to predicate.  Add tests.* Handle cases other than local/threadIdx.* Turn warp to the old behavior.* Modify test to cover global/blockIdx.* Fix a typo.* Update test_te_schedule_ops.py with more testing coverage in test_local_stage_predicate; remove test_schedule_schedule_ops.py which was added by mistake.,0
[TENSORFLOW]reduce ops updated (#5180),1
[Node] Provide guide to user who has difficulty register SEqualReduce (#5300),5
Legalize - Use Non-recursive Rewriter. (#5296)* Legalize - Use Non-recursive Rewriter.* Cleanup.,5
[NODE] General serialzation of leaf objects into bytes. (#5299)This PR refactors the serialization mechanism to support generalserialization of leaf objects into bytes.The new feature superceded the original GetGlobalKey feature for singletons.Added serialization support for runtime::String.,1
Adding support for TFLite QnnSub operator. (#5230),1
[BYOC] Refine DNNL Codegen (#5288)* Improve DNNL* Add bind params* trigger ci,1
"[RUNTIME] Initial implementation of Hexagon runtime support (#5252)* [RUNTIME] Initial implementation of Hexagon runtime supportThis is only the TVM runtime. The FastRPC libraries, simulator driver,etc. will be provided in subsequent commits.* Fix pylint complaints* Fix some more pylint complaints* Add link to the Hexagon SDK website* Extract VTCM marker into a common variable* Implement device->device memory copy* Disable unsigned PDs by default* Ensure that --hvx_length is present in sim_args if HVX is enabled* Remove the line about clang from README.mdApparently things work with libstdc++.* Mention to set USE_RPC=OFF when building libtvm_runtime.so for Hexagon* Remember to use codegen_hvx in validate_hvx_length* Add a line about minimum version of LLVM",0
[NDArray] Set shape_ in NDArray::FromDLPack (#5301),5
[REFACTOR][IR] Move to runtime::String (#5276)* Use runtime::String* move string to tvm namespace* add const char* constructor* implicit cast from std::string,1
Update device_annotation.cc (#5291),1
[FRONTEND][TENSORFLOW] Fix gather_nd indices (#5279)* [FRONTEND][TENSORFLOW] Fix gather_nd indices* retrigger CI,0
"[PYTORCH]Repeat, Reciprocal & Reshape Op support (#5280)",5
[Arith] linear system and equation solver (#5171)* [arith] linear system and equation solverCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* avoid constructing analyzer every time* generate random test cases and address commentsCo-authored-by: Sergei Grechanik <sergei.grechanik@gmail.com>* rename linear_system to int_constraints* add comments and use random seed* message for reporting failure with seed* add SEqualReduce to IntConstraints; allow variables & ranges to be NoneCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>Co-authored-by: Sergei Grechanik <sergei.grechanik@gmail.com>,1
[CI] Fix the hexagon string (#5304),0
"[BYOC] Refine AnnotateTarget and MergeCompilerRegion Passes (#5277)* add target to region* refactor annotate_target* Make all unit test working* quick fix* enable BN, unit test failed* Fix vm test, unit test. Refactor annotate_target a bit.* quick fix fusion* revert fusion change* style fix* Refactor merge region pass* format* minor fix* Skip e2e test* lint* support AnnotateTarget multiple runs* Add HasAttr and revert DNNL codegen* address commentCo-authored-by: Zhi Chen <chzhi@amazon.com>",0
[RELAY][FRONTEND][CAFFE2] add Mul and ConvTranspose operator (#5302),1
"[RUNTIME] Introduce RValue reference(move) support to TypedPackedFunc (#5271)* [RUNTIME] Introduce RValue reference(move) support to TypedPackedFuncThis PR introduces RValue reference support the PackedFunc calling convention to address the above issue.Specifically, when an argument is a r-value reference, we will use a assign a different type code(`kObjectRValueRefArg`),and pass `Object**`  (the address to the Object pointer) instead through the values array.The callee can choose to move out this Object pointer and set the original Object pointer from the caller side to be nullptr.We also add an experimental move support to the python side(marked as _move so to indicate the dev nature).This enhancement will enable copy on write optimizations through out the TVM stack.* Address review comments* fix compilation",0
[Frontend][TensorFlow]Improve TensorFlow Static Shape Tensor Array (#5243)* Support TF Frontend Static TensorArray* Fix pylint* Fix lint* Move get_tensor_array_shape into prelude* Fix lint* Fix common,0
[BYOC] Add example of Composite + Annotate for DNNL fused op (#5272)* merge change from dev branch* fix string issue* bring comanic's change back,0
"[LLVM] Fix generation of LLVM intrinsics (#5282)* [LLVM] Fix generation of LLVM intrinsicsThe type list in the call to llvm::Intrinsic::getDeclaration is notthe intrinsic's signature, it's the list of overloaded types. Withoutthis fix, the updated unit test would cause the following error:TVMError: LLVM module verification failed with the following errors:Intrinsic name not mangled correctly for type arguments! Should be:llvm.ctlz.i32i32 (i32, i1)* @llvm.ctlz.i32.i1Special handling for llvm.prefetch, sig matching for overloaded ints onlyThe prefetch intrinsic returns void in LLVM, while it returns i32 in TVM.This case needs to be handled specially, because rule-based intrinsictranslation would cause invalid LLVM type to be created.Do the signature matching only for overloaded intrinsics. It's not neededfor non-overloaded ones, so this can save a bit of compile-time.* Include intrinsic name in the error message* Fix number of arguments for llvm.fmuladd and llvm.pow",0
"[PYTORCH]Abs, Arange, Softplus ops (#5295)* [PYTHON]Abs, Arange, Softplus ops* Review comments updated",1
"[IR][TRANSFORM] Enable CopyOnWrite for passes. (#5309)This PR enables the copy on write optimizations passes:- Enable COW for IRModule both TIR and relay passes.- Enabled COW for PrimFunc in TIR passes.Need more thoughts into whether/how to enable COWfor relay::Function, due to some function passes dependon the presence of IRModule for context information,and the std::move of the related function to nullptrmight affect the related behavior.",4
[Requantize] Cleanup and Optimize Lowering (#5286)* Adding Cast back to Int32 in FixedPointMultiply.* Removing extra clip.* Fix space.* Retrigger.* Retrigger.,0
Remove PrimExpr from String (#5311),4
[Rust][CI] Restore Rust CI (#5137),5
"[Intrinsic] Add log1p, ldexp, atan2, hypot, nextafter, copysign (#5312)* [Intrinsic] Add log1p, ldexp, atan2, hypot, nextafter, copysign* Lint",1
"[Torch] Support Python list, more realistic recurrent networks (#5306)* use funcs from prelude, pass around convert_map* get relay input type from user ishape* handle tuple unpack* experimenting with static tensor array* use prelude concat instead of cons + rev* minor clean up* fix layer norm conversion bug, unwrap tensor array* add infer shape on tensor array* pass around prelude for now* compile worked but runtime error* fix tensor array wrapping* begin list dynamic test* is_list_dynamic first version* finish dynamic list test* a few fix* use shape_of function if Any is found* improve size conversion* working on adding free vars to loop block* fixed inlined inner loop issue* clean up free var handling* add support for tensor array concat* adding ta concat on last axis* fix concat, but got runtime error* disable concat on axis -1 for now* add lstm tests* revert unrelated change* fix stacked bidir test* minor fix to test* relax tol a bit, revert dnnl change to avoid conflict* simplify infer type, use input tensor shape rather than concat shape* more shape fix",0
[PYTORCH]Reduce_ops support added (#5308)* [PYTORCH]Reduce_ops support added* Review comments updated* typo bug in qnn test,0
[REALY][OP] fix typo (#5315)Signed-off-by: windclarion <windclarion@gmail.com>,0
[Topi] Tensorcore support for Conv3D (#5284)* one weird trick.* Added schedule knob for different workloads.* Initial conv3d tensorcore working.* Added conv3d tensorcore strategy.* Added layout conversion to tensorcore friendly format for conv2d and conv3d.* Add target name check.* Fixed bad names and depthwise check.* Removed duplicated attribute assignment.,0
"[RUNTIME][IR] Allow non-nullable ObjectRef, introduce Optional<T>. (#5314)* [RUNTIME] Allow non-nullable ObjectRef, introduce Optional<T>.We use ObjectRef and their sub-classes extensively throughout our codebase.Each of ObjectRef's sub-classes are nullable, which means they can hold nullptras their values.While in some places we need nullptr as an alternative value. The implicit supportfor nullptr in all ObjectRef creates additional burdens for the developerto explicitly check defined in many places of the codebase.Moreover, it is unclear from the API's intentional point of view whetherwe want a nullable object or not-null version(many cases we want the later).Borrowing existing wisdoms from languages like Rust. We propose tointroduce non-nullable ObjectRef, and Optional<T> container thatrepresents a nullable variant.To keep backward compatiblity, we will start by allowing most ObjectRef to be nullable.However, we should start to use Optional<T> as the type in places wherewe know nullable is a requirement. Gradually, we will move most of the ObjectRefto be non-nullable and use Optional<T> in the nullable cases.Such explicitness in typing can help reduce the potential problemsin our codebase overall.Changes in this PR:- Introduce _type_is_nullable attribute to ObjectRef- Introduce Optional<T>- Change String to be non-nullable.- Change the API of function->GetAttr to return Optional<T>* Address review comments* Upgrade all compiler flags to c++14* Update as per review comment",1
[BYOC] Enhance partitioning and external codegen (#5310)* Remove duplicated output args* address comment* fix codegen c* improve comment* VisitExprDefault_* deduce type,0
[COMMUNITY] @mbaret -> Reviewer (#5322),3
add memoized expr translator for use by backend codegen (#5325),1
[CODEGEN][CUDA] Fix vector load (#5226)* Fix high-low bit bug in __pack_half2* Fix vector load* Add unit8 support for PrintVecElemLoadExpr and BroadcastNode,0
[Frontend|MXNet] SwapAxis operator support (#5246)* MXNet swap axis* MXNet swap axis* swap axis review comment* swap axis review comment,5
[TE][BuildModule] Fix import in dump pass ir (#5327),0
"[RELAY][PYTORCH]isNan, isinf, isfinite, ceil, clamp, round ops (#5316)* [RELAY][PYTORCH]isNan, isinf, isfinite, ceil, clamp, round ops* Review comments",5
"[TIR] Refactor MakePackedAPI to target dependent stage. (#5326)Previously MakePackedAPI was in the target independent stage,but never the less requires the device_type information that will bebinded at a later target dependent stage.The previous implementation was due to the limitation of LoweredFuncwhich can not carry buffer_map info(so they have to be lowered right away).This is no longer the case after the unified IR refactor.This PR migrates MakePackedAPI to a target dependent stageand removes the un-necessary BindDevice pass.",4
[RELAY] Remove re-exports of tvm.transform (#5337),4
[LLVM] Use llvm::FunctionCallee in IRBuilder::CreateCall with LLVM 11+ (#5338)The older variants of CreateCall have been deprecated and were recentlyremoved from LLVM. This caused compilation failures.,4
"[CI] Fix build.sh to propagate --network=host to the docker build command (#5336)* when passing --net=host to build.sh it needs to be also   sent as --network=host to ""docker build"", so that both   build and run will use the same network configuration",0
[Runtime][Relay][Cleanup] Clean up for memory pass to enable heterogenous execution support. (#5324)* Cleanup type pack and unpack for tuples.* Clean up the memory_pass using common helpers* Clean up memory.cc* Refactor pass* Add doc strings* Fix CPPlint* Fix PyLint* Fix* Apply suggestions from code reviewCo-Authored-By: Zhi <5145158+zhiics@users.noreply.github.com>* Fix typoCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>,0
"Windows Support for cpp_rpc (#4857)* Windows Support for cpp_rpc* Add missing patches that fix crashes under Windows* On Windows, use python to untar vs wsl* remove some CMakeLists.txt stuff* more minor CMakeLists.txt changes* Remove items from CMakeLists.txt* Minor CMakeLists.txt changes* More minor CMakeLists.txt changes* Even more minor CMakeLists.txt changes* Modify readme",0
"[PYTORCH]Take, Topk op support (#5332)* [PYTORCH]take, topk op support* Ci Failure fix",0
[TOPI] Using x86 schedules for ARM conv2d. (#5334),5
[TOPI] Improve get_valid_count and nms performance for CUDA (#5339)* get_valid_count updated to have correct results* speedup nms* update nms* revert back nms* recover one test for get_valid_count,1
"[PYTHON] Enhance with_attr API, cleanup MakeAPILegacy in testcases (#5335)",3
"[TIR] Remove ProducerConsumer and AllocateNode::new_expr (#5333)* [TIR] Remove ProducerConsumer and AllocateNode::new_exprThis PR removes two legacy IR parts in TIR that are deprecated.ProducerConsumer node only serves as a hint markup and may no longer beinformative after extensive transformations in the pass.If necessary, we can add related info via AttrStmt.The new_expr field in the AllocateNode is deprecated since it can just bereplaced by a LetStmt.- Remove dependencies of passes on ProducerConsumer.- Remove ProducerConsumer from the IR.- Remove the deprecated fields (new_expr, free_function) from AllocateNode.* Fix additional testcases",0
"[BYOC] Prevent duplicate outputs in subgraph Tuple (#5320)* Fix duplicate output in partitiongraph* Add test case* Fix test_annotated_regions with duplicate compiler_end outputs* Revert ""Fix duplicate output in partitiongraph""This reverts commit e1f8ef3f4ca5b2aaa31ace6fa968bb50e5e4d1fa.* Prevent duplicate outputs in Tuple in PartitionGraph* Fix lint* Add another test case for when regions are merged, and when TupleGetItem was duplicated* Pull GetFunctionOutput out of branch, improve description of GetFunctionOutput* Use std::move for GetFunctionOutput. Fix typo with testcase name* Use tvm.transform.Sequential",0
"[Tutorial, QNN] Add tutorial for loading quantized PyTorch model (#5321)* add pytorch tutorial code and doc stub* add more docs* formatting, more docs* typo fix* try make sphinx happy* add performance section* type and nit fix* format fix",0
[DOCS] Bring relay docs to the top-level flat view (#5343)- Changes most of the relay docs to use autosummary.- Bring relay API docs to the top-level flat view for easier discovery- Removed a few cases of re-exports.,2
[TOPI][PYTORCH]Logical & Bitwise operator support (#5341),2
[RELAY][BYOC] Register pattern tables from external codegens (#5262)* [RELAY][BYOC] Register pattern tables from external codegensThis adds utility functions to support registeringand retrieving pattern tables used by MergeComposite forexternal codegens.Change-Id: I5be165a321440e48b15ff6aff4970e0c67496aaa* Updated DNNL tests to use pattern table mechanism* Removed pattern table standalone test* Change reg to _op,1
[RUNTIME][CRT] support DLTensor whose ndim == 0 (#5344)Signed-off-by: windclarion <windclarion@gmail.com>,5
"[BYOC][FIX] Fix typo in ""default"" (#5348)Default annotations were incorrectly being named 'defualt'which results in them not being removed in PartitionGraph.",0
enable tsim and fsim for GPU build (#5352),5
[CRT]Compilation warnings fixed for 32bit and 64bit compilation (#5349),0
[PYTORCH]Tensor creation ops support (#5347),5
[Hexagon] Add hexagon_posix.cc to TVM/RT sources in the right place (#5346)This file was added before the variable with TVM/RT was initialized.The initialization overwrote the addition.,1
[TOPI-ARM] Do not alter layout if layout is NHWC (#5350)* [TOPI-ARM] Do not alter layout if layout is NHWC* Add test.,1
[TIR] Make lower_warp_memory support extent(threadIdx.x) < warp_size (#5307)* support extent(threadIdx.x) < warp_size in lower_warp_memory* more docs for lower_warp_memory,2
[RELAY][PYTORCH]GroupNorm op support added (#5358),1
docker: Drop caffe2 download progess bars (#5359)Change-Id: Ia15c3c8f41f75423814e559f6fdb062098f19464,4
fix fuse over functions that are handled by external codegen (#5365),0
[RUNTIME] FastRPC interface for Hexagon runtime (#5353)* [RUNTIME] FastRPC interface for Hexagon runtimeCo-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>* Explain store offset in a comment in launcherCo-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>,5
"[TIR][REFACTOR] Migrate low-level passes in tvm.lower to the Unified IR pass manager. (#5364)- Migrate BoundCheckers and Simplify- Migrate RewriteUnsafeSelect and RemoveNoOp- Migrate UnrollLoop and StorageRewrite- Migrate InjectDoubleBuffer and InjectVirtualThread- Migrate LoopPartition and Vectorize- Migrate CoProcSync, LiftAttrScope, InjectCopyIntrinWe still keep ir_pass registerations for now.Need a separate PR to refactor the parts before the StorageFlatten.",4
[TIR] Fix lower_warp_memory when there are >1 warp buffers (#5368)* fix recursion in lower_warp_memory* post-order mutation,0
Add cuda target check to dense tensorcore schedule. (#5376),1
Remove developer facing api from frontend exports. (#5375),4
"[TIR][REFACTOR] Remove te::Tensor dependencies from TIR passes. (#5372)* [TIR][REFACTOR] Remove te::Tensor dependencies from TIR passes.te::Tensor is an useful object for tensor expression, but bringsun-necessary reverse dependency in TIR nodes such as Provide and Realize.This PR is a first step to remove this dependency. We will use Buffer in all the placeswhere the te::Tensor was used. The rough correspondence are:- Provide -> BufferStore- Realize -> BufferRealize- HalideCall -> BufferLoad.After this change, we can not use IRModule of PrimFuncs cleanly to represent TIRat any point of the optimizations. Buffer will serve as the abstraction for the TIR datamodels to represent the intermediate storages and their constraints.We still keep Realize/HalideCall and Provide as TIR nodes for now to make the change minimum.Right after ScheduleOps, we call SchedulePostProcToPrimFunc to canonicalize the temporary IRgenerated by TE(which contains these nodes) to the TIR.The TIR optimizations are now mostly migrated to to the pass manager.Followup PRs are needed to migrate the remaining few passes.* Fix dev tutorial",0
[PYTORCH]Unary Ops (#5378),5
"[TIR][REFACTOR] RewriteForTensorCore -> te/schedule (#5379)* [TIR][REFACTIR] RewriteForTensorCore -> te/scheduleRewriteForTensor depends on the schedule information, which makes it differfrom a typical pass(which should get all the information from the input TIR).As a result, we refactor it as a SchedulePostProc step for now.We should revisit it later as we introduce more support for tensor core patterns in the TIR.* Fix VTA to fit the new IR Pattern",0
[Blocksparse] Pipeline for lowering dense model to sparse-dense (#5377),5
[REFACTOR][TE] Inline -> te/schedule/operation_inline.h (#5386)Rationale: inline is a transformation used in te torewrite its internal expressions. It is not a formal IRModule->IRModule transform pass.Also removed the python test as the test is covered by stage.compute_inline.,3
"[ARITH] Remove the legacy Simplify, migrate to Analyzer. (#5385)The legacy Simplify/CanonicalSimplify are now a thin wrapper around the Analyzer.This PR removes these functions and migrated every place that requiressimplification to enforce Analyzer creation.The new API would encourage more Analyzer sharing and potentially enablecontext-aware analyzer-based simplification.",1
[ARITH] Remove legacy const pattern functions (#5387),4
Add ability to have multiple copies of same input to onnx_inputs. (#5389),1
"[Topi, ARM] Disbale Winograd for quantized tensors. (#5363)* [Topi, ARM] Disbale Winograd for quantized tensors.* Relaxing float",5
"Fix test_ir_type. (#5390)* The void return type is not None/nullptr, it's VoidType or   TupleType([]).",0
"Tf2 test fixups (#5391)* Fix oversight in importing tf.compat.v1 as tf.* Actually disable test for lstm in TF2.1Since the testing framework actually uses pytest, the versioncheck needs to be moved.",0
[PTYTHON] Migrate VTA TIR passes to the new pass manager. (#5397),1
[LLVM] Use ArrayRef<int> in calls to CreateShuffleVector (#5399)This switch was made in LLVM 11. Previously this function was expectingmask indices of type uint32_t. This variant is now deprecated.,5
[KERAS]Minimum & AlphaDropout op support (#5380),5
"Factor out import of common tflite.Operator in tflite frontend. (#5355)* Restructure imports in tflite frontend.These python modules are needed for every tflite file parsed.Factorize out imports of the common most ones.Now that the import of operator is common, asserts can be commonized.Loses 473 lines of duplication.* Only restrict to tflite.Operator",2
[Fix] Remove the duplicate PrintIR pass in Relay (#5403),0
Update dmlc-core to latest (#5401),1
"[TIR] Enhance Substitute, python bindings for Substitute/PostOrderVisit/IRTransform. (#5400)Substitute now takes a std::function to customize more replacing behaviors.Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>Co-authored-by: Siyuan Feng <hzfengsy@sjtu.edu.cn>",5
[Relay] Fix memory leak when accessing NDArray (#5413),0
Customize SI prefix in logging (#5411)* Customize SI prefix in logging* Include unit test,0
[LLVM] Replace calls to Type::getVectorNumElements (#5398)This function has recently been removed from LLVM 11. Use alternativeway to obtain vector element count (VectorType::getNumElements) whichworks for all LLVM versions.,4
"Don't remove() TempDirectory in __del__ after atexit hook runs. (#5414)* Use atexit to remove TempDirectory before interpreter shutdown. * Can't rely on complex functions from __del__ anyway. * Fixes warning message on my box:       Exception ignored in: <function TempDirectory.__del__ at 0x12be10680>       Traceback (most recent call last):        File "".../tvm/python/tvm/contrib/util.py"", line 55, in __del__        File "".../tvm/python/tvm/contrib/util.py"", line 51, in remove        File ""/usr/local/opt/python/Frameworks/Python.framework/Versions/3.7/lib/python3.7/shutil.py"", line 509, in rmtree        AttributeError: 'NoneType' object has no attribute 'path'",0
[TIR][REFACTOR] Remove ir_pass in favor of analysis/transform. (#5415)This PR removes ir_pass(old style pass functions) in favorof analysis/transform(new style pass manager).,1
[RUNTIME][CONTRIB] CoreML Runtime (#5283)* [RUNTIME][CONTRIB] CoreML Runtime* fix lint* fix CI* use xcrun to compile coreml model,0
[DOCS] Migrate HLS documents from md to rst (#5419),2
fix [RUNTIME][VULKAN] vkBuffer released before memory copy command send to GPU (#5388) (#5418),0
[Frontend] Asymmetric padding of convolution support (#4803),1
[cuDNN] Add cuDNN grouped convolutions support (#5319)Signed-off-by: Wei Pan <weip@nvidia.com>,1
[CI] Migrate Tensorflow and Tensorflow lite in CI to  2.1.0 (#5392)* Migrate Tensorflow and TFLite in the CI up to 1.15.2The latest stable version of Tensorflow and Tensorflow litein the 1.x series is 1.15.2. The tflite frontend is receivingsupport for versions of tflite > 1.14 but there is no consistenttesting.There are 2 failures already in the source base with tf 1.15and I'm concerned this will just get exacerbated over timeif we don't have CI picking this up and I view this as a steppingstone towards stepping CI to TF2.x.The test failures that I have commented will get issues raisedfor them as issues to be fixed.* Comment out run of qnn_mobilenet_v3_netThis is another test that fails with TFlite 1.15.2* Skip the qnn_mobilenet_v3 test in the pytest fashion.* Switch docker versions to support Tensorflow 2.1.0* Fix up pytest imports and usage.* Skip these tests currently for Tensorflow 2.1.0,0
"[DOCS] Migrate some markdowns to rst, fix sphinx3 warnings (#5416)* [DOCS] Migrate some markdowns to rst, fix sphinx3 warnings* Add note block",0
[BYOC] Use Non-Recursive Visitor/Mutator (#5410)* Non-Recursive AnnotatedTarget and MergeAnnotation* Non-Recursive AnnotatedRegionSet and RegionMerger,5
"[RFC] Pytest environment improvements (#5421)* [RFC] Pass pytest options globally.In many places having a global pytest flag is useful . For me with thebuild and test of tvm , I would like to be able to globally pass inpytest options as part of development flow or CI flows where one wouldlike to measure other things regularly that need measurements includingpytest coverage data that I would like to experiment with across the stack.This has been achieved with an additional setup-pytest-env.sh file intests/scripts rather than putting in something in every single task testscript and something I would like to avoid.This now means the -v option to pytest is superfluous. I did considerhaving a pytest.ini file but that doesn't allow me to pass any oldenvironment variable in and this seems to be the compromise.* Improve other use case documentation* Rationalize pytest environment.* Remove the setting from docker/with_same_user.* Take the opportunity to migrate common PYTHONPATH andTVM_PATH into the common environment setting.* Fixup vta fsim* Be more explicit with common PYTHONPATH* Fix python path for task_python_vta_fsim.sh properly* Fix nit in documentation.",0
[MXNET]DepthToSpace & SpaceToDepth Operator (#5408),5
Add option to specify flatbuffers location (#5425),1
[FRONTEND][MXNET] support elemwise logic ops (#5361),2
"[PY][FFI] Introduce PyNativeObject, enable runtime.String to subclass str (#5426)To make runtime.String to work as naturally as possible in the python side,we make it sub-class the python's str object. Note that however, we cannotsub-class Object at the same time due to python's type layout constraint.We introduce a PyNativeObject class to handle this kind of object sub-classingand updated the FFI to handle PyNativeObject classes.",1
"[PYTORCH]where, addcdiv, addcmul op support (#5383)* [PYTORCH]Where, addcdiv, addcmul op support* Review comments fixed",0
"[FRONTEND][TFLITE]Gather, StridedSlice op support added (#4788)* [FRONTEND][TFLITE]Gather, StridedSlice op added* Review comments fixed",0
"misc fixes for ROCm (pointer lifetime, runtime::String refactor) (#5431)",0
"Corrected TVM autotuning on GPU (#5432)Added missing ""tir"" in tvm.tir.analysis.verify_gpu_code(f, kwargs)",1
[RUNTIME][OBJECT] Introduce static slots for common objects. (#5423)The _type_child_slots can be used to enable quick type checking optimizationby checking the whether the type index is within the bound.This PR enables these static slots:- Introduce a static assert to avoid the scenario when a developer forget to  _type_child_slots when the field is set for the type's parent.- Revamp and assign static type index to common runtime objects- Add a DumpTypeTable call to allow developer monitor the current situation  of type table and offers suggestions for the slots(ideally the slots equals  the number of children so there is no overflow.,1
"[RELAY][PYTORCH]cosh,sinh,log2,log10,log1p op support (#5395)* [RELAY][PYTORCH]cosh,sinh,log2,log10,log1p op support* Review comment fixed* Gradient testcase added",0
"[PYTORCH]Rsub, Embedded, OneHot ops support (#5434)",5
fix miopen pad (#5433),0
Add TopK to ONNX Frontend (#5441)* Add TopK to ONNX Frontend* respond to review comments,1
[CodeGen] Cleanup generated code (#5424)- remove unnecessary white spaces from storage kind- do not start a new scope for vectorization as temporary  variables are alll uniquely generated.The above two changes make vectorized code much cleaner.Signed-off-by: Wei Pan <weip@nvidia.com>,1
[RELAY] Move frontend utils (#5345)* [RELAY] Move frontend utilsThe util file currently under frontend is used fromoutside of frontend (in qnn/op/legalizations). This suggeststhat the file should be pushed up to a higher level.The benefit from this change is that importing qnn no longeralso imports all the frontends.* Inline get_scalar_from_constantChange-Id: I1cc64e9ecb0eadb6ac0f7b62e6ea174644af4ad4* Remove util.py from RelayChange-Id: If9cd7cf3fc0bd1861a3a9b5604f338e084d8db96* Shorten functionsChange-Id: Ieb537d82e6ee52421ff05a90cd00a03679ffebf2* Line lengthChange-Id: I1d216b7e73a060c4f118f5da50ce58b18eba907f,2
[KERAS]Embedding layer (#5444),5
[Docs] VTA install doc migration from md to rst (#5442),2
Improve IntervalSet's floormod (#5367),5
"[ONNX]GatherNd, Round, IsNaN, IsInf (#5445)",5
[relay][topi] Add operation relay.nn.dilate() which calls topi.nn.dilate() (#5331)* Add operation relay.nn.dilate() which calls topi.nn.dilate().* Fix typo* Set op pattern to injective,0
[Pytorch] fix translation of transpose when axis argument is as a list (#5451),0
[TFLite Runtime] Add TFLite Runtime dependencies to CI CPU docker build (#5437),1
Add RoiAlign to Onnx frontend (#5454),1
[Topi][Cuda]Optimizations of global_ave_pool for NHWC layout (#5450)* Optimizations of global_ave_pool for NHWC layout* Optimize the code format to pass inspection of pylintCo-authored-by: Shawn-Inspur <wushaohua@inspur.com>,2
"Improve Docker cache reuse by pointing to the current version of the image, (#5466)on top of another image to be used as reference.",5
[COMMUNITY] @liangfu -> committer (#5460),3
[Frontend][TFLite] L2_POOL_2D operator (#5452)* TFLITE fill and splitv ops* l2_pool_2d op changes in comment* TFLite l2_pool_2d op added test case in main* TFLite L2_POOL_2D added check for quantized input,1
[Frontend][TFLite] support for FILL and SPLIT_V operators (#5330)* tflite spliv ops* TFLITE fill and splitv ops* TFLITE fill and splitv ops* TFLITE fill and splitv ops* remove unnecessary operator check,4
[TFLITE] Match TFLite shape for SSD custom op (#5473)This patch ensures that the output shape from TVM'sDetection_PostProcess is the same as TFLite's andexpands the unit test to confirm this.Change-Id: If5db95741533f131241dfebbaa7708dbd528fe70,3
"[CODEGEN][CUDA] Fix a bug when vectorized load&store was involved for… (#5428)* [CODEGEN][CUDA] Fix a bug when vectorized load&store was involved for ""char2""* Add unittest for char2* vector element load support char2&add some unittest for vector element load* Merge common up logic&Support char3&Add unittest for char3",0
[CI] update ci-gpu to the latest (#5469),1
[BYOC] Bind constant tuples in graph partitioner (#5476)* Bind constant tuples in the graph partitionerChange-Id: I815b32b5445a536c1837369b04f67dbbb0aed900* Add partitioning testChange-Id: I3a492ec8d1beab4830214e3bc8da2a7c80771ca4* Rename test targetChange-Id: Ie32f37c1395ff597c0047ad3a93ed04ce3f3125d,1
[VTA][Runtime] clear sram index in reset (#5470)Co-authored-by: Zhang Hao <zhanghao@4paradigm.com>,5
[intrin] a few more math functions (#5468),5
[FRONTEND][TFLITE]Logical not op support (#5475),2
[team] add reviewer kparzysz-quic (#5482),1
[RUNTIME] Improved Packed FFI for optional. (#5478)Allows Optional<NDArray> and module to be passed with the right type code.,4
"[VTA] Fix VTA compile issue (#5481)* [VTA] Fix Pynq driver build issue.Issue:When doing vta compile in xilinx FPGA, the pynqdriver.cc report cannot find <vta/driver.h>Solution:add related path.* Fix libvta load fail issue.issue:run vta on pynq board libvta.so load failed.solution:fixed VTA.make logic issue",0
[Fix] Add ConstantNode to IsAtomic (#5457)* add constantnode to atomic* Add ToANormalForm to FoldConstant,0
"[RUNTIME][uTVM] AutoTVM + uTVM for Cortex-M7 (#5417)* Prototype for micro TVM.* Cleanup and sync micro tvm prototype.* Use /std:c++14 with MSVC. * Per tqchen: project has already moved to C++14 * Presubmit failed for code that built locally on gcc.* fix ASF lint, and fix add_asf_header too* Compiles with USE_MICRO=OFF.* Cleanup TargetPtr and word size representations.* fix compile warning* address logan's comments* address logan and liangfu comments* address thierry's comments* address u99127, liangfu, tmoreau89 commentsCo-authored-by: Logan Weber <weberlo@cs.washington.edu>",0
Removing older Object detection TFlite test (#5477),3
[IR] Initial stab at std::string->String upgrade (#5438),5
"Make ""none"" DataType explicit (#5491)* Make ""none"" DataType explicitThe None data type is created when converting an empty string to DataType.Add functions to create it and recognize it. Convert it to the ""void"" LLVMtype in LLVM codegen.* Rename ""none"" to ""void""* Map VoidType:Type -> Void:DataType in GetRuntimeDataType* Map Void:DataType -> VoidType:Type in GetType",1
"[Hexagon] Change ""scalar"" and ""stack"" in IDL from ""inrout"" to ""in"" (#5487)Co-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>",4
[MXNET]broadcast and logical op support (#5461)* [MXNET]broadcast and logical op support* Review comment fixed,0
[REFACTOR][BOYC] Non recursive partitioning (#5493)* non recursive partitioning* refactor maps* rebase upstream* refactor shared output* address commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>,1
Link necessary libraries when building runtime for Android (#5496)- Link libgcc to enable use of thread-local storage (ThreadLocalStore  requires emutls).- Link liblog when building with Hexagon support.,2
[TFLite] Model importer to be compatible with tflite 2.1.0 (#5497),5
[Rust] Fixes for wasm32 target (#5489)* [Rust] Fixes for wasm32 target* [Rust] Add test for wasm32* allow cargo config to be into repo* Disable wasm tests in CI,0
[uTVM] Reset target and wait for runtime initialization on connect. (#5499)* This ensures a clean runtime environment for tuning and evaluating   tasks. * This patch assumes that the code flashed to target executes the ARM   BKPT or RISC-V EBREAK instructions after startup.,2
bump tophub rocm version (#5504),5
Fix Canonical Simplifier (#5505),0
[RUST][RUNTIME] Fix workspace (#5503)* [RUST][RUNTIME] Fix workspace* use ok_or_else instead of ok_or,0
"[REFACTOR][RPC][PROCOTOL-CHANGE] Modularize the RPC infra (#5484)* Update dmlc-core which was mistakenly overriden* [REFACTOR][RPC][PROCOTOL-CHANGE] Modularize the RPC infra.This PR refactors the RPC protocol to make it more modularized.- RPCSession: represent a set of features that need to be implemented- RPCEndPont: End point that forwards the RPCSession requests over a communication channel.- RPCModule: Exposes an RPCSession as an rpc device in the TVM Runtime API.In the new design, the local machine is presented as a special case of RPCSession.The remote is just another client session that calls into RPCEndPoint.The RPC communication path is as follows.```client -> ClientSession -> EndPoint[client@n0]-> networking[between n0 <=> n1]-> EndPoint[server@n1] -> LocalSession[@n1]```Because of the new modular design, we can now chain more sessions together.For example, we can now run the following proxy setup (testcase in test_runtime_rpc.test_session_constructor).```client -> ClientSession -> Endpoint[client@n0]-> networking[between n0 <=> n1]-> Endpoint[server@n1] -> ClientSession -> Endpoint[client@n1]-> networking[between n1 <=> n2]-> Endpoint[server@n2] -> LocalSession[@n2]```We can also implement other types of Sessions.As an example, We introduced a PopenSession that communicates withthe another process via a pipe.We also add more comments about the internal of the RPC.The communication protocol is simplfied using a similar convention as PackedFunc.This allows us to further reduce the amount of special remote syscalls.Due to the major improvement and simplification, we are making a non-compatible update to the RPC protocol.It means that the client and server needs to be upgraded to together in order for it to function correctly.This PR also introduces a versioning mechanism to the current RPC procotol,so that future upgrade will be produce more user friendly with error messages.* Address review comments* Remove ld library path",0
[RPC] Call sync in remote cpu to gpu copies (#5512),5
"[QNN] Support CallNode inputs in qnn.concatenate (#5360)* [QNN] Support CallNode inputs in qnn.concatenateCurrently, qnn.concatenate assumes that its 1st arg(data) is a TupleNode. This may not necessarily be trueif the input is a CallNode which returns a value oftuple-type. This patch handles the CallNode case byinserting TupleGetItemNodes.* Fix lint* Add testChange-Id: I40b55517b8b1dabbeca89337f80c0c8e62e34981* Use isinstanceChange-Id: I731a231113c5214528373ef52b603a9f05ec502a* isinstance fixChange-Id: Ib3495532f6e4feb5aae3d3096cedd4dc4676cdb4* Use elif/else ifChange-Id: Id8123ea2dd9ce3d8267609de7b5602bb84b084fb* Fix lintChange-Id: Ib6899bb22260575aa3f5d8b51b5d2a0277ee2b10* Lint fixChange-Id: I56cf1930315344e42d956818a6c68e80836ae786* SpacesChange-Id: I3edab192e32bafa9ffdc915315791c63279d85dc",0
[RPC][BUGFIX][BACKPORT-0.6] Fix bug in rpc ring buffer shrink (#5516),0
"[PATCH] [ring_buffer.h] Improve commentary for RingBuffer (#5518)bytes_available refers to the number of bytes used in the ringbuffer. At the same time, fix a typo.",0
[TFLITE]Nit: Function names made consitent (#5515),5
fix prelu importer and add tests: (#5521),0
[RPC] Fix the multihop cpu case (#5522),0
[RUNTIME] Improve PackedFunc robustness (#5517)* [RUNTIME] Improve PackedFunc robustness- Add static assert to warn about unsupported type deduction.- Always inline template expansions for PackedFunc calls.* Fix style issue,0
"LRN only supports 4D tensors, remove it from alter_op_layout (#5520)",4
Fix an issue with Upsampling and update one test to hit the broken usecase (#5530),0
[TFLITE]Select op support for tflite frontend (#5486)* [TFLITE]Select/Where op support for tflite frontend* Review comment fixed* Review comment fixed,0
[FRONTEND][TFLite] Fully connected op conversion made in sync with TFLite (#5510)* [FRONTEND][TFLite] Fully connected op conversion made in sync with TFLite* [1] Test case added* [2] Review comments handled* [3] Prints removed,1
[TOPI][Winograd] Optimization of Conv2d Winograd algorithm on Tensor Core (#5485),5
"Cache PrimExpr instead of raw pointers in bound analyzer (#5533)The objects that the raw pointers point to can be deallocated and newobjects can be allocated at the same address, all while these pointersare still in the cache. This can lead to unexpected behavior, forexample to calculated bound conflicts with previously cached values.Caching PrimExpr will prevent the objects from being deallocated whilethe cache is active.",1
fix a few bugs with shape inference and types in the onnx importer (#5534),0
[Frontend][TFLite] ADD_N operator  (#5474),1
"[WEB][RUNTIME] TVM WebAssembly JS Runtime (#5506)* [WEB] Remove the old web runtime* [WEB][RUNTIME] TVM WebAssembly RuntimeThis PR introduces a brand new TVM web runtime based on the WASM standard API.Main highlights:- The new runtime is rewritten using the Typescript.- The new runtime now directly interfaces with WebAssembly's standard API,  instead of relying on emscripten's API.  This change will make the js runtime more portable to runtime variants.  For example, we could also try to make it interface with the tvm's rust runtime implementation.- System library can be provided through WASI  - We also build a hack to enable Emscripten to generate a WASI like    bundle for runtime environment on the Web.- The wasm generation now uses the mainlin LLVM.- Dynamic link(dlopen) is not used due to limitation of wasm,  instead we rely on the recent new RPC refactor to directly  restart a new session for each wasm binary sent to the RPC.* Address review comments* Skip tensorcore test",1
[RELAY][ONNX]ReduceLogSumExp Operator support (#5453)* [RELAY]LogSumExp Op Support* [ONNX]LogSumExp Op Support,2
[RPC][BUGFIX] Fix remote device sync (#5538),0
[Refactor][std::string --> String] IRModule is updated with String (#5523)* [std::string --> String] IRModule is updated with String* [1] Packedfunction updated* [2] Lint error fixed* [3] Remove std::string variant,0
[RUNTIME] Store nullptr PackedFunc as nullptr for better error propagation (#5540),0
[Relay-TFLite] FP32 and Quantized Object Detection Model (#5479)* TFlite e2e FP32 Object detection model* Fix test* [Relay-TFLite] Quantized activations* Flexbuffer parsing* Lint* Relaxing checks.* Github reviews* commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-34-212.us-west-2.compute.internal>,0
"Changes to cpp_rpc to make it work on Android (+ Hexagon offloading) (#5535)* Changes to cpp_rpc to make it work on Android (+ Hexagon offloading)- Implement getNextString to break up std::string into words. stringstream  just doesn't work on Android.- string::find_last_of doesn't look for the last substring, but the  last character from a given string.- Use SIGTERM to terminate processes (this isn't necessary, but using  SIGKILL is not a good practice).- Convert ""./rpc"" to a full path. When a module is uploaded and offloaded  to Hexagon, the dlopen on Hexagon needs an absolute path (or a path  without directories).* Only set the absolute patch on non-Windows platformsWindows has different macros for the maximum path length.",4
Add Onnx Pad v11 (#5539),1
fix restructured text (#5541),0
[CRT]fix to reduce RAM size during loading model (#5507)* [CRT]fix to reduce RAM size during loading model* Release graph_json memory immediately after reading,0
Load platform specific lib for tvmdsoop instead of only so (#5542),2
"[RPC] Improve RPCServer AsyncIO support. (#5544)* [RPC] Improve RPCServer AsyncIO support.When the RPCServer is in the async IO mode, it is possible for the serverto directly serve async function that may return its value via a callback in the future.This mode is particular useful to the web environment, where blocking is not an option.This PR introduces the Async support to the RPCSession, allowing the AsyncIO driven serversto serve the async functions. These functions will still be presented as synchronized versionon the client side.Followup PR will refactor the web runtime to make use of this feature.* Address comments",1
[Rust] Add first stage of updating and rewriting Rust bindings. (#5526)* Add tvm-sys* Use as_mut_ptr* Address CR feedback* Update rust/tvm-sys/src/datatype.rsCo-authored-by: Nick Hynes <nhynes@berkeley.edu>* Final CR comments* Fix find and replace error in frontendCo-authored-by: Nick Hynes <nhynes@berkeley.edu>,0
[TE] Fix MakeLoopNest for warp memory (#5382),0
[TIR][Printer] text format printer considering future parsing use (#5483),5
[Optimization] Warp level reduction support for CUDA (#5498)- Added the warp level reduction support- Upgraded shfl intrinsics to the sync version.- This is the building block for scheduling softmax like operations.Signed-off-by: Wei Pan <weip@nvidia.com>,1
"A clone of test/python/unittest/test_runtime_micro.py, however (#5546)modified to run specifically on ARM cortex-M hardware, whichcurrently is just the STM32F746 discovery board.Signed-off-by: Tom Gall <tom.gall@linaro.org>",2
[CI] Install wasmtime for WebAssembly tests (#5494),2
"Apparently, ONNX Conv with no 'pads' defaults to zero padding (#5548)",1
"[WEB] WebGPU support (#5545)This PR introduces WebGPU support to tvm.The WebGPU runtime is directly built in javascript(as WebGPU uses JS as the first class citizen API)and exposes back to the tvm's runtime via PackedFuncs.One important note is that `ctx.sync` is not async.This is due to the fact that WebGPU is a purely async API and we cannot block in the web environment.So the current best way to use the js api is to wrap things in an async function.When copy a GPU array to CPU, `await ctx.sync()` need to be called to wait for copy completion.We use a AsyncIO rpc server to serve the async functions to the clients.",5
[TOPI][RELAY][TENSORFLOW]Math ops added (#5502)* [TOPI][RELAY][TENSORFLOW]Math ops added* Extra newline removed* CI fix* Review comments fixed* Review comments fixed,0
[RUNTIME] Hexagon driver for offloading kernels to simulator (#5492)* [RUNTIME] Hexagon driver for offloading kernels to simulator* Add sim_dev as external project when building with Hexagon/sim support* Change target CPU for sim_dev to v60,1
"[LINT] clang-format the h,cc,m files. (#5557)This PR prepares for our migration to use the clang-formatas part of the linter system.",2
"[BYOC, MergeComposite] Add additional check before re-using the cached match (#5552)* Add additional check before re-using the cached match in merge composite* clean up ExtractPattern calls",1
"[WEB] Setup lint, doc, test (#5556)",3
[CI] Update ci-cpu to bionic (#5555),1
[CI] Update ci-cpu to bionic (#5554),1
[Fix] Fix conv2d alter op for arm cpu (#5532),0
"[FRONTEND]onnx, mxnet, pytorch mathops added (#5561)",1
Fix topi test for tensorcore (#5563),0
[Refactor][std::string --> String] IR is updated with String (#5547)* [std::string --> String] GlobalTypeVar is updated with String* [std::string --> String] GlobalVar is updated with String* [std::string --> String][IR] ADT is updated with String* [std::string --> String][IR] OP is updated with String* [std::string --> String][IR] Attrs is updated with String input* [std::string --> String][IR] GlobalVar is updated with String* [std::string --> String][Test] Pyconverter is updated with String change,1
[DOCKER] Fix vulkansdk in the ci-gpu (#5566),0
[CI] reintroduce docker stage for wasm tests (#5565)* [DOCKER] Introduce ci-wasm* Add Jenkinsfile* Rename prepare to prepwasm so it won't run by default,1
[CI] Update ci-lint to use the latest image that contains clang-format (#5568),1
[DOCKER] Add clang-format and nodejs to ci-lint (#5567),1
"[TARGET] Phase out WebGL (#5570)The graphics API is moving towards next generation.Vulkan/Metal on the native and WebGPU on the web.Due to the limited programming model, we cannot get the best compute performance in WebGL.Now that the mainline already have both WebGPU and vulkan support, this PR phases out WebGL.",5
[LINT] Enable clang-format. (#5572)* [LINT] Enable clang-format.* Add more docs,1
[CI] Update the ci-gpu to the lastest build with the new vulkansdk. (#5571),1
[Relay] enable blocking format in x86 conv2d and fold scale axis (#5357),5
[CI] Fix clang-format error (#5577),0
Allow ubuntu_install_darknet.sh to work in both 18.04 and 16.04 (#5574),2
[PYTORCH]expand bug fix (#5576),0
"[CI] Enable llvm-11 and llvm-10 in build tests, recover webdocs. (#5579)This PR ties up the last loosen end of the recent CI update.",1
[PYTORCH] Support max_pool2d_with_indices (#5549)* Use real output name instead of node_name* Add pytorch max_pool2d_with_indices converter.* Add test for maxpool2d with indices* Add explicit assert for single output* Only consume output (not indices) from max pool 2d with indices* undo change,1
[Relay] Fixed bug in attribute parsing for pool layers. (#5582)* Fixed pooling bug.* Added tests and fixed more cases.,0
[RELAY][TF] Support symbolic newshape for Reshape (#5429)* [RELAY][TF] Support symbolic newshape for Reshape* Only need to pass data* Use MakeReshape() in Reshape()* Change newshape to Expr* Create a template for Array<T>* Fuse reshape when newshape is constant* Make newshape Optional* Use bool() of OptionalCo-authored-by: Li Xiaoquan <xiaoquan.li@denglin.ai>,1
Add prim::device op (#5584),1
Fix the runtime raise error (#5586),0
"[RELAY][Convert Layout] Specify additional layouts in convert layout pass (#5422)* [RELAY] Specify additional layouts in convert layout pass* This patch means that you can specify an additional layout, rather than using the layout chosen by default during conversion.* This is specifically useful for external codegen when a 3rd party library needs to target a specific kernel layout for example.Change-Id: I3ef9cf45ead574801870a38af9768f93e29aab10* Use mapping of op name to list of desired layoutsChange-Id: Ibd691a3cb93e73a394f36112668ad52a84c7d5a2* Fix issue with code blockChange-Id: Ibb4e38c05ad4312b7dea845be699b8d5d57e0a94* Address comments, Improve tutorialChange-Id: Ib824eead329d551c338234de3b2d814693afd0ec* Fix lintingChange-Id: Ie9e1891f590b3a7496a56ff8362cdda9d4b5fa75* Test uses NCHW default layout. Unrelated issue with NHWC.Change-Id: I1c16f0db73db56f5e9536db3fe5eb2624c3b595c* Fix mistake in tutorialChange-Id: I944041245d27af262dc96f1cd8117f1f19272062* Address multiple commentsChange-Id: If33a1e34acd8fc37d1c7797ee189a6448a392672* Improve tutorialChange-Id: Ib04142c94c7958ab5067947d2ff4c84354e3d0c5* Fix Clang-formatChange-Id: Ieff39e3f0817d22579c68b3287e972a3b0fcfbc8",0
Add a quantized conv2 unit test for the tflite front-end (#5558)Signed-off-by: Giuseppe Rossini <giuseppe.rossini@arm.com>,1
[Relay][Transform] Safe check added for Merge Composite (#5562),1
"[MXNET]abs, round, reciprocal, sign, softsign, hard_sigmoid (#5587)",5
[Hexagon] One more fix for concurrency count (#5589),0
Fix JSON graph dumping. (#5591)* Previously this function placed a JSON-escaped string containing   the JSON-encoded graph.,0
[DOCS] Improve document in reflection (#5593),2
"Overestimate binary size for microTVM compiled binaries. (#5590)* Overestimate binary size for microTVM compiled binaries. * Currently uTVM binary section sizes are computed by summing the   sizes of all symbols in the section. * This method produces errors because it presumes the linker works in   a particular way, rather than analyzing the linked output. * As we intend to move away from linking inside TVM (RFC   forthcoming), just using this stopgap to make forward progress   until then.* address weberlo comments* fix regression (use 64 bit word size)",0
[TFLite Runtime] Fix bug and re-enable RPC execution test (#5436),0
"[Relay][VM] Memory planner (part 1) (#5144)* Start on memory planningWIPMove to test_memory_passes.pyWork on memory planningPost-rebase and VM changesPlumb through the offsetsBasic tests all pass, fix offset to data buffer.Fix compile errorsFix wsApply suggestions from code reviewCo-Authored-By: Haichen Shen <shenhaichen@gmail.com>Address CRUpdate src/runtime/vm/vm.ccCo-Authored-By: Haichen Shen <shenhaichen@gmail.com>Fix another commentFix lintFixFixFixLint is done?FixMore fixTrying to debugNo clueFix lint* Fix docs* Disable aggressive constant eval* It works* Fix lint* Found issue with dynamic* Fix the pass, but runtime segfaults* fix scalar tensor, test_any_elemwise passes* Fix split pass* Fix 0-rank issues* Fix* debug* apply Haichen's patch and clean up* lintgit add .* fix serializer and test_tyck_alloc_tensor test* Fix the constant lift pass in presence of closures* Restore old finder* Fix rebase issues* Fix* Fix* Fix issue coercing the shapes incorrectly from i64 to i32* Fix linting* Fix clang format* Format memory.cc* Fix 0-rank case* Add fix for (0,) shape* Ignore shapes for now* Apply suggestions from code reviewCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>* Update src/runtime/vm/executable.ccCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>* Fix* lintCo-authored-by: Zhi Chen <chzhi@amazon.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>",0
Add ostream formatters for TargetPtr/TargetVal. (#5592),1
"Pattern Language, Matcher, Rewriter, and Function Paritioner (#5231)",5
"[Reduction] Fix cross thread redunction (#5551)- The predictions were not correctly applied after transformation.  This leads to normal reduction itervar appearing outside of the loop,  which is undefined. See detailed comments.Signed-off-by: Wei Pan <weip@nvidia.com>",0
Fix TVMArray layout on device (#5599),0
[LLVM] Represent alignment information in LLVM IR (#5598),5
Add debug mode to tempdir() (#5581),0
[PYTORCH]ImplicitTensorToNum support added (#5603),1
[PYTORCH]Matmul fix for batch_matmul (#5604),0
fix rpc server bug on VTA (#5607),0
[REFACTOR][IR] Streamline ir/op Registry (#5609)* [REFACTOR][IR] Streamline ir/op RegistryThis PR refactors the attrregistry mechanism in the ir/op intoa separate common base. The common base will provide a foundationfor other attr related registries such as target and pass.We also streamlines the terminology of the registry API.- Use AttrMap for the column maps returned by the registry- Use RegEntry to refer to the registry entry.* Address review comments,1
[TFLITE]GATHER_ND (#5508)Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>,5
[CUDA] Fix codegen for warp shuffle intrinsics (#5606)* fix shfl intrin* improve test_lower_warp_memory_cuda_half_a_warp,0
Fix a typo. (#5611)Co-authored-by: Zeng Liyong <liyong.zeng@streamcomputing.com>,0
fix pattern topological order (#5612),0
"[BYOC] Remove kCompiler attr from external functions (#5615)Functions destined for external codegen keep their kCompiler attribute which means SkipFunction returns true when running a pass over such functions during the codegen step. This makes sense during graph partitioning, however when lowering the functions for codegen the is no reason to keep this behaviour.Allowing this behaviour will mean a codegen can run a pass on functions only intended for one 3rd party library. Specifically, allowing pre-processing of a series of sub-graphs right before it is passes through codegen. This helps ensure that the functions destined for the 3rd party library are in the expected format. For example, we may want to ensure that these functions have a kernel layout of OHWI because the 3rd party library only supports OHWI. This wouldn't be possible before partitioning the graph as we don't know how the graph will be partitioned ahead of time.Change-Id: Ia68b9da335ef1acfc405a8528aac823de60a65c2",4
[Relay]Improve Shape Func handling for Tuple inputs (#5467)* Improve Shape Func handling for Tuple inputs* Fix lint* Improve* Fix build,0
[Relay][Refactor][std::string --> String] Relay updated with String (#5578),1
[KERAS]Global MaxPool3d and AvgPool3d support (#5098),5
[IOS] Fix build error of iOS RPC (#5621)* [IOS] Fix build error of iOS RPC- Update to C++14- Use the latest RPC protocol- Resolve CoreML dependency* Fix clang-format error,0
Fix three typos (#5620)Co-authored-by: Zeng Liyong <liyong.zeng@streamcomputing.com>,0
[Frontend][Tensorflow] Gather nd bug fix for one dim support in tensorflow (#5588)* [Frontend][Tensorflow] Gather_nd one dim support added* Test case added* Doc error handled* Review comment handled: reverting new attr introduced* Check added at mxnet frontend* Doc error handled* TFLite test case failure resolved,0
[MXNET]MaxPool3d and AvgPool3d Ops support added (#5614),1
[PYTORCH]ReflectionPad2d op (#5624),5
"[BYOC][MergeComposite] if root->args[i] isn't a CallNode, then Donwcast<Call> will check fail (#5623)we needn't execute L131 ""call_map->Set(arg, new_arg)"", because when argis CallNode and root->args[i] is not CallNode, new_arg will be a nullpointer. There is no point in caching null pointer.Signed-off-by: windclarion <windclarion@gmail.com>",1
[DOCS] Move the api docs to the api subfolder (#5626)* [DOCS] Move the api docs to the api subfolder* Update numpydoc location* Ignore 403* make sure folder exists,1
"[RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph (#5616)* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraphIf the annotated compiler region contains multiple outputs wheresome of the outputs are tuple output, the current PartitionGraph willcreate tuple of tuples. This will not be handled by the runtime.This commit flattens the such tuples and re-create them after thecall site of the partitioned function.Change-Id: I4e7ccbda73c129a9f4ae8705d5c9f2af6ab99ef6* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph    *code refactor : extracted the passes as a sequentialChange-Id: If4bc00b00a96fa244358d602fc1a361498342f46* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph   *further refactorChange-Id: I69ddd0e835e88ef97da8a3a3b949be3f7b619c02* [RELAY][BYOC] Fix the creation of tuple of tuples in PartitionGraph    *class description comment amendedChange-Id: I55720bf0467c96e979e1ab56c40d9d209e0f9456",0
"[NODE][PASS] Introduce config to PassContext. (#5631)This PR introduces a new config field to the PassContextto allow it store arbitary config values.To make sure that the config is validated, we allow each passto register the config key they would expect and the corresponding types.We also introduce a CreateObject from Map<str, Object> to allow config creationfrom a json-nest(like in vscode) in python.We added an example of UnrollLoopConfig.Followup PR should migrate the passes to use the new config field.",1
another cmake fix (#5630),0
Fix typo in test script (#5635),0
Label Pattern Partitions (#5627)* Label Pattern Partitions with a default label to prevent nested partitions and an optional user supplied-label* Add node names in topological order to Partitioned attribute* respond to review comments* move partition tag into const in attr namespace,1
"[RELAY][PYTORCH]Resize3d, Upsample3d op support (#5633)",5
[TUTORIAL]TFLite QNN Tutorial (#5595)* [TUTORIAL]TFLite QNN Tutorial* Review comments,5
Extend AttrPattern to support CallNode and FunctionNode attributes (#5637)* Extend AttrPattern to support CallNode and FunctionNode attributes* Update tutorial and add breaks* add func attr test,1
[DOCS] Fix the QNN TFLite tutorial build (#5641)* [TUTORIAL] Fix execution error of TFLite quantized tutorial* Assign TensorCore to docs build,0
[RUNTIME][VULKAN] Seg fault in WorkspacePool's destructor (#5632) (#5636)* [RUNTIME][VULKAN] Seg fault in WorkspacePool's destructor (#5632)* fixed this issue by changing WorkspacePool's destruction order* make line < 100 charactors long,0
[PYTORCH]Padding support (#5638),1
Remove unnecessary print (#5642),4
[CI] Allow CI_PYTEST_ADD_OPTIONS to be unbound. (#5644)This patch allows the test script to execute normallywhen CI_PYTEST_ADD_OPTIONS is not available.,1
"[Runtime] Introduce runtime::Array (#5585)* Introduce runtime::Array* Sync with dmlc-core* Tests added: size, capacity, empty, front, back, push_back, pop_back, insert * 2, erase * 2, resize, reserve, clear",1
[CI] Add log check to the sphinx gallery docs (#5643)* [CI] Add log check to the sphinx gallery docsThis PR add log check to sphinx gallery tutorials to preventthe case when sphinx failed to capture the error in tutorials.* Fix the status,0
[RELAY][BYOC] Preserve type information in Merge Composite (#5640)Keep the type information when extracting patternsso that it can be used as part of 'check' functions.Change-Id: I16cc70c3d013a794d2ceefb5bec815129c7b8825,4
Add a check Callback to the Pattern Paritioner (#5646)* add a check callback to the paritioner* fix doc string* fix unit test spelling* add a test with types,0
"[Relay, Topi][OP] Correlation (#5628)* [Relay,Topi] Correlation* fix* move* typo* Update test_topi_correlation.py",0
HG: Commit message of changeset 6281661. (#5622)[Relay] Move compiler_begin/end_op to local static objects,4
[AutoTVM] Update XGBoost verbosity option (#5649),1
[RUNTIME] Resolve constexpr issue in debug mode. (#5651)static constexpr is a bit weird before c++17.They are not inlined by default and does not have symbols after compilation.It usually isn't a problem when they are inlined(in c++17 they are inlined by default).But will create compilation error when passed to functions that take (const)references.This PR fixes the problem so that we can compile on debugmode.,0
"µtvm debug improvements (#5648)* Forever loop in UTVMDone to aid debugging* Use parameter and callback function as a micro debug hook. * Previously, users had to uncomment a region of code in   micro_session.cc and recompile to debug. Now they can pass in a   key in the micro.Session config:       config = tvm.micro.device....generate_config()       config['debug_func'] = _python_launch_gdb       with micro.Session(config) as sess:         ....* clang-format* Only forever loop on device (on host this blocks unittests)",0
[REFACTOR][IR] Migrate IRModule ObjectRef to not-null (#5654),5
Upgrade XGBoost to latest (#5658),3
Increase bss section size. (#5660)* Likely broken in PR 5590.,5
"[PatternLang] Convert PatternGrouper to do pre-order, non-recursive analysis (#5653)* make the PatternGrouper iterate over the input Expr in a non-recursive pre-order fasion* add a comment",1
"[Relay,Topi][OP] affine_grid and grid_sample (#5657)* [Relay,Topi][OP] affine_grid and grid_sample* lint",5
[TIR][BUILD] Remove buffer params from pass config. (#5652)Buffer configurations can be passed during constructionand does not need to be part of the build config.This is a refactor step to simplify the BuildConfig for the PassContext migration.,4
handle likely in IRMutatorWithAnalyzer (#5665),5
[TOPI] Improve CUDA softmax scheduling (#5600)- Do not use multiple kernels- Schedule with warp reductions- Fixed a bug on the lower warp memory pass- Fixed warp shuffle intrinsics for the nvptx backend.Signed-off-by: Wei Pan <weip@nvidia.com>,0
"[Relay][Op]Support symbolic TopK, Ones, Zeros and Full (#5459)* Support symbolic TopK, Ones, Zeros and Full* Fix pylint* Add docstring for topk shape func* Fix grad* Fix lazy_gradient_init* Fix parser* Fix print ir text* Fix lint* Improve pattern_util* Fix topk* Fix build* Use Optional for attribute* Fix clang-format* Minot fix* Fix pylint* Fix build warning* Fix parser* Move ToScalar* Fix lint* Fix lint* Make topk shape func as data independent when k is constant.* Fix lint* Minor fix",0
[PYTHON] Add buffer name when creating tensor bindings (#5670),1
"[REFACTOR][TIR][API-Change] Migrate BuildConfig to PassContext. (#5668)* [REFACTOR][TIR] Migrate BuildConfig to PassContext.This PR migrates the TIR configurations from BuildConfig to thePassContext used by the unified IR.Moving forward, PassContext will be the unified way to configure passes in the TVM stack.Changes- Refactored TVM_PASS_REGISTER_CONFIG_OPTION to take in the reference type.- Removed BuildConfig.- Migrated the passes to use PassContext.* Update include/tvm/ir/attrs.hCo-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>",1
[Doc] Misc doc fix (#5672),0
[C++ RPC] Fix C++ RPC build problem on Linux (#5671),0
enable amd_apu device on vulkan target (#5659),5
[AutoTVM][TOPI] AutoTVM incorrect measurement (#5511)* [AutoTVM][TOPI] AutoTVM incorrect measurement* create new placeholder with converted layout* update _schedule_winograd,1
[POC][PatternLang]Remove constants from partitioned functions (#5663)* remove constants from partitioned functions* remove print statements,4
[TF] Support TupleWrapper as direct ancestor of control flow ops (#5639),5
add tvm.micro pydoc to sphinx (#5661)* add tvm.micro pydoc to sphinx* making build pass and addressing tqchen comments,1
add a check for null function attributes (#5674),1
[BYOC] Pattern Language MergeComposite (#5656)* Pattern Language MergeComposite* fix DNNL pattern* Use builtin binary operator syntax for demo* Improve unit test,0
add a testcase for #5674 (#5677),1
Call previous excepthook in tvm_excepthook. (#5675)* Call previous excepthook in tvm_excepthook.* Rename prev_excepthook.* Create a tvm_wrap_excepthook to wrap a given excepthook with tvm custom excepthook workand call it on system previous excepthook.* Add docstring.,1
Fix the shift column for scale_shift_nchw and scale_shift_nhwc in C topi (#5679),0
[Bugfix] Fix Python debugger segfaults with TVM built with LLVM (#5685)* Import readline before loading libtvm* make lint happy,0
[DOC] Improve Pattern Language Docs (#5676)* [DOC] Improve Pattern Language Docs* address comments* address comments,1
[TFLITE]Quantize & Dequantize op (#5394)* [TFLITE]Quantize & Dequantize op* Testcases added* Review comment fixed,0
[TIR][REFACTOR] std::string -> String Migration in TIR nodes (#5596)* [TIR][REFACTOR] std::string -> String Migration for Var node and SizeVar Node* update json_compact.py,1
[PatternLang] Add ConstantPattern (#5689)* Add ConstantPattern* update doc,1
"[PYTORCH]Minor bug fixes (#5683)* [PYTORCH]Minor bug fixes* Review comment fix, testcase added* Added testcase for bert model",0
"[Relay] Fix dataflow_pattern.rewrite() hang if Match in IR (#5680)rewrite() quits only if graph stop changing, but ExprMutator  always creates new Match node. This patch fixes this.",0
[RELAY] Fix segfault in pretty print when ObjectRef is null (#5681)* [RELAY] Fix segfault in pretty print when ObjectRef is nullEncountered when pretty printing module with function attribute equal to NullValue<ObjectRef>().Change-Id: I2e7b304859f03038730ba9c3b9db41ebd3e1fbb5* Add test caseChange-Id: I579b20da3f5d49054823392be80aaf78a055f596,0
[REFACTOR][RELAY] move fallback_device to config (#5690),4
@zhiics -> PPMC (#5692),5
[COMMUNITY] @masahi -> PPMC (#5691),3
Support more dtypes for TVMDSOOp (#5694),5
[ONNX]LpPool Support added (#5696),1
"In memory_plan, check if value is not None, instead of just checking value as boolean. (#5700)",2
[PatternLang]Conditionally Embedding Constants in Partitioned Functions (#5693)* Embed constants in the partition function if the pattern explicity requests constantsfix rstfix pylint* improve comments based on Cody's feedback,0
[ONNX] Skip ADD inside Gemm op when vector is zero (#5697),1
[BYOC] Support Tuple Output in C/DNNL Codegen (#5701)* Support tuple output runtime* fix unit test,0
[REFACTOR][RELAY] Replace build_config with PassContext (#5698),4
[PYTORCH]floor_divide support for squeezenet (#5702)https://github.com/apache/incubator-tvm/issues/5133#issuecomment-636330705,5
[AutoTVM][TOPI] Fix bifrost spatial packing conv2d auto tune (#5684)* [AutoTVM][TOPI] Fix bifrost spatial packing conv2d auto tune* [AutoTVM][TOPI] Putting placeholder replacement in compute* Fix winograd kernel replacement* Fix sanity check: Line too long,0
[Arith] ExtendedEuclidean merge impl to int_operator (#5625),5
fix typo: anchor windoes should be anchor windows (#5706),0
[REFACTOR][PY] relay.op.Op -> tvm.ir.Op (#5705)* [REFACTOR][PY] relay.op.Op -> tvm.ir.Op* Improve the error check,0
[PatternLang] Simplify Pattern API Implementations (#5703)* Add syntatic sugar; include pattern to API docs* fix doc warnings,0
[PYTORCH]ReplicationPad support added (#5708),1
Remove deprecated opengl files (#5711),2
Remove opengl runtime and cmake (#5712),4
[BUGFIX][CRT] Fix Compilation Error in CRT (#5713),0
Rename tvm_dso_op to libtvm_dso_op (#5714),5
[Object] Unify StrMapNode and MapNode (#5687)* Pass cpptest and py unittest* fix graph runtime* right fix* fix a bug that runtime::String's operator < is actually compare by address* Update container.py* Renaming* Address comments* lint* Replace ObjectHash in object.py,0
"[MXNET]Softmin, trunc op support added (#5715)",1
Avoid downloading when TOPHUB_LOCATION is NONE (#5720),5
[Object][FFI] Introduce runtime::String::CanConvertFrom (#5718)* [Object][FFI] Introduce runtime::String::CanConvertFrom* Update container.h,1
[Object] Restore the StrMap behavior in JSON/SHash/SEqual (#5719),5
Fix generating types like float44 and float88 (#5722),0
"[ONNX]ReduceL1, ReduceL2, ReduceSumSquare, ReduceLogSum ops added (#5721)",1
"[TENSORFLOW]StatefulPartitionedCall/PartitionedCall Ops support added  (#5617)* Implemented functionInvocation Unit Test for StatefulPartitionedCall operator(working) and initial changes for placeholder(not working as of now)* Placeholder exercises with tvm* placeholder interim* SPOP Test cases structure* New test cases for spop* miscellaneous test cases for spop* Placeholder samples..working with shapes explicitly passed* Variables test case. Works with the same fix of shape_dict* SPOP Positive test cases first iteration* support output tensors as function args, multiple functions* Corrected Indentation* filewritter is only for debug purpose* support variables in function args* First working iteration of positive spop test cases* Removed commented code, simplified code* Code Reorganization- First working iteration of positive spop test cases* corrected variable name after refactor* Code Reorganization- First working iteration of positive spop test cases* move code inside mapped operator function* Removed extra line* support variables in function args* Removed commented code, simplified code* move code inside mapped operator function* Code Reorganization- First working iteration of positive spop test cases# Conflicts:#tests/python/frontend/tensorflow/test_forward.py* Code Reorganization- First working iteration of positive spop test cases* Function invocation more test cases* Simplified & Merged different Function Invocation Test cases* support invocation of nested callablesno need to explicitly handle paratitioned andstatefulPartitioned condition in convert_operator function* Simplified and Uniform testcases* support invocation of nested callablesno need to explicitly handle paratitioned andstatefulPartitioned condition in convert_operator function* Simplified and Uniform testcases* removed duplicate and renamed testcase* Negative scenario added for testing operator statefulness. Only Exception to stateful operators are Partitioned & StatefulPartitionedOp which have capability to execute even stateless operators within them* Miscellaneous reorganization changes for spop scenarios* Miscellaneous reorganization changes for spop scenarios* Corrected import of tensorflow modules safely using try except and other code reorganization* Negative scenario for resource variables handled* Documentation update for code* SPOP change in function handling* handle nested subgraph* refactor* get op def compatible with tf 1x & 2x* Fixed liniting issues* added doctsring and few nits* Merged changes for positive test cases and negative test cases* Moved StatefulPartitionedCall test case to the end of the TC list* Fixed some typos and semantics* dmlc-core* dmlc-core* fixes* Addressing Review comments in the PR for SPOP support* Fixed pylint errors* Corrected tensorflow import syntax* Placed the op_def_registry module import outside of for loop* Removed new stateful operators list and combined these operators with missing operators to display as single list. Also removed throwing seperate exception for stateful opsCo-authored-by: Prashant Sail <psail4444@gmail.com>Co-authored-by: maheshambule <mahesh_ambule@persistent.com>",0
"[AutoTVM, Relay] Clear compile engine after task extraction (#5724)",5
Fix runtime::String backward compatibility in JSON (#5725),0
"codegen llvm: move nvptx-specific intrinsic handling into codegen_nvptx (#5726)See discussion in #5600.I'm also throwing in a pointer lifetime fix for the context held byNVPTX because otherwise topi/tests/python/test_topi_softmax.pywould sefault for me. With the test, I can also run resnet-18 onthe nvptx target in gpu_imagenet_bench.py.",0
"[TOPI,RELAY][TFLITE] Sparse to dense operator (#5447)* [Relay][Frontend][TFLite] Add parser support for shape and rangeSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* [TOPI,RELAY][TFLITE] Sparse to dense operatorSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* use param name in documentationSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* sphinx doc errors fixedSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* incorporated review commentsSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Missing a blank line...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>* use get_tensor_exprSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Accidently removed this function in the rebase...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>* support default value for default_valueSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* clang format fixesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* topi pylint fixesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>",0
[Frontend][TFLite] Add parser support for shape and range (#5329)* [Relay][Frontend][TFLite] Add parser support for shape and rangeSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Incorporated review comments and used new functionsSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Few cosmetic changesSigned-off-by: Dhruva Ray <dhruvaray@gmail.com>* Removed an extra line added by rebase...Signed-off-by: Dhruva Ray <dhruvaray@gmail.com>,1
"[REFACTOR] Separate ArgTypeCode from DLDataTypeCode (#5730)We use a single enum(TypeCode) to represent ArgTypeCode and DLDataTypeCode.However, as we start to expand more data types, it is clear that argumenttype code(in the FFI convention) and data type code needs to evolve separately.So that we can add first class for data types without having changing the FFI ABI.This PR makes the distinction clear and refactored the code to separate the two.- [PY] Separate ArgTypeCode from DataTypeCode- [WEB] Separate ArgTypeCode from DataTypeCode- [JAVA] Separate ArgTypeCode from DataTypeCode",1
"[ONNX]MaxRoiPool, Mod & Xor op support added (#5729)",1
ROCm: Add warp shuffles and enable reductions (#5727)Thank you @masahi and @wpan11nv for the feedback,1
Change 'delete's in Relay VM Instruction dtor to 'delete[]'s (#5735),2
Fix reshape usage in ARM Winograd (#5732),0
[TEST] Fix flaky topi/tests/python/test_topi_pooling.py:test_adaptive_pool (#5736),0
Fix the values for test_fmod since it fails way too often otherwise (#5723),0
fix small bug about dense_grad (#5695),0
[REFACTOR][ARITH] Remove legacy compute_expr.h (#5738)Replaces most of the ComptuteReduce using foldl.,4
Add some docs on downstream consistency (#5742)https://github.com/apache/incubator-tvm/pull/5730#issuecomment-639567636,1
sequential cpp test (#5745),3
"[REFACTOR][TE][TIR] Call::Halide => ProducerLoad, DSL/TIR decouple. (#5743)In the HalideIR's design, DSL components and IR are mixed together.For example, Call::Halide can containa reference to a function which isconstructed in the tensor expression language.While this coupled design simplifies certain aspect of the DSL construction,it prevents the TIR to evolve as a clean standalone IR:- The additional tensor expression provided in the function is opaque to the IR  and may become obsolete as we transform them.- The duplication of the information in the DSL tensor and IR makes it hard to  design a stand-alone text format (when there are elements shared in the tensor  expression and normal statements).This PR aims to clearly de-couple the TIR from high-level DSL structures(tensor expression),while still provide clear extensions to build DSLs on top of the TIR.We introduce a DataProducer as a base class for high level tensor expressions objectsthat produce data. We then introduce ProducerLoad to replace the Call::Halide usage,so that the Call node can always be self contained and used for low-level calls.The high-level tensor expression DSL can still generate a PrimExpr that contains a ProducerLoad.These PrimExprs contains fragments of information that can be combined together togenerate a low-level TIR PrimFunc.We also state clearly that DataProducer **should not** appear in any TIR PrimFunc.Instead, the high-level DSL layer should lowered DataProducers to Buffers and TIR statementsthat produces these buffers. We can further provide verifications to validate such invariance.Changes:- Introduce DataProducer to serve as a base class for Tensor in tensor expressions.- Migrate use of Call::Halide to ProducerLoad- Migrate the other usages of Calls.We will also create follow-up PRs to migrate the remaining two DSL related IR nodes(Realize/Provide)to use the DataProducer.",1
Don't add cast for TF batch norm when type isn't changing (#5731),1
[ARITH][BACKPORT-0.6] fix a min/max simplify bug (#5749)* fix a min/max simplify bug* fix cpplint* turn into oposite when c1val<0 and add more case* fix c1=0Co-authored-by: xqdan <danxiaoqiang@huawei.com>,0
"[TOPI][Relay][OP] support dynamic NMS(Non Maximum Suppression), symbolic begin, end, and strides for strided_slice (#4312)* [TOPI][Relay][OP] Dynamic NMS and strided_slice* Incorporate comments* fix nnvm compatibility issues* fix InferCorrectLayout* Minor fix* fix for fuse* Workaround to pass batch_size into hybrid function to handle dynamic shape* Seperate rearrange* fix lint* fix ci, comments* change attr to Optional<T>* clang format* remove empty lines* partial ignore for end of strided_slice* pylint* add out_indices for gpu get_valid_counts* change to slice_mode* clang-format, fix comments* fix comment* change slice_mode to string* fix CI* update docstringCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>",0
Add Scatter to Topi/Relay/ONNX via hybrid script (#5619)* I can construct scatter but not embed it in a Relay Graph* working 1-4 dimesion scatter* add scatter to ONNXfix lint* isolate tests to cpu backend* Fix i386 test* fix gpu tolerance* use elemwise_shape_func for scatter* fix incorrect rebase,0
[Minor][Test] Clean WASM environment before build (#5759),3
[Bugfix] Fix reshape (#5739)* Fix reshape* fix doc warning* fix ci* address comments,0
"[REFACTOR][TIR] Provide->ProducerStore, Realize->ProducerRealize. (#5750)This PR finishes up the final step for DSL/TIR de-coupling to refactorProvide/Realize to use the DataProducer.As in the case of ProducerLoad, ProducerStore/Realize are not supposedto appear in a vaid TIR function ans are only used by high-level DSLsas intermediate structures.",5
[Rust] Second stage of Rust Refactor (#5527)* Add tvm-rt crate* Backport changes from frontend branch* Format* Add ASF headers* Address self-code review* Replace with helper* Fix lint* Fix* Clean up repro debugging* WIP* Remove global resgistry to fix one memory issue* Fix* Format* Format* Update rust/tvm-rt/README.mdCo-authored-by: Jason Knight <binarybana@gmail.com>* Format* Duplicate TVM macros* Split macros* Restore old macro for old crates* Repair macros* Fix format* FormatCo-authored-by: Jason Knight <binarybana@gmail.com>,0
[topi] block sparse dense on cuda (#5746),5
[Relay] Fix for recursive let (#5757)* Make let processing iterative* Try again* Fix pretty printer overflow* cleanup* fix lint* Fix text printerCo-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Jared Roesch <jroesch@octoml.ai>,0
[TOPI][RELAY][PYTORCH]Conv3d_transpose op support added (#5737)* [TOPI][RELAY][PYTORCH]Conv3d_transpose op support added* Test cases in topi/relay* conv3d_transpose_ncdhw_python added* Review comments fixed,0
"Fix gelu in PyTorch frontend, tighten numerical checks (#5763)Previously, the PyTorch frontend approximated gelu with fastgelu.To provide a more faithful conversion, we implement gelu instead.We also tighten the numerical comparisons between PyTorch andTVM-from-PyTorch to 1e-5. The object detection models need anincreased tolerance of 1e-4 to pass.I had to throw in a few fixes for missing conversions(probably due to working with very new PyTorch).I must admit the GoogLeNet/NasNet test didn't run on my machine,probably due to problems at my end.",0
Add ShapePattern and DataTypePattern (#5760),1
"Make batch matrix multiplication on GPU tunable (#5752)This is primarily aimed at the AMD GPU backend and done as partof a project for AMD, but should work for all users of the GPUschedule.",5
[TIR][REFACTOR][API-Change] Migrate the tvm/tir/expr.h to construct style. (#5773)This PR migrate tvm/tir/expr.h to the new constructor style that isconsistent with the rest of the codebase and changes the affected files accordingly.,1
[TIR][REFACTOR][API-Change] Migrate tir/stmt.h to use constructor. (#5778)This PR migrate tvm/tir/stmt.h to the new constructor style that isconsistent with the rest of the codebase and changes the affected files accordingly.,1
[Frontend][TensorFlow] Improve Control Flow and TensorArray (#5699)* Improve TF parser control flow and tensor array* Fix tf tensor array scatter* Add ssd test* Add back static ta test* Minor fix for frontend and test_forward* SplitRel for dynamic shape* Fix test ssd* Fix loop var naming issue* Minor improve* Fix format* Fix clang format* Fix tensor array in pytorch frontend* Fix stack size issue for ssd test* Address comments* Fix slice size* Fix build* Rebase,0
[DOC][FIX] Fix some typos in git-clang-format.sh (#5786),0
fix #5686: remove a overstrict assert in MakeAllreduce (#5686) (#5785),0
[RUNTIME] Add compile_shared option to linux compile utility fn (#5751)* feat: Add compile_shared option to linux compile fn* feat: Add compile_shared option for linux compile util fn* fix: Fix minrpc testcase use executable compilation* fix: Fix binutil case where call create_shared to create executableCo-authored-by: baoxinqi <baoxinqi@4paradigm.com>,0
[REFACTOR][API-Change] Migrate all Object construction to constructor. (#5784)This PR migrates all the remaining object constructions to the new constructor stylethat is consistent with the rest of the codebase and changes the affected files accordingly.Other changes:- ThreadScope::make -> ThreadScope::Create- StorageScope::make -> StorageScope::Create,1
[Topi] pass-by-value -> pass-by-const-reference (#5783),4
[topi][relay] Add operation gather to relay. (#5716),1
[CODEGEN][CONTRIB] CoreML codegen (#5634)* [CODEGEN][CONTRIB] CoreML codegen* import coremltools only when it is necessary* fix pylint errors* don't import contrib.coreml when using runtime lib* skip coreml codegen test in CI* don't register relay.ext.coremlcompiler in __init__.py* move tvm/contrib/coreml.py to tvm/contrib/target/coreml.py* use existing transformers for graph partitioning* skip test only when coremltools is not available* add check for annotation* move _register_coreml_op to python/tvm/relay/op/contrib/coreml.py* skip compile when xcode is unavailable* relay.op.Op -> tvm.ir.Op* set USE_COREML on* refine test,0
fix calibration pass to support multiple functions (#5768)Co-authored-by: Ubuntu <ubuntu@ip-172-31-43-142.us-east-2.compute.internal>,0
[cmake] update vulkan rules (#5777),1
Add ignore storage_order attribute to onnx pooling parser. (#5781),1
"[BYOC][FIX] Infer types in MergeComposite (#5766)If InferType isn't run between partitioning passes,function calls are inserted which don't have a type.This can result in failures for patterns which wantto check types.This works around it simply by running InferType afterevery partitioning.Change-Id: Ie0887f0564a41eb0913bfe42a362e8effe9681b9",0
[FRONTEND]Darknet support batch size for yolo (#5688)Fix the issue reported in https://discuss.tvm.ai/t/yolov3-tiny-batch-input-test-failed/6796,0
[PYTORCH]aten::norm support added (#5776),1
"[TENSORFLOW]Conv3d Transpose OP added (#5775)* [TENSORFLOW]Conv3d Transpose OP added* Testcase updated, tf cpu supports only ndhwc",1
[TF] Support symbolic inputs of Fill (#5762)* [TF] Support symbolic inputs of Fill* Rebase and simplify. Value has been converted to constant if it istf.Constant,5
[COMMUNITY] @wpan11nv -> Reviewer (#5790),3
Edit onnx parser to infer values in post order (#5755)* edit onnx parser to infer values in post order to speed up onnx imports with many calls to infer_value* fix pylint,0
[TIR][REFACTOR] Cleanup unused classes (#5789),5
Fix tf parser (#5794),0
support aten::type_as in the pytorch frontend (#5787)* support aten::type_as in the pytorch frontend* use _convert_data_type to convert torch type to tvm type and add more types in the type_as test,1
[TIR][REFACTIR] Update TIR nodes std::string->String. (#5793)This PR updates the remaining TIR node's member to useString instead of std::string.,1
[TEST] Temporary disable fp16 type_as test for PyTorch Frontend (#5799),3
[ONNX] Skip multiply with 1.0f constant for GEMM import (#5800)* [ONNX] Skip ADD inside Gemm op when vector is zero* [ONNX] Skip multiply with 1.0f constant for GEMM import,1
[TIR][REFACTOR] Add tir prefix to type keys (#5802),0
[QUANTIZE] Add config switch for nn.dense layer type. (#5801),1
[topi] fix sparse dense schedule on cuda (#5803),0
Allow RPCWrappedFunc to rewrite runtime::String as std::string (#5796),5
[topi] fix strategy for sparse dense cuda (#5782),0
[CI] Move cpu-only frontend tests to a CPU stage (#5807),3
[MXNET]conv3d and conv3d_transpose addedx (#5814),1
"Pin hand landmark network to version 0.7.4. (#5813)* Versions above 0.7.4 are broken due to changes in the   quantization operations in the model, which are current   not supported by TVM.Fixes #5774.",0
[CI] Limit number of threads in all jobs (#5815),5
[COMMUNITY] Siju Samuel -> Committer (#5817),3
Error msg update (#5818),0
"[Frontend][TFlite] Add parser support for relu6, leaky_relu, relu_n1_to_1, log_softmax (#4805)* [Frontend][TFLite]Add support for relu6, leaky_relu, relu_n1_to_1, log_softmax* add implementation in parser* add qnn tests for each operator* Implement clip operation for quantized relu6, relu1* add 'clip' as in the quantized fused operations* remove redundant assertions and imports* Fix floating value quantization for RELU6 and RELU1",0
[Relay][OpStrategy] Tweak cublas/cudnn priority level (#5820)* Tweak cublas plevel* update* trigger ci,1
fix relay.build to not change the module argument in place (#5822),0
"[MergeComposite] Fix InferType when module contains Prelude (#5797)A function may refer to other resources in the same module, so keep  the content of original module when infering a function.",0
[Fix] Fix recursive let for well formed check (#5780),0
[RUNTIME][String] Overload string operators (#5806),5
"[Relay, Topi] [Frontend][TFLite, MXNet] ReverseSequence operator (#5495)* TFLite reverse_sequence op* TFLite add_n implementation* reverse_sequence implementation* reverse_sequence implementation* reverse sequence* TOPI,Relay,TFLite - Reverse SequenceSigned-off-by: maheshambule <mahesh_ambule@persistent.com>* Reverse Sequence small fixesSigned-off-by: maheshambule <mahesh_ambule@persistent.com>* lint fixesSigned-off-by: maheshambule <mdambule07@gmail.com>* TFLite reverse_sequence opSigned-off-by: maheshambule* MXNet SequenceReverse implementation* clang format* clang format* review comment fixes",0
[Frontend][TensorFlow]Fix TF Dynamic input shape (#5825)* Fix TF Dynamic input shape* Remove warning* Add test,0
[Frontend][MXNet] Support a few contrib ops in mxnet (#5819)* support for bert in mxnet1.6 and gluonnlp0.9* fix converter* Add test cases* add a todo,0
"Add a combine batch_matmul pass (#5791)* Add a combine batch_matmul passContrary what you might expect, this doesn't share as much code withthe combine dense pass as it does with the combine 2d conv pass.This is because it concatenates the ""output feature"" dimensions.* fix docstring",0
"[KERAS]RepeatVector, Conv3DTranspose op support added (#5833)",1
[Torch][Quantized] Fix converting serialized quantized models (#5839)* [Torch] Fix converting serialized quantized models* clean up dtype check* comment clean up,0
ffi (Object): make class dict visible in instances (#5843),2
`tvm` crate stage 3 of Rust refactor  (#5769)* Adapt to new macro* Add tvm crate* Fix out of tree pass with new bindings* Super slick API working* Add examples* Delay egg example and add ASF headers* Move array.rs around* Remove outdated tests will restore in CI PR* Fix some memory issues* Fix ref counting issue* Formatting and cleanup* Remove out-of-tree for now* Remove out-of-tree,0
[RUNTIME] Introduce MetadataModule to separate code compilation/interpretation and weight initialization (#5770),5
"fix batchnorm infer_value error, add regression test and unit test (#5845)",0
Additional canonicalization added for AddNode (#5846),1
[AutoTVM] Suppress the warning messages when compile engine selects impls (#5821),5
"[FIX] Recover global state after test_util.py (#5824)In test_util.py, a program exit is simulated to testthat the error throwing behaviour is accurate.Unforunately, this also deletes necessary global stateand so all subsequent tests that run and use tempdirthrow the same error.This patch is a simple fix to restore the global stateat the end of the test.Change-Id: I62fef46167e47f6af43271e2ce1db30f54857647",0
[Object] Introduce POD-C Compliant tvm::Map (#5740),5
[DataType] Add bfloat16 (#5601),1
Add Python Classes for all Attrs (#5853),1
Fix map assign issue in CI test (#5854),0
[DOCS] Update has_dtype/has_shape to pattern lang doc (#5847),1
[Target] Introduce Target Id Registry (#5838),5
[QUANTIZE] Add nn.batch_flatten as quantizable. (#5805)* [ONNX] Skip ADD inside Gemm op when vector is zero* [QUANTIZE] Add nn.batch_flatten as quantizable.,1
[Bugfix][Build] Fix building with LLVM-10 on macOS (#5859),0
Fail early before running invalid dynamic graphs (#5856)* fail early before running invalid dynamic graphs* fix an issue with the VM comment,0
"Improve type handling in PyTorch frontend (#5834)* Improve type handling in PyTorch frontend- Use type information from graph for inputs if available. Check  against shape information from graph if available.- Allow user to set default dtype (default to float32 for sanity and  compatibility).- Implement type promotion to follow PyTorch mechanism. This includes  fixing the handling of many ""Scalar"" overloads in PyTorch binary ops.- Fix arange/linspace type semantics.- Added support for traced functions. (Because it really is about the  ""self"" input handling.)Aside from adding an optional default_dtype keyword argument, this does notchange the signature/requirements of from_pytorch.* Fix scalar detection using numpy.isscalarand address other review comments. Thank you @siju-samuel* refine test criteron on qnn_test::test_serialized_modules, fix bool conversion of const",0
keep parameter names from PyTorch (#5887),5
[COMMUNITY] Matthew Brookhart -> Reviewer (#5886),3
"[TIR][REFACTOR][API-CHANGE] Change Call.name to Call.op(RelayExpr) (#5863)* [TIR][REFACTOR][API-CHANGE] Change Call.name(string) to Call.op(tvm::Op/RelayExpr)This PR brings a major refactor to the tir::Call structure.The current Call structure uses a string field(name) to identify thefunction/intrinsic being called. This approach is limited as we startto expand TIR to be more structured. In particular, we are interested inthe following aspects:- Type a function and perform better compile time type checking so that we  can find errors early.- Register additional properties about an operator, such as:  - Whether an intrinsic can be vectorized  - What is the adjoint function of the intrinsic(for tensor expression AD)  - Whether the operator has side effect.- Perform specific codegen about an intrinsic if necessary.- Call into another function in the same module.The refactor changes the Call.name field to Call.op.The Call.op field has a RelayExpr type, and we can pass:- A tvm::Op which represents the corresponding intrinsic.- A tvm::GlobalVar for calling into another function in the IRModule.All the current intrinsics are migrated by registering an tvm::Op.Because the unified IR shares a single Op registry. We use the ""tir""namespace for tir related intrinsics, for example bitwise and is now registeredunder `tir.bitwise_and`.To simplify upgrade, we introduce a `tir.call_extern` intrinsicthat allows us to call into arbitary external function without type checking.However, we should move towards more type checked variants in the system.Under the new op design. We should no longer try to pattern match all thespecific intrincis. Instead, we should rely on attr of each Op to do transformation.For example, the vectorization pass depends on the TVectorizable property of the op,which can be registered independently.In this way, we can still grow the number of intrinsics when necessarywithout having to change all the passes.The same rule applies for tensor expression AD. Currently we are performingAD by pattern match on operators like exp, sin, cos. We should insteadchange to the ajoint registeration mechanism like those in relay.Followup refactors need to be performed, including:- Fold the Call.call_type into operator's attribute.- Enrich the operator registry information- Refactor passes(e.g. AD, intrin lowering) to use the attribute based transformation* Fix nms* Fix remaining testcase* Address review comment",0
[RFC] Improve quantized convolution performance for armv8 architectures (#5754)* Improve quantized conv2d performance for armv8Signed-off-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: I3a3d29f5332dd9b3354e8e0dfb24677a521f9c8f* Add ASF header to conv2d_gemm.pyChange-Id: I33853279e39c849ae1b555a9c91d7557985a0a35* Run clang-format-10 on c++ filesChange-Id: Ieee22f032e595dabfc1616ab33466fcbf8d94365* Fix pylint errors/warningsChange-Id: I435d4d7bca7500db99547f4401fdc0d0995a1ff4* Fix pylint errors/warnings in topiChange-Id: I2fc1ad8453e9020072ab967c849df5390c2967b5* Fix legalizations tests for aarch64Change-Id: I0a67a49a7849f52ef7d57b9292ce9125bbb7cb2c* Reintroduce conv2d_nhwc_spatial_pack.arm_cpu and int16 castChange-Id: I91b67fabd475e90a9b75f2dd5ecfee851265e0bb* Switch type of legalization depending on the strategy usedChange-Id: I9a03040a8c40a6cd2658ed14c3751e05a8e19f2b* Revert last commitChange-Id: Ice34101e358e3ce8ebfb12c58f73e910ba5de8e8* Fix the auto-tuner by registering the correct schedulesChange-Id: Id9273688b2620e1ea849ab01b4c46af8fbf37fd0* Address review commentsChange-Id: Ia1755a0af7b6d159072d9f0c93c932c481101e48* Improve usability and readability of conv2d_gemm_weight_transformChange-Id: I3333186bbc2fe4054b58ce15d910e3be7b315482* Change variable name to weight in Conv2DGemmWeightTransformRelChange-Id: Ifb5f1f33af7512fe67c6b049b20a42a0bb2d26c9* Fix clang-10 linting errorsChange-Id: I25ccc844d9cee23766096e1daddb6180abc413a6* Trigger testsChange-Id: Id37706fb7cf77a87a3cc817ecf8046297d9ca95a,0
remove fatal (#5888),4
[Relay]Allow every runtime module to handle constants (#5885)* update source module* address comment,1
Fix the python intrin rule (#5895),0
Rust Refactor Stage 4: Rewrite Rust graph runtime to use new APIs (#5830)* Port graph-runtime to new API* --amend* Fix file lint* Remove old travis file* Add @kazum's patch* Update rust/tvm-sys/src/datatype.rsCo-authored-by: Andrew <amcharg@gmail.com>Co-authored-by: Andrew <amcharg@gmail.com>,0
add a few gradients (#5899),1
Add Binary Intrinsic ops to TIR Ops in C++ (#5900)* Add Binary Intrinsic ops to TIR Ops in C++* clang-format,1
Allow implicit conversion in TVM FFI to tvm::Bool (#5907),5
PyTorch frontend: fix handling of duplicate use of a model weight (#5897)This happens e.g. in shared input/output embeddings in BERTor siamese networks.Thank you @siju-samuel for reporting.,0
Don't multiply by constant 1 uselessly in dense (#5911),5
Fix serialization of inf float value (#5912),0
[PatternLang] Support any index matching for TupleGetItem (#5909)* support any index matching* update doc,1
[TIR][REFACTOR] Deprecate FreeStmt (#5890)Currently FreeStmt is not being used.While it can be useful to have an early free hintwe can always use an intrinsic instead of a first class statement.,2
"Add MicroTVM tutorial using the STM32F746 discovery board (#5655)* Add MicroTVM tutorial using the STM32F746 discovery boardwith a sample tflite modelSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fix: add a reference to the new turtorials/micro directorySigned-off-by: Tom Gall <tom.gall@linaro.org>* fix: Cosmetic, align Micro TVM text with dividerSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fixes to remove warnings, spaces for readability, code blocksSigned-off-by: Tom Gall <tom.gall@linaro.org>* remove use of dload in favor of requests for obtaining the TFLite modelSigned-off-by: Tom Gall <tom.gall@linaro.org>* add setup for CMSIS_ST_PATHcomment out portion of tutorial that will not run without a physical board availableSigned-off-by: Tom Gall <tom.gall@linaro.org>* Fix warning due to ** in python but part of a comment blockThe block is commented out since it can only run on deviceSigned-off-by: Tom Gall <tom.gall@linaro.org>* Numerous reworks to address feedback.Within docs/conf.py place the microTVM tutorial prior to the VTA tutorialsWithin the micro_tflite  - rework section headers  - reorder code so model prep code is all in one place as well as code    for running on device  - address indentation feedback  - remove '' '' usage which I mistakenly thought was getting around a    sphinx issue involving **Signed-off-by: Tom Gall <tom.gall@linaro.org>* Change disable_vectorize to use current approach with tvm.transform.PassContextChange to pull example model from github with download_testdataAdd 2.5K tflite modelCouple of small changes following https://sphinx-gallery.github.io/stable/syntax.htmlSigned-off-by: Tom Gall <tom.gall@linaro.org>* remove use of relay.build_config in favor of PassContextSigned-off-by: Tom Gall <tom.gall@linaro.org>* Couple of minor 4 space fix upsSigned-off-by: Tom Gall <tom.gall@linaro.org>* Change to use tvm.transform.PassContext for disable_victorize and disabling FuseOpsSigned-off-by: Tom Gall <tom.gall@linaro.org>* Remove binary module from repoChange download_testdata back to pull model from linaro serverSigned-off-by: Tom Gall <tom.gall@linaro.org>* Couple of small cosmetic changes. (spaces and extra lines)Signed-off-by: Tom Gall <tom.gall@linaro.org>* Convert link to tf docs to examine a tf lite model to use RST syntaxSigned-off-by: Tom Gall <tom.gall@linaro.org>",0
[Thread Backend]Fix CPU Thread Binding for Multiple Sockets (#5918)* Fix CPU Thread Binding for Multiple Sockets* Backward compatibility,0
CUDA device API & VerifyGPUCode pass update (#5898)* Add kMaxRegistersPerBlock device api for cuda* Add vectorize check to verify_gpu_code* Lint fix* Cast fix,0
[Relay][Vm] Some performance improvement to VM (#5901)* make alignment constant* tweak copyto and loadscalarint* some safety check* x* lint* fix,0
Update install.rst (#5858)* Update install.rstminor cleanups/corrections* Update install.rstFixed broken link,0
"Two small fixes to AMDCPU codegen for LLVM 10+ and ROCm 3.5+ (#5920)- For LLVM 10+ we need to avoid calling Align with 0, or else  we get a crash.- For ROCm 3.5+ we need to use code object 3 (the default in LLVM 9+)  but for ROCm < 3.5 we want the code object 2.- As we want to separate codegen from the API, we need to add  a device api query for the version.  But every one else wants now one, too. (But I only filled it  in for CUDA for now.)- I'm throwing in an addition of kMaxRegistersPerBlock for ROCm.  This was introduced for CUDA in #5898.",0
[BACKPORT-0.6][Bugfix][Arith] keep div_mode during floordiv simplify (#5922),0
refine error (#5929),0
[TE] Add LegalizeInvalidAttach to legalize the compute_at location after split or fuse (#5917)* Add LegalizeInvalidAttach* lint & typo* lint & typo* address comment* fix lint,0
[PatternLang] Don't rewrite expressions used outside of the pattern (#5930)* Don't rewrite expressions used outside of the pattern* add comments,1
[Arith][GPU]Rewrite simplify fix for Vectorized Cooperative Fetching (#5924),0
Add TupleGetItem to CSE (#5931)* Add TupleGetItem to CSE* rename a local variable,1
Update code_review.rst (#5923)editorial pass with corrections,1
[Runtime] Only initialize required module (#5926)* init required modules* trigger ci* trigger ci,5
[CODEGEN][CONTRIB] Various update for CoreML codegen (#5934)* [CODEGEN][CONTRIB] Various update for CoreML codegen* fix lint error,0
add dnnl (#5936),1
"[TIR][OP][API-CHANGE] Remove CallNode.call_type in favor of attribute. (#5937)This is a followup refactor for tir::Call.Now that we have switched call->name to call->op, the function effect propertycan be registered through the op itself, so we no longer need the call_type in the CallNode.- Introduce CallEffectKind to provide a more fine grained categorization of calls.- Introduce call_pure_extern and call_llvm_pure_intrin to  allow us to indicate pure calls in those cases.- Migrate existing usecases to the new API.",1
Update date in the NOTICE (#5942),1
"[TIR][PASS] Remove legacy HoistIfThenElse (#5944)This pass has not been migrated to the new transform API,and contains potential bugs per https://github.com/apache/incubator-tvm/issues/5559.Given that it is not being actively used, this PR remove this passfrom the collection.Followup PRs are more than welcomed to land a better version thatconforms with the new transform API.",0
fix string argument mismatch in GraphRuntimeCodegen (#5933),0
[Doc] minor fix for release doc (#5948),0
"[TIR] Improve Let/LetStmt support. (#5949)Let/LetStmt are useful primitives to create variable bindings.While let binding are harmful for simplification and integer analysis,they are useful for other cases:- C0: LetStmt is useful to represent a step that has side effect(e.g. call a PRNG)- C1: Let expression can be used to create deep nested expression for complicated functions.This PR improves the let support in the following ways:- Enable vectorization support for let- Change let simplification strategy to simplify the most trivial case  while ignore more complicated cases(to avoid deep nest explosion)- Enhance arith module to handle const bound and modular set for let.The overall recommendation is to only use Let in the cases when necessary(C0, C1).",4
raise right error in tensorflow split op (#5951),0
add rm xla attributes in tf docs (#5950),1
[RELAY][VM] Add shape_of instruction (#5855),1
[REFACTOR][TIR][API-Change] Range/IntSet API style consistency. (#5953)- Range::make_by_min_extent -> Range::FromMinExtent- Update the APIs in IntSet to use CamelCase,1
[BUGFIX] Add cuda 11 to contrib.nvcc.find_libdevice_path() (#5902),0
[Relay] symbolic max_output_size  (#5844)* symbolic max_output_size* pylint* fix ci,0
[TIR][ANALYSIS] Refine side effect analysis. (#5954),5
"[OpenCL] Fix OpenCL get_valid_counts errors due to intrinsic atomic_add (#5857)* [OpenCL] Fix atomic add used by get_valid_counts* Rename l -> load, add flag to enable atomics* Opencl doesn't do data rearrangement",0
Fix some typo errors in license header (#5956)Signed-off-by: leonwanghui <wanghui71leon@gmail.com>,0
[RELAY][GRAD] handle Tuple/TupleGetItem in first order gradient (#5946)* handle Tuple/TupleGetItem in first order gradient* Unify MultiOnes/MultiZeros.,5
"Amendments for gradients (#5941)* Amendments for gradients- We fix the dtype handling of consts in generated gradients.- We add a collapse_sum_to instruction mirroring the collapse_sum_like.  While for general definitions (potentially dynamic shapes),  collapse_sum_like is the first choice, when moving to static,  using collapse_sum_to will greatly simplify the graph.  (This simplification is not part of the PR.)* Fix Broadcast rel description in commentThank you, @MarisaKirisame",0
"Fix the meaning of conv{1,2}d_transpose output_padding parameter. (#5758)* Add output_padding to generic* Add output_padding to the reference impl* Add output_padding to arm_cpu* Add output_padding to the test* Add output_padding for cuda* Add output_padding for x86* Make use of the new output_padding argument in Relay* Adjust conv2d_transpose Relay test* Fix lint errors* Fix the VTA declaration of conv2d_transpose* support for output padding in conv2d transpose* some output padding will break IR pass* Fix new conv2d_transpose test* Update tophub* Fix conv1d output_padding too.* Fix the conv1d_transpose reference function.* Fix the cuda impl* fix the topi test for conv1d* format* Add tests for conv1d_transpose output_padding and some check that the values are valid.* Add check in the implementations* Add checks to the implementations of conv2d* Make use of the output_padding argument from topi in relay.* Fix relay tests asking for invalid output_padding* Fix line length* Fix vta tests* Update tophub references* Trigger CICo-authored-by: Thierry Moreau <tmoreau@octoml.ai>",0
"Make first order gradient graphs more efficient (#5959)Previously, nodes are visited as often as they are used and each time aderivative is computed. Only at the leaves were the contributions ofeverything added. This patch changes this to add at any node that isused several times.",1
Fix small typo in nn.conv2d_gemm_weight_transform (#5925)* Fix small typo in nn.conv2d_gemm_weight_transformChange-Id: I7844d898ebf82592f78f478982262ef95f83cc3e* Add TOPI conv2d_gemm unit testsChange-Id: I9ed82a68acffcf0dd9720781f8be4aada9d8e6e4,0
"Raise an exception when extern function does not return Stmt (#5964)The function for tvm.te.extern should return either PrimExpr or Stmt,however there is no check if it actually does so. If it does not, theresult may be a segmentation fault later on. Catch this case early on,so an informative message can be shown.",5
Print right number of parentheses for LoadNode (#5965)Stop printing the unnecessary ')' after each LoadNode that didn'thave a matching '('.,5
Improve docker/bash.sh to handle git worktrees (#5970)* improve error code when git ls-files fails* fix docker/bash to handle git worktrees,0
Add MXnNet parser for box_decode (#5967),1
[DYNAMIC] Add Dynamic reshape to a dynamic namespace and add DynamicToStatic Pass (#5826)* Dynamic reshape passing tests* Add Dynamic to Static Pass* rename test file to prevent pytest conflicts* fix clang build* add nested dynamic shape test* remove cuda tests until VM supports dynamic shapes* rename namespace from dynamic to dyn* fix lint* fix lint again* Remove incorrect doc strings* remove dynamic behavior from standard reshape* fix some tests* merge dynamic and static interfaces in python* fix missing import* missed a reference to relay.dyn.reshape* fix vta example* respond to review comments,0
[RELAY] Add resnet-3d & Update network definitions for NHWC layout (#5945),1
"[Relay/TOPI][OP] Add meshgrid op in Relay, TOPI, Pytorch frontend (#5961)* Add meshgrid op with pytorch importer* Fix c++ lint* Fix pylint* Meshgrid: add scalar test for pytorch, add topi python wrapper* Add indexing mode attr.* Add MeshgridAttrs python binding* c++ lint",0
[TOPI] Fix x86 conv2d template when tuning with unpacked layout (#5938)* fix x86 conv2d and conv2d_transpose template* address comments,0
fix tvm relay testing tf.py typo error (#5977),0
"[LLVM] Remove redundant function CreateBufferVecPtr (#5982)The functions CreateBufferPtr and CreateBufferVecPtr do the exactsame thing, so there is no need for both of them to exist. Thelatter is only used in place, which further suggests that thedistinction is unnecessary.",4
[Target] Migrate data structure of TargetNode (#5960),5
[Tutorial] Demo showing how to run a pruned 🤗 model. (#5975),5
"[LLVM] VectorType::get with two parameters is deprecated in LLVM 11+ (#5984)In LLVM 11+ the distinction between fixed and scalable vector typeshas become more explicit. Before the introduction of scalable vectortypes VectorType::get(e,n) created what is now a fixed vector type.With the addition of scalable types, it is recommended to useFixedVectorType and ScalableVectorType classes directly. Alternatively,there is a VectorType::get that accepts a 3rd parameter indicatingwhether the type should be fixed or scalable.Using the older VectorType::get that implicitly assumes the fixed typeis deprecated and LLVM now generates a warning.Change calls to VectorType::get to FixedVectorType::get to avoidcompilation warnings.",0
"[TFLite] QNN support for TFLite 2.1.0 quantized models (#5848)* [TFLite] TFLite 2.x parser quantization support.* Address comments. Fix a bug for depthwise conv* Added tests for relu, conv, quantize. Address comments.* Using web-data. Minor refactoring.* Removing TF hub package* Trigger CI.* Handle TFLite input layer naming.* Addressing reviews.* Retrigger CI.",0
[Target] Use TargetNode::attrs for Target serialization (#5993),5
[Relay][Frontend][Onnx] Small bug fix for Conv1D imports. (#5995)* Fix autopad bug in onnx importer for conv1d.* Fix output shape in test.* Undo commented out lines oops.,0
[Arith] Inequalities solver (#5618),5
Fix tune_relay_cuda.py (#6001),0
Update main.yml (#6002),1
Undefuned names: import os for line 324 & import re for line 308 (#6003),5
Dynamic Tile Op (#5983)* first working dynamic tile passes first test* add dyn tile to dynamic_to_static* fix cpplintt* respond to review comments. Thanks @siju-samuel* make dynamic tile compatible with numpy API,0
[DOCKER] Only pass pythonpath for ci images (#6005),4
Fix what looks like bizzare copy-paste issue (#6010),0
"[LLVM] Auto-convert shuffle with single index to ""extract element"" (#6006)* [LLVM] Auto-convert shuffle with single index to ""extract element""Data types with a single lane are treated as scalars in TVM. On theother hand, in LLVM there is a difference between a scalar type anda vector type with a single lane. Because of that, a shuffle witha single index is equivalent to extracting an element in TIR, butnot in the generated LLVM IR. This patch changes the LLVM codegenfor shuffle to auto-convert single-lane vectors to scalars.* Try another build",4
"Cache object refs in loop partitioner instead of object pointers (#6004)* Cache object refs in loop partitioner instead of object pointersLoop partitioner modifies the IR, which can cause TIR objects tobecome dead and be destroyed. To avoid working on junk data cacheobject references instead of object pointers.* Fix format/lint errors",0
[VTA] Move compiler related registry items to vta/build_module.py (#6012),4
"[Frontend][MXNet] MXNet frontend support for AMP cast op (#5976)* amp_cast* fix test* more tests* test more ctxs* fix doc* fix typo* address CR comment* fix lint* revert doc change* Revert ""revert doc change""This reverts commit a410dd5569730ac81af67ddb333c3afbe97eddd7.* fix doc* Update relay_pass_infra.rstCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>",0
[TEST][FLAKY] test_arith_solve_linear_inequality.py::test_multi_equal (#6014),3
Option to specify alternate directory to output build to (#6016)This is useful when you would like to manage 2 separate builds in the same tvm tree. You can specify a build directory when using make by adding OUTDIR=alternate-build-dir.Change-Id: I3efed1135343f3903007115ce5dd683ef7bd9e8c,1
[Frontend][Relay] Add Parser 2.0 (#5932),1
Remove deplicate line (#6017),4
[PYTORCH]Gather op support added (#6013)* [PYTORCH]Gather op support added* retrigger,1
"[RUNTIME] if a param not in input, we still consume it's data (#5990)so the read pointer of stream can move forwardSigned-off-by: windclarion <windclarion@gmail.com>",4
fix typos in comments and relay tutorial (#5999)* [TypoFix]fix typos in comments and relay tutorial* retrigger,0
"[TARGET] each option of target str should only contain one '=' (#5988)src/target/target_id.cc ParseAttrsFromRawString L222:if ((pos = FindUniqueSubstr(s, ""="")) != -1)require option contains only one '='Signed-off-by: windclarion <windclarion@gmail.com>",2
[DOCKER] Pin keras version (#6032),5
[CI] Update ci-cpu to the latest (#6031),1
"[BYOC] JSON Runtime with DNNL End-to-End Flow (#5919)* json runtime* json dnnl WIP* fix ArrayNode usages* Support composite functions* DNNL json runtime: conv2d/add/relu/dense/bn* add a more complex example* fix bias memory issue* rebase to upstream* merge to metadata module, remove the unused driver* handle constant* support composite functions* support DNNL constant* clean up* Simplify dnnl user code* GetDataSize* fix dense bug* improve cmake* zero copy* add unit test* move json to contrib/json* fix cmake* lint* max_digits10 for fp serialization* only keep base getfunction* fix lint* zero copy for all data entries* address comments* enable ci* address comment; fix bug* address commentCo-authored-by: Zhi Chen <chzhi@amazon.com>",0
[CI][ACL] Enable ACL installation in ci_cpu docker container (#5916)This patch adds a cross-compiled ACL build to the ci_cpu dockerfile used for CI.Change-Id: I66e1521ab553306bc7367b65acc0363e750f0211,1
Add creation of Hexagon device in RPC client (#6035),1
"[Bug fix] Fix in arm_cpu/conv2d_alter_op for NHWC quantized (#6027)* Bug fix] Fix in arm_cpu/conv2d_alter_op for NHWC quantizedFew minor typos to be fixed in topi/arm_cpu/conv2d_alter_op.py for theNHWC quantized route:- Kernel shape was misread (CO, IC, KH, KW) -> (KH, KW, IC, OC)- Pad along the K dimension was misspelled: pad_k -> pad_K- Workload name was wrong: ""conv2d_NHWC_int8_without_tranform.arm_cpu""  -> ""conv2d_NHWC_quantized_without_transform.arm_cpu""This submission fixes those errors and add a further test for conv2d_alter_op.pyChange-Id: I0622df05f1d4d15311946f6e75f1840a34815a5b* Move -target to -mtripleChange-Id: Ieff80c774e8ab0fa7f48d83d50a79f3a62e8fe13* Retrigger testsChange-Id: I5541bed54eacc5063bf4a4fda725209cc23f621e",0
[REFACTOR][RELAY] Move invoke_tvm_op and shape_func to vm dialect (#5958)* [REFACTOR][RELAY] Move invoke_tvm_op and shape_func to vm dialect* address comments,1
[Relay][Dyn] Dynamic TopK Op (#6008)* add dynamic topk op* add topk to dynamic_to_static pass* fix TF test* fix pylint,0
"[LLVM/CPU] Terminate basic block after ""ret"" instruction (#6036)* [LLVM/CPU] Terminate basic block after ""ret"" instruction""Ret"" is a terminator in LLVM IR and there should be no instructionsin the basic block following it. When generating a ""ret"", end thecurrent block and start a new one.",1
"µTVM CRT modifications for on-device RPC server (#5921)* Reorganize CRT into parts, public API, and add standalone build. * Create a make-based build in src/runtime/crt. This is intended to   be built in build/standalone_crt (generated by running ninja   standalone_crt in build/). Its job is to build CRT without   depending on headers not explicitly allowed in CRT. * Create a ""public-facing"" CRT API targeted to firmware running   alongside CRT in include/tvm/runtime/crt. Developers who are   integrating the CRT are the target of this API. * Reorganize CRT internally into common/ and graph_runtime/   pieces. Build each pieces as a separate statically-linked library. * Slim down TVMGraphRuntime public-facing API to just the functions   that are used externally. * Updates to apps/bundle_deploy to make this work.* Add TVMFuncRegistry, CRT test infrastructure, and tests. * Also add error_codes.h, a file containing error codes returned by CRT.* Add TVMErrorf()* [API_CHANGE] Integrate func registry into CRT. * NOTE: This changes the default API for functions exposed under the   CRT by the TVMFuncCall API. `resource_handle` is now always given   as a new 6th parameter. * `resource_handle` is NULL when invoked on a global function and a   pointer to the module owning the function otherwise.* Generalize arena-based memory manager.* lint* Fix git-clang-format arg parsing* add apache header* add mutable func registry tests* git-clang-format* fix more lint* Move memory_test to crttests.* fix tests* checkpoint* checkpoint* bundle_deploy demo_static works* rm debug printf* git-clang-format* fix lint* add asf header* pylint* update build configs for jenkins* make regression compiler happy* fix build errors in regression GCC* address comments* git-clang-format* fix for 32-bit cpp regression* fix incorrect use of memcpy and tests for 32-bit* clang-format",0
[Relay][Frontend][Onnx] GRU Layer Support (#6020)* GRU debugging and testing added to onnx frontend.* All tests working and code formatted.* Fix lint issues.* Add a test case and changed RNN argument parsing.* Small refactor.,0
[CODEGEN] Fix code generation bugs for C/CUDA & Improve VerifyGPUCode pass (#6041),0
"[LLVM] Create TBAA information based on the unrelying buffer type (#6046)Currently, the TBAA information is based on the access type, i.e.the data type from the load or store instruction. When the samememory area is accessed with different types, the correspondingload/store instruction may end up not being aliased to each other.This could lead to incorrect code being generated.An example of when such a situation can occur is when two differentbuffer_decl's are created for the same buffer:  ba = buffer_decl(... dtype = 'int16' ...)  bb = buffer_decl(data = ba.data, dtype = 'int32x32' ...)Then instructions  ba[x] = 0  ... = bb[x]may be reordered in the final code due to the alias info indicatingthat they are not aliased.",2
[Relay] Add pass for getting calibration data from a relay module (#5997)* add simple pass to extract outputs* complete pass that collects all function inputs/outputs* add analysis pass for collecting outputs* reorganize the files* add the first test* update test with tuples* clean up Python code* merge with upstream* clean up transform.py* add comments for cpp files* fix lint issues* update submodules* modify files according to the review* fix style and typo* fix lint error* add checks for repeated function calls* fix lint error* merge review comments* small simplification* revise the code according to the review comments* add username in TODO* use IRModule directly* use better APIs according to the review* apply comments from the reviewer* retrigger ci,0
"Add support for tflite arg_min and arg_max (#5992)* [Relay][Frontend][TFLite] Add parser support for arg_min_max* this implementation supports only the case when the axis is a scalar* tflite 1.13 removes all dims of size 1, Relay doesn't do this* WARNING: every newer version of tflite > 1.13 needs keepdims=TRUE* Migrated to tflite 2.1.0keepdims set to False and added some checksNote the unit tests emmitted following warning:/workspace/src/te/schedule/bound.cc:119: not in feed graph consumer = compute(T_multiply_red_temp, 0x53f5050)* linter* Removed quantized argminRemoved quantized argmin due to inablility to provide proper test case* added negative ranges* re-trigger CICo-authored-by: Ina_Dobreva <Ina.Dobreva@arm.com>",1
[Frontend][TFLite] Fix fully_connected converter when batch size is not 1 (#6038)* Fix fully_connected when batched* Remove unused variable,0
"Fix conv2_gemm after target structure update (#6037)After target structure changed in this RFC:https://discuss.tvm.ai/t/rfc-tvm-target-specification/6844/42The conv2d optimizations was broken for the following reasons:- ""target"" is now called mtriple (this changes how we test if the  architecture is AArch64)- when we invoke ""clang.create_llvm"" we still need to specify the  ""--target"" option (set to aarch64-linux-gnu)This submission reverts those changesChange-Id: I04c597b91ca5800ddf4471255e2a358c60bc048e",0
[IR] Fix a primitive check error (#5991)* fix primitive check error* assuming every Op has Type defined* CHECK_NE -> CHECKCo-authored-by: Liangfu Chen <liangfc@amazon.com>,0
"Refactor to expose MakeOp functions to C++ (#6047)* Initial Refactor* add templated nn Make* functions* fix build typo* inline functions, fix unit tests",0
Fix pytorch frontend prim::Constant issue (#6051),0
[BYOC][COREML] Handle one symbol for each runtime (#5989)* [BYOC][COREML] Handle one symbol for each runtime* LOG -> DLOG,2
"[Hexagon] Remove use of designated initializers from hexagon_module.cc (#6055)They are an extension, not yet a part of the C++ standard.",4
"[RELAY][DYN] Dynamic broadcast_to, zeros, ones (#6007)* Dynamic BroadcastTo* fixed lint!* add test_one_hot() back* add one_hot registration back* Dynamic BroadcastTo* fixed lint!* add one_hot registration back* fixed lint.. again* fixed lint* lint* responding to comments* skipping cuda in dynamic test* skipping cuda in dynamic test* fixed i386 test and GPU test* lint* starting ones and zeros* fixed dynamic ones and zeros, wrote dyn ones and zeros test* added static version of zeros, ones and added a check for size of types to static BroadCastToRel* added dynamic to static pass for zeros and ones, dynamic test and dynamic to static test* removed op_str in dyn to static pass test* fixed lint* fix lint hopefully* removed import const* removed import that was actually used* copy all attributes from broadcast_to, ones, zeros, full* responding to comments* fixed build error* finishing rebase* fix lintCo-authored-by: Lily Orth-Smith <lorthsmith@Lilys-MacBook-Pro.local>",0
"[Ansor][AutoTVM v2.0] Part 0: Ansor minimum system for auto schedule generating (#5962)* Code migration Start (#1)* Init commit: Code migration Start* Add loop_state.cc/h* Add ComputeDAG basic test* Split transform_step out & Update more UTs (#3)* Split transform_step out* Update GetProducers & GetConsumers* Update UTs* Add UT for CacheReadWrite & Some bug fix* Add search_task, measure and serialization (#4)* Add FollowSplit & FollowFusedSplit tests* Update dag.InferBound & its UT* Add search_task, measure and serialization* Update Serialization UT* Add MetaTileRewritePolicy (#5)* Add feature* Add cost_model, meta_tile_rewrite_policy* Add MetaTileRewritePolicy basic UT* Basic Python API for State (#6)* Add Basic Python API for State* Add UTs for State* Add Python API: Measure & Task (#7)* Update the return value of state operation* Add task* Copy measure.py & utils.py* Fix LocalBuilder* Fix LocalRunner* Add ansor.auto_schedule() API; First AutoSchedule working version(#8)* Add basic Python support for ansor.auto_schedule* Update AutoSchedule API* Bug fix for get the attach point of a fused iter* Update UT after infer bug fix* Bug fix & Add python serialization API (#10)* Delete C++ UT hack since Python is ready* Add ndarray.non_empty* Update Serialization python API* Improve code style, python wrapper and test cases (#11)* Update c++ code style and unit test* Update python State wrapper and test cases* fix unit tests* Add RPCRunner & OpenCL/CUDA test (#12)* Add RPCRunner & OpenCL search test* Add CUDA search test* Add RPCRunner test* rebase to upstream/master* Add Ansor basic tutorial (#13)* Add basic tutorial* migrate feature extraction (#14)* Add XGBModel & RPCRunnerWarpper (#15)* Add XGBModel & RPCRunnerWarpper* Revert ""Add Parallel Granularity Mutation""* Migrate workload_registry.py (#16)* add workload registry* update* update* add task scheduler (#17)* Add conv2d cuda tutorial with workload registry (#18)* add tune_test.py (the old tune_wkl.py) (#19)* add tune_test.py (the old tune_wkl.py)* update* fix measure* fix for gpu* Code refine for tune_test.py & Add a pre load callback (#20)* Bug fix for tutorials* Add PreLoadMeasuredStates* Add search_callback support for task tuner* Code refine for tune_test.py* Update* Update* Update* Update* Bug fix* Add python custom sketch rule (#21)* Add custom sketch rule* Bug fix* Ansor Relay Integration (without layout rewrite) (#22)* relay integration* Add tune_op_subgraph.py & Some code clean for tune_network.py (#23)* Add single op tune scripts* Add tune subgraph support* Merge all op & all subgraph to one file* Rename file* add explicit_unroll_max_extent (#25)* Add Index simplification & API update (#26)* Add vectorized cooperative_fetching test* Update math simplify for vectorized CF* File rename* Update tune_network* API update* Update PreLoadMeasuredStates & Some bug fix (#27)* Add a threading wrapper to fix the test bug* Set default TVM_USE_AUTO_SCHEDULER to false* Update PreLoadMeasuredStates callback* Add tensorize step for loop_state (#31)* Add tensorize step* State python api update (#33)* Start to update api* Add compute_dag to state* API update* kernel layout rewrite (#28)* kernel layout rewrite* remove some hacks* add defuse_ops pass and move kernel_layout_rewrite pass after fuse_ops pass* set TVM_RELAY_DISABLE_BUILD_CACHE for task extraction and prepare_layout_rewrite* [cache flush] port cache flush to ansor (#32)* Improve relay integration (#34)* tmp checkpoint* Improve relay integration* Improve relay integration* Fix xgb error & Simplify dispatcher (#35)* Rename ""MetaTileRewritePolicy"" to ""SketchPolicy"". (#36)* Rename ""MetaTileRewritePolicy"" to ""SketchPolicy"".* Add a new class for auto_unroll_max_step, storage_offset in StageNode* fix tune_op_subgraph.py* rebase* Migrate all node::make to noderef's construct function (#37)* Start to move xxxnode::make to noderef()* Update* Update* Finish transform_step* Finish comute dag & auto schedule* Update* Update* Update* Update* Update* Code refine* Code refine* Code refine* Update* Update* Some lint fix & Recover the double constructor of tvm::PrimExpr (#39)* lint fix* clang-format-fix* pylint fix* Update* Recover the double constructor of tvm::PrimExpr* Fix pylint* pylint fix* pylint fix* Add MutateComputeLocation and MutateParallel in evolutionary search (#40)* Add MutateComputeLocation and MutateParallel in evolutionary search* fix lint* Improve loop state python API (stage_tensors -> stage_ops) (#41)* improve loop state python API (stage_tensors -> stage_ops)* fix* ComputeDAG bug fix & Add Custom TensorCore Matmul Example (#42)* Bug Fix* Sample example of Custom TensorCore Matmul* Rever Commits, Start to build minimum Ansor system* Code clean for minimum Ansor system* Bug fix & Delete AccessAnalyzer* Delete attachmap & Code clean* Doc updateUpdate statenode::stages from vector to Array* Headfile update & Python doc update* clang-format fix* pylint fix* Update* Doc update* Update* Bug fix after code merge to the new master* clang-format fix* Update* Update* Update std::vector to Array; Update verbosity setting; Some commemtsaddressed* std::vector->Array & std::string->String* Add init_state to ComputeDAG* Update* Update some unordered_map to Map* clang-format fix* Comments addressedDelete ReplayAndInferBoundDelete ReplaySteps & InferBoundCommon* Lint fix* Update* Update* Update* Update* Update* Update* Update* Update* Update* Rename ansor namespace to auto_schedule* Update* Rename ThreadPool to ParallelFor* Add parallel_for* Remove ThreadPool* Update python/tvm/auto_schedule/auto_schedule.py* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>Co-authored-by: Minmin Sun (孙敏敏) <minmin.smm@alibaba-inc.com>Co-authored-by: Zhao Wu <zhaowu@apache.org>",0
"Build crttest and cpptest separately. (#6057)* Build crttest and cpptest separately. * Try to fix random CI crashing, likely caused by concurrent cmake execution.* Revert to -j8",0
[RUNTIME] Support module based interface runtime (#5753),5
[Ansor][AutoTVM v2.0] Part 1: Rename namspace form auto_schedule to auto_scheduler (#6059)* Rename namespace auto_schedule to auto_scheduler* Update* Lint fix,0
[Doc] update frontend tutorials to new model based runtime (#6063),1
"[TARGET] ONNX codegen (#5052)* Relay to ONNX converter* Relay to ONNX op test cases* Relay to ONNX end to end model test cases* Add test cases to jenkins* CI CD fixes* ONNX codegen* ONNX codegen* ONNX codegen* onnx testcases* ONNX codegen* test onnx* ONNX codegen* shape calculation* move onnx codegen to contrib/target* review comments* ONNX target use visitor* onnx fixes* lint fixes* doc string changes* review comments* review comment fixes* review comment* pytest skip* rename type to node type* test* Fix for constantshpae, add exp, fix for metadatamodule* Fix cpplint* change error tol values",0
[clflush] Enable x86 cpu cache flush (#5914),5
[BYOC][Optimization] Run accelerator specific optimizations  (#6068)* register and invoke optimization pipeline for external codegen* add unit test,1
[Relay][Pass] Merge two consecutive reshape ops (#6052),4
"Add operation scatter_add to relay, based on scatter implementation. (#6030)",1
"Fix error message in Buffer::vstore, NFC (#6056)* Fix error message in Buffer::vstore, NFC* Fix whitespace in comment as well",0
[RUNTIME][CRT] init TVMPackedFunc's name (#6044)or else src/runtime/crt/graph_runtime/graph_runtime.c TVMGraphRuntime_RunLine 639 will show messy code.Signed-off-by: windclarion <windclarion@gmail.com>,5
Remove unnecessary std::cout (#6072)* Remove unnecessary std::cout* Trigger CI,4
[AutoTVM][BugFix] Fix variable name conflict with OpenCL keyword (#6048)Co-authored-by: Yanming Wang <yanmwang@amazon.com>,0
"[VTA] Fix FSIM Compile Error. (#6070)Issue:when set vta target into ""sim"", vta compile would get fail andshow error message ""fatal error: vta/driver.h: No such file or directory"".Solution:set VTA_HW include path correctly.",0
[ARITH] Improve vector simplification for float operands (#6043),5
"Refine LSTMBlockCell to support dynamic rnn (#5963)1. Refine conversion of `LSTMBlockCell`       1) Make its output follows definition in TensorFlow       2) Avoid introducing variables which doesn't match any placeholder nodes in TensorFlow graph    2. About change in test_forward_ptb       States nodes of LSTMBlockCell in this PB file  are actually Constant node.       TF can feed data to those Constant nodes but relay can't do that, so current conversion of LSTMBockCell introduces extra variables to solve this issue.       But this causes that relay IR doesn't match original TF graph. This PR solves this issue by convert those states node into placeholders.",2
[TOPI] Fix the filter width parameter in depthwise_conv2d (#6081)* [TOPI] Fix the filter width parameter in depthwise_conv2d* Retrigger buildCo-authored-by: Venkat Rasagna Reddy Komatireddy <quic_rasagna@quicinc.com>,0
[Fix] Add missing expr visitor for any (#6082),0
"Fix LocalBuilder on macos with python 3.8. (#6083)Python 3.8 changes the default way multiprocessing creates new processeson macOS from forking to spawing. Spawning requires all objects to bepicklable. Nested functions and lambdas are not picklable, so thiscommit fixes the one instance of nested functions in the codebase thatwas causing issues.",0
[Test] Add missing test for fast erf (#6058)* add missing test for fast erf* trigger ci,1
Fixed point multiplication improvements for AArch64 (#5980)* Fixed point multiplication improvements for AArch64Change-Id: Ib3c10348d4c0eac11fa92b39cc6e792560e9eba4* Fix python linting errorsChange-Id: I4cf5ac18aa24b39374b83805dcc8e1663e173909* Fix doxygen errorsChange-Id: Ie3c861f8ead3f1ea5b30d5e9d7d94e222299d407* Fix arm_cpu injective testsChange-Id: I6ad9da61b61e6bd737627f26fba59767418c07cd* Fix python linting errors - 2Change-Id: Ic864a235aa5da5786393cbf6146dd815c121df5e* Fix arm_cpu injective tests - 2Change-Id: If9ca1cc3d947b1656c836c7f88de90470d92f979* Redesign: introduce a qmuls (q-multiply and shift) general intrinsicChange-Id: I1966fef9aee32eab50e4b984bbe81018488c8c02* Fix python linting errors - 3Change-Id: Ib87a19a8ee2d532954a7db1eb5793666e7aef366* Addressing review commentsChange-Id: Ie82e75204e5a421d17660f381f3e31fc325cd26c* Fixing test failuresChange-Id: I74cc675764cf8d260fe68a41e770b1ec7e84729a* Renaming qmuls to q_multiply_shiftChange-Id: I5a8ed60ba855208040304fcdf6e1ea28061f06ad,0
"[Relay][Dyn] Add dynamic reshape grad (#6080)* add dynamic rehape grad* fix lint* fix unit tests, warning",0
[Docs] improve the doc of release (#6091),2
lint: add opencl .cl file type (#6092),1
[Ansor][AutoTVM v2.0] Phase 1: Add RPC Runner (#6077)* Add rpc runner* Update* Update* Add clflush & non-empty ndarray TODO hints* Update* UT Update* Update timeout in UT,1
[DOCS] Cleanup docs build instructions. (#6094),2
MXNet pre-quantized BERT (#6039)* MXNet pre-quantized BERT* Comments.* Trigger.* Retrigger CI* Retrigger CI* Retrigger CI* Retrigger,5
[DSL/TE] Scalar support for `te.extern`  (#6079)* fix make shape with scalar shapes* add test* add test* remove scalar shape assertion* fix the data type for overflow problems* add extra testsCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>,0
[Cmake] Add default value for option USE_DNNL_CODEGEN in the cmake (#6099),1
delete declaration of unused op_node (#6102),5
load empty config (#6100),5
Update SGX example Cargo.toml (#6067),1
"[BYOC][Contrib] Arm Compute Library integration (#5915)* [BYOC][Contrib] Arm Compute Library integrationArm Compute Library (ACL) integration using the BYOC infrastructure. This will enable offloading select operators from a relaygraph to ACL so we can achieve faster inference times on Arm CPU's due to hand crafted optimized routines. The PR adds initialsupport for offloading FP32 conv2d, maxpool2d and reshape to ACL. ACL codegen is used to generate a JSON representation of anoperator or 'ACL layer', the ACL runtime then uses this representation to construct a layer, cache it and create a packedfunction to for the graph runtime to call into.RFC here: https://discuss.tvm.ai/t/rfc-byoc-arm-compute-library-integration/7082Change-Id: If756dcea787ea346b1508e9a191b7eed7bd02b7f* Refactor ACL integration to support JSON runtime* Now uses JSON runtime* Addresses tutorial comments* Rename acl to arm_compute_lib in user facing apiChange-Id: I3b5ef80607f713e898363e82ab4398fbc2cf267a* Address commentsChange-Id: I041fda14f3bf9975f3518ba8a4e3ab43ba98403d* Address comments* correct mistakes in tutorial* reshuffle runtime to use fewer macro blocks* preprocess module using ""optimize"" functionality* use new module apiChange-Id: I219488e617e5767edd7489b43b8bfce876cd24b8* Enable ACL codegen tests in CI* Skips runtime tests as these are not supported on x86.Change-Id: I6843c003a2604afe95cfdccf2323d2a336b56fe5* Fix check for runtimeChange-Id: I3f9eec15c599f01b1105d624fb053b73bfb6ed41* Address comments* Add warning to ACL engine creation* Correct runtime check so it doesn't fail when codegen not present* Improve testing to check acl partitions is what is expected* Check results of multiple runs testChange-Id: I9522950930805b9b601dad03269adcf8ed3138cc* Address comments* Multiple style improvements* Use base class for creating json node for single op* Move GetSource to base class* Improve annotation checksChange-Id: I8219659c4b99e86df887cd914720157cb94c61a0* Improve tutorialChange-Id: I8f610bd37af1e3740fd48c2d502bcc4727d9d712* Initialize conv with nullptrChange-Id: I6c37f0d75a064001c74e171ff83b9f7a7c3f1918",0
[Relay][VM] Add ReshapeTensor instruction in the VM to replace the reshape op (#6089)* [VM] Add reshape tensor instruction* update* lint* fix* fix,0
[Ansor][AutoTVM v2.0] Phase 1: Add annotation/compute_at/compute_root/compute_inline steps (#6073)* Add annotation step* Add compute_at/compute_root/compute_inline* Doc update* Update* Update* Update measure record UT* Update* Update* Update* Move state implementation to step* Move measure_record implementation to step* Order update & API update* Update the order of state api* Update,1
"Update installation doc with minor improvements (#6104)Make some minor improvements to the install from source docabout flags to enable, package managers, and virtual environments.",1
[Rust][CI] Move CI over to new Rust crates and try to fix flaky test. (#6011),0
"[DOCS][REFACTOR] Organize Design and Architectures (#6097)* [DOCS][REFACTOR] Design and ArchitecturesThis PR refactors the design and architecture docs.Previously this part of documentation was quite unstructured, and lacks a globalview of the overall architecture.This PR takes a stab in resolving the problem- Provide a guided overview of the current TVM's overall design- Categorize the specific docs into architecture components or How tos.* Apply suggestions from code reviewCo-authored-by: Jared Roesch <roeschinc@gmail.com>* Apply suggestions from code reviewCo-authored-by: Jared Roesch <roeschinc@gmail.com>* Update per comment* More updates per feedbacks* clarify external codegen* Update per commentsCo-authored-by: Jared Roesch <roeschinc@gmail.com>",1
"[Rust] Clean up conversions between TVM and Rust functions (#6114)* Replace ToBoxedFn with From* Compact and improve Typed and ToFunction impls- Clone one less time- Don't panic if number of args is wrong, return an error- Actually drop functions/closures on the rust side* Retry",0
Improve reduction schedule on arm CPUs (#6110)* Improve reduction schedule on arm CPUsChange-Id: I9cd85deac6a57666b82ff7250d827652a4000d82* Retrigger CIChange-Id: I5efd99e34268e6bb990904a4b98e1edf2174b26b,4
"Register Shape Func for Some Operators to Handle Dynamic Shapes (#5955)* Register Shape Func for Floor OperatorRegister the shape function for `floor` operator. Otherwise, a bug will happen when input of floor is any.* Register shape func for log* add shape function for crop_and_size* change import location* add mirror_pad shape function* add test cases for crop_and_resize and mirror_pad shape funcs* support different layout* fix pylint error* fix pylint error* add test for nchw layout* block nchw test* test for nchw* use tvm.testing.assert_allclose insteadCo-authored-by: lisiyuan <lisiyuan@nucflow>",0
[RELAY][Fix] i64 indices (#5235)* fix* resolve comments,0
[Rust] Some rust cleanups (#6116)* Some rust cleanups* Turn off default features for bindgen* Upgrade some deps for smaller total dep tree* Switch (/complete switch) to thiserror* Remove unnecessary transmutes* Fix null pointer assert* Update wasm32 test,0
Add 'get_num_inputs' to GraphRuntime (#6118),1
[Relay] Keep fixed dim when unifying dynamic shape (#5795),0
[Relay]Port eliminate_common_subexpr to non-recursive form (#6134)Co-authored-by: Zheng Jiang <zhejiang@amazon.com>,5
[Flaky] TFLite quantized conv test (#6084),3
[Relay][VM] Allow to config allocator type and refactor vm code structure (#6105)* [Relay][VM] Allow to config allocator type and refactor vm code structure* fix doc* fix* update* trigger ci* trigger ci* trigger ci* trigger ci* fix doc warning,0
[AutoTVM][BugFix] Fix autotvm on the conv2d_nchw_winograd.mali operator (#6130)* [AutoTVM] Fix conv2d_nchw_winograd.mali* Fix pylint errorCo-authored-by: Yanming Wang <yanmwang@amazon.com>,0
[TOPI] Fix CUDA Library Tuning (#6132),0
[Ansor][AutoTVM v2.0] Phase 1: Access Analyzer (#6103)* add access analyzer* add test cases* move header files and polish comments* fix lint* update* fix lint* address comments* fix lint,0
add attr option mfloat-abi for arm32 (#6123)* add attr option mfloat-abi for arm32* retrigger,1
[Fix] Remove the tvm web from version update (#6122),0
[Relay] Fix interpreter for dyanmic shape input of ndarray_size (#6086),0
[Ansor][AutoTVM v2.0] Phase 1: Add cache_read/cache_write steps (#6107)* Add cache_read/cache_write step* Update* Update* Update* Update state->current_compute_dag to Optional* Update* Update doc* Update* Update* Doc update* Update,1
[Relay][OP] Support NMSv4 ingestion from TF. (#6085),5
[DOCS][REFACTOR] Reorganize the docs. (#6146)- Move most toctree to `:hiden:` so there can be top-level categorizations in the navigation bar.- Move frontend guide into design and developer guides- Move get started tutorials into its separate folder.Co-authored-by: Chris Hoge <chris@hogepodge.com>Co-authored-by: Chris Hoge <chris@hogepodge.com>,2
[CI][TEST] Temporary disable nmsv4 test (#6151),3
Add TVM application extension with WASM runtime (#5892)* Refactor wasm runtime module and resovle conflict errorsSigned-off-by: leonwanghui <wanghui71leon@gmail.com>* Fix some cargo clippy warningsSigned-off-by: leonwanghui <wanghui71leon@gmail.com>,0
[Android][RPC] Add missing RPC sources after refactor  (#6113),1
[DOCS][REFACTOR] Clarify Docs Categorization (#6155)This PR categorizes the docs into a few categories:- How To- Tutorials- References- Deep Dive- MISCCo-authored-by: Chris Hoge <chris@hogepodge.com>Co-authored-by: Chris Hoge <chris@hogepodge.com>,2
Adding t-vi as a reviewer (#6149),1
"[Ansor][AutoTVM v2.0] Phase 1: Add follow_split and follow_fused_split steps (#6142)* Add cache_read/cache_write step* Update* Add follow split and follow fused splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>Conflicts:src/auto_scheduler/compute_dag.ccsrc/auto_scheduler/transform_step.ccsrc/auto_scheduler/transform_step.htests/python/unittest/test_auto_scheduler_loop_state.py* add loop_state.pySigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update* Update* Update state->current_compute_dag to Optional* Add some doc strings for Follow_Split and Follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Check code using c-lintSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add more doc strings and change the order for follow split.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_split and follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add record test for follow_fused_split.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add test record for follow_fused_split1. delete a comment2. add ""fuse"" between follow_split and follow_fused_splitSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add doc strings for some functions and variablesSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Fix the code format in src/auto_scheduler/transform_step.hSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update* Update doc* Update* Update* Fix follow_split and follow_fused_split record test.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Doc update* Update some doc stringsSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Fix code style and some function definitions.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add comments on parameters.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Add more doc strings and fix some.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* UpdateSigned-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>* Update.Signed-off-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>Co-authored-by: chengfan.jcf <chengfan.jcf@alibaba-inc.com>Co-authored-by: jingbang.yjb <jingbang.yjb@alibaba-inc.com>",0
[CI][Caffe Frontend] add caffe environment (#6023)* [CI][Caffe Frontend] add caffe environment* [CI][Caffe Frontend] change the caffe deps into BVLC distribution.* [CI][Caffe Fronted] simplify configuration while installing tzdata for precompiled caffe.* [CI][Caffe Frontend] add more information about tzdata.* [CI][CaffeFrontend]remove the ci for gpu env and change to pip3 envCo-authored-by: fernchen <zifeng.cf@alibaba-inc.com>,1
[TIR][Bugfix] Improved massive build times caused by tir.floormod and tir.floordiv. Fixed Topi testcase. (#5666)* Improved uncommon case of floormod and floordiv. Removed dependence on np floor_div and fmod.* Fixed clang-format complaints* Streamlined floormod and floordiv lowering logic* Improved build times by expressing int64 case of tir FloorMod and FloorDiv using let nodes* Updated use-def analysis and llvm codegen to support duplicated letnodes.* Corrected misuse of var_map_ in llvm codegen* Updated backends that support LetNode* Changed floormod and div lowering logic to avoid using FP on systems that don't support it.* Fixed formattingCo-authored-by: pankratz <pankratz@ualberta.ca>,0
Correct runtime.load_module (#6161),5
"[Topi, x86] Using MKL blas for quantized dense (#6115)* [Topi, x86] Using MKL blas for quantized dense* Typo* CBLAS_OFFSET only available for MKL* Skipping tests as GPU CI uses Openblas* RetriggerCo-authored-by: Ubuntu <ubuntu@ip-172-31-0-202.us-west-2.compute.internal>",3
[Relay] Handle ndarray_size in FoldConstant (#6156)* [Relay] Handle ndarray_size in FoldConstant* Use Optional,5
"Improve error messages in graph tuner, graph runtime, and module loader. (#6148)* Raise error if no operators are found in GraphTuner* Raise error if key cannot be found in graph runtime inputs* Detailed error message when module loader is not found",0
[Ansor][AutoTVM v2.0] Phase 1: Add pragma/storage_align/rfactor steps (#6141)* Add pragma/storage_align/rfactor step* Update* Update* Update UT* Update,1
[CI] Update ci-cpu to the latest (#6164),1
[Parser] Typo in mod creation (#6165),5
"[BYOC][ACL] Support asymmetric per-layer quantized operators (#6109)* [BYOC][ACL] Support asymmetric per-layer quantizationAdds support for asymmetric per-layer quantization in the ACL runtime. This includes support for qnn.conv2d, nn.maxpool2d and reshape. Reflected these changes in codegen and runtime tests.Change-Id: I8f610bd37af1e3740fd48c2d502bcc4727d9d712* Address commentsChange-Id: I4f9e3e7dbf6053066927cf07c4c19ecc88572e9d* Fix tutorialChange-Id: I4371e9d97a120fb7776db40ffcde60f46927af4d* Improve test infrastructure* Doc-string for generate trials* Output params on errorChange-Id: Ib2e2b1fcdf05cdc77f7f4fb4b46395f28c129957",0
fix bug when converting constant nodes with types of int64 or float64 (#6159)Co-authored-by: yuweilong <yuweilong03@meituan.com>,0
[TF] Fix some shape mismatches between TF and Relay (#6166)Make ndarray_size output scalar  Make gather_nd output scalar if needed,0
[AutoScheduler] Improve doc string (#6176),5
Buffer logger assert removed (#6147),2
[BYOC] Retire the example json runtime (#6177),5
Fix incorrect function signature in header (#6172),0
[Relay] Fix bug in transpose_shape_func (#6180),0
[DOCS] Improve the docs build instructions (#6173),2
"[FIX] Fixes #6096 (#6131)Clear the compile cache between module builds so that schedule changeswill have an effect. Also, clear the warning cache so that schedulechanges properly list untuned ops.",0
[TEST] Temporary disable conv2d grad strided flaky test (#6183),3
[REFACTOR] topi -> tvm/topi (#6186)This PR migrates the topi library as a sub namespace of tvm.,5
[CI] Remove topi from the CI cache (#6188),4
[BUILD] Remove libtopi from the build (#6189),4
[FoldConstant] Create Interpreter for each constant subgraph (#6195),5
[Fix] avoid unexpected throw in AttrInitEntry (#6128),0
[TIR] Enhance VerifyGPUCode (#6194),5
[TIR][Transform] HoistIfThenElse added (#6066)* [TIR][Transform] HoistIfThenElse added* lint error resolved* Pass position changed* pylint error resolved* CI issues resolved* Frontend tflite test case failure resolved* [1] Review comment handled* [2] Review comment handled* [3] Review comment handled* Lint error resolved,0
[Target] Rename target_id => target_kind (#6199),5
[AutoScheduler] Fix alignment of note (#6181)* Fix alignment of note* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,0
[DOCS] Added casting to hybrid script doc and fixed pass infra doc (#6174)* updated hybridscript docs and pass infra docs* forgot uint16,0
[CI][ETHOSN] Enable CI for Ethos-N (#6171)This introduces the necessary changes to docker tosupport building the Ethos-N driver stack. This isrequired for subsequent patches which introducethe Ethos-N integration into TVM.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com># Please enter the commit message for your changes. Lines starting# with '#' will be kept; you may remove them yourself if you want to.# An empty message aborts the commit.## Date:      Mon Jul 27 15:43:41 2020 +0100## On branch ethosn-ci# Changes to be committed:#modified:   docker/Dockerfile.ci_cpu#new file:   docker/install/ubuntu_install_ethosn_driver_stack.sh## Untracked files:#CombinedMemoryMap.hex#OutputModel.hex#config.txt#docker/install/ethosn_cap/#docker/install/ethosn_driver_dev-20.05-dbg-20200612-141030.tar.gz#ssd.npy#tests/python/integration/test_tir_gemm.py#tests/python/relay/test_pattern_annotate.py#tests/python/unittest/failure.py#,1
[TFLite] Implemented PADV2 Operator for TFLite and added support for constant values in PAD. (#6167),1
[RELAY] Basic block normal form (#6152)* initial commit* refactor utils* add util* revert anf test* update test* fix logging* fix scope bug* complete tests* remove logging* revert refactoring* add one more test case* fix missing var binding* fix test* fix lint* fix lint* fix clang-format* fix lint* fix lint* commit missing code* add analysis api* fix lint* fix lint* lint* add test for func* address CR* fix typo* fix return type* fix lint* refactor classes* fix lint* remove prints* address commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-138.ec2.internal>,0
[COREML]Unary ops support added in frontend (#6196)* [COREML]Unary ops support added in frontend* Used coreml enums,1
[Rust] fix #6205 (#6207),0
"Change the meaning of conv3d_transpose output_padding to match conv{1,2}d_transpose (#6065)* Change the meaning of output_padding to correspond to conv{1,2}d_transpose* Fix long lines* Fix the relay test* Add missing doc.* fix size ordering problem",0
Fix compile warnings. (#6204),0
[Target] 64-bit RPi4b target (#6211),5
Pass mfloat-abi to LLVMModule::Init (#6150)Fix lint,0
fix compilation error with cuda 11 (#6213),0
[Relay] pytorch frontend support conv1d (#6203)* [Relay] pytorch frontend support conv1d* add tests for conv1dCo-authored-by: xutianming.xtm <xutianming.xtm@bytedance.com>,1
match pytorch 1.6 googlenet pretrained model (#6201) (#6212),5
[RUNTIME] Enable auto conversion String->DLDataType (#6214),5
[DOCS] Update pass infra tutorial (#6193)* [DOCS] Update pass infra tutorial* update tutorial,1
[Ansor][AutoTVM v2.0] Phase 1: The base class for cost models (#6187)* add the base class for cost models* address comments* Update tests/python/unittest/test_auto_scheduler_cost_model.pyDisable test if user doesn't have llvmCo-authored-by: Zhao Wu <zhaowu@apache.org>,1
[Relay][Dynamic] OneHot operation (#6209)* Dynamic OneHot Op* refactor dynamic_to_static* add onehot to dynamic_to_static pass,1
"[µTVM] Add --runtime=c, remove micro_dev target, enable LLVM backend (#6145)* need to fill address of globals in tvmfuncregistry* llvm func registry generator works!* lint fixes* rm hexdump include* bring bundle_deploy back to life and add to CI* revert gcda additions* git-clang-format* fix check for --system-lib and test_runtime_micro target* fixup compile flags for bundle_deploy CRT and improve robustness* git-clang-format* add debugging info* git-clang-format* initialize ret_values in PackedFunc_Call.* retrigger CI* fix log messages* git-clang-format* remove default for --runtime target opt* put backtrace behind a flag and enable it* simpify ReadString(), fixing bad instruction exception on os x.* git-clang-format* uncomment tests* reorder backtrace ldflags for linux gcc",0
[PYTORCH]Std op without specified dimensions support (#6226),5
"[ONNX]Mod operator, bug fix (#6160)* Onnx mod, bug fix* Added comment for the mod/floor_mod behaviour difference between numpy & relay",0
Reshape with dynamic shape arg (#6208)Reshape operation updated to take shape from second operand.In case if shape is provided using second operand itcan be a tensor now.,1
[C++ RPC] fix typo to keep same with source code (#6220)Signed-off-by: windclarion <windclarion@gmail.com>,0
[Relay][Pass] Support combine multiple dense op just into dense (#6062)* feat: Support combine multiple matmuls to flat matmul* fix: Change to_batch -> to_batch_matmul and enrich docstring* feat: Add wrapped batching ops pass for python,0
[runtime][cublas] fix typo (#6230),0
[Relay][Dynamic] Add Dynamic Resize Op (#6198)* WIP* optionally remove output shape inference from topi* fix resize* add resize to dynamic_to_static passadd resize to dynamic_to_static pass* fix clang-format* fix bad rebase* add argument to dynamic resize doc string* fix i386 test* fix lint,0
[FIX] Verify that tensor reshape is valid. (#6215),0
[BYOC][JSON] json_node.h should include data_type.h (#6224)Fixes compilation issue after #6214.Change-Id: I07e25356bbfe4a7bd0950f2672441ce1c338dc3f,0
[uTVM] fix crt building and running error (#6231)Signed-off-by: windclarion <windclarion@gmail.com>,0
[Relay][Op] Add unbiased variance op and corresponding support in pytorch frontend (#6232),1
[COMMUNITY] jcf94 -> Reviewer (#6241),3
[TFLite] Implemented ONE_HOT Operator for TFLite (#6223),5
[TIR][Hybrid] Hybrid Script Support for TIR (#6227),5
"[TOPI, Cuda] Fix conv2d_transpose output padding (#6236)",0
[BYOC][ACL] Improve installation tutorial (#6170)* [BYOC][ACL] Improve installation tutorialImproves installation script so that ACL can be built natively and improves tutorial to give clearer information on how ACL can be installed using two different methods.Change-Id: I6cec98b4b0a7dc2b151b36583d3d28f2b85f8702* Address commentsChange-Id: I88db6d9d539a8f06e2dfe1b9a0a3ac7a4b46cece,1
[Relay]Refine tensorflow frontend 1.x & 2.x compatibility (#6240)* [Relay]Refine tensorflow frontend 1.x & 2.x compatibility* fix lint error* revert gpu related changes,0
"fix cuda half math function is undefined: hpow, htanh (#6225)",0
"[Topi,x86] Split MKL from BLAS. (#6182)Make cblas and mkl seperate entities in cmake and topi, allowing usersto use both a BLAS library and MKL. In the future, MKL specificfunctions can be added easily. MKLDNN is also split off from MKL andBLAS for the same reasons.Other improvements:  - cblas and mkl strategies are now only applied when they are viable.  - compile_engine will log which implementation it has chosen and why.",1
[RPC] Update build support for cross compiling apps/cpp_rpc with OpenCL (#6229)* Standardize support for building and cross compiling apps/cpp_rpc.* Add cmake coverage for building the C++ RPC server binary  and update documentation.* Add support for linking against custom OpenCL SDK employing  a custom find_opencl macro. This can be useful when cross  compiling with a custom OpenCL device driver.* Update OpenCL related documentation.* Add embedded linux build instructions to apps/cpp_rpc/README.md andensure pthread is linked against when OS=Linux is defined. Removeoutdated apps/cpp_rpc/Makefile.,1
[TFLite] Implemented EXPAND_DIMS Operator for TFLite. (#6243),5
"Fix division range estimation error in simplifier (#6244)Division a/b assumes maximum values when b is close to 0. Accountfor that when estimating the range for a/b when 0 belongs to theestimated range for b.Assume that a division by zero cannot happen in a valid program,so in such cases treat the range for b as a union  [b.min_value, -1] u [1, b.max_value]",0
"[JVM] Support overriding RPCWatchdog termination behavior on Android and other platforms (#6216)* Instead of performing a system exit and leaving unhandled items onthe activity stack, finish the RPCActivity and return cleanly to theMainActivity where the RPCActivity can be restarted automatically.* Update doc. string for checkstyle.",1
[Ansor][AutoTVM v2.0] Phase 2: Basic CPU Sketch Search Policy (#6184)* Init commit to pass the compile* First commit to pass the test* Update* Add UTs for sketch generation* Update* Add ASF to new UT file.* Update rule for winograd* Update* File renamed* Lint fix,0
[Parser] Parser 2.0 part 2  (#6162)* Add code from livestream with JK* Fix errors parsing ResNet* Parse metadata section efficiently and do most of plumbing to resolve metadata section references.* WIP* Change meta reference to an operator* Meta references now work* MetaReference expansion now works* Start working on source map and move diagnostic context* Convert tokenizer and parser to use new machinery* Kill to_json* Fix comment in type_infer.cc* Remove old parser* Rename parser tests and remove old ones* Record span end information* Convert to using spans everywhere* Add span fields back to all Relay constructors* Start passing spans* Pass spans around visitors* Format* Fix* Fix* disable reference lint from string helpers* Fix tokenizer* Fix issue with empty metadata section* Document new span fields and small tweaks* Formatting* Add span doc fields* Add format tweak* Improve errors and fix the semantic version tags in Prelude* Update gradient.rly* Clean up broken spans* Clean up parser tests and turn on previously skipped tests* Update errors to handle skipped cases* Tweak* Tweak* Format* Fix some minor issues with ADT tests* Format* Fix path* WIP* WIP* Fix ir_text_printer* format* Formatted* More formatting* Repair test cases* Fix CI* Retrigger CI,0
"Revert ""fix cuda half math function is undefined: hpow, htanh (#6225)"" (#6249)This reverts commit ed04cdd35f1990959ec788be0131b1388fd11d31.",0
[TOPI] Fix reduction (#6250),0
Fix newer GCC compiler warnings. (#6257),0
[COREML]Reduceops support added to frontend (#6252),1
Support mxnet _contrib_SyncBatchNorm (#6245),5
[Ansor][AutoTVM v2.0] Phase 1: feature extraction for cost models (#6190)* [AutoScheduler] add feature extraction* fix lint* fix gpu test* address comments* improve flop estimation* rebase* refactor with group* fix* Apply suggestions from code review,0
[CI] Add apt repository for clang-11 and llvm-11 (#6256)- Add specific apt repositories to install clang-11 and llvm-11 - Fix #6255,0
update tutorial to new TARGET as micro_dev is no more (#6262)Signed-off-by: Tom Gall <tom.gall@linaro.org>,1
Improve NHWC depthwise convolution for AArch64 (#6095)* Improve NHWC depthwise convolution for aarch64We created a default schedule (no auto-tuning or tensorization) nameddepthwise_conv2d_nhwc which does a decent job at optimizing depthwisefor NHWC layouts (on aarch64).Change-Id: I01e32903f6c1950623f33eae18484e70244fe0af* Add tuning knobs in depthwise scheduleChange-Id: I15080e7f12b16e6c6aba99a04e42023845eeabf1* Introduce padding policyChange-Id: If12a6d05dce9153861550ddef1ee5216809dd1e1* Vectorize paddingChange-Id: I7e2062a40358bf111c0366a449945eb077fb2e30* Legalize depthwise convolution (2x improvement) and fix tuning issueChange-Id: I4b82c58b167e40b0b7747d28293bbb488c505dd9* Adding assert on paddingChange-Id: Idf8eeaaface5eb7799109cd00f437e404778b9cd* Fix python lintingChange-Id: Iac16a8daea1268f0eb331fe4ec18a62408106cf9* Removing commented codeChange-Id: I1412f22ad9864273d77a7bf38a6768694339b7f0* Revert test file to make CI passChange-Id: Ica3eff8f9f0fd4c6f32f7ae80adc922f8b16cec9* Enabling only arm_cpu testsChange-Id: Icbaafcb39e892a5d1a4685133c1699e4d1a8e07e* RebasingChange-Id: Ibb23f1d4e0d0107e4e3b3571437161cdc2ee2909,0
[LINT] Fix clang-format (#6264),0
"Trivial fix, up the rodata section for the discovery board to 512 bytes. (#6259)This is more reasonable as the trivial tflite example module needs 208 bytes.Signed-off-by: Tom Gall <tom.gall@linaro.org>",0
"fix cuda half math function is undefined: hpow, htanh (#6253)",0
[Relay][Dyn] Dynamic full operator (#6260)* moved full from other branch* fixed some typos* fix lint* add final newline* fix int64 test,0
"[BYOC][ACL] Add support for dense (fully connected) layer (#6254)* [BYOC][ACL] Add support for dense (fully connected) layerThis patch adds the ability to offload dense (or fully connected) operators to ACL.For fp32 a single dense layer can be offloaded, or the composite variant: nn.dense, nn.bias_add? (ACL does not currently offer fused activation).For uint8: qnn.dense, nn.bias_add?, qnn.requantizeChange-Id: I83ea00b2aa6bdc5d9ef5cd6d54bbf981e523bd14* Don't offload dense layer with unsupported datatypeChange-Id: I856eb2298499fdf22c172ba7f85d21033d3cc920",1
add dilation in x86 NCHWc depthwise conv support (#4962) (#6267),1
[Parser] Add support for parsing the any dimension.  (#6277)* Add case for any dimensions* Fix second test case,0
"[TESTS] Decrease test times by introducing testing model (#6235)Adds a new testing model `tvm.relay.testing.synthetic` which is a small,but representative model. Replaces resnet with this model in many tests.",1
"Update precision in the ONNX strided_slice, update precision of ToScalar (#6272)* Update precision in the ONNX strided_slice, update precision of ToScalar* fix tests",0
Added support for tflite quantized maximum and minimum (#6018)* Added support for tflite quantized maximum and minimum* Unit test simplifiedBugfix in unit test. Unit test slightly simplified* re-trigger CI* renamed use_real_qnn to ignore_qnn_params,0
[Target] Creating Target from JSON-like Configuration (#6218)* [Target] Creating Target from JSON-like Configuration* Address comments from Cody* fix unittest* More testcases as suggested by @comaniac,0
Improve error messages for memory verifier and gpu memory verifier (#6281)* [FIX] Print exactly what issues the GPU memory verifier encountered.* [FIX] Print exactly why memory verifier failed.,0
[CI] Update ci-cpu to the latest (#6283),1
[Build] Reflect Compile-Time CMake Options into libtvm.so (#6280)* Initial comit* Address comments from @tqchen,1
Add Quantize/Dequantize Partitioning (#5940)* Implement quant/dequant partitioningon our wayget clooooooserclean up (part 1)clean up (part 2)clean up (part 3)clean up (part 4)clean cleancleaanaannanaaananaananaananaanclkjsdflkjlfsjdflkjrevert parser changesadd docsroll lintroll lint* add option to toggle fully integral check* convert dtype collector to C++* remove need for `with_dtype`* remove unused imports* roll lint* partially address feedback* roll lint* upgrade to new parser* retrigger CI* roll the dice again,1
"TVMC - a command line driver for TVM (#6112)* Introduce a command line driver to compile, run and tune models, using TVM graph runtime * Include tvmc tests and integrate tvmc with linting, testing and CI * RFC: https://discuss.tvm.ai/t/rfc-a-tvm-command-line-interface/5165Co-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>Co-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>",3
[Frontend][Relay] Fix node indices attribute error for tensorflow 2.3 (#6288)* Fix errors caused due to node attributes* Add node_indices attr for old keras pkg support,0
[Build] Add cmake options into libinfo (#6286)* [Build] Add cmake options into libinfo* Address comments from @tqchen* Add LLVM version to libinfo,1
[random] support random fill (#5913),5
[ONNX] Update slice to infer attributes when not graph inputs (#6276)* Update ONNX Slice converter to infer slice attributes when necessary.* Linting,1
[COMMUNITY] @kparzysz-quic -> committer (#6290),3
[TOPI] Support int4/int8 conv2d tensor core with HWNC layout (#6121)* int4 tensorcore* a draft for new int4 schedule* update layout* add inline option* clean code* increase search space* fix kernel shape* update intrinsic* update intrinsic* support int4/int8 hwnc layout* remove useless code* remove useless code* remove useless code* remove useless code* fix int8 transpose* fix assert* add asf header* CI* CI* CI* fix bugfix bugCo-authored-by: Leyuan Wang <laurawly@gmail.com>,0
[TEST] use rpc.LocalSession for simple tests (#6294)To avoid flaky due to networking.,3
[Torch] Support index_select (#6295)* support index select* minor fixCo-authored-by: masa <masa@pop-os.localdomain>,0
"Gather operation with indices as tensor expr in TFLite frontend (#6168)* gather with indices as tensor exprAdded handling of indices as tensor exprto gather operation, unit tests amendedCode cheking out of boundary error refactoredin more ""pythonic"" way. Fixed bug in negativeaxis value normalisation* replaced with get_tensor_expr",0
[RUNTIME][REFACTOR] Use new to avoid exit-time de-allocation order problem in DeviceAPI (#6292),1
[Autodiff] Optimize and eliminate the Jacobian tensor for te.autodiff (#6078)* [Autodiff] Optimize and eliminate the Jacobian tensor for te.autodiffCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>* fix lint* fix clang-format* add comments and magic number* clang-lint* address some comments* remove FreeVarsVisitor* fix constexpr lint* fix lint* fix lint* add Map.Merge* lint* change Array::Concat & Map::Merge to global functions* fix lint* move functions to global* static -> inlineCo-authored-by: Sergei Grechanik <sergei.grechanik+h@gmail.com>,0
Add tvm::support::hexdump() debug utility (#6154),0
[Support] Add parallel_for support to run a loop in parallel (#6275),1
[Ansor][AutoTVM v2.0] Phase 1: XGBoost Cost Model (#6270)* port xgb cost model* add xgboost cost model* fix lint* address comments* address comments* Fix,0
[Hexagon] Initial support for Hexagon codegen (#6261)* [Hexagon] Initial support for Hexagon codegenThis commit does not support parallel execution or prefetch.LLVM 7 or later is required.* Set native_vector_bits_ based on target features* Initialize hvx_bytes* Remove commented out line,4
[Torch] Fix cast to long (#6301)* [Torch] fix cast to long* retrigger,0
"[BYOC][ETHOSN] Introduce the Ethos-N BYOC integration (#6222)* [BYOC][ETHOSN] Introduce the Ethos-N BYOC integrationThis is the first of 3 PRs to introduce the Ethos-Nintegration into TVM via the BYOC framework. It addssupport for partitioning and compiling for theEthos-N77 target with CPU fallback for unsupportedoperators. Additionally, runtime support is added inthe form of an Ethos-N runtime module. In this initialPR, only quantized concatenate and split are supportedwith follow-up PRs adding support for many further operators.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>* Turn off USE_ETHOSN_HW by defaultChange-Id: Ie2ce4528e16e93aa83df46f8a229c0ce89b45252* Update capabilities fileChange-Id: Iebd0c62d6bc7e446662abdee4882ac874ad98aa3* Fix missing headerChange-Id: I0c89e380dd1d795755a1884c06a7b317a99fe297* Update cmake comments on ETHOSN_HWChange-Id: I2e96a1c818a82e5174fd94e483b0bdb3e4375a7d* Add checker for case when USE_ETHOSN=OFF and USE_ETHOSN_HW=ONChange-Id: Id5c9cfb866914a0298b44ead40fcbe3764ce443c* Fix 'available' booleanChange-Id: I78e54fb9f472d2815886bea4d94b7247e0d129de* Check availability in op registrationChange-Id: Iecfea7dca7301dd684199c9b32f99f2113fdfd56* Remove unnecessary lineChange-Id: Idf5cab853027adb0b0292de877e6dc02683821d7* Simplify getting output_sizeChange-Id: If4643924768c2d7ea98525e9f792b7223cc2bcdf* Remove unnecessary new lineChange-Id: Ia689c59cac28bd91e237ceecd829d8cf56d0d9c1* Remove NOLINTSChange-Id: I149b97b28b516c7d9288a0858b2fbf1497e70250* Remove unused parts of PRChange-Id: I2db5b89d8fe2c114ab92305cdcf06d0fc45f4d2a* Fix CI Ethos-N settingsChange-Id: Idd955755d6f6d1cd3843462f627d0d952729e467* Removed unnecessary line in infraChange-Id: I0ea866adf5d9166db85dd82d013a631d991ae633* Remove unnecessary len in infraChange-Id: I869e8233d41c6ab7c2dc80f47d976c974043b80c* Rename 'cpu_ops' to 'host_ops'Change-Id: I79a6ffcfd48cd055d279f493c672ec82f0c68e5c* Added explanation on mockingChange-Id: I1e88c07a47464e44cb45c6a327ec9c7e2d70cc94* IsEthosOp -> IsEthosnOpChange-Id: I4fc1b462a74f8fae231ebafac614dd8d45be0feb* Improve documentation in ethosn_api.hChange-Id: I5586a7ba7ce71da667a6a9c6dd2e591028eb43b2* No longer iterate over module when compilingChange-Id: I80e1d494c6d574be06a2375e831343485712914d* Move EthosnCompiler implementations into codegen.ccChange-Id: I5bb6e9f62722d930d9dc040ac62bf87f29dd74c5* Fix lintingChange-Id: Ia44ec741a5330ad289cc6b5cd2bb1ed784fe6afc* Refactor EthosnAPI compilation functions into EthosnCompilerChange-Id: Iee0aecbe43a84fefb437ab9ff064e3f8b42c80a4* Improve docs for Tvm2NpuChange-Id: Ia39e9e1508513ca39c1d585fbccc3ae38fcbb9fb* Move more implementation out of headersChange-Id: I1e33084ceb520b75f06b4d7a4acff5b9b2225bd5* Move implementation in ethosn_api.hChange-Id: I51ab386892a2aa84aa47d03641aac8468f5737ae* Improve docs for capabilities.hChange-Id: Iaaee508aafa1cbb7650a04ed87bd6c1b91823a58* Use else() in cmakeChange-Id: I4b64a87f32b3616ec87c9937d9fc998b8dc5d7b4* Use GetDataSizeChange-Id: I16988f3adbe6e03fc47fa0a77cb5febb7a02eaab* Use const&Change-Id: I664982d219f9a7d1f961dbfe84d12f66e2e5f5cb* Fix python lintingChange-Id: Id965ccc037fd40cbdfcb58d922cc8d5fb8c87dfe* Remove load/save to fileChange-Id: I7f8c3f5c8948c3f15551d28e3fee6e00120663ef* data->dataChange-Id: Ifb861ebbfeaaf4b154f4b1515f83a46aecf86e50* Remove specific cpu targetChange-Id: I920568cc7a81cd77d44f8604f571340a330f3e62* Test export/load moduleChange-Id: Ib605458127485e2015ac012ec515ced5900705f3* Fix cmake garbageChange-Id: I32f3c967192c7c278ef33c52cac5fb5da682cd1bCo-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>",0
[Relay] change device annotation from post DFS to recursive (#6124)* change device annotation from post DFS to recursive* add testcast for recursive device propogation,1
[AutoScheduler] Fix flaky test (#6307),0
[RELAY][DYN] Implementation of the dynamic pad operator (#6284),5
Constant input attr added to fully connected operation in TFLite frontend (#6228)* Constant input attr added to fully connected operationAn ability to handle constant input attr added to fully connected operationUnit tests amended.* renamed wrap_input to const_input* removed extra spaces,1
Changed TVMCTVMContext to TVMContext (#6306),4
[Torch] Fix dtype handling for modules with integer parameters (#6311)* return the correct type for GetAttr node* keep _get_pytorch_value_type intact* add test and handle quantized param,0
"[COREML]multiple output support, reshape, split ops added (#6296)* [COREML]multiple output support, reshape, split ops added* Review comments addressed",1
[Target] Add python binding to new JSON target construction. (#6315)* Add python binding to new JSON target construction.* Added json string parsing and new test.* Add error type.* Add error type in json decoding check.* Fix sphinx formatting.,0
[TEST][FLAKY] fix random fail (#6312)* [TEST][FLAKY] fix random fail* increase size and error check range,0
[RELAY][DYN] Dynamic upsampling relay op (#6273)* implementing upsampling op* fix lint* fix lint again* add doc to upsampling shape func* fix set attrs build problem* fixing imports* reverting data layout transform changes* moved layout template to header file* changing python module from nn.dyn to dyn.nn* adding support for more layouts to upsampling* fix lint* fix upsampling doc* change _nn.py doc* failed flakey test* fix build after merge,0
Retrigger build. (#6304),5
[TIR] Enforce buffer pointer var type to be consistent with dtype. (#6317)Now that we have type_annotation in tir::Var.We should make sure that the type annotation to be consistent with the dtypein Buffer declaration and Allocation.This change allows future passes to directly use the content type information via type_annotation.This PR turns on the enforcement on Buffer and also fixed a few cases for Allocate.A follow up PR need to fix a few more cases in the hybrid script parsingbefore everything can be made consistent.,0
[RELAY][MXNET][FRONTEND] add support for MXNET numpy operators (#6054)* [RELAY][MXNET][FRONTEND] add supports for OPs in numpy from mxnet* Update test_forward.py* Update mxnet.py* Update mxnet.py* Update test_forward.py* update and bugfix* test for multiple dtypes* Update test_forward.py* add data type and optimize coding style* replace pytest.skip with @pytest.mark.skipif* Update test_forward.py* update pytest style* Update test_forward.py* Update test_forward.py* Update test_forward.py* Update test_forward.pyCo-authored-by: Ubuntu <ubuntu@ip-172-31-39-169.ap-northeast-1.compute.internal>,0
[BUG_FIX] Fix resize test (#6298)* fix resize tests* add different scale to resize tests* fix dynamic to static resize test* fix error throwing in topi resize* fix topi and importer tests* fix lint* flakey test failed* make resize test less sensitive; had floating point rounding err on gpu* remove nearest_neighbor + half_pixel option from pytorch importer* remove nearest_neighbor + half_pixel in upsample3d,0
[Frontend][Pytorch]Add Pytorch advanced indexing (#6318)* Add Pytorch advanced indexing* Minor fix for test* Fix for cuda,0
[Relay] Make check stricter: disallow inserting function with free vars into module. (#6313)* savelintlintfix testfix test* fix,0
[RUNTIME][FFI] Fix cython FFI compact with np.int64 (#6321),0
[Ansor][AutoTVM v2.0] Phase 2: Basic GPU Sketch Search Policy (#6269)* Add PreloadMeasuredStates & Split search_policy.py* Add GPU sketch rule* Update* Bug fix for log record* Lint fix* Update tutorial* Update* UT fix* Remove tutorial* Update* Update* Update UT* Lint fix* Update* Update,0
[Relay] Support for PyTorch Non-Maximum Suppression (#6314)* [Relay] Support for PyTorch Non-Maximum Suppression* fix comment* add verify_model_vm,0
[OpFusion] Make the max number of fused ops configurable (#6327),5
[Frontend][Relay] Keras softmax and prelu fix (#6278) (#6278)* prelu and softmax with NHWC layout consideration* fix lint* fix lintCo-authored-by: Dongming Yang <dongming.yang@streamcomputing.com>,0
[BYOC][ACL] Enable remote device via environment variables (#6279)* [BYOC][ACL] Enable remote device via environment variablesImproves the ACL remote testing infrastructure by allowing a remote device to be specified via environment variables. This means external scripts can be used to enable the runtime tests. By default an RPC server will not be used and the runtime tests will be skipped.Change-Id: I8fc0b88106683ac6f1cbff44c8954726325cda21* Use json file as configuration for testsChange-Id: Iadce931d91056ed3a2d57a49f14af1ce771ae14b* Do not load the test config during class creationChange-Id: If718b5d163e399711111830f878db325db9c5f84* Add check for existence of fileChange-Id: I2568bca7f4c3ad22ee8f9d065a9486ee3114f35c,1
[FIX][VM] Fix relay vm optimize (#6322)* [FIX][VM] Fix relay vm optimize* retrigger ci,0
[Relay] Make check stricter by using Feature. Fixed multiple bugs. (#6326)* savelintlintlintfix lintlintupdatelintsavesavesavelintformatformatsavesavefixuse a form more suitable for numeric checksave* save* save* lint* save* lint* fix* fix,0
save (#6338),5
Use auto-tuner to improve conv2d_gemm performance (#6117)* Use auto-tuner to improve conv2d_gemm performanceThe following tuning entities have been introduced:- Unrolling and vectorizing input matrix transform- Reordering gemm to exploit parallel threads- Unrolling `gemm_quantized` intrinsic- Interleaving `gemm_quantized` intrinsicChange-Id: Icd3ab005663f78a80672e71ef368f6d0efa4a401* RebasingChange-Id: Id27b6de705b16b93df8e885868961fa0321497be* Fix python lintingChange-Id: I77d880424c3e7ce9de67c970ddb2cf2a92b52f79* Fusing batch into inner dimensions before parallelizingChange-Id: Ic58d1138ab96d58d12f5855f0e1044f10d9e6e9b,0
[DYN][RELAY] Resize support for NCHW-convertible layouts (#6293)* fix lint* fix typo* remove channel_axis from resize shape func* fix lint,0
"[MSVC] Make able to compile with MSVC (#6341)* fix: make suitable for msvc, clang* clang-format* refactor: use DMLC_ATTRIBUTE",0
ROCm changed name of library and removed old one in ROCm 3.7 release. (#6345),4
Add `init` member to ReduceNode (#6138)- This patch adds a new member to ReduceNode called init which allows  initialization with a custom ProducerLoad or a Float/Int immediate.- This allows initialization of the output Tensor of a reduction with  another Tensor instead of the `identity_element` defined in the  CommReducer- One example use case for this node is to initialize the Output of a  convolution reduction with the Bias values thereby saving the  Bias-add computation.,1
[TESTS] add gpuonly tests for python unittests and integration (#6346),1
[Relay/TOPI][TFLite] Implemented MATRIX_SET_DIAG Operator for Relay/TOPI and TFLite Frontend. (#6303)* Corrected docstring error.* Minor changes.* Changed MATRIX_SET_DIAG registration from broadcast to injective.,0
[Caffe Frontend] introduce caffe frontend for tvm (#6206)* [Caffe Frontend] introduce caffe frontend for tvm.* [Caffe Frontend] fix bugs for generating caption in tutorial.* [Caffe Frontend] delete statement for python2 and modify the function name.* [Caffe Frontend] change the directory which will hold the tmp fileswhen testing the caffe frondend.* [Caffe Frontend] delete tutorial about caffe frontend.* [Caffe Frontend] delete some print statementsCo-authored-by: fernchen <zifeng.cf@alibaba-inc.com>,0
"[BYOC][ACL] Improved pooling support (#6248)* [BYOC][ACL] Improved pooling supportAdds support in ACL for the following relay pooling operators and composite functions:  * nn.avg_pool2d (fp32), cast + nn.avg_pool2d(uint8) + cast => AVG pool  * nn.global_max_pool2d => Global MAX pool  * nn.global_avg_pool2d, cast + nn.global_avg_pool2d(uint8) + cast => Global AVG pool  * power(2) + nn.avg_pool2d + sqrt => L2 pooling (for fp32 only)Tests updated to reflect these changes.Change-Id: I1644b67b60ebb252344eb9695a521d2d958c724e* Address commentsChange-Id: Ibe8a61b4c42da246ce54701c89ea985b423c8f83* Fix not checking output saturationChange-Id: Ia6f3d9db31cfb8c417d8556d29961210fea418b2* Use defined set of trialsChange-Id: Ib180e3a0cbb84d6fa00c7e1994f58cb62662db15* Rebase masterChange-Id: I5c932751cd38da06d6f2b397be5d8ab7fdeb169f",0
[Relay][Training] Make AutoDiff thread through global function. (#6336)* save* lint* lint* fix warning* fix test* save,0
[BYOC][ETHOSN] Add support for quantized convolution (#6335)* [BYOC][ETHOSN] Add support for quantized convolutionThis PR adds support for quantized convolution. Thisincludes mapping it via a composite function and allthe necessary methods to convert from Relay to theAPIs in Support Library.Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>* Fix padding changeChange-Id: I0794b0ac6190478e2d1b858ad0dd90f37fc0207b* Add docs to Tvm2Npu methodsChange-Id: Iab865619b449a3d0dd6bb0dbdcb198acd529fc4e* Remove generate testsChange-Id: I51f90499f7ce82a1ce49f0731d3d50627e1d0225Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>,0
[Ansor][AutoTVM v2.0] Phase 2: Evolutionary Search (#6310)* init commit* Add rest rules* refactor* address comments* improve test* address comments,1
typo (#6352),5
"Add docker/lint.sh, for running dockerized lint scripts locally (#6333)* Add -i option to docker/bash.sh * Allows scripts to invoke dockerized commands interactively, for   better Ctrl+C.* Add docker/lint.sh to run lint step locally in the docker VM. * This allows developers to run lint using the official versions of   the lint tools without needing to lookup the docker image name. * Move all lint scripts to tests/lint/ * Point Makefile to those new scripts. * Update apache rat script to filter untracked/gitignore'd files when   run with `docker/lint.sh`.* fix bash_source[0]* explicitly set the author for CI* try environment variable override* try config option* remove =traditional from ignored option to increase git compat* address comments, fix behavior under git worktrees* address cppdocs comments* address lint.sh comments* address zhi comments, update pull_request rst",0
"quanitze operation expanded to take const argument (#6127)* quanitze operation expanded to take const argument* amendmentsused get_tensor_expr, added _test_forward_quantize_dequantize_const test",1
"Improve Rust bindings: Map, Array, String, various IR nodes (#6339)* Fix datatype* Add initialize macro* Add some TIR nodes* Better downcasting* Improve Array and add Map* Convert to new string API* Clean up some warnings* Add ConstIntBound type* Run cargo fmt* Remove debug prints* Add some more ops* Fix some string codeCo-authored-by: Jared Roesch <jroesch@octoml.ai>",0
[CMAKE] Compatible for ROCm before 3.7 (#6359),5
[Target][Codegen] Use target class in all codegens (#6347)* [Target][Codegen] Make all code generator use Target class instead of target string* Remove dep to TargetNode::str() in LLVM module* Allow  for llvm nvptx codegen* ...* Address comments from Cody* Rename UpdateTargetConfig => UpdateTargetConfigKeyValueEntry,1
"[Torch] Add cast to double, fix flatten conversion (#6357)* support cast to double and fix flatten conversion* also support batch flatten, add test* add flatten test* clean up",0
[DOCKER] Use clear name that is separate from ASF brand for cache (#6360),5
[DOC] Fix mistyped word (#6362)* Fix the doc mistyped word in `tvm.te.hybrid.build` functionCo-authored-by: gigo <gigo_liao@qbitsemi.com>,0
[Ansor][AutoTVM v2.0] Phase 2: Update heavy operations with parallel_for (#6348)* Update auto_scheduler with parallel_for* Update* Update* Update* Update inferbound,1
[DOCKER] Fix Dockerfile.demo_android (#6361)* [DOCKER] Fix Dockerfile.demo_android* fix,0
[TIR][Transform]Block scope hoisting added (#6238)* Block scope hoisting added* lowering flow added with 2 variants* Fake commit to trigger ci with pass default enabled* CI Failure resolved* Optimize for if var list iteration* More test case added* Fake commit to disable failed test cases* Pass default value restored* [1] Review comment handled* [2] Review comments handled,1
[Torch] Fix aten::max and aten::min conversion (#6372)* fix aten::max and aten::min conversion* remove print,0
[BYOC][JSON] Support input nodes with multiple entries (#6368)* Support input nodes with multiple data entries* Rename input_var_idx_ to input_var_eid_,5
[Relay] support i64 indices (#6143),5
add missing dependency (#6375),1
"[Relay] Enhance relay.split(), allow splitted dim to be dynamic (#6289)* [Relay] Enhance relay.split(), allow splitted dim to be dynamic* Add assert in shape function* Fix CI",0
acquire gil while finalizing PackedFunc (#6378),5
"[Torch] Support logsumexp, clean up unnecessary infer_shape usage (#6374)* clean up infer_shape usage, add logsumexp op* add more tests for logsumexp* remove commented code",1
[RUNTIME][CRT] use macro to replace hardcode number (#6365)Signed-off-by: windclarion <windclarion@gmail.com>,5
[RELAY] Fix the FoldConstant Regression for VTA (#6377)* [RELAY] Fix the FoldConstant Regression for VTA* [CI] Fix error guard that was missed in VTA.This PR fixes an error guard during the documentation build step.- Temporary disables VTA frontend tutorial due to  the regression of deploy_detection,0
"[TESTS] Refactor tests to run on either the GPU or CPU. (#6331)Much of the time spent in testing is duplicated work between CPU and GPUtest nodes. The main reason is that there is no way to control whichTVM devices are enabled at runtime, so tests that use LLVM will run onboth GPU and CPU nodes.This patch adds an environment variable, TVM_TEST_DEVICES, whichcontrols which TVM devices should be used by tests. Devices not inTVM_TEST_DEVICES can still be used, so tests must be careful to checkthat the desired device is enabled with `tvm.testing.device_enabled` orby enumerating all devices with `tvm.testing.enabled_devices`. Alltests have been retrofitted with these checks.This patch also provides the decorator `@tvm.testing.gpu` to mark a testas possibly using the gpu. Tests that require the gpu can use`@tvm.testing.requires_gpu`. Tests without these flags will not be runon GPU nodes.",1
Make docs build again when not everything is enabled (#6386),2
[Frontend][TensorFlow] Improve TensorFlow control flow nodes ordering (#6387)* Improve TensorFlow control flow nodes ordering* Fix Lint,0
[TFLite] Support for 'SAME' Padding option for TRANSPOSE_CONV operator of TFLite. (#6381)* [TFLite] Support for 'SAME' Padding option for TRANSPOSE_CONV operator of TFLite.* Added support for 'SAME' Padding option for TRANSPOSE_CONV operator for all  valid kernel sizes.* Added tests for 'SAME' Padding option for TRANSPOSE_CONV operator.* Minor Changes.,1
[DOCS] Fix the docker binary cache location (#6390),0
[RELAY][VM] Enable heterogeneous execution for Relay VM (#6337)* vm heterogeneous execution* context analysis on module* fix profiler* fix memory plan* add more unification* add serialization* add gpu tests for test_adt* cache visited functions* path compression* C++ context analysis* remove python context analysis* add tests* clean* lint* fix* enable gpu test for dynamic namespace* remove GetParamsContext* fix comments and add doc for context analysis* cache context* cache allocator* rebase and fix comments,0
"[Bugfix][Printer] Avoid adding annotation twice for ConstantNode (#6364)* [Relay] Add user-defined constant node printer* fix constant node printer, which appends annotator twice when meta=true* fix lint",0
[Relay] Fix Type Arguments not Attached (#6385),0
[CI][Contrib] Add Vitis-AI docker installation (#6342)* [CI][Contrib] Add Vitis-AI docker installation* rename ubuntu_install_vai_packages.sh to ubuntu_install_vitis_ai_packages_ci.sh* Add Dockerfile.demo_vitis_ai and environment scripts* Add comment to docker/bash describing Xilinx Vitis-AI specific setupCo-authored-by: anilm (generated by with_the_same_user script) <anilm@xhdabidk40.xilinx.com>Co-authored-by: Anil Martha <anil.martha@xilinx.com>Co-authored-by: Jorn Tuyls <jornt@xilinx.com>,1
[METAL] Use CFBridgeRetain for retaining the allocated resource (#6393),5
"[RELAY][DYN] Dynamic UpSampling3D Op (#6353)* frontend and start of cpp impl* upsampling3d typerel and makefunc* impl upsampling3d dynamic to static pass* passes test_dyn_upsampling3d_infer_type_const* fix bugs and improve doc for resize and upsampling* code cleanup* make tests more complex* code cleanup, fix test_dyn_upsampling3d_run* fix typo* ci not working",0
[Relay/topi] Support scalar inputs in where op (#6383)* support where with scalars* add test for where with scalar* add comment,1
Remove unintentional pytest dependency. Fix #6398 (#6399),0
Add safe up/downcasting to the Rust object system (#6384)* Revamp the rust object system with safe subtyping* Small nits,1
[VTA][Xilinx] Update to Vivado 2020.1 and Pynq 2.5 (#6402)* vivado version update* update docs,1
"[TARGET] Add layout_transform, clip and expand_dims in onnx converter (#6366)* Add layout_transform, clip and expand_dims in onnx converter* remove _add_input and address comments* address comments",1
[Fix] fix compilation error when setting USE_RELAY_DEBUG (#6380)* fix compilation error when setting USE_RELAY_DEBUG* awake github ci-test* remove unnecessary debug log,0
iadd conv2d_transpose alter layout (#6358)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,1
reshape with non constant shape argument (#6411),5
[ONNX] Add Clip importer to handle when min/max are provided as inputs. (#6251)* [ONNX] Add Clip importer to handle when min/max areprovided as inputs.* Use relay.op.minimum/maximum to handle dynamic bounds for Clip.* Update test to new testing standard,1
[TESTING] Fix the error when running tests with default targets (#6394),0
"bugfix: ""verify"" call parameter name changed (#6382)",0
"[Torch] Miscellaneous fix, enable some VM GPU tests (#6418)* fix strides conversion* enable gpu target for some vm tests* fix pooling stride None caseCo-authored-by: masa <masa@pop-os.localdomain>",0
[TFLite] Implemented MATRIX_DIAG Operator for TFLite. (#6397)* Added implementation for MATRIX_DIAG Operator.* Added tests for MATRIX_DIAG Operator.,1
Remove comparison of unsigned expression < 0 warning (#6319)* fix: remove warning for if (unsigned < 0...)* change used types related to dlpack ndim to int32_t,0
"Dynamic Strided Slice (#6316)* Dynamic Strided Slice* fix clang-format lint* remove debug print* respond to review comments* respond to yongwww's comments* fix bad rebase* revert hybrid-script assert* reformat mxnet change* use new testing api* while getting test to work with the new testing API, refactor all of the tests iin the dyn directory",0
[AutoTVM][Ansor] Enable random fill and CPU cache flush for AutoTVM and Ansor (#6391)* [AutoTVM][Ansor] Enable random fill and CPU cache flush for AutoTVM and Ansor* Trigger CI* add assert check of random fill function* remove duplicate tvm.get_global_func* Add check random_fill exists on remote devices* solve pylint,1
[PASS][ConvertLayout] Fixes AttributeError during ConvertLayout to NHWC (#6419)Fixes an issue described in #6410. In order to retrieve the shape a tensor `checked_type` should be used.Change-Id: I991d194d9cc15ee20464ff2e239fd05c035000c8,0
[Ansor][AutoTVM v2.0] Phase 2: Layout Rewrite in AutoScheduler (#6297)* enable layout rewrite for auto scheduler* refine* update* fix CI* fix CI* fix CI* resolve review comments* add ut* resolve comments* resolve comments* fix coding style,0
[RELAY][REFACTOR] Mix mode context analysis (#6403)* mix mode context analysis* add uses_gpu decorator for more tests* revert visit counter* relax visit limit* lint* bump visit limit to 19* typo,1
Switch Windows CI to build Release instead of Debug (#6427),0
[RELAY] Allow StructuralEqual/Hash via Var.vid (#6424)* [RELAY] Allow StructuralEqual/Hash via Var.vid* Reduce MSVC warning* Fix the data* Add atol of resize3d,0
Address issue #6415 using compiler-rt half-float function. (#6431),1
hot fix (#6434),0
"[Relay, Torch] Fix stack op axis check, support torch::stack conversion for a static list  (#6433)* fix torch::stack conversion, add dynamic stack test* add test to relay stack* add comment* add more comment* uncomment relay op tests* check for List ADT properly* improve assertionCo-authored-by: masa <masa@pop-os.localdomain>",0
"[METAL] set MTLBuffer purgeable state (#6376) (#6438)* [METAL] set MTLBuffer purgeable state (#6376)When using manual reference counting, MTLBufferpurgeable state should be set before releasing.* Fix lint error from tvm-ci",0
"[Rust] Improve the error reporting in build.rs files by using anyhow. (#6401)* Improve build.rs error handling.Instead of just unwrapping use Result on main function, and use anyhow to add error context.* Remove NDArray and Python changes* Format* Fix build.rs* Apply suggestions from code reviewCo-authored-by: Greg Hale <ImAlsoGreg@gmail.com>* Format* Fix build.rsCo-authored-by: Greg Hale <ImAlsoGreg@gmail.com>",0
"[Target] Tags, Composite Target, Unified Interface (#6369)* Add `set_attr_preprocessor` to TargetKind registry, which is used to pre-process attribute maps.* Use `set_attr_preprocessor` for NVPTX and ROCm backend to check and add mcpu and mtriple.* Add TargetTag registration and retrieval on C++ side and python side. Allow creation of Target using the tag name.* Unify target creation on C++ side, replace Target::Create and Target::FromConfig with the constructor.* Unify target creation on python side, deprecate tvm.target.create and encourage direct use of the constructor of tvm.target.Target instead.* Add initial support for composite target.",1
Fix broadcast shape (#6422)* Fix broadcast shape* Fix test* Minor fix,0
[Relay] Add Defunctionalization Pass  (#6400)* type args not automatically inferred...* working on type arg infer* fix type arg infer* WIP* wip* wip* revert type_infer* working* fix up test* fix* remove DeGlobal* lint* fix std move* comments* fix comments* review* style,0
"[QNN][Relay] Fixed bug in quantized conv2d. (#6420)* Fixed bug in quantized conv2d where when kernel size = (1,1)  and strides != (1,1) it would raise size mismatch error.* Added test to check qnn.conv2d with kernel size = (1,1) and  strides != (1,1).",0
[Relay][Op] Fix Reshape Compute (#6396)* Fix Reshape Compute* Fix test* Fix lint* Fix lint* Fix* Fix lint* Fix test* Rebase test,0
"CUDA: broaden path detection (#6444)Debian/Ubuntu repackaged CUDA has slightly different pathsAlso, add CUDA versions 10.1, 10.2.",1
[Relay][Topi][Op]Advanced indexing (#6388)* Add Relay adv_index op* Support single index tensor dynamic shape* Support more dynamic index* Fix lint* Minor fix for comment* Fix lint* Fix lint* Fix test* Fix,0
ROCm: use GcnArch for mcpu and ApiVersion to select code object version (#6447),5
[Format] Convert all Python code w/o CI (#6448)* Add black setup* Tweak pyproject.toml* Fix syntax issues* Fix* Tweak* Black all Python code,0
Add black to lint docker image (#6451),1
[runtime] fix: BooleanToTranspose function definition conflict (#6452),0
[TEST] Fix Some Failed Test Cases and Tutorials of The Issue #6453 (#6454),0
"[topi, x86] for 1d loop, make outer loop parallel after split (#6455)Co-authored-by: masa <masa@pop-os.localdomain>",5
[WINDOWS][MSVC] Fix MSVC warnings (#6450)* [WINDOWS][MSVC] Fix MSVC warningsThis PR fixes various warnings bought by MSVC.TODO: deprecate `__tvm_main__` symbol and updatetestcase so windows works as normal.* Fix unicode problem in data_layout,0
upstream (#6436),5
"[FIX,TESTING] Add tvm.testing to the docs (#6458)",0
[APPS] Add How to deploy graph runtime example under new module factory (#6459),1
[ONNX] Add support for GatherElements conversion (#6446)* support onnx GatherElements* remove print* run blackCo-authored-by: masa <masa@pop-os.localdomain>,1
[COMMUNITY] comaniac -> Committer (#6463),3
[COMMUNITY] lhutton1 -> Reviewer (#6461),3
[RPC] Update RPC module to enable remote linking. (#6462)Remote linking is useful when the linker is not availableon the host environment.,1
[runtime] fix: remove anoymous namespace and rename BooleanToTranspose (#6465),0
[CMAKE] Improve FindLLVM to handle llvm-prefix with space. (#6466)* [CMAKE] Improve FindLLVM to handle llvm-prefix with space.Useful for windows settings where llvm can sits in Program Files.Also updated the windows compiler to use clang.* Additional updates,0
"[ONNX] Update Slice op conversion to take strides into account, clean up tests (#6467)Co-authored-by: masa <masa@pop-os.localdomain>",1
[RFC][Formatting] Add scripts for applying Black to the Python code. (#6437),1
add aten::pixel_shuffle implementation (#6328) (#6468),1
[Formatting] Fix black script for Python formatting (#6469),0
[Relay]Some backend improvements for PT OD models (#6464)* Some backend improve for PT od models* Fix clang* Fix pylint* Enable gpu tests* Minor fix,0
[DOC] Fix Some Broken Web Links (#6475),0
µTVM RPC server and Part 1 of AutoTVM compilation infrastructure (#6334),5
add git diff filter (#6484),1
[BYOC][ETHOSN] Introduce further operator support (#6355)* [BYOC][ETHOSN] Introduce further operator supportThis PR introduces support for the following operators: - Quantized Fully Connected - Quantized Addition - Depth-to-space - Max/Avg Pool 2D - Quantized Relu (Clip) - Reshape - Quantized SigmoidCo-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>* Skip tf imports if not availableChange-Id: I11bcf4a78014fa63e7b8e3b0cb00eecfd6cb7760* ethos -> ethosnChange-Id: I1fb1a11d0765f6d69f04c24b9c24e08665b8af6a* Reduce random testing in test_additionChange-Id: Id06063a0a0cf5f01356df23dc5d4bbbcb47cfa99* Reduce random testing in test fullyconnectedChange-Id: I330408dfabc4bd804373f100581ce909ff724052* Fix dumb mistake with renameChange-Id: I2c5007be485b323116a0e8bab0f9106ea5ec834b* Added comments to update the hashes in network tests when necessaryChange-Id: I13828c918c959daa492b9ed942a882c86d6690d1* Fix github nameChange-Id: Idaa70ab9c2ec8db2828d51d15e7c23f28670ec82* Use black formattingChange-Id: I538171bd547a16395bef155a1dad28e8b3e347f2Co-authored-by: Leo Blonk <Leo.Blonk@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>,0
[Minor] Fix typos in Ansor (#6425),0
[FIX] Save tensor size with alignment (#6487),0
"[BUG][ConvertLayout] Fix qnn.conv2d layout conversion too many values to unpack (#6442)This patch follows a previous bugfix in #6419. I made a very simple oversight for qnn.conv2d in that tinfos also contains qnn parameters. Therefore, we need to extract data_info and weight_info differently.Change-Id: Ib0ad01f427543371380d0bb604a77b5e0ec1103d",0
"[Formatting] Fix python formatting issue (#6491)After #6442 was merged, black started complaining about formatting of test_pass_convert_op_layout.py. Format here.Change-Id: I04346fa06e22b722b619488b895b47c6943e3fd9",0
"[PY] GraphRuntime: Update the tutorials to the module-based interface (#6482)* [PY] GraphRuntime: Update the tutorials to the module-based interface.Also added document about the encouraged usage.In particular, we encourage the following usage.lib = relay.build(...)gmod = graph_runtime.GraphModule(lib[""default""](ctx))I have changed most of the tutorials and apps.Some follow up PRs are needed to update some of the tests code.* Fix VTA tutorials",0
Fix missing import in bifrost schedule (#6479),0
bumping vta version (#6495),5
black format master (#6494),5
[BUILD] enhance build script for optional vta dep (#6497),5
extending FindVulkan to build RPC server on Windows correctly (#6498)Co-authored-by: Mei Ye <meiandmimi@yahoo.com>Co-authored-by: Mei Ye <meiandmimi@yahoo.com>,5
[TUTORIAL][ANSOR] Using the template-free auto-scheduler on CPU (#6488)* add tutorial* add tutorial* update* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address comments* fix bugs* add the exmple for resuming the search* fix lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[Frontend][Pytorch] Improve Pytorch frontend for object detection models (#6449)* Improve Pytorch Frontend* Add tests* Fix pylint* Improve data cast* Use int64 for slice axis* Fix lint* fix roi_align(..., aligned=True)* Minor fix* Add e2e test* Add asf header* Minor change* Use dynamic topk* Improve test* Rollback topk* py format* remove print* More improve* Fix test* Improve addmm* Fix test* Fix format* Fix format* Fix test scatterCo-authored-by: q.yao <streetyao@live.com>",0
fix a typo in topi key (#6502),0
Add PT OD tutorial (#6500),1
Switch CRC-CCITT libraries (#6499),5
[TIR][Hybrid] Hybrid Script Improvement (#6507)* [TIR][Hybrid] update* [TIR][Hybrid] python formatting,1
[RPC] Lazily import micro when starting an RPC server (#6505)* [RPC] Lazily import micro when starting an RPC serverSince #6334 the RPC server cannot be started unless USE_MICRO is enabled. I've tracked this down to an import in `python/tvn/exec/rpc_server.py`: `from tvm import micro` in the top level list of imports. This will mean that we try to import micro when it's not been built. Fix this by lazily importing micro when initializing an rpc server with micro enabled.Change-Id: I8f22d81e215cfe4ac0662b0a99bdf02a3e91f90c* fix lintChange-Id: I8b78b678374bc82b3b66a7b3595ed4f1684e7d90,0
improve doc for relay.nn.dense (#6508),5
[COMMUNITY] hypercubestart -> Reviewer (#6511),3
Add several op mapping in PyTorch frontend (#6472)* Add copy_ and clamp_ in PyTorch frontend* add true_divide in PyTorch frontend* more test cases for copy_* fix format* remove copy_* fix format* skip true_divide for torch < 1.5,0
[Relay]Allow dynamic batch for arm conv2d (#6509)* Allow dynamic batch for arm conv2d* Add TODO,1
"Add beagleboard ai, thunderx and stm32mp1 to the arm_cpu target. (#6501)* Add beagleboard ai, thunderx and stm32mp1 to the arm_cpu target.Signed-off-by: Tom Gall <tom.gall@linaro.org>* updates from black on target.pySigned-off-by: Tom Gall <tom.gall@linaro.org>",1
[RELAY][OP] roi_align operator alter layout (#6443)* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layout* [RELAY][OP] roi_align operator alter layoutCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,5
"[tvmc] command line driver 'compile' (part 2/4) (#6302)* [tvmc] command line driver 'compile' (part 2/4) * Add 'compile' subcommand into tvmc (tvm.driver.tvmc) * Add frontends: Keras, ONNX, TensorFlow, tflite, PyTorch * Add tests for the 'compile' subcommand * Enable command line driver tests as part of integration tests * Skip tests if the cross-compilation toolchain is not installedCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>* tvmc: adjust TODOs* tvmc: fix linting errors* Address code-review comments* Adjust pytest fixture to not break when there is no tensorflow* Fix frontend tests, to cope with different frameworks in different images* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Fix lint and code-review issues* Re-format with black.* tvmc: Move dependencies to extras_requiresCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Dmitriy Smirnov <dmitriy.smirnov@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>Co-authored-by: Jeremy Johnson <jeremy.johnson@arm.com>Co-authored-by: Ina Dobreva <Ina.Dobreva@arm.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[ANSOR] Auto-scheduler tutorial for GPU and necessary refactor/fix (#6512)* add gpu tutorial* refactor mutation in evolutionary search* update* update double matmul* fix lint* add double matmul test* fix mutate compute location* fix sketch search policy* fix lint* update* address comments* fix PruneInvalidStates,0
Update CI badge location (#6517),1
[TOPI] Fix declaration_conv2d_transpose_impl (#6428),0
[CI] Cancel previous build if new commit has been pushed to a PR (#6518),1
"QnnBinaryLayout bugfix + unit test (#6513)An attempt to fix an issue which appeared when ConverLayout passruns on quantized binary operations like qnn.add:def before():    x = relay.var(""x"", shape=(2, 2))    y = relay.var(""y"", shape=(1, 2))    return relay.Function(        [x, y],        relay.qnn.op.add(            x,            y,            lhs_scale=relay.const(0.0156863, ""float32""),            lhs_zero_point=relay.const(127, ""int32""),            rhs_scale=relay.const(0.0117647, ""float32""),            rhs_zero_point=relay.const(85, ""int32""),            output_scale=relay.const(0.0235294, ""float32""),            output_zero_point=relay.const(128, ""int32""),        ),    )The issue manifested itself as  [bt] (2) ./src/tvm/build/libtvm.so(tvm::relay::qnn::QnnBinaryBroadcastLayout(tvm::Attrs const&, tvm::runtime::Array<tvm::tir::Layout, void> const&, tvm::runtime::Array<tvm::tir::Layout, void> const&, tvm::runtime::Array<tvm::Type, void> const&)+0xa9) [0x7fadc080d949]  [bt] (1) ./src/tvm/build/libtvm.so(tvm::runtime::Array<tvm::tir::Layout, void>::operator[](long) const+0xb6) [0x7fadc0382996]  [bt] (0) ./src/tvm/build/libtvm.so(+0xb50c12) [0x7fadc037cc12]  File ""/workspace/include/tvm/runtime/container.h"", line 681IndexError: Check failed: 0 <= i && i < p->size_: indexing 1 on an array of size 1",0
[LINT] Use fmt off to disable problematic black fmt (#6519),5
[AutoScheduler] Improve hyperlinks in the tutorial (#6521)* improve auto-scheduler tutorials* improve tutorials* fix lint,0
Enable more warnings when compiling with clang 10.0 or greater (#6456),5
[TOPI] Group conv2d NHWC op implementation (#6510),5
[Torch] Clean up usage of try ... infer_value() ... except (#6504)* clean up infer value usage* try silence pylint* remove unused variable* make on_failuare optional* make on_success optional TrueCo-authored-by: masa <masa@pop-os.localdomain>,4
"fix libtvm build dependencies when USE_MICRO is ON. (#6524)* previously, building from scratch would fail with Unix Makefiles   due to cmake limitation",0
[DOCS] Change some tutorial text (#6514),2
Remove settings about SGX in config.cmake (#6530)removed settings about SGX since SGX is removed from TVM core,4
[Doc][Fix] Fix a typo in hybrid script tutorial. (#6525),0
Add the SYSTEM keyword to all cmake include_directories commands for 3rd party or external headers. Warning flags should only be applied to code within the tvm repository. (#6531),1
[TUTORIAL] Fix Some Failed Tutorials of The Issue #6453 (#6534),0
[FIX] fix the python script for building resnet (#6526) (#6527),0
[Ansor] Parallel the InitPopulation (#6529),5
[BYOC][ACL] Add maximum support for float32 (#6506)* ACL integration: add maximum support for float32.* Added the code generation flow in arm_compute_lib.py* Added the runtime calls in acl_runtime.ccChange-Id: I69c5522f05a46c1dd235da5d57fe499134de0425* Add maximum to the list of supported functionsChange-Id: Ia49087756be4c3ac92a3dc76fe03fb00de468f8d,1
"[Relay] Show yolo detection result in text. (#6367)* [Relay] Show yolo detection result in text.Issue:Current yolo detection only provide a image drawing solutionto output detection result, but for console user, such resultwould not available for a view.Solution:Here we add a text show function to show detection result intext format for testing in console scenario.* rebase upstream and merge code change.* address review comments.",1
Remove unused pylint 1.9.4 from docker installation script (#6538),2
"tvmc: solve a linting error on onnx command line driver frontend (#6536)* Updates onnx load function from ""load"" (a compatibility attribute)   to ""load_model"" (the actual function) * Add a pylint command, that we don't see currently on upstream CI,   but it reproduces when linting it locally.",0
[Relay/TOPI] Added 'offsets' and 'alignment' attributes to MATRIX_SET_DIAG. (#6429)* [Relay/TOPI] Added 'offsets' and 'alignment' attributes to MATRIX_SET_DIAG.* Added support for 'offsets' and 'alignment' attributes of MATRIX_SET_DIAG.  (Similar to MATRIX_SET_DIAG V3 of TF)* Added tests for 'offsets' and 'alignment' attributes of MATRIX_SET_DIAG.* Changes by black.* * Added offset check in Relay.* Minor changes.* Added more tests and some minor documentation changes.,1
Add alternate cublaslt library name. CUDA 11.0 uses cublasLt. (#6541),1
[Rust] Allow convert Context to ArgValue (#6544),5
update webgpu api (#6547),1
[Frontend][Onnx] Added broadcasting to prelu alpha. (#6549),1
"[Relay/TOPI] Added dilation_value attribute to dilate operator. (#6550)* Added dilation_value attribute to dilate operator of Relay/TOPI.  (Enables custom value for dilation, instead of always 0)* Added tests for dilation_value of dilate operator in Relay and TOPI.",1
Generalize the use of booleans to support all cmake boolean values. (#6515)* Generalize the use of booleans to support all cmake boolean valuues.* Update to use a more simple method to detect if variable is a false value* Fix some errors* Debug CI issue* Fix logic error in cmake changes* Use new IS_TRUE_PATTERN to make intent clear,0
[COMMUNITY] Add Ziheng's key for ASF release (#6552),1
Feat(frontend-pytorch): Add input types argument and Support cast to float16. (#6546)1. Add input types argument for converting TorchScript file.2. Support casting float32 to float16 when converting to operation.,1
[TIR] Fix rewrite_simplify tir::builtin::shift_left (#6555),0
Make missing desired layout non-fatal (#6553),5
[Ansor][FLAKY] Bug fix for compute at mutation error (#6557),0
Add proper cmake PATHS when multiple NAMES. (#6558),1
Zhi's key for ASF release (#6554),5
Make CMakefile/config.cmake/install_tvm consistent (#6562),2
Rename tvm.hybrid.script to tvm.script. (#6522),5
"Bring Your Own Datatypes (#5812)* Add ChangeDatatype pass and unittest* [WIP] Jared's work on FriThis was work that Jared did on my computer, trying to get Inception v3 running.* Fix simplify inference to work over different data types.* Formatting* Copy setup code from other test file* Logging in Relay* Remove duplicate TVM_DLL* Add Sub, Mul, Div, Max to bfloat lib* Fix previous broken rebased commit* Remove line* Add LowerCustomDatatypes to build passes* Upcast ints to custom datatypes too, as well as to floats* Add and use convert_ndarray* Lower Call* Relay: create constant scalars of custom dtypesWe use the same method we use in TVM: store the value in a double.* Custom datatype formatting in Relay* Update unittests* Add simpler example that's not working yet* Add Python unittests to Makefile* Fix bug* Fix function name in GetPackedFunc call* convert_ndarray makes its own executor* Add simple test case* Move setup() calls* Use convert_ndarray* Change import to make it more specific* Fix another Registry::Get call* Allow users to register minimum functions for custom datatypesThis commit allows users to register global functions named`tvm.datatype.min.<type name>` which take the number of bits in the custom typeand return the corresponding minimum value (as a double).A similar commit will need to be created for max, whenever that ends up beingneeded!* Remove check for float* Add test* Fix inception test* Add MobileNet* Lower custom datatypes before intrinsics* Add exp and sqrt bfloat functions* [buggy commit] Lower intrinsics like sqrt, expThis commit has bugs in it, I'm fairly certain.* Formatting* Fix bug* Add lowering for new ops in test* Add int to bfloat* Remove print* Add all tests* Correct image size* Add TODO* Add ""notbfloat"" typeThis type is for testing purposes. It just stores a float in a uint32. It wasused to confirm the fact that my bfloat ""implementation"" is very numericallyunstable and was causing issues when running the model.* Convert argumentsNot sure how necessary this actually is.* Rewrite custom datatype constants in Relay* Add test_ops* Print constants in Relay* Use topi.testing* Test conv2d* Add test_model* Comment out model tests* Register notbfloatThis could be unregistered at some point later* Add commented codeRemove later* Add posit tests* test_ops_same_function* [temporary] move incomplete commit to macbook* Add more to tests* Formatting* Uncomment add* Remove bad tests* Change comments* Change function name and docstring* Change main function* Restructure tests* Fix visibility of posit functions* YAPF* Switching keywords around to resolve build errors on some systems* Improve test by running smaller mobilenet* Add test_cast* Change datatype name; add simple test* Rename to posit32* Merge 3 posit types into one file* Add a nop type* Remove bfloat* Refactor test comments* Refactor conv2d test* Add optional tolerance arguments* Add posit8 and posit16* Add comment about posit8* Whoops -- actually add noptype to CMakeLists* Add rtol, atol to run_workload* Add noptype to tests* Run noptype over other models, too* Pass correct arguments to calls* Fix line length errors* Raise tolerances (again) to avoid flaky test* fix style* add test for tanh, log, sigmoid* Remove references to bfloat, notbfloat* Change comments* Remove old test file* fix min func* refactoring unit test file* use posits es2* cleanup* comment* coment if_then_else* support different bit widths* use random seed to create stable tests* update documentation* removed nop-type and code consistency* add batchnorm test* rebase and update* fix tests and format* pylint* change order of include* include order* fix style* remove posit c linkage* update universal* fix style* fix test* fix overflow error with minfunc and posits* style* use change_dtype to convert params* update universal* fix fatal error* fix constant repr* minor update to posites2* update universal* fix rst* fix invalid import and sqrt* update universal* comments* comments and expand testing* increase atol/rtol for custom[posites2]32* Re-add newline* Remove comment* Remove opt level and comment* Change docstring* Add TODO* Add file header and newline* Update docstring* Update file docstring* Update docstrings* Delete todos* create_min_lower_func* add better debugging message* docs* add BYODT tutorial* add todo* Reformat some of tutorial to RST, plus code fixes* tutorial notebook runs now* fix hyperlink* rebase* add to tutorial* fix mobilenet model* add skip tag* black lint* add compiler flag and add dummy float* myfloat and posites2 test* remove universal* lint* lint* add setup* build with USE_POSIT for CI/CD* fix posit cmake* add cd /* undo docker changes* change tutorial to use myfloat* move files* lint* fix* remove filter* fix lint* fix suggestionsCo-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Andrew Liu <andrewlliu@gmail.com>",0
[APP] Fix misprint in demo.cc during initializing of picture tensor data (#6566),0
[BYODT] fix CMAKE flag name + update documentation (#6567)* documentation fixes + better name* fix up comments,0
[AutoScheduler] Improve the rule of mutating parallel granularity (#6568)* fix mutate parallel* fix comments* fix lint* fix tutorials* update* fix tests* address comments,0
[TEST] Temporary disable test_mutate_parallel (#6572)* [TEST] Temporary disable test_mutate_parallel* Use skip,3
Fix missing te in the code example (#6569)- caused `AttributeError: module ‘tvm’ has no attribute ‘thread_axis’  error- Fix baesd on https://discuss.tvm.apache.org/t/attributeerror-module-tvm-has-no-attribute-thread-axis/6606,0
Fix android runtime error (#6575)codes for USE_RANDOM in tvm4j was missing.,0
"[tvmc] Fix command line argument variable name (#6574)* This is a variable that we changed names during code review,   and it was incorrectly left behind with the previous name.",0
Support mxnet dot and LogisticRegressionOutput (#6542),2
[Doc] Update release document (#6573),1
[tvmc] unify all logs on a single logger 'TVMC' (#6577),2
[COMMUNITY] vegaluisjose -> Committer (#6582),3
Add rocblas_sgemm_strided_batched impl. (#6579),1
disable stacked bidir test (#6585)Co-authored-by: masa <masa@pop-os.localdomain>,3
"[tvmc] Introduce 'tune' subcommand (part 3/4) (#6537)* tvmc: introduce 'tune' subcommand (part 3/4) * introduces a subcommand to drive auto-tuningCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>* [tvmc] address code review comments* adjust --min-repeat-ms default value logic* re-arrange rpc arguments to be --rpc-tracker=hostname:port and --rpc-key=str* use a local reference of the tvmc logger* add --target-host, default to llvmCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Luke Hutton <luke.hutton@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>",1
"[RUNTIME] NDArray CopyFrom/To Bytes always synchronize (#6586)* [RUNTIME] NDArray CopyFrom/To Bytes always synchronizeThe previous behavior of non-sync can be unsafe for GPU devices.In particular, the need for an explicit synchronization couldleads to confusion behavior e.g. asnumpy does not immediately returnthe right content for vulkan.Also brings the requirement of array being contiguous.Right now we encourage compact array since they are easier for optimization.We can consider bring support later by introducing a compactify PackedFunc(which might need be jitted).",5
[BYOC][ETHOSN] Fix tests for new module API (#6560)* [BYOC][ETHOSN] Fix tests for new module APISome of the downstream variants of our tests hadbeen broken by a recent change to the API of build.This both fixes that and refactors a couple of testsso that they will run entirely in upstream CI andwe won't see this sort of failure again.Change-Id: I841266eef0e2e89cc76e0526fc6cd3fc8d1326d8* Only run mobilenetChange-Id: Ie41c6d2c13c4473ecaa5c50c33d2c1589c742796* Improve docsChange-Id: I2c8bde44278e4cbc9cea5c5cbd4bb3c316ec37ae* More docsChange-Id: Ia9973915eecea647689535cc1e6eef9228111324,0
properly pass through command-line args in docker/bash.sh (#6599),4
dynamic conv2d for cuda (#6598),5
Allow datatypes besides fp32 in conv2d_transpose for cuda. (#6593),5
"add black-format to docker/lint.sh, suppport in-place format (#6601)",1
[Parser] Fix parsing op string attributes (#6605)Co-authored-by: Lei Liu <lei.liu@streamcomputing.com>,0
[docs] Missing documentation 'autodocsumm' (#6595),2
[Doc] add KEYS to downloads.apache.org (#6581),1
Add ci_qemu docker image (#6485)* Add ci_qemu docker image* build qemu from source* add ubuntu1804 install script* receive keys from keyserver* pin to zephyr 2.3.0* install 2.4.0* add cache directory* revert 3rdparty/vta-hw,1
[tvmc] Introduce 'run' subcommand (part 4/4) (#6578)* [tvmc] Introduce 'run' subcommand (part 4/4) * Add 'tvmc run' subcommand to execute compiled modules * Include options to locally or remotelly using RPC * Include support to cpu and gpu devicesCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* adjust based on code review comments* make test fixture to safely skip environments without tflite* make --help option more clear* improve error message to show expected inputs* code-review adjusts* update doc-string to default zeros->randomCo-authored-by: Marcus Shawcroft <marcus.shawcroft@arm.com>Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>,0
"[tvmc][docs] Getting started tutorial for TVMC (#6597)* [tvmc][docs] Getting started tutorial for TVMC * Include a tutorial, demonstrating basic capabilities of   TVMC, by executing a full pipeline (tune, compile, run)   on a ResNet-50 model.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* apply suggestions from code reviewCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>* adjust text according to code-review* improve reading flow into tuning sectionCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",2
[Bugfix] Simplify reduce expression in te.gradient (#6611),0
[RELEASE] Bump version to 0.7.0 (#6614),5
[BUG_FIX] Fixes #6608: CHECK(data != nullptr) causes type checking to fail (#6610),0
Updated runtime to run under FreeBSD. (#6600)* Updated runtime to run under FreeBSD.setenv CXX to proper binary - c++ or g++9 for FreeBSD 12.0.* Update python/tvm/runtime/module.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Update python/tvm/rpc/server.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Changed to use os.environ.get* Fixed format.* Yet another lint fix.Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[Ansor] Support multiple output ops and fix Python API printing (#6584),0
[RELEASE] Update NEWS.md for v0.7 (#6613),1
[ETHOSN] Update to 20.08 version of the ethosn-driver. (#6606)- Updated ethosn relay backend to account for 20.08 api changes around cascading and quantization.   - Note: 20.05 compatibility is maintained for now to avoid compilation and test failures while the docker image still uses 20.05. The version switch can be removed along with associated compatibility measures when no longer necessary.,1
[VERSION] Version for v0.8 cycle (#6615),5
"Dynamic ONNX Importer (#6351)* Change onnx importer to use dynamic upsampling3d (#3)fix pylint* Refactor ONNX frontend to be dynamicMake OneHot dynamicSupport BatchMatMul with dynamically shaped inputsfix dynamic broadcastAdd null checks to broadcast_to rel functionsfail more isolated broadcast_to testuse StructuralEqual instead of pointer comparisions in dynamic_to_static passadd an optional weight freeze argument to onnx importerconvert onnx resize to dynamic opadd dynamic expand to onnx importeradd a shape_func for powerfix BERTSquad, linthandle onnx graph initializer parameters more intelligently* Dynamic ONNX importer: Upsampling and Pad (#2)fix lintfix Call referencefix a type issue with expandfix a bad test refactorrespond to review comments, fix batch matmul tests* black format* fix batch matmul test* add dynamic strided slice to the onnx importer* fix clip importer* fix qnn tutorial* fix bad merge, respond to review comments* add a simple dynamic model test* Add dynamic-shaped autopadding to convolution and pooling ops* fix dynamic issues in a few ops* fix pylint* disable tests onnxrt doesn't support* fix pytorch test* respond to review comments* add documentation about partially supporting dynamic shapesCo-authored-by: Lily Orth-Smith <lorthsmith@octoml.ai>",0
Fix Strided Slice Infer Layout (#6621),0
"[FIX,AUTOTVM] Print warning when all autotvm tasks fail with errors (#6612)* [FIX,AUTOTVM] Print warning when all autotvm tasks fail with errors.* formatting* write errors to tempfile* wording* wording* don't duplicate errors* Ensure we have a string for an error",0
Fix a bug with Alter Op Layout (#6626)* Regression test for a Scalar type issue in Alter Op Layout* fix the regression test by avoiding the Scalar optimization if types aren't defined,0
bump dockerfile (#6632),2
"[Rust] Improve NDArray, GraphRt, and Relay bindings (#6563)* WIPWIP* Add support for loading Python packed functions* Flesh out Relay AST in Rust* More tweeks for getting functions out* Deploy Rust docs as part of build* Add some more types* Introduce NDArray 2.0* Work on NDArray 2.0 before restoring tests* Formatting and code fixes to get it to compile* Add more Rust bindings- Converts Conv2d attrs to use tvm::String, so that we can add Rust binding- Uses Type for checked_type in Rust bindings- Fix type key in Rust bindings- Make data field contain NDArray in Rust bindings* Clean up object ptr passing.* WIP* Add debugging for NDArray and fix all test cases* Add breaking test* Dispatch some todos* Format* Fix ndarray size and len* Add BiasAddAttrs rust bindings* Add DenseAttrs rust bindings* Change to TVM string* Add more Rust bindingsAdd GlobalPool2DAttrs Rust bindingAdd ExpandDimsAttrs Rust bindingsAdd MaxPool2DAttrs rust bindings* Fix some test attributes* Improve the NDArray api* Fix some more ndarray stuff* Get the resnet demo kinda working* Add SoftmaxAttrs Rust bindings* Implement Hash and Eq for Relay Exprs* Add underscore to unused function* Fix broken ass resnet script* Improve some ndarray conversions* Make sure the build script runs correctly* Clean up ResNet example tremedouslyExpose C++ graph runtime via cleaner Rust API rewrite example.* Add ASF header* Format* Format* Format resnet rust python script* Add type files and refactor span* Format* Format* Change types from std::string to tvm::String in packed function* Add ASF header* Fix test w/ ndarray's API change* Fix array test* Fix anyhow import* Put back some anyhow stuff* Clean up* Try and fix tests/scripts/task_rust.sh* Disable ResNet for now* Turn off building of Rust docs until we update CI* Actually disableCo-authored-by: Jared Roesch <jroesch@octoml.ai>Co-authored-by: Gus Smith <guscomps@gmail.com>",0
Fix example code (#6627),0
[TVMC] fail gracefully in case no subcommand is provided (#6625),5
"Add dot product support for quantized convolution. (#6445)* Add dot product support for quantized convolution.We added two new intrinsics in: topi/arm_cpu/tensor_intrin.py, namely- mmla4x4: compute a matrix multiplication between tile A(4,4) and tile  B(4,4)- mmla16x4: compute a matrix multiplication between tile A(rows,4) and tile  B(4,16)Then we used those intrinsics in two separate strategies. We added thestrategies in topi/arm_cpu/conv2d_int8.py and implemented the schedulesin topi/arm_cpu/conv2d_gemm.py. In particular:- schedule_conv2d_gemm, when accelerated, packs matrix A, compute GEMM,  and unpack the resulting matrix. This uses the mmla4x4 intrinsic- schedule_conv2d_gemm_hybrid doesn't do any packing on A and C which  are in native form.  This uses the mmla16x4 intrinsicPlease note that for the limitations of `tensorize` we need to padmatrix A in both cases (when dimensions are not multiple of the tilingshape)Change-Id: Id0d818d84ffc458c6dad7983fd350a0f3d5db395* Add back nhwc_spatial_pack strategy as defaultChange-Id: I8b1826a7ae1d742956296e8d157da19955a4942c* Fix linting through BlackChange-Id: Ic74ef5461a90bca9f4d4980a214137e384d5f923* Fix python lintingChange-Id: I5fb8a2ae4467a87bd3470f6b3753c074f9b7cc78* Addressing review commentsChange-Id: I284b1f2c121051e672f548d6c6ee2a3267854e31* Fix black linting issuesChange-Id: I1813b0226b536aedee0dce9eeeba27aa2d95518b* Fixing failing test and adding tests for dot-product compilationChange-Id: Ic040722abd5538fccb85af4de922394c939e7000* Fixing linting and review commentsChange-Id: If09e3baa514c85dc78d3c27c2ac2fa2e01773d89* Fixing black linting and address commentsChange-Id: I857b28b6f9b23307d8c1eebc509de6ad2783c756* Address review commentsChange-Id: I63d1a639d4a72abeb33148fd2868cd356ef84122",0
[Topi] Allow batch_matmul to broadcast along batch dimension. (#6616)* Allow batch_matmul to broadcast along batch dimension.* Added typerel checking.* Fix style issue and respond to feedback.* Fix style.* More formatting issues :(* Fix issues after merge.* Comment update.* Small tweak.,0
[COMMUNITY] areusch -> Reviewer (#6637),3
[CI] fix Python dependency required by cpp tests to work standalone (#6639),0
[apps/bundle_deploy] Link demo_* targets with LDFLAGS and also with -lm. (#6636),5
Add qemu build step to CI (#6644),1
add a test for assymetric padding in ONNX conv and fix importer (#6646),0
missing header for GraphRuntimeFactory in android_rpc (#6648),5
"Fix leakyReLU support for CoreML (#6651)The original implementation failed with the following error:File ""../include/tvm/runtime/packed_func.h"", line 372TVMError: Check failed: type_code_ == kDLFloat (8 vs. 2) : expected float but get Object",0
[CI] make sure graphviz is on both ci-cpu and ci-gpu images (#6645),5
"Add Range op to ONNX, make tvm arange op support negative steps (#6647)",1
[µTVM] Avoid use of builtin math functions (#6630),5
"[FIX,AUTOTVM] More descriptive error message when an autotvm task is not (#6652)found.",0
[TEST][TEDD] improve TEDD tests to also run on CPU Docker image. (#6643)* Amend regular expressions to match with what is being reported   by CPU Docker image Graphviz * Fix typo on dependency checking function * Organise imports,0
"[Diagnostics][Relay][InferType] Refactor InferType to work on whole module, and use new diagnostics. (#6274)* Refactor the type checker to use diagnosticsAlthough this patch is very large and seemingly disjoint thefixes are required to get it working for the entire stack.I started with first changing InferType to use the diagnostics,these weren't yet in the pass manager so this required changesto module and module pass. InferType wasn't actually writtencorrectly as a pass requring refactoring there, then in orderto add spans to AST it required turning on AnnotateSpans whichin term required changes to the parser, and module to makeit possible to use the errors. These changes to parse and modulerequired changes to diagnostics and InferType. Althought seeminglydisconnected there are hidden cycles between the components whichrequire simultaneous change in order to remove the old errorreporting.A huge change due to this patch is that the module no longerimplicitly type checks functions which are added.* Apply suggestions from code reviewCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* Apply suggestions from code reviewCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Clean up parser* CR feedback* Apply Bobs suggestions* Fix up Python interface for diagnostics* Fix test_ir_parser and formatting* Fix cpplint* Fix lint* Fix format* More lint* Fix format* Kill dead doc comment* Fix documentation comment* Rebase fixups* Add docs for type.h* Fix parser.cc* Fix unittests* Fix black* Skip previously typechecked functions* fix ACL* Fix numerous issues* Add repr method* Fix issue with Pytest, I am ready to cry* Fix the rest of tests* Kill dead code* Fix dignostic tests* Fix more tests* fix more tests (#11)* Fix diagnostic.py deinit bug* Fix deinit issue* Format* Tweak disabling of override* Format* Fix BYOC* Fix TensorArray stuff* Fix PyTorch* Format* FormatCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Zhi <5145158+zhiics@users.noreply.github.com>",0
"Faster sparse_dense on GPUs (#6580)* Faster sparse_dense on GPUs.This new sparse_dense requires a padded matrix, so a new op`sparse_dense_padded` has been added. AlterOpLayout should transform`sparse_dense` to `sparse_dense_padded` when possible on the gpu.* formatting* more formatting* Check that alteroplayout is definedbefore using it* check if FTVMAlterOpLayout exists before using it* formatting* restore message passing* Fix sparse_dense and sparse_dense_padded docs* Fix old sparse_dense, autotvm and sparse_dense dont play well together* Remove unused imports* clarify warp count in cuda_transpose* Document multidimensional access* Warn users not to use sparse_dense_padded* rename nn.sparse_dense_padded to nn.internal.sparse_dense_padded",0
"Revert ""[Relay] Keep fixed dim when unifying dynamic shape (#5795)"" (#6658)* Revert ""[Relay] Keep fixed dim when unifying dynamic shape (#5795)""This reverts commit 782190e88b1941fdbe31101af260bee06b81bf72.* run infer type on test_sparse_dense_padded_alter_op() to fix CICo-authored-by: masa <masa@pop-os.localdomain>",0
[AutoScheduler] Improve the GPU tutorial by deleting measure_ctx earlier (#6660)* del measurement process in the tutorial* fix* trigger CI,0
[AutoScheduler] Improve test cases (#6657)* Improve test cases* update* fix lint* fix lint* trigger CI* address comments* trigger CI,0
[Frontend][Tensorflow] Fix TF 1.15 conv2d_transpose parsing (#6589)* Fix conv2d_transpose parsing in Tensorflow frontend for TF 1.15* Add comments and convolution tests without AddShapesToGraphDef,0
"[BYOC][ACL] Support add operation (#6532)* [BYOC][ACL] Support add operationAdded support for an ""add"" operation implemented via ACLfor fp32 and quantized uint8 data types* Addressed lhutton1 comments* linter",1
Fix typographical error. (#6664),0
[Relay][MXNet] Support broadcast_like (#6561),5
[CI] Move to use main as the default (#6665),4
[Torch] Object detection support update for PyTorch 1.6 (#6659)* update split* fix* cast nms output to int64* add more comment and numel test* fix lint* also supported the latest master (1.7)Co-authored-by: masa <masa@pop-os.localdomain>,0
don't validate AttrInitEntry until a value has attempted to be set (#6672),5
[CI] Set main as default in github actions (#6669),5
[BYOC] Support control flow in annotate_target (#6641)* Change annotate target* Annotate_target* Revert namespace changes* Add tests for if-else node* Add while_let testcase* No merging in ifelse* Remove scope builder* Add ops* Replace < with less* Linter* Pass Tests* Change back to static const* Cpplinter* address PR comments'* PR Comments* Clang-format check* PR Comments* PR Comments* Change back to Insert Ann in AnnotateARgsCo-authored-by: Ritwik Das <dasritwi@3c22fb14d7c6.ant.amazon.com>Co-authored-by: Ubuntu <ubuntu@ip-172-31-3-223.us-west-2.compute.internal>,1
TF argmax - handling int64 datatype (#6674)Co-authored-by: Ubuntu <ubuntu@ip-172-31-0-202.us-west-2.compute.internal>,5
[CODEGEN][COREML] Call InferType explicitly in coreml test (#6676),3
"Adjust Vulkan queue selection and creation logic (#6662)* Adjust Vulkan queue selection and creation logic- The queue selection logic rewrite addresses a bug in the oldimplementation where the code would pass an invalid queue index whenselecting any queues other than number 0.- The new implementation will attempt to use compute-only queues whichare common on AMD GPUs. It's not clear how much difference will thismake but hopefully it would lead to better scheduling.The queue changes were made as with the old configuration autotvm caused mysystem to hang, stutter or otherwise become unstable and crash. With the changeI'm able to run autotvm tuning inside a desktop environment.* Return multiple queue family indexes from GetComputeQueues* Only create one queue",0
Install xgboost>=1.1.0 in CI container (#6679),2
Fix format error in integrate.rst (#6677),0
Revert #5238 (#6680),5
[AutoScheduler] Fix a bug in thread binding (#6683)* fix for lstm use case* update,0
"[ARITH] Introduce iterator (quasi)affine map detection. (#6667)* [ARITH] Introduce iterator (quasi)affine map detection.The loop transformations (split, fuse) create bijectivemaps from a collection of source iterators to target iterators.DetectIterMap is a function that detects such bijective mappingsfrom the lowered index expression.We choose the term quasi affine to be consistent with theterminology used by in polyhedral compilation.DetectIterMap can handle symbolic integers(in split/fuse) to some extent.The utility can be useful in detecting loop transformationpatterns and data layout change patterns in TIR.* Update per feedback",1
[REFACTOR] util => utils for consistency in the project. (#6684)* [REFACTOR] util => utils for consistency in the project.* Update CMake,1
[Relay][Frontend][Onnx] Allow A to B broadcasting of batch_matmul and reverse strided slice (#6681)* slice and batch_matmul fixes.* Bug fix in shape inference.* Test backwards strided slice.* Fix batch_matmul dynamic shape function.* formatting.* Fix edge case for implicit broadcast,0
"Add µTVM Zephyr support + QEMU regression test (#6603)* Split transport classes into transport package.* Introduce transport timeouts.* black format* Add metadata-only artifacts* Simplify utvm rpc server API and ease handling of short packets.* add zephyr test against qemu* Add qemu build config* fix typo* cleanup zephyr main* fix nonblocking piping on some linux kernels* don't double-open transport* validate FD are in non-blocking mode* gitignore test debug files* cleanup zephyr compiler* re-comment serial until added* remove logging* add zephyr exclusions to check_file_type* add asf header* lint* black format* more pylint* kill utvm rpc_server bindings, which don't work anymore and fail pylint* fix compiler warning* fixes related to pylint* clang-format again* more black format* add qemu regression* Fix paths for qemu/ dir* fix typo* fix SETFL logic* export SessionTerminatedError and update except after moving* fix test_micro_artifact* retrigger staging CI* fix jenkins syntax hopefully* one last syntax error* Add ci_qemu to Jenkinsfile* build in qemu* address liangfu comments* fix new bug with list passing* retrigger CI",0
int32 pooling with int64 shapes (#6687)* Failing tests for Int32 avg_pooling with Int64 shapes* fix pooling implementations,0
"[Docker] Update CI CPU and GPU images based on new Docker build files. (#6690)* Turn on Rust docs and MxNet based ResNet* Add deps needed for Rust examples and docs* Setup Rust path* Bump Jenkinsfile* Fix broken version setting, which instead redirects stdout and stderr* Update Jenkinsfile* Format* Disable Rust change for now* Completely disable ResNet* Reset test changes* Remove temporary labels* Remove patch needed for docs",0
"[FIX,MICROTVM] Skip microtvm tests if microtvm is not built (#6693)",0
[TFLite] Fix detection of crop in convert_batch_to_space_nd (#6670),0
Fix tutorial broken by Docker build (#6694),0
"[Torch, Quantization] Necessary workaround to prepare for 1.6 update (#6602)* add support for 1.6 quantized models* fix lint* move version check function to a common utils* fix lintCo-authored-by: masa <masa@pop-os.localdomain>",0
[Relay] Change some passes to mix mode (#6695),4
[LLVM][WINDOWS] Recover windows support for the latest LLVM (#6698)Windows COFF requires comdat information to support weak-like linkage(via any).This patch fixes the windows LLVM support after LLVM-8.,0
Resolve more warnings in msvc (#6702),5
Add cloudpickle dependency to docker images (#6701),1
Refactor diagnostic to avoid circular dependencies (#6692),5
[TEST] Address flaky error in test_any (#6705),0
[Frontend][Relay] Fix MXNet frontend to support NLP backbones in GluonNLP (#6699)* updateUpdate type_relations.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate transform.ccUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pyupdateUpdate mxnet.pydebugUpdate generic.pyUpdate topi_integration.pyfix bugupdateUpdate test_forward.pyUpdate test_forward.pyfix test caseUpdate mxnet.pyupdateUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.pyUpdate mxnet.pyUpdate mxnet.pydebugUpdate mxnet.pyUpdate mxnet.pyUpdate test_forward.pyUpdate mxnet.py* address comments* Update mxnet.py* Update mxnet.py* fix* improve where test* Update test_forward.py* Update test_forward.py* Update test_forward.py* update* Update mxnet.py* Update mxnet.py* Update mxnet.pydebugUpdate common.pyupdateUpdate mxnet.pyupdateUpdate test_forward.pyUpdate test_forward.py* update* fix lint* Update mxnet.py* Update test_op_level1.py* fix lint,0
[PYTHON][WINDOWS] More robust dll loading behavior after python3.8 (#6707)The dll search directories need to be manually addedby os.add_dll_directory after python3.8.,1
[AutoScheduler] Add task scheduler (#6663)* Add task scheduler* fix lint* fix tests* fix tests* fix tests* fix test cases* fix test cases* fix tests* address comments,0
[AutoSchedule] Support multiple cache read and fix bugs (#6686)* Add shape to DAG print* avoid useless cross-thread reduction* Fix stage order* support multiple cache_read* lint* fix* fix* address comment* fix ci* Trigger CI & Update doc stringsCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,0
[CI] Update docker to latest (#6708),1
Fix the Type bug in ConvertSSA. (#6709)Co-authored-by: YushengMa <yusheng.ma@streamcomputing.com>,0
"[BYOC][TensorRT] TensorRT BYOC integration (#6395)* TensorRT integration using JSONRuntimeSupport input nodes with multiple data entriesFix failing testsSupport layout transform, add engine cachingAdd commentAdd PruneSubgraph passUse prune_subgraph pass, make params member of trt runtime classHide deprecation warnings coming from TRT headersRemove general prune subgraphSave/load use_implicit_batch and workspace sizeClean upFix cpp lintAddressing review commentsRefactor testsUse relay.bind instead of VarReplacer. Improve some annotation functionsAdd TRT docsUse DLOG, formattingUse logging.info instead of printalso  refactor integ testsalso  refactor integ testsFormattingFormattingFormat pythonfix python formatFix pylintFix sphinx precheckAdd tensorrt.rst to toctreeAllow codegen to be tested when TRT runtime is not available. Enable TRT codegen in CIlintyAddress more commentsFormattingFormatting* Documentation changes* Address comments* Rename USE_TENSORRT->USE_TENSORRT_CODEGEN and USE_TENSORRT_GRAPH_RUNTIME->USE_TENSORRT_RUNTIME* Fix comment typo* Test CI without TRT codegen enabled* formatting* Enable USE_TENSORRT_CODEGEN in CI* Change file_util.h -> file_utils.h",0
"[Relay][Frontend][Onnx] Loop Support (#6700)* Onnx loop almost working, checkpointing for safety.* Very close to working.* Last piece is fixing scan initialization.* snapshotting for debug.* Fix Josh's issue* Use subgraph proto class.* Loop with scan.* Simple loop test now working.* Scan outputs now working.* Added second loop test.* Removed unneeded helper functions.* Remove bad merge artifact.* Cleaned up scan output creation.* Cleaned up some style mistakes.* Add pylint skip for unused-argument.* Remove onnx dependency.* Remove now obsolete checks for 0 shaped tensors.Co-authored-by: Jared Roesch <jroesch@octoml.ai>",0
[Bugfix] Auto scheduler tutorial failure on CI (#6723),0
Fix InferCorrectLayout for dynamic upsampling and add a regression test (#6712)* add a regression test* fix dyn upsampling infer layout* fix lint,0
[TVMScript] refactor (#6734)* [TVMScript] refactor* [TVMScript] pylint* [TVMScript] pylint,5
[AutoScheduler] Use tempfile in tutorials (#6728)* Use tempfile in tutorials* address comment* Update tutorials/auto_scheduler/tune_matmul_x86.py* Update tutorials/auto_scheduler/tune_conv2d_layer_cuda.pyCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,1
[FIX] Fix cublas batch matmul (#6715)* Update batch_matmul.pyUpdate batch_matmul.py* fix,0
"[FIX,CMAKE] Use set_property with append flag instead of (#6725)set_target_properties.set_target_properties does not append to existing properties. There werea couple place where previously set properties were overridden withdifferent properties. For example, the debug flags for relay were notset correctly because set_target_properties was called twice in a rowwith different options.",0
[COMMUNITY] junrushao1994 -> committer (#6719),3
"[LLVM] Create fixed vector size according to latest LLVM12+ changes (#6717)The vector handling code in LLVM keeps evolving to accommodate scalablevectors. As a result, code related to vector sizes changes quite often.",0
[AutoScheduler] Guarantee init population sampling outputs a valid set (#6713),5
[Relay] Minor fix for some TF OD models (#6729)* Minor fix for some tf od models* More fix* Minor fix* Fix lint* Minor fix,0
[CONDA] Revamp conda recipe. (#6732)* [CONDA] Revamp conda recipe.- Combines two packages into a single recipe.- Enable windows build.- Better packaging hash tag (use git string).* Address comment,1
[Hexagon] Use nullptr instead of 0 in hexagon_device_sim.cc (#6718)Passing 0 produces compilation warnings.,2
[Docker] Turn on Rust docs and MxNet based ResNet (#6640)* Enable ResNet and Rust docs* Tweak* Format* Fix issue with overwriting,0
[RELAY] Refactor FoldConstant to skip TNonComputationalOps (#6720)* add TNonComputational to qnn ops and change FoldConstant* remove comments* check if op in nonComputational map* forgot to mark device_copy op as TNonComputational* hacky fix to fuseops pass* fix typo* manually skip device_copy in fold_constant* Update src/relay/transforms/fold_constant.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
"Add pytest-xdist and pytest-profiling to the base installation packages. (#6736)For building and testing some small portions of the python testsuite,I've been playing off and on with xdist and pytest-profiling.We know it's not safe for the entirity of CI yet but this couldenable smaller parts of pipelines that folks use using thecommon scripts to be parallelized or indeed profiled for moreinsight into where time is spent in building and testing TVM",1
[FFI][BUGFIX] Fix memory leak when Pac callback argument is NDArray (#6744)* [FFI][BUGFIX] Fix leak when Packed callback arg is ndarray.Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>* Fix for rust ts and jvm* Update rust/tvm-rt/src/to_function.rsCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[LLVM] Avoid warnings when compiling getNumElements with LLVM12+ (#6738)* [LLVM] Avoid warnings when compiling getNumElements with LLVM12+Extract the element-count code into GetVectorNumElements and make itcompile cleanly with all LLVM versions.* Trigger another build,5
Add BatchNormAttrs Rust bindings (#6678),1
[Torch] Support bincount and scatter_add ops (#6740),1
[Docker][CI][BYODT] add universal to Docker image (#6654),1
[Relay][Frontend] Tensorflow version support upgrade from 2.1.0 to 2.3.1 (#6706),5
Update include and src dir CHECK* to ICHECK* (#6745),1
[MKL] Fix offloading of batch_matmul to MKL (#6752)* fix mkl offloading of batch matmul* name fix and add doc* add doc for lib argCo-authored-by: masa <masa@pop-os.localdomain>,0
"[WASM] Update support for latest emcc, add ffi test. (#6751)",1
add onnx resize v10 and unit test (#6726),1
[CI] Update wasm emcc to latest (#6755),1
[CI] Introduce all platform test for windows/mac/linux. (#6756)This PR introduces a minimal set of test cases thatare supposed to run in all platforms during CI.The set of testcases are supposed to help onplatform dependent regression.See tests/python/all-platform-minimal-test/README.md for guidelines.- Enable windows mac LLVM build via conda with cython support.- Test on all platform test cases.- Update implementation to improve MSVC support.,1
[BUGFIX] Fix topi matrix multiplication using tensorcore to run faster (#6749),0
"[Fix,Conda] update conda download url (#6760)Co-authored-by: Shibui Yusuke <yusuke.shibui@ShibuinoMacBook-Pro.local>",0
"[ARITH] iter_affine_map bug fix, stride generalize (#6753)",0
"[FIX,PYLINT] Fix pylint errors on MacOS with Python 3.8 (#6746)These errors do not seem to show up in CI, but they show up locally.",0
[VERSION] Enhance version.py to support git-describe. (#6757)* [VERSION] Enhance version.py to support git-describe.This PR enhances version.py with a --git-descrbe optionwhich allows it to generate a git describe based versiontag for potential dev related nightly packaging during thedevelopment cycle.The behavior of the normal relase remains the same.Note that the version.py still modifies the files inplaceand we only recommend using it during a clean clone based workflow.The setup.py is also updated to take advantage of the version.Note that the git info is already captured by the c++ side in a previous PR.The tool is mainly used to create PEP compatible python wheels.* Update per comment,1
"[TVMC] 'tvmc run' --rpc-tracker and --rpc-tracker fail due to argparse misconfiguration (#6762)to be identified as a list of strings, rathat than the expectedstring type.Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>",5
"More CHECK to ICHECK (#6758)* Address apps, docs, and nnvm directories* Catch some that were missed* crt has it's own logging.h* Fix missing include",0
[LLVM] Add target feature string to function attributes (#6763),1
"[FIX,MICROTVM] Add requires_micro decorators to microtvm tests (#6747)* [FIX,MICROTVM] Add requires_micro decorators to microtvm tests",0
[Relay] A set of utilities that allows a model to be run efficiently on tensorcores. (#6748),5
[VERSION] Make script path invariant (#6766),5
[AutoScheduler] Re-organize logs files for tutorials (#6768)* reorganize logs files* fix lint,0
[CI] Update PyXIR version to 0.1.3 (#6769)Co-authored-by: Shibui Yusuke <yusuke.shibui@ShibuinoMacBook-Pro.local>,1
[CI] Update ci-wasm to latest (#6772),1
"[Relay, TOPI] Complete rewrite of where op to support broadcasting (#6759)* where type rel with broadcast* add tests for where with broadcast* clean up tests* uncomment other tests* add more tests* update doc* CHECK -> ICHECK* add where any test* fix format* remove useless detections for one* set manual seed* ported shape broadcast helper func to hybridscript* remove shape function helper from cppCo-authored-by: masa <masa@pop-os.localdomain>",0
[Relay] Fix dynamic case for Squeeze and Split (#6739)* [Relay] Fix dynamic case for Squeeze and Split  Squeeze: Allow removed dimension to be dynamic and check it in shape  function  Split: Fix negative axis* Fix comments,0
Scatter on Cuda (#6533)* working cuda scatterfix lintfix pylint again* cuda scatter with threading* add dynamic shape tests* remove unused variable,0
[ARITH] Tight bound for floormod (#6771),5
Fix version check bug (#6784)* Fix version check bug* Update pytorch_utils.py* Update pytorch_utils.py* Update pytorch_utils.py* Update pytorch_utils.py,0
[API] Added remove_global_func to the Python API (#6787)This is useful for unregistering functions after a test.Change-Id: Ic39499aa8f36bfe5470bc1f058ad3b96cf52b49c,1
[ManifestAlloc] Handle TupleType inputs in CheckReshapeOnly (#6776)* Changes in CheckReshapeOnly to support TupleTypes as inputThis arises insed ManifestAllocPass inside relay.vm.compile* [ManifestAlloc] Handle TupleType inputs in CheckReshapeOnly,2
[Relay][Training] Add more missing gradients (#6767),1
[FIX][AUTOTVM] Make autotvm work with spawn (#6790)Like #6671 this PR fixes autotvm when using the spawn start method formultiprocessing. I've added some tests to make sure that things workwith spawn in the CI.,0
"[Torch, QNN] Support dynamic quantization flow to enable importing quantized transformer models (#6782)* add stub and test* per channel quantize* calculate qparam correctly* import qbert working* support batched qdense* test batched input* fix mkl offloading of batch matmul* reduce range become True in torch 1.6* fix for 1.6* Revert ""fix mkl offloading of batch matmul""This reverts commit cd90aa783688c68e1b12633eea4d2690d9e3a5a5.* fix merge* fix* lint fix* fix black* more black fix* fix version check for 1.5.1* disable assert on v1.4 (strange pytorch issue)* minor fix* use dequantizeCo-authored-by: masa <masa@pop-os.localdomain>",0
TFLite failures resulted from TF latest version upgrade resolved (#6774)* TFLite failures resulted from TF latest version upgrade resolved* [1] Review comments handled,3
[VTA] quant support for alu-only op (#6191),5
Only use thrust for cuda target (#6722),5
[REFACTOR] Remainings of util => utils (#6778),5
TF frontend: add expm1 op (#6783)* TF frontend: add expm1 op* TF frontend: add description for expm1* TF frontend: use overload operator - instead of subtract* TF frontend: Limits the range of input data in the Expm1 testCo-authored-by: xup <xp224797@alibaba-inc.com>,1
[TVMC] use common function to obtain target from --target value on 'tvmc compile' (#6788)- This is solving a TODO item on tvmc,5
fix a bug in convertSSA. (#6785),0
[QNN] Optimize requantize for power of 2 and fix dequantize for per-channel quantized input (#6675)* [QNN] Optimize requantize for power of 2 and bug in dequantize* Comments* Docs* Comments* Ethos,0
"[FIX,AUTOSCHEDULER] Fix auto_scheduler to run with multiprocessing's spawn start method (#6671)* Fix multiprocessing with spawn issues* address reviewer feedback* Fix tutorials* formatting* undo autotvm work* Undo tutorial changes* Add spawn tests* fix test",0
[CI] Keras version upgraded from 2.3.1 to 2.4.3 (#6793),5
[TVMSCRIPT] Add synr dependency in preparation for tvmscript diagnostics overhaul. (#6795),1
"[BYOC] Allow custom codegens to register their own constant updater (#6697)* [BYOC] Allow custom codegens to register their own constant updaterCurrently, all codegens using BYOC must make use of the defaultConstantUpdater pass. However, certain codegens, like Ethos-N,don't want to store any constants in metadata module. Thisprovides an interface (via a global) to register a customconstant updating method and assigns a 'null' updater for theEthos-N codegen.Change-Id: Ibd71d3091f992362eeede5d894eedb373b2dbc8f* Fix to use symbol in const nameChange-Id: I0ade81af9002d413c5b20a50488018e8cd8d8bad* Remove ;Change-Id: I61967bc4997efb87f87b49dad7e0a660c536ef35* Remove ccompiler constant updaterChange-Id: Iea9ee0f689683512fa114afeadeccb7fc9048e4f* Unregister updater after testChange-Id: I8009940bb2ac949f2c3f0d72c943a5b74afd6954* Create UpdateConstants utility functionChange-Id: I83c8c6f92cfe3be3a7e811e98a4eec17590186ff",0
[AutoScheduler] Relay integration : Task extraction (#6710)* add task extraction* fix evo search* fix tests* fix test* fix docstring* fix docstring* update workload registry* fix warning* fix test* fix fallback* fix lint* fix tests,0
Fix mutate auto unroll (#6807),0
"[CI] Pin h5py version to < 3.0 to workaround issues with TF/Keras (#6808)* Pin h5py to use the previous major release (2.x) and not   new version 3.0, due to incompatibilities with TF and Keras   that make TVMC and Frontend tests to fail",1
Extract channels from weight shape for conv2d. (#6805),5
"[µTVM] Add serial transport, parameterize µTVM Zephyr test, run on physical HW (#6789)* [BUGFIX] Respect infinite-timed session start timeouts. * When debugging, the intended behavior is to set the session start   timeout to infinite to allow the user to configure the debugger. * At present, if a session start retry timeout is defined, the   current logic will bail after the retry timeout expires. * This change makes the session start logic retry forever, once per   retry timeout.* Document RPCEndpoint::Create.* Add stm32f746xx to tvm.target.micro() call; fix parameter name. * This API is expected to just be used with positional args, not   kwargs, so this change isn't expected to cause any breakage. * model is more inline with the rest of the file, given TVM Target   Specification RFC.* [BUGFIX] If session start fails, exit transport context manager. * If an error occurred during session setup, then complex transports   e.g. DebugWrapperTransport would not de-initialize.* Align transport writes/reads in TransportLogger* fix syntax errors which were not exercised in previous PR* Remove microTVM logic from standard RPC server, add debug shell. * microTVM uses the host RPC server as a way to launch a debugger in   a dedicated, separate terminal window. microTVM needs to be able to   launch the debugger itself, because its model of the device   flash/debug flow separates these two things into distinct   operations implemented by shell commands (for maximum portability   across frameworks). * microTVM can be configured to launch the debugger (e.g. GDB) in the   same terminal as is used for flashing, but this is sub-optimal   because then it hides any logs emitted by the device. * Using the standard RPC server was hard because GDB expects the user   to issue SIGINT to interrupt program flow, but due to the RPC   server's necessary use of multiprocessing, multiple signal handlers   needed to be SIG_IGN'd, and further, because libtvm.so is   intentionally frontend-agnostic, it's difficult to include signal   handling directly in that binary (Python expects you to call   PyErr_CheckSignals, but we don't require and don't want to require   python-dev to compile libtvm.so, and this is the only such case   where libtvm.so is expected to block the main thread for a long   period of time). * Here we implement a separate microTVM debug shell python script   using the non-blocking server implementation.* Add serial transport, parameterize test_zephyr to work on real hardware* add pytest test fixture, missed from previous change. * this test fixture helps to parameterize the test case* address leandron@ comment from #6703",0
[CI] Add m6g instance (ARM64) to mainline CI (#6804)* [CI] Add m6g instance (ARM64) to CI (#6781)* [CI] Add m6g instance (ARM64) to CI* address commentsCo-authored-by: Ubuntu <ubuntu@ip-172-31-54-90.us-west-2.compute.internal>* [CI] fix cpp test (#6796)* Update tests/python/unittest/test_target_codegen_x86.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Ubuntu <ubuntu@ip-172-31-54-90.us-west-2.compute.internal>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[CI] Move back Keras to 2.4.3 (#6810)* I mistakenly moved it back to 2.3.1, now fixing it",0
[CI] Update to latest (#6812)- Fix Keras related regression.,0
[OBJECT] Update types slots for baseexpr and primexpr (#6814),1
[Rust][Diagnostics] Add initial boilerplate for Rust diagnostic interface. (#6656)* Add initial boilerplate for Rust diagnostic interface.* Codespan example almost working* WIP* Hacking on Rust inside of TVM* Borrow code from Egg* Update CMake and delete old API* Fix Linux build* Clean up exporting to show off new diagnostics* Improve Rust bindings* Fix calling* Fix* Rust Diagnostics work* Remove type checker* Format and cleanup* Fix the extension code* More cleanup* Fix some CR* Add docs and address feedback* WIP more improvments* Update cmake/modules/RustExt.cmakeCo-authored-by: Robert Kimball <bobkimball@gmail.com>* Update rust/tvm/src/ir/diagnostics/mod.rsCo-authored-by: Robert Kimball <bobkimball@gmail.com>* Clean up PR* Format all* Remove dead comment* Code review comments  and apache  headers* Purge test file* Update cmake/modules/LLVM.cmakeCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Format Rust* Add TK's suggestion* More CR and cleanup* Fix tyck line* FormatCo-authored-by: Robert Kimball <bobkimball@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>,0
TF frontend: add softsign op (#6799),1
[TENSORFLOW]Sparse2Dense support (#5767)* [TENSORFLOW]Sparse2Dense support* Formatting issues fixed,0
"[AutoScheduler] New layout rewrite option: Weight pre-transpose (#6750)* Add pre transpose support for layout rewrite* Update* Bug fix* Bug fix* Update* Bug fix* CI Fix* Update* Update* Re-trigger CI* Update* Update test_auto_scheduler_layout_rewrite.py* Update test_auto_scheduler_layout_rewrite.py* Update task_scheduler ut, re-trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>",0
Update stale link to new location (#6819),1
"[rust][tvm-graph-rt]: maintain error sources when propagating errors, swap Mutex for RwLock (#6815)",0
Improve AArch64 depthwise convolution through smlal/smlal2 intrinsic (#6711)* Improve depthwise convolution through smlal/smlal2 intrinsic- Added an intrinsic to load a single int16x8 vector and produce two  int32x4 output vectors through smlal/smlal2 instructions- Changed the NHWC depthwise schedule to accomodate the aforementioned  intrinsicChange-Id: I347c3bf98fa8dd87057304dcda0d78e558424c57* Address review comments* Rebasing - 2* Rebasing - 3* Rebasing - 3* Fix linting,0
[CI] Torch 1.7 update to mainline (#6828),1
"[TF] Fix a bug in _stridedSlice() (#6829)When stride < 0, the slicing range for whole demension should be  [-1, -(dim+1))",0
[CI] remove unused environment var (#6824),4
"[TVMC] 'tvmc tune' --rpc-tracker and --rpc-tracker fail due to argparse misconfiguration (#6822)Fix an error with `tvmc tune`, that causes --rpc-tracker and --rpc-key to be identified as a list of strings, rather than the expected string type.Removing the unnecessary nargs solves the issues.This is a follow-up of https://github.com/apache/incubator-tvm/pull/6762",0
"Fix Annotate Target to support freevars(relay.zeros, relay.ones etc) of any size (including zero)  (#6826)* Fix Annotate Target* Add Test Cases* Formatting* Comments C++* Remove Unnecesssary test cases* typo* annotate_targetCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>",0
[DOCS] Enable theme with header and footer. (#6834)Also fixed a sphinx warning in pytorch.,0
Update link (#6838),1
[BYOC] FTVMAnnotateTarget method signature update (#6786)Signature of FTVMAnnotateTarget changed to runtime::TypedPackedFunc<bool(const Expr& expr)>which allows to utilise extra information from passed expr argument.,1
[CI] Disable flaky tests (#6841)* [CI] Disable flaky tests* format,3
[Relay][Frontend] SparseTensorDenseMatMul support for Tensorflow (#6685)* [Relay][Frontend] SparseTensorDenseMatMul support for Tensorflow* Lint error resolved* [1] Review comments handled* [2] Review comments handled,0
Register shape functions for some image related ops (#6373)* debugging* added three shape funcs* fix lint* address comment* resolve conflicts* resolve conflicts* resolve conflicts* resolve conflicts* resolve conflicts,0
[TopHub] Bump the versions (#6837)* [TopHub] Update version* trigger ci,1
"[Graph memory plan] Support nested tuples (#6809)* add test* test working* uncomment other tests* remove redundant visit* test double nesting* support nested tuple in CallNode's return type* Revert ""support nested tuple in CallNode's return type""This reverts commit 66225eda33f37647cfc11ceb8caa2125dfe88d0d.",1
[CI] Add python setup script (#6844),1
Syntax error String::fromwe() should be String::from() (#6846)Co-authored-by: Mikael Sevenier <mikael.sevenier@sima.ai>,0
[AutoScheduler] Bug fix for layout rewrite CI error in i386 (#6830),0
[CI] Add more guidelines about local setup (#6848),1
[FIX] Add task_ci_python_setup.sh to the arm CI (#6850),0
Update SimplifyInference documentation (#6853),1
"[µTVM] Add virtual machine, test zephyr runtime on real hardware (#6703)* Split transport classes into transport package.* Introduce transport timeouts.* black format* Add metadata-only artifacts* Simplify utvm rpc server API and ease handling of short packets.* add zephyr test against qemu* Add qemu build config* fix typo* cleanup zephyr main* fix nonblocking piping on some linux kernels* don't double-open transport* validate FD are in non-blocking mode* gitignore test debug files* cleanup zephyr compiler* re-comment serial until added* remove logging* add zephyr exclusions to check_file_type* add asf header* lint* black format* more pylint* kill utvm rpc_server bindings, which don't work anymore and fail pylint* fix compiler warning* fixes related to pylint* clang-format again* more black format* add qemu regression* Fix paths for qemu/ dir* fix typo* fix SETFL logic* export SessionTerminatedError and update except after moving* fix test_micro_artifact* retrigger staging CI* fix jenkins syntax hopefully* one last syntax error* Add microTVM VM setup scripts* obliterate USE_ANTLR from cmake.config* add poetry deps to pyproject.toml - mainly taken from output of `pip freeze` in ci-gpu and ci-lint* initial attempt at setup.py + autodetect libtvm_runtime SO path* hack to hardcode in build* make pyproject lock* Add ci_qemu to Jenkinsfile* build in qemu* checkpoint* create diff for jared* add missing stuff* address liangfu comments* fix new bug with list passing* release v0.0.2* works on hardware* switch to pytest for zephyr tests* add missing import* fix option parsing* remove extraneous changes* lint* asf lint, somehow local pass didn't work* file type lint* black-format* try to fix ARMTargetParser.h #include in LLVM < 8.0* rm misspelled deamon lines* move to apps/microtvm-vm* fetch keys from kitware server* fix path exclusions in check_file_type* retrigger CI* reorganize vm, add tutorial* fixes for reorganization - enable vagrant ssh* update ssh instructions* rm commented code* standardize reference VM release process, add prerelease test* remove -mfpu from this change* fix exit code of test_zephyr* rm unneeded files, update check_file_type* add asf header* git-black* git-black against main* git-black with docker* fixes for virtualbox* black format* install python3.8, for zephyr gdb* timestamp zephyr vm name, permits launching multiple VMs* log warning when initial vagrant destroy fails* revert changes moved into #6789* address leandron@ comments* black format* black format* add --skip-build to test subcommand, detach device from other VMs* black format* address leandron@ comments* don't rm release test when building only 1 provider* revert pyproject.toml* remove need to copy pyproject.toml to root * this often contributes to erroneous changes to that file",0
[Rust][IRModule] Flesh out IRModule methods (#6741)* WIP* WIP* WIP* WIP* Disable WASM and fix rebase* Work on finishing tests* Make entire object system printable* Write some more tests for IRModule* All tests pass* Format* Restore module.cc* Bump syn,0
[TOPI] Enable scatter_add on GPU  (#6856)* enable scatter gpu test on cuda* adding update_func arg* pytorch scatter_add gpu tests working* update 3d and 4d scatter* enable scatter_add gpu testCo-authored-by: masa <masa@pop-os.localdomain>,1
[Relay][Frontend][Onnx] If Operator Support (#6730)* If operator support in ONNX.* Small tweak.* Added uses_gpu tag.* Disable test on GPU until onnxruntime version is updated.* Use parametrize_target to specify CPU only.* Just dont use onnxruntime for now i guess.,1
"[QNN] Dynamic scale, zero point in qnn.op.dequantize (#6849)* add dynamic dequantize* register quantize and dequantize as opaque* make tests better* black* remove main fn* fix black again* move tests* fix import* fix import again* try again* fix import",0
"[TVMSCRIPT] Using diagnostics for TVM Script (#6797)* [TVMSCRIPT] Using diagnostics for TVM Script* fix lint* More documentation, improve some error messages* Apply suggestions from code reviewCo-authored-by: Leandro Nunes <leandron85@gmail.com>* Add synr to ci setup and setup.py* remove typed_ast dependencyCo-authored-by: Leandro Nunes <leandron85@gmail.com>",0
[BYOC] [ACL] ACL Runtime padding workaround (#6724)This workaround prevents execution of operations via ACL runtimein case if arguments or output tensor require memory padding.Workaround is applicable to all ACL versions prior forecoming ACL 20.11(which will not use data padding).,1
Fix the build error for wasm-standalone app (#6862),0
Update arm_compute_lib.rst (#6861)Updated correct path in readme,1
[Bugfix][Module] Fix recursive GetFunction in runtime::Module (#6859),0
[BYOC][CONTRIB] Vitis-AI codegen integration (#6343)* [BYOC][CONTRIB] VITIS-AI integration* Remove environment related files* Update vitis_ai.rst* Add review changes* Remove new lines and note frame in vitis_ai.rst* use sys.exit* Add condition for vitis_ai runtime exec function* remove unused graph_json* correct indentation* use code python instead of bash* Rename VITISAI.cmake to VitisAI.cmake* use relay.ext.vitis_ai.options.build_dir in comparison* Re-add deleted docker related files* Make use of PyXIR XGraph and RuntimeModule serialization & refactor flow* Fix linter errors* Fix linter errors* Address sphinx warnings* Add infertype to fix Vitis-AI annotation test* Renaming util to utils* Add Vitis-AI flag to config.cmake file* Move vitis-ai config options to compiler sources instead of runtime sources* Fix clang-format errorsCo-authored-by: Anil Martha <anil.martha@xilinx.com>Co-authored-by: anilm (generated by with_the_same_user script) <anilm@xhdabidk40.xilinx.com>Co-authored-by: Jorn Tuyls <jornt@xilinx.com>,0
"[TIR] Make loop unrolling in LoopPartition optional (#6823)* [TIR] Make loop unrolling in LoopPartition optionalFor certain analysis/tensorization, it can be usefulto keep the loop structure when partitioning loops.The current behaviour removes For loops of length 1.This change introduces the option to preserve theseloops with the 'unroll' flag.",4
"[RELAY][OP] Support MXNet-style attributes for reshape_like (#6851)* add MXNet-style reshape_like attrs support* lint* document, switch to int, add more tests, style* add example usage in documentation* fix doc formatting",0
fix first-order AD on tuple arguments (#6827),0
[Relay] Mix mode type inference (#6704),5
"[FIX,RPC] Skip RPC tests when using multiprocessing's spawn method (#6858)The rpc tests are broken when running under pytest with multiprocessingusing spawn. I suspect this is because pytest tests each function in aseparate process and does not import the full module.",0
Add smmla/ummla support in quantized Conv2d (#6802)* Add smmla/ummla support in quantized Conv2dThis introduces support for `smmla`/`ummla` instructions in TVM:- Added `is_mmla_available` function in `arm_utils.py`- Added the tiling node + tensorization schedule in `conv2d_gemm.py`- Added the intrinsic support in `tensor_intrin.py`- Added the test-case in `test_topi_conv2d_int8.py`Change-Id: Iff48c77f16fe1e64ecb733da965a879651ce635f* Address review comments and test failures* Fix linting* Rebasing,0
"Update search for bitcode files for rocm 3.9 (#6865)rocm 3.9 moved the bitcodes, we adapt to that.As this gives opaque error messages that are hard to debug(loading the module fails with could not initialize shared objectbut does not tell you about the missing symbols), we tightenthe checks at this stage:- we become more strict with missing bitcodes,- we let the linker fail loudly for unresolved symbols.",0
making quantization tweaks (#6731),5
conv1d_transpose speedup. (#6840)Improve performance of transposed convolution by avoidingredundant multiplication by zero values from dilated data.Co-authored-by: Ubuntu <ubuntu@ip-172-31-74-104.ec2.internal>,5
Fix bug in processing script (#6867)The argsort command returns a new array that is the sortedindex rather than a new sorted value array. This patchstores the sorted index in a new variable and uses it toreference the predicted values.,0
[COMMUNITY] New committer -- @mbaret (#6873),1
[AutoScheduler] Make SearchTask and ComputeDAG serializable (#6842)* serialize task and dag* fix test* more tests* format* format* format* trigger ci,0
"[BYOC][TRT] Allocate GPU data buffers and transfer data when needed (#6872)* Allocate data buffers for gpufix* Rename AllocateDeviceBuffer, update docstrings* Remove unneeded cast",0
register auto-scheduler to more ops (#6879),5
More flexible conv2d_NCHWc_int8 generic operator. (#6714),5
[AutoScheduler] Fix the occasional crash caused by split memo (#6883),0
[DOC] Improve the order of tutorials within a subsection (#6880),5
[RELAY][OP] roi_pool operator alter layout (#6516)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,5
Do not show meta-data when printing IRModule (#6881),5
TF frontend: add rint op (#6818)* TF frontend: add rint op* Added negative numbers to the test,1
[Relay][TF] Keep node name in span (#6885),5
[TVMC] add cl support in tvmc runner (#6831)* [TVMC] add cl support in tvmc runner* Cleanup comment and asssert device type in else case,1
[GCC] Fix GCC8.1 and GCC8.2 template dispatch compilation issue (#6893)* update* [GCC] Fix GCC8.1 and GCC8.2 template dispatch compilation issue,0
Fix bug of generate-unmatched-brackets in CodeGenC::PrintSSAAssign (#6887),0
[TIR] Add spans to all ExprNodes (#6860),1
[AutoScheduler] Improve tuning with random cost model (#6835)* fix* more fix* fix* revert* format* Update sketch_policy.cc* increase measure trial to avoid flaky,0
fix (#6902),0
"Dynamic gpu tests, add dynamic strided slice to topi (#6870)* enable GPU tests for dynamic ops* strided-slice can't do 0-sized output tensors, remove test* move dynamic strided slice into topi* add python interface to topi dynamic strided sliceadd python interface, tests* autoformat* fix bad copy/paste* fix doc string* disable topk on gpu for now, remove invalid slice test",0
[AutoScheduler] Add winograd support in tuning networks (#6877)* add winograd in auto-scheduler* trigger CI* address comments* fix tests* fix test,0
Bump up tophup cuda version (#6908),5
"[TFLite runtime] Allow to set number of threads to TFLite interpreter (#6901)* Support for setting thread count in TFLite runtime,Co-authored-by: FrozenGene <zhaowu@apache.org>* fix lintCo-authored-by: FrozenGene <zhaowu@apache.org>",0
[AutoScheduler] Tutorial on auto-scheduling a network for GPU (#6882)* add a tutorial on auto-scheduling a network for cuda* fix typo* fix training time printing* fix lint* fix* upload logs* fix* use weighted sum as the default objective function* update ci logs* fix the bug in kill_child_processes* fix test* address comments* add early stopping in task scheduler & fix a stuck issue in measurement* fix lint* trigger CI* fix early stopping,0
Fix edge cases in const_int_bound and fold_scale_axis (#6911),0
[TRT][BYOC] handling dynamism in TensorRT to support OD models (#6905)* handling dynamism in TensorRT to support OD modelsrefactoring test tensort codeadded comments to dynamic check wrapperlog.warn changed to logger.infoTRT codegen taking slice_mode into accountTRT codegen to handle both stride_moderefactoring TRT codegenadding a test for dynamic offload[TRT] bug in codegen for slice_mode=endctx determined from target in test + io test was missing* Addressed the formatting/refactoring comments* Addressed comment in TRT codegenLint formatting* Lint error* using slice_mode during strided slice registration in tensorrt.py* removed a few blank lines* addressing cli comment on elif-return* Added decorator for tensorrt functions with dynamism checkskip_codegen added for test_tensorrt::test_dynamic_offload* addressed comments in PR + black linting* resolved import error in test_tensorrt* import mxnet location changed to pass CI* test_integration removed as components were run by pytest anyway,0
Consolidate RPC Context helper functions (#6915),5
"Make TVMLogf platform-independent (#6916)* Make TVMLogf platform-independent. * Some platforms need to use an alternate printf() to support basic   things like %zu. Since %zu is platform-specific, we prefer to   use a printf() that supports it or allow the platform to fix it up   as needed.* git-clang-format",0
[TF parser] Handle int64 dtype in range (#6918),5
[ShapeFunc] Handle weights in shape func (#6912)* [ShapeFunc] Handle weights in shape func* Comments,5
[Doc] Minor improvements for auto-tuning tutorials (#6919),5
[Relay] Add dynamic SparseToDense (#6892)* [Relay] Add dynamic SparseToDense* Fix comments,0
[CI] Update actions miniconda (#6926),1
[AutoSchedule] Extract tasks via compile engine (#6903)* make use TOPI schedule optional* extract auto_schedule task* format* add extract mode* silent autotvm* fallback to TOPI* use PassContext* lint* surpass fallback warnings* nit* fix test* address comments* address comments* doc* address comments* lint* skip unsupported tasks* reigger CI,0
Make AutoScheduler handling of errors during measure consistent with AutoTvm (#6909)* Match ansor handling of 'too many errors' during measure to that of autoTVM and match default level of logging* Set correct level of verbosity for debug modeCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>* Lint* trigger CICo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>Co-authored-by: Taylor Zowtuk 84152750 <taylor.zowtuk@huawei.com>,0
[Relay] Add space_to_batch_nd and batch_to_space_nd operators (#6477)* [Relay] Add space_to_batch_nd and batch_to_space_nd operators* Correct python-format errors* correct lint errors* tflite frontend to use batch_to_space and space_to_batch operators* Add new pad_value parameter with default value is 0 for space_to_batch_nd and correct variable names* Fix cppdocs - add documentation for pad_value,0
"[AutoTVM][RPCRunner] timeout is not passed correctly (#6924)* [AutoTVM][RPCRunner] timeout is not passed correctly* like @merrymercy suggests, scale timeout with (n_parallel + 1)* Apply suggestions from code review* Apply suggestions from code reviewCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>",4
[CI] Install libc6-dev-i386 to compile wasm32 (#6886)* [CI] Pin wasmtime version to 0.16.0* Keep the wasmtime version to the latest,2
[TensorFlow] Support NonMaxSuppressionV5 (#6933),5
[DOC] Fix typo (#6920),0
[AutoScheduler] Fix task scheduler restoring (#6934)* [AutoScheduler] Fix task scheduler restore* miner fix,0
[COMMUNITY] New committer -- @mbrookhart (#6936),1
Add Handling of Zero Len Arguments (#6923)* Update tensorrt.py* Update tensorrt.py* Update tensorrt.py,1
[AutoScheduler] Improve warning messages (#6935)* [AutoScheduler] Improve warning messages* fix lint,0
explicitly use new to avoid exit-time destruction of global state for VM (#6938),1
Lazy import XGBoost (#6939),5
"[µTVM] Fix problems with the debug flow (#6930)* Allow blocking read and write in micro transport, for debugging.* add support for None timeout to micro transport, add tests* fix GdbTransport and friends. * GDB itself was just busted (would not launch inferior properly) * GdbDebugger would kill the debugger without waiting for user   input. change to always wait for an explicit user quit. * immediately resurrect Ctrl+C handler when debugger dies. * remove on-terminate callback complexity, unnecessary",0
fix tvm.relay.build() docs (#6940),0
Bug-fix] Fix tir allocation with multiple lanes (#6941)* Bug-fix] Fix tir allocation with multiple lanesThis PR stemmed from https://github.com/apache/incubator-tvm/pull/6907and it is fixing a small error in the getter and setter of a buffer forthe case where `t.lanes > 1`. I also added a test to stress the issue.* Address dtyped vs non-dtyped constant cases,0
[AutoScheduler] Register workload when deserializing tasks (#6927)* [AutoScheduler] Register workload when deserializing tasks* fix name* format* merge* fix test* more checks,0
[DOCS] Improve windows build instruction via conda (#6944),2
[Relay] Add DefuseOps pass (#6946)Co-authored-by: minminsun <minmin.smm@alibaba-inc.com>Co-authored-by: minminsun <minmin.smm@alibaba-inc.com>,1
"[µTVM] Remove binutils module, no longer needed after microTVM refactor. (#6947)",4
"AArch64 base algorithm refactoring in LLVM (#6907)* AArch64 base algorithm refactoring in LLVM- I refactored the assembly in arm_cpu/tensor_intrin.py to use LLVM+TIR- Removed the `interleave` boolean parameter in the intrinsic to switchamong two different interleaving modes. LLVM will now take care ofinterleaving the instructions- Applied the changes accordingly to conv2d_gemm.py to call the rightinstrinsicNote: I found LLVM very sensible to the choice of the `-mcpu`.So, in order to preserve performance, it is important to specify theright `-mcpu` when creating the LLVM target* Fix linting* Fix linting -2* Fixing comments* Address review comments* Fix spaces around ':' in docstrings",0
Fix code to work with cmake 3.2 (#6952),0
[PatternLang] Remove unnecessary check (#6958)Thanks @mbrookhart,4
[Bugfix][AutoScheduler] Strictly select impl using plevel (#6956)* [Bugfix][AutoScheduler] Strictly select impl using plevel* lint,0
[AutoScheduler] Task scheduler callbacks (#6945)* [AutoScheduler] Task scheduler callbacks* docstring* address comments* Delete the explaination of callback in the tutorial* fixCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,0
Cleanup comments (#6951),5
[AutoScheduler] Fix task extraction (#6965)* [AutoScheduler] Fix task extraction* fix* fix* trigger CI,0
"Fix #6954 uTVM, fix when building the runtime for native hardware (#6957)* Fix #6954 which when building the runtime for native hardware failswith -march= is missing.This fix:1) adds support for march2) picks a senable setting for f746 discoveryThere is an interesting downside to this fix involving scheduling that likely needs discussion.In the microcontroller world we really should be setting ex: -march=armv7e-m depending on whatcortex-m is being used.-mcpu isn't as important when it comes to a command line compiler.Signed-off-by: Tom Gall <tom.gall@linaro.org>* Fix #6954 which when building the runtime for native hardware failswith -march= is missing.This fix:1) adds support for march2) picks a senible setting for f746 discoveryThere is an interesting downside to this fix involving scheduling that likely needs discussion.In the microcontroller world we really should be setting ex: -march=armv7e-m depending on whatcortex-m is being used.-mcpu isn't as important when it comes to a command line compiler.Signed-off-by: Tom Gall <tom.gall@linaro.org>",0
[DOCS] Update to reflect the repo name change (#6967),1
Raise ImportError for XGBoost (#6969),0
"Bug fix for debug builds in micro_session.cc (#6968)* If the build decides not to inline kReceiveBufferSizeBytes,  we will encounter a linking error.Change-Id: Ibbe5b20fdd63acb2b4652ca9896f5737eaf14b00",0
[CI] Disable ASF header checking on untracked files (#6975)* This patch will disable checking for ASF header in untracked  files that are never going to make its way into the repo.* That would help developers to have their untracked local files.Change-Id: Ie9f1aae28a474f10f52f22fe9e27a52afd95b4be,2
[AutoScheduler] Print the time used for measurement (#6972)* [AutoScheduler] Print the time used for measurement* address comments,1
[AutoScheduler] Check duplicated names in the compute dag (#6973)* [AutoScheduler] check duplicated names in the compute dag* fix lint* fix pooling* fix pooling,0
[Frontend][Relay][Parser] fix unparsable yolo formals (#6963)* fix yolo formals* fix lint* move test to test_forward,0
Don't fuse take with dynamic inputs (#6979)* add a regression test for fusing dynamic take* add legalize for take that stops fusion on dynamic inputs* fix lint* fix typo,0
bumping vta version (#6977),5
"Add Relay option to link parameters into runtime Modules (#6917)* refactor RPCSessionContext utils* Make TVMLogf platform-independent. * Some platforms need to use an alternate printf() to support basic   things like %zu. Since %zu is platform-specific, we prefer to   use a printf() that supports it or allow the platform to fix it up   as needed.",0
Add initial support for quantized transpose convolution in Relay (#6899)* Add initial support for quantized transpose convolution in RelayThis work is based on @jainris initial PR: https://github.com/apache/incubator-tvm/pull/6523I added a relay.qnn.conv2d_transpose node. The strategy I followed is toconvert to int16 and invoke nn.conv2d_transpose (which already exists inrelay). Main changes:- The node declaration lives in relay/qnn/op/convolution_transpose.cc- Cast int8->int16 and subsequent offset removal is in tvm/relay/qnn/op/legalizations.py.- I added and tested the operator in the tflite front-end- I added a unit-test in Relay for qnn.conv2d_transposeCo-authored-by: Rishabh Jain <jainris@users.noreply.github.com>* Fix linting* Addressing review commentsCo-authored-by: Rishabh Jain <jainris@users.noreply.github.com>,0
[AutoScheduler] Accelerate feature extraction for winograd (#6981)* [AutoScheduler] Accelerate feature extraction for winograd* fix an overflow in feature.cc* address comments* address comments* Update include/tvm/te/schedule.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Use a smaller min_repeat_ms* Use a smaller min_repeat_msCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
Fix GraphRuntime with -link-params over RPC (#6985)* Fix GraphRuntime with remotely-linked params. * Previous test did not exercise this correctly.* fix incorrect function name,0
[Hardware][Verilator] Integrating and simulating hardware accelerators in TVM (#6971)* add files* update interface* update* fix comment* fix fmt* remove widget repo* remove,0
Fix C runtime NDArray allocation bug (#6991),0
Demote session traffic logs to DEBUG log level (#6989),0
"Include required CMSIS headers in Cortex-M micro kernel. (#6988)* The existing kernels referenced CMSIS functions presuming that   those functions were defined by user code. This was the case with   the old blog post build flow. Add #include, since it's impossible   to compile the kernels without it. * TODO: port those functions to the micro kernels and remove external dependency",1
Fix the shape check for vta dense strategy (#6983),0
[AutoScheduler] Skip useless calls to RewriteLayout (#6993)* [AutoScheduler] Skip useless calls of RewriteLayout* fix lint* fix lint,0
[AutoScheduler] Use a smaller retry number (#6996),5
[AutoScheduler] Use a smaller iteration number for GA to acclerate the search (#6994)* [AutoScheduler] Use a smaller GA iteration number* fix* fix* add a new argument to control the search policy from task scheduler,0
[Backend][Verilator] Multiple fixes (#6995)* bump vta-hw submodule version* fix cmake related stuff,0
add files (#6986),1
fix docker image when installing rust (#7004),0
[TVMC] use target_host when it is set (#6855)* [TVMC] add cl support in tvmc runner* [TVMC] use target_host when it is set* Cleanup comment and asssert device type in else case* add a test for tvmc compiler* remove unused func,1
Dynamic Batch Support for TRT  (#6955)* add_annotate_fn* Reshape_ann_fn* Prune Subgraph* Dynamic Shape* Make PT Mask RCNN Work* Cleanup* Remove comments* Remove COmments* GetBatchSizeFix* Fix Remove Droupout* Fix Remove Droupout* TRT Runtime* Add MaskrCNN R50* New Testing code* Fix black* Test Maskrcnn r50 done* Test MR50* Space typo* Change Log to Dlog* Move test to tensorrt.py* Remove imports* Remove function* Add it to trt* import error* Imports* Add torch to CI* trt_test* Check test* Revert Pytorch install* Fix* test dynamic batch* TRT* Resolve PR comments* Zero batch size addCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>,0
"[RELAY,TOPI] Add scatter_nd op (#6854)* [RELAY,TOPI] Add scatter_nd opScatter_nd is the inverse of gather_nd and also happens to be itsgradient. The implementation here is not optimized. There are no cpu orgpu specific implementations.* formatting* Fix tests* formatting* specify types on test* Fix grad test* scatter_nd cuda impl* cuda impl* x86 impl* formatting* fix shape rel* fix tests* formatting",0
[TOPI] deformable_conv2d in NHWC (#6999)* [TOPI] deformable_conv2d in NHWC* Update python/tvm/topi/generic/nn.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/topi/testing/deformable_conv2d_python.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* style* fix* styleCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
Fix call mkl gemm in mkldnn.py (#7007)Co-authored-by: zhangfucheng <zhangfucheng.jason@bytedance.com>,0
Use channels from attrs if possible (#7011),5
[µTVM] Minor fixes to the Reference VM tutorial (#7012)* Add recommendation to install vbguest plugin.* Update directories to match checked-in.,0
[Backend][Verilator] regression tests (#7000)* add files* update tests* test this* test this case* update jenkins file* fix offload* update* update variables* rollback ci files,0
[µTVM] Modify reference VMs to support new µTVM demo (#7001),1
[AutoScheduler] Support layout rewrite for whole networks (#6987)* [AutoScheduler] Add layout rewrite pass in relay* fix* fix lint* fix attrs* trigger CI* Apply suggestions from code review* trigger CI* Update python/tvm/auto_scheduler/relay_integration.py* Update python/tvm/auto_scheduler/relay_integration.py* Update python/tvm/auto_scheduler/compute_dag.py* Trigger CI* Apply suggestions from code review,0
Fix trt Test (#7016)* Fix trt Test* Fixed stuff* Done* fix 0* Trigger BuildCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>,0
[AutoScheduler] Add a tutorial on auto-scheduling a network for x86 CPU (#7019)* [AutoScheduler] Add tutorial on auto-scheduling a network for CPU* update* update* update* improve* improve* address comments* add help on layout conversion* add help for layout conversion* update target string* update cuda logs,1
[auto_scheduler] metal default hardware params (#7022),5
[µTVM] Fix paths in the reference VM tutorial and add vbguest recommendation (#7015)* Add recommendation to install vbguest plugin.* Update directories to match checked-in.,0
[Diagnostics] Add environment variable for controlling top-level printing and fix issue with pretty printing/parsing roundtrip. (#6874)* Update Parser in order to handle the NMS code* Add support for displaying traces optionally* WIP* Fix* Fix error reporting in parser and clean up __init__.py due to CR* Format* Quick fix for If* Fix format* Fix lint,0
"[RPC] Prefer IPv4 between IPv4 and IPv6 (#7013)This change fix problem with version of IP protocol on MacOS.  Previousthe `rpc_tracker` and `query_rpc_tracker` were not able connect to eachother with default hostnames.The root cause was in method `socket.getaddrinfo`. In `rpc_tracker` thedefault hostname is ""0.0.0.0"" and `getaddrinfo` returns IPv4 type. In`query_rpc_tracker` the default hastname is ""localhost"" and`getaddrinfo` on MacOS returns IPv6 type. Note: on Linux both have IPv4type.These tools worked by different protocols and this is why`query_rpc_tracker` wasn't able connect to `rpc_tracker`.Now we will prefer IPv4 type. And both `rpc_tracker` and`query_rpc_tracker` will use the same version of protocol.",0
[CI] Hotfix CI (see #7010) (#7025),0
[AutoScheduler] Misc update to hardware parameter and task scheduler (#7020)* [AutoScheduler] Mics update to hardware parameter and task scheduler* update* update* update* update* fix* fix* update* improve warning message* update* lint* update* update* fix* Apply suggestions from code review* trigger CI,0
[Topi] Fix GPU Dynamic Topk by Improving Dynamic Strided Slice in Topi (#7018)* Fix GPU dynamic Topk* Fix style* Minor fix* Simplfy dynamic checking* Fix lint* More improvements* Disable test any topk,0
[Relay][Pass] Clean up DCE tests in preparation for refactoring.  (#7029)* Clean up DCE tests* Format* Fix* Fix,0
[AutoScheduler] Refactor task interface for tuning single operators (#7028)* [AutoScheduler] Refactor task interface* updae tutorials and tests* update* fix lint* fix lint* update* fix test,0
"Save PyTorch frontend state in object (#7023)While the functional approach is pretty neat, we ended up havingglobal state (default frontend, dtype) and it'll be more soon(caching of inferred types, see #6900). To not have to pass aroundthe state, this moves the op conversion into a class with instanceshaving the state.",2
[Relay][Frontend][Onnx] Add support for Size op in Onnx frontend. (#7031)* Add support for Size op in Onnx frontend.* Simplify target testing.,1
[Frontend] Prevent tflite frontend from producing int64 shape/parameters (#7030),5
Add version 11.1 in finding CUDA libdevice (#7033)* Add CUDA 11.1 libdeviceMaybe we should have a >= check instead.I also added a fallback to detect the version if version.txt ismissing. Calling nvcc for this has been inspired by what PyTorchdoes when compiling extension modules.,1
[CI] Update docs style dependency. (#7034),1
"[GraphRuntime] remove print from GetInputIndex (#7027)* remove print* retrigger CI, flaky test failure",3
[TOPI][OP] cuda for argwhere (#6868)* argwhere* cuda schedule* sort argwhere result* Use single block and thrust to fix flaky behavior* format* used dynamic strided_slice* Fix dynamic strided_slice* try new strided_slice* Improve dynamic strided slice to bind data depedent shape var.* all tests pass* remove print* use new strided_slice* cleanCo-authored-by: Yao Wang <kevinthesunwy@gmail.com>,0
Implement Keras Conv1D (#7035),5
[TVMSCRIPT] Attach span information to tir nodes in tvmscript (#6910),5
[AutoScheduler] Improve CPU matmul tutorial (#7037)* [AutoScheduler] Improve matmul tutorial* fix,0
[Relay][Topi] Fix GPU NMS when return_indices is True (#7005)* Add rearrange_indices* Fix output type* Clean test* Fix pylint* Fix CPU nms multi-batch* Diable test* Minor fix* Minor fix,0
[AutoScheduler] Remove `max_registers_per_block` in HardwareParams (#7040)* [AutoScheduler] Fix hardware params* address comments,0
[FIX] disable cuda test for argwhere (#7042)* disable cuda test for argwhere* Fix lintCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,0
[AutoScheduler] Add tips on resuming the search from a log file (#7039)* [AutoScheduler] Add tips on resuming the search from a log file* Trigger CI,1
[ROCm][Auto scheduler] Support Auto scheduler and NHWC convolution on ROCm (#7038)* add nhwc + winograd support to rocm strategy* support rocm hw parameters in search task* run analysis pass for rocm too* run black* pylint fix* use IsGPUTask functionCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,0
[TOPI] GPU scatter_add using atomic (#7044)* use atomic add for faster 1d scatter add* update tests* run black* more pylint fix* remove fp64 bintcount testCo-authored-by: masa <masa@pop-os.localdomain>,0
[DOCS] Document cloudpickle dependency in tutorials (#7049),2
[Auto Scheduler] Add target host to measure record (#7046)* [Auto Scheduler] Add target host to measure record* Fix PyLint* Fix lint* Solve the serialization logic when we don't have hardware params* update auto scheduler log,0
[Relay][Frontend][Onnx] MaxUnpool Operator (#7036)* Added maxunpool test.* MaxUnpool implemented and tested.* Lint fix.* Add explicit output shape in tests.,0
"[LLVM] Support atomic for GPU backend (NVPTX, ROCm) (#7051)* support atomic add on llvm* make atomic builtin intrin* test bincount on nvptx* use builtin::atomic_add* add atomic llvm codegen test, only works on int8 input somehow* supports fp32 atomic* drop support for cpu atomic* add comment* add atomic gpu unit test* reenable other tests* add doc string* run black* fix build with llvm 8 and older* fix format* do not run float32 atomic test on ci* do not run scatter_add 1d with float inputs on CI* fix typo* add todo comment for cpu backend* fix build on ciCo-authored-by: masa <masa@pop-os.localdomain>",0
fix missing ffi binding of relay.attrs.DequantizeAttrs (#7054),0
"[BYOC][TRT] Support batch norm for all ranks <=5, and all axes (#7026)* [TRT] Support batch norm for all ranks <=5, and all axis* Add return false* Fix TRT < 6 build",0
"[TOPI] GPU scatter 1D via sorting based approach (#7056)* add thrust stable sort* rename* scatter via sort working* correctly handles negative indices* clean up, add some comments* add doc string* remove scatter benchmark stuff* add more doc* fix typo* lint fix* silence lint* fix py format* check for thrust availablity before testCo-authored-by: masa <masa@pop-os.localdomain>",0
fix nvcc compile option to be compatible with older cuda (#7065)Co-authored-by: masa <masa@pop-os.localdomain>,0
"[FIX] Improve error messages and docs (#7064)- Better document tvm.relay.create_executor- Print what was provided in src/target/llvm/llvm_module.cc,  src/tir/transforms/arg_binder.cc,  src/tir/transforms/storage_rewrite.cc",0
Incremental type inference (#6900),5
[µTVM] Allow for platform-specific malloc in runtime (#6948),5
[Auto Scheduler][Auto TVM] Fix infer tile size for NHWC winograd (#7068),0
[AutoSchedule] Compatibility improvement with XGBoost v1.3.0 (#7069)* [AutoSchedule] Compatibility improvement with XGBoost v1.3.0* lint,5
[TFLite] added scalar axis value handling in reduce (#6970)Axis value in reduce can now be specified as scalar,1
[AutoScheduler] Delete deprecated file auto_schedule.py (#7071),2
[FIX] Remove debugging print statement (#7072)Somehow a unnecessary print statement was left in the codebase.,0
[Relay] Support deformable Conv2D NHWC (#7075)* [Relay] Support deformable conv2D NHWC* add test case* fix lint* lint,0
Fix QNN type inference (#7074)* check for incomplete types in QNN Relation functions* add regression test from #7067* respond to review comments,0
[AutoTVM] Compatibility improvement with XGBoost v1.3.0 (#7076),5
[Relay][Strategy] Allow cuda cross compilation without physical device. (#7063)* Allow cross compilation of cuda targets without physical device.* Formatting.* Add warning when architecture cant be found.* Use target instead of autotvm arch specification.* Change warning message.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,1
[VTA] update 3rdparty submodule (#7081)* update vta* remove tvm,1
#7058 [Tutorial] Import errors in deploy_detection.py and deploy_classification.py (#7059)* Update deploy_classification.pyChanged import paths* Update deploy_detection.pychanged import paths and stop layer id* Update deploy_classification.pySet working directory to TVM base.* Update deploy_detection.pySet working directory to TVM base. Changed layer id in comment to 186.* reverse changes in deploy_classification.pyReversed changes in import path. Imports working as expected.* Reverse change in import pathsImports working as expected.* Update deploy_detection.pyDuplicated graph_runtime import removed.,0
[VTA][OpenCL] add device_annot support in graphpack (#6125)* add device_annot support in graphpack* on_device annotation* lint* typo* fix lint* fix lint,0
Handle case where ListConstruct makes a python list which is output of whole model (#7088),5
Add softplus operator conversion to Onnx. (#7089),1
Add test for MergeComposite on a QNN graph (#7080),1
Fix winograd infer tize (#7092),0
"Rollback changes to SSA begin/end scope for Store in (#7073)C codegen. Instead, scope binary operator codegen inCUDA to fix the issue originally addressed by 5f4b9a9.",0
Fix missing header inclusion. (#7097),0
clean standalone CRT files in microTVM VM rebuild script (#7095),2
[metal] support int64 (#7105),5
"Update tune_relay_vta.py to support single board (#7100)- support single pynq board run, change is credited to 'https://github.com/i24361's change, https://github.com/i24361/incubator-tvm/blob/0472b1f347976229a29be8a6e60b626a0604c8df/vta/tutorials/autotvm/tune_relay_vta_with_one_board.py- fixes the save fail- issues and changes are discussed in https://discuss.tvm.apache.org/t/vta-workaround-for-autotuning-with-one-pynq-z1-board/8091/9",0
"[ONNX] NMS in ONNX (#6839)* NMS partially working on CPU, fails on GPU* support dynamic iou_threshold* WIP NMS with while loops* working nms with dynamic shapes* add a test with dynamic score_threshold and pass it* Fix type checking in lambda lift* ONNX NMS working on GPU, had to remove threading from some kernelsfix lintfix lambda lift testsfix unit testsrespond to review commentsfix lint* better parallelize get_valid_counts* improve nms parallelization* respond to cuda/thrust enablement issueCo-authored-by: Jared Roesch <roeschinc@gmail.com>",0
[TOPI] sparse_dense Op sparse_data input added  (#6889)* [TOPI] sparse_dense op sparse_data input added* [1] clang issue resolved* [2] python format resolved* [3] lint error resolved* [4] Review comments handled* [5] Lint error resolved* [6] Review comments handled* [7] Review comments handled* [8] Review comments handled,0
[ONNX] Fix a bug with reshape imports when an initialized target shape is used more than once (#7109)* Fix a bug with reshape imports when an initialized target shape is used more than once* run autoformat,0
[COMMUNITY] New reviewer @leandron (#7112),1
[Relay][ConvertLayout] Support deformable conv2d (#7087)* add test case* fix* support* test case* fix* fix test* fix bug* add comment,0
[metal] update language version (#7116)* [metal] update language version* fix mps,0
[Relay][Frontend][Onnx] Auto extract onnx input shapes when possible. (#7115)* Auto extract onnx input shapes when possible.* Remove shape dict definition in tvmc.,4
"[metal] Remove support of `double` type (#7118)According to latest Metal spec (v2.3), `double` is still not supported.",3
[TF frontend] add support for StridedSlice to input a single constant (#6949)* [TF frontend] add support for StridedSlice to input a single constant* add test for strideslice with a single number input* fix bug,0
"[BYOC] Added ""include_non_call_ops"" parameter to AnnotateTarget pass (#6655)* [BYOC] Added annotate_non_call_ops parameter to AnnotateTarget passAdded annotate_non_call_ops parameter to AnnotateTarget pass to preventnon-call to be promoted to previously annotated operationsThis is useful in case if you are not running MergeCompilerRegionspass after AnnotateTarget.* linter* Tuple and TupleGetItem handling* resored transform.py, added missing tests to main* requested changes",1
[CI][ACL] Switched to ACL 20.11 (#7106),5
Add autoscheduler support to tvmc (#7070)* Add autoscheduler support to tvmc- Add an autoschedule module to tvmc- Extract common tuning option between autotuner and autoscheduler- Add testing* Linting and small bug-fixing* Addressing comments and refactoring* Fix linting* rebasing* Addressing comments - 2* Addressing comments -3Change-Id: I207872757473210681d9db04bfdcd2c5e6deaa05* Addressing comments - 4Change-Id: I11f73c9b32e83c013cfb2224ccce06f60a128af7,0
[TOPI] Fix GPU Dynamic Op Schedule (#7117)* Fix GPU dynamic op schedules* Fix dynamic shape nms* Fix* Fix test format,0
"[Relay][Topi]Add Sort Op to Relay (#6978)* Add sort op to relay* fix lint* fix sort docstring* fix docs* add TODO, shape_func, cleanup* add dynamic tests for sort and argsort",0
Fix spelling in some comments (#7124),0
Add ACL testing to the CI for AArch64. (#7122)Add testing for ACL to the CI for AArch64. A PR followsto add this to the Jenkinsfile once the docker changes land.We also need a separate script to run the tests as the fullintegration tests are currently broken.,1
"Add `is_floating_point` and `div_` PyTorch ops (#7128)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* Revert changes to tests* reintroduce newline* Fix style",0
"Fix a bug in batch_matmul that te.max should be used (#7111)* Fix a bug in batch_matmul that te.max should be used* Additional fix to batch_matmul- add to this PR, https://github.com/apache/tvm/pull/7111* Remove previous change for numpy test batch_matmul* Add test to dynamic batch matmul* Fix what clang-format flagged* Skip dynamic batch matmul test on cuda- error is found during test as shown below``` File ""/home/jojo6174/tvm-installation/tvm/src/tir/analysis/verify_memory.cc"", line 202RuntimeError: Memory verification failed with the following errors:```* Skip dynamic batch matmul test on nvptxCo-authored-by: Insop Song <insop.song@gmail.com>",0
[Frontend] Unnecessary default warning msg changed to debug (#7119),0
"Update `is_floating_point` to handle bfloat16 (#7133)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* Properly handle bfloat16 in is_floating_point* Revert test changes* revert whitespace changes",1
"[CONTRIB] PopenPoolExecutor (#6959)PopenPoolExecutor implements a ProcessPoolExecutor backed by popen.- Only handles invoking functions in tvm namespace.- Unlike multiprocessing, does not require __main__ block,  which means it can directly run on jupyter notebook.- Come with timeout and fault tolerant support to timeout  long running jobs, and restart the process when an error happens.Recommended usage: it is recommended to create a pool and reuseit in a long running job(e.g. autotuning) so that the processare reused when possible.",0
Added additional information to the from_onnx tutorial (#7127),1
[CUDA] Parallel Cuda Mergesort (#7099),5
[TFLite] add support for float16 (#7093)* [TFLite] add support for float16* add testi case* add test case* add comments,1
[TOPI] Simplify GPU NMS IR and optimize a bit (#7136)* remove get_valid_counts from pytorch nms* fix pytorch nms for negative score* merge reset by -1* move max_out_size handling to triangle loop* update torch nms test* fuse the last two kernels* parallelize the first kernel* merge first and last kernel* remove unnecessary cases* fix typo* revert pytorch frontend change* fuse rearrange step with triangle loop* fix max_output_size handling* check if already surpressed* fix topi vision test by wrapping tir const around int argument* fix for num anchors = 0 case* fix missing zero init of num valid boxes when the input is empty* add some comments and missing doc* typo fix* add a guard against zero dim grid / thread block inside ir_buidlder* typo fix* trigger CI,0
Fix a few OpNode argument field descriptions when registered (#7140),0
"Created CSourceMetaData module for model metadata (#7002)* Created CSourceMetaData module for model metadata* Currently, there is a MetaData module to capture constants  conditionaly if the runtime modules implement const init  PackedFuncs. However, this one relies on a load process  in which the metadata is created on volatile memory that  may be not usable in uTVM environments.* There is a need for model level metadata that is valid  across all runtime modules such as the func registry  when creating a system-lib.* This commit implements a CSoureMetaData module to hold  func registry that collects function names from the  runtime module and generates a c source file to be  linked with final artifact.* Modified and added export_library for utvmChange-Id: Ie2e8e2aea1a66520f03fe8af7cc5bdf27339ea10* Created CSourceMetaData module for model metadata* fixed llvm_module to return null pfs for  get_symbol and get_const_varsChange-Id: I84810e0695d4d6fb314af2469117f965eed71b51* Created CSourceMetaData module for model metadata*fixed bundle_deploy testsChange-Id: I0d1332a4abbb6830531784c59264021bbbd7148a* Created CSourceMetaData module for model metadata*fixed export_library not to insert ""options"" when targeting tar*fixed unit testsChange-Id: Ia1686889498b71af66f1a0311a059154ad3c2c3e* Created CSourceMetaData module for model metadata* enable wasm to support csource metadata module* disabled non DSOExportables from using csource metadata moduleChange-Id: Ie09beaad35cbc2ef738d1d24d91e249b5e099569* Created CSourceMetaData module for model metadata* changed const pfs to be called only on external modules  or DSOExportable modulesChange-Id: I6ad28f166c0fc27a2548c851bf9287ec805550d1* Created CSourceMetaData module for model metadata* CSourceMetadata module wrapper is only created for c/llvm targetsChange-Id: I13cb4140c17e2e1f91d495b15a1ff7eeab9fb14d* Created CSourceMetaData module for model metadata*target should be defined to use csourcemetdata moduleChange-Id: Id8e55b23d0007a79c550334de2c0fec63d40171f* Created CSourceMetaData module for model metadata* reinstate llvm func registryChange-Id: I53e0754b6fb533637f08b25e98064d8c04092de4* Created CSourceMetaData module for model metadata* addressed comments and fixed bugsChange-Id: I26401685dc803aeaf7642c865df88d683419e859* Created CSourceMetaData module for model metadata* addressed a missed commentChange-Id: I65e65c30bc780a946f3f1b8372c40a49a5c20582* Created CSourceMetaData module for model metadata* te build interface should only include c-source metadata if  targetting ""c""Change-Id: Ie23cb8c6231c1f2de6d2827084774e3510288098* Created CSourceMetaData module for model metadata* c_source modules should be created only if they are  non-DSO exportableChange-Id: I53f2f8e9caa41f133446f8881b9dc541ebeee8cc* Created CSourceMetaData module for model metadata* documetation misalignment in source_module.ccChange-Id: I83e2c29b1f2980ca65a694304720dc58a5cb7879* Created CSourceMetaData module for model metadata* typo : same object file written as a dependency in the MakefileChange-Id: I8becc4196d286cfb6372768687b3c836799dcb78* Created CSourceMetaData module for model metadata* removed unused param from a briefChange-Id: Ie4db2aca3b7ea147bd8c65ef5d1cc2146f530e76* Created CSourceMetaData module for model metadata* made export library use c as the format for c source modulesChange-Id: Ie2fd6204414f0fa43988a8082d18af7a3225e237* Created CSourceMetaData module for model metadata*addressed a nitChange-Id: I6084b8c06ddfaaece295439dbab589e6e202b664",0
[COMMUNITY] @jcf94 -> Committer (#7141),3
[Auto Scheduler] Mali Support (#7132)* [Auto Scheduler] Mali Support* Fix doc* fix lint* address comments* fix doc,0
"Add `is_floating_point()` test and better type support in `verify_model_vm()` (#7134)* Add div_ and is_floating_point operators* Add handling of exprs to op, update tests* add test + supporting functions* Revert whitespace changes* Properly assign dtype to random integers* Reformat with black* Switched default dtype logic, removed extra line",1
[TFLite] pack operation extedned with const args (#6984)pack operation now accepts constant arguments,5
[BYOC] [ACL] include_non_call_ops = False (#7121)ACL codegen now uses AnnotateTarget pass with include_non_call_ops = Falseto prevent promoting non-call ops under the target of its arguments.Squeezenet unit test added.,1
[Rust] Impl IsObjectRef for Array (#7138)* impl isobjectref for array* array test* cargo fmt,3
"Add a FunctionPattern, remove unused attributes in CallPattern (#7151)* Add a FunctionPattern, remove unused attributes in CallPattern* update docs",1
[AutoScheduler] Improve SearchTask and ComputeDAG serialization (#7145)* Use self.dag in Python object* Add sch to ComputeDAG* address comment,1
[AutoScheduler] Support string processing to records (#7144)* [AutoScheduler] Support string processing to records* doc* remove log,2
[TOPI] cuda reduction schedule (#7131)* complex reduce* fix* fix* fix,0
"[TOPI] GPU sort IR refactor to enable sort by keys (#7157)* sort refactor initial import* sort test working* scatter 1d with positive indices working* remove negatiev indices, using extern for now* minor fix* minor fix* add sort by key test* revert scatter change* add document* fix py formatCo-authored-by: masa <masa@pop-os.localdomain>",0
"Support mode=instance, spatial for MXNet l2_normalize (#7062)",2
[AutoScheduler] Python based measure callbacks (#7143)* add* make it work* format* add poilcy* comment* move test* format* fix ci* Delete useless old codeCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,0
Slight optimize the default injective schedule (#7158),5
[AutoScheduler] Add layout rewrite support for dense and batch matmul on CPU (#7161)* [AutoScheduler] Add layout rewrite for dense and batch_matmul* Fix test & Address comments* Fix shape inference* fix test,0
[Relay] Add fast_softmax (#7163)* [Relay] Add fast_softmax* fix* fix,0
[AutoScheduler] Fix the conflict of thread pool in measurement (#7166),0
[Torch] Fix PyTorch NMS conversion for negative scores (#7137)* Fix pytorch nms conversion for negative scores* updated mask rcnn test to verify outputs and also run cuda target* set rpn_post_nms_top_n_test to 200* fix parameter name* dump output box information* simplifying,0
[Doc][AutoScheduler] Improve hyperlinks in tutorials (#7167)* [AutoScheduler] Improve tutorials* fix lint* address comments,0
[AutoScheduler] Enable winograd for conv2d and layout rewrite for conv3d (#7168)* [AutoScheduler] Enable winograd for conv2d & Enable layout rewrite for conv3d* fix test* fix test* update tutorials,0
[Rust] More Rust bindings for Attrs (#7082),5
[FIX] Fix using num_workers in omp (#7078)Co-authored-by: zhangfucheng <zhangfucheng.jason@bytedance.com>,0
Update the docs stale links (#7169),1
[AutoScheduler] Update layout rewrite option setting for measuring (#7156)* Add layout rewrite options for measure* Update schedule for inserted transform stage* Set layout rewrite when tuning for network* Update the log version,1
[µTVM] Add platform timer and RPCTimeEvaluator to enable AutoTVM (#6964)* Add platform timer to microTVM.* Address liangfu comments* cppformat* clang-formatCo-authored-by: Liangfu Chen <liangfu@apache.org>,1
[Torch] Support hard_swish op (#7174)* imp_hardswish* format* fix* hard_swish_inplace test case,0
[TFLite] Reshape - support different qnn params for input and output (#7159),5
"Asymmetric padding and dilation in conv2d workload (#7142)* added asymmetric padding to conv2d workload* fixed depthwise conv2d padding* Added fix to include dilation in workload output width calculation* Added missing dilation to arm_cpu/conv2d_int8.py workload* Fixed dilation for x86 conv2d* Improved dilation workload integration in x86* Fixed x86 conv2d_alter_op to add dilation* Local linting not always producing same output as CI, probably my fault* Fixed bug, tested locally* Abusing CI until I can figure out how to reproduce the same behaviour of running integration tests locally.* Ammeded conv2d_int8 test* Updated workload, improved unit tests* Added depthwise conv2d workload test",0
[Relay][fix] Stack should take exprs that evaluate to tuples (#7130)* Fix stack to take Relay exprs that evaluate to tuples* Doc tweak* Linting fix,0
[AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space (#7175)* [AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space* Update python/tvm/autotvm/task/space.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: ZhaoYanjie <roger.zhao@montage-tech.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[TOPI] Parallelize GPU NMS inner loop (#7172)* make NMS inner loop parallel* use one block two avoid global sync issue* temp disable write by only thread 0* leave a TODO on write by only one thread* add some comments, remove check the check on negative class id* minor improvement when topk is available* fix write by a single thread",0
[AutoScheduler] Use VM to extract tasks for dynamic models (#7173)* use VM for dynamic shape* make it work* add test* finalize* finalize* format* address comment* comment* improve task extraction,1
[AutoScheduler] Fix policy for zero-rank output (#7180),0
[Auto Scheduler][fix] Add dense strategy for mali (#7181)Signed-off-by: leowang1225 <810916296@qq.com>,0
[Relay][Op] Remove reverse attribute from reshape and reverse_reshape operators. (#7086),4
Parallelize cumsum in get_valid_counts (#7123)* Parallelize cumsum in get_valid_counts* make the scan loop exclusive* switch to directly using exclusive scan* perform inner loop of final writes on anchor threads* fix flaky testfix lint* remove final cuda kernelCo-authored-by: masa <masa@pop-os.localdomain>,0
[Fix] Tensor core type issue for dense (#7187)* fix tc type issue for dense* fix lint* rm float 32Co-authored-by: Leyuan Wang <ziyu.guo@bytedance.com>,0
"Remove seemingly invalid SoftPlus (#7189)- `Softplus` is added in 12/10/2020 from this https://github.com/apache/tvm/pull/7089- However, I see that there were `SoftPlus` (not the P is in capital) was already in.According to [Onnx spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md), it is `Softplus` not `SoftPlus`.",1
"[Frontend][MXNet] add _npi_subtract_scalar (#7191)* [Frontend][MXNet] add _npi_subtract_scalar- add mxnet numpy operator, subtract- https://github.com/apache/tvm/issues/7186- https://mxnet.apache.org/versions/master/api/python/docs/api/np/generated/mxnet.np.subtract.html* Fix python style using black",0
"Makes sure g_last_error is null terminated. (#7190)This addresses GCC 10 error:```""src/runtime/crt/common/crt_runtime_api.c""include/tvm/runtime/c_runtime_api.h: 在函数‘TVMAPISetLastError’中:src/runtime/crt/common/crt_runtime_api.c:42:3: 错误：‘strncpy’ specifiedbound 1024 equals destination size [-Werror=stringop-truncation]   42 |   strncpy(g_last_error, msg, sizeof(g_last_error));      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~cc1：所有的警告都被当作是错误```",0
Fix ICHECK_NOTNULL in logging.g (#7193),0
Fixed temporary lock_guard instances. (#7199),0
"[CUBLAS, CUDNN] Support dynamic batch size (#7194)* support cudnn and cublas on dynamic batch size* added test for cublas* add comment on algo choice",1
ReshapeAttrs no longer has reverse (#7205),5
[ConvertLayout] slice_like support (#7184),5
[AutoScheduler] Add custom build function (#7185)* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* cheduler] Add custom build function* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>* [AutoScheduler] Add custom build functionSigned-off-by: leowang1225 <810916296@qq.com>,1
Fix prelu bug in onnx frontend. (#7208),0
"[PatternLang] Add Syntatic Sugar to the C++ pattern API and support DataType Attribute Matching (#7120)* Add Syntatic Sugar for C++ Pattern API, Support DataType Attribute match* add missing tests* fix lint* fix license edit* fix bad rebase",0
[µTVM] Raise a better error when project_dir does not exist (#7165),0
Allow condition in if op to be an array. (#7215),5
"[Frontend][MXNet] add _npi_stack, issue #7186 (#7209)- https://github.com/apache/tvm/issues/7186- add MxNet stack, `_npi_stack`- https://mxnet.apache.org/versions/master/api/python/docs/api/np/generated/mxnet.np.stack.html?highlight=stack",1
[Fix][Autoscheduler] Costmodel enhancement & bug fix for graph debug runtime (#7197)* Enhancement for autoscheduler cost model* Bug fix for graph_runtime_debug* Update* Lint fix* Update* Update* Add file exist check for cost model load* Update* Update* Lint fix* Update* Bug fix,0
"[RELAY] Fix reshape header file (#7218)The header file definition of InferNewShape wasincorrect, this patch fixes it.Change-Id: Id24b8eccb52323692fe88bdda46cc49cba54588c",0
[µTVM] Add documentation (#7164),1
[TIR][REFACTOR] Enforce allocate to use the correct var pointer hint. (#7216)* [TIR][REFACTOR] Enforce allocate to only accept buffer_var with correct PtrType.This is a refactoring step to cleanup legacy issue of opaque buffervar without ptr type information. Now all the allocation comes with the rightpointer data type. Places touched:- TVMScript Parser: add the right info to get the correct pointer type.- Cross thread all reduce: set the right pointer type.- Storage rewrite: setup the right pointer type.- Custom dtype: remap the variables with new pointer type.x* Address commentsCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>,1
[AutoScheduler][Relay] Control compile engine cache via PassContext (#7220)* [AutoScheduler][Relay] Control compile engine cache via PassContext* lint* lint,4
[Arith] Simplify cast (#7045),5
[ConvertLayout] Support transpose (#7214)* [ConvertLayout] Support transpose* format* fix ci* fix axes missing* fix* fix NCHW[x]c* Update src/relay/op/tensor/transform.cc* fix negative* fix,0
"[BUGFIX] Change debug_runtime to represent times in seconds internally (#7227)* Add FrontendTestModule, a Module which can have Python functions.* fix units and use of scientific notation in debug_runtime variable names* remaining updates to formalize debug_runtime returns time in sec* Add test for debug runtime output* black format* git-clang-format* pylint",0
Fix Get Valid Counts when the number of boxes is zero (#7229),0
[CI] make sure submodule checkout in clean state (#7228),5
[AutoScheduler] Do not return naive schedule in tracing mode (#7226)* [AutoScheduler] Do not return naive schedule in tracing mode* lint* fix,0
"[RELAY,TOPI] Threefry PRNG: splittable and stateless (#7083)* [RELAY,TOPI] Threefry PRNG: splittable and stateless* Fix sphinx?* Lint fixes* sphinx fixes round 2* fix inputs for tests* reorganize to random, fix uninitialized memory bug* silence linter* silence linter even further* s* strengthen Threefry key type checking, add tests* replace static variable with function for Threefry key type* lint fix* Remove old todos, improve assert messages* describe how random number is generated* add tests for incorrect output size. also vary test sizesCo-authored-by: Altan Haan <ahaan@octoml.ai>",0
[TOPI] Treat undefined elements as constants in Array (#7232)* [TOPI] Treat undefined elements as constants in Array* Add a checker* fix* add test case,0
"Revert ""[AutoTVM-FIX] avoid unexpected value(1) of search space when get length for uninitiated search space (#7175)"" (#7236)This reverts commit f2ab977de0ac543cae77d3bef76af1b56dd61eed.",0
[AutoTVM] Add index boundary check in ConfigSpace.get() (#7234)* [AutoTVM] Add index boundary check in ConfigSpace.get()* Fix unit testCo-authored-by: Yanming Wang <yanmwang@amazon.com>,0
[CUDA]batch_matmul tensorcore schedule (#7146)* add batch_matmul_tensorcore* add bmm cublas autotune* add bmm tests* out_shape for bmm_tensorcore* fix comments* code format* add todos for tensorcore datatype checking* fix lint* fix have_tensorcore* add dtype check for batch_matmul_tensorcore,0
[TFLite] Quantized version of unit test for Dense (#7113)Added quantized version of unit test for FullyConnected/DenseAdded check for -1 in case if bias not supplied,1
"[BYOC][ACL] Depthwise convolution support (#7206)* [BYOC][ACL] Depthwise convolution supportAdded support for depthwise convolution. ACL only supports depth-wise convolution when kernel size is 3x3 and 5x5 and strides are (1, 1) or (2, 2), if this is not the case then fallback to TVM.Also rework tests to remove non-deterministic trials.*Compute Library for the Arm Architecture (ACL).*All credits to Luke Hutton @lhutton1Change-Id: Ida1f5802a65377b84325edf14a0149242c1af857* linter* CHECK -> ICHECKCo-authored-by: Luke Hutton <luke.hutton@arm.com>",1
"[FIX,TUTORIALS] Import tvm.testing in tutorials that use it (#7248)",0
add default value for leaky relu alpha (#7259),1
[ONNX] Fix issues for Clip and RoiAlign (#7237),0
Do not use ICHECK in nnvm (#7255),5
Fix TRT weight conversion when first dim of weight shape is 1 (#7253),0
Add op_name in error message for Pool (#7243)* add op_name in error message for Pool* fix tiny issue for arguments* fix tiny issue for LpPoolCo-authored-by: luyaor <luyaor@luyaordeMacBook-Pro.local>,0
"Remove check_correctness in AutoTVM, which is busted (#7250)",4
[Torch] Restore class-aware NMS for detection models by graph rewrite (#7154)* add a pattern to rewrite nms to batched nms* update object detection test to add rewrite* updated tutorial* add doc* fixed coord_start* test fixed by setting force_surpress=False* revert tutorial change* add some comment to explain the pattern* update NMS pattern following frontend change,0
"[THRUST] Faster multi dimensional argsort by segmented sort (#7195)* remove sort nms* add segmented sort by key impl* bug fix, test pass* updated fast path condition to work for all dims",0
"Unpack NMS inputs into bbox, scores and class ids (#7257)commit fe8fda81774c2e1a4d434179f62e3a299e084cb7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 30 20:31:29 2020 +0900    fix write by a single threadcommit 0c21e36d58f81adeedec1749aeb04ed4e93a7f36Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 04:32:18 2020 +0900    minor improvement when topk is availablecommit 68c686617c818a81f31c6696c99c5dae68405becAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 04:10:24 2020 +0900    finish concat outputcommit 37d7a198010a7bfef85158bbc22b6673e43b2973Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Dec 29 03:59:28 2020 +0900    fixed topk handlingcommit 1913f9764dc5987deb2c6228112c18b98533831cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:34:24 2020 +0900    more refactoringcommit 70c65f099da7cf8a18ffbaadadbd6dc814a804feAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:27:15 2020 +0900    unpack input datacommit 3a273975b1456991fd3f70e055cd5f7c2cdd79feAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 21:22:16 2020 +0900    slight change to initializationcommit 9b42008b42004f5f05cdaa51e2f6feeadf99abb1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:50:36 2020 +0900    add some comments, remove check the check on negative class idcommit 0aa375d67ad14cae8431958e17d1901dd94d1f6bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:39:49 2020 +0900    leave a TODO on write by only one threadcommit d75ee0a62b8e2fb8912ff226ea8bedb8ed78764dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 28 19:13:04 2020 +0900    temp disable write by only thread 0commit 20b563031adf56f93a7bcfe5b853c477175f4f80Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 10:06:43 2020 +0900    use one block two avoid global sync issuecommit dd1e23068f6fdadc5cb3c3a1872c3fff42f4e2eaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 07:59:19 2020 +0900    make NMS inner loop parallelfix write by a single thread",0
"[µTVM] Avoid listing links when probing serial ports (#7265)SerialTransport.open() probes automatically the device name based upon agrep regex if a device name is not provided. The code expects to find onlya single device. Currently when it probes for the available serial ports itincludes in the list the device names that are also symbolic links.Since _find_openocd_serial_port() always returns a serial number for agiven serial port (not the device name path) the available device namesare always probed when the openocd flash runner is used.It's not uncommon that device drivers create symbolic links for certainkinds of serial devices, specially those that provide a serial port plusan additional endpoint to program the device attached, like a ST-Linkinterface, etc.As a consequence the current code fails to select the correct device namewhen symbolic links exist and the openocd flash runner is used.That commit changes the probe behavior to avoid listing symbolic links whenprobing the device name for the target serial port.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
[Frontend][TFLite] Densify Op added (#7048)* [Frontend][TFLite] Densify Op added* [1] Review comments handled* TODO added for sparse_to_dense Op usage* stale comments removed,1
Change the all #pragma once to ifdef include guard (#7264),4
"Reorder dynamic to static and simplify inference, lower DynamicToStatic Opt Level (#7213)* reorder dynamic to static and simplify inference, add a dropout unit test* lower dynamic to static opt level* autoformat test* raise DynamicToStatic to opt level 2 to match Constant Folding",1
[DOCS] Fix figure links (#7268),0
"[µTVM] Fix two warnings when deprecated forms are used (#7269)* [µTVM] Specify loader for yaml.loadSpecify the loader to be used by yaml.load as the current form used withoutspecifying explicitly a loader is deprecated since PyYAML 5.1 and willthrow a noisy warning.For details, please see:https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-DeprecationSigned-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Avoid using tvm.target.createAvoid using tvm.target.create as it's deprecated and usetvm.target.Target directly instead.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
Adding aten::unsqueeze_ to PT Frontend (#7231)* Added Ops* Regular* Remove copy* Remove copy* Tests* BlackCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-149.us-east-2.compute.internal>Co-authored-by: Ubuntu <ubuntu@ip-172-31-19-34.us-east-2.compute.internal>,1
update vta-hw version (#7271),1
[FIX] Remove leftovers from check_correctness (#7272)* [FIX] Remove leftovers from check_correctness* remove unused numpy import,0
[CUDA] [Codegen] Ensuring atleast one thread block for dynamism (#7273),5
[AutoScheduler] Fix layout rewrite for axis with extent=1 (#7279),0
[AutoScheduler] Fix typos in feature extraction and cost model (#7280),0
[PatternLang][Bugfix] Ensure CallNode attrs are not undefined before checking (#7278)* Correct handling of call node attrs to handle non-operator calls (attrs may be undefined)* Linting fix,0
switch to more portable bash pipeline syntax (#7274),2
Add MicroTVM support for the STM32F746 Discovery board (#7225)* Add MicroTVM support for the STM32F746 Discovery boardSigned-off-by: Tom Gall <tom.gall@linaro.org>* Add reference to the discovery board in the docsSigned-off-by: Tom Gall <tom.gall@linaro.org>,1
fix mcpu on os x (#7276),0
[PatternLang] Add If pattern (#7282)* Add if patterncommit 1ee052fd494a5bdd881c242c3ea0c95cf2a613e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 22:19:17 2020 +0900    add commentcommit c846a6999e9c9e48fbc019780e705a990f46cb22Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 21:14:20 2020 +0900    max_out_size rewrite added to the testcommit 2c7c7fbd0e6563aba694e7fb6baa7bda8e4fadcaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 20:57:55 2020 +0900    max_out_size rewrite workingcommit 319e930acb8162c1ec4a5d4fb71d134580a68f13Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 20:43:16 2020 +0900    refactor dyn strided slice patterncommit fb6917b703440748800bde624bc20efaf5798b8aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 11:21:33 2020 +0900    update NMS pattern following frontend changecommit 255a98f1da8f300d4fe417cce3587c0d71e38ed3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 24 05:19:31 2020 +0900    add some comment to explain the patterncommit 52cea1cc2bff533ca60acfc2416477fc8b058428Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 08:35:14 2020 +0900    revert tutorial changecommit d3e0e0d7e2427c40067d6ad2680ec5b3f0076223Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 08:02:29 2020 +0900    test fixed by setting force_surpress=Falsecommit 2fa1a574f932001be2d8f601338a342dab92f79cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 07:22:32 2020 +0900    fixed coord_startcommit 6ba88f27dec1bdb0b0ba746c268591a59264088eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 06:50:46 2020 +0900    add doccommit 8d386b6a1c92ce4fe3349ff20e320199a1b5b310Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:27:26 2020 +0900    updated tutorialcommit 3206b49ecfdd874e0ff8feb0fa586c4c4282f705Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:04:44 2020 +0900    update object detection test to add rewritecommit 74bebb2f4376aeb67d8c4aad395f9f2661fe6b3eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Dec 23 05:02:15 2020 +0900    add a pattern to rewrite nms to batched nmscommit f410e6dde0ed949b90312c5a7ddbb6c234f9acc1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 22:20:16 2020 +0900    add commentcommit f1e078b0724bd22e7be0a812055e1c7c650d94daAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 26 19:54:22 2020 +0900    Add if pattern* add doc* add test* doc formatting* cpplint fix,0
[Frontend][Tensorflow] Sparse_Dense Op CSR scheduling issue resolved for Cuda & X86 (#7148)* [Frontend][Tensorflow] Sparse_Dense Op CSR scheduling issue resolved for both cuda & x86* [1] Review comments handled* [2] Review comments handled* [3] Review comments handled,5
[BYOC][bugfix] Handle empty tuples in annotation pass (#7288),0
"[µTVM] Add ST STM32F746 disco board to tflite tutorial script (#7254)Currently tutorial script 'micro_tflite.py' assumes that all boards withtarget STM32F746 are Nucleo boards. As a consequence once that target isselected the script automatically defaults to the Nucleo board. However,the STM32F746 is also used on Discovery Kit boards (aka disco) which arequite similar but have some differences, so Nucleo config and final imagedon't work on the disco boards.That commit adds a way to select a different dev board and adds commentsaccordingly, informing how to use the script with STM32F746 disco boards.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
Bring back numbered lists to TVM docs. (#7290)* Upstream fix in https://github.com/tlc-pack/tlcpack-sphinx-addon/commit/995178d81e6e38eabbc28da2b285b68583c88769,0
"[VM] Per-input, data dependence specification for shape func (#7210)* made TShapeDataDependant array* add stub* dyn strided slice working* reshape also working* remove log* works on maskrcnn* lint fix* fix cpp test* remove stale pop back* add more doc* dependant -> dependent* remove redundant check* remove data_dependent_",0
"[uTVM] Initial BYOC support with c-source module (#6950)This commit mainly introduces a byoc c-source moduleexample to uTVM. Moreover, it carries certain modificationsto the example codegen_c external module generator codeto generate utvm friendly c-source.Change-Id: I09f3a42017d518dd5b6c89e3fe0a0332b80088b0",4
A few typo fixes in the uTVM design doc. (#7291),0
"Change const to used dtype if it is passed in (#7285)* Add fix and unit test for const autoconvert dtype.* formatting* Address review comment, casting input value to int32* Fix failing test* Augment unit test",0
"[TEST] Fix test_topi_batch_matmul_tensorcore.py:test_batch_matmul requirement (#7294)* this test current sets a requirement to ""uses_gpu"", which   causes it to fail in cpu-only machine * this patch changes it to be ""requires_tensorcore"", as per discussion   on issue #7277",0
[TIR] Support Return in TIR (#7084),5
Add QEMU setup to uTVM tutorial. (#7296),1
[TUTORIAL] Add gpu instructions and results to deploy_sparse (#7298),1
[COMMUNITY] @Laurawly => PMC (#7307),3
[AutoScheduler] Bug fix & Custom sketch support (#7260),0
[TIR][REFACTOR] ForNode introduce thread binding and remove legacy field (#7306)[TIR][REFACTOR] ForNode update- Remove deprecated device_api.- Add ThreadBinding for_type.- Add additional annotations.More style consistency refactor to make the ForNodeto be consistent with rest of the codebase.- ForType => ForKind- Add constant prefix k to enum consts per Google C style- Introduce ForKind to the python side.,0
[Relay][Frontend][Onnx] Compare against onnxruntime more consistently during testing (#7300)Co-authored-by: Josh Fromm <jwfromm@uw.edu>,2
[TOPI] Minor perf improvement for GPU scatter (#7233)* improve scatter 4d init* do not launch sorting based scatter for small input* do not use hard coded num threads* separate sort based implementation* register scatter as autotvm task* add missing import* fix strategy* add dedicated schedule and dummy flop* add test tuning script* try adding dummy knob* skip random_fill when a tuning workload is from scatterThis reverts commit 1fed88321e640b509fc46fac7da3b3cb79719552.* cleanup memcpy ir* remove scatter tuning script* make sure zero init arguments* add comment on why skip random init for scatter* restore ctx syncCo-authored-by: masa <masa@pop-os.localdomain>,0
[TFLite] Added ability to infer shapes for arguments (#7293)Added an ability to infer argument shapes if shapes are not present inTFLite files. The set of networks on which the patch was tested isinternal to Arm. Any help with creating unit tests would be appreciated.,1
"[TOPI] Make cumsum IR reusable, add thrust scan (#7303)* import changes from scan branchcommit cf0d4fdf3bf8fa6e1d6abf631042de28176923c3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 10:12:01 2020 +0900    get valid count test workingcommit eb142d3ee9bb16ddf8d37fdec10c1bcda209deaaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 07:22:00 2020 +0900    integrate new cumsum changecommit f89684d73dad1f863b4fd291e8804b5c24eae94fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 25 06:56:46 2020 +0900    remove ceil_div from nmscommit a2ad4dea87d9a637745fb0a40ff9bbdde286194aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 20:36:34 2020 +0900    add api for returning reduction from ex scan outputcommit b7f4ef7006b722e365533bec53b1f104aa056da2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 19:49:07 2020 +0900    move ceil_div to utilscommit a9a57e34317b1f254165c3a88e465e33c7fda01bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 20 19:38:15 2020 +0900    rename prefix_scan.py to scan.pycommit 03ed43ff550a435a28740ce1fa62cea71b90cf2cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 19 06:12:55 2020 +0900    surpress cpplintcommit abceac980d8dfd94072acc228108d1fcd94a214cAuthor: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:36:24 2020 +0900    support more data typecommit 3e7d1f81821a1e221cbb1322ef5b23f273f51c42Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:09:51 2020 +0900    1d thrust scan workingcommit ac13b407e21a83ca57240cad205c32a5d000f999Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 19:49:25 2020 +0900    adding thrust scan supportcommit 65634e86c33786541485dc6461a96da833332297Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 19:01:11 2020 +0900    add thrust scan python stubcommit 9876c901ee8b406bc9d75ba91c4734d55f85811bAuthor: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 20:55:14 2020 +0900    introduce prefix_scan.py and move scan ir in nms.pycommit 667bdd3b135a03b53937fdb664915e07f1365ee1Author: masa <masa@pop-os.localdomain>Date:   Fri Dec 18 15:06:18 2020 +0900    make the scan loop exclusivecommit 480787bc072bfc59dcc279038c772f8ad2ec03e9Author: mbrookhart <mbrookhart@octoml.ai>Date:   Thu Dec 17 10:01:11 2020 -0700    Parallelize cumsum in get_valid_counts* fix for 1d scan* rename* cast to out dtype* do not run return reduction for inclusive scan* remove another ceil_div definition* adding scan test* add scheduling for scan op, fixed scan 1d test* pylint fix* add doc string* add more thrust scan test* add dynamic get valid count test, including empty size tensor* fix hard coded gpu targets for cpu only env* try retunring early if scan_size is 0* another change for empty tensor and thrust pathCo-authored-by: masa <masa@pop-os.localdomain>",0
"[BYOC][ACL] removed ACL 20.05 limitations (#7251)Removed checks for padding in according with changes in ACL 20.11*ACL stands for ""Compute Library for the Arm® Architecture""",1
[COMMUNITY] tkonolige -> Reviewer (#7311),3
"[TFLite] Strided slice handling of shrink_axis_mask improved (#6998)* [TFLite] Strided slice handlig of shrink_axis_mask improved1. Added removal of dimensions if result is a scalarto mimic TensorFlow behaviour. E.g.:    tf.strided_slice([1,2,3], [0], [1], [1], shrink_axis_mask=0)    <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>    tf.strided_slice([[[1,2,3],[4,5,6],[7,8,9]]], [0, 0, 0], [3, 3, 3], [1, 1, 1], shrink_axis_mask=7)    <tf.Tensor: shape=(), dtype=int32, numpy=1>2. Added extra check to assert_allclose to check shape equalitiesas np.testing.assert_allclose() does not distinguish between cases like:    np.testing.assert_allclose(1, np.array(1))    np.testing.assert_allclose(1, np.array([1]))    np.testing.assert_allclose(np.array(1), np.array([1]))* unit tests fixed",0
[TOPI] Rewrite GPU argwhere using exclusive scan (#7314)* use ex scan to write argwhere* add doc,1
[COMMUNITY] @jwfromm -> Committer (#7316)* [COMMUNITY] @jwfromm -> Committer* add areas,1
"[µTVM] Add TVMPlatformGenerateRandom, a non-cryptographic random number generator. (#7266)* [uTVM] Add TVMPlatformGenerateRandom, and use with Session nonce. * This change is preparation to support autotuning in microTVM. It   also cleans up a loose end in the microTVM RPC server   implementation. * Randomness is needed in two places of the CRT:    1. to initialize the Session nonce, which provides a more robust       way to detect reboots and ensure that messages are not confused       across them.    2. to fill input tensors when timing AutoTVM operators (once       AutoTVM support lands in the next PR). * This change adds TVMPlatformGenerateRandom, a platform function for   generating non-cryptographic random data, to service those needs.",1
Made tensorflow IsNan actually work (#7320)* Made tensorflow IsNan actually workIsNan was added to tensorflow.rst in fa1b859f but this commit makes IsNan actually work* Added test case for tensorflow.is_nan,1
Fix an issue with dynamic functions overwritting call arg types (#7295)* Fix an issue with dynamic functions overwritting call arg types* fix a bug for un-annotated inputs* normalize names in TypeSolver::Unifier* fix name normalization,0
add a shape function and dynamic test for round (#7324),1
relax tolerance for dlpack test (#7325),3
get_top_results works on a copy of output (#7327),5
[BYOC][Verilator] add support to dynamically load hardware library (#7286)* add files* remove import* remove os import* reorder header* fix header order cpplint* lint fix,0
[AutoScheduler] Fix conv3d's op strategy for auto-scheduler (#7328),0
[PatternLang] Add a relay LetPattern (#7332)* Add a relay LetPattern* fix If copyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix If copyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[FIX,AUTOTVM] Add flop counts to cublas (#7297)",0
add Verilator to CI (#7098),1
"[Tutorial] Autoscheduler on ARM devices (#7326)* arm tuning tutorial* adjustment to get RPC working* fix lint* fix target* integrate Leandros comments* dont request remote in CI* use API from auto_scheduler, not autoTVM and updated comments* make ci-runnable* fix the formatting* address Zhaos comments* full run stats* taking Zhaos comments into consideration",0
[AutoScheduler] Separate shapes from DAG hash and enable schedule sharing (#7317)* [AutoScheduler] Separate shapes from DAG hash and enable schedule sharing* Update CI logs* lint* fix registry* add message; fix layout rewrite mismatch* update message* support other formats,0
"[FIX] Infer input shape in sparse_dense_padded's alter_op if one does not exist (#7308)* [FIX] Infer input shape in sparse_dense_padded's alter_op if one does not existIf there are multiple alter_ops in a model, the first alteration doesnot run type inference for the subsequent ones. In this case, we don'thave the shape information, so we run the inferencer manually.* add todo",0
Fix warning showed with GCC10 (#7336)catching polymorphic type 'struct dmlc::Error' by value,0
[Relay][Training] Add more gradients (#7323)* add more gradients* add documentation,1
fix tanh gradient and update tests to use downstream gradient (#7340),0
[CMake] use wrong flag name (#7341)Signed-off-by: windclarion <windclarion@gmail.com>,5
"Add resource_handle to TVM_DLL_EXPORT_TYPED_FUNC. (#7338)* In #5921, resource_handle was added as a parameter to   TVMBackendPackedCFunc, which is the typedef for functions called by   LibraryModule's function lookup. * It appears TVM_DLL_EXPORT_TYPED_FUNC was overlooked in that PR,   although there don't seem to be any runtime affects known so   far. However, making this definition proper to avoid any compiler   warnings/debug tool problems. * See also https://discuss.tvm.apache.org/t/rfc-misra-c-changes-for-rpc-support/7098/5",0
"[Relay, TOPI] Add numpy style cumsum op (#7334)* Add cumsum relay/topi op* relay tests working* add torch frontend converter* fix for importing detr* fix bad merge* begin cuda cumsum* support non innermost axis* support rank higher than 3* making binop parameter* fix overflow issue in thrust scan* generic binop parameter working* relay test working* fixed for bool input* remove pytorch change* fix pylint* doc update* Update python/tvm/topi/cumsum.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Update tests/python/relay/test_op_level3.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* add example outputs* add supported input and output dtype in thrust log* adding more loop var names* fix cpplint* fix missing check for the cuda target in nms thrust sort* parallelize cpu cumsum* making binop argument tir function* update doc for binop* doc updateCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>",0
Add resource_handle to both TVM_DLL_EXPORT_TYPED_FUNC and TVM_DLL_EXPORT_PACKED_FUNC macros in packed_func.h. This is a patch PR for #7388. (#7343)Co-authored-by: JC Li <jinli@nvidia.com>,1
"[FIX] Don't add $TVM_HOME/.. to the include path when compiling code. (#7342)If the user has a dmlc-core directory next to the tvm directory, thisdmlc-core directory will be incorrectly used when compiling files withcc.py.",0
[PRNG] Add check to PRNG to make sure that unsigned integer arithmetic is wrapping (#7287)* [PRNG] Add check to PRNG to make sure that unsigned integer arithmetic is wrapping* Add threefry_test_wrapping: a manual test for wrapping unsigned arithmetic.* fix test to actually run on the target* formatting* lint,0
[Torch] Various updates for PyTorch frontend   (#7348)* add conversion for detr* remove explicit broadcast_to before batched matmul* use take with wrap mode* add test for transformer and negative indices* add sort and argsort* add logical_and* support masked_select* add gpu targets to masked_select test* improve sort conversion,1
[AutoScheduler] Enable schedule sharing in dispatch context (#7344)* [AutoScheduler] Enable schedule sharing in dispatch context* Update python/tvm/auto_scheduler/dispatcher.py,1
"[Torch] More graph rewrites for Faster RCNN / MaskRCNN (#7346)* add post nms topk to max_out_size rewrite* add argsort conversion* scatter pattern first cut* matching seems to working* dup matching fixed* add converter* conversion seems working* add reshape, use take* remove pytorch argsort converter* update test* add doc",0
[Autodiff] Deterministic gradient compute (#7321)* fix unstable compute* fix* fix* lint* sort linear equation* sort inequalities* fix* fix find* lint* fix find* lint,0
[COMMUNITY] @trevor-m -> reviewer (#7352),3
[Relay][Frontend][Onnx] Robustify Loop Importer (#7353)* Add test for array loop.* Fixed scalar issue.* Formatting.* Fix injective schedule for dynamic shapes.,0
"If an expression has two branches, and the pattern ignores one with a wildcard, allow grouping via dominator analysis (#7355)",5
Fold If when the condition is Constant (#7354),5
Update uTVM code to work with the nRF5340DK dev board. (#7331)* Various fixes to get nRF5340 working. Not yet there.* nRF5340 test runs locally.* Various fixes to get nRF5340 working. Not yet there.* nRF5340 test runs locally.* Add `nrfjprog --recover` for nRF5340DK* Cleanup.* Remove debugging code.* Revert submodule update.* Remove debugging code.* Fix comment.* Remove -keys argument.* Adding some debugging code* Fix passing west command to ZephyrFlasher.* Various fixes to get nRF5340 working. Not yet there.* nRF5340 test runs locally.* Add `nrfjprog --recover` for nRF5340DK* Cleanup.* Various fixes to get nRF5340 working. Not yet there.* nRF5340 test runs locally.* Remove debugging code.* Fix comment.* Remove -keys argument.* Fix merge.,0
[Frontend][Tensorflow] Sparse dense matmul adjoint option added (#7267)* [Frontend][Tensorflow] Sparse dense matmul adjoint option added* [1] Review comments handled* [2] Review comments handled* [3] Review comments handled,1
"[Relay][PatternLang] Bug fix of rewrite func attr (#7358)When using pattern with attr of functions, such attrsmostly does not exist for op node. Therefore, hasattrcheck has to be done for op nodes.Change-Id: Ia313ab34be95ccc793c32fd8e5e5ef566b78685b",0
[RUNTIME] Improve error messages for TypedPackedFunc (#7152)* [RUNTIME] Improve error messages for TypedPackedFunc- TypedPackedFunc now prints the function name when the incorrect number  of arguments is passed.- TypedPackedFunc now prints the function name and which argument when  an argument cannot be converted to the correct type.* check argument conversion by template deducing argument types* switch from template approach to TVMMovableArgValueWithContext* move passes back into cc files* remove error message prefixes* Remove TVM_ICHECK_TYPE_CODE. Rename name to optional_name.* revert changes to module pass for later PR* reverted too much* documentation* formatting* more docs* unify error message language. TypedPackedFunc contrustor that does not take a name* Update include/tvm/runtime/packed_func.hCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[Relay] Type Relation Fixes (#7362)* fix an error in the dynamic Full Type Relation* Add Diagnostic Errors to Broadcast Type Relations,0
Remove MemoryPlan from VM passes (#7361),4
Some docstring fixes. (#7367),0
[Relay][Frontend[Onnx] Add testing for output datatypes and fix related bugs. (#7364)* Add testing for datatypes and fix related bugs.* Fix lint issue in onnx.,0
fix grad for zeros and ones (#7357),0
[BYOC][Verilator] change runtime registry function name (#7351)* use lowercase for verilator runtime registry function* lint fix* update comment,0
disable one of rewrite in torch detection test (#7365),3
[Refactor][VM] Port memory_alloc to c++ (#7369)* Port memory_alloc to c++* remove memory python pass,4
[CUDA][PASS]Legalize tensorcore (#7147)* add pad_to_tensorcore & legalize for dense/bmm/conv2d* fix pad & slice* fix comments* fix comments* resolve conflict* resolve conflict* support only fp16* add tests/python/relay/test_pass_legalize_tensorcore.py* add tests for legalize tensorcore* fix pylint* fix pylint* code format* use_gpu test only; fix conv2d_alter_op* fix tests params* revert transform fix,0
swap pytorch and tvm import order (#7380),5
disable other rewrite to test CI (#7371),3
fix duplicated symbol bug in external codegen (#7383)Co-authored-by: 袁航剑 <yuanhangjian@bytedance.com>,0
"[Parser] Fix tokenizing inf (#7370)* fix tokenizing inf* use ParseNumber to parse inf, handle -inf* fix neg handling* fixed multi negation* refactor* use while loop* simplyfing* fix lint* simpler implementation per altan's suggestion* disable flaky test",0
Improve op_type missing message (#7384),5
[COMMUNITY] @hzfan -> reviewer (#7360),3
Refactor Dynamic to Static (#7368)* DynamicToStatic Refactor* fix test* add regression tests* cleanup* skip PrepareInput if the arg is already a constant* fix an issue with type inference with global functions,0
[Relay][Passes] Iterative A-normal Traversals (#7374)* [WIP][Relay][Passes] non-recursive a-normal traversals* fix clang warning* Refactor ANormal Iterative traversal into a higher order function utility with lambdas* refactor missed pass* add explict use of  to lamdbas,0
"Fix missing round(), floor(), ceil() for target C lowering (#7382)",0
[FFI] Improve error messages when array/map types do not match in function calls (#7330)* [FIX] Improve error messages when array/map types do not match in function calls* missed some places for renaming* Rename Mismatch to CheckAndGetMismatch. Add Check back in. Use Optional::defined.* Optional<String> -> String* formatting* move ObjectTypeChecker template specializations into where thier respective classes are defined so they will always be found correctly,0
[TOPI] Add einsum operator (#6370)* [TOPI] Einsum* Fix tuple* fix oshape* * test* * Fix lint* * Remove useless define* * Move to einsum header file* * Fix single value situation* * Fix CamelASE* * Print stride* * Fix single input bug* * fix lint* * Fix lint and add comments* * create test einsum* * Fix lint* * Fix comments,0
"[TFLite] Added check for dynamic range quantization (#7114)* [TFLite] Added check for dynamic range quantizationAdded check to prevent optimized with ""dynamic range quantization""tflite files to be loaded as the optimization is not fully supported.https://www.tensorflow.org/lite/performance/post_training_quantization#dynamic_range_quantization* linter* linter* unit test fix",0
"Generate requirements.txt from Python spec (#7289)* Generate requirements.txt from Python spec.* add tests, collect actual requirements (first cut).* add tornado and cloudpickle* add xgboost* add xgboost version restriction* cleanup and prepare for merge* black format* add type annotations and docstrings* remove example requirements.txt* fix setup.py extras_require* use typing. classes for type annotations, python 2 compatible :)* fix python2 typing.Pattern* retrigger CI* address comaniac comments* retrigger ci",0
[Bugfix][AutoScheduler] Fail to register ComputeDAG when deserializing tasks (#7395)* [Bugfix][AutoScheduler] Fail to register ComputeDAG when deserialize tasks* fix test* trigger ci,0
[CI] Temporary increase ci timeout (#7403),5
[RPC] Replace timestamp with counter (#7389),5
Support negative pad values (#7375)* Support negative pad values* Update test_op_level2.py* Update pad.cc* Update test_op_level2.py* PR Comments* Update pad.cc* Address PR Comments* CI Error* CI Error* CI ErrorCo-authored-by: Ubuntu <ubuntu@ip-172-31-28-115.us-east-2.compute.internal>,0
Fix Bug in Bilinear Interpolation and Add Deform Conv to PT FrontEnd (#7397)* Fix Bug in Bilinear Interpolation* Add NHWC Tests* clean* Fix Bug and Add Deformable Conv PyTorch for completeness* Add Tensor Utils* Remove stuff* Include vector* PR Comments* Empty Commit for CICo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
[AutoScheduler] Support early_stopping per task (#7377)* [AutoScheduler] Support early_stopping per task* address comment* fix test* Update python/tvm/auto_scheduler/task_scheduler.py* Update python/tvm/auto_scheduler/task_scheduler.py* trigger ci* trigger ci,0
[CI] Add back the tests after timeout adjusted (#7408),1
[Relay][Frontend][Onnx] Refactor where importer to support dynamic shapes. (#7394)* Refactor where importer to support dynamic shapes.* Add a test for dynamic where.,1
Add cuda tags and unit test (#7410)* Add cuda tags and unit test* Add missing space* Remove extra indent* Modify macro def position* Fix clang format* Fix clang format for set_config,0
check for dynamic rank before accessing value (#7414),5
"[VM] Minor refactor for C++ memory alloc (#7413)* started moving things to header* directly call InvokeTVMOp* done all memory op* also refactor AllocTensor* declare Prod* remove cached func for Add, Multiply, Divide* lint fix* revert test change* remove tensor.h and declare Prod in pattern_utils.h",0
"Fix AutoScheduler for anaconda python (#7387)In case of non cpython flavour of python, the task passed to measure processshould be serialized using pickle approach. The task includes workloadwhich is a list of Tensors. The list should be serialized and deserializedas an atomic object.",0
Fix compilation when Arm FP16 extensions are enabled (#7386)Fixes incorrect number of template parameters in call to sort()Signed-off-by: Matthew Bentham <matthew.bentham@arm.com>,0
Jenkinsfile changes for #7333. (#7388),2
[µTVM] Add VMWare to Reference VM instructions (#7221)* support vmware_desktop provider for microTVM reference VM* update tutorial* python format* try to fix sphinx warning* fix sphinx warning* retrigger CI,0
Generate JUnitXML from pytest (#7407)* Generate JUnitXML from pytest.* address tkonolige comments,1
"[FIX,CMAKE] Only compile runtime files once (#7417)* [FIX,CMAKE] Only compile runtime files once* copy defines to tvm_runtime_objs",0
[TVMC] Allow manual shape specification in tvmc (#7366)* add ability to optionally overide tvm shapes* add help documentation for --shapes* improve documentation* reformat test_compiler using black* Incorporate feedback from ekalda for better pytorch support and testing.* address feedback* switch input shape syntax to be more pythonic* add commentary* reformat common.py* fix lint issue* format common.py with black* torch/pytorch test hiccup* add -s to setup-pytest-env.sh for clearer error msgsCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>,0
[AutoScheduler] Add sampling to dispatcher (#7376)* [AutoScheduler] Add sampling to dispatcher* address comment* make measurment configurable,1
[ONNX] Add CumSum operator to ONNX frontend (#7391)* [ONNX] Add CumSum operator to ONNX frontend* Fix lint and add attributes to CumSum* Fix CumSum test* Add support exclusive attribute* Add support reverse attribute* Fix clang-format* Fix lint* Move reverse calculation to ONNX frontend and add exclusive to GPU* Add test for int type,0
[Relay][Topi][CPU] Dense with weight transform (#7404)* Add CPU dense weight transform* Fix format* Fix python format* Fix pylint* Minor fix* Add test* Do not need to infer layout for dense* Fix test* Rename dense_pack* Fix test* Fix lint* Fix dynamic shape dense* Fix lint* Fix autotvm task extraction test* Disable AlterOpLayout in micro_tflite.py tutorial,0
"[FIX,CMAKE] Only set Clang flags for C++ files (#7424)Clang flags were set for all file types, causing nvcc to error out.",0
TRT Dynamic Reshape Fix (#7412)* Dynamic Reshape* Changes* Add test cases* Add test cases* PR COmments* CI Error* EmptyCommitCIErrorCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
Simplify full broadcast (#7423)* convert argwhere(full(const)) to reshape(arange())* Add IsWildcard syntatic sugar* add a simplify expression to fold full into broadcast ops* Allow constant folding of full-like ops after SimplifyExpr* fix a bug with the Attr Pattern matching* remove skip_list,0
[Arith] Fix iter_affine_map with non-const extent (#7437),0
Stop running some python testsuites twice (#7430),3
[BYOC][TRT] Fix small bug preventing TRT runtime compilation for versions < 6 (#7372)* Fix small bug preventing TRT runtime compilation for versions < 6* Trigger ci,0
Make the TVM targets list available in Python (#7427)* Make the TVM targets list available in PythonChange-Id: I8602723fe57aaf32cee5392d4387a637115dd363* Rename the APIs to get target kindsChange-Id: I2e6e32e025e3614a148a30a31e5a2c52fd3563cc,4
"Replace type punning with memcpy. (#7415)The type punning in the existing code is undefined behaviour in C.In particular, the existing code fails when running on Arm Cortex-M devices.On Cortex-M, accessing a uint64_t that is not 8-byte aligned generates a hard fault.Change-Id: I2aecaa220e581af7c91a8bc7886499d70e2aa6f2",4
"Fix double compile of runtime sources for TRT, ACL (#7436)",0
[TIR][Printer] Fix SelectNode TIRTextPrinter bracket mismatch (#7405)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,0
Update tags with minor fix (#7448),0
Add ROCm docker (#7422),1
[AutoScheduler] Fix distill record (#7439)* [AutoScheduler] Fix distill record* update comments,0
"[Relay][Op][Bug] Fix missing return in scatter_nd cuda strategy (#7447)* fix missing return in scatter_nd cuda strategy* add Relay test for scatter_nd, fix documentation",0
Make keras reshape less restrictive (#7446),5
"[µTVM] Use standalone_crt build tree for all µTVM builds (#7333)* Build microTVM using standalone_crt in build tree.* black format* pylint* try stashing entire standalone_crt in hopes it will not upset jenkins* Put standalone_crt in correct Jenkinsfile stash bundle* include build prefix* switch to python script for expanding globs* revert attempt to use globs in pack_libs, switch to building standalone_crt* properly revert pack_lib changes* fix typo* retrigger CI* revert pyproject.toml* update Jenkinsfile approach to use task_ci_setup.sh",0
"[ONNX] Make the ONNX Importer More Static (#7429)* Construct static Ops if inputs are Constant* Expose FoldConstant as a function in addition to the pass* refactor onnx importer to do more static imports by constant foldingfix pylint* fix test regressions* fix style, two bugs* pipe freeze_params through sub_graphs when importing loops and control flow",0
[VM] Move param bind to OptimizeModule (#7451)* [VM] Move param bind to OptimizeModule* add test to verify the number of free vars after opt* remove const from OptimizeModule,1
[Frontend][MXNet] Add support for MXNet GroupNorm (#7409)* Add support for MXNet GroupNorm* Fix python lint* Fix lint,0
update stm32mp1 arm_cpu target configuration (#7443)Add the -mcpu information to complete the picture.Signed-off-by: Vincent ABRIOU <vincent.abriou@st.com>,1
[FRONTEND][TFLITE] get input tensor information from graph (#7400)* [FRONTEND][TFLITE] get input tensor information from graph* remove bare-except* fix lint* delete empty line* comment change* move some of the tflite frontend code from tvmc to tflite.py* update shape and dtype when user provided them* remove unused var. pass user provided shape_dict* remove duplicate code,0
"[µTVM] Print .elf statistics for a model runtime built with Zephyr (#7449)* [µTVM] Print .elf statistics for a model runtime built with ZephyrCurrently there isn't any statistics about the used resources by a modelruntime built with Zephyr, making it difficult to have any idea about, forinstance, the amount of memory taken by the operations necessary to run themodel.Since Zephyr's SDK already exposes the statistics about various memoryregions on linking by passing '--print-memory-usage' to the linker, it'spossible to use it to have an idea about the amount of memory used by themodel and how much memory is left on the device.That commit adds a simple method to extract the memory region informationout of the build output and then uses it to show memory usage statisticsfor various memory regions when Zephyr finishes building the image to beflashed to the target device.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* v2: Fixes accordingly to Andrew review- Catch StopIteration in case of a weird output or no additional lines  after the last memory region- Use of _LOG.info() instead of plain print() for better control over  the output by the main script- Set log level in micro_tflite.py script as an example on how to get  the new memory usage statistics and also because currently that's the  main script used to test microTVM + Zephyr's SDK- Improve statistics headerSigned-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix buildIt seems build system is using Python < 3.7, so 'text' argumentis not present as an alias for 'universal_newlines'. To satisfyit use old 'universal_newlines' argument which is available priorto Python 3.7.* Fix buildAvoid exception anti-pattern when catching StopIteration* Retrigger CI",0
Add IdentityN operator for TF Frontend (#7452)* Add frontend code and tests* Add Frontend CodeCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,1
docker/bash.sh: lookup docker image in Jenkinsfile. (#7453)* This PR makes it possible to type   `docker/bash.sh ci_cpu tests/scripts/task_config_build_cpu.sh`   and the same version of ci_cpu as is used in Jenkins will be   used to run the command.,2
[BYOC][Verilator] Refactor Verilator runtime (#7406)* new experiment* save* refactor* refactor library* add profiler* refactor* refactor* add docs* update comment* add deallocator,1
"Make spelling of ""axes"" consistent (#7460)",5
"[Relay][Topi] Add max mode to ROI align (#7440)* ROI align with max on cpu passes* onnx test file was not running gpu testsgit status!* all passing* fix lint* lint again* lint* lint* typo* remove import* fix import* add inf, -inf to hybridscript and respond to comments* shorten code* make atol lower",0
"[ROCM] Add Thrust support (#7458)* enable rocm thrust, confrimed to work on sort and scan* add rocm argsort strategy* Abort if CXX is not hipcc* add more strategy* add missing import* fix lint* show supported data type in err msg* try remove rocthrust* add missing include for rocthrust* more minor changeCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>",0
SparseFillEmptyRows Op (#7442)* Initial Commit* Fix formats* Remove comments* Black* THreeops* Add Frontend Code* Add Default Value to feed dict* Add Frontend Code* New test Cases and new code to handle them* Add Python Implementation''* Remove stuff* Remove unused imports* Pylint* Pylint* PyLint Shape Func* Make tests cpu only* Add unsorted tests* Add frontend code* Row Major Sorting Only Test* Handle Dynamic Shapes* Add dynamic input shapes* Dynamic Shape Tests* Add documentation* Dtypes* PR Comments* Added comments and changed naming* Add comments* Comments to Shape Func* Documentation* PR Changes* PR Comments* Resolve input and output dtype compatCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
[Bugfix][Relay] Crash in match_exhaustion.cc when given an empty tuple pattern or constructor with no args (#7459)* [match_exhaustion] Fix cartesian product to handle empty tuple patterns or constructors with no args* Test cases do not actually exhibit the fixed bug* Mistake in comment,0
Report JUnit test results for all TVM Python tests (#7450)* Enable JUnit parsing for Python tests* retrigger CI* prefix junit results with FFI type* remove - in junit prefix,0
[ETHOSN] Add support for default Ethos-N78 configuration. (#6982)Note: 'ETHOSN_VARIANT_CONFIG' must be set to test against Ethos-N78 and this adds support for one configuration of Ethos-N78 in TVM.,1
debug operator--() in include/tvm/node/container.h (#7461),0
"[TOPI, Relay] Support roi_align NHWC layout (#7463)* begin nhwc roi align* integrate mode change from upstream* adding test* support nhwc shape func* update strategy* refactoring test* refactor test* refactoring* fix lint* update relay op tests",0
Set TOpPattern=kOpaque for scatter_nd (#7464),5
[RUNTIME] Fast path for single thread run to allow app level threading (#7454)* Fast path for single thread run to allow app level threading* add sync counter to avoid error in one of tests,0
[Torch] Add index_put operator (#7465)* [Torch] Add index_put operator* Skip test_frontends.py::test_load_model__pth,1
"[Relay][Bugfix] Fix off-by-one error in BiasAddRel, use new reporting (#7467)* Fix off-by-one in BiasAddRel, use new reporting* No need to mark xfail if the exception is caught* lint",0
[AutoScheduler] Fix the type inference for conv3d (#7475),0
Get tvmc version from tvm (#7478)Change-Id: I6a6e78080f36e4e3e1689e03ea48e759fcd8e466,4
"[TVMC] Add composite target passes for compilation and tuning (#7304)* Extend --target syntax to cover multiple targets for compilation and tuning * Add a new composite_target module to implement custom codegen passes into TVMC * Provide implementation to integrate TVMC, to target Arm Ethos-N NPU and   Compute Library for the Arm Architecture (ACL)Change-Id: Iaee53fe22f0c14eb4e4c8ec47e72bade0c5e32cc",1
"[Frontend][Tensorflow] Support explicit_paddings for TF 2.x (#7445)* Ignore some TF2.0 attributes* Support explicit padding for conv2d, max_pool, conv3d* Remove conv3d explicit padding test since TF API doesn't allow it",1
make test_runtime_rpc use pytest.main() (#7482),3
"[TIR] Specialize MutateArray in StmtFunctor. (#7486)StmtFunctor applies context dependent copy on write,which requires check over all the dependency chain.Such function is better suited as a special implementationto avoid misuse. This PR refactors the code to specializethe function.Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>",5
"[CUDA][THRUST] Enforce -libs=thrust to allow thrust offload (#7468)* add contrib/thrust.py* update cuda strategy* remove is_thrust_available, update nms, scan, sort and tests* remove unused import* trigger CI* update* add note on how to enable thrust in ssd tutorial* add warning* Revert ""update""This reverts commit c1629b39e5277003a82cbf31fe4da493537bc05f.Co-authored-by: masa <masa@pop-os.localdomain>",1
Fix cuda nms handling of additional per box features (#7483),0
Fixed minor misspelling (#7499)Co-authored-by: mshr-h <mshr-h@users.noreply.github.com>,0
[Target] Add target host field for target specification (#7462)* Add target host field in Target* Add host as a config field to target* Add target host support for Python api* Add unit tests* Adjust format for cpplint* Remove unnecessary  after  in Python file* Remove redundancy and add param description* Fix format issue* Fix param description* Add unit test for duplicate target hosts,0
"[RELAY][Parser] Optimize relay parser to restore calls attrs (#7347)* [RELAY][Parser] Optimize relay parser to restore attrs for non-Operator calls* To avoid too much modification to the native class, only print out the attrs  type key of non-Operator Call in relay printer. Then reconstruct the attrs object  after parsing this attrs type key value in Relay parser.* fix lint* fix ci* add test case",0
[Frontend]Make onnx gemm tensor C optional (#7489)* Make onnx gemm tensor C optional* fix codestyle* add tests* fix codestyle,0
"[CRT] Create C-runtime-style metadata module for llvm builds (#7398)* Create C-runtime-style metadata module for llvm builds.* maybe address manupa's comment* lint* actually address manupa comments* comment and rename* git-clang-format* pylint* cpp warning* try to fix apps/bundle_deploy* black format* build correct file* Use save() for C++-runtime targeted artifacts.* fix build_module LLVM metadata module conditions* fix test comment* black format* further restrict CRT MetadataModule creation* Fix test_link_params* black format and address zhiics comments* fix test_link_params, i think?",0
"Fix stack overflow when partially-__init__ Node raises exception. (#7481)* Fix stack overflow when partially-__init__ Node raises exception. * If a Node subclass raises an exception and ctypes is in use before   __init_handle_by_constructor__ is called (or self.handle is   otherwise set), a Python stack overflow could result. This is   because the unset handle slot causes self.handle accesses to   fallback on the getattr(self, 'handle') method, invoking   NodeGetAttr. * Then I believe this causes an infinite loop. * The fix is to make Node.__getattr__ raise AttributeError for all   attributes in __slots__, then make __del__ tolerant to missing   self.handle. * I don't believe cython is affected because it implements a   descriptor to access its underlying chandle and that shouldn't be unset.* black format* actually use handle instead of self.handle",0
[COMMUNITY] @d-smirnov -> reviewer (#7510),3
[Relay][Frontend][Onnx] Fix GEMM converter when C is not a parameter. (#7509)* Fix onnx gemm with non parameter C.* Add gemm tests for C.* Fix formatting.,0
[AutoScheduler] Fix the type inference for conv2d (#7501)* fix type inference for conv2d* fix,0
"[TVMC] rename composite target ""acl"" to ""compute-library"" (#7508)* Renames the ""acl"" composite target to point to the specific   library it represents",5
Support creating Bool constants in the pattern_utils (#7507),5
[Frontend][Tensorflow] Support range like axis in tf.raw_ops.All for TF 2.x (#7502)* add TF2.x raw_ops.all axis range support* apply linting* fix range() func input,0
[BYOC][VitisAI] Fix issue in Vitis AI codegen out tensor names matching & update docs and docker (#7350)* Fix bug in vitis ai codegen out tensor names matching & update docs & update docker* Update vitis_ai.rst* Move gpg-agent package installation to vitis ai core script* Refactor install_vitis_ai_core script* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update vitis-ai docs pynq/edge setup & adjustements for comments* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Reorg Vitis AI dockerfile to make sure gpg-agent is installed before llvmCo-authored-by: Jorn Tuyls <jornt.tuyls@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
Support CombinedNMS in TF frontend. (#7520),5
[Frontend] TF V2 sparse.todense() test added (#7473)* [Frontend] TF V2 sparse.todense() test added* [1] Review comments handled,1
[DOCS] Remove incubating from docs (#7525),2
[PYTHON] Enable proper error message in python package (#7521),0
"Introduce module_loader to AutoTVM. (#7337)* Introduce code_loader to AutoTVM. * Prepares for autotuning with microTVM, and provides extension hook   for VTA.* add vta hook* git-black* pylint* Add missing import* Fix import problem* add missing import* rename code_loader to module_loader* rename remote_kw to remote_kwargs* black format",0
Many fixes to get unit tests passing on Windows. (#7431),0
use checked_type instead of type_annotation (#7522),2
[Torch] Avoid adding unnecessary slicing (#7479)* simplyfing* improved fast path for slice* update rewrite pattern for maskrcnn,1
[Relay] Enforce static dim for non-concat axis if one or more tensors have static dim (#7487)* enforce static dim for non-concat axis* assign any when all dims are dyn* add missing case* simplify* add test* only enforce static dim constraint if concat output is dynamic* more update to concat type rel* update tests* fixed compile warning,0
"[Frontend][Tensorflow] Add unique operator (#7441)* Initial commit of the unique operatorAdd unit tests for unique operator* Add tensorflow unique op* Refactor unique to use sort-based algorithm* Change relay.unique test to run only on cpu* Change topi.unique test to run only on cpu* Change range to parallel for parallelizable loops* Add return_counts option for relay.unique and topi.unique, add pytorch frontend* Fix pylint* Patch pytorch frontend* Initial support of topi.cuda.unique* Refactor to use ir_builder directly* Modularize adjacent difference* Refactor to simplify* Fix typo* Combine _unique and _unique_with_counts* Reuse indices_ptr to remove arange_ptrCo-authored-by: Yanming Wang <yanmwang@amazon.com>",0
"[Torch] Pool ops, convert strides and pool_size to int (#7517)* Convert strides and pool_size to int* Make helper function, add test* Fix lint",0
SparseReshape Op (#7477)* SparseReshape Inital Code* Done* Format* Add empty tests* Formatting* SanityCheck* formatting documentation* Documentation* Only Enable CPU* Add support for CUDA* Stuff* Add Dynamic Support* Parallelize GPU Impl* Documentation* Documentation* Import* Import* Remove unnecessary code* PR Comments* Schedules* Tests* Dtypes* Black* Parallelize CPU* CI errorCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
[BUG_FIX][TOPI] Allow topi resize to accept more options (#7532)* Make topi more permissive* Remove testing stuff* lint* Downsampling tests,0
[ONNX]fix datatype on Reciprocal op (#7519)* fix datatype on Reciprocal op* clean up test case,0
[CI] Move ci-cpu to use llvm-11 (#7541)* [CI] Move ci-cpu to use llvm-11* Fix the testcase of x86 codegen by relax the register names.,0
Add create_local_debug_runtime to micro exports (#7528)* Add create_local_debug_runtime to micro exports.* retrigger CI,0
Don't run non-tvm_op GraphRuntime nodes in Debug Runtime over RPC. (#7512)* Don't run non-tvm_op GraphRuntime nodes in Debug Runtime over RPC. * These are filtered out in SetupOpExecs for normal debug runtime operation.* retrigger CI* retrigger CI* address tkonolige comment,0
Add test_forward_index_put to __main__ (#7542),1
[torch] Add narrow operator (#7535),1
[Torch] Simplify contiguous (#7544),5
add missing equal sign (#7531),1
Fix typo in relay.vm.Executable (#7543)Co-authored-by: Yanming Wang <yanmwang@amazon.com>,0
[Runtime] Special Memory Scope Support (#7488),5
Fix foldconstant involving dropout (#7550)Co-authored-by: masa <masa@pop-os.localdomain>,0
[TensorIR] introduce Block and BlockRealize (#312) (#7553)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[Autoscheduler][VM] Autoscheduler layout rewrite pass to VM (#7516)* fix type inference for conv2d* fix* adding the autoscheduler layout rewrite pass to VM compiler passes* revert edits applied in other PR* minor fix* fix* formatting fix* lint,0
fuse constant padding into conv kernels (#7515)* fuse constant padding into conv kernels* change the kernel to support other layouts* add channel-last test* add a comment about bailing early,1
[Codegen][CUDA] Fix: cuda codegen vectorize cast (#7561)* fix: cuda codegen vectorize cast* style: fix python coding style* fix: missing break* refactor: directly split by factorCo-authored-by: jiangchengquan <jiangchengquan@bytedance.com>,0
[Torch] Fix converting torch slice op with dynamic slice length (#7549)* Fix converting torch slice op with dynamic slice length* use isinstanceCo-authored-by: masa <masa@pop-os.localdomain>,0
[Pass] Profiling TVM compiler passes (#7500)* basic pass profiler prototype* allow enable/disable of pass profiling* lint* add example pass profiler usage as test* render pass profiles to String instead of stdout,1
[TIR] Add TIR While node (#7425)* add while node* update visitors* binary search lowering works* llvm codegen working* cuda codegen working* nms updated to use while loop* add missing upper bound check too* add mandelbrot test* add gpu mandelcommit ee2363bf8131830cf0fb112890befd6be6a03f36Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Jan 29 11:44:02 2021 +0900    enable extern lib offload for nvptx* rename test* run black* add doc* add collatz test* add while + vectorize test* simplify bin search* Add special case visit method to storage_access.cc* disallow while loop inside vectorized loop* disallow trivial condition since we do not have break* error out in CoprocSync for now* error out LiftAttrScope for now* add placeholder to inject_vpthread* refactor to use MakeAttach* handle WhileNode in InplaceOpVerifier* error out in InjectVirtualThread* try handle WhileNode in StoragePlanRewriter* remove WhileNode visitor from storage rewrite* add while loop storage rewrite test* update tests* move test_vectorize_while_fail to  test_tir_transform_vectorize.py,0
[RELAY] Modify some passes to not stack overflow on many lets. (#7558)* [RELAY] Modify some passes to not stack overflow on many lets.Passes modified:- inline primitives- dead code- lambda lift* one fix* small fix* .at -> []* fix,0
[torch] Add linear operator support (#7569),1
[Tensorize] Support conds depend on outer loop vars inside tensorize scope (#7497)* [Tensorize] Support conds depend on outer loop vars inside tensorize scope* Reformat,2
[CI][VitisAI] Update CI Vitis AI PyXIR version (#7575)* Update Vitis AI CI PyXIR version to v0.1.6* Add --depth 1 to PyXIR clone command,1
[SPIR-V] Add SPIR-V lowering for While node (#7574)* Add SPIR-V lowering for WhileNode* test vulkan in while loop tests,1
[Relay][Quantization] Fix Bug Which Cause Negative Left Shift Op (#7432),0
[Relay][bugfix][error reporting] BiasAddRel does not check for a negative index being out of bounds (#7554),0
compile engine dump tir and shape funcs (#7552),5
[RUNTIME] Move Map into runtime (#7570)* [RUNTIME] Move Map into runtimeThis allows us to use Map to store parameters needed at runtime.* node.{Array|Map} -> runtime.{Array|Map}* missed some renames,4
[AutoSchedule] Fix a flaky test (#7580),0
[AutoScheduler] Querying and sampling in task extraction (#7571)* [AutoScheduler] Query in task extraction* trigger ci,5
[DOCKER] Fix: install script regarding get-pip.py during docker build (#7579),0
[ETHOSN] Add support for 20.11 Ethos-N driver stack release (#7506)- Updated ethosn relay backend to support 20.11 api changes. - Removed legacy support for 20.05. - Added a mechanism to specify the ethosn driver stack version.,1
Fixes for using Python APIs from Rust. (#7085)* Rewrite the Rust Module API and change some imports causing crashes.This commit also updates the docs to remove outdated information.* Renable Python test and remove warnings* Python test still flaky* Fix broken module test* Fix broken test* Reset test file,0
"Add segment sum Op to relay and 7 corresponding TF Ops , fix scatter_add dynamic bug  (#7562)* Add segment sum Op* Remove unnecessary* Documentation* Black* Add GPU* Uncomment* Add documentation* Add dynamic tests* Add TF Op* Add Sparse Segment Sum* Add test coverage* PR Comments* Int64 tests* Add SparseSegmentSqrtN* Add SparseSegmentSqrtNOp* Deduplicate code* Add SparseSegmentMean* Parametrize Tests* Remove* Modularize* Black* Modularize Code* Pylint* PR Comments* Add scatter add tests* Remove TestCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>",0
[BYOC][TensorRT] Make TRT runtime robust to empty or weird subgraphs (#7581)* Prevent TRT runtime crash for duplicate inputs and outputs* Add empty subgraph unit test,1
[SPIRV] Support Bool buffer argument (#7591),5
[PyTorch] Guarantee data input is the first argument (#7592),5
[CI] Bump arm version (#7584),5
Fix for dynamic batch size conv2d nhwc (#7598),0
[Frontend][MXNet] Fix default value for is_ascend in topk (#7568)* Use correct default value of False for is_ascend* Add unit test for default topk is_ascend value,0
[BYOC][TRT]Fix groups cannot divide output channel count error for deconv when groups>1 (#7595)* trt num_outputs* asdf* fix lintCo-authored-by: Leyuan Wang <leyuan.wang@bytedance.com>,0
Support negative axis for gather (#7600)* Fix negative axis in gather* Clang Format* Black* Empty CommitCo-authored-by: Ubuntu <ubuntu@ip-172-31-42-251.us-east-2.compute.internal>,0
[Vulkan] Support passing 64 bit scalar  (#7572)Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"Fix autotuning, broken in #7337 (#7566)* Fix autotuning, broken in #7337* retrigger CI, because I don't understand how it passed",0
[RUNTIME] Add device specific timers (#7472),1
[Relay][Pass] Avoid stack overflow when using PostOrderRewrite (#7588)* init* fix* fix,0
[TOPI] disable test_shift with i8 datatype (#7597)https://github.com/apache/tvm/issues/7539Co-authored-by: guoweijun <guoweijun@baidu.com>,3
[AutoSchedule] Sparse dense tuning support with custom sketch rule (#7313)* Add sparse dense tuning tutorial* Add sparse input fusion* Update the dag to support output fusion* Update* Add task input to search_task* Update* Add search_inputs to measure* Lint fix* Lint fix* Update* Update* Update* Update* Add file save load support* Update* Update* Update* Remove add_task_inputs API* Update* Update* Update* Lint fix* Lint fix* Lint fix* Lint fix* Update* Add example ci_log* Update* retrigger ci* Update* Update* Update* Lint fix* Lint fix* Lint fix,0
Move SimplifyConvPad to a new pass and don't enable it by default (#7603)* Move SimplifyConvPad to a new pass and don't enable it by default* rename pass* move files* fix lint* adjust test tolerance,0
"[Executor][Bugfix] Properly return and unflatten outputs from GraphExecutor (#7604)* properly return and unflatten outputs from GraphExecutor* lint* cleaner approach, not sure what I was thinking before* remove unused import* forgot copyto cpu* make solution even cleaner using iterator",0
[CUDA] BF16 support (#7014),5
"[Torch, QNN] Support quantized mobilenet v3 from torch 1.8 (#7606)* [Torch] support hardsigmoid* qhswish first impl* add qhardsigmoid but the result is not correct* add qmv3 to test* comment fix",0
[TE] Fix bug in AutoInlineElemWise and implement AutoInlineBroadcast (#7602)* [TE] Fix bug in AutoInlineElemWise and implement AutoInlineBroadcast* [TE] Add AutoInlineBroadcast API to schedule_pass.h,0
[Relay] add ShapeFunc for tanh (#6898)* add ShapeFunc for tanh* _schedule_dense_small_batch turn autotvm off when dense's inner dim is unknown* fix CI pylint,0
[Relay] Fix relay op strategy for cuda dense int8 (#7586)* [Relay] Fix relay op strategy for cuda dense int8* Remove uint8 && Add autotvm task extraction test for relay graph that contains dense op (int8 * int8 -> int32)* Reformat the code of test case,0
Add logging to diagnose flaky ci-qemu test (#7610),1
[Relay] add ShapeFunc for one_hot op (#7490)* [Relay] add ShapeFunc for one_hot op* fix pylint* add test for shapefunc of one_hot op,0
[RUNTIME] Unify load params interface (#7559),5
[FIX] Fix clang12 warnings (#7593),0
[Runtime][Object] Add Object::unique() (#7615),1
[Bugfix][AutoScheduler] Correctly resume status (#7614),0
Added MaybeAlign to CreateAtomicRMW calls to fix build for LLVM13 (#7617),0
Prevent host Vulkan SDK blocking cross-compilation (#7609),5
[SPIRV] Minor update to TIR sort to make it work on VK/SPIR-V (#7607)* sort started to working* static size sort seems to be working* test sort on vulkan* add nvptx to sort test too,1
"Allow cuDNN in non-CUDA non-system dir (#7608)cuDNN is not a builtin library of the CUDA toolkit package.The user can install it in the CUDA directory, the systemdirectory, or anywhere else. This patch relax the restrictionof locating cuDNN in the CUDA directory. This is helpfullwhen trying out different versions of cuDNN.",2
Fix RelayVM for 32-bit platforms (#7605),0
Fix TVM compile without LLVM (#7621)* Fix TVM compile without LLVM* Fix formatting,0
[SPIR-V] Fix pushconstants offset calculation for 32 bit values (#7620)* Fix push constant offset for 32 bit value* add test* remove unused function from test* add dynamic cumsum test* skip if vulkan is not enabled* replace dynamic cumsum test with dynamic argsort for nowCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,0
"Introduce Model Library Format export format (#7533)* Introduce Model Library Format export format. * This function produces a stable on-disk representation of TVM's   compiler output. * It's intended just for use with the C runtime for microTVM right   now. It could be expanded for other use cases. * This PR implements the Model Library Format RFC, which ultimately   is intended to support the Project Generator API (RFC   forthcoming). * There may be some changes to the format without revving the version   number until downstream consumers are known. The Project Generator   API is the first such known downstream consumer. * There are no plans currently to support generating old Model   Library Format from TVM. The version number is intended as a   compatibility check between the generator and downstream consumers.",2
[Runtime][Contrib][Verilator] remove explicit destructor call (#7485),4
fix:getcwd not work on android platform (#7390)* fix:getcwd not work on android platform* replace `exit()` with `_exit()` on subprocess in `cpp_rpc`Co-authored-by: rqg <ranqingguo318@gmail.com>,0
Improve tensor mismatch ICHECK message (#7335)* Improve tensor mismatch assert message,5
[CUDA][TOPI] Fix CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES with NMS for certain GPUs (#7623)* Use less threads for certain GPUs to avoid register limit* Move util function to nvcc.py* Fix lint,0
Grammar fix (#7622),0
[TIR] Add PreOrderVisit and VisitPrimFuncs (#7627)* [TIR] Add PreOrderVisit and VisitPrimFuncs* Update stmt_functor.h* address comments* fix lint,0
[AutoScheduler] Fix incorrectly array context device and hide info at the beginning (#7632)* [AutoScheduler] Fix incorrectly array context device and hide info at the beginning* Lint fix,0
[MIPS] Fix CALL16 reloc at 0x290 not against global symbol (#7634),0
[Test] Add Test Case to Cover Bug Fix by PR#7432 (#7601),0
[ONNX] Use take instead of min in NMS conditions (#7633),2
[Ansor] Add HW param for Vulkan tuning (#7626)* add HW param for VK* query warp size properly* guard against warp_size < 4 caseCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,1
[TOPI] Sparse Add Op added (#7435)* [TOPI] Sparse Add Op added* lint resolved* TF frontend support added* Test case added* [1] Review comment handled* [2] Review comment handled* [3] Review comment handled* [4] Review comment handled* [5] Review comment handled,1
[Relay][QNN] Simulated Quantize and Dequantize (#7613)* Add initial implementation of flexible simulated qnn ops.* Added proper topi testing and fixed qnn axis bug.* Add injective schedule wrapping.* Stuck on typerel problem.* Relay integration fully working.* Simulated quantize totally finished.* Change dtype to be a scalar rather than tensor.* Undo change to quantize.* formatting.* Fix attritubes.* Fix negative axis dequantize bug.* Add topi simulated dequantize.* Add simulated_dequantize op to topi and relay.* Formatting.* Test negative axis perchannel dequantization.* Lint formatting.* Change import order to make lint happy.* Fix pytest.* Directly return make call.* Clarify disable mode for simulated qnn ops and fix typos.* Line too long oops.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,0
"Introduce Apple BNNS backend (#7299)* Introduce Apple BNNS backendThis is simple JSON based runtime which offload execution ofsome operation into Accelerate frameworks via BNNS api.Works only for: * macOS 11.0 and later * iOS 14.0 and laterSupported primitives: * conv2d and fusing with bias and relu * dense and fusing with bias and relu/gelu * batch_matmulSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add conv2d DW testAlso fix some pylint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix clang-format issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Refactoring. Add TView abstractionSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add several more onnx topologies into testsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Avoid redundant tensor allocationSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix conv_splitter issueSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix isse with bias {1,1,1,1}Signed-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Min. Rename fileSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix review comments. InitialSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] test refactoringSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix cpplint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix clang-format issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix python formatSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix pylint issuesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Fix pylint. Second attemptSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add integration documentation* Check onnx import before useSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [BNNS] Add instance normalization operator* Add fusing sigmoid activation after conv2d* min changesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Add pooling operations to BNNS runtimeSupports `nn.max_pool2d`, `nn.avg_pool2d`, `nn.global_max_pool2d` and`nn.global_avg_pool2d` operations* Fix lint* Fix lint* Apply comments* Fix documentation* Fix comment to refer to BNNSCo-authored-by: dlexplorer <elvin.nnov@gmail.com>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>",0
[PROFILING] Combine USE_VM_PROFILER and USE_GRAPH_RUNTIME_DEBUG into a single flag USE_PROFILER (#7637),0
[RUNTIME] Switch time evaluator to use device specific timing. (#7631),5
fix missing qparams in aten::upsample_nearest2d (#7646),0
"[docs] Getting Started with TVM: Auto Scheduler and matmul (#7644)Moves the auto scheduler with matmul example into the tutorial,expands to follow the flow of the larger getting started tutorial.Indended to follow the AutoTVM tutorial on matrix multiplication.",2
"[TVMC] Allow options on --target to contain dots. (#7651)* Allow tvmc compile --target options to accept dots * Adds testing for dot separator in quoted and unquoted   values * Add an ""unquoting"" conditional so that quoted and   unquoted strings look the same when parsed",1
[docker] fixed ci-gpu docker environment path typo. (#7648),0
Fix issue when group attribute isnt defined in convtranspose. (#7655),0
revert SET_LLVM flag (#7657)Co-authored-by: Lei Wang <34334180+NjtechPrinceling@users.noreply.github.com>,5
fix build break for android_rpc (#7664),0
[TVMC] Refactoring to document the --target regex and simplify test cases (#7654)* Adds comments to document the regex being used to parse the   --target=value string * Concatenate test cases without reducing the number of asserts   or number of actual tests,1
"[TVMC] Fix to check whether a path passed to --target is strictly a file (#7663)* When we use file with --target, the validation in place was only   checking whether it was a valid path. For the case in which the   path is a directory, it causes a crash when tvmc then tries to   open the path. * This fix moved the check to be strictly for files, not only a valid   path",0
Fixed strided_slice size (#7659)Co-authored-by: Akira Maruoka <akira.maruoka@fixstars.com>,0
Remove pytest dependency in arm_compute_lib.py (#7556)* Add OpAttrContext class which allows to temporarily change an attribute of an operatorChange-Id: I19b809a105ea8769e56bd89e028e090959a08728* Replace TempOpAttr with OpAttrContext in arm_compute_lib.pyChange-Id: I1c42dd6a29e765b06ce28192397016efeea2e82a,1
[Relay][Pass] Simplify consecutive transpose/layout_transform (#7656)* [Relay][Pass] Simplify consecutive transpose/layout_transform* lint* fix* support negative* comment,0
init the concat tensor with 1s and then slice them away (#7666),5
"[TOPI][GPU] Mergepath sort with odd-even block sort (#7611)* Mergepath sort with odd-even block sort* fix lint, add test* respond to review comments* speed up tests by reducing dtype skews* fix bad rebase* change threading to support vulkan* fix lint* only sort if the data is non-empty* fix lint again* fix for vk* move if to higher scope* fix typoCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>",0
"[docs] Getting Started with TVM: TVMC Tutorial (#7640)* Getting Started with TVM: TVMC TutorialAn update of the TVMC tutorial, follows the introductionand installation sections of the new getting started tutorial* Update tutorials/get_started/tvmc_command_line_driver.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Style and formatting fixesCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>",0
add nvcc support (#7668),1
Fix relay.testing.darknet convert_image (#7667),0
[Torch] Remove unnecessary reshapes for batch_matmul (#7675)* [Torch] Remove unnecessary reshapes for batch_matmul* lint* fix* reorder* lint,0
[SPIRV] Declare int64 capability by default (#7681),5
"[Runtime] Extend Graph Runtime To Support Cuda Graph Launch (#7616)* add graph runtime cuGraph poc* lint format* add unittest* fix review comments* Update CMakeLists.txtCo-authored-by: Cody Yu <comaniac0422@gmail.com>* build cuda graph runtime in gpu test* Revert ""build cuda graph runtime in gpu test""This reverts commit f286711e4126c696860be3ec3d82400ca8542bd5.* rename cuGraph to CUDA Graph* rename cuda_graph* rename cuda_graph* lint format* Update src/runtime/graph/graph_runtime_factory.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/testing.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix lint error* remove unnecessary warn* add test, fix lint* fix lint W0223Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[COMMUNITY] @areusch -> Committer (#7679),3
"[Frontend,TOPI] Improve dynamism for BatchMatmul and Dense (#7496)* [TOPI] Dense cuda schedule support dynamic dimension* [TOPI] batch_matmul cublas te computation support dynamism* [Frontend] tensorflow frontend: dynamic support for BatchMatmul* [TOPI] nn batch_matmul te computation support dynamism* fix CI* Update python/tvm/topi/nn/batch_matmul.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/topi/cuda/batch_matmul.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* remove concat_dynamic_shape function* update topi dense op integer checking* fix ci* Update python/tvm/relay/frontend/tensorflow.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update batch_matmul.py* [Frontend] add test for batch_matmul in dynamic shaped caseCo-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[Relay][QNN] Relax simulated qnn tests to prevent flakiness. (#7684)* Relax simulated qnn tests to prevent flakiness.* Change name of helper to make pytest happy.,3
[Relay] Add TopPattern to nn.dropout (#7685),1
[TVMC] Allow optional arguments to be passed to importers (#7674)* add support for optional args for frontends tvmc* remove unnecessary comments* Add changes suggested by Matt W. via PRCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>,1
[RUNTIME] Add libbacktrace for backtraces with line numbers (#7153)* [RUNTIME] Add libbacktrace for backtraces with line numbersCo-authored-by: Robert Kimball <bobkimball@gmail.com>,1
[Relay][Training][Pass] Factor out first-order AD to a module pass (#7677),4
Default value for graph_runtime Init lookup_linked_param_func (#7676),5
[CPP_RPC] allow user supplied work dir (#7670)* [CPP_RPC] allow user supplied work dir* clang format,5
[TFLite] Cast operator adapted for MLIR-based convertor (#7639)* [TFLite] Cast operator adapted for MLIR-based convertorCast operator now can be executed in MLIR-based version.Unit test updatedChange-Id: I30e5c1c9d69355116b560af8f6d0582b2d593538* Comment addedChange-Id: I3e2d29ef201283de337168d0b82679b63ca2fcf4,1
Free TensorRT engine and context (#7702),5
Change behavior of onnx importer to throw when user provides an input no in the graph. (#7699),4
[Vulkan] Workaround for zero size allocation (#7691),5
[AutoScheduler] Add function name in message (#7703)* [AutoScheduler] Add function name in message* fix,0
[TOPI][CUDA] Fix 0 valid boxes case for NMS when return_indices=False (#7700)* Handle 0 box case for return_indices=False case* Add unit test for mx NMS,0
[RUNTIME] Cleanup build for libbacktrace (#7706)* [RUNTIME] Cleanup build for libbacktrace- Introduce TVM_USE_LIBBACKTRACE value macro to be consistent  wth other value macros(instead of relying on disabled flag).- Introduce AUTO mode for libbacktrace- Temporary disable MacOS support in light of recent bug report.- Refactor out the libbacktrace.cmake to libs- Properly use TVM_DLL so that code is cross platform.- Fallback to the weaker dmlc impl when backtrace is disabled.* Update Logging.cmake* Update the macro check order to be consistent with the rest.,0
[torch] Use try_infer_value for clamp min/max (#7712),5
[TensorIR] TVMScript Parser/Printer (#7630)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,2
[TensorIR] add TIRTextPrinter support for Block and BlockRealize (#7716)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[ETHOSN] Add support for Ethos-N 21.02 driver stack release. (#7628)- Updated default Ethos-N driver stack to 21.02  - Fixed some test failures associated with this change,0
[TOPI] Use fixed thread block size in unique op for Vulkan (#7718)* [TOPI] Use fixed thread block size in unique op for Vulkan* forgot to add min for non vk backend,0
Fix auto scheduler crash when set with consumers is empty (#7708)Set with consumers is empty during preparing auto scheduler sketchesfor Metal device. Added check on the size of the set.In case when the set with consumers is empty we just skip this rule.,0
"[CI] Improve docker/build.sh to accept a docker tag parameter. (#7707)* This adds a new '--tag' parameter so that we can   build docker images on a particular tag, not only ':latest'   as given by Docker * It opens up the possibility of generating ""staging"" images on   a different tag, in the same servers as we keep the production   images * By default it keeps previous behaviour of using ':latest' tag.",1
"Fix graph_tuner ancestor duplication (#7704)A diamond dependency currently generates a duplication of ancestors:  A / \B   C \ /  Dunder some conditions, this scenario leads to the following dictionnaryentry: ""D"":[A, A] instead of ""D"":[A].This results in failures when subsequently trying to transpose nodestates based on this data.Change-Id: I72f9b19286bbab0581b851c228b9d0e79ead400f",0
Fix GraphModule.load_params to allow passing parameters that are not an expected input (#7665),0
[TORCH] Implement avg_pool1d (#7694)* [TORCH] Implement avg_pool1d* [TORCH] Unify creation of avg_pooling operations* [TORCH] Add tests for avg pooling with padding* [TORCH] Make format checks happy with unified avg_pool,1
"[METAL] Fix memory leaks in Metal runtime (#7714)* [METAL] Fix memory leaks in Metal runtime1. In case when we build runtime without ARC, we can have problems with   memory releasing. Due to some of Objective-C methods returns   autoreleased pointers, we should specify `autoreleasepool` blocks to   determine life cycle of these pointers.2. Added workaround for problem with work group size.   Sometimes auto scheduler generates parameters when work group size   is more than possible. And in this case we got assert from Metal   library. Added check for this situation and it helps to avoid   assert.3. Fixed memory leak problem when fill tensor by random data.   DLManagedTensor increases reference counter in NDArray but nobody   delete this DLManagedTensor in proper way. This is why memory which   was allocated by NDArray was never released.4. Removed unnecessary retains. It is not necessary use retain in some   places where they were used, due to we build metal runtime without   ARC.* Use const_cast instead of creation DLManagedTensor",0
"[microTVM] Update nrfjprog on reference virtual machine (#7723)* update nrfjprog and integration test* merge* Revert ""merge""This reverts commit 58d5d9187448e6580b6b780821eb2ea42ec34e8e.* fix comments* fix clang* revert format* new line* format",0
[FIX] Fix temporary allocation size in threefry (#7709)* [FIX] Fix temporary allocation size in threefry* bump sizes,0
[ONNX] Onnx node tests (#7720)* WIP* some fixes* more fixes* fix some conv_transpose tests* fix out of bounds slice* fix flatten import* fix logsoftmax and softmax tests* fix Error in Upsample* fix onehot* normalize errors* fix gather with negative indices* parameterize test* skip unsupported tests* clean up* fix rebase* fix lint* add an error message when we find an un-identified tensor,0
[TVMC] Python Scripting Init Files (#7698)* add to init files for clean tvmc python* black reformat init.py* adjust tests to new imports* black test files* tell lint ignore defined-builtin error for tvmc compile* add colon to match lint syntax* change import so must use tvm.driver.tvmc instead of tvm.tvmcCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>,0
"[µTVM] Rev ci-qemu to 0.02 (Introduce onnx python dependency) (#7728)* Fix ci-qemu build, add ONNX* rev ci-qemu to staging",0
"[crt] fix heap corruption from bad allocation (#7735)The type of runtime->storage_pool was changed at some point from TVMNDArray to TVMGraphRuntimeStorageEntry. This change was not reflected in the call to the allocation for its buffer. If this unclaimed space is allocated to something else, data corruption will happen.",0
[TensorIR] Fix parser autocompletion mode (#7737)Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,0
"Better grouped convolution for CPU targets (#6137)* integrated with v0.8* Rebase, and undoing accidental removal of auto scheduler NHWC support* Added ASF license header* Minor bug fixes* Added asymmetric padding supportFixed linting* Improve linting* Better linting, disable final linting checks* Fixed final linting errors (figured out how to run lint tests locally)* fixing linter formatting part 1* fixing linter formatting part 2* fixing linter formatting part 3* Update conv2d.pyFixed merge issue* Rebase, and update responding to some comments* Fixed AutoScheduler bug for NHWC case* removed infer_pad from GSPC* Rebase, and undoing accidental removal of auto scheduler NHWC support* Added ASF license header* Minor bug fixes* Added asymmetric padding supportFixed linting* Improve linting* Better linting, disable final linting checks* Fixed final linting errors (figured out how to run lint tests locally)* Update conv2d.pyFixed merge issue* Rebase, and update responding to some comments* Fixed AutoScheduler bug for NHWC case* Minor fix* Fixed removal of infer_pad to no padding* Fixed unexpected linting errorCo-authored-by: Perry Gibson <Perry.Gibson@glasgow.ac.uk>",0
"[Topi, Relay] Add cumprod (#7722)* make cumbinop, refactor cumsum, add cumprod* cumsum exclusive test* Add cumprod + flesh out cumsum testsadd cumprod and testsreinstate testsrethink* add rudimentary scan implementation* add attributes of cumprod node* add cumprod strategy* add cuda strategy* python relay node construction* change attrs to be reusuable* add cumprod nodes* complete tests* Fix some typos about sum --> prodtypos fix sum -> prodmore typosmore typo fixesmore typosadd doc strings* Use Bool instead of int to represent exclusivemake exclusive a bool up and down stackfix xfix bool errit is a bool nowfixfix thingformatting to pass linterlint pythoncumprod pylintfix attributefix orderingadd exclusivity tests for end to endfix thingscuda identity_value* Overall improve formatting, add doc message correctionssimplify constructionclang-formatmore testsundo simpler construction due to function passing stufffix docsmore exclusive doc changesmore fixins""* merge cumsum and cumprod to scan, merge testsfix stuff* remove other mentions of cumbinop -> scanop* lint formattingCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@Andrews-MacBook-Pro.local>",0
"Fix missing <cassert> header, caused compilation failure. (#7740)",0
[CI] Temp disable rust docs build (#7743),2
[Refactor] Rename TVMContext to Device (#7721),5
[Bugfix] Fix usages of logging-related macros (#7748),0
Bump ci-cpu and ci-arm container versions. (#7745),5
"[docs] Getting Started with TVM: AutoTVM and Matrix Multiply (#7643)* [docs] Getting Started with TVM: AutoTVM and Matrix MultiplyThis patch moves the matrix multiplcation example tuningwith AutoTVM to the tutorial directory, and expands on thecontent. This follows and builds on the section on TE* Applying lint style* Fix license* Apply suggestions from code reviewCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Change comparison tolerance to smaller valueCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",0
[RUNTIME][WEB] Cleanup logging for web runtime. (#7750)* [RUNTIME][WEB] Cleanup logging for web runtime.The log(info) won't work for web runtime due to the usage of timefunction.- Introduce TVM_LOG_CUSTOMIZE anf TVM_LOG_STACK_TRACE.- Reorganize the log customization code to wasm_runtime(so non-gpu usecase can apply).- Update the testcase to cover the logging.* Fix windows build and address comment.,0
[ARITH] detect iter affine map with predicate (#7752),5
"[docs] Getting Started: Introduction and Installation (#7638)* Getting Started: Introduction and InstallationThe first two sections of the ""Getting Started with TVM"" guide.A high level introduction to TVM, and a slight introductionon installation options.Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>",2
[Logging] Bring back the stack size optimization (#7756),2
"Clean up uTVM demo runtime, add ONNX model test and tutorial (#7557)* Some docstring fixes.* Couple of small fixes:- Use `west attach` instead of `west debug` in commandline to prevent  debugger from resetting device.- Fix warning on use of led_pin in zephyr-runtime/src/main.c.* Adding Zephyr demo runtime.* Cleanup of uTVM tests and demo runtime.* Working on QEMU support.Need to add board-specific prj.conf files.* Adding board-specific prj.conf files.* Some cleanup.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Adding data for unit tests.* Cleanup demo_runtime code.* Fix up tutorial.* Couple of small fixes:- Use `west attach` instead of `west debug` in commandline to prevent  debugger from resetting device.- Fix warning on use of led_pin in zephyr-runtime/src/main.c.* Adding Zephyr demo runtime.* Cleanup of uTVM tests and demo runtime.* Working on QEMU support.Need to add board-specific prj.conf files.* Adding board-specific prj.conf files.* Some cleanup.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Lots of hacking to get ONNX model to run on QEMU and nRF5340.Added test_onnx unit test.Still need to clean up tutorial.* Adding data for unit tests.* Cleanup demo_runtime code.* Fix up tutorial.* Fix tutorial.* Fix tutorial and runtime.* Fix merge conflicts.* Fix merge conflict.* Remove redundant files.* Revert dep.* Fixup* Add new files to check_file_type.py.* Adding missing ONNX file.* Fixup docs.* Fix linting rule.* small fixes* Add missing file to check_file_type.py.* clang-format this file.* Fix formatting.* Black formatting.* Lint comments.* Fix path for test.* Bump CI.* Update from_onnx.* fix path* Fixing* Revert dmlc-core to 21cc7de0dc9fd6acb796e1be6181fa8e6b6c8f41* Fix path again.* Fix tutorial to not use actual Zephyr.* Revert submodule version change* Fix bad merge.* Trying to fix this mess.* Fix formatting.* context -> device* Removing tutorial since I can't get it to pass CI.Co-authored-by: Mehrdad Hessar <mehrdad.hessar@gmail.com>Co-authored-by: Andrew Reusch <areusch@octoml.ai>",0
Make Autopad static when available (#7755),5
[VTA] Make more explicit error message during sim lib loading failures. (#7761),0
[ARITH] normalize iter affine map expr to PrimExpr (#7759),5
[FIX] Fix android projects (#7764)src/runtime/logging.cc was missing from the runtime files list,0
Rename GraphRuntime to GraphExecutor (#7653),5
[ONNX] Enable GPU in ONNX importer tests (#7438)* remove hardcoded target and ctx* fix c-codgen for floating point mod* MDisable onnx gpu test for argmin / argmax so we can get this fix merged. Matt or myself will fix later but we don't have time right now.* lint* fix black* Add flag to task_python_frontend.sh to only run GPU enabled tests on GPU* black again* Enable GPU for test_nonzero* Respond to comments* Don't test batch matmul on CUDA* Turn cuda off for dynamic batch matmul test* Fix task script* Flaky test* another flaky testCo-authored-by: mbrookhart <mbrookhart@octoml.ai>,0
Add support for using the VM across the RPC boundary.  (#7746)* Get basic verison of VM RPC working* Test case passes* Clean up PR* Lint* Format* Address Andrew R and TK feedback* Add comment for Andrew* Address Zhi's comment* Format* Fix broken test,0
[Autoscheduler][Sparse] Add sparse dense end to end model tuning support for x86/arm cpu & Some bug fix (#7635)* Add sparse dense end to end model tuning support* Add sparse tuning for arm network* Bug fix for tflite frontend dense with layout rewrite* Move the random_bsr_matrix to sparse.utils,0
Fix typo in include/tvm/runtime/crt/crt.h and NEWS.md (#7770)* Fix typo in include/tvm/runtime/crt/crt.h and NEWS.md,0
[Relay]Frontend][Onnx] Add a converter for ATen Nodes (#7747)* Add support for ATEN DLRM ops.* Fix bugs and test.* Force Aten mode and add extra aten ops.* CI torch version is too old for aten argument.* Add assert for embedding_bag node.* Use new Aten override argument.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,0
[GO] Fix go bindings (#7696),0
[crt] fix shift out of type bounds (#7733)* [crt] fix shift out of type bounds,0
"[Target] Add support for target object with host field compatible with previous api (#7534)* Fix legacy code on target host* Modify legacy code for target host change* Add tests and fix merge issue* Add condition for same host* Modify all files for new target host api compatibility* Add newline* Change import format* Optimize test file* Add match error info for unit tests* Fix for heterogeneous targets* Fix format for dict iteration* Fix target host type error* Skip one testcase for tvm infinite loop bug* Fixed bug for target map compatibility* Fix another TargetsMap issue* Fix typo and infinite loop error* Temporary fix for handle issue* Fix vm target* Add condition support for str case* Add GetHost function and fix previous bugs* Fix measure_record.cc* Fix search_task.cc* Fix compiler.cc, memory_alloc.cc* Fix driver_api.cc* Fix format* Fix bugs and GetHost function usage* Fix clang format* Fix bug* Modify python tests* Change python unit tests to new target api* Fi test_runtime_heterogeneous.py* Modify tutorials & remove extra print* Update more tests to new api* Refine the tutorial target usage* change argument name for Target constructor function* Fix target export function* Fix and validate all tutorial usage* Remove unused argument* Fix format* Fix bug in driver/build_module.py for heterogeneous target* Fix bug in driver/build_module.py for heterogeneous target more* Fix target host type error* Fix cudnn target host bug* Fix according to reviews, add helper function in python* Refactor code as helper function* Expand helper function* Fix bug add and update python helper function* Update target hosts* Fix format & refresh function* Fix unit test bug* Fix bug in refreshing host* Fix bug* Add SetHost function* Update export function* Fix format* Fix export bug in target* Fix bug on host referencing* Addtional tests* Address review issues* Fix format target.py* Fix issues and format* Add some 3rd party dependencies* Merge main branch* Fix target.h format* Remove redundent import* Fix function name* Add parameter name* Fix new code bug* Fix bug in lowering",0
[PYTHON][RPC] Make rpc proxy jupyter friendly via PopenWorker. (#7757)* [PYTHON][RPC] Make rpc proxy jupyter friendly via PopenWorker.* Rework the contrib tests that was previous broken.,3
"[Relay][Pass] ConcretizeLike and EliminateIdentity rewrites for SimplifyExpr (#7731)* factor out some common code for DF rewriting, add ConcretizeLike* slight refactoring, add EliminateIdentity pass* lint* merge ConcretizeLike and EliminateIdentity into SimplifyExpr* nits and lint* remove static stuff* document* definitely ran clang-format but ok* make ToScalar return optional, fix missing virtual destructor* lint* tweak scalar conversion API to maintain compatibility",0
[TE] Bugfix for reduction that involves multi-outs with where cond (#7692),0
"[CodeGen][OpenCL] Limit OpenCL built-in vector lanes to 2, 3, 4, 8, 16. (#7777)",5
[ARITH] Subspace division (#7760),5
[microTVM] Fix RVM onnx dependency and Zephyr document update (#7774)* fixing poetry* fix onnx issue* add zephyr README* Update README.md* clean up* moved onnx* replace with poetry* add tflie,0
"[Profiling,VM] Profiling interface for VM and Graph Runtime (#7624)* [Profiling,VM] Profiling interface for VM and Graph Runtime* lint* fix test* make profiling test actually run* Try to better match the graph runtime function names to vm* formatting* DurationNode.value -> microseconds; PercentNode.value -> percent; make frame sorting optional.* renaming for the tvmcontext -> device change* formatting* remove old vm profiler get_stat api* fix tests",0
[CI] Rust CI Changes (#7773)* Tweak CI* WIP* CI Tweaks for Rust CI* Fix* Fix LLVM issue,0
Fix compilation errors with clang 11 (#7783)- Replace llvm::MaybeAlign::MaybeAlign() with llvm::MaybeAlign().- Use ICHECK instead of assert in hexagon_module.cc.,0
"[Hexagon] Reenable compilation of TVM runtime for Hexagon (#7784)- Add support for Hexagon SDK 4.x (different directory structure)- Conditionally disable functions not present on Hexagon (popen, etc.)- Bump sim_dev architecture target to v65 (older versions can still be  used with older compilers).Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>Co-authored-by: Ravishankar Kolachana <quic_rkolacha@quicinc.com>",1
add the --net=host cmd line arg to the docker/bash.it script (#7780),1
[ONNX] Dynamic Gather (#7787)* add regression test* fix regression* fix lint,0
"[TVMC] Separate model loading from model compilation in TVMC. (#7739)* add to init files for clean tvmc python* adjust tests to new imports* add to compiler.py* update so model loads in drive_compile* update test_compiler.py to load outside of tvmc.compile, need to correct one error* fix mock.patch test* remove merge artifact (circular import issue)* change typo and merge artifact* fix import in test_compiler.py* black needed files* remove unnecessary argument model_format from compile_module* load before compile in conftest.py* fix conftest.py issue* fix typo in test_compiler.pyCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>Co-authored-by: Josh Fromm <jwfromm@uw.edu>",0
"[docs] Getting Started With TVM: Tensor Expressions (#7768)* [docs] Getting Started With TVM: Tensor ExpressionsUpdate of getting started with tensor expressions.Adds a matrix multiplication example to be used in later tutorials,makes CPU primary target and GPU optional for wider audience reach.* Fix linting",0
"[TVMC] Allow direct numpy inputs to run_module (#7788)* progress, graph params need to figure out* black and lint* change np.load(inputs_file) to happen in drive_run* make inputs optionalCo-authored-by: Jocelyn <jocelyn@pop-os.localdomain>",2
"[PatternMatcher] Support matching tuples, call nodes, and functions with variable numbers of inputs (#7754)* Allow TuplePattern to have null fields and match any tuple* support matching functions and call nodes with variable numbers of parameters* remove development code that was commented out* add docs for fuzzy matching",1
Disable Rust CI (#7793),5
[Target] Fix empty target and host for autotvm task (#7791),0
[AutoScheduler] Add task.desc for its function name (#7794),1
[Relay]Frontend][Onnx] Remove pop that interferes with nested loops. (#7781)* Remove popping that interferes with nested loops.* Only check user inputs in the outer-most graph scope.* Fix style.Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,0
"[microTVM] Update Zephyr 2.5 (#7786)* update to zephyr 2.5* unbreak test_zephyr* fix stack size* always create packer.log* add qemu debugging* fix transport with debug false* size down ring buf, shouldn't need to be so large* update to zephyr 2.5* fix buffer size* cleanup* cleanup* remove debugger* nit* update ci script* remove debug mode* fix packer log* comment* update ci_qemu* change zephyr version on Vagrant* make it compatible to zephyr 2.4Co-authored-by: Andrew Reusch <areusch@octoml.ai>",0
[µTVM] Try to fix qemu hangs in the CI #7590 (#7769)* Try to fix qemu hangs in the CI. * Remove __pycache__ directories only underneath checked-in   subdirectories to hopefully avoid long find runtime.* try just removing the check,0
[CI] docker images build script cmd line args optional (#7776)* allow COMMAND to be empty when building a container* clarfiy the difference between build.sh and bash.sh* fix typo and highlight command as option in usage snippet,0
"[docs] Getting Started with TVM: Auto Tuning with Python (#7767)* [docs] Getting Started with TVM: Auto Tuning with PythonFollows up on the TVMC tutorial, shows how to accomplish the same tasks using the python api* Apply suggestions from code reviewCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Fix some rendering errors, add descriptions of tuning parametersCo-authored-by: Cody Yu <comaniac0422@gmail.com>",0
ONNX bitshfit (#7800),5
[FIX] tvm.testing.parametrize_targets documentation for arguments does not match what it is acutally using. (#7778),0
"[µTVM] apps: Fix Zephyr code example for STM32F746 boards (#7772)Commit c39a6e25d ""Clean up uTVM demo runtime, add ONNX model test andtutorial (#7557)"" changed the location of the Zephyr code example toapps/ so this commit updates the tutorial examples under tutorials/microto reflect the new location where src/main.c resides.Since commit c39a6e25d also split Zephyr configuration per boards,under project's boards/, this commit also adds a proper config for theSTM32F746 Discovery board and fixes a nit in the comment inboards/nucleo_f746zg.conf. It also removes CONFIG_MAIN_STACK_SIZE=50 fordisco and nucleo boards since the MCUs for both boads are Cortex-m7based, not Contex-m33.For the new boards/stm32f746g_disco.conf CONFIG_TEST_RANDOM_GENERATOR=yis set, otherwise when CONFIG_ENTROPY_GENERATOR=y linking will failwith the following error:rand32.h:33: undefined reference to `z_impl_sys_rand32_get'Finally, the size of 'uart_data' temporary buffer is reduced a bit (to8 bytes) to free some additional bytes in SRAM, since most MCUs have a1-byte FIFO, like it happens with Cortex-M-based MCUs.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[TVMC] Runner.py Updates (#7779)* change runner to ms instead of s, consider reformatting* adjust formatting and test in test_runner.py to be more realistic* change device in run_module runner.py to be mandatory* make hostname optional in run_module, in runner.py* update order and doc string* remove print statement* black files* device error lint* argument order was incorrect* arguments funkiness attempt fix 2* Fix merge with main.Co-authored-by: Jocelyn <jocelyn@pop-os.localdomain>Co-authored-by: Josh Fromm <jwfromm@uw.edu>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>",0
[ONNX] Initial work to import pre-quantized ONNX Models (#7802)* Add QuantizeLinear and DequantizeLinear* DynamicDequantizeLinear,1
[FIX] Make HashCombine stable across platforms (#7801)* [FIX] Make HashCombine stable across platformsPR #7605 inadvertatly broke cross platform hashing when when it switchedsize_t to uint64_t. This cause a different specialization of HashCombineto be used. Unfortunately the new specialization used std::hash which isimplementation dependent. I've added tests to make sure this doesn'thappen again.* fix template specialization issues,0
[COMMUNITY] @kevinthesun -> PMC (#7803),3
[M1b] Scaffolding ScheduleState data structure (#7765)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>,2
[TVMC][VitisAI] Enable Vitis AI target through TVMC (#7577)* Enable Vitis AI target through TVMC & change PassContext API's* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/target/vitis_ai.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Change Vitis AI  API to  & address comments & fix linter issues* Update docs/deploy/vitis_ai.rstCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update docs/deploy/vitis_ai.rstCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Add Vitis AI initiliazation to separate init config in TVMC composite target registry* Lazy load pyxir package in Vitis AI codegen to avoid hard dependency for TVMC* Fix TVMC Vitis AI test for compiler.compile_model API change* Lazy load pyxir package in Vitis AI partitioning passCo-authored-by: Jorn Tuyls <jornt.tuyls@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>,0
[Rust] Make TVM Rust bindings installable via Cargo.  (#7503)* Make TVM Cargo installableRewrite the Rust Module API and change some imports causing crashes.This commit also updates the docs to remove outdated information.Fixes for version bumpUpdate build.rs to use new tvm-build versionTweak build.rs to use release version of tvm-buildAdd docsAdd Readme for tvm-sys crate.Fix Cargo verisions for pre-releaseAdd READMEMove generated code to OUT_DIRFix pathAdd descp for tvm-sysTweak versions for publishingTweak versions for publishingAdd README for tvm-graph-rtConform to Apache branding guidelinesFix capsAdd headerRemove warningFormatClean up buildTurn docs back onTweak CIWIPRemove CI changes* Disable docs* Fix,0
Added missing include file (#7808),1
introduce pass lower_init_block (#7806)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,4
[microTVM] Zephyr: RISCV support for Zephyr QEMU RISCV-32/64 (#7804)* working on qemu* debugging* riscv hacks* config added* change target platforms* fix merge* debugging issue with zephyr 2.5* cleanup* working on qemu* debugging* riscv hacks* config added* change target platforms* fix merge* debugging issue with zephyr 2.5* cleanup* testing* pass riscv64* fix merge* small fix* update vm_name* add zephyr version* add comment for riscv32 issue* remove debug messages* cleanup* cleanup* change workspace* fix zephyr version* cleanup* change to symlink* fix flag* add comment* lint check* lint fix* fix format* rename debugger* rework argsCo-authored-by: Mehrdad Hessar <mhessar@octoml.local>Co-authored-by: Andrew Reusch <areusch@octoml.ai>,0
[TF frontend][bugfix]Avoid making a new node when already has span info (#7789)* Avoid making a new node when already has span info* add test* add test* add test* fix* fix* move test to test_forward.py* fix* fixCo-authored-by: xiaoqiang.dan <xiaoqiang.dan@streamcoputing.com>,0
Add logical_not shape registration. (#7820)Co-authored-by: Ubuntu <jwfromm@jwfromm-cpu-dev.itxhlkosmouevgkdrmwxfbs5qh.xx.internal.cloudapp.net>,1
[Vulkan] Support uniform buffer object for passing many scalar arguments (#7717)* ubo codegen first cut* begin runtime change for UBO* allocate and bind ubo* query memory type for uniform* refactor* do not use float64* trying an approach similar to push constant* add more log* do not delete ubo when not using it* cumsum and nms test working with ubo* remove log* cleaning up* formatting* revert BufferArgument change* refactored codegen* minor fix* introduce value kind for ubo* fix cpplint and revert float64 change* query push constant size using runtime API* let vkmap/unmap allocate and delete host_buf* doc update* fix typoCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,0
"Revert ""[Vulkan] Support uniform buffer object for passing many scalar arguments (#7717)"" (#7821)This reverts commit 5bc1cec4c4acf0a54889227c1d19a6b65b6803c2.",4
[ONNX] Support optional outputs for ONNX nodes (#7818)* Support optional outputs for ONNX nodes* add comments,1
fix compiling warning in simplify_expr.h (#7828),0
[BugFix] Fix the race condition issue of packed func. (#7246). (#7619)Co-authored-by: wenxizhu <wenixzhu@tencent.com>,0
[DOC] Grammar fix (#7824)A grammar fix for runtime document.Co-authored-by: Joey Tsai <chunit@qti.qualcomm.com>,0
Fix Metal accuracy problem caused by <dtype>3 vectors usage (#7830)On example of float3 datatype:Using of float3 data type for loading of data cuncurrently into dense array sharedbetween all threads in Metal threading group can lead to data race between threads.float3 datatype has size and and alignment eq to 16 bytes while kernel assumes tocopy 12 bytes in arbitrary not aligned places.Using of packed_float3 datatypes solves the issue,0
"[TVMC] --disable-pass option added to compile mode (#7816)* [TVMC] --disable-pass option added to compile modeAdded --disable-pass option to TVMC compile mode to disallowcertain supplied passes in PassContext for the compiler.Change-Id: Iae1849d7b051ac9288509dc458a58788c865537a* Added test, addressed requestsChange-Id: If688f65441d3aa9967ab823adf899cfc704bd097* added printing of available passesChange-Id: I7a4706c03c0d64cade4977d431bcb25b3708f213* C0415(import-outside-toplevel)Change-Id: I33d6f6f86d182de2e21e895ec2dfe9f11f5916dd",1
[COMMUNITY] Bohan Hou -> reviewer (#7837),3
[COMMUNITY] Siyuan Feng -> reviewer (#7836),3
"[µTVM] Add support for mps2_an521 board (#7813)* [µTVM] Zephyr: Allow user inform if a board is emulatedSome boards supported by Zephyr that run emulated by default, i.e. their.yaml config file sets the field ""simulation: qemu"", don't have theprefix ""qemu_"" on their names, so µTVM can't currently recognize it asan emulated target to properly use the QEMU transporter (instead of theserial port) to open a session against it. Such a boards usually havereal physical (hardware) counterparts, being specific boards and notgeneric or ""fake"" ones simply tied to a CPU type of interest.That commit allows the µTVM user to explicitly inform that µTVM needsto use the QEMU transporter to open a session against a given board byadding the suffix ""-qemu"" to the board name. That is necessary becausefor boards that don't have the name prefixed by ""qemu_"" and even thoughrun emulated by default on Zephyr there is no easy way to detect them,since it's not possible to determine it by looking at any Cmakegenerated file or by using the `west` command to query that info.The case where the board is emulated by default but has the prefix""qemu_"" in its board name is already handled by the current code.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Add new target mps2_an521This commit adds a new µTVM target to support the Arm reference boardMPS2-AN521, which is based upon a Cortex-m33 core.For more details about that board, please see:http://developer.arm.com/tools-and-software/development-boards/fpga-prototyping-boards/mps2Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [µTVM] Add an example for the mps2_an521 boardThis commit adds an example on how to run the Zephyr demo under apps/using as a target the Arm mps2_an521 board, which is emulated by defaulton Zephyr. The example is added to the tutorial script micro_tflite.py,where other examples for other targets exist.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix lintFix lint accordingly to the CI error.* Satisfy lintSatisfy lint about boolean expression format.* Address suggestion from AndrewAddress suggestion from Andrew in the review.Also updates the comment about suffix being trimmed off.Thanks,Gustavo",0
[TensorIR] [Script] adding support for opaque block (#7829)* change complete tag* add parsing support for opaque block* address and add testcase* address* address,1
[RUNTIME] Add clear() function in tvm::Map class (#7826)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,1
[Topi & Relay] Add quantization support for the vision transform model in GPU (#7814)* Add cuda batch matmul int8 support for quantized vit model* Fix for combine parallel pass with dense and batch_matmul* Reformat based on lint* Add plevel & update the file download method,0
[Relay][Pass] SimplifyCastLike/Cast and ConcretizeFullLikeRewrite rewrites for SimplifyExpr (#7827),4
"Misc. improvements to documentation/build setup for first-time builds. (#7840)- Makefile  - Target ""crttest"" ignored OUTPUTDIR variable- .gitignore  - Added ignores for download test data/models.- docs/README.txt  - Missing quotes on sphinx dep, needs pinned autodocsumm version- docs/contribute/pull_request.rst  - Use ""ci_lint"" docker image  - Updated C++ test instructions to refer to the from_source installation for gtest.  - Updated python test instructions with synr package dependency- docs/langref/relay_expr.rst  - Updated reference for example usage of TempExpr. `src/relay/pass/alter_op_layout.cc`    no longer exists, and `src/relay/transforms/alter_op_layout.cc` doesn't use TempExpr.    Picked a different use case as example.- tests/scripts/task_cpp_unittest.sh  - Updated ""make crttest"" to run only if ""USE_MICRO"" is enabled.  While USE_MICRO is always enabled    in the CI builds, task_cpp_unittest.sh is also recommended for use in    docs/install/from_source.rst, which does not mandate USE_MICRO.- docs/install/from_source.rst  - Added -DMAKE_SHARED_LIBS=ON to the google test cmake config.  By default, only static libs are    generated for gtest, while TVM's build preferentially selects the shared libs.- tutorials/get_started/auto_tuning_with_python.py  - Changed norm_img_data to avoid loop, improve readability- tutorials/get_started/relay_quick_start.py  - Previous version used different input data passed to the initial and deployed module, then    asserts that the results should be the same.  Modified so that the same input data are passed in    both cases.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[FIX] Fix howto_deploy (#7841)We were missing files in tvm_runtime_pack.cc,0
[ONNX] Make input shape immutable (#7844)Co-authored-by: Yanming Wang <yanmwang@amazon.com>,2
[BugFix] Print doubles with precision 17 in SaveJSON and TVM script printer (#7846)* [BugFix] SaveJSON type double with precision 17* [BugFix] Fix for TVM script printer,0
"[TOPI, Relay] A new NMS op variant for ONNX NMS / TF Combined NMS (#7796)* initial import* add c++ boilarplate* add python boilarpolate* update onnx frontend* fixing* update onnx frontend* fix shape* minor update* fix* fix shape func* fix for no box* more fix* made things 64 bit* int64 tweak* max_output_size doesn't need to be a callback* remove all_class_nms schedule* minor simplify* remove expand_dim* refactoring* simplify nms loop* cpu all_class_nms stub* updating ir for cpu* working with cpu* update cpu strategy, relay op also working* fix cpplint* fixing pylint* enable gpu test for onnx nms* tweak parallel* pyformat and lint* fix relay nms test* doc update for cpp relay* updating tests* updated tests* fix converting score_threshold to Expr* update doc* doc fixCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>",0
[BYOC][TVMC] bugfix: disabled_pass -> disable_pass (#7850)Change-Id: I22d24a2e219103485a6a1519ce6256a104103ebb,0
"Fix PyTorch matmul conversion when given (2-dim, N-dim) input pair (#7845)* [AutoScheduler] Fix incorrectly array context device and hide info at the beginning* Lint fix* Lint fix* update repo* Fix Pytorch matmul conversion when given (2-dim, N-dim) input pair* update measure.py* Lint fix* fix bug && add ut for pytorch matmul* update ut* Lint fix* update commit* Lint fix",0
[ConvertLayout] Squeeze and reduce ops (#7835),5
thread local handle for rocblas (#7851),5
Allow microTVM Reference VM to be launched when TVM is a submodule. (#7854),5
"Fix Zephyr flashing on physical hardware, busted in #7813 (#7853)",0
[FIX] Fix RPC for the VM (#7810)* [FIX] Fix RPC for the VM,0
Fix typos in comments (#7862)Fix two typos in comments.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
[Vulkan] Support uniform buffer object for passing many scalar arguments (Take 2) (#7833),4
[TIR] Add a new intrinsic count leading zeros for LLVM and SPIR-V (#7825),1
[TensorIR][M1c] LCA detector (#7848)Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,5
"[METAL] Fix issue with GPU fails (#7819)* [METAL] Fix issue with GPU failsAdded first run to auto scheduler. This run is necessary for checkingthat the generated kernel is correct. When we just run time evaluatorwith incorrect kernel then it is possible that our application on iOSdevice will be added to ignore list because of big number of committedincorrect kernels. One run before running auto scheduling helps us toavoid this problem.Added complete handlers to all command buffers in Metal runtime. Ithelps to handle GPU errors and report about this error to the hostapplication.In case when error happened, we have to create a new stream. Addedmechanism for error handling and streams creating from python interface.* Try to fix QEMU build* Apply comment* Apply comments and fix build* Apply comments and fix lint* Fix CI",0
[TOPI][SPIRV] Cast to float32 not float64 before log2 in sort/scan (#7669)* [TOPI] Cast to float32 before log2 in sort/scan* revert sort change since this seems unnecessary* only does cast to float32 on vk + dynamic input case* check against IntImm instead of Var* revert change* use clz for ceil_log2 when compiling for vk* add doc on ceil_log2* fix pylintCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>,0
[Hotfix] typo in Vulkan runtime change causing severe perf regression (#7871)Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>,0
[TensorIR][M1b] Schedule class (#7847)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Jared Roesch <roeschinc@gmail.com>,2
"[µTVM] Zephyr: Add STM32F746 disco board as a test platform (#7863)Add STM32F746 Discovery board as test platform so tests can run againstit by using:$ pytest test_zephyr.py --microtvm-platforms=stm32f746xx_discoSince that board has the same MCU identifier as the ST Nucleo board,the test platform identifier for Nucleo board is renamed and asuffix _nucleo is added to differentiate it from the ST Disco board.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[frontend][tflite] float16 quant support (#7736)* [frontend][tflite] float16 quant support* remove skip conditions in tests,3
[TIR] An analysis pass to calculate workspace size for primfuncs (#7859)* Add workspace size calculation for primfuncsThis commit introduces functionality to query the workspace sizeas required by a tir primfunc by looking at tir.allocates inside of itChange-Id: I6f8ca90408b6e35d17ec818998a0f158a268a2a6* Add workspace size calculation for primfuncs*change int --> size_tChange-Id: If7fafec0269937d70184e7696e44386b74116d86* Add workspace size calculation for primfuncs* int --> size_t change for analysis.hChange-Id: I9e5c5e5f8458663390c50cf56f1a11910687928d* Add workspace size calculation for primfuncs* lambda scope fixChange-Id: I0c9b4c529150de8e0a5e170887cc935b7d0f6af2Co-authored-by: Chenfan <jcf94@outlook.com>,0
"[Relay] Recursive destructor call replaced with non-recursive for Call nodes. (#7832)* [Relay] Recursive destructor call replaced with non-recursive for Call nodes.Recursive destructor call replaced with non-recursive (based onExpandDataflow) for Call nodes. This prevents OutOfStackexception during unwinding a chain of destructors for large-sizedsubtrees based on smart-pointers.Change-Id: Ib9da3ff8af3a0a41287b8ce9ab2bee2d0813d01cAddressed requested changesAddressed requested changes, simplified the codeadded unit testChange-Id: I7fdd44da3b6c366a555fd9157fa3630b6e789d64* removed inline befor Call destructorChange-Id: I6328e423670f185393d50ccd3d6fdc1326be3767",1
[VTA] Update vta-hw dependency (#7874),1
[BYOC][ACL] ACL migrated to v21.02 (#7649)This PR switches ACL* version from v20.11 to v21.02ACL stands for Compute Library for the Arm® Architecture.Change-Id: Id364b571d5611ca6eb6d2bde09448a65aae3f73b,4
"[µTVM] Zephyr: Add MPS2-AN521 board as a test platform (#7864)Now that MPS2-AN521 board is supported as a µTVM target, add it as testplatform so tests can run against it by using:$ pytest test_zephyr.py --microtvm-platforms=mps2_an521Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
[TensorIR][PASS][M1c] PlanUpdateBufferAllocationLocation (#7873)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,1
[ONNX] Fix more upstream tests (#7842)* fix unsqueeze test* fix dynamic strided slice with negative indices* add Shrink importer* fix selu defaults* Implement Hardmax* add a comment to the test* Fix typo,0
Protect child process enumeration in AutoTVM (#7887),5
[VTA][OpenCL] intelfocl (#6126)* intelfocl support* disable tsim test* bugfix to vta autotvm* disable tsim test in task_python_vta_tsim.sh* fix integration test* update vta submodule and re-enable tsim tests* remove unnecessary comments,0
[PROFILER] Add CSV output to profiler (#7797)* [PROFILER] Add CSV output to profilerThis patch changes the profiler output from a string to a Report object.A Report can either output CSV or the usual human-readable table.* no spaces after commas* Update src/runtime/profiling.ccCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* fix gcc* fix test* overall percent fix* rename overall -> device_metricsCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>,0
[Relay] Add support for relay expressions as pad value for static pad (#7860)* add support for expr as inputs to pad* fix improper amount of args* add dynamic padding test* infer type better test* add comments to type relations* fix infer type layouts* proper return shape* proper shape infer type* make the tests pass by setting the conditions* make codegen reflect reality* make ternary operations more pythonic* proper infer layout* fold explicit padding* fix pattern matching in contrib* revert tests for contrib now that pattern matching works* revert import changes* add newline,0
"[RPC][REFACTOR] Use PopenWorker to handle RPC Server. (#7889)Previously the rpc server relies multiprocessing to start a new process and does not work under jupyter.It also have a popen mode that does ensure the socket start listening before returning the port number.This PR switches the implementations use PopenWorker. The port number is returned after the socketget binded, which resolves some of the RPC flaky issues(need sleep to wait the server to start).It also makes the RPC server jupyter friendly.",1
Add support for the quantized RESIZE_BILINEAR operator to relay TFLite frontend (#7866)Change-Id: I46008e5b7edc49d32847acd6d166374a8d85058g,1
"[Frontend][Tensorflow] Support SAME padding for dynamic h, w when stride == 1 (#7885)* Support SAME padding for dynamic workloads when stride == 1* Fix lint* Fix lint",0
"[ONNX][TOPI][RELAY] Resize refactor (#7883)* adds rounding mode for nearest neighbor, passing onnx unit tests for nearest neighbor* passing all linear test. passing all nearest tests except crop and resize, which needs a dynamic implementation of crop and resize* most of the bicubic tests are working* fix exclude outside* remove dead code* fix lint* fix defaults to match old implementation* fix lint* fix gpu tests* fix lint again* change order of operations to prevent GPU rounding errors",0
"[Runtime] Driver version + consistent clock speed units (#7867)* Added kDriverVersion to DeviceAttrKind, implemented for VulkanDeviceAPI.The vulkan backend has had inconsistencies that look correlated todrivers used.  This will help in collecting information fortroubleshooting.* Changed units for OpenCL's clock rate from MHz to kHz, to match Cuda/ROCm.* [Docs][Runtime] Additional documentation for tvm.runtime.Device, DeviceAPI feature matchingPrimarily documentation, with some changes to the OpenCL DeviceAPI tomatch available features in cuda/vulkan.* Added CL_TARGET_OPENCL_VERSION definition, for use with unified OpenCL headers.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Docs] Update Dev. Doc. on how to add a new relay operator (#7893)* first draft of add op* first pass editting doc* make main title visible again* address masa's comments,1
[RPC] microtvm: fix RPC large transfer size issue (#7838)* fix rpc for microtvm* apply feedbacks* bundle deploy fix* fix func registry size* mv constant* fix copyfromremote* address comments and fix error* change rpc default max size* Trigger Build* add checks* Trigger Build* fix ICHECK,0
[Relay][ONNX] 1-D global and adaptive pooling. (#7906)* 1D adaptive pooling added and tested.* Apply formatting.* Add onnx integration and tests.* Busted by lint.,1
[ONNX] Support NMS Center Box (#7900)* [ONNX] Support NMS Center Box* fix silly mistake in contional,0
Update ICHECK error message with link to documentation page. (#7869),0
Bring back GraphRuntimeFactory loader for now. (#7868)* Address issue #7822.,1
[Relay] Shape func fix for all_class_nms and where op (#7910)* fix missing cast to int64 in all_class_nms shape func* fix scalar in where shape func* add add test* update test* minor fix* add where scalar shape func test,0
[ONNX] Support importing Conv with missing attributes (#7899)* [ONNX] Support importing Conv with missing attributes* fix removal of attributes ONLY when they are default and for autopad* move comment to the right place,0
"Remove unnecessary bracelet around make_int (#7907)Turning from `((make_int4)(exp))` to `(make_int4(exp))`. The former case is incompatible with ""macro-defined function"".",4
RelayTextPrinter is now non-recursive. ExpandDataflow refactored (#7817)* RelayTextPrinter is now non-recursive. ExpandDataflow refactoredRelayTextPrinter is now non-recursive to allow printing largergraphs. ExpandDataflow is generalised to have separate node expander.Change-Id: Id5a3a470fbc8b90822502fbc8d24d534df1ea355* requested changesChange-Id: Iac69766428d5b9783279cb02a57064fd82842001* unit test addedChange-Id: Id20ae72f9f5f8dd92d4d182360b28156c035e667,1
"[µTVM] Clone Zephyr 2.5.0 from maintenance branch (#7891)* [µTVM] Clone Zephyr 2.5.0 from maintenance branchClone Zephyr 2.5.0 from maintenance branch 'v2.5-branch' instead of fromrelease tag 'v2.5.0'. The maintenance branch is stable and includes allthe most recent fixes backported to Zephyr 2.5.0.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Retrigger CIRetrigger CI since the error reported for python3:i386 in test./tests/scripts/task_python_integration.sh seems to be a CI glitch, notrelated to the change here proposed, plus it was not possible toreproduce it locally, where task_python_integration.sh passes.",0
[ConvertLayout] Keep span in ConvertLayout (#7895),5
"[Target][Lowering] Update Op Intrinsic Lowering Mechanism And Intrinsic Lowering Pass (#7809)This PR updated the intrinsic lowering pass to support the new op registry and avoid overloading the global tvm registry. Meanwhile, it kept the fallback mechanism to find the most suitable lower intrinsic function, e.g., llvm.FLowerIntrinsic vs. default.FLowerIntrinsic. All previous op registration are ported to new functions, and some missing ops would be added in separate PR.",1
[Frontend][Tensorflow] SelectV2 and BroadcastArgs op support for tf2 models (#7901),5
[TIR][SPIR-V] Fix computing clz on int64 input for vulkan (#7913)* Fix computing clz on int64 input for vulkan* rebase fixCo-authored-by: masa <masa@pop-os.localdomain>,0
[TE] Fix bug if find a loop in compute_at attach path (#7898),0
[BugFix]: Convert tuple to int (#7880)* DEBUG: Convert tuple to int* Add test cases for test_conv2d_hwcn()* CI pass,0
Enable StackVM in AutoTVM (#7897),5
[CodeGenC] Fix bugs when calling extern functions (#7911),0
[FIX] skip_conv_layers will affect quantization of nn.dense (#7795)* [FIX] `skip_conv_layers` will affect quantization of `nn.dense`* [ add ] quantization test case for dense & conv2d* [ fix ] reformat* [ reformat ] test file,0
"Remove pointer arithmetic in StorageObj::AllocNDArray (#7890)The data pointers returned by AllocDataSpace are intended to be opaquehandlers, where previous implementation assumed pointer arithmetic isvalid on them.  Updated to instead use the byte_offset field toindicate the offset in the allocated array.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Tensorize] Fix compute reusing (#7920),0
[TensorIR][PASS] CompactBufferAllocation (#7923)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,4
"[TOPI][RELAY][ONNX] Scatter ND (#7927)* passing topi tests* passing relay tests, needs better shape checking still* support ONNX operator* add shape checking back in* fix lint* update docstring",0
"Use new SBT Debian Repo before bintray is shutdown (#7926)* Use new SBT Debian Repo before bintray is shutdownWe started seeing issues with building the TVM Docker images, and they stemmed from the SBT (Scala Build Tool) installation which was using bintray instead of a later SBT Debian URL. Thankfully bintray were just running some brown outs before they turn the service off on May 1st:https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/So I took the Scala repo URL from here:https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Linux.htmlWhich is the suggested SBT URL as they may change the backend again in future:https://github.com/sbt/sbt/issues/6294* Future-proof sbt repo setup",1
[DOCS] Remove stale Auto TensorCore CodeGen tutorial (#7924),2
[RUNTIME][BUGFIX] Fix DSO module problem when its parent get destructed. (#7918)* [RUNTIME][BUGFIX] Fix DSO module problem when its parent get destructed.Previouslyw we set the context of the DSO module to be the root module.This can cause problem when the root module is not the dso module andget destructued early (but we still need the dso module).This PR makes the following change to fix the problem.- Merge multiple DSO modules to one during export.- Set the context to be the (only one) dso module.- Updated testcase to cover the problem.The enhancement creates some restrictions on the dso import hierachy(all dso modules needs to be merged without a cycle). This is the casefor our current use scenario. The merged logic is also more consistentas the library itself is merged.* Address review comments.,0
[microTVM] Refactor zephyr installation + Update Zephyr RVM doc (#7915)* refactor* script* update* fix* different zephyr branch* trigger build,0
Rev ci-qemu container to v0.04. (#7946),5
init (#7943),5
[TIR][TRANSFORM] Return value support in tir.tvm_call_packed (#7932)This PR fixes the return value support in tir.tvm_call_packed- Clarified the semantics of the intrinsics- Fix a problem when lowering call packed with nested scopes(let bindings)- Added regression tests to cover the changes,0
[Tophub] Race condition fixed in folder creation (#7940)* [Tophub] Race condition fixed in folder creationTophub download routines switched to Pathlib's `Path.mkdir`in order to avoid race conditions in creation of folders,0
adding giuseros to reviewers list (#7950),1
[COMMUNITY] New Reviewer -- Meteorix (#7944),1
[AutoTVM] [TOPI] Support AutoTVM for int4 tensorcore (#7831)* initial* int4 asnumpy* remove* random test* format* random* remove unused import* change dist range* add fuse_pack in* random engine* reformat* remove import* add cuda context* refactor code,1
[Target][Legalization]Add Tir Level Legalization Function Registration And Update Intrinsic Lowering Pass (#7936),1
[OpenCL] Refactor cl_program generation (#7834)* Refactor OpenCL runtime module to build separate cl_programsfor each kernel. This can avoid pathological bugs in thevendor specific OpenCL compiler that may be triggeredwith large programs.* clang-format* Remove check on program size when deconstructing.* Refactor into SplitKernels method.* Limit number of loops for kernel parsing* Add return doc for SplitKernels per CR.,0
[microTVM] Increase host memory size (#7933),5
Correctly build with -runtime=c without -system-lib (#7954),5
"[RPC] Bugfix. Removed server forcing IPv4 protocol (#7953)Removed forcing IPv4 protocol from python RPC server implementationto be in correspondence with the RPC client implementation which isused `platform default`. This had led to situation when ""localhost""was translated as 127.0.0.1 for the server (IPv4 protocol was used),but the client translated it as ""::1"" and was trying to connect toserver using IPv6 protocol and was getting ""ECONNREFUSED 111 Connection refused"".Change-Id: I44802eb1ea78f3b36ac664f0be7237e62084c234",0
[Topi] Fix arm_cpu bitserial schedule with elemwise ops. (#7929),0
"Fix a memory leak in SetParams (#7960)* Fix a memory leak in SetParamsToDLPack creates a DLManagedTensor instance, but nobody delete this i8nproper way. We can use operator-> for getting access to DLTensor.* Remove other usage of ToDLPack()",0
[RELAY] Turn reshape into nop in graph executor backend. (#7945)* [RELAY] Turn reshape into nop in graph executor backend.Previously we are generating the function calls for reshape.This PR updates the optimization to turn reshape into nop:- Tag a fused function as reshape only if it only contains reshape.- Update memory planner to force input output to share the same piece of memory- Update the graph runtime codegen to emit nop when reshape only function is encountered.* Address review comments.* Additional comment and TODOs on the rationale,1
fix Relay build docstring (#7963),0
Replace 0.0.0.0 with 127.0.0.1 for client connections (#7766)* Rename references to 0.0.0.0 to localhost. Also change references to 127.0.0.1 to localhost so that all references are consistent. 0.0.0.0 is not the same as localhost.,4
[Docs] Update links and fix typos in docs and readme (#7965),0
[NVCC] Bugfix nvcc command tool that relies on the compile time env (#7964)* [NVCC] Bugfix nvcc command tool that relies on the compile time env* Update python/tvm/contrib/nvcc.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[ONNX] More Unit Tests! (#7956)* support same lower and maxpool in autopad* fix isinf tests* lower tolerance on roialign test becuase the onnx result is cropped to 4 decimal places* slow support for bottom-k* throw with nullptr in gathernd and scatternd, fix typo* fix lint* fix a copy typo",0
[Codegen] Fix assertion errors in llvm backend when using llvm debug build (#7959),0
Improve dtype detection in loop to fix onnx tests. (#7934),0
"[Graph Executor Debugger] Fix parameter dump (#7903)* remove debug mode* reformat* format* address comments* add single call for all layers* fix test* revert* address comments* address comments* fix rerun node* fix error* format* raise error on array()* fix java* Revert ""fix java""This reverts commit c4cf952dbc5c9c32d65ef0ca05d6ecbb5c06d5aa.* bring back for java api* fix error* cleanup* format* rm redundancy* add last execution track* trigger build* address comments* format* fix name overlap* Trigger Build* trigger build* trigger* trigger",0
[TensorIR][Pass][M1c] FlattenBuffer (#7962)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,4
[DOCS] Update to show github version (#7948)* [DOCS] Update to show github version* Remove dup code,1
"[µTVM]: Zephyr: Add mps2_an521 board to the CI (#7914)Since Arm reference board mps2_an521 is now added as a test platform totest µTVM with Zephyr and that test platform runs by default emulated,plus Zephyr docker images were updated to use Zephyr v2.5-branch, addthe mps2_an521 board as a platform to be automatically used by the CI.That change will allow testing µTVM on top of Zephyr running on aCortex-m33 MCU. Currently only a x86 VM is used for that kind of test.Hence it will help ensure that there is no regression on Arm-based MCUs.That commit also adds explicitly the parameter --microtvm-platforms=hostto the current x86 test for ease of reading on which test platforms aretriggered in the CI ('host' is the default platform when that parameteris ommited, so nothing changes for tests on the x86 VM).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
[RPC] Make tracker jupyter friendly (#7961)This PR uses the PopenWorker to handle the tracker start upand makes the tracker jupyter friendly.,5
[Relay][Pass] Update SimplifyTranspose to correctly simplify rank changing layout transforms (#7807),1
"[BYOC][TensorRT] Fixes for explicit batch mode, Support reduce to scalar, Support split op (#7967)",0
"[Vulkan][Runtime] Added dummy implementations for TVMStreamHandle operations (#7969)rpc_runner_run interacts with stream handlers following PR #7819.Vulkan currently executes adds everything into a single command bufferper CPU thread, so there isn't a corresponding concept of streams.Therefore, added no-op implementations for these DeviceAPI methods.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
"[Vulkan][Runtime] Uniform buffer bugfix, minor cleanup (#7966)* Bugfix, missing decoration on uniform buffer arguments.Caused segfault when running on NVidia GPUs, with models that requireduniform buffer arguments for constants.* Updated test_target_codegen_spirv.py to use @tvm.testing.requires_vulkanPreviously, these tests would show success if USE_VULKAN=OFF.  Now,they correctly show that they are skipped instead.* Minor cleanup on the vulkan runtime.- Explicitly require int64 support at device creation time, since the  TVM-generated shaders require it.- Allocate an appropriate pool size for the buffer inputs, including  both uniform and storage buffers.* [Vulkan][Tests] Merged test_target_codegen_spirv.py into test_target_codegen_vulkan.pyCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[TVMC] A simplified TVMC API for python scripting (Part 1). (#7823)* Introduce new TVMC Python API.* Add simple testing model.* Split result utils into stand-alone file.,1
[Relay][Autoscheduler] Fix autoscheduler matmul without units. (#7957)* Fix autoscheduler matmul without units.* Fix lint.,0
"[ONNX][TOPI][Relay]Support dilations in pooling operators (#7928)* change more pooling operatorsdilations -> dilation to match old field names in convfix python interface into new relay nodesfix order of argumentsupdate type relation for dilationschange topi interface to use dilations* spooky, there are two implementations! Change to 1 topiuse generic poolnd instead of 2d implementation for topiremove old pooling topi* rename pool --> pool2d in topichange pool -> pool2d, make topi tests work nowmake op level 2 pass with interface changesfix dilation being hardcoded to 1proper calculation for avgs among dilationsproper avg pool padding behaviorchange name of pool test to pool2d test* add poolnd baseline implementationmore fixes to edge cases for poolnd, delete old versionsreplace topi tests with new baseline python versionclean up testsmake tests more readable kind ofadd dilation topi tests FINALLYremove see_pool.pyremove dilation from grad* fix subtle implementation detail between topi and baseline python pool op* rewrite tests to be more generic for relay pooling opsadd relay dilation tests, FINALLYadd some comments to testing codelinting and formattingadd ASF headermake 10/10 for black formatting lolmore appeasing the formatting godswowadd parameters to documentationfix test importJostle CIfix more broken unit tests using old version of poolfix wrong var used for bound calcadd dilation to arm testsadd docstring to python make funcs* fix pattern utils out of place args* properly forward more tests to use dilations in poolingformattingmore formattingrelax constraints on test to make it passrelax more constraintsfix some pytorch frontend errorsfix errorbetter test conditionsjostle build* fix padding bug with ceil modejostle buildcleaner pool conditionremove see_pool.py again* add dilations field to onnx importerblacking filesblack file* address matthew's comments",0
[COMMUNITY] New committer -- sslyu (#7968),1
"[SPARSE] Improve sparse performance on ROCM (#7935)* [SPARSE] Improve sparse performance on ROCMThe current sparse dense gpu kernel uses warp level storage to handlingcaching of data. Warp level storage uses shuffle intrinsics, which areslow on rocm (because they actually read and write to shared memory).Rocm does provide intrinsics to do the correct memory management, butthey are not available through tvm. Instead this PR switches to usingshared memory on rocm devices. Performance is about 2x faster.* default to shared mem* formatting* formatting",2
[FIX] Fix deploy_sparse tutorial (#7939),0
[Frontend][Keras] Fix Dense with 3d inputs (#7753)* Fix keras rnn dense* Fix unit test* Fix unit test,0
[DOC] Add how to enable IR debug messages. (#7978),0
[Relay][Parser][Bugfix] Fix parsing hierarchical attibute names (#7976),0
[Frontend][Tensorflow]add batch_dim support for gatherV2 (#7951)* add batch_dim support* fix lint* add check for num of arguments for topi.take* fix gpu test cases* add check for batch_dims in take_grad,0
"[FIX,VM] Fix get_outputs on the vm with a single output (#7902)* [FIX,VM] Fix get_outputs on the vm with a single outputThe VM uses an ADT for multiple outputs and an NDArray for a singleoutput. The single output case was not being handled.* check if the user specified the correct index",0
"[PROFILER] Add shape, structural hash, and layout information to profiling (#7894)* [PROFILER] Add shape, structural hash, and layout information to profilingAdd a new pass that which inserts the layout and structual hash of the op into theattrs of Functions.* includes* fix gcc5 issue* old gcc fixes",0
"[AOT] Introducing AOT in TVM (#7785)* [AOT] Introducing AOT in TVMThis change adds the code generation and minimal runtime API to use theAhead Of Time (AOT) compilation flow. The main logic is contained in:- src/relay/backend/aot_codegen.ccWhich produces a TIR PrimFunc traversing the Relay graphThe runtime interface (authored by @mousius) leaves a gap for futureiterations using platform-specific features from RTOS.Currently AOT runs successfully on x86 in a host OS, running thesetests on micro is coming soon.This PR is based on the RFC described here: https://discuss.tvm.apache.org/t/implementing-aot-in-tvm/9206Co-authored-by: Christopher Sidebottom <Christopher.Sidebottom@arm.com>Change-Id: I9f731c953231f129e1472298915dddc01788efd7* Rebasing 2Change-Id: Ia0a533a49960f1cb4bf3c3833511e539cf7c459f* Applying comments/refactoringChange-Id: Iea1832355f8b1d4c921d02c6b4ceec7db3a681c1* Fixing comments + refactoring - 2Change-Id: I7200cc17b297e42bf67dcdef6f643e86991ca0a8* fix lintingChange-Id: Iba6544ac7101595696b352b8702345cf916625f6* fix linting - 2Change-Id: I7f80d16005f2c621d37a9aae2cbbd61df0277cbe* fix linting - 3Change-Id: I7a1ba40afeea46d5f122563a20cd4b2f08751a1e* fix testsChange-Id: I1297ccc54dd6d93647f421e0beb226f410bf73f5* Addressing comments - 3Change-Id: Id25d1382c30d6d0a0013b5e8986fb8cd886666dc* Addressing comments - 4Change-Id: Ibe29676abe3b75161b5a0903e007118a8318d862* fix tests - 2Change-Id: I2117f9d4392bfd87102ecbef0993c8b320f479a0* fix tests - 3Change-Id: Ic0373543b0f9a54dbd4dc32d428272f7293200ba* fix tests - 4Change-Id: I8a6f229c9a3a9e169779c8d49cbfa3f473348b1f* Addressing comments - 5Change-Id: Ib9ccd07c87392034a21b2eb70955d0b091b780f1* fix tests - 5Change-Id: I4b13c3b548ced414991e83072e9e6fc99b64f939* fix tests - 6Change-Id: Id5af1f778ae25bc60849cc054a605181c1b7a765* addressing comments - 6Change-Id: Id94a2bbcaae891f9498d41be538f13a952f55b81* fix linting - 4Change-Id: I371a0aa5b81824b5a3a1278fac22ace57832027a* add missing fileChange-Id: If359bef96dd0773ead4f75f0d9f5234276347e2d* fix buildChange-Id: I73fc1feb6f7b5d454a528e3289228484dc2b07d5* addressing comments - 7Change-Id: I7f908f3908ffc77e408391f62edcc06f2600c6c2* addressing comments - 8Change-Id: I90bced4e18259a6d42e6a406d93958e204f3859e* rebasingChange-Id: Id28751b069bd046f00faee301b2b446b2ea4fab8* Addressing comments - 9Change-Id: I06c9f280de0a9bf0ca5545bbbbfcc70cb66831b3* fix tests - 7Change-Id: I739f29779862f05def36e5f3e0722019596d17f8* Addressing comments - 9Change-Id: Ie736f40a5225f4e56e79006753d7732127da5408* Applying comments + fixing testsChange-Id: I83e16068b93aaccc7a86b79d42f13328bc76b53d* Applying comments - 10Change-Id: I443d72f53913849f3c28fd6e416162d1ca99e647* Addressing comments - 11Change-Id: I7fefbd0076949b9c38d0abbf2759ebf1502de330* Addressing comments - 11Change-Id: Iad028144d7b394b2dd2fce41a35ca689d1680200* fix tests - 7Change-Id: I14286e665dcdba1e9bc10bb5a27dd6ced50372b0* fixing tests -8Change-Id: I7b4c966da9680870ceda1704c749ee3bdc751926* fixing tests - 9Change-Id: Icf62128a604998ed1b7d5af4cbeadf7d39196d0bCo-authored-by: Christopher Sidebottom <Christopher.Sidebottom@arm.com>",0
Fix post-merge conflict between #7785 and #7945. (#7982),0
[FIX] Fix autoscheduler tuning on sparse matrices where there are multiple with the same shape (#7974)* [FIX] Fix autoscheduler tuning on sparse matrices where there are multiple with the same shape* formatting* remove unreachable code,0
"[VTA] Infinite recursive device_api.ext_dev call fix (#7985)#3843 fixed the infinite recursive call for the Xilinx boards, but didn't fix it for the intel boards. This fixes it for the DE10 (same missing symbol problem with same fix).",0
[Frontend][Keras] Support nested layers recursively in keras frontend (#7949)* Support nested layers recursively in keras frontend* Fix lint* Fix issue* Fix formatting* Fix unit test,0
[BYOC] Remove ext params stored in metadata from params to avoid duplication (#7977)* Remove ext params stored in metadata from params to avoid duplication* Add test for duplicate params,1
adding Leandro to committers (#7999),1
[DLPACK] Support the new python array api with DLPack (#7993)* [DLPACK] Support the new python array api with dlpack* Fix lint,0
[RELAY] Enable registering op with python (#8002)Add a new API register_opNote: Implementing a op by pure python is still limited:  1. Custom type relation (add_type_rel()) is still not     available in python.  2. Setting number inputs (set_num_inputs()) needs     plevel > 128 in python.     (see tests/python/relay/test_ir_op.py),1
[TensorIR] CreatePrimFunc from TE (#7987)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,2
"[UnitTests] Removed unnecessary file creation from unit tests. (#7998)Some of the unit tests produced output when run, even for a successfultest.  Edited these tests to either write to a temporary directory, orto suppress the file creation entirely.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
"Improved MLF to contain workspace info (#7938)* Improved MLF to contain workspace infoAdded functionality to calculate workspace, io and constantmemory required by each primfunc and main function. Moreover,the workspace information required by each primfunc and mainis reported in metadata.json in the Model Library Format(MLF).- added functionality to record tir and relay primfuncs- added tests for model_library_format changesChange-Id: Ib4a8b787345aa35f8a1645e8a648fad84de37bce* Improved MLF to contain workspace info* disable AoT for now* addressing commentsChange-Id: I5f041ec461b02dac6ea9c96ea50eb400d55eef53* Improved MLF to contain workspace info* addressed comments* added aot executor supportChange-Id: I9b54a7939d8ccb3c6ce0454f0fe62866ac66eb5c* Improved MLF to contain workspace info* removed redundant utils.pyChange-Id: I256dd88fab31a595bf9509bd1c4ab59b0c145b1e* Improved MLF to contain workspace info* removed redundant ffi apiChange-Id: I9ad6795aa839edfdfd05b902d4531fb0a20e894d",1
allow annotation info for relay var (#8000),5
"[BYOC][TensorRT] Add nn.batch_matmul, nn.layer_norm, erf (#8005)",1
"Fix executor for different compilers (#8006)* Fix executor for different compilersAt the moment compiling this file throws multiple errors with C++ compilers, this change proposes to fix them.1. `tvm_model_t->run_func` of type `TVMBackedPackedFunc` returns an int at the moment which is different from the signature of this function `tvm_runtime_run`, implicit casting is not favorable in many compile chains and throws errors.2. The index of iterators were of type `int` while that of `model->num_input_tensors` and `model->num_output_tensors` were of type `uint32_t`, this type difference again throws errors in many toolchains, and can potentially cause incorrect calculations.3. C Style struct initialization of tensors with `(DLTensor){...}` is not supported in many C++ toolchains and throws “non-trivial designated initializers not supported” error. Explicitly setting values should work in all cases even though it looks a little less nice.* changing type to size_t* fix format for clang",0
add onnx reverse sequence op (#7771)Co-authored-by: xp224797 <xp224797@alibaba-inc.com>,1
"Ignore invalid git tags when running ""git describe"" in version.py. (#8009)* When using version.py, the presence of tags not conforming   with vMAJOR.MINOR.REV can potentally cause version.py to   fallback to the default release tag (currently ""0.8.dev0"") * This change makes version.py ignore tags that do not conform   with vMAJOR.MINOR.REV by using ""git describe --match ..."".",4
"[Vulkan][Codegen] Spir-V codegen, correct labels/blocks in WhileNode. (#8013)Previously, the WhileNode assumes that evaluating the loop conditionwill not introduce any additional labels.  If this assumption isviolated, such as for a WhileNode whose condition is an if/elsestatement, then the OpLoopMerge instruction appears in the wrongblock.The unittest added exercises this code path, but doesn't yet trigger afailure.  Once spvValidate is enabled for all vulkan codegen, thenthis unit test will catch the failure mode.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
"[Bugfix][Vulkan] Call VulkanDeviceAPI destructor on program exit (#7997)Most of the TVM Global() functions allocate with ""new"" and donot deallocate, as the OS can clean up any leftover buffers atthe end.  In this case, we need the VulkanDeviceAPI destructorto call vkDestroyInstance, to prevent a segfault on exit whenusing some nvidia drivers.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
adding Manupa to reviewers (#8012),1
Fix bug with non-fp32 gemm in onnx frontend. (#8011),0
"Remove minimum seed constraint on XGB Tuner (#7992)* remove minimum seed* reset 3rdparty dep* add items to 'visited', parametrize min seed records* add comment* fix lint* add tests",0
"Explicitly set HardwareParams in test_auto_scheduler_sketch_generation. (#8018)* This test depended on the number of CPU cores available, and failed   when cores < 4.",3
[Fix] CI QEMU Install libpython3.8 (#8020)* add python lib* fix,0
[CI] Added llvm-12 to ubuntu1804_install_llvm.sh (#8008)Change-Id: If614604ae6f5dd8d29ee3acf1635c38a30bd85ff,1
Bumped Ubuntu version to 18.04 for ci_gpu (#7970)Change-Id: I8b13fda08ab002c16a082baaaedd973a063fab99,4
"[DOCKER,CI] Add PAPI to docker images (#8016)",1
"change a, n, l to A, N, L (#8027)",4
support concat in recast (#8028),5
"Rename gpu to cuda, and bump dlpack to v0.5 (#8032)",5
fix docs of threefry_split and threefry_generate (#8035),0
Remove warning which is adding too much noise (#7975),1
[ONNX] QLinearConv Support (#8007)* Add QLinearConv for onnx frontend* Reformat* Squeeze 1D tensor for weight_scale & weight_zero_point* Doing dequatize -> quantize if y_scale is not constant,1
[Relay][AlterOpLayout] Fix strided slice type change. (#8022)* Fixed strided_slice alteroplayout bug.* add test for non standard int8 conv2d padding.* Add test for large index slices.* Us same dtype as input in strided slice.,0
Rename gpu to cuda in java/rust/typescript (#8036)* rename gpu to cuda in java/rust/typescript* fix rpc test to call cuda,0
Update CI images (#8031)* [CI] Updated docker images* fixes to build images* fix arm image version* update qemu* reset jenkinsfile,0
Mark zephyr install world-writable in docker image to unblock #7995. (#8037),2
Fix minor issues in the tvmc tune CLI (#8039)* [TVMC] convert timeout flag to intfixes Check failed: type_code_ == kDLInt (11 vs. 0) : expected int but got strwhen setting the timeout option using the cli flag.* [TVMC] fix typo in tvmc tune help,0
[uTVM][AOT] Adding workspace byte alignment (#8019)* Adding workspace byte alignment* This commit adds byte alignment support for workspaces* Updating AoT tests to use calculate workspacesChange-Id: I88380d875269e1ffa4a51a9cceefd51b3042f1a7* Adding workspace byte alignment* fixed aot_memory cpp tests* add new error type for stack allocator bad freesChange-Id: Iadb4770ac761ef5edb80308e18120443d269c83d* Adding workspace byte alignment* addressing comments + LIFO changeChange-Id: I1e8ad47e11e220f879bf936da2abb3d111db89f0* Adding workspace byte alignment* addressing comments furtherChange-Id: Idb07d28b55520d8897d7dbcb9ef4aad5e3e7b35c* Adding workspace byte alignment* addressing comments - add a default constant to alignmentChange-Id: Id3f486bfdc0bd57d54b3c4097885cb54675196ca,0
sort.cc added to runtime for nms compatability (#7942)* sort.cc added to runtime for nms compatability* remove include* fix clang lint* sort includes in alphabet orderCo-authored-by: Alexander Serov <alexander@tech5.com>,0
[Fix][Runtime] Use flatBuffersBuffer_ in EdgeTPURuntime::Init() (#8034)* Use flatBuffersBuffer_ in EdgeTPURuntime::Init()* Specify target_runtime for tflite_runtime.create()* Add a comment for describing the dependent TF version,0
Pytorch Conv Transpose Padding Fix (#7958)* fix conv transpose import from TF* fix String::fromwe() to String::from()* * fixing pytorch converter to take into account the output_padding parameter for conv transpose operations* updating pytorch converter to correctly convert conv1d to conv1d in tvm inestead of a flattened conv2d unless under circumstances of grouped convolution* updating pytorch converter to correctly convert conv1d transpose to conv1d transpose in tvm instead of a flattened conv2d transpose* added tests to cover these latest additions* * removing print statements used for debugging* * fixing typos and formatting* * fixing formatting* * fixing grammar* * formatting fixes* * updated formatting after running pylint and python_format checksCo-authored-by: Mikael Sevenier <mikael.sevenier@sima.ai>,0
Fix AttributeError when TEST_DATA_ROOT_PATH is set (#8047)Initiate a Path object from TEST_DATA_ROOT_PATH to fix the error:AttributeError: 'str' object has no attribute 'mkdir',0
"[RUNTIME] Improve signal handling in python env. (#7919)* [RUNTIME] Improve signal handling in python env.Python execution environment handles the signal by cachingthe signal a state and invokes the handler when executiongoes into the python interpreter.This model can cause problem when runnning a long runningc++ function. As keyboard interrupt can only be caught in the end.Additionally, because python registered special signal handlers.Socket operations can return EINTR that needs to be explicitlyretried when the interrupt is not a KeyboardInterrupt.This PR adds the following changes to resolve these problems.- Allow execution env(python) to register CheckSignals function  to the TVM runtime.- Add runtime::EnvCheckSignals to check the signal error.- Add retry when EINTR is encountered in socket.- Register the python C API functions in cython mode.To testout the EnvCheckSignals, run the following code```pythonimport tvm.testingtvm.testing.run_check_signal(10)```Note that the C API functions are only registered in cython FFI modebecause ctypes have problems invoking these functions. This howeverwon't affect the correctness, but will defer the interrupt handlingto function return sites.Co-authored-by: Andrew Reusch <areusch@octoml.ai>Co-authored-by: Robert Kimball <bobkimball@gmail.com>* Address comments* Alternative implementation that preserves python exception.* Address comments* Update check signalsCo-authored-by: Andrew Reusch <areusch@octoml.ai>Co-authored-by: Robert Kimball <bobkimball@gmail.com>",0
[Tests] Fix requires_gpu (#8050),0
[TensorIR][M1c] Lower and build TensorIR (#8044),5
Fix recast of relay ops without attributes (#8043)* Fix recast of ops without attributes* fix test for pylint pass,0
"[METAL] Fix codegen for inf and erf (#8054)* [METAL] Fix codegen for inf and erfFixed Metal codegen with using `inf` constant. Constant `INFINITY` isused now instead of `inf`.Also, Metal doesn't have `erf` built-in function. So, we are using`fast_erf` from tir. User will see warning message when we willgenerate `fast_erf` instead of `erf`.* Apply comments* Fix clang-format* Fix lint",0
"Always docker/build.sh with --no-cache. (#8038)* Almost every docker container starts with apt-get install foo bar * This is inherently not cacheable, meanwhile the lack of --no-cache   has bitten nearly everyone I've talked to who's tried to rebuild. * Adding now to address this repeated problem.",1
Fixes a link in doc. (#8064)Signed-off-by: Tao He <sighingnow@gmail.com>,0
allow module exits without del (#8063),5
[IR] Add storage scope to PointerType (#8017)* Add storage scope to PointerType.* Apply suggestions from code reviewCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,1
[BYORTL][Verilator] update ops and add MobileNet (#7972)* update* update vta submodule* cpp fmt* python fmt* skip if tflite is not available* fmt* change assertion* update comment,1
Move infer_value to _get_list_param (#8051),4
"[Vulkan] Broke out implicit device requirements into SPIRVSupport (#8048)Codifies the current requirements that are implicit in the shadersbuilt by CodeGenSPIRV (e.g. can read from 8-bit buffers).  The nextsteps for this development are (1) to query driver/device supportinformation from the device, (2) to pass these query parametersthrough the Target, and (3) to ensure correct shader generation evenwhen features are not supported.Step (3) will require exposing the target properties to relayoptimization passes.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",4
[Frontend][TFLite] Use axis.size instead of len(axis) (#8060)The variable axis is an ndarray.,2
"[CONTAINER] Add default python iterator for Map. (#8061)* [CONTAINER] Add default python iterator for Map.* formatting* add keys(), values()",1
Only allow 4d or 5d inputs to TRT nn.pad (#8073),5
[AutoScheduler] Make RecordReader error-free (#8066)* fix bugs in the auto scheduler record:* reformat the code* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code,0
[Autoscheduler] Add sparse conv2d(1*1) support for auto_scheduler (#8065)* add sparse conv2d support for auto_scheduler* add description* fix bug* fix annotation* Lint fixCo-authored-by: laiyin.lyc <laiyin.lyc@alibaba-inc.com>,0
"Add flag to build static version of TVM runtime (#8059)* Add flag to build static version of TVM runtimeSetting BUILD_STATIC_RUNTIME to ON (default: OFF) will cause astatic libtvm_runtime.a to be built.This library will then need to be linked into other projects with--whole-archive linker option, or otherwise the linker may removefunctions that are not used at the time of linking, such as functionsregistered (in the TVM registry) via global constructors.* Add BUILD_STATIC_RUNTIME with a description to cmake/config.cmake* Explain the issues with posix_memalign on Hexagon* Empty commit to restart build",1
Custom dyld linker for iOS mach-o executable files (#7875)* [IOS-RPC] Fix compilation iOS_PRC appSigned-off-by: Alexander Peskov <peskovnn@gmail.com>,0
"[Relay, TOPI] Support dynamic slicing on first few axes, keeping the rest static (#8068)* Supporting dynamic slice on first few axes* fix index normalization* update dynamic slice tests* pylint fix* fix loop index dtype* fix more dtype issue",0
[VM] add removeUnusedFunctions pass in vm memoryopt (#8040)* add removeUnusedFunctions pass in vm memoryopt* fix lint,0
[TensorIR] change IntRV to ExprRV (#8077),4
Add support for the quantized TANH operator to relay TFLite frontend (#8024)Change-Id: I70df765e1562fa586ed0ffd0e07b8858f7fbb831,1
"[AOT] Remove lookup parameter function in AOT (#7988)* AOT] Remove lookup parameter function in AOTThis PR aims at removing the function call to extract the parameterswithin the AOT main function by introducing a tir::lookup_param builtin.This has different benefits:- In AOT we now only use the v_handle field- We save cycles by not calling an intermediate function to extractlocal parameters- We reduce code size, since we don't need to pack a call to extractparameters and we don't need to produce the lookup_param functionanymore within the compilation unitChange-Id: I36c2f0724a79606424a4374f4f5cd669bb2a8a55* addressing commentsChange-Id: I83ba0189f559d310b5a80fe0bcc4d601b490d21a* retrigger CIChange-Id: I84ab4a526d1284ded41fe95636e94c15412f6b28",1
"[TOPI] Custom schedule for standalone transpose in cuda (#8030)* [TOPI] Custom schedule for standalone transpose in cuda* check if input is not Any* fix vta test* check input shape* fix injective* move transpose out of sparse.py* update comments, use warp size* missspelled transform* formatting* rename test* comment* fix tests",0
"[CI][Docker] set environment variables for UTF-8, to prevent errors when running `black` (#8089)* Sets environment shell encoding to UTF-8 * This prevents the black formatting tool to exit with the following error:   ""RuntimeError: Click will abort further execution because Python was    configured to use ASCII as encoding for the environment""",0
[Relay][PRNG] Add uniform distribution generator wrt threefry PRNG (#8041)* Add uniform distribution generator wrt threefry PRNG* fix lint* remove the redundant print* modifications based on review* update docs* update uniform algorithm to use bit operations only* add type restrictions* minor fix upon review* update test and error information,0
"[Vulkan][Codegen] Added spvValidate check after vulkan shader generation (#8098)spvValidate found the bug that was fixed in #7966, along with a fewother issues on missing capability/extension declarations.  Now thatall unit tests pass with it enabled, would like to enable by default.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Relay, ONNX] Support gather_nd batch_dims attribute for TF/ONNX (#8084)* Add GatherND batch_dim support* adding tests* test working* improved reference code* refactor ref func* batch dim 2 tests from tf all passed* batch_dim -> batch_dims* add example* minor change* add onnx test* fix onnx version* fix lint* remove move on batch_dims* fix pylint* fix compiler warning* add shape constraint for batch_dim and update doc* make the output shape doc clearer",0
[BYOC][Verilator] Skip mobilenet test if Verilator is not available (#8094)* skip mobilenet test when verilator is not available* add skipped to pytest* add pytest,1
"[TVMC] add the support of the cross compiler options (#7922)Add the possibility to provide the cross compiler options when using thetvmc compile functionality.With some cross compiler, toolchains --sysroot option (at least) need to bedefined.tvmc/test_compile.py as been updated to introduce simple tests to validatethe cross options functionnality.Signed-off-by: Vincent ABRIOU <vincent.abriou@st.com>",1
[Refactor] Rename asnumpy -> numpy in NDArray (#8083),5
doc: fix description of stop_fusion annotation (#8095),0
Fix some typos (#8101)* fix bugs in the auto scheduler record:* reformat the code* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code* fix some typos* fix some typos* fix some typos* fix the port number typo,0
[cpplint] Fix C-style cast linting errors from cpplint 1.5.5 (#8106),0
Remove clang-7 requirement for vulkan. (#8107)* Breaks build with new 18.04 ci-gpu docker container.,1
[Vulkan] Remove some interface block decoration (#8102)* Remove block decorator for shared/local variables* Fix lint,0
Add a default warp size 1 for vulkan and opencl (#8109),1
[Docs] Update stale links (#8111),1
[APPS] Add logging to the bundle. (#8115),1
[Hybrid Script]Fix some syntax errors (#8116)* [RUNTIME] Add clear() function in tvm::Map class* [Hybrid Script]Fix some syntactic errorsCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,0
remove self-include in runtime/container.h (#8117),4
[TensorIR][M2a] Verification of cached flags (#8114)* [TensorIR][M2a] Verification of cached flagsCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Address comments* Update src/tir/schedule/analysis/verify.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,1
"[CI,DOCKER] Bump gpu image to cuda 11.0.3 (#8119)",5
[Frontend] [Tensorflow2] Added test infrastructure for TF2 frozen models (#8074)* added test infrastructure for frozen TF2 models* linting with black* removing some comments* change in comment in sequential test* addressed the comments* refactored to place vmobj_to_list in a common file* Added helper function in python/tvm/relay/testing/tf.pyCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Refactor tf according to CI errorCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Added docstringCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* removing printCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xiao <weix@amazon.com>,0
"[Relay][TOPI] Fix compute and schedule bugs for conv2d_winograd_nhwc on mali device. (#8091)1. add argument `auto_scheduler_rewritten_layout=""""` in conv2d_winograd_nhwc_mali;2. add `need_auto_scheduler_layout=True` for conv2d_strategy_mali andconv2d_winograd_without_weight_transfrom_strategy_mali.Signed-off-by: haizhu.shao <haizhu.shao@gmail.com>",0
"[TVMC] Add support for the MLF to 'compile' command (#8086)* [TVMC] Add support for the MLF to 'compile' commandAdd support for the Model Library Format (MLF) to 'tvmc' so users canoutput compilation artifacts to a MLF archive passing the new flag'--output-format mlf'. For instance:$ python3 -m tvm.driver.tvmc compile ./sine_model.tflite --target=""c"" --output sine.tar --output-format mlfwill generate a sine.tar archive that is serialized accordingly to theMLF.Since the MLF is currently meant to be used only on micro targets, anerror is generated if one tries to run a MLF outside a micro context.The micro context does not exist yet but will be later introduced aspart of the [RFC] ""TVMC: Add support for µTVM"".That commit also adds 3 pytest tests to test tvmc + MLF.Finally, it also fixes some missing periods in the 'compile' commandhelp sections and renames export_format to output_format so there isno confusion with flag '--dump-code', which contains ""formats to export""in its help section.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix missing importorskip in the import_package testFix missing importorskip() in the import_package test allowing thetest in question to be skipped when 'tflite' is not installed in thetest environment, otherwise the test will fail with:[...]>       archive_path = exported_tvmc_package.package_pathE       AttributeError: 'str' object has no attribute 'package_path'",0
[Relay][PRNG] Support generating data of any shape in threefry_generate (#8085),5
[Relay][dismantler] Added handling of packed func (#8004)Added handling of CallNode objects created via packedfunctions invocation + test cases.Change-Id: I5374abc59a3b0f79f27364c45f1a5789536df940,1
[METAL] Split kernels and compile them separately (#7980),5
"[TensorIR][M2a] Structural Error Reporting (#8121)This PR is part of the TensorIR upstreaming effort (#7527), stage M2a.In this PR, we implemented ScheduleError, an error reporting mechanism for schedule primitives to report user-face error messages, with the functionality of rendering the TIR out in the TVM script syntax.This set of APIs allows future improvement of error location rendering, e.g. more colorful rendering mechanisms like synr does.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>",0
Fix typos and format in comments (#8132)* Fix typos and format in commentsFix typos and format in comments about the registry manager ofpacked functions.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Fix lintNo more than 100 characters per line is allowed.,0
Fix typo in a comment (#8129)Fix typo in a comment about AOT executor.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
"[Vulkan] Add device capabilities to Target, use in codegen (#8127)* [Vulkan] Enable instance/device extensions- Vulkan requires that extensions be explicitly enabled if used.  Explicitly list out which extensions are required (currently none)  and which are optional.* [Vulkan] Extract device information from vulkan API.- Based on vkGetPhysicalDeviceProperties and  vkGetPhysicalDeviceFeatures, determine which Vulkan capabilities are  supported, pack into a Target.* [Vulkan] Query instance-supported apiVersion before creating instance- Previously, vkCreateInstance was called to initialize Vulkan 1.0.* [Vulkan] Moved options for dedicated allocation and push descriptors to environment variables- Query support for dedicated allocation and push descriptors along  with the rest of the device support.  Move the options to disable  their use from compile-time variables to environment variables  `TVM_VULKAN_DISABLE_PUSH_DESCRIPTOR` and  `TVM_VULKAN_DISABLE_DEDICATED_ALLOCATION`.* [Vulkan] Move option for vulkan validation layers to environment variable- Moved to enable faster use as a debug tool.  If  `TVM_VULKAN_ENABLE_VALIDATION_LAYERS` is a non-empty string,  validation layers will be enabled.* [Vulkan] Explicitly enable vulkan features in device creation- Vulkan requires that features be explicitly enabled before use.  For  each feature that the device supports and a shader might use,  declare it in the call to `vkCreateDevice`.* [Vulkan] Avoid repeated queries for device attributes.- Implement `VulkanDeviceAPI::GetAttr` based on the per-device values  stored in the Target.  This pulls all logic for querying device  parameters is in a single location.* [Vulkan] Implement ""from_device"" flag for the vulkan target.- With the number of device capabilities that may or may not be  supported by a vulkan driver, it can be tedious to input them.  Specifying ""-from_device=0"" now indicate that any unspecified values  should be read from the device.* [Vulkan][Codegen] Read vulkan device capabilities/limits from Target- Previously, the codegen assumed that all device features were  present.  Now, the codegen reads device capabilities from the  Target, and throws an error if codegen would require use of an  unsupported feature.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[CUBLAS] Remove deprecated CUBLAS_TENSOR_OP_MATH flag (#8130)This flag is causes CUBLAS to use tensore cores on all operations. Withf32 or f64 operations, this leads to loss of accuracy.",4
[FastMath] Add fast_softmax support in fast_math pass (#8138)* Add fast_softmax support in fast_math pass* Lintfix* Update,0
[Codegen][CUDA] Fix make_int4x cuda codegen vectorize (#8137)Co-authored-by: wangyucheng <wangyucheng@sensetime.com>,0
[lint] Fix black whitespace errors (#8124)Change-Id: I927b43df95a8db8b042bc3cf2a1f23739d102b9d,0
"[Cuda][Codegen] Check for cuda include dir in /usr/include. (#8135)Currently, on linux platforms, only checks for cuda install directoryin /usr/local/cuda/include.  The `nvidia-cuda-dev` package of Ubuntu20.04 installs at /usr/include, so it would be good to check thatlocation as well.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
[COMMUNITY] New committer -- trevor-m (#8141),1
[microTVM] AOT Demo (#8075)* initial* remove compare* temp fix* debugging* hack* hack for testing* both test pass* cleanup* fix tests and tutorials* restructure* cleanup* cleanup* fix check files* fixed for physical devices* address comments* reduce nrf stack size* update sample url* format,0
Pin black version (#8139)This commit pins the black version to provide stability.It is expected that the pinned version will be moved forward periodically.Change-Id: Ied866bff85a1a832959bc1d4673a7fdec68128a7,2
"[IR][Pass][Instrument] Pass instrument framework (#7952)* [IR][Pass][Instrument] Pass instrument frameworkThis commit provides utilies to instrument passes:  1. Add a new namespace tvm.instrument  2. Introduce PassInstrument and PassInstrumentor to PassContext     Example     -------    passes_mem = #... Impl of memory instrument    passes_time = tvm.instrument.PassesTimeInstrument()    with tvm.transform.PassContext(        pass_instrumentor=PassInstrumentor([passes_mem, passes_time])):        tvm.relay.build(mod, 'llvm')        passes_mem.rendor()        passes_time.rendor()  3. Integrate existing PassContext::Trace() and timing profile* [IR][Pass][Instrument] Fix python test_pass_manager.py* Fix comment* Fix lint* Fix test_pass_annotation* Fix test_pass_annotation.py* Fix lint* Fix test_pass_annotation.py* Fix test_pass_annotation.py* Fix review comments* Fix tutorial use_pass_infra.py* Fix review comments* Fix review comments* Fix typo* Fix review comments* Fix review comments* Fix unittest error: test_cow_pass* Fix unittest error* Add more test cases for exceptions* Fix nit* Doc override_instruments()* Fix review comments* Fix lint* Fix EnterContext exception behavior",0
"[Vulkan][Refactor] Split out vulkan.cc into separate distinct functionality. (#8157)This is in preparation for additional refactoring.  Functions areorganized according to group similar functionality together, tominimize the amount of file-to-file transfers needed later.  The maindivisions are between VulkanDeviceAPI,VulkanModuleNode/VulkanWrappedFunc, VulkanThreadEntry, andVulkanContext.Other than minimal renaming of private functions and addition of somecomments, this commit should have zero changes to the functionsdefinitions themselves, only to their arrangement within thesrc/runtime/vulkan directory.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[CI] Cleanup stale logs for auto-tuning (#8160),2
"[Docs] Added developer documentation for DeviceAPI and Target. (#8082)* [Docs] Added developer documentation for DeviceAPI and Target.* [Docs] Update on the DeviceAPI/Target documentation.- Clarified wording based on suggestions from @csullivan- Fixed incorrect links to `c_runtime_api.h`* [Docs] Update on the DeviceAPI/Target documentation.- Switched from argument style example of `tvm.target.Target` to a  JSON-formatted string, based on @zxybazh's suggestion.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
rev jenkins containers for #7995 (#8155),2
"[Relay] Support dynamic indices size in gather_nd and scatter_nd (#8105)* add gather_nd shape func* refactor gather_nd ref funcs* add dynamic gather_nd test* gather_dim -> num_indices_per_tuple* support dynamic scatter nd* minor fix* fix pylint* rename to index_rank and make it Optional* pylint, do not use -1 for default value",0
[AutoTVM][AutoScheduler] Add workaround to alter op layout bug in task extraction. (#8143)* Add workaround to alter op layout bug in task extraction.* Only copy mod.,0
Fix tvmc tuner for cases when uTVM is not enabled (#8153),0
"[VM] Avoid round-trip Target->str->Target conversions (#8161)Currently, in some cases this round-trip cannot be completed.  Forexample, if an Integer value has a value outside a 32-bit signedinteger range, or if a String value contains spaces.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
[CMake][Minor] Update CMake warning flags (#8152),1
[Fix] Fix conv2d HWNC type strategy (#8147)* fix conv2d strategy* fix style* fix styleCo-authored-by: wangyucheng <wangyucheng@sensetime.com>,0
[CI] Fix the CI after image update. (#8164)- ci-gpu needs env var update to support all GPUs.- Update git-black to fix the recent encoding error due to latest black dep.,0
"[CI][DOCKER] Fix cuda11 nvidia-docker support for non-Tesla gpus (#8163)Starting cuda11, libcuda can be linked to a version of libcuda in/usr/local/cuda/compact. The particular linked librarydoes not work for non-Tesla GPUs, causing ""no CUDA capable devices found""even though nvidia-smi shows available GPUs.This PR makes makes sure we always prioritize linking/usr/lib/x86_64-linux-gnu/libcuda.so.1so the nvidia docker cuda11 images works for non-Tesla GPU envs.",0
[FastMath] Add cuda & x86 schedules for fast_softmax (#8150)* Add cuda & x86 schedules for fast_softmax* Bug fix* Re-trigger CI,0
Update auto_tuning_with_python.py (#8158),1
allow libbacktrace to be used when cross compiling the runtime (#7917),5
[microTVM] make RVM memory and number of cores variable (#8154)* ram/cpu variable* tvm prefix,0
"[ONNX] [Relay] Update unique operator to match ONNX output (1D only) (#8099)* Fix topi test case and docs (tvm was returning inverse_indices and claiming it was indices)* Passes on CPU, fix unique op test* more changes* mtrying to fix optional outputs in onnx importer* TupleGetItem is being passed a stringgit add python/tvm/relay/frontend/onnx.py debugging print statements* Unique is passing onnx unit tests* fix indices* change comment* fix return of compute unique* black* fix lint* Some stray .asnumpy()s got through my merge, fix)* fix lint* revert changed .numpys* missed a few* fix more .asnumpy* fix black* Fix op level 3 test* remove prints* Fix pytorch and tf importers* black* fix lint* fix indentation* fix topi test",0
Add function attribute for shape func for profiling (#8148),1
"[Vulkan][Docs] Minor updates following Vulkan target query. (#8151)- Better error messages, specifying which capability is needed.- Documentation section outlining the different target capabilities,  which vulkan properties they correspond to, and which spir-v  capabilities they are required by.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Vulkan] Remove dependency on Target from -from_device functionality. (#8171)The `tvm.target.Target(""vulkan -from_device=0"")` functionality wasinitially implemented by generating/returning a Target.  This brokeusage of libtvm_runtime.so, since Target is only defined in libtvm.so.This commit reimplements the functionality without the dependency onTarget, Integer, Bool, or IntImm.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",4
[Strategy] Add group_conv2d_nchw_int8 in cuda strategy (#8167)* add group_nchw_int8.cuda* fix* fix style* fix style* fix style* fix style* fix style* fix styleCo-authored-by: wangyucheng <wangyucheng@sensetime.com>,0
"[Relay, TOPI] Refactor strided_slice and add axes argument (#8165)* Initial importcommit 667011f10320918e4dcd47ac2b57fe49849e5440Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 16:28:57 2021 +0900    Squashed commit of the following:    commit 95242d86ea5de96925430c0a74b6e91e299fb5ab    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu May 27 15:45:19 2021 +0900        Add function attribute for shape func for profilingcommit b8ede24ff987eb152bde7cc15afce004a88aeb5fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 10:21:06 2021 +0900    layout transform support completecommit 5782b7070288eb0de122f5dab91b38c26166a7d7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 27 08:31:11 2021 +0900    support layout transform part1commit e94aa6b2a916607234c89eddcd07afdfa8085786Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 19:47:46 2021 +0900    moved utilities to its own filecommit 8bf88913b9bc02730120a0695138ed3fb8ed49aeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:39:50 2021 +0900    fix formatcommit e89d599d6a10021167feb241483693260aa535f2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:33:02 2021 +0900    ToVec -> ConvertToVeccommit 001982ce1419504f1f0e1d116d57dd34f0180008Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:26:56 2021 +0900    formatcommit fae57f9bd67b29880a7552b5149d03120924cdacAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:24:35 2021 +0900    use Any for relay type rel pathcommit 053eee2e6f58749af0b68cca52fd530afc0f6454Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 17:14:44 2021 +0900    fixcommit fbb099c8e66caf846c773e180c66a2b336bd64a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:39:37 2021 +0900    refactor type relcommit ecfe3cd43e3968375505d5393959ec19da4b5c01Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:23:47 2021 +0900    workingcommit b357c2f8825603d8ba9ee2424a7f572e12c29852Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 16:07:07 2021 +0900    refactoring output shape calccommit f69ef407cf003c7977a4564185949d3f6b5c0219Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 14:23:36 2021 +0900    bug fix end param initcommit a5611c9a1f243f4b9a56539e7e8a15661374920cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:42:31 2021 +0900    fix test shapecommit e79931a264f0d8ed63a333ec4ab10a72cff22a84Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:42:03 2021 +0900    dyn slice tests left as todo now workcommit 7db4cea31378eed85dfae1cb03fb5a97394f7fe3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 13:36:30 2021 +0900    remove dynamic input specific opcommit 510bce6a181604e5eb3f2bd1951ae035a4090700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 12:52:30 2021 +0900    refactoring dynamic slicecommit 1b3969ade9ee98651b8157ecab1c675410a84ee5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 09:06:46 2021 +0900    fix slice axes dispatchcommit 9a795606fb71ec08cefe5bfa904f1ab32c18da4bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 08:32:54 2021 +0900    refactor computecommit 80442f86bbf9f0582823d5903021e3bae61a4662Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 08:11:18 2021 +0900    fixed output shape, refactored version workingcommit d2538ae980a1c731b646467c615d742efeb65e25Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 07:56:05 2021 +0900    restore another slice variantcommit 36aa777eacd8426a850d08b528e9addcd36a4894Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 24 06:41:50 2021 +0900    refactoring slice with axescommit 32698b74df211829777e5493e82bf7425364acb4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 13:11:01 2021 +0900    fix axes null checkcommit 54fb723d23d351551b75d879198aafb1eac2dedeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 12:52:18 2021 +0900    Revert ""[Bugfix][Vulkan] Call VulkanDeviceAPI destructor on program exit (#7997)""    This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.commit 37eaf579d47190bc42ad64f9ac34c93a9dac3ce5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 04:30:37 2021 +0900    remove wip layout transform support for slice with axescommit 9bcb2ada60fadadd1f29a6d09e6b4fc5104efd3fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 18:01:59 2021 +0900    fix pylintcommit 7063a09ef1b98849e98194e8a9e47455cd1b5fa3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:57:03 2021 +0900    minor fixcommit 96c9231b5b2cbf2f36b4096d54f1f5ac4033d361Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:54:16 2021 +0900    support dynamic scatter ndcommit d4a4db8a8b518b1ef9e6abacfa23a9e1b76fd1b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:33:19 2021 +0900    gather_dim -> num_indices_per_tuplecommit a489375f0b31948a13e41f5967960305453c7049Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:23:46 2021 +0900    add dynamic gather_nd testcommit 533854a006c16359842451b8690cb8639b47635dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:18:26 2021 +0900    refactor gather_nd ref funcscommit 36a4501a151070760559f6ce4cfa574202b4d0c8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 14:36:34 2021 +0900    add gather_nd shape funccommit 1853c35d883e501e484d5f74adb3081f916761d5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 22 04:20:39 2021 +0900    add eyelike supportcommit 150e945290cbc595bd370dcae7e96e24597fbf04Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 04:08:37 2021 +0900    migrating inlined topi compute to topi/transform.hcommit 763ac37f725c2cb89a3221621b69da0e6ac39ed8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 03:45:37 2021 +0900    strided slice with axes support* fix bad merge* fix cpplint* fix pylint* more cpplint fix* fix compiler warning* add doc* add tests* typo fixed* support axes argument in topi cpp strided slice* Properly test axes argument in relay tests* fix bad merge (revert vm change)* fix tests",0
"[BYOC][TensorRT] Reuse TRT engines based on max_batch_size for dynamic batching, improve device buffer allocation (#8172)* Reuse TRT engines based on max_batch_size for dynamic batching. Improve how device buffers are allocated* Fix python formatting* Allow user to configure engine building mode using TVM_TENSORRT_MULTI_ENGINE* Update doc* Typo",0
[TVMC] Fix tvmc compile to extract target and target_host from --target (#8176)* [TVMC] Fix tvmc compile to extract target and target_host from --target * Removes validation to accept up to two TVM targets and   set them as target and target_host* Update python/tvm/driver/tvmc/common.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
fix UTF (#8185),0
"[TensorIR][M2a] ComputeInline,ReverseComputeInline (#8170)This PR is part of the TensorIR upstreaming effort (#7527), which adds the first 2 schedule primitives:- compute-Inline- reverse-compute-inlineCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",1
"[Vulkan][UnitTests] Compatibility fix for test_vulkan_unique(). (#8186)relay.unique return values changed in 6baccc13, updating vulkan unittests to match.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Vulkan] Corrected typo in Vulkan capability error messages. (#8187)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>,0
"[Vulkan][Refactor] Pull out vulkan initialization into VulkanInstance and VulkanDevice (#8188)* [Vulkan][Refactor] Broke out VkInstance setup/teardown into managed class.- Previously, the VkInstance was directly owned by the  VulkanDeviceAPI.  Now, VulkanDeviceAPI owns a  tvm::runtime::vulkan::VulkanInstance that does setup/teardown of the  VkInstance.  This way, the teardown is done even if a later  initialization step throws an exception.* [Vulkan] Renamed VulkanContext to VulkanDeviceRenaming to match with the tvm.context to tvm.device rename.* [Vulkan][Refactor] Extracted VulkanDevice initialization into VulkanDevice class* [Vulkan][Refactor] Removed the VkPhysicalDeviceProperties member variable from VulkanDevice- Now that there is a separate VulkanDeviceProperties class, the  redundant VkPhysicalDeviceProperties can be removed.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
"Onnx eyelike (#8191)* add ONNX EyeLike converter* need to implement k* test pass* eyelike tests all pass* Revert ""test pass""This reverts commit 0aa7347aaf27493d53492c9f0be305bf8358760b.* removed comments, black'd, lint* changed == to is in onnx.pyCo-authored-by: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: Jocelyn <jocelyn@pop-os.localdomain>",1
Complete register op from python (#8079)* Complete register op from python* fix lint* fix lint* fix lint* fix comments* fix* fix* fix comments* fix lint* fix lint* add comments* fix build* fix* add exception case* fix* fix comments* fix* fix* fix* fix* fix* fix* fixCo-authored-by: xiaoqiang.dan <xiaoqiang.dan@streamcoputing.com>,0
"[Codegen] Use ""target.build.$TARGET_KIND"" for all codegen functions. (#8071)* [Codegen] Use ""target.build.$TARGET_KIND"" for all codegen functions.- Removed special case for ""micro_dev"" target.  Instead, register  BuildCHost as both ""target.build.c"" and ""target.build.micro_dev"".- Renamed ""target.build.build.aocl_sw_emu"" to  ""target.build.aocl_sw_emu"".  Appears to be a typo introduced in  #841725cc585* [micro_dev] Removed references to non-existent micro_devdevice_api.micro_dev was removed in745e542e4deaf44f3d6e5665299aa85ef8f4a6b9, but several references stillremained.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
"[METAL] Fix the rest memory leaks in Metal runtime (#8175)* [METAL] Fix the rest memory leaks in Metal runtimeWhen we throw exception from autoreleasepool, then the resources won'tbe released in proper way. In the documentation we can see that ""Whenthe block is exited with an exception, the pool is not drained."".Link on the documentation:https://clang.llvm.org/docs/AutomaticReferenceCounting.html#autoreleasepoolImplemented a wrapper which handles all exceptions in autoreleasepoolblock and throw them after this block.* Apply comments* Add documentation comments to wrapper and macro",0
Fix prelu bug in pytorch frontend (#8192)* Fix prelu bug in pytorch frontend* Fix lint error* fix lint error* Fix lint error* Try to fix lint error* Fix lint errorCo-authored-by: huangyuheng <32429436+hyhzxhy@users.noreply.github.com>,0
[TE/TIR] Fix create_prim_func to properly handle rank 0 tensors. (#8128)We handle lowering rank 0 tensors to rank 1 buffers with a singleelement.,0
"[CMake] Add compile-time check that libtvm_runtime.so has no undefined symbols. (#8178)If libtvm_runtime.so erroneously requires definitions that are onlypresent in libtvm.so, the -Wl,--no-undefined flag forces them to becompile-time errors rather than runtime, and would be caught by theCI.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[AOT] Initial implementation of --unpacked-api (#8023)* [AOT] Initial implementation of --no-typed-operatorsBased on the discussions in the AOT embedded improvements RFC, this adds a flag to the target which changes the internal operators to an unpacked API. The unpacked API spreads the input buffers across the operator function, for example:int32_t operator(void* arg0, void* arg1);As opposed to the traditional packed API:int32_t operator(void** args);Uneffected is the entrypoint function, which retains a packed API forcompatibility with other parts of TVM. This is done by changing thepasses taken by none entrypoint (CallingConv::kEntryPoint) functions.* Move entrypoint generation outside of main passesThis removes the logic for deciding the entrypoint from the compilerpasses and instead moves it into the metadata code generation. By movingthe generation, we can generate a variety of entrypoints on top of thecompiler output (such as the micro entrypoint discussed in the RFC).* Use buffers in make_unpacked_api tests* Enable --no-typed-operators for llvm* Change --no-typed-operators to --typed-operators=0 to match other options* Refactor typed-operators lookup into use_typed_operators_(Also contains minor clean up of output variables)* Rename --typed-operators to --unpacked-api(Also moves the entrypoint name to a constant)* Move all properties into init list to avoid double init* Remove AutoTVM breaking default and improve clarity",1
fix py files (#8194),0
Run ONNX Node Tests on available targets (#8189),3
"[Relay, TF] Support converting TF combined_nms using Relay all_class_nms (#8174)* import from branchcommit c86bcf48fa6acd19647a7a096b9e1a5d4e56cc74Merge: 0fa88051b da75b2a52Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 12:13:29 2021 +0900    Merge branch 'tmp' into all_class_nms_tfcommit 0fa88051b3d30337674a07e579b78e8cb254cd66Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 06:24:57 2021 +0900    Revert ""handling case when num detections is smaller than max_total_size""    This reverts commit 61e70b82f338300224b22f4d6bdda349e7aa5aca.commit 67251504c652e36106718617c5ae8b42c61deffcAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:06 2021 +0900    handling case when num detections is smaller than max_total_sizecommit 39549aa25267617671ca2a82ca517442065afe97Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:32:37 2021 +0900    simplify frontendcommit ca9470ba68e68c81902b0a3bad4bf5b5f0aa311eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:25:13 2021 +0900    update op definitioncommit 47bdef9e0fcdbab4671dd46044be5acac24b2f2bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 19:47:04 2021 +0900    remove unnecessary maskcommit 445a7daf1afb794be5f03473c70b172f06556d05Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:54:19 2021 +0900    remove in_buffercommit 71879b115b3bfe8087b73618bdab16fd61fbed86Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:48:22 2021 +0900    minor fixcommit 72e055a721ee7d698e7b3a3f58ab074ab78b57b2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:45:37 2021 +0900    make it more readablecommit a1fe7c46d6bb77da51de24b46bec881ed19d4cb3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:44:14 2021 +0900    clean upcommit 0c659bf27f9b90dd53455abd0d24b42f86e802bbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:33:54 2021 +0900    improve sort on cpucommit 480f6b782427dd46514efbf7028de4fb9f5ff9aaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:29:53 2021 +0900    collect indices and scores in one kernelcommit 2b441c391a25930092b55f12dadc31832400277bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:47:31 2021 +0900    initialization bug fixed in cudacommit d43e801289621e71a79c71308dedeef0969264beAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:23:09 2021 +0900    cpu nms bug fixedcommit 025010e42110388d0de2bc2ffcd76fbe14a188fbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 11:09:47 2021 +0900    add cpu implcommit 787d8399ff160694ecd2c4a9721a5825ca945d81Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:38:20 2021 +0900    refactoringcommit 05404305d1323b475829de10aa68f1f8791686ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:03:51 2021 +0900    initial import    commit 5ff0985625ec75f117af37017ebf4089dafb8a46    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:02:45 2021 +0900        cleanup    commit 199f9b67c2d471a761f743e6ea5fa414c899bd3f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:00:15 2021 +0900        Revert ""add gather_nd shape func""        This reverts commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855.    commit 47a05c4c8f5a56a1685848210229aaa083b92880    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:53:00 2021 +0900        format    commit 9dcd0f02b25d658c94fa23e2cc65a9424ed8a1a5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:48:43 2021 +0900        make it static    commit eb06393939f1b8d8130f3815dc0f66223c9aa4f3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:14:31 2021 +0900        restore old impl and use it for q != 1 case    commit 115a5dfcf9b552fb2682534d82bbf638e661c0aa    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:00:40 2021 +0900        fixed score gathering    commit d2035626a72f8df71c514e3293337f6fda723353    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 08:53:14 2021 +0900        minimum fixed    commit 3fe91e8846b6d2075ae1d9a162c4b70b08cc8024    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 06:59:39 2021 +0900        batch issue fixed    commit 19e3e84690c0289c85001597046969d0c8dc92c2    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:29:15 2021 +0900        zero padding working        This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.    commit ce7848ba7def5a22659b09de039b2df12c0114f9    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:12:47 2021 +0900        pylint, do not use -1 for default value    commit 968f3bd230ed4855b45fd739dfc86edc2335ec80    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:07:31 2021 +0900        rename to index_rank and make it Optional    commit 9e06b8491e0ce1c981a5059f28135319f96978d0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 18:01:59 2021 +0900        fix pylint    commit 81dc6050dcbe59915a8f9b4f78b4aaf9fdba89a6    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:57:03 2021 +0900        minor fix    commit 54297b6128863d07e7dded71cc40077726faf2db    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:54:16 2021 +0900        support dynamic scatter nd    commit e25c225ce747c4e84452e6e7b32eeb0d71b2995d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:33:19 2021 +0900        gather_dim -> num_indices_per_tuple    commit aaa6211e7ef3ce520b8711a78cf7eb2af52e7acc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:23:46 2021 +0900        add dynamic gather_nd test    commit 3a9fe5dfa5faeadbcdb882ff70039ea7bccb61a3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:18:26 2021 +0900        refactor gather_nd ref funcs    commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 14:36:34 2021 +0900        add gather_nd shape func    commit b0200643a184294a2f2b3cce7208c4d257987424    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:01:11 2021 +0900        working on zero padding    commit 456741790dd5e73f3f76ef7a5ede6e1014de8b2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 03:21:52 2021 +0900        working    commit 7f5c76d0090950985888781f071ca341e2fa5695    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 02:37:50 2021 +0900        relay type inference works, debugging topi    commit 4a4b8dfbfdc65d7a6e77ed0a6e8b09af162b77ad    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 15:08:16 2021 +0900        add max_total_size to attributes    commit 7218b2f7b4de0c796d69f23084cd688e28f7b461    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:50:58 2021 +0900        tf frontend update    commit cde4a1fdd15ed898b1f7299e99377cceaaee2732    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:17:14 2021 +0900        all class nms tf mode first cut    commit 5f349f77c9c230ee636aceb52547502319c8ad77    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 06:54:34 2021 +0900        begin supporting per batch output    commit 0044365affac6667a02d15791a59040702f8990b    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Mon May 3 19:46:28 2021 +0000        initial    commit 168a617e48b062417b766d6400b0c6b856084cfa    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Fri Apr 16 20:31:32 2021 +0000        initia;        lcommit da75b2a52e9a8daa322168d1b6026e144d42d5bbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:58:19 2021 +0900    do minimum in topicommit 52c5e8a5bca56f93778990d4faa87c7e7b342ba7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:54:49 2021 +0900    more simplifycommit 44d88cdecd87468b630fd16a7d1e1214e86eabfaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:51:39 2021 +0900    simplifycommit 74e19174f1b2d40ee8f4d08c7a61667bc3dd69b5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:39:37 2021 +0900    blackcommit fc3a38e1cb699b66340c7742cb74188fdbe92bf5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:37:30 2021 +0900    minor changecommit f88e2a3a98a7ee283622e57712e28634374e5e2cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:14:54 2021 +0900    minor refactorcommit f2d7ed410a0b835586929706873ee1f448d1f955Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 07:08:47 2021 +0900    support the case when there is not enough boxcommit 0f184a6bf6e533c91d26c98a7a17f8d7970364ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 06:24:16 2021 +0900    Revert ""handling case when num detections is smaller than max_total_size""    This reverts commit 61e70b82f338300224b22f4d6bdda349e7aa5aca.commit d7180f27cfaffbbd1ab1ce970ca605133bc812eeMerge: 61e70b82f 06ac2052aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:37 2021 +0900    Merge branch 'gather_nd_shape_func' into tmpcommit 61e70b82f338300224b22f4d6bdda349e7aa5acaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:43:06 2021 +0900    handling case when num detections is smaller than max_total_sizecommit 453a79bd05f67653be8b90db80ecde12d343aea6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:32:37 2021 +0900    simplify frontendcommit 2fc5f1ed3de49266f1eb72aed25d26457da78491Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun May 30 05:25:13 2021 +0900    update op definitioncommit 8afbd30c0fbbd40902acc4196a18b448f2a93266Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 19:47:04 2021 +0900    remove unnecessary maskcommit ff870f7e972e289953ca0e5daa444c09e5095efaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:54:19 2021 +0900    remove in_buffercommit e71b922b6cdf129ef51e91928635374a1f02a6fcAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:48:22 2021 +0900    minor fixcommit b02faaead24d2d14d3b67bf04ee23f9df9bfecbeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:45:37 2021 +0900    make it more readablecommit 6baee99ed1b57be8da06c00e17d6b92083668ac0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:44:14 2021 +0900    clean upcommit 7a2a2df8b696faf7c4280fd9a3f9fbdf8f5c3e03Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:33:54 2021 +0900    improve sort on cpucommit afad2a2e920c98d269c6000035f31392cff7b6a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 16:29:53 2021 +0900    collect indices and scores in one kernelcommit c5718e299a82ffe5e60bc1fee679b2b0405346e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:47:31 2021 +0900    initialization bug fixed in cudacommit 5623e3f8f71de1dbec55c83a300fa4131cd82aadAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 15:23:09 2021 +0900    cpu nms bug fixedcommit c40eaecd87513a6869098ed95c03b8553c350414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 11:09:47 2021 +0900    add cpu implcommit 6c7aaeb44f5586b57e7b1bfd7772d1b78a9eae1fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:38:20 2021 +0900    refactoringcommit 7b87922279121f06cdcc77a41ac6c8f59b6d5549Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 29 10:03:51 2021 +0900    initial import    commit 5ff0985625ec75f117af37017ebf4089dafb8a46    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:02:45 2021 +0900        cleanup    commit 199f9b67c2d471a761f743e6ea5fa414c899bd3f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 10:00:15 2021 +0900        Revert ""add gather_nd shape func""        This reverts commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855.    commit 47a05c4c8f5a56a1685848210229aaa083b92880    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:53:00 2021 +0900        format    commit 9dcd0f02b25d658c94fa23e2cc65a9424ed8a1a5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:48:43 2021 +0900        make it static    commit eb06393939f1b8d8130f3815dc0f66223c9aa4f3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:14:31 2021 +0900        restore old impl and use it for q != 1 case    commit 115a5dfcf9b552fb2682534d82bbf638e661c0aa    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 09:00:40 2021 +0900        fixed score gathering    commit d2035626a72f8df71c514e3293337f6fda723353    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 08:53:14 2021 +0900        minimum fixed    commit 3fe91e8846b6d2075ae1d9a162c4b70b08cc8024    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 06:59:39 2021 +0900        batch issue fixed    commit 19e3e84690c0289c85001597046969d0c8dc92c2    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:29:15 2021 +0900        zero padding working        This reverts commit 58c3413a30e5b03208b6281651d38ee02c44f9c1.    commit ce7848ba7def5a22659b09de039b2df12c0114f9    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:12:47 2021 +0900        pylint, do not use -1 for default value    commit 968f3bd230ed4855b45fd739dfc86edc2335ec80    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 13:07:31 2021 +0900        rename to index_rank and make it Optional    commit 9e06b8491e0ce1c981a5059f28135319f96978d0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 18:01:59 2021 +0900        fix pylint    commit 81dc6050dcbe59915a8f9b4f78b4aaf9fdba89a6    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:57:03 2021 +0900        minor fix    commit 54297b6128863d07e7dded71cc40077726faf2db    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:54:16 2021 +0900        support dynamic scatter nd    commit e25c225ce747c4e84452e6e7b32eeb0d71b2995d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:33:19 2021 +0900        gather_dim -> num_indices_per_tuple    commit aaa6211e7ef3ce520b8711a78cf7eb2af52e7acc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:23:46 2021 +0900        add dynamic gather_nd test    commit 3a9fe5dfa5faeadbcdb882ff70039ea7bccb61a3    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 17:18:26 2021 +0900        refactor gather_nd ref funcs    commit 1ff4d53f057e7bfd1c6dff31a81f866727bef855    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 21 14:36:34 2021 +0900        add gather_nd shape func    commit b0200643a184294a2f2b3cce7208c4d257987424    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 04:01:11 2021 +0900        working on zero padding    commit 456741790dd5e73f3f76ef7a5ede6e1014de8b2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 03:21:52 2021 +0900        working    commit 7f5c76d0090950985888781f071ca341e2fa5695    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat May 29 02:37:50 2021 +0900        relay type inference works, debugging topi    commit 4a4b8dfbfdc65d7a6e77ed0a6e8b09af162b77ad    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 15:08:16 2021 +0900        add max_total_size to attributes    commit 7218b2f7b4de0c796d69f23084cd688e28f7b461    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:50:58 2021 +0900        tf frontend update    commit cde4a1fdd15ed898b1f7299e99377cceaaee2732    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 14:17:14 2021 +0900        all class nms tf mode first cut    commit 5f349f77c9c230ee636aceb52547502319c8ad77    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri May 28 06:54:34 2021 +0900        begin supporting per batch output    commit 0044365affac6667a02d15791a59040702f8990b    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Mon May 3 19:46:28 2021 +0000        initial    commit 168a617e48b062417b766d6400b0c6b856084cfa    Author: Trevor Morris <trevmorr@amazon.com>    Date:   Fri Apr 16 20:31:32 2021 +0000        initia;        lcommit 06ac2052ab843be950ff3abf6ce8d52803adc5e5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 28 13:12:47 2021 +0900    pylint, do not use -1 for default valuecommit 2adc42618580c967bd49d53c0724382f9cf87772Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 28 13:07:31 2021 +0900    rename to index_rank and make it Optionalcommit c458da6e80b0ff7b6e2ca729a49755f42dfe3702Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 18:01:59 2021 +0900    fix pylintcommit b7faf0f93bd3ba4fc0eb88f1fac31c8d9525c883Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:57:03 2021 +0900    minor fixcommit c03164116046670963f1d04529bfe94c5030ad17Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:54:16 2021 +0900    support dynamic scatter ndcommit 56f3f0ea3fae4ba049101fcb4571b8999a3bda1cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:33:19 2021 +0900    gather_dim -> num_indices_per_tuplecommit 081823b0129093602bb7f512f326eeb10bfb1906Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:23:46 2021 +0900    add dynamic gather_nd testcommit 6b2655baf867b4d08e7d21ffe5f854228ced57e9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 17:18:26 2021 +0900    refactor gather_nd ref funcscommit f9f5dfbe2a65eff8aa6718bf05fd8a843c5df08fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 21 14:36:34 2021 +0900    add gather_nd shape func* make combined nms converter public* do topk on smaller score tensor* update tests* remove max_total_size attribute, do minimum in relay side* fix topk* update relay doc* update doc* fix pylint* update shape func for tf mode and add test* name change* reject dynamic inputs* revert gather_nd change* do not try to support dynamic batch size in tile rep* check batch_size is int* fix dtype issue in scan* fix slicing before topk",0
"[Texture support][Part 0] Device API and runtime support (#7711)* Add TVMBackendAllocTexture and support in OpenCL device API.* Add runtime optimized caching allocator.This should be replaced with AOT memory planningwhen the relay/tir/compile engine refactor lands.* Few bug fixes for runtime texture allocator.* Add OpenCL device api support for image2d<float16> textures.* Update OpenCL DeviceAPI to support Image2D data spaceallocations and copying to/from host/image2d directly.Allocation employs a lowering convention to 2d imagesfor activations and weights.* Fix to follow OpenCL spec. for indexing.* Rename texture_pool.h -> texture.h* Move Nd to 2d lowering convention code into runtime textureutilities that can be shared by codegen and the runtime.* Update texture lowering utilities* Add TODO comment about pitch support* Remove FreeTexture* Fix ICHECK comment* Partial cherry pick from @ZihengJianggit@github.com:ZihengJiang/tvm.git:52822c5bd[RUNTIME] OpenCL texture memory.* Remove runtime and device texture APIs.* Add OpenCL packed functions for texture workspace (de)allocations.* Add OpenCLBuffer structure to trackmemory layout through OpenCL Device API.* Rebase: TVMContext -> Device* Implement DLTensor* overload of CopyDataToFrom in OpenCL DeviceAPI.* Implement OpenCL CopyDataFromTo(DLTensor*...)overload and tensor shapes to calculate image extentwhen copying date directly to or from texture cache.* Update format (cpp-lint)* Update format (clang)* Buffer descriptor name change and formatting.* Add texture pool documentation.* Update runtime to use new global.texture scope.* Move texture_pool.cc into opencl impl.* Add test coverage for copying in and outof storage allocs of texture scope.* Documented APIs and structures, renamed buffer descriptor layout tags.Co-authored-by: ZihengJiang <ziheng@apache.org>",0
Fix typo (#8197),0
fix bug in dense_nopack if dynamic input shape (#8166),0
[RUNTIME][REFACTOR] Re-organize Containers into SubFolders (#8183),2
update python code style to 3.6 (#8199),1
[CI][DOCS] Fix the sphinx doc style for sphinx4 (#8198),0
Fix incorrect device name in TVMC. (#8181)* Fix incorrect device name in TVMC.* Rename gpu -> cuda.* Bump CI.,0
Add thread_warp_size for Metal device in default target attributes (#8202),1
Fix conv2d_nchw for opencl intel graphics (#8201),0
"[QEMU] Add number of cores, target list for build (#8156)* num of cores* add target list* comments* cleanup* cleanup* trigger* address comment* comments",1
[FIX] Allow tokenizer to parse numbers greater than INT_MAX. (#8120),0
"[Frontend, Tensorflow2] Adding TF2 frontend code with support for control flow ops  (#8142)* adding tf control flow ops with a different frontend codeCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Some minor fixes* Fixing output order in TF2 outputsCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* Using black* RefactoringCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>* resolving a bug with passing output tensors for Functional Graphs* fixing multi output for graph runtime* adding docstring edits* linting + black* linting + black* linting + black* removing unnecessary output propagation across function* addressed comments in PRCo-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Xiao <weix@amazon.com>",0
[Relay] Convert a fake quantized or QAT graph into QNN ops (#8126)* Convert a fake quantized or QAT graph into qnn ops* fix pylint* fix typos* use an identify function for some ops* rename the pass from quantize_fake_quantization to fake_quantization_to_integer* add definition for affine,0
[Fix][microTVM] QEMU RPC issue (#8021)* add test* fix test* add parameter to test* cleanup* format* address comments* address comments* direct read/write from/to ring buffer* merge fix* add comment,0
[Docker] Add external directory mount (#8144)* add mount option* comment* cleanup* trigger* trigger* address comments* address comments* trigger* fix without --mount option* address comment* hopefuly last commit :D,0
Support dequantizing scalar inputs (#8207),5
use an empty module for fold_constant (#8208),5
[TIR] Fix data dependent indexing when lowering TE to TIR (#8217)A conversion pass was missing the recursive VisitExpr statement.,0
[VM] Better error messages (#8218),0
"Auto-tuning a Convolutional Network for ARM CPU (tutorial error, bug reports)  (#8103)* tune_relay_arm.py tutorial modify* Lint fix* Re-trigger CICo-authored-by: Chenfan <jcf94@outlook.com>",0
[TVMSCRIPT] Add tir.min node in tvm script (#8219)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,1
[Metal] Remove matching Metal to OpenCL in tophub (#8211),4
"Graph executor: remove unnecessary unique_ptr, NFC (#8214)",4
"[DOC] Improve ""Getting Started with TVM"" tutorials and fix warnings (#8221)* improve src/README.md* fix intro* fix more warnings* improve docs* update* update* update* update overview image",0
Expose list of PassContext configurations to the Python APIs (#8212)* Expose C++ PassContext::ListAllConfigs via its Python counterpart   tvm.ir.transform.PassContext.list_configs() * Add unit tests for the C++ and Python layers,1
[RUNTIME] ShapeTuple Container (#8200)* Add ShapeTuple.* Update NDArray.* Documents.* Lint.* Lint.* Lint.* Address comment.* Address comment.* Address comment.* Lint.* Lint.,1
"[Frontend, Tensorflow, Tensorflow2] Tensorflow frontend op refactor (#8179)* Refactoring the ops from Tf1 frontendCo-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>* Resolving an import bug and refactorCo-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>* Tf2 frontend importing from common ops* linting and unused imports fix* Applying changes from commit id f4ec5fd4ae346dbdd8e915c048aeed94b44f6776Author: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>* Minor lintingCo-authored-by: Rohan Mukherjee <mukrohan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>Co-authored-by: David Huang <davhuan@amazon.com>Co-authored-by: Xingyu Zhou <zhoxingy@amazon.com>Co-authored-by: Srinidhi Goud <srinidhi.goud29@gmail.com>Co-authored-by: Xiao <weix@amazon.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>",0
Fix use of wrong variable (#8227)* Fix use of wrong variable* Fix docstrings,0
Add metadata information to the listing of PassContext configuration listing function (#8226)* Rename PassContext::ListConfigNames() to PassContext::ListConfigs() and its   Python counterpart tvm.ir.transform.PassContext.list_config_names -> list_configs() * Adjust PassContext::ListConfigs() to include also metadata (currently only including the data type) * Adjust unit tests,1
fake quantization to integer (#8228),5
[CuBLAS] Support implicit broadcast in batch_matmul (#8229),5
[COMMUNITY] Egor Churaev -> reviewer (#8231),3
[LLVM] Fix CodeGenLLVM::LinkParameters (#8213)- Generate valid LLVM IR.- Set proper alignment on the constant variables.,0
"[AutoTVM] Added @functools.wraps to function decorators (#8237)This helps in debugging, as the function name, arguments, anddocstrings show the function name from the source code instead of thewrapper function.(e.g.`<function tvm.topi.cuda.dense.dense_small_batch(cfg, data, weight, bias=None, out_dtype=None)>`instead of`<function tvm.autotvm.task.topi_integration.register_topi_compute.<locals>._decorate.<locals>.wrapper(*args, **kwargs)>`.)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Metal] Reduce number of threads for reduction layers (#8206)Reduced default number of threads in reduction kernels for Metal.Default code generation generated thread block with the following size:32x32x1. With this size number of threads per threadgroup was equal to1024 (32 * 32 * 1). Sometimes device doesn't have enough resources andin this case we will get an exception that the block size is greaterthan value of maxTotalThreadsPerThreadgroup.To prevent such situation we decrease default number of threads. Withthis fix every model should work with default codegen and auto-tuning orauto-scheduling will select the optimal number of threads.,0
support matching attributes with more complext objects (#8240),5
"[µTVM] Zephyr: Fix missing board-specific config file in build dir (#8230)Currently board-specific config files (boards/*.conf) are notcopied from Zephyr project dir to the destination build dir, soas a consequence the per board configs are not used when buildingthe runtime libraries, like libcommon. Hence, for instance, it'scurrently not possible to set CONFIG_FPU per board since it onlytakes effect when it's set in the generic 'prj.con' config file.This commit fixes it by copying to the build dir (to each libdir) the proper .conf for the selected target board. For example,if target 'qemu_x86' is selected 'qemu_x86.conf' is copied tothe boards/ dir inside the lib dirs, so Zephyr build system canfind it and combine it with configs found in the generic 'prj.conf'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
Fix compile time and runtime errors of EdgeTPURuntime (#8133)* Fixed the destruction order tflite::Interpreter and EdgeTPUContext* Fixed include omission* Formatted,0
"[Vulkan][Refactor] Move ownership of per-CPU-thread objects to VulkanDeviceAPI (#8196)* [Vulkan][Refactor] Moved VulkanStream ownership from VulkanThreadEntry to VulkanDevice- Implemented ThreadMap, a container for per-thread objects.  Unlike  dmlc::ThreadLocalStore, ThreadMap is intended for use as a  non-static thread-specific lookup.- Added ThreadMap<VulkanStream> as a member to VulkanDevice, updated  all uses.* [Vulkan][Refactor] Pulled VulkanBuffer allocation/deallocation into constructor/destructor.- VulkanBuffer owns the VkBuffer and VkDeviceMemory that it allocates,  and deallocates on destruction.- VulkanHostVisibleBuffer owns a VulkanBuffer, and additional calls  vkUnmapMemory on destruction.* [Vulkan][Refactor] Move the VulkanStagingBuffer to be owned by the VulkanDevice- Previously, was owned by VulkanThreadEntry, so any use required  looking up both the thread entry and the device.  Now,  thread-specific lookup is handled in the VulkanDevice class.* [Vulkan][Refactor] Move ownership of per-thread uniform buffer to VulkanDevice- Previously, VulkanUniformBuffer was owned by VulkanThreadEntry, so  any use required looking up both the thread entry and the device.  Now, thread-specific lookup is handled in the VulkanDevice class.* [Vulkan][Refactor] Moved ownership of per-thread workspace pool to VulkanDeviceAPI- Previously, the WorkspacePool was owned by VulkanThreadEntry, and  required a lookup from VulkanDeviceAPI::AllocWorkspace.  As a  result, non-global VulkanDeviceAPI would interact with each other.* [Vulkan][Refactor] Moved ownership of per-thread active device id to VulkanDeviceAPI- Previously, the active device was owned by VulkanThreadEntry, so  lookups to multiple global variables were required.  Now, everything  goes from the VulkanDeviceAPI.- Removed VulkanThreadEntry, as all functionality has been moved to  either VulkanDevice or VulkanDeviceAPI.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
"[BYOC][ACL] Prevent dilated pooling (#8149)* [BYOC][ACL] Prevent dilated pooling Added check preventing avg_pool2d and max_pool2d to bescheduled for execution via ACL* runtime if dilation otherthan (1, 1) is provided as ACL does not currently supportdilation attribute in pooling layer.*ACL stands for ""Compute Library for the Arm® Architecture""Change-Id: If8f65d3a154e09f880bec73dd756d9f985a20ff2* linterChange-Id: If91809350786e69f59596301e0cbd3def6815cd0",1
[ETHOSN] Removed support for 20.08 version of the driver stack. (#7858)- Replaced capabilities header file with api calls introduced by the 20.11 ethosn driver stack release.  - Removed 20.08 driver stack support and updated all affected code.,1
"[microTVM] Add QEMU build to RVM image (#8190)* num of cores* add target list* extension* qemu* fix* comments* add qemu to setup build* fix* add mps2 test* merge fix* add commit option* add log* fix* fix zephyr init* rename* fix zephyr init* uncomment* fixed qemu isntall* cleanup* version* add commit option* fixed qemu isntall* add docker import* cleanup* fix* cleanup* fix* fix zephyr path* fix* fix* address comments* fix test* fix* add wait* comments* changed test to script* add checks* fix zephyr* Revert ""add wait""This reverts commit 70f3c7d840028d81823b99ae12ae6969b80d9a91.* address comments",0
[TOPI][batch_matmul] Allow cblas batch_matmul implicit batch_size broadcast (#8250)* Allow cblas batch_matmul implicit bcast* Add cblas batch_matmul bcast when batch_a=1,1
doc: fixes to dataflow_pattern (#8247),0
Unify Python and C++ TIR lower API (#8110),5
Move Micro TVM top level page (#8249)The micro TVM page was moved during a recent docs update. Thispatch moved the top level index to the former location.,1
[CI] [ComputeLibrary] Use pre-built binaries instead of compiled (#8245)* [CI] [ComputeLibrary] Use pre-built binaries instead of compiledPre-built Compute Library binaries are now downloaded (credits to @leandorn)instead of on-site compilation.Change-Id: I9fd66ce02141813f02382b95351a382ccf775584* Added Apache 2.0 LicenseChange-Id: I3c2af1a86984f81c4ee9408925af9c51510a978f,1
Fix build break in android_rpc (#8252),0
make simplify inference iterative (#8246),5
[BUG FIX] Add _type_has_method_sequal_reduce to Span and SourceNode (#8248)* add field _type_has_method_sequal_reduce to Span and SourceName* retrigger CI,0
[Target] Allow 'true' and 'false' strings in conversions to integer (#8254)* [Target] Allow 'true' and 'false' strings in conversions to integerThis will allow Bool parameters to take true/false values insteadof 0 and 1 only.* Convert the string to lowercase.* Reserve memory for lowercase string* Add include <cctype>,1
Update parsed kernel sources check. (#8257),1
Add check to only cast opaque handles to cl::BufferDescriptor at runtime. (#8256),1
[microTVM] Add wait to QEMU Setup   (#8236)* add wait* address comments* address comments* test added* add suggestions* fadd message assert for test,1
Fix compilation of tvm runtime for iOS (#8242),0
"[Metal] Fix run metal model when non first device is selected (#8261)In case when we select non first Metal device, we got problem instream, due to we used wrong device_id in CopyDataFromTo.",0
Fix docstrings in tvm.relay.cast_like (#8262)* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos,0
[tvmc] Add a --config option to `tvmc compile` (#8253)* Allow to send some configurations to the PassContext via command line * Add various validations to the new option with appropriate error messages * Add unit testing,0
[TVMSCRIPT] Fix printing of rank 0 buffer access (#8215)* [TVMSCRIPT] Fix printing of rank 0 buffer accessAlso improve error messages and fix min/max/Select.* fixes* return fix* remove print,0
"[Frontend, Tensorflow] Support for broadcasting in batch_matmul when shapes differ (#8251)* Support for broadcasting in batch_matmul when shapes differ* refactor* refactor logic for reshape in conditional* refactor",2
Fix GatherND attribute registration (#8269),0
"[Metal] Fix bad stream after interrupted tuning session (#8244)* [Metal] Fix bad stream after interrupted tuning sessionAfter interrupted tuning session, we may face the problem that thestream object was released, but we didn't create a new one. In this caseit wasn't possible to run a new Metal task on the device withoutrestarting rpc application.Created a global function `metal.ResetGlobalState` which should becalled in RPC application when the connection was closed. In thisfunction, we reinitialize the streams of Metal devices. And itguarantees to us that the new RPC session will work with the correctstreams.* Refactor metal_device_api- Rename function GetStream -> CastStreamOrGetCurrent- Add several checks on device id- When we use `SetStream` with nullptr, then the default stream will be  associated with the device.",0
[Relay][Convert Layout] Enable layout transformation for image.resize op (#8205)* Enable layout transformation for image.resize op* Change str map function to str and index retrieval* Fix for pytorch frontend segmentation models test,0
[CUDA][PASS] conv2d NWHC/HWNC legalize tensorcore (#8222)* add conv2d leg* minor fix* fix pylint* fix pylintCo-authored-by: wangyucheng <wangyucheng@sensetime.com>,0
"[topi][CuDNN] Removed requirement for GPU from topi conv2d_cudnn.cuda and conv3d_cudnn.cuda (#8276)Previously, `conv2d_cudnn.cuda` would use cudnn's benchmarkingfunction to select a forward convolution when `cfg.is_fallback`, and`conv3d_cudnn.cuda` would use cudnn's benchmarking at all times.After this commit, both expose the cudnn algorithm choice as anoption.  If `cfg.is_fallback`, the local device will be benchmarked ifpresent, otherwise will select a default cudnn implementation.In the future, to better support RPC use-cases, the fallback configshould be based on cudnn-specific parameters saved in the Targetobject.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",4
Fix a word typo and add spaces. (#8278)Co-authored-by: kueitang <kueitang@qti.qualcomm.com>,0
[IRPrinter] Prevent multiple printing of optional info (#8279)* fix* test,0
add metal to list of choices (#8282),1
"[Vulkan][Codegen] Fixed SPIR-V scoping bug with threadIdx (#8281)* [Vulkan][Codegen] Fixed SPIR-V scoping bug with threadIdxUnlike in Cuda, where the threadIdx.x/y/z are separate built-invariables, in vulkan the built-in thread index consists of an arraythat must be dereferenced.  If `te.thread_axis(""threadIdx.x"")` isfirst declared inside a scope, then that same python object is passedas the IterVar of a separate scope, this breaks the spirv scopingrules, and may result in an undefined variable.  This only applies tothreadIdx/blockIdx variables, as all other variable declarations obeytir's scoping rules.To resolve, the threadIdx and blockIdx variables are declared at thetop of the function declaration.  This makes the generated spirv codefollow the same semantics as tir/cuda.* [Vulkan][Codegen] Refactor of GetLocalID and GetWorkgroupIDShared behavior separated out into GetBuiltInValue.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Bug Fixed] Make query_rpc_tracker show the correct device server port and customized address (#8203),0
"[CuDNN] Remove GPU dependency from tvm.contrib.cudnn.conv_output_shape (#8275)Previously, if the local server couldn't initialize a CuDNN-enabledGPU, it couldn't generate code that uses CuDNN's forward conv.  Thiscommit adds a python implementation of conv_output_shape, along withtests to verify that the outputs are matched to CuDNN's output.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Relay][Dataflow] Fix test_rewrite_function_with_fuzzy_body test check (#8287),0
[VM][PooledAllocator] try reallocation once when OOM (#8285),5
"support adb-shell style cpp_rpc (#8223)* support adb-shell style cpp_rpc* fix review problems,  #8223* add comment & use /data/local/tmp dir in shell terminal case* fix spelling errors* fix spelling errorsCo-authored-by: rqg <ranqingguo90@qq.com>Co-authored-by: rqg <ranqingguo318@gmail.com>",0
"[Relay] [Pass] Add mixed precision (e.g. FP16) model conversion pass  (#8069)* Initial skeleton for fp16 pass.initial green gray and red listsmove fp16 conversion to own fodlersecond pass examplesplit up files a bit morecool nodes broinitial transofmr pass* Working python version of fp16 pass.fix topi conv2d not casting kernel to output typeworking resnet, but conv2d topi intrinsics need worktests for resnetadd more tests, extend coverage for converterupdate tests, ensure red ops convert back to fp32clean up code a bitsimplify fp16 output dtype examinationfix passupdate testsinitial coloring* Rewrite python passes in C++inspect arg fieldsadd propagate colors pass""private -> public inheritance""rewrite draftfull transformation in c++remove printsfp16 pass the proper wrappinginsert extra cast to pass type checkingfix previously broken test by removing cast in wrong scenarioremove old python_files* Extend support to things besides CallNodes. E.g. tuples and letsfp32 invalidate typing instead of cast addingbasic testsskeleton code outStash work -- casting based on checked typesworking let statementsadd more ops, handle functions more generallyadd multiply, fix broken casesupport TupleNodes properly, move hash function for datatypes into data_type.h""update simple let test with structural expectationcleanup p1remove old file* Rewrite how and when casting is done by checking types directly.add support for GPT2, BERTadd some more commentsnew single pass versionformattingmake a lot of things const referencesclean up testsmore cleanupmore commentsfinal commentadd newline* linting and formatting* add AST header* remove todo* lint errors2* remove i386 incompatible features* Trigger CI again* set seed* lint* address animesh's initial comments* mutate attributes only if they were originally floats* initial comments from matthew* add comment on hashing strat* add missing ;* edge case when mutating attrs* Cody's easy to address comments* add test to show green-red casting works* remove np.random seed from each test* remove as many references to fp16 types in favor of generic mixed types* rename RED, GREEN, GRAY to MIXED_PRECISION_ALLOW, etc.* skeleton for supporting arbitrary mixed types* cool tests* Using MixedModeMutator* rename things ToMixedPrecision* rename passes to amp.cc* rename tests to match transform* clean up typos* rename even better to_mixed_precision* don't insert into cache when dtypes equal* new python interface for registering ops* cleaner registering ops* add fp64 structural test* clean up and comments* make copy of attributes* asf header* pylint* remove TODO which is solved* Apply nits from code review (comaniac)Co-authored-by: Cody Yu <comaniac0422@gmail.com>* change cast_node_cache --> cast_node_cache_* add check for returned vals* better error msg* docstring for pass in python* fix default behavior to be proper* better error reporting via single flag* priority to 0* address more nits* fix story telling slightly* restart* correct docstring* change class fields to have _ at end* add class docstring* add comment on accumulation dtype hack* ADT warnings* add todo* fix linterCo-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[Auto Scheduler] Make the opt_level of task extraction adjustable (#8288)* fix bugs in the auto scheduler record:* reformat the code* use the os.path.abspath* change error to warning* reformat the warning code* fix some typos* fix the port number typo* fix a typo* make query_rpc_tracker show the correct port and the customized address* disable the pycharm reformat* reformat the code* make the opt_level of extract_tasks adjustable* Update rpc_server.py* fix a typo* Update tracker.py* support checking the port and customized address* reformat the code* fix a typo,0
[TensorFlow][Frontend] Adding InversePermutation Op (#8277)* [TensorFlow][Frontend] Adding InversePermutation OpComputes the inverse permutation of a tensor. This Op is used by Mask R-CNNor other object detection models.* uncomment test_read_variable_op* restore several tests* fix lint error* fix python linting error* fix lint error* restore mistakenly deleted codes,0
refact: rm unused variable (#8290),5
[microTVM] Refactor uTVM to microTVM (#8283)* refactor* rename utvm_rpc_server.h* rename file* rename* rename file* rename* variables* directories* format* trigger* more refactor* one more* format,2
Fix deprecated use of numpy.asscalar. (#8292)Replace use of numpy.asscalar with the use of .item .Anyone know how to use pylint to catch these uses before they land.,0
Fix bulleted lists in TVM documentation. (#8268)* These currently do not render due to https://github.com/readthedocs/sphinx_rtd_theme/issues/1115 * Breakage was likely caused due to https://github.com/apache/tvm/issues/7995,0
[RPC][CPP] Add support of cpp RPC-server for Apple (#8224),1
Check for presence of LLVM configuration. (#8293)LLVM is a pre-requisite for configuring Compute Libraryand offloading to Ethos-N NPU.Better to get the error message at build time ratherthan debugging SEGVs in testsuites.,0
Fix Intel OpenCL SDK search path for Windows (#8301)Co-authored-by: Andrey Malyshev <andrey.malyshev@gmail.com>,0
[BYOC][NNAPI]: Add testing package to ci_cpu image (#8088)This commit adds Android SDK to the ci_cpu image for supporting tests of Android NNAPI BYOC.,1
Turn on Compute library testing in CI for AArch64 (#8291)* Turn on Compute library testing in CI.This pull request turns on compute library testing in CI by1. Handling import errors in Compute Library Integration.2. Setting the configuration to the right path for ACL.This handles import errors for packages in Compute library integration.This pull request allows for the AArch64 CI to pick up nativecompute library testing and tests the operators being offloaded atruntime.* Fix typo* Fix up use of ubuntu_install_arm_compute_lib.sh in Dockerfile.ci_arm* Move to using pre-built ACL binaries for ci_arm* Fixup the path for installation to be /opt/acl as it originally was.* Fix up the issues with paths.Once this is done ci_arm will need to be rebuilt though will continueto work seamlessly.,0
Update ONNX versions (#8304),1
"Fix rst formatting in documentation (#8303)Whitespace is required before "":sup:"". Using backslash-escaped whitespaceprevents it from appearing in the processed document.",0
"[Docker] Update tensorflow/tflite/xgboost versions (#8306)* [Docker] Updated tensorflow/tflite version to 2.4.2Tensorflow update required following update to cuda 11.0.  Based onhttps://www.tensorflow.org/install/source#gpu, the 2.4 branch oftensorflow should be used with cuda 11.0.- Removed pinned version of keras/h5py, no longer needed.  https://github.com/tensorflow/tensorflow/issues/44467#issuecomment-720631688- Updated tflite version to 2.4.2.  Also, tflite install script now  reads the installed version of tensorflow, to keep the version  matched in the future.* [Docker] Corrected version pinning of xgboostPreviously, due to missing quotes, installed most recent version ofxgboost, piping the results to a file named '=1.1.0'.  Now, installsxgboost at least at version 1.1.0.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[TVMSCRIPT] add more type support in script function parameter (#8235)* [TVMSCRIPT] add float type support in script function* [TVMSCRIPT] add more type support in script function parameterCo-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,1
Port StorageInfo and StaticMemoryPlan data structure (#8297),5
[rust] convert error msg to string for panic macro (#8289),0
Install curl in ubuntu_install_core.sh (#8310)Rationalize the installation of curl among all the docker install scripts.,2
"Fix ordering of tf and tflite installs in ci_cpu (#8312)The recently merged 8306 PR introduced a depedencyfor tflite installation that tf must be installed first.However, that PR did not correct the ordering in ci_cpu whichdoes not have that ordering.Change-Id: Ib82c2b33e4e123d4562682e9e97b21bfe23cc0ef",0
[DOCKER] fix sphinx install versions (#8316),0
[Relay][Training] Additional gradients (#8307),1
"[Docs] Prevented docs/1 file from being generated. (#8029)* [Docs] Prevented docs/1 file from being generated.Typo in tests/scripts/task_sphinx_precheck.sh caused $TVM_HOME/docs/1file to be created with stderr output, rather than merged stderr andstdout.* [Docs] Corrected sphinx build warnings- Previously, several warnings were generated by sphinx, but were  unintentionally suppressed.  This PR resolves the sphinx warnings.* [Docs] Corrected additional sphinx build warnings.- Rebased on main and corrected warnings, now up to date as of commit  53e4c603.* [Docs] Corrected additional sphinx build warnings- Rebased on main and corrected warnings, now up to date as of commit  1f2ca068c.* [Docs] Corrected additional sphinx build warnings- Rebased on main and corrected warnings, now up to date as of commit  d0791d3db.* [Docs] Ignore sphinx warnings from missing ""git describe"" and sckit-learn versions.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Relay][Frontend][Onnx] Enable group_conv1d import through conv2d conversion. (#8321)* Enable group conv1d import through conv2d hack.* remove silly commented out lines.,4
"[UnitTests] Automatic parametrization over targets, with explicit opt-out (#8010)* [UnitTests] Explicitly list tests that were enabled by TVM_TEST_TARGETS but were skippedPreviously, these were removed by a filter intvm.testing._get_targets(), and weren't listed at all.  With thischange, they are instead removed by pytest.skipif, and show up asexplicitly skipped tests in pytest's summary when usingtvm.testing.parametrize_targets.* [UnitTests] Automatic parametrize_targets for tests that use (target,dev)Should make it easier to convert tests from usingtvm.testing.enabled_targets to use pytest's parametrized testsinstead.* [UnitTests] Added ability to explicitly exclude a target from a particular testUses tvm_exclude_targets variable, which can be set (1) in theconftest.py to apply to a test directory, (2) in a test script toapply to that module, or (3) on an individual test function to applyto it.  The @tvm.testing.exclude_targets decorator is provided forreadability in case #3.* [UnitTests] Refactored test_topi_relu.py to use pytest.mark.parametrize* [UnitTests] Added tvm_known_failing_targets option for the unittests.Intended to mark tests that fail for a particular target, and areintended to be fixed in the future.  Typically, these would resulteither from implementing a new test, or from an in-progressimplementation of a new target.* [UnitTests] Known failing targets now marked with xfail instead of skipif* [UnitTests] Removed tvm_excluded_targets and tvm_known_failing_targetsThese were implemented to exclude or mark as failing an entire file ordirectory of tests.  Inhttps://discuss.tvm.apache.org/t/rfc-parametrized-unit-tests/9946/4,it was pointed out that the global variables would be vulnerable totypos in the names, resulting in the option being silently ignored.The decorators `@tvm.testing.exclude_targets` and`@tvm.testing.known_failing_targets` do not have this failure mode,and are the preferred version.* [UnitTests] Added helper functions to tvm.testing.- tvm.testing.parameter() defines a parameter that can be passed to  tests.  Tests that accept more than one parameter are run for all  combinations of parameter values.- tvm.testing.parameters() defines multiple sets of parameter values.  Tests that accept more than one parameter are run once for each set  of parameter values.- tvm.testing.fixture() is a decorator that defines setup code.  The  `cache=True` argument can be passed to avoid repeating expensive  setup across multiple tests.* [UnitTests] Bugfix for auto parametrizing of ""target""Previously, if the @parametrize_targets were present, but had other@pytest.mark.parametrize after it, ""target"" would get parametrized asecond time.  Now, it checks more than just the closest ""parametrize""marker.* [UnitTests] Renamed ""cache"" argument of tvm.testing.fixture to ""cache_return_value""* [UnitTests] Minor updates to parametrized test implementation.As recommended by @tkonolige:- Avoid infinite loop if LLVM target isn't enabled- Update documentation for preferred use cases of  tvm.testing.parametrize_targets, and recommended alternatives.* [UnitTests] Minor updates to parametrized test implementation- Documentation, removed previous example usage of tvm.testing.parametrize_targets* [UnitTests] Changed accidental use of pytest fixtures to a NameError.- Previously, a fixture function defined in a module was accessible  through the global scope, and the function definition is accessible  if a test function uses that name but fails to declare the fixture  as a parameter.  Now, it will result in a NameError instead.* [UnitTests] More careful removal of fixture functions from module global scope.- Initial implementation only checked hasattr(obj, ""_pytestfixturefunction"")  before removing obj, which gave false positives for objects that implement  __getattr__, such as caffe.layers.  Now, also check that the value  contained is a FixtureFunctionMarker.* [UnitTests] Copy cached values when using tvm.testing.fixture(cache_return_value=True)To avoid unit tests being able to influence each other through ashared cache, all cached fixtures are passed through copy.deepcopyprior to use.* [UnitTests] Added meta-tests for tvm.testing functionalityCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Vulkan] Implement sync for SyncThread(""warp"") (#8320)- Add sync if a SyncThread(""warp"") node is present.  The sync is done  at spv::ScopeSubgroup if supported (Vulkan 1.1+), and at  spv::ScopeWorkgroup otherwise.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
fix first-order AD tuple/projection expr duplication (#8318),0
[tvmc] Fix inconsistent usage of host_name -> hostname (#8324)* This prevents a python error when running tuning via   and RPC tracker on tvmc. * Add test case,0
"[CI] Install curl in the context of ubuntu_install_nodejs.sh (#8326)* Make sure that curl is installed, as this script is used on   ci_lint, which does not need all the packages installed by   ubuntu_install_core.sh",2
Initial support for enabling MyPy in CI  (#8302),5
[COMMUNITY] Reviewer: wyc-ruiker (#8328),3
"[Docker] Fix ordering of tf and tflite installs in ci_qemu (#8315)Similar to #8312, the recently merged #8306 required tflite to beinstalled after tf.  However, #8306 did not correct the ordering inthe dockerfiles.  After this and #8312, all dockerfiles should be upto date with the correct ordering.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Relay, TOPI] Add negative log likelihood loss (nll_loss) op (#8056)* add nll_loss* enrich the doc and rename parameters* update upon review* add tests* update based on reviews* update upon reviews* update upon reviews",1
[DOCKER] Update lint to reflect the latest state (#8330)Pins mypy version.,1
Make sure there is no tie in scores in NMS test (#8335),3
[CI] Pin mypy version (#8329),5
[Relay to Onnx Conversion test] Fixed relay.var initialization (#8322)* Fixed issue in bias variable initialization by passing arg name (shape),0
[Relay] Remove in-place modification of attributes in layout transform (#8309)* stub* mnist test working* porting InferCorrectLayout* compiles with new infer layout* remove log* fix qnn concat* do not run dense pack alter op test on gpu targets* cleanup* add test* cpplint* CHECK -> ICHECK* doc update* restore try catch* split inferred_layout into seperate fields* Update InferCorrectLayout functions following struct field change* fix cpplint,0
"[Makefile] Updates to top-level makefile. (#8317)* [Makefile] Minor cleanups to up top-level makefile- Renamed OUTDIR variable to TVM_BUILD_PATH- Delegate emcc calls to the makefile in ""web"" directory- Separated out build rules by type (e.g. C++, java, web, formating/linting)* [Makefile] Allow TVM_BUILD_PATH to include a list of directoriesIntended for development purposes, where building both debug andrelease versions at once may be useful.* [Makefile] Preserved behavior of using root tvm/config.cmake if it exists.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Relay to Onnx][LRN] (#8323)* Added support for LRN operator* [Relay to Onnx]* Added unit test case for LRN* * reformatted* * reformatted (2)* * fixed formatting issues in relay to onnx conversion script* * fixed formatting* change single quotes to double* set space to 4 instead of 2* * reformatted onnx.py: corrected spaces* [Relay to Onnx] LRN* Assert if axis != 1* * fixed formattingCo-authored-by: zxy844288792 <zhoxingy@amazon.com>,0
"[Vulkan] Improved error message for extern calls passed to SPIR-V codegen. (#8332)Previously, the codegen indicated that there was an extern call.  Now,also indicate what that extern call is, to aid in debugging.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Vulkan] Added debug saving of Vulkan shaders, environment variable documentation. (#8333)Frequently, looking at the shaders generated by the Vulkan codegen isuseful for debugging.  While this can be done by checking the`mod.imported_modules[0].get_source()`, that requires the shader tofirst pass validation.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[TEST] Fix flaky test nll (#8344)* [TEST] Fix flaky test nll* Update the tol,0
Remove an extra print from the relay astext tests (#8342),3
ffi: add missing binding for FixedPointMultiplyAttrs (#8353),0
[AutoScheduler]Simplify the code (#8351),5
"[AOT] Name mangling in AOT (#8014)* [AOT] Name mangling in AOTMini-RFC is here: https://discuss.tvm.apache.org/t/mini-rfc-name-mangling-in-aotWith this change we'll mangle the name of global symbols so that we can bundletogether multiple models in the same application.The relay.build interface has been left unchanged, which means I amresuing mod_name as a prefix for all functions. If mod_name is None thena ""_tvm"" prefix is used.I had to add two different compilation functions:- _CompileEngineLowerWithModuleName to mangle all the operators with the mod_name- PartitionGraphWithModName to mangle all the operators produced by BYOCI could have changed signature of both, but that would have meant a veryinvasive refactoring.I refactored the aot test utils and added some tests for multiplemodels.Change-Id: I30e93fa075f660054577ea36cf9268ec0c6eebcb* retrigger CIChange-Id: I4f11da7fce1327ad89bb25f25209b57077b2c6a3",0
"[RPC] Fix android rpc connection to tracker (#8327)* [RPC] Fix android rpc connection to trackerAfter commit 0bbaf0e, android_rpc wasn't able connect to rpc_tracker.Added addr field to cinfo.* Fix CI",0
"[Onnx] Support Bidirectional RNNs (#8337)* modify lstm to be easily bidirectional* make it obvious some matriciies are packed via prime notation* fix var name* more var names* add op split* keyword arg names* missing implicit cls arg* deal with extra dimensions* last of the fixes* refactor rnn tests to support directions* bidirectional tests* test forward results* go backwards* more fixes* reverse tokens on reverse pass* parameterized directions* double up activations in bidirect* slow attribute forgetting* lstm interface is v. confus* test forward complete* add GRU outline* revisiion2* why was tehre a not* gru tests* missing bounds, copy pasta!* add comment* ensure all args fp",0
bump sphinx-addon version (#8360),1
[TVMC] Add vulkan to targets of tvmc run. (#8359)This allows to run compiled models via Vulkan.,1
[ONNX Parser] Add warning in case of opset mismatch (#8356)* add warning in case of opset mismatch* fix CICo-authored-by: elenaslavutina <elena.slavutina2013@gmail.com>,0
[Relay][Parser] Support slash in identifier. (#8352)* [Relay][Parser] Support slash in identifier.Variables from tensorflow may contains '/' in name (x/y/z).* Check identifier name after parsing.,2
"[AMP] Turn off accumulation data types for mixed precision pass (#8341)* don't use mixed precision accumulators* turn off fp32 accumulators for now, adjust passing test cases* Add TODO on cuda codegen for failures. Make test case pass on cuda for nowtest to mixed precisionmore testsadd internal func callbroadcast failuresmoreeeadd comment and change lstm unit test to pass on cuda* remove debug statements* to mixed precision* rebase main* rtol and atol adjustments* bump up tolerance again* jostle CI",0
[Docker][QEMU] Update gpg server (#8319)* update* trigger,1
[TEST] Disable flaky TF combined NMS test (#8364)Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>,3
[TIR] Tighten up invariance of CopyOnWrite in recursive stmt visitor (#8358),5
Decoupling AOT from graph memory planner (#8096)* Fix an issue with storage-rewrite pass and packed functionsChange-Id: I13888471d4b8927a4012d6a8e749fb7a8935dd77* RebasingChange-Id: I7aa12e0217b8a2e1ff2a97a7c5fdda6b7597ae64* Addressing commentsChange-Id: If9f1ee190690f9a810fe41eb1933d736f1eb4ec3* Add a pass to legalize packed callsChange-Id: I8aa43d3a1b837b03a5cf3c6b32fc760bd78d3436* Add a unit test for the legalization passChange-Id: I5b0d75380ff660dd5a0acf5b14fa84bb992fbec4* rebasingChange-Id: I52ceab5cf6e9b54390cb36c18dbb8e22505d8e18* Use common StorageInfoChange-Id: Ia8b7de1373f167ca7d0d69a99846d417405bbe48,0
"[Bugfix, CuDNN] fix segfault when cudnnDestroy called with destroyed cuda context (#8267)* fix: cudnnDestroy called after cuda context is over* refact: rename global var with `g_`* clang-format* refact: let cudnn handlers leak",0
"[Topi][Unittests] Parametrized tests in `test_topi_dense.py`, split out gpu-independent implementations (#8336)* [Topi][UnitTests] Parametrized tests in test_topi_dense.pyNow, tests run for multiple data types, can be extended withadditional datatypes.* [Topi] Separated generic-gpu nn.dense implementations into topi.gpu.denseAs a follow-up to the renaming of ""gpu"" to ""cuda"", separatingimplementations that require CUDA (e.g. dense_cublas.cuda) fromimplementations that require any GPU, but not necessarily a CUDA GPU(e.g. dense_small_batch.gpu).My intent is to pair this migration with the extension of unit teststo cover additional GPU runtimes, migrating only implementations thatrun correctly on non-CUDA GPU devices.* [Vulkan][Codegen] Updated storage sync to avoid incorrect matmul results on some GPUs- In ThreadAllreduceBuilder, separate out load/store so that they can  have a memory barrier in-between.- In Vulkan codegen, added Workgroup memory sync for subgroup thread  sync, since the different subgroup threads can still access  workgroup memory.  Longer-term, may need tir enhancements to  separate out sync of control/memory.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Build] Add CUDA_VERSION and GIT_COMMIT_TIME (#8372)* [Build] Add CUDA_VERSION to libinfo* add git commit time,1
Fix compute library installation on AArch64 (#8371)* Fix compute library installation on AArch64* empty,0
"[Unittests] Added a meta-test for tvm.testing.fixture behavior in case of a broken fixture. (#8343)In these cases, the test function should be marked as failing thesetup, and should not run.  This is pytest's default behavior, andshould work whether or not a fixture is cached.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Vulkan][Unittests] Add parametrization to vulkan unit tests. (#8348)This also switches to using `vulkan -from_device=0` by default, andmarks tests as `pytest.xfail` if the device does not support thefunctionality being tested.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[FIX] Detect like cores by looking at scaling_max_freq instead of (#8370)cpuinfo_max_freq,0
[Matmul] Add matmul op (#8234)* Add Matmul Op* Recover DenseAttrs* Add grad for matmul & some update* Update matmul cuda default schedule* Add blas support for matmul* Lint fix add update doc strings,0
Fix issue with importing models using Tensorflow Lite 2.4.x schema (#8375)Tensorflow Lite has changed the opcode for BuiltinOperatorsto be represented as 32 bit integers instead of 8 bit integersin the schema.This is an attempt to fix this in a way that is clean to handlemultiple versions of tensorflow lite in the frontend.,0
[MyPy] Minimal type checking on TIR schedule (#8367)* [MyPy] Minimal type checking on TIR schedule* [MyPy] Remove set +e from runnning script,4
"Update the tvmc tutorial with additional requirements (#8334)* Updates the tvmc tutorial with additional requirementsThe pre- and post-processing scripts supplied in this tutorialrequire pillow to be installed, and this tutorial also requiresthat onnx be installed. This patch indicates those are requirementsfor successful completion of this tutorial.* Fix typo in tvmc tutorial, update to new tvmc outputFixed a typo in a tutorial command, and updated the outputto reflect the current output of TVMC* Update tutorial to indicate tvmc operating system supportTVMC does not currently work on macOS or Windows.",0
Support QLinearAdd from onnx runtime com.microsoft contrib ops. (#8305)* support QLinearAdd* fix comment line length* use platform independent temp directory,0
[Metal] Add pass for splitting kernel with huge number of args (#8313)* [Metal] Add pass for splitting kernel with huge number of argsThe Metal has some limitations on the number of input parameters. Moreinformation can be found here:https://developer.apple.com/documentation/metal/buffers/about_argument_buffers?language=objcIn this commit a new pass for splitting functions with big number ofarguments to smaller parts was added. In parameter `max_function_args`we can specify the maximum number of kernel arguments for specifictarget and then split kernel when the number of arguments exceeds thevalue of `max_function_args`. Currently this pass works only for concatlayer.* Add getting number of output parameters* Fix CI and apply comments,0
"[Tuning] Allow multiprocessing spawn to work (on macOS llvm at least) (#8363)* go to callable class* add some documentation and naming* extend comment* manually do logic to avoid bug with pointer comparison* revert changes to light change, correct comment'* more principled change, but also kind of hacky* test other tuning methods* remove check;* jostle CI",0
fix ci-arm build process (#8377),0
"[FIX] Fix depthwise conv2d on non-cuda GPU platforms (#8379)The depthwise_conv2d schedule had a bad check to make sure the iterationaxis was not larger than max_num_threads. Now the iteration axis isbounded by its size or max_num_threads, whichever is smaller.",0
[cuDNN] Add support for log_softmax (#8369)* log_softmax strategy and cudnn impl* add log_softmax cudnn test* silence terrible pylint suggestion* fix typo,0
"Allow tvmc to compile models with AOT executor in MLF (#8331)* Allow tvmc to compile models with AOT executorThe tflite_compiled_model fixture was getting duplicated a few times soI've added a parameterized fixture tflite_tvmc_compiler which combinestmpdir_factory setup with compile_modelNested targets broke a basic string split, so in cases where we usenested targets I replaced the string split with shlex split* Clarify that graph JSON is required only for graph executorPlus other clean ups* Change parametrize fixture to use string instead of list",0
[TIR][TVMScript] specialize (#8354),5
"[Refactor] Remove dead code from depthwise_conv2d for Intel graphics (#8381)After fix a66186b, I saw that it should be necessary to do the same fixfor depthwise_conv2d for intel graphics. I saw that we never used theremoved code and it is just the same code fromcuda/depthwise_conv2d.py. So we can use the cuda implementation when itwill be necessary.",0
[BugFix][Relay] Fix type relation for batch_matmul (#8376)* fix type relation for batch_matmul* fix lint,0
fix keras install (#8391),0
Fix np.int and np.float usage in the tree. (#8389)* Fix np.int and np.float usage in the tree.Newer versions of numpy give loads of warnings that suggestthat np.int and np.float will be deprecated. CI uses pytestand these warning logs clog memory for testing and make itslower.* Fix formatting,0
Add missing annotation for requires_gpu in test_topi_dense.py Requires GPU (#8387),1
"Add ""operator"" style to Model Library Format (#8072)* rename _update_target and document its function* make tvm.build return OperatorModule to return multiple outputs* allow retrieving the var names used in TIR repr* add Operator Model Library Format and test* Add pathlib convenience functions to utils.TempDirectory.* fix tests* black format* git-clang-format* pylint fixes* add asf header* change memory map to make more sense, fix tests* address giuseros comments* align GetVarName with future TypedPackedFunc* fix test* clang-format* rev model library format to v4 (bad merge)",0
macOS is now supported (#8396)Remove warning about macOS support from tutorial,4
[microTVM] Add Nucleo stm32l4r5zi board to zephyr (#8386)* add stm32l4r5zi_nucleo* add parameter for test qemu* file type check* fix test* change order* revert,0
"[Torch] Remove unused conversion (#8397)* fix weight shape in torch.mm conversion* Revert ""fix weight shape in torch.mm conversion""This reverts commit a1a8fd313c999060db675848f8b3de3e1c78e468.* [Torch] remove unused conversion",0
[Arith] Inverse affine map (#8384)* [Arith] Inverse affine map* [Arith] Inverse affine map* Update iter_affine_map.h* Update iter_affine_map.h* Update iter_affine_map.py* Topology order visit* doc* fix* address comments* lint* remove print,0
Actually add Compute Library tests to the Jenkins File (#8394),1
Support aten::flip (#8398)* Support test aten::flip* Support aten::flip,3
"[Relay][TOPI] Resize 1D (#8346)* rename resize to resize2d* refactor resize_2d* Add resize1d op, normalize attribute names across ops* normalize resize3d to match the API of 1D and 2D* fix lint* fix relay tests from API change* refactor topi tests, docs* fix method naming in framework frontendsfix more frontend issues* refactor resize tests to reuse components, add more coordinate tranform modes to tests* add cubic resize reference kernel and tests, add relay tests for resize1d* fix pylint* fix test typo",0
[Docs] Fix for broken link in apps for wasm-standalone dir (#8045)* [fix] Broken link in apps for wasm-standalone* [fix] Broken link in apps for wasm-standalone* [CI] Manual trigger for CI,0
add aten::masked_fill_ in pytorch frontend (#8403)Co-authored-by: Jackson Hsieh <chengpi@amazon.com>,1
fix storage rewrite index remap (#8338),0
Cleanup more uses of np.bool and np.int. (#8399)In a similar vein to previous pull requestsreplacing deprecated use of np.bool and np.int fromnumpy with bool and int.https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations,2
[Fix] Update stale relay.Module API in docs/comments (#8411),0
"[ONNX] Wrap 'If' if it has multiple outputs (#8385)* [ONNX] Wrap 'If' if it has multiple outputsWithout this wrapper, an assertion in from_onnx() will fail with theerror message showing """"Number of output mismatch""* [ONNX] Test If nodes with multiple output tensors* Fix formatting issues",0
"[DOCS] Add docs for Pass Instrument (#8220)* Fix AttributeError when TEST_DATA_ROOT_PATH is setInitiate a Path object from TEST_DATA_ROOT_PATH to fix the error:AttributeError: 'str' object has no attribute 'mkdir'* [DOCS] Add docs for Pass Instrument - Add a tutorial about how to use pass instrument. - Add related sections in Pass Infrastructure documents.* Fix ir.rst, the length of separator.* Fix unused local name* Fix linting errors* Fix linting errors* Fix linting errors* Address code-review feedbacks* Fix linting* Fix the order of tutorial.* Add exception handling. Address feedbacks.* Fix CI error -- clearing instruments in global pass_ctx* Clarify section hierachy.* Emphasize to use decorator instead of subclassing* Add a sentence to explain Pass Instrument. Fix typo.* Shrink python docs a little.* Fix tag name.* Address feedbacks.",0
"Revert ""Actually add Compute Library tests to the Jenkins File (#8394)"" (#8400)",1
"Refactor the compile engine into a cleaner interface. (#7518)Duplicate the CompileEngine interface.Refactor the graph_runtime_codegen to invoke the new LowerTE passMore changesThings appear to be workingSome tracing to get Relay code to flow through too.Disable some assertions as exp.Tweak printing for nowFix a few bugs: (#13)1. Don't add relay main function to list of lowered TIR functions2. Don't skip visiting call to relay function in graph runtime codegenRemove debug prints.Start refactoringSplit out shared data structuresFix implicit duplicate decl of IsDynamicClean up handling of name + global prim fnClean up the code and debug issue introduced by previous hackClean up the debuggingDo C++ lint clean upUpdate src/relay/backend/graph_executor_codegen.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Clean up handling of external functionsAdd more error messagesMore clean upUpdate src/runtime/graph_executor/graph_executor.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Update src/runtime/graph_executor/graph_executor.ccCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Update src/relay/backend/te_compiler.hCo-authored-by: Haichen Shen <shenhaichen@gmail.com>Update src/relay/backend/te_compiler.hCo-authored-by: Haichen Shen <shenhaichen@gmail.com>FixCRMore CRFormatFix lowering path for C++Fix testsRemove uncessary changeClean up a few more thingsCI fixFix the default contextFixFix broken test casesUpdateFixWIPClean up storage data structuresWIPWIPFix build errorsRemove TVMLowerFix lintLint againfix blackMove UpdateMainWorkspaceSize into te_compiler.ccFix link errorsFormattingChange UpdateMainWorkspaceSize to return Map<String, FunctionInfo>Workaround for GCC 5 error caused by enums in maps (GCC 5 is on i386 CI)Testing how functions should be namedLintChange how function metadata is updatedAttempt to update aot_executor_codegen to use new StaticMemoryPlan instead of storage_device_mapPass memory plan through LowerTE into UpdateMainWorkspaceSize so that we don't need to run GraphPlanMemory an extra timeFix return in UpdateMainWorkspaceSizeLintTry to fix UpdateMainWorkspaceSizeFix construction of static memory planClean up code while debuggingAdding UpdateWorkspaceSize backAdd closure + call to UpdateFunctionMetadata (WIP)UpdateFunctionMetadata builds; weird error with device ctx map though. Not sure if it came from this change or something elseAdd some debugging of UpdateMainWorkspaceSizeStarting to move UpdateFunctionMetadata call to use process_fn infraUWhat target should be passed to UpdateFunctionMetadata?UpdateFunctionMetadata is not workingggAdded some comments about UpdateFunctionMetadata for JaredFix the creation of function metadataTry another stab at cleaning up the informationFixPort StorageInfo and StaticMemoryPlan data structure (#8297)Restoring reshape optFix testsCaught a nasty typo from Lily, Map::Set does not mutateFormatDisable stupid Google style warningRebase cleanupFormattingAdd docstring for storage infoBlackPost rebase fixRemove printsDisable assert that doesn't make sense for nowFix lintAdd copying attrs from relay node to graph node; still need to figure out how to do this in the case of global varsWork with Lily to fix graph attrsTry to figure out where extra arguments are coming from; fix mergepasses the profiling testClean upFix profile testRemove debuggingAdd attributes for BYOC uTVM caseFormatDumb typoAnother fix for byocFormatFix last 3 failing testsFormatFix final two test casesFormatFix lintFix againFixFix auto scheduler codeFix issueAddress CR commentFormatCo-authored-by: Jared Roesch <roeschinc@gmail.com>",0
"[Relay] Fix index order in conv2d computation for Arm CPU. (#8361)When dilation is larger than value 1 in conv2d with NHWClayout, the ordering of indexes when accessing data arrayin computation of convolution appears to be incorrect.'data_vec' is defined aslambda n, oho, owo, kh, kw, ic, ohi, owi:But accessed asdata_vec[n, oho, owo, kh, kw, ohi, owi, ic]This patch fixes the order of indexes and modifies the testso that it is suitable for running on an AArch64 CPU.",0
[microTVM] Add fixture to zephyr test (#8393)* fix testing* trigger,0
[Relay] Add support of conv2d with NHWC for Mali (#8422)* [Relay] Add support of conv2d with NHWC for MaliAdded template schedule for conv2d NHWC reusing similar strategyas for NCHW layout. The schedule is also added to thecorresponding test that can be run to verify correctness.* [Relay] Fix issue from pylint in conv2d for Mali,0
"[PyLint] Minor updates to pass pylint locally. (#8424)With either the ci_lint docker image, or the matched version ofpylint==2.4.4, I got two lint errors running locally that didn't showup in the CI.  Fixing them.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[Frontend] Check LLVM enabled/installed (#8414)-Some ops(ex:view) call infer_value when converting a model into Relay IR.-If LLVM is not enabled, it leads to segementation fault.Co-authored-by: kueitang <kueitang@qti.qualcomm.com>",2
[Bug] Fix x86 dense schedule extern ops (#8420)* [Bug] Fix x86 dense schedule extern ops* more* lint,0
[Doc] Fix Relay pattern rewrite (#8425),0
[CUDA] dense_tensorcore/batch_matmul_tensorcore support int8/int4 (#8402)* add int8/int tensorcore for dense/batch_matmul* fix bug* fix lint* Apply suggestions from code reviewCo-authored-by: Chenfan <jcf94@outlook.com>* fix for reviewer* fix lintCo-authored-by: Chenfan <jcf94@outlook.com>,0
[Arith] Simplify MatchFusePattern in InverseAffineMap (#8427)* [Arith] Simplify MatchFusePattern in InverseAffineMap* fix,0
[TOPI] Bugfix for topi.prod (#8416),0
Improve XGBTuner document (#8428)* chore: improve xgboost_tuner docstring* chore: remove whitespaceCo-authored-by: Siwa <siboon@sertiscorp.com>,2
[TVMSCRIPT] TVMScript Parser support BufferSlice indices (#8408)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,5
"Replace RuntimeError in _lookup_task with deferred error. (#8421)* Replace RuntimeError in _lookup_task with deferred error.This allows unknown tasks to be created (e.g., when parsingautotvm log files) but not invoked.* Format.* Update python/tvm/autotvm/task/task.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Matt Welsh <mdw@mdw.la>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
fix flaky TF test (#8431),0
[microTVM][RVM] Fix clock skew on virtualbox (#8395)* fix virtualbox clock skew* address comment* trigger,0
[Relay] Add support of conv2d with NHWC for Bifrost (#8430)Reuse generic Mali strategy for conv2d with NHWC inBifrost target.,1
fix wrong log of tir pass VerifyMemory (#8445),0
"[Relay to onnx conversion fixes][Pool, Pad] (#8435)* [Relay to Onnx conversion][Pool]* added missing ceil_mode in average pool and max pool conversion* [Relay to Onnx conversion][Pad]* Fixed issue in Pad conversion: changed pad_value to input instead of attrs* Refer to PR: https://github.com/apache/tvm/pull/7860* Updated unit test for Pad* Fixed some formatting errors",0
[Relay to onnx conversion][New ops] (#8436)* [Relay to Onnx conversion]* added support for Sigmoid op* added unit test* [Relay to Onnx conversion][Copy]* added support for Copy op* added unit test* [Relay to Onnx conversion][Round]* added support for Round op* added unit test* [Relay to Onnx conversion][Cast]* added support for Cast op* added unit test* [Relay to Onnx testing]* fixed formatting* * fixed formatting issues* * fixed formatting issue in onnx.py* [Relay to Onnx conversion][Conv2d Transpose]* Added support for conv2d transpose operator* Added unit test case. Unit test is similar to the conv2d unit test.* * Fixed formatting errors,0
[ROCM] Fix undefined symbols by adding library (#8446)* Add libhsa-runtime64 reference.* Remove lib in library definition.,0
Fix address and port reported by android_rpc to tracker (#8405),0
[Bugfix] Fix broadcast type func with incomplete type (#8438)* [Bug] Fix broadcast type func with incomplete type* fix,0
[COMMUNITY] @junrushao1994 -> PMC (#8450),3
[MyPy] Extend type checking and annotation for TIR (#8429),5
"[BugFix][TOPI] Fix the integer overflow problem of the scatter_nd op. (#8415)* Fix the integer overflow problem of the scatter_nd op.* Fix scatter_nd's crash problem:1. Existing scatter_nd cuda implementation has a very large bound,   which could overflow int32 range when input tensor shape is   large enough;2. The overflow could cause the if statement always evaluate to   true, thus conducts invalid memory accesses;3. We fix this problem in this commit by reducing the bound, the   original large bound is not only unnecessary, but also degrading   the performance; With this fix, scatter_op's performance improves   100x on some cases.Co-authored-by: wenxizhu <wenxizhu@tencent.com>",0
Add qnn batch_matmul operator (#8401)* Add qnn batch_matmul operator- add support of the different out type for x86 batch_matmul* Fix code style* Add out_dtype to generic batch_matmul* Restore fixe in batch_matmul for dynamic shapes* Fix documentation for qnn.batch_matmul* Remove debug code* Modify zero point for qnn batch_matmul test,0
[microTVM] Fix Stack Size Issue for Zephyr AOT Demo on Physical Hardware (#8453)* increase size* add --unpacked-api=1 option* format,0
[Relay] Modify create_executor to pass params (#8418)* Overload create_executor to accept params* [fix] Add stringdoc for new param in create_executor,0
"[PROFILING] Use PAPI to collect hardware performance counters on CPU and CUDA (#7983)* [PROFILING] Use PAPI to collect hardware performance counters on CPU and CUDAThis PR adds an optional dependency on PAPI(https://bitbucket.org/icl/papi/) in order to collect hardwareperformance counters on CPU and CUDA. These performance counters includedata like total cycles, instructions executed, and cache misses. Userscan control which performance counters are collected by setting theTVM_PAPI_${DEVICE}_METRICS environment variable to a semicolon separatedlist of metrics.* Update CMakeLists.txtCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* move thread pool reset out of crt* add docs* comments* formatting* forgot one doc* kDLGPU -> kDLCUDA* Refactor API to more closely match pass instrument's.* forgot files* formatting* more lint* fix docs* optional loading of papi metric collector in python* more formatting* fix check* update docs and default value* formatting* addressing andrews comments* fix docs* address comments* move shared initialization code into private function* move most definitions from papi header to implementation fileCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>",0
[Relay][ONNX] Batch_matmul to dense optimization (#8440)* [ONNX]Add batch_matmul to dense optimization* Add extra check to avoid unnecessary reshapeCo-authored-by: Ubuntu <ubuntu@ip-172-31-14-16.us-west-2.compute.internal>,1
[TOPI] Add support for arbitrary dtypes to CSRMV and CSRMM (#8437),1
"[Refactor] Enforce attaching storage scope to PointerType (#8366)* Add storage scope to ProducerRealize, always create a buffer with scope* update schedule_ops.cc* update schedule_postproc_to_primfunc.cc* restore more realize_scopeThis reverts commit b66c3baa54feeb8e34016713a1be21802b3296bf.* make the default scope be """" instead of None in ir builder* restore realize_scope visit in storage_flatten.cc* update storage_access.cc* make sure buffer var is of PointerType in ir builderThis reverts commit e650b6c24cabd52a073064e51c2e4fee816e88fd.* enforce default storage scope of global* added remap pass but does not work yet* fixed all reduce issueThis reverts commit 8e20003c5325085ed22ee57180aca18644b3b5ab.* simplify* trying mitigation for aot test* merge remaining changes from initial branch* remove use of attr::storage_scope from codegen* restore a visit to AttrStmt with attr::storage_scope in storage_rewrite* disable check* lint fix* revert default scope to """"* format* fix volatile access to shared mem in lower all reduce* fixed gpu coorporative load/store test* pass storage scope to PointerType in tvm script parserThis reverts commit 99cfb9d18781dcfdea169d920450f9063ab18b6b.* fixed tvmscript roundtrip test* fixed tir flatten buffer test* fixed test_tir_transform_hoist_if.py* use storage scope global by default in aot_executor_codegen.cc* add missing default storage scope in create_primfunc.cc* restore StorageInfo struct in llvm backend* UpdateStorageScope -> WithStorageScope* fixed lower warp memory test* GetStorageScope -> GetPtrStorageScope* Enable storage scope invariant check in AttrStmt constructor* remove GetPtrStorageScope and WithStorageScope from public header* move RemapStorageScope to its own file* add more method to RemapStorageScope* update lower_thread_allreduce to use RemapStorageScope* RemapStorageScope -> UpdatePointerStorageScope* remove realize_scope from hybrid script* removed realize_scope in schedule_ops* remove realize_scope from schedule_postproc_to_primfunc* remove remaining realize_scope usage from schedule_ops.cc* remove realize_scope usage from storage_flatten.cc* fixed test_tir_transform_lower_warp_memory.py following realize_scope removal* Add storage scope to ProducerRealize, always create a buffer with scope* update schedule_ops.cc* update schedule_postproc_to_primfunc.cc* restore more realize_scopeThis reverts commit b66c3baa54feeb8e34016713a1be21802b3296bf.* make the default scope be """" instead of None in ir builder* restore realize_scope visit in storage_flatten.cc* update storage_access.cc* make sure buffer var is of PointerType in ir builderThis reverts commit e650b6c24cabd52a073064e51c2e4fee816e88fd.* enforce default storage scope of global* added remap pass but does not work yet* fixed all reduce issueThis reverts commit 8e20003c5325085ed22ee57180aca18644b3b5ab.* simplify* trying mitigation for aot test* merge remaining changes from initial branch* remove use of attr::storage_scope from codegen* restore a visit to AttrStmt with attr::storage_scope in storage_rewrite* disable check* lint fix* revert default scope to """"* format* fix volatile access to shared mem in lower all reduce* fixed gpu coorporative load/store test* pass storage scope to PointerType in tvm script parserThis reverts commit 99cfb9d18781dcfdea169d920450f9063ab18b6b.* fixed tvmscript roundtrip test* fixed tir flatten buffer test* fixed test_tir_transform_hoist_if.py* use storage scope global by default in aot_executor_codegen.cc* add missing default storage scope in create_primfunc.cc* restore StorageInfo struct in llvm backend* UpdateStorageScope -> WithStorageScope* fixed lower warp memory test* GetStorageScope -> GetPtrStorageScope* Enable storage scope invariant check in AttrStmt constructor* remove GetPtrStorageScope and WithStorageScope from public header* move RemapStorageScope to its own file* add more method to RemapStorageScope* update lower_thread_allreduce to use RemapStorageScope* RemapStorageScope -> UpdatePointerStorageScope* remove realize_scope from hybrid script* removed realize_scope in schedule_ops* remove realize_scope from schedule_postproc_to_primfunc* remove remaining realize_scope usage from schedule_ops.cc* remove realize_scope usage from storage_flatten.cc* fixed test_tir_transform_lower_warp_memory.py following realize_scope removal* Address comments* Remove blank line diffCo-authored-by: Masahiro Masuda <masahi@129@gmail.com>Co-authored-by: masa <masa@pop-os.localdomain>",0
[Fix] Explicitly retain `__hash__` of `StringImm` (#8449)* Fix missing `__hash__` of `StringImm`* Make __hash__ a methodCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[CUDA] Improve injective schedule to enable half2 (#8457)* [CUDA] Improve injective schedule to enable half2* lint* fix* trigger ci,0
[Fix] Remove unused variable in GraphExecutorCodegen (#8465),0
"[Docs] Corrected typo in googletest build instructions. (#8459)Incorrectly specified ""MAKE_SHARED_LIBS"" option instead of""BUILD_SHARED_LIBS"".  In future, perhaps googletest should be pulledinto 3rdparty submodules to minimize the setup of a dev environment.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
"[RPC] Fix cpp_rpc connection to rpc_tracker (#8388)* [RPC] Fix cpp_rpc connection to rpc_trackerFixed connection cpp_rpc application to rpc_tracker which was broken bythis commit: 0bbaf0e.Also, made it possible to create a linux shared library on MacOS.Without this change default tuning didn't work on MacOS.* Add explicitly check on dylib",0
[Relay][Frontend][ONNX] Add ConvInteger support. (#8456)* Add ConvInteger support and fix some ConvTranspose padding bugs.* Simplify pads check.* Fix style.* Remove changes to conv_transpose.,0
[COMMUNITY] comaniac added as new PMC member (#8470),1
"[UnitTests] Minor fixes to unit tests for cudnn/vulkan targets (#8462)- Marked tests as @requires_cudnn to avoid failure on platforms  without cudnn.- Replaced target ""vulkan"" with ""vulkan -from_device=0""Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Codegen] Remove compile_enginer header (#8471)* [Codegen] remove compile_enginer header* fix lint,0
[Relay][Onnx][Frontend] Add RandomUniform converter and tests to onnx frontend. (#8426)* Add RandomUniform converter and tests to onnx frontend.* Fix comments.* Remove weird import.* Add test against golden array.* Retrigger CI* Improve test comment.* Retrigger CI.,0
[AMP] Add default op attribute registration to __init__.py (#8460)* add attribute registration to init* blackify* remove unused improt* jostle ci* avoid circular import* change order to match orig* other thingsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
Fix auto-scheduling after 9c6658721 (#8478),0
[Relay][Frontend][ONNX] Allow importing models with malformed Loop nodes. (#8475)* Snapshot* Undo comments.* Add testing for malformed loop nodes.* Format oops.,1
"FoldScaleAxis became non-recursive (#8325)* FoldScaleAxis became non-recursiveFoldScaleAxis moved from ExprVisitor and ExprMutatorto non-recursive MixedModeVisitor and MixedModeMutator.The specific transforming part itself is still recursive,however the underlying traversal machinery is non-recursive.Change-Id: I8bf40bd1f3f039ef0705c665a34a4624067048a1* Added extra empty lines as requestedChange-Id: I242ec95f92b3dfc7fa3dd89385f56ab07c6e72a8",1
DeviceType enums match dlpack (#8407)* DeviceType enums match dlpack* intentionally left blank,5
fix typo (#8484)* fix typo* Fixed typos in documentation,0
[TVMC][FIX] Compiler supports input with a slash (#8481),0
fix minor misspelling (#8476)Co-authored-by: Masahiro Hiramori <mshr-h@users.noreply.github.com>,0
[VM] Fix the shape function of conv nhwc (#8480)* Add dynamic support for conv2d nhwc,0
[BYOC] add multi functions support in partition pass (#8464)* add support for multi function* address commits and fix lint* fix testcases and using a set to avoid duplicate func name* fix lint,0
Fix _get_yolo_detections (#8477),0
"apps: microtvm: Disable `CONFIG_FPU ` for Zephyr runtime (#8055)`CONFIG_FPU` was being enabled by default for every platform,regardless of whether or not the platform using the sample app actuallyhad a HW FPU unit. As a result, FPU instructions may be included onplatforms that aren't able to support them, or in a best-case scenariowe will get a warning about the conflict during builds, which pollutesthe CI output, in a worst-case scenario a fault.This change removes the `CONFIG_FPU=y` setting from being set at theapplication level, since this flag should be set at the chip level forany platform that has an FPU.Signed-off-by: Kevin Townsend <kevin.townsend@linaro.org>",2
[TVMSCRIPT] Support tir.abs node in tvm script (#8488)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,5
[Frontend][Tensorflow2] Stridedslice and concat_v2 fix (#8483)* fix for strided_slice when begin > end in case of shrinkaxis_mask* fix for name_hint missing error for concat_v2 op* removing a local fix* adding more testing capability to concat_v2,0
[VM] Allow serialization of function attrs which are strings (#8485)* [VM] Allow serialization of function attrs which are strings* add test,1
[AutoTVM] Re-enable `ref_input` (#8113)* [AutoTVM] Re-enable ref_input* add ref_input on measure_option* add ref_input unittest* fix: test reformat* [autotvm] [ref-input] refine test and description* [autotvm] [ref-input] revert arg on measure_option,0
[Bugfix] [tir] do not simplify 'Any() - Any()' to 0 (#8266)* fix* fix lint* remove* address comments,0
"Switch from CompileEngine to TECompiler in Interpreter (#8486)This continues on:https://discuss.tvm.apache.org/t/rfc-relay-tecompiler-rewrite-existing-compile-engine-to-match-updated-compiler-flow/9233and #751, this time just replacing CompileEngine with TECompiler in the Interpreter,using the JIT helper added to help the transition.Some whitespace improvements while there.",1
Fix dynamic batching when use_implicit_batch=False (#8461),0
[ARITH] fix zero iter bug in arith (#8494)* fix* black,0
Add missing shape functions for relay.nn operations (#8489)* Update _nn.pyadd a few missing shape functions* Update _nn.pyUpdated conv_transpose shape function to accomodate conv1d_transpose* added tests for new functions* fixed a lint error* fixed shape func error* attempt fixing cuda error,0
src/runtime/module.cc (#8496),5
"Update Docker CI (#8193)* add failing onnx tets* point jenkins at new docker* support convtranspose opset 11 autopadding* Don't force output shape for conv transpose tests, add 1D and 3D cases* disable test until CI update complete* try updating docker images again* skip a test until update complete* next try at docker images* manage TF memory use in TF1 tests* support explicit padding for NCHW TF padding test* Update to tagged tlcpack imagesThanks, Andrew!Co-authored-by: Andrew Reusch <areusch@gmail.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>",1
[ETHOS-N] Re-enabled tests and updated module hashes (#8498),1
"Keep CODEOWNERS file up to date. (#8500)* Keep CODEOWNERS file up to date.The CODEOWNERS file was used as a mechanism to mark committers' areaof expertises and faciliate the review process. This PR attempts tobring its state to up to date. This is of course non-comprehensive,but can serve as a starting pt to help us to find the right personto shepherd the PRs.* Update .github/CODEOWNERSCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update CODEOWNERS* Update .github/CODEOWNERSCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update .github/CODEOWNERSCo-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Chenfan <jcf94@outlook.com>",1
"[Frontend, pytorch] Vc/pytorch lstm (#8447)* lstm layer conversion to relay from pytorch model (TorchScript) was supported* bidirectional LSTM layer was supported for pytorch API* lstm tests were implemented. fixes in pytorch lstm* fix pytorch bidirectional lstm. update test comment* black format and some small fixes* LSTM with projection was supported for pytorch frontend. test was updated by new combination of LSTM types* lint fixes* add bias switcher for LSTM types test. fix LSTM implementation in pytorch frontend for case without biases. exception in the test for conversion LSTM with projection from pytorch to ONNX* transfer test_lstms to pytest format* onnx model saving was implemented through io.BytesIO. creating/removing tmp dir was removed. remove unneccessary comments* gpu target was added to the testCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>",0
"Rename runtime-config to executor-config and add documentation for Model Library Format (#8270)* Rename runtime-config to executor-config.* Add documentation.* address comments, make tests pass* fix unit test* fix sphinx doc errors* address manupa comments",0
"Fix 8093, Enhance Buffer Index Simplify (#8204)",0
[Refactor] Remove scope attribute from Buffer class (#8463)Co-authored-by: masa <masa@pop-os.localdomain>,4
Enable ONNX tests that needed onnxruntime 1.7.0 (#8502),3
Organize the CodeOwners file: (#8512)- Order by depth first- Always show the complete prefix,0
[TIR] Bugfix for zero number arguments tir functions. (#8515)* [TIR] Bugfix for zero number arguments tir functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
"[Relay] Fix bug in test_op_level3 (#8508)* [Relay] Fix bug in test_op_level3Test case failed due to missing mode=""add""* Empty",0
"[TensorIR][M2a] Fuse, Split (#8467)* Fuse&split (#408)Co-authored-by: jinhongyi <323195289@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>",5
"[Relay] Support resize in the ONNX conversion (#8455)* [Relay to Onnx]* Added support for resize2d op* Added unit test* [Relay to Onnx][Resize]* Fixed formatting errors* [Relay to Onnx][Resize]* Fixed issue in resize conversion: round maps to round_preferc_ceil* Updated resize unit test to test for coordinate transform mode andround* Known issue: Does not match for (NN, align_corners) and Cubic* * Fixed formatting errors* * Fixed some more formatting errors",0
update qemu install (#8518),1
"[Topi][UnitTests] Parameterize conv2d and depthwise_conv2d tests (#8433)* [UnitTests][Topi] Updated test_topi_conv2d_nchw.py to have parametrized tests.- Better error messages, displays which workloads/targets failed and why.- Fixed bug in topi.nn.conv2d._get_workload exposed by the  parametrized tests.  Incorrect padding if the ""SAME"" parameter is  used with dilation>1.- Fixed bug in tvm.topi.x86.group_conv2d._get_default_config, missing  dilation parameter in call to _get_conv2d_workload.* [UnitTests][Topi] Parametrized the tests in test_topi_depthwise_conv2d.pyIn preparation for parametrizing to test on float16 as well.- Single test_conv2d test with parameters for layout/input sizes.- Extended the support for NCHWc layouts, so that they could be  included in the parametrization.  (Implemented  topi.testing.depthwise_conv2d_python_nchwc and  topi.nn.scale_shift_nchwc, added layout argument to  topi.nn.depthwise_conv2d._get_workload).Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[CUDA] Initial support for dynamic shared memory (#8466)* send dyn shmem size to runtime* add dyn shared storage scope* associate buffer var and its storage scoe in split_host_device* tried NVPTX but failed with INVALID_PTX error* test stub* dynamic shmem reduce working* log2 issue fixed* nvptx working* refactor llvm shmem allocation* make linkage argument* support rocm too* send dyn shmem param to hip runtime* remove alloc map from split_host_device.cc* remove attr::storage_scope from split_host_device* lint fix* formatting* update calling convention doc* minor update to test* remove log* remove kDynShared, dyn.shared -> shared.dyn* support backward compat* update json/binary reader/writer* thread_axis_tags -> launch_param_tags* ThreadAxisConfig -> LaunchParamConfig* remove use_dynamic_shared_memory from FunctionInfo meta data* revert change in test_tir_ir_builder.py* make sure kUseDynamicSharedMemoryTag is the last tag* remove continue* update doc string following name change* more comment update following name changeCo-authored-by: masa <masa@pop-os.localdomain>Co-authored-by: Masahiro Masuda <masahi@129@gmail.com>",0
[microTVM][Cortex-R5] Add zephyr cortex-r5 board to Zephyr  (#8519)* cortex r5 added* add aot demo,1
[TVMSCRIPT]Fix script printters StructuralEqual check failed (#8499)Co-authored-by: honghua.cao <honghua.cao@streamcomputing.com>,0
[PROFILING] Add json output to profiling reports (#8503)* [PROFILING] Add json output to profiling reports* format json in comments,1
[PRINTER] Fix the repeatitive cast in scripr printing (#8531),0
"[Frontend, Tensorflow2] Added support for TensorList ops (#8454)",1
"[CMake] Split out libinfo.cc into a separate target. (#8520)Every `*.o` file in the cmake-generated makefiles have a dependency onthe target's `flags.make` file.  The `flags.make` file contains thecompiler flags for all objects in a target, not just the `*.o` filecurrently being compiled.  As a result, even though `libinfo.cc` isthe only file that has the `TVM_GIT_COMMIT_TIME` and`TVM_GIT_COMMIT_HASH` definitions, every file in the `tvm_objs` targetwas recompiled whenever the commit id was changed.By splitting `libinfo.cc` out into a separate target, no other filesneed to be recompiled when committing or changing branches, unlessthere are actual changes to the file.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",2
[RUNTIME] Fix TypeKey2Index when for root Object (#8547)* [RUNTIME] Fix TypeKey2Index when for root Object* Temp skip tsim tests,0
"[TFLite] Mimic the TFLite's 2.4 reader's behaviour (#8538)In TFLite 2.4, the builtin code value can be either in""deprecated_builtin_code"" field or ""builtin_code"" field (as longas the value is less than 127) and similarly to the TFLite'sreader, we should use the higher value of the two.Change-Id: I0d738f9257187903b4c5b4cc5a8733a451ddc02e",4
Remove unused variable in topi cpp test (#8549)* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Fix docstrings in tvm.relay.cast_like* use correct formats to avoid chaos* Remove unused variable in topi cpp test,0
[RPC] Add explicit type cast to print. (#8524),1
[Bugfix] Visit each input param of the function in ExprVisitor visit_function (#8521),0
[FFI] Specifically check handle for recursion during shutdown (#8548)NOTE: previously slot may get overriden by child class and itis better to directly check for handle here.,5
"Add a `--context-path` for build.sh, allowing to test Dockerfiles (#8557)in different directories, and still get relative paths to work.* Add new option --context-path to build.sh* Keeps the default as the `dirname` of the Dockerfile, so  no changes expected in the current behaviour",1
Fix #8510 (#8511),0
[Frontend][TENSORFLOW] Add support for unpack with dim 0 after tensorlist stack (#8558)* enable testcase when tensorlist stack follows by a unpack for dim 0* address reviews and improve the docstring,1
[Bugfix] fix android rpc app undefined reference problem (#8530),0
"[TensorRT, BYOC] Handling a corner case in TRT RemoveDropout pass (#8506)* [TensorRT, BYOC] Handling a corner case in TRT RemoveDropout pass* changing visit logic",2
[BUGFIX] fix illegal memory access bug in reduce op schedule by constriant threadIdx.y (#8566)Signed-off-by: ziqiang.pzq <ziqiang.pzq@alibaba-inc.com>Co-authored-by: ziqiang.pzq <ziqiang.pzq@alibaba-inc.com>,0
Re-enable Compute library tests. (#8573)ci image v0.06 does not appear to have the flakiness shown in ci image v0.05.However what changed between the 2 remains a mystery and needs furtherdebugging. However for now re-enable this to see how this fares in CIFixes #8417,0
[Test] Fix AutoScheduler test to cover Conv2D Winograd (#8539)* optimize resize vertor* tmp* DoMultiLevelTiling* modify size_t to int* modify* modify level fill* Update utils.ccmodify format_lower* format lower count* delete blank lines* delete blank lines* re-commit message* Update graph_executor.hadd set_output_zero_copy* add setoutputzero* add set output zero* Update graph_executor.cc* Update graph_executor.h* delete const_cast* add common function chechDltensor* Update graph_executor.h* Update graph_executor.cc* add output_ sort* Update graph_executor.cc* add a.nodeid == b.nodeid* add unit test for set output zero* add include <algorithm>* modify Setoutput zero copy* modify by clang-format* add unit test for set output zero* rrealy ut go back* rrealy ut go back* modify input->output* delete sort output input* modify build_module_test.cc* midify winograd UT* re-pr* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit* empty commit,0
fixenhance robustness of DefuseOps (#8564),0
"[Contrib] Added default non-verbose to download_testdata(), pass to download() (#8533)* [Contrib] Added default non-verbose to download_testdata(), pass to download().Minor cleanup as well, while in the file- Using tempfile.TemporaryDirectory instead of explicit cleanup.- Pass through verbose/retries arguments if replacing a corrupted  copy.* [Contrib] Switched download.py from print statements to logging* [Contrib] Added shutil.copy2 fallback after downloading file.Initial implementation using tempfile.TemporaryDirectory assumed thatthe tempdir and output location were on the same drive, and could berenamed.  This update falls back to copying from the temporarydirectory, in case the tempdir is on a different drive.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Coreml] Fix Coreml Input Shape Handling (#8562)* convert ot python list like expected* test example* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,0
[AutoScheduler] Fix task extraction with TE compiler (#8560)* [AutoScheduler] Fix task extraction with TE compiler* fix* test* Update python/tvm/auto_scheduler/relay_integration.py,0
add support for softmax and log_softmax with MIOpen (#8543),1
[TOPI][CUDA] minor change on assert statement in conv2d_NCHWc_int8.cuda (#8554)* [TOPI][CUDA] minor change on assert statement* [TOPI][CUDA] reformatting,4
"[TOPI] Fix `nn.pool*d` issue with 'vectorize' function and add unit tests (#8541)* Fix issue in 'vectorize' function for 1D and 3D tensors* Add pooling tests for channel last layouts* Add support for more general layouts in ""poolnd"" implementation* Reformat with 'black'* Fix lint issues",0
[Bugfix] Fix #8536 Get Target When Heterogeneous Execution (#8537),0
[Bugfux] wasm32-standalone app repaired (#8563),0
Disable pip cache when creating Docker images (#8575)* This is a good practice to save storage space in  the Docker images being created* Also sort pip package lists alphabetically,5
[TOPI] Add transpose_a/b & dynamic shape support for batch matmul (#8527)* Add basic support for batch matmul transpose* Update* Lint fix & add tf convert support* UpdateLint fix* Bug fix for qnn.batch_matmul* Bug fix for tensorflow test* Add grad support for batch_matmul* Lint fixRe-triggle CIBug fixRe-triggle CIRe-triggle CIRe-triggle CI,0
[Bugfix] Preserve IRModule type definition and imports in NameMangleExtFuncs (#8523)* bug fix and add tensorarray with partition pass test case* change test function location and address comments* Update tests/python/relay/test_pass_partition_graph.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* trigger CICo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[TIR] cast disparate floating point types for binary ops (#8517)* handle upcasting case* test upcasting tests for tir* address comaniac comments* formatting* add negative tests* fix failing test now allow other thingsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,0
[Refactor] Remove AttrStmt with storage_scope key (#8516)* Remove all attr::storage_scope usage* pyformat* fixed VTA tests* Update TIR text printer to print storage_scope on allocate* print storage scope in AllocateNode ReprPrinter* Fixed accidently removed scope tag check* remove unused functionCo-authored-by: masa <masa@pop-os.localdomain>,0
[VM] Bug fix for numpy scalar input in vm (#8553)* Bug fix for numpy scalar input in vm* Bug fix* Re-triggle CI* Update* Update UT* Re-triggle CI,0
[Torch] Reduce testing time of LSTM tests (#8583)* reduce testing time* lint issues were resolved. weights for test are always randomly generatedCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,3
"[FIX][CI] hotfix check_grad perf regression (#8581)* hotfix check_grad perf regression: lift compile out of hot loop* hoist interpreter creation out of python closure, fix weird conv2d bug on arm cpu* lint* try one more fix",0
[Vulkan] Prioritize discrete GPUs as device_id=0. (#8588)- Added device_type to the device-queried information.- Sort the vulkan devices by the device_type.  Priority is discrete >  integrated > virtual > cpu > other.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>,1
speed up reference resize kernel (#8592),5
"Delete pytest-results as part of CI workspace preparation. (#8594)* Otherwise, stale pytest-results could appear in builds.",3
use sizevar when convert any to tir (#8555),5
Fix storage_access not visiting else branch (#8525)* Fix storage_access not visiting else branch* fix conflict with #8516 in the test* update thread sync test following #8516 update,0
[AOT][Stack Allocator] Fix Initial Memory Misalignment (#8487)* add flag* fix and test* format* fix memory memory_align function* fix and address comments* format* fix crt aot test* comments* fix test* trigger* trigger* trigger* trigger* triggerCo-authored-by: Mehrdad Hessar <mhessar@ip-172-31-20-199.us-west-2.compute.internal>,0
"[Vulkan] Rewrote PointerValueTypeRewrite transform (#8528)* [Vulkan] Rewrote PointerValueTypeRewrite transformIn C-style codegen, pointer types can be freely cast between scalarand vectorized types (e.g. `float16x4* <-> float16*`).  In SPIR-V,these are separate types, and no such casting is allowed.  This waspreviously handled by having a special-case for `Ramp(base, stride=1,lanes)` in the codegen.  That method didn't cover all possible cases,including Broadcast nodes used as indices.PointerValueTypeRewrite previously re-wrote the AllocateNode andparameter pointer types, but didn't update the Load/Store node.  Thischange tracks which variables can be updated to a vectorized type, andthen updates all references to those.  This includes removing the`RampNode`, as the vectorization is then included as part of thevariable type.* [StorageRewrite] Updates as recommended in review.- Added explicit TODO(Lunderberg) for follow-ups- Pass `checker.info_map_` instead of `checker` to  `VectorTypeRewriter`* [Vulkan] Allow for pointer rewrites that change base type.A single memory allocation may have more than one type of data storedwithin it.  This allows the PointerTypeRewrite pass to recognize if afunction only uses the pointer as a particular base type.  This wasn'tan issue in C-based codegen, but is required for Vulkan.  Since Vulkanshaders do not permit type-casting, the cast must be done when passingthe pointer argument into the shader.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[TensorIR][M2a] Reduction Factoring (RFactor) (#8544)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[TensorIR] Support for match_buffer from subregion (#8585)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[VTA] Recover rpc server support (#8604),5
[Refactor] Unify the shared pass prefix between vm and graph (#8526),0
[CUDA] Support multiple TIR-level dynamic shared memory allocations (#8571),5
"[TOPI][CUDA] Improve the performance of scatter_nd (#8479)* [TOPI][CUDA] Improve the performance of scatter_nd by:1. Split into 2 kernels, one does the ""Init"" and another does the ""Update"".   Thus they can have different Grid/Block configurations to better utilize   SMs.2. Use atomic_add instead of direct assignment, which could avoid the race   condtion when multiple indices point to the same location of the output   tensor. With this moidification, it's safe now to use more CUDA threads   to gain more parallelism.* Fix python code format.* FIX: [TOPI][CUDA] Improve the performance of scatter_nd #8479- Split ScatterND kernel into 2 sub-kernels using ib.new_scope()- Replace ib.for_range() with blockIdx.y- Using atomic_add when mode == ""add""- Keep threadIdx.x less than max_threads of GPU* Comment added* Add fallback implementation when ""mode=add"" meets int64- Atomic_add from CUDA doesn't support int64 data type- Change ""ind{i}"" to ""ind%d""%i, where names of relay.var could correctly display* Python format* Fix line too long* CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Empty, for CI pass* Exchange blockIdx.x and blockIdx.y* check for Vulkan or metal* Fallback to previous algorithm when mode==update* Update python/tvm/topi/cuda/scatter.pyCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>* Assign TODO* Swapping then and else blockCo-authored-by: wenxizhu <wenxizhu@tencent.com>Co-authored-by: CaptainDuke <captainduke328@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>",0
[BUILD] Add caching to CMake (#8373)* ccache* ccacheFix formattingAdd comment about nvccChange default to AUTOMore progressAdd auto as a modeDisable ccache in CIadd-cache-to-cmakeFix typo* Fix rebase* flaky test,0
[Meta Schedule][M3a] Instruction and Trace (#8615),2
"Add support for AOT in external code generation tests (#8591)This adds support for the external code generation tests to use AOT. Aspart of this the existing logic in check_result was split out intomultiple functions, this allows selectively disabling those that aren'tsupported such as JSON outputs not being supported in AOT. I've replacedexisting checks to skip tests with @pytest.mark.skipif macros as they'vebeen moved out of the `check_result` function.",1
[CI] Fix global pip cache disable change (#8590)* The fix to disable cache needs to run after pip is installed* This is quick follow up fix after #8575,0
[DOCS] Fix scipy docs inv (#8619),0
[runtime] Remove unused parameter. (#8580)* [runtime] Remove unused parameter.* fix build issue when TVM_CRT_DEBUG enabled,0
"Introduce --interface-api={c,packed} parameter (#8280)* Introduce --interface-api={c,packed} parameterThis introduces structures generated to provide a documented and stable userfriendly interface to a TVM generated model, as can be seen in the AOTdemo application:```struct tvmgen_default_inputs inputs = {  .input_1 = input_data,};struct tvmgen_default_outputs outputs = {  .output = output_data,};int ret_val = tvmgen_default_run(&inputs, &outputs, NULL, NULL);```To facilitate this, some other changes are included:* Removed dependency on `aot_executor.{c,h}` in tests, pending thediscussion in the interface RFC as to whether we keep them.* Moved creation of test DLTensor's into the AOT test utils, in future thiscan be replaced by loading via the Python API or otherwise* Introduce `parametrize_aot_options` which can be used to testpermutations of AOT which work together - for now this filters Cinterface and packed operators* Updated demo application to generate the header for demonstrationpurposes, we should consider porting the demo application to ModelLibrary Format and using the toolchain in the Zephyr App via CMakeinstead?This patch builds upon the improvements @giuseros made to AOT testingand name mangling from #8014* Tweak metadata variable description and MLF target loop* Remove direct usage of `relay::Var` in meta_data.hThis looks like the only place that could be causing the Windows CI failures, so trying removing the additional header in meta_data.h* Linting fix* Post-rebase files fixingThese tests were somehow transmuted in transit, I've updated them to themost recent variant of the test helpers.* Strip back interface API to just inputs and outputsThis removes any speculative structures from the generated code and cleans up some of the documentation.* Add header guards and tweak documentation",0
Docker env for Arm® Ethos™-U55 Port (#8514)* Docker env for Arm® Ethos™-U55 Port* Added Arm® Corstone™-300 Reference System for testing* Added Arm® Ethos™-U driver stack* Added installation of Arm® Vela.Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>Change-Id: Ie3cc43943c876d95618a39887aa666da20bcb1e4* Docker env for Arm® Ethos™-U55 Port* Removes /opt/arm/cmake/bin from the path* Parameterizes Arm® Ethos™-U55 driver stack version numberChange-Id: I2162b40f82241fd013643cbfa8847b60d7f4f5a1* Docker env for Arm® Ethos™-U55 Port* Adds ethosu as an extra to /python/gen_requirements.pyChange-Id: I2162b40f82241fd013643cbfa8847b60d7f4f5a1* Docker env for Arm® Ethos™-U55 Port* Added comment explaining why Vela version needs to be pinned to 2.1.1Change-Id: I1ade280faa5274cca78899f4dae9e596b16fb5df,1
"[FIX,PROFILING] Add USE_PAPI configuration to config.cmake (#8567)",0
[Fix] Fix a typo in include/tvm/ir/function.h (#8617),0
"[Relay][Quantization] Extend FakeQuantizationToInteger to more ops (#8241)* support scalars in quantize and requantize* Add affine type support for ops with multipe output, use it in concat, move to header* support new ops, refactor tests* add more binary opsfix pylintfix blackblack broke pylintoops on black* fix a typo in a branch and add a test that hits it* improve comments",0
"Fix test_external_codegen, broken by #8591 (#8630)",0
"[DOCS] TVM install addenda for M1 Macs (#8568)* instructiosn for m1 mac* typos* above to below* nits, link against python issue on github* correct link* more cleanup* correct source* address chrishoge suggestionsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",1
Parametrize ONNX Unit tests (#8621),3
"[Refactor] Avoid Override Generic Op Strategy in ""hls.py"" (#8614)* [Refactor] Avoid Override Generic Op Strategy in ""hls.py""* Fix The Broken CI Test Cases",0
"[microTVM][RVM] Set the number of cores based on the VM sizing (#8624)Set the number of cores for scripts and builds that run inside the RVMbased on the specified number of cores for the VM.Currently Vagrant doesn't set env. variable TVM_CI_NUM_CORES with thenumber of cores available in the VM created by Vagrant, as a consequencethe scripts and builds (like the ones used to build TVM and QEMU) thatrun inside the VM after it is created will use the default number ofonly 2 CPUs, so not using the full CPU resources available in the VM,in case there are more than 2 cores available.This commit sets TVM_CI_NUM_CORES equal to the number of cores availablein the VM created by Vagrant so the builds (which use that environmentvariable to find out the number of CPUs that must be used for thebuilds) can use all the CPUs available, speeding up the builds.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
"[Target] Enable device querying for all targets. (#8602)- Move ""from_device"" argument definition from ""vulkan"" target to all  targets.- Add device querying to TargetInternal::FromConfig, using  ""from_device"" argument.  If present, these have lower priority than  explicitly-specified attributes, but higher priority than the  default attribute values.- Add default no-op DeviceAPI::GetTargetProperty.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
"[Runtime] Add graph_executor get_input_index API. (#8633)* [Runtime] Add graph_executor get_input_index API.In graph_executor use case, user can use set_input withinput index to set input parameter, but there is no straightforward way to get correct index number with input name, hereprovide get_input_index API to do such work.* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update src/runtime/graph_executor/graph_executor.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Update python/tvm/contrib/graph_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",1
"[Relay] Change Default ""opt_level"" of Sequantial from 2 to 0 (#8634)",4
"[Target] Allow spaces in target attributes (#8587)* [Target] Allow for spaces in target attributes.Some target parameters, such as the device_name on vulkan, have spacesin them.  This prevented round-trips between string and Targetobjects, which can occur in some cases.* [Vulkan] Fixed ""device_name"" property querying.* [Target] Switched from escaped spaces to quoted spaces.Instead of -attr=value\ with\ spaces, will instead be written as-attr='value with spaces'.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
"[AMP] Disallow fp16 conversion for arange op (#8644)* [AMP] Do not allow fp16 cast on arange inputs* add test* Add comment explaining the issue with fp16 ""end""",1
"[microTVM][RVM] Fix platform name in base-box-tool (#8612)Platform boards passed to base-box-tool.py need to be a subset ofplatform boards support by 'tests/micro/zephyr --microtvm-platforms='.Currently base-box-tool.py only accepts the 'stm32f746xx' ST board,which is not supported by 'tests/micro/zephyr --microtvm-platforms='. Asa consequence if one passes '--microtvm-platform=stm32f746xx' tobase-box-tool.py the 'tests/micro/zephyr' test will fail.That commmit fixes it by adding two new platforms to base-box-tool('stm32f746xx_nucleo' and 'stm32f746xx_disco') which are supported bytests/micro/zephyr and by removing the nonexistent 'stm32f746xx'platform. The new platform boards are quite similar and share the sameUSB VID and PID.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[Target] Several minor corrections to the device property query (#8651)- Pass parameters through TVMRetValue as std::string instead of  runtime::String- Remove escaping of spaces inside quotes for target attributes.  Updated unit test to verify round-trip behavior.- Added missing ""device_type"" query for Vulkan.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
"[TEST] Refactor RPC test to isolate runs into a sub-function (#8656)We kill the rpc server in the del function. When a serverco-exist with remote resources in the same function scope,the destruction order is not determined.This can cause server to be destructed before the actual remote array.As a side effect, it can cause sometime test to timeout due towaiting on the socket.",3
Fix rust rt link (#8631)* Fix support for linking to only libtvm_runtimealso ensures that the ResNet example uses the new support.* Fix build.rs to rebuild if the Python script changesCo-authored-by: Jared Roesch <roeschinc@gmail.com>,0
[Frontend][Pytorch] add suppport for 'aten::upsample_bicubic2d' (#8648)* fix* lint,0
[Bugfix][Target] Correct passing of target-queried bool/int parameters (#8660)Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>,0
[TensorRT] Add transpose_a/b for TensorRT batch_matmul (#8607)* Add transpose support for tensorrt batch_matmul* Address PR comment* Refactor to add ONNX_DEFAULT_CONFIGS,1
[Fix][Frontend][TOPI] minor bugs (#8622)* fix* fix* lint,0
[AutoScheduler] Fix deserization of workload registry entry (#8662),0
"[TENSORIR] Add `from_legacy_te_schdule` attr to TE PrimFuncs (#8641)* [TENSORIR] Add `from_legacy_te_schdule` attr to TE PrimFuncsThe `from_legacy_te_schedule` marks PrimFuncs created from TEscheduling. Passes that only operate on TE scheduling check this attrsand no op if it is not found. If `from_legacy_te_schedule` is false ornot set, then it is assumed that the PrimFunc is from TensorIR. Passesspecific to TensorIR now check for the absence of this attr.* formatting* enable passes regardless of te or not",1
Move flake8 to ci_lint (#8652)* Move flake8 to ci_lintThis fixes the scenario where you lint with ci_lint but it can stillfail in PR due to flake8 being injected only into the Mac build.* Disable flake8 until the docker changes have landed,0
[Support] Linear Congruential Random Engine (#8642)* Add linear congruential engine.* Fix typo.* Minor fix.* Fix comments and intros.* Change to unsigned.* Minor comment fix.* Fix unsigned rand state to signed.,0
[Frontend] Unified LSTM cell (#8599)* fuse dence sum* remove excess copying* dev LSTM in ONNX* alternative implementation of LSTM in onnx frontend. It is quicker than current one without tuning* LSTM_dev2 was implemented in onnx frontend* LSTM dev in pytorch frontend* LSTM cell implementation was transferred to common place. Unneccessary code was removed* lint fixes* Weights permutation for LSTM layer in onnx frontend* LSTM cell description was added* arguments and values were renamed. descriptions of some methods were added* LSTM output shape and actvations input format were fixed in onnx frontend* empty. tvm-ci test* unbind method was transferred from onnx frontend to common.py* unbind method was transferred from pytorch frontend to common.py* lstm cell was transferred from op/layers.py to frontend/common.py* clean up weight dictionary initialization* fix pytorch frontend wrapper over unbind method* minor fix of comments* empty. tvm-ci test restart* empty. tvm-ci test restartCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
"[UnitTests] Apply correct requires_gpu() pytest marks for parametrized target (#8542)* [Onnx][UnitTests] Excluded additional onnx tests- The onnx tests `test_basic_convinteger`, `test_convinteger_with_padding`, `test_range_float_type_positive_delta_expanded`, and `test_range_int32_type_positive_delta_expanded` don't run correctly on CUDA targets, so they are added to the exclusion.- Parametrized over the relative directory name, rather than the full directory name.  This improves readability of the pytest output, and keeps the same parametrized test name across different python version.- Changed the target-specific skips to check the target kind, rather than the full target string.* [UnitTests] Apply correct requires_gpu() pytest marks for parametrized targetPrevoiusly, the addition of tvm.testing._target_to_requirement pytest markswas handled by the parametrize_targets function.  The_auto_parametrize_target function assumed that a unit test that was alreadyparametrized had all markings needed.  If a unit test was explicitlyparametrized using @pytest.mark.parametrize, these marks would be missing.In most cases, this explicit use of @pytest.mark.parametrize('target', ...)should be avoided, but has value in the case of marking with multipleparameters with @pytest.mark.parametrize('target,other', ...).  This usecase isn't yet supported by the tvm.testing.parameters function.  Therefore,if this occurs, detect it and add the appropriate marks.* [UnitTest] Bugfix, applying requires_* markers to parametrized targets.Initial implementation did work correctly with@tvm.testing.parametrize_targets.Also, went through all cases where ""target"" is used to parametrize onsomething other than a target string, and renamed.* [Onnx] Switched from using pytest.skip to tvm.testing.known_failing_targetsAfter merging of the `tvm.testing.parametrize_targets` and`tvm.testing._auto_parametrize_target` code paths,`known_failing_targets` can be used in both cases.* [Testing] Enable `Target` object as argument to _target_to_requirementPreviously, tvm.testing._target_to_requirement required the argumentto be a string.  This commit allows it to be either a string or a`tvm.target.Target`.* [Testing] Auto-target parametrization, handle pytest ParameterSetIf the unit test has already been parametrized with pytest.params toadd parameter-specific marks, respect those existing marks.This can happen in some cases in the CI, uncertain yet what is causingthem.  Maybe pytest-xdist related, but there's some difficulty inreproducing it locally.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[VM] Add get_input_index support. (#8661),1
"[VTA] Fix vta rpc server, refactor launch cond to not depend on sys.argv (#8671)",0
[FIX] Fix threadpool reset by killing threads before destroying their shared queue (#8658),0
[microTVM][Zephyr] Add skip for AOT test (#8628)* add hex indicator to message* add pytest skip* trigger* trigger,1
Allow rust tvm build configuration through cargo features (#8665),5
"[Relay][QNN] Support for non scalar zero points in qnn.conv2d (#8620)* conv2d working, fixing conv2d_depthwise* Depthwise conv2d working.* Make convinteger work on cuda.* Simplify code and add tests.* Formatting.* Fixed fallback broadcasting.* Fix fallback broadcasting.* Formatting.* Fix lint* Merge with new test parameterization.",0
"[Topi][Testing] Float16 unittests for dense, conv2d, depthwise conv2d (#8529)* [Topi][Testing] Minor cleanup for python reference implementations- Use input dtype for dilate/conv2d accumulate in python  impl. Previously, the python implementations of dilation and conv2d  would use numpy default dtype in some cases, rather than the input  data's dtype.- Added fallback for datatypes not supported by scipy.signal.convolve2d (e.g. float16).- Refactored to avoid duplication, use common get_pad_tuple functionality.* [Topi][UnitTests] Added float16 tests to test_topi_dense.py* [Topi][UnitTests] Added float16 to test_topi_conv2d_nchw.py* [Topi][Float16] Added float16 tests for depthwise conv2d.* [UnitTests] Explicitly set seed for float16 testsIntended to avoid flaky test failures later due to rounding errors.* [UnitTests] Fixed a few failing unit tests.- ref_data must be a test fixture, not acquired through  request.getfixturevalue, in order to have the random_seed be known.- dilate_python's return value didn't follow `out_dtype`.- The test_topi_conv3d tests had the reference results computed in  float64, due to dilate_python() not respecting the input data type.  With the correct dtype, the tolerances needed to be slightly widened.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
Add batch_matmul convertion to FQ2I pass (#8635),1
LowerWarpMemory: remove unneeded shuffle when accessing from the same thread (#8681),4
[Target] Add __launch_bounds__ directive as part of the CUDA code generation (#8678),1
"[microTVM] Project API infrastructure (#8380)* Initial commit of API server impl.* initial commit of api client* Add TVM-side glue code to use Project API* Change tvm.micro.Session to use Project API* Rework how crt_config.h is used on the host. * use template crt_config.h for host test runtime; delete   src/runtime/crt/host/crt_config.h so that it doesn't diverge from   the template * bring template crt_config.h inline with the one actually in use  * rename to MAX_STRLEN_DLTYPE * Create a dedicated TVM-side host crt_config.h in src/runtime/micro* Modify Transport infrastructure to work with Project API* Add host microTVM API server* Zephyr implementation of microTVM API server * move all zephyr projects to apps/microtvm/zephyr/template_project* consolidate CcompilerAnnotator* Allow model library format with c backend, add test.* Update unit tests* fix incorrect doc* Delete old Zephyr build infrastructure* Delete old build abstractions* Delete old Transport implementations and simplify module* lint* ASF header* address gromero comments* final fixes?* fix is_shutdown* fix user-facing API* fix TempDirectory / operator* Update micro_tflite tutorial* lint* fix test_crt and test_link_params* undo global micro import, hopefully fix fixture* lint* fix more tests* Address tmoreau89 comments and mehrdadh comments * fix random number generator prj.conf for physical hw * uncomment proper aot option",0
[Contrib] Support fp16 input in cpu sort (#8672),5
[Refactor] Rename .asnumpy() to .numpy() (#8659),5
[Relay][TOPI] Remove redundant cuda kernels caused by fusion of less & logical or (#8618)* [Fix] Remove redundant cuda kernels caused by fusion of less_less_logical_or* put the check function in reduction.py and add UT* fix CI issue* fix CI* fix CICo-authored-by: saury <saury@saurydeMacBook-Pro.local>Co-authored-by: saury <lifei59@meituan.com>,0
[community] @electriclilies -> Reviewer (#8684),3
Fix error when compile tvm with latest llvm14git (#8682),0
"[UnitTests] Added cuDNN to default test targets (#8383)* [Target][UnitTests] Look up target requirements based on tvm.target.Target- Read target.kind.name instead of using string manipulation.- Target device query on a non-existent target is no longer an error.  This occurs if expanding `vulkan -from_device=0` on a non-GPU  machine.* [UnitTests] Added cuDNN target to default test targetsSome unit tests explicitly test cudnn in addition to`tvm.testing.enabled_targets()`.  This moved the cudnn checks into thesame framework as all other targets, and adds it to the default listof targets to be run.  Also, added `@tvm.testing.requires_cudnn` fortests specific to cudnn.* [UnitTests] pytest.xfail for CuDNN conv2d with asymmetric padding* [Topi][CuDNN] Added handling of dilation to conv2d_cudnn* [Topi] Skip dynamic batch matmul on cudnn, vulkan, openclPreviously, cuda/nvptx targets were excluded.  Changed it to look upby target.kind.name, and to also exclude vulkan/opencl, as the dynamiclookup currently doesn't work on those backends.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
[Relay] Replace compile engine with TE compiler in the VM (#8501)* [VM] Add imports to new TE in VM compiler* [VM] Add comments to compile engine usages* [VM] Replace depreceated CachedFunc of compile_engine with TE_compiler* [VM] rm compiler engine compiler.cc* [VM] Replace compile engine with TECompiler in memory allocator* [VM] Add relay interface to te_compiler* [Relay] Fix linting errors* Move TEcompiler to VMCompilerContext; add global func into IRmodule when lowering in TEcompiler* add back the check* skip the check for ext func in tecompiler* skip tvm::build for external functions* trigger ci* retrigger ci* retrigger ci* remove the unnecessary loop in tecompilerCo-authored-by: YuchenJin <yuchenj@cs.washington.edu>,0
"Remove unused variables in AOT tests (#8686)These were re-introduced in https://github.com/apache/tvm/pull/8380,noticed when I went to rebase https://github.com/apache/tvm/pull/8650.",3
[Meta Schedule][M3a] Traced Schedule (#8623)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[microTVM] Add Arduino CLI support to ci-qemu (#8504)* Add Arduino CLI support to ci-qemu* Install latest version of Arduino SDK* Remove unnecessary --fix-missing* Tweak to clarify what URLs go with what* Retrigger CI* Temporarily replace buggy Spresense core,0
[AutoScheduler] Fix FLOPS estimation (#8695),0
Improve the error message in module.cc (#8694),0
[FIX] Correctly link to PAPI (#8691),0
[Torch] Fix ELU conversion (#8699),0
Rev ci-qemu to 0.07 (#8698),5
[microTVM][Zephyr] Fix: Test fails on hardware because of short timeout (#8677)* add timeout* rename timeout and change timeout to a reasonable value* fix tests after project api merge* retrigger because of flaktest,0
add in-place methods used by Tacotron2 to pytorch frontend (#8692)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>,1
[Rust] Restore the Rust CI testing after Docker image update (#8657)* Fix Rust CI* Turn Rust CI back on,0
"[Docs] Added documentation on pytest target parametrization. (#8638)* [Docs] Added documentation on pytest target parametrization.Follow-up from #8542, to document existing features.* [Docs] Updated pytest parametrization documentation following reviewCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",1
[Rust][Fix] Memory leak (#8714)* Fix obvious memory leak in function.rs* Update object pointer,0
"Force a gc between sphinx-gallery items to reclaim GPU memory. (#8722)GPU memory is only released once the PackedFunc for evaling the model is gcedby Python. In CI we're noticing intermittent 'CUDA: Out of memory' failureswhile processing the tutorials, and tracing showed there was no gc happeningbetween items. Not confident this will solve the problem but worth a try.",5
[microTVM] Zephyr Test Refactor (#8713)* refactor host to qemu* remove unused variables* remove skip-build arg* fix microtvm test script,0
"[Docker] Refactor/clean-up of docker/bash.sh (#8670)* [Docker] Refactor/clean-up of docker/bash.sh- Added detailed help message, displayed using `-h` or `--help`.- Optional flags handled using `getopt`, can now occur in any order.- `--mount` flag may occur more than once.- Switched from short arguments to docker-run to long arguments  (e.g. `--volume` instead of `-v`).  Short arguments are good  shortcuts for interactive work, but can be more difficult to read in  longer scripts.- Mount the `.tvm_test_data` folder, to avoid re-downloading test data  already available in the host environment.* [Docker] docker/bash.sh CI fixDash-prefixed arguments as part of the command now require prefixing with-- to separate them from arguments intended for docker/bash.sh* [Docker] docker/bash.sh, consistent quoting* [Docker] Added --repo-mount-point for docker/bash.sh* [Docker] Updated command-line parsing of docker/bash.sh- Maintained previous behavior, any unrecognized flags after the  docker/bash.sh are part of the command, no -- is  needed. (e.g. docker/bash.sh ci_gpu make -j2)- Reverted changes to Jenskinsfile to add a --, no longer needed.* [Docker] Fixed multi-argument commands* [Docker] docker/bash.sh check permissions before mounting ~/.tvm_test_data* [Docker] Consistent workplace directory in docker/bash.sh for JenkinsSome locations in the CI perform build commands outside of the buildsteps (e.g. tests/scripts/task_ci_setup.sh#L38), and cmake doesn'tlike it if the build directory changes.  These should probably bemoved into the build steps of the CI, and be packed in tvm_multilib inthe Jenkinsfile, but for the meantime maintaining a consistent/workspace directory on all CI nodes allows cmake to run.* [Docker] Updated bash.sh for MacOS compatibilityMacOS has an older version of bash that handles arrays slightlydifferently.  All instances of array expansion `""${ARRAY[@]}""` shouldinstead be written as `${ARRAY[@]+""${ARRAY[@]}""}`.  Otherwise, `set -u`will erroneously complain about an undefined variable. Seehttps://stackoverflow.com/a/61551944 for details.Even though this is an older version of bash (observed in version3.2.57), this is the last major version available under GPLv2 and istherefore the default version on MacOSX.  At some point, the`docker/bash.sh` could be migrated to python for ease ofmaintenance/testing.",0
"[Docs][UnitTest] Updated target parametrization documentation (#8724)* [Docs][UnitTest] Updated target parametrization documentationThe intended audience are developers writing unit tests, or debuggingunit tests that have failed.  Therefore, moving the recommended styleto the top of the section, and the implementation details to thebottom.* Documentation updates as recommended by tkonolige",0
increase atol for float32 (#8712),5
Refactor AOT Test Utils parameters into object (#8650)* Refactor AOT Test Utils parameters into object`compile_and_run` was getting quite complicated to understand as well as being mostly duplicated by `comile_and_run_multiple_models`.This patch pulls out some common parameters into a data class `AOTTestNetwork` which makes it clearer what each parameter is doing and provides documentation.* Rename Network -> Model and sizebytes -> size_bytes,3
"Convert AOT to TECompiler (#8697)* Convert AOT to TECompilerThis removes the dependency on ""compile_engine.h"" from aot_executor_codegen.cc. This required a few changes to how AOT was operating:* AOT run_model is now based on the post lowering main_module* AOTOnDemandAllocator is ran twice to ensure SIDs are updated post-lowering* Moved to using tec::UpdateFunctionMetadataTests are passing, but would appreciate other validation :smile_cat:* Clarify reasoning behind replanning memory later* Use main_func_info rather than bespoke logic in AOTThis moves from using the bespoke AOT UpdateMainWorkspaceSize to theLoweredModule main_func_info property to unify with Graph executorcodegen.",1
Remove qemu installation from Zephyr RVM (#8701),2
[Relay] Dense alter layout fixed for packed input (#8669)* clean up typerel* add layout transform when input is 3D* add test* update doc to clarify that only 2D input data is supported* add weight_layout attribute in dense* remove explicit layout transform from dense_alter_op.py* Add DensePackInferCorrectLayout to insert layout transform* relax type rel* revert type rel relax and add check on dim* introduce DensePackAttrs to avoid breaking dense op* try fixing arm compute lib test* Update tests/python/contrib/test_arm_compute_lib/test_dense.pyCo-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>* formattingCo-authored-by: lhutton1 <35535092+lhutton1@users.noreply.github.com>,0
[TIR] Use PopenPool instead of multiprocessing.pool (#8492)Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[CI] Add Arm Compute Library to Arm CI unit test pipeline (#8734),1
"[UnitTest] Updated tolerances to avoid flaky unit test. (#8723)* [UnitTest] Updated tolerances to avoid flaky unit test.The result was correct, but the atol was just small enough to triggera CI error for a value that was close to zero in an unrelated PR at#8670.https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-8670/16/pipeline/#step-236-log-1703* Also updated 32-bit version of test_conv2d_nchw",0
[Torch] chunk and unsafe chunk (#8718)* alternative chunk op was implemented in pytorch frontend. aten::unsafe_chunk was added to op map in pytorch frontend* chunk was replaced by new one in pytorch frontend. it is faster in 2.5 timesCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,1
enhance tir signed-unsigned cast (#8706),5
[TVMC] Switch profile flag to use new profiler (#8710),1
"[TensorIR][M2a] Storage Align (#8693)This PR is part of the TensorIR upstreaming effort (#7527), which adds the oneschedule primitive storage_align.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>",1
"[Docs] Moved the generated tutorials folders into a _staging folder. (#8735)* [Docs] Moved the generated tutorials folders into a _staging folder.Previously, reorganization or renaming of tutorials could causedocumentation tests to fail in CI.  The CI checks out the version tobe tested, which may still have generated documents in`docs/tutorials` and `docs/vta/tutorials`.  If a PR moves these todifferent folders, then they show up as duplicate `*.rst` files,resulting in sphinx warnings.This commit makes a `docs/_staging` folder in which sphinx is run.All tutorials are generated within this folder, and the entire foldercan be deleted with `make clean`.  As a result, it is safe toreorganize the tutorial without impacting CI.* Updates based on reviews.* Changed graph_runtime references in deploy_classification.py to graph_executor* Removed unnecessary graph_runtime import from tune_alu_vta.py",1
Add parameter to allow caller to supply a Runner (#8747)* Add parameter to allow caller to supply a Runner* Add unit test for passing in runner to graph tuner,1
"[Vulkan] Check at codegen if the shader is within shared memory limits. (#8746)Previously, shaders that do not respect device limits for sharedmemory could result in segfaults that occur during the call to`vkCreateComputePipelines`.",5
"[VTA] Make vta graph_pack compatible with latest TVM, and bring back object detection tutorials. (#8731)* [VTA] Make vta graph_pack compatible with latest TVM, and bring backobject detection tutorials.* remove deploy_detection.py.* move out deploy_detection.py from legacy folder.* fix build error.",0
[FRONTEND][PYTORCH] Support fo nn.SiLU added (#8753),1
update docs (#8736)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
Fix use of fallback AutoTVM knobs in default scheduling (#8707)* Fix use of fallback AutoTVM knobsPreviously knob values depended on order of explicit cfg update and cfg.define_splitcalls in fallback mode* Add test for define_split with fallback defined values,0
[TF] Support TensorFlow < 1.13 for test_sparse_add (#8647),1
Install rust in ci-lint so cargo fmt can move to lint stage. (#8727),2
"[Onnx Operators] Celu (#8741)* complete celu op* forgot to add test* change order in convert_map, remove comment, delete import hiccupCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>",1
[Fix][TOPI] remove wrong fix in x86's dense_nopack operator (#8687),0
[microTVM] Fix warnings on Zephyr tests (#8740)Fix the following warning message on Zephyr tests:DeprecationWarning: Please use input parameter mod (tvm.IRModule)instead of deprecated parameter mod (tvm.relay.function.Function)Signd-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
Allow Linker script files to be committed (#8745)This is a source file type needed for https://github.com/apache/tvm/pull/8744Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Grant Watson <grant.watson@arm.com>,2
Fix builtin_fp16.h path according to: https://discuss.tvm.apache.org/… (#8705),0
[AutoScheduler][FIX] Fix exception handling in measure.py (#8754)* fix exception handling* fix linting* stringify the exception from MapResult* use repr instead if str,0
add support for half_pixel_centers in resize (#8689),1
Make from_tensorflow.py more GPU memory friendly. (#8763)* Make from_tensorflow.py more GPU memory friendly.Sphinx-gallery runs everything in a single process. Theredoesn't appear to be any easy way to force Tensorflow toreturn memory other than terminating the process. This atleast gives us a little more wiggle room.* Also deploy_sparse.py. Should probably also be done to tensorflow.rst.,5
Add DictAttrs to IRModule and refactor DictAttrs utility functions (#8750)* Add DictAttrs to IRModuleNodeMove GetAttrs to be a member of DictAttrsGeneralize WithAttrs to work with IRModule and move to attrs.hChange func->GetAttr to func->attrs.GetAttr* lint* Fix documentation* fix typo* Another typo!* Revert GetAttrs to ->attrs.GetAttrs change* Didn't mean to revert these* Revert a few more things* Add GetAttrs to IRModuleNode,0
adding gromero as a reviewer (#8765),1
"[Community] @Mousius -> Reviewer (#8764)* adding Mousius to reviewers,  name update for Siva Reddy* making Siva's name consistent",1
Enable custom images to be set in TVM Jenkinsfile (#8721)* This work is needed to enable automatic testing of our   newly built Docker images as part of CI * The default value is set by variables in the same   Jenkinsfile and are used when no custom values are   provided,1
"[UnitTests] Require cached fixtures to be copy-able, with opt-in. (#8451)* [UnitTests] Require cached fixtures to be copy-able, with opt-in.Previously, any class that doesn't raise a TypeError in copy.deepcopycould be used as a return value in a @tvm.testing.fixture.  This hasthe possibility of incorrectly copying classes inherit the defaultobject.__reduce__ implementation.  Therefore, only classes thatexplicitly implement copy functionality (e.g. __deepcopy__ or__getstate__/__setstate__), or that are explicitly listed intvm.testing._fixture_cache are allowed to be cached.* [UnitTests] Added TestCachedFixtureIsCopyVerifies that tvm.testing.fixture caching returns copy of object, notthe original object.* [UnitTests] Correct parametrization of cudnn target.Previous checks for enabled runtimes were based only on the targetkind.  CuDNN is the same target kind as ""cuda"", and therefore needsspecial handling.* Change test on uncacheable to check for explicit TypeError",0
[TIR] Change Integer Implicit Conversion Rule to C Standard Way (#8733),4
[Relay testing] densenet implementation fix (#8704)* Fixed testing densenet bug* Fixed code format using black,0
Fix ci-qemu Arduino install dir (#8766),0
Update QemuTransport#write() to match new write API contract. (#8761)* suspect this should fix #8278,0
Add PaddlePaddle dependency in docker file (#8742),1
Add params.* to Jenkins file parameters (#8771)* Prefix all parameters with params.* so that it checks   whether parameters exist before using them * This is a follow-up fix on #8721 so that existing PRs work   without being re-triggered manually twice,0
"[Relay] Refactor Interpreter to treat lowering as IRModule->IRModule rewrite. (#8597)* This continues the work outlined in the RFC  https://discuss.tvm.apache.org/t/rfc-relay-tecompiler-rewrite-existing-compile-engine-to-match-updated-compiler-flow/9233This gets about halfway there for the Interpreter:* Remove direct access to TECompiler from interpreter, and instead call  tec::LowerTEExpr when 'preparing' a module and expression for evaluation.* Make clear there's no phase distinction between create_interpreter and  evaluate on the Python side -- both must be prepared together as a single IRModule.* But in return make sure the result of evaluate on the Python side is a packed func  ready to directly apply 'simple' arguments to an already interpreted closure.* The interpreter builds and caches primitive TIR functions (and their corresponding  dynamic shape functions) as packed funcs as they are encountered.* Cleanup uses of interpreter for constant folding on the C++ side.Future work:* Fold LoweredModule into IRModule so tec::LowerTEExpr is just another pass.* Get rid of the implicit caching of lowered functions in TECompiler.* Make calling convention from Relay to TIR explicit, and remove all the function  attribute hackery currently needed so the interpreter can correctly invoke lowered  functions as it encounters them.* Make TECompiler private. Though could do this now it will make migrating the VM and  AOT uses of CompilerEngine harder.Force a gc between sphinx-gallery items to reclaim GPU memory. (#8722)GPU memory is only released once the PackedFunc for evaling the model is gcedby Python. In CI we're noticing intermittent 'CUDA: Out of memory' failureswhile processing the tutorials, and tracing showed there was no gc happeningbetween items. Not confident this will solve the problem but worth a try.* Get rid of logs spam.",1
Add support for QLinearMul ONNX op (#8773)* add qlinearmatmul* noop* mul not matmul* refactor some common qlinear op test code,1
skip aot checks when USE_MICRO=OFF (#8772),5
"[TensorIR][M2a] Parallel, Vectorize, Bind & Unroll (#8716)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>",2
"Add onnx opset v13 support for softmax, logsoftmax (#8625)* add more support for softmax ops* noop* noop",1
[Relay] Extract dataflow matcher data structure into header (#8774)* extract dataflow matcher data structure into a header file* lint* lint,2
Rev ci-qemu to v0.08 (#8776)* Remove synr from pip-installed package list * synr is installed by task_ci_setup* rev ci-qemu to 0.08,2
Restore License (#8779),5
Expose FTVMInferCorrectLayout Python interface (#8755)Co-authored-by: kueitang <kueitang@qti.qualcomm.com>,5
Remove old AOT Executor code (#8758)* Remove old AOT Executor codeThis removes the old AOT execution functions that relied on the model descriptor which was removed in https://github.com/apache/tvm/pull/8280.* Remove rogue tvm_model_t from demo app* Remove aot_executor from demo CRT libs,4
[microTVM] Project API Arduino support (#8708)* ProjectAPI Arduino support* Compile and run integration tests* Add support for other Arduino boards* Unit tests for project generation* AOT support* Arduino RPC server* Incorporate ProjectAPI changesAdd Arduino tests to CI* Copyright notices* Fix Andrew's PR comments* Additional PR commentsPR comments and Python 3.6 supportLinting fixRe-add test onnx fileTest Arduino cli bug workaroundSupport new hardware targetsTemporary fix for testsFormatting issueSpelling fixAdd test case for exact FQBN matching* Add unit tests from apps directory to task_python_microtvm.sh,0
[FIX] Bug fix for batch_matmul parameters mismatch (#8785),0
[CI] Rev ci-cpu to v0.76 (#8786)- This includes changes up to commit 1a95f9bd0,4
"[microTVM][RVM] Fix base-box-tool command in README.md (#8613)This commit fixes the platform argument order for base-box-tool.py'test' command in the documentation about the RVM. Currently the examplein documentation places <platform> before option[--test-device-serial=<serial>], whilst the correct order is after allthe options, so trying to use the 'test' command arguments in the orderas suggested by the documentation will not work.This commit also fixes a typo (inovke -> invoke).Finally it tweaks a bit the text format: lines with maximum 80 columns,a better diagram format for the dir structure, and a better format forthe bash commands. A link is added too for easy access to the""microTVM Reference VM tutorial"" found in tutorials/micro directory. Acouple of command examples were also added to the documentation.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[Android][RPC] Fix Vulkan runtime support. (#8791)Update Android RPC app to reflect the newVulkan source code tree structure.,0
"Add synr==0.3.0 dependency for Docker images and Python dependency. (#8801)- PR #8776 removed `synr` as a dependency to be installed in the Docker  images, making the images to need manual intervention so that we could  run tests.- Thir PR reverts synr (with current constraint as observed in  tests/scripts/task_ci_setup.sh) to be part of the Docker image.",1
"[UnitTest][Flaky] Increased tolerance on onnx test_forward::test_aten (#8798)Default tolerance had about 2-3% failure rate (8/300 iterations), andcaused failures on unrelated PRs(e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-8784/1/pipeline#step-485-log-1156).New limit of `atol=5e-7` chosen to be above the maximum delta of3.5e-7 observed in 300 iterations.",1
"[Docker][Vulkan] Allow Vulkan GPU access in docker container. (#8784)- The environment variable NVIDIA_DRIVER_CAPABILITIES must include  ""graphics"" in order to expose Vulkan drivers to the container.  This  is added both to Dockerfile.ci_gpu for future image builds, and to  docker/bash.sh for compatibility with current images.- The configuration files needed by the vulkan launcher and glvnd must  be exposed to the container.  These are only included in  `docker/bash.sh`, as they may vary by host and so cannot be baked  into the image.",1
Extend tune_relay_x86 tutorial to measure default and kernel level tune (#8794),5
[TIR] Fix buffer scope in structural equal (#8768)* fix buffer scope in structual equal* make global equal to empty,0
[CONTRIB] Allow customized initializer in PopenPool (#8789),5
[Frontend][TFLite] Implement fake quant (#8780)* [Frontend][TFLite] Implement fake quant* remove unused variable* fix linting errors* add more tests* use pytest parametrize instead of a separate function,0
"[Texture support][Part 1] TIR lowering and OpenCL support (#7686)* Add support for kTexture storage rank.* Add scaffolding for texture_flatten pass.* Add scaffolding for texture allocation.* Implement 2d texture flattening to builtin tir.text2d_alloca.* Lower BufferStore/Load to builtin texture store/load.* Add vectorizable attribure to texture load and store.* Support auto-vectorization on the innermost (RGBA) axis.* Add read/write_imagef opencl codegen for builtin texture load/store.* Add TextureType support.* Add InferTextureAccess pass to deduce __read_onlyand __write_only access qualifiers for texture vars.Also refactor use of restrict keyword to be var dependent.* Implement texture allocation as external function in TIR lowering.* Remove commented lines.* Add nd->2d texture flattening.* Bug fixes in opencl codegen (row<>col, access quals.)* Improve texture codegen by explicitly allocating local vectorfor the texture load. Also support indexing individual elementsof the RGBA vector.* Remove automatic vectorizationcode as it is no longer needed.* Improve SSA local use when storing texture read to scalar buffer.* Define texture flattening convention suchthat the outer Nd-1 axes are stored as rows,and the last axis is stored as columns.* Add tir lowering and opencl codegen support for float16 textures.* Disable SSA when texture load is immediately casted.* Allow RGBA extent to be of length 1.* Add pass to forward externally allocated texturesin place of textures realized from cache_read. Fixto better follow indexing spec.* Add buffer_common.h to house buffer offset simplification routines.* More refactor and clean up in texture lowering.* Add IsTextureType to tir and allow buffervar type annotation to be TextureType in additionto PointerType.* Bug fix in texture access qualifier inference pass* Step toward handling external texture buffer forwardingwhen external buffer is not stored directly to cache_read realized buffer.For example when it is conditionally stored via an IfThenElse node whenpadding is used.* [Part 2/3] Support texture:weight lowering convention for externally providedtexture buffers. Need to propagate this to allocated textures whencache_read(texture) is used for weights.* Bug fix in texture access qualifier inference pass* Tighten constraint on external buffer forwarding --cache_read(texture) cancellation -- to avoid incorrectprograms. Currently only forward through if_then_else nodeand direct external loads. For if_then_else, still needproper analysis of structural equality between buffersand access patterns to determine if an external buffercan replace the texture buffer realized via cache_read.* Use texture lowering convention from texture runtime util.* Use updated texture lowering utilities* Use inherited visitor overloads in texture flattener.* Add check in codegen for float/half untilread/write_image codegen supports other types.* Rename tir texture builtins* Remove codegen and tir runtime dependence on for TVMBackendAlloc/FreeTexture.* Dispatch texture allocas via target specialized tir.tvm_call_packed* Remove kTexture scope and use kGlobal with texture tag.* Remove TextureType.* Remove TextureType from OpenCL codegen.* Remove TextureType from TIR lowering.* Remove dependency on MergeMulMod.* Revert ""Add buffer_common.h to house buffer offset simplification routines.""This reverts commit 027628259229aaee051dbf1dfbed4e63ef820544.* Prune include list* Add more documentation to texture flattening.* Add TextureFlatten transform to refactored tvm lower API.* Apply clang formatting.* Blacken python APIs.* Apply cpplint changes.* Attempt to extract storage scope from pointer scope.* Remove ExternalBufferForwarding (cache_read cancellation) for now.* Apply MyPy.* Clang format* Only visit RealizeBuffer body for texture storage.* Fix bad merge.* Utilize OpenCL preprocessor to switch betweensampler-less and codegen provided sampler fortexture reads depending on whether the openclruntime is 2.0 compliant.* Add texture codegen test example.* Refactor tests to use pytest parameterization.Blacken tests.* Respond to CRs.",0
[CODEGEN][OpenCL]: fix tir.erf codegen to opencl directly (#8756)* register tir.erf to lower opencl directly* add opencl codegen unit test* change erf opencl codegen unit test for checking there is erf in the source not erff,0
[TIR] Support fold constants in specialize process (#8803)* support fold constants in specialize* replace Substitue() with VisitExpr() in specializer.,5
Fix typos (#8787)Fix a couple of typos in comments about the IR/AST node reflection codeand a typo in a comment about the main member of the TVMModule struct.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
"[TensorIR][M2a] Reorder (#8767)This PR is part of the TensorIR upstreaming effort (#7527), which adds a schedule primitive: reorder.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>",1
[microTVM] Fix platform name for qemu_x86 in Zephyr AOT tests (#8762)Currently two Zephyr AOT tests (test_tflite and test_qemu_make_fail) arenot running when qemu_x86 platform is selected because the platform nameis wrongly listed as 'host' in the match list for not skipping thesetests. This commit fixes it.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
"Remove duplicated PackedFunc C++ test (#8812)I came across this file whilst looking at the C++ tests and realised it's aduplicate of the PackedFunc tests which doesn't get invoked.```$ diff -u tests/cpp/contrib/bnns.cc tests/cpp/packed_func_test.cc--- tests/cpp/contrib/bnns.cc   2021-07-30 12:59:33.830443830 +0000+++ tests/cpp/packed_func_test.cc       2021-08-23 12:47:43.193708421 +0000@@ -17,6 +17,13 @@  * under the License.  */+#include <dmlc/logging.h>+#include <gtest/gtest.h>+#include <tvm/runtime/packed_func.h>+#include <tvm/runtime/registry.h>+#include <tvm/tir/expr.h>+#include <tvm/tir/transform.h>+ TEST(PackedFunc, Basic) {   using namespace tvm;   using namespace tvm::tir;```",2
"[Vulkan] Remote target.h #include (#8813)Was added in #8127, should have been removed in #8171 along with therest of the references outside of libtvm_runtime.so.  This didn'timpact the Vulkan+g++ builds, because no symbols were accessed outsideof the runtime library.  However, it broke the Vulkan+Windows builds,which expected symbols due to the `__declspec(dllexport)` defintion of`TVM_DLL` on MSVC (see #8805).  This wasn't caught by the CI build onWindows, because it doesn't perform the Vulkan build.",1
"Use CTest for C++ tests (#8809)By using the `gtest_discover_tests` CMake macro the CPP and CRT tests can be configured to build binaries with a single test runner each. Once CTest has information about tests it can be used in IDE extensions such as [CMake Test Explorer](https://marketplace.visualstudio.com/items?itemName=fredericbonnet.cmake-test-adapter).`ctest` can also run tests in parallel using the `-j` flag, which could be interesting in future.",3
"Add LowerTEPass, and convert calls to LowerTE to application of LowerTEPass (#8802)* Initial commitInitial stab at IRModule -> LoweredModule conversion func, notesAdd external_mods and main_func_info to conversion funcsMTest lowered module to ir modulefix problem with conversion funcs + print stmtsAdd LowerTE passAdd pLowerTEPassAAdd LowerTEPass to graph_executor_codegen.ccUse LowerTEPass instead of LowerTe in graph_executor_codegen.ccCode cleanupAdd docs, more cleanupFormatting* Fix bad rebase* Address 1st round of comments* Use tir kTarget instead of relay one* Change target string to Target obj* removing target string causing issues* Fix typos* Revert target str -> target obj changes* Don't use Update : IRModule because it is broken* Fix check* flaky test?* lint",0
"[FIX] Remove leftover instances of USE_GRAPH_EXECUTOR_DEBUG (#8796)* [FIX] Remove leftover instances of USE_GRAPH_EXECUTOR_DEBUGsingle flag, USE_PROFILER. This PR cleans up the last few remaining usesof USE_GRAPH_EXECUTOR_DEBUG.* formatting* Update CMakeLists.txtCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
Remove unused allocated memory in crt initialization (#8819)Currently TVMInitializeRuntime() allocates 250 bytes dynamically to backbuffer 'func_registry_memory' which is never used. That is not much ingeneral but besides being twice the current necessary amount for theruntime (allocated to back 'registry_backing_memory' buffer) that amountcan be important to be saved on memory-constrained devices (microTVM).This commit removes the 'func_registry_memory' buffer which is allocateddynamically in TVMInitializeRuntime() since it occupies 250 bytes and isnever used.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,2
"Remove unnecessary memset in TVMMutableFuncRegistry initialization (#8818)Remove unnecessary memset() call in TVMMutableFuncRegistry_Create()when initializing a TVMMutableFuncRegistry struct. All struct members(registry.names, registry.funcs, and max_functions) are alreadyinitialized properly before returning, hence some CPU cycles might besaved (usually 12 bytes in a 32-bit platform and 24 bytes in a 64-bitplatform must be written with 0 by memset()).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
Run AOT tests against reference system (#8744)* Run AOT tests against reference systemThis introduces an alternative way of running AOT tests using the reference system added in https://github.com/apache/tvm/pull/8514. This gives us additional assurance that the AOT output runs successfully on embedded platforms in our core test suite.I've also changed calculate_workspace_sizes to debug_workspace_sizes and default to False in most cases as it only needs to be True for a few cases to check theoutput with the debug flag - this was discovered trying to allocate 16MB in an embedded test :scream_cat:Co-authored-by: Grant Watson <grant.watson@arm.com>* Skip AOT reference system tests in i386 container* Add comment clarifying the reference system runnerCo-authored-by: Grant Watson <grant.watson@arm.com>,0
"[Rust] Fix memory leak #2 (#8725)* Add C++ API for computing type key from type index* Try and isolate leak* Rewrite the bindings to fix the ArgValue lifetime issueThere are still quite a few issues left to resolve in this patch, but I believe the runtimechanges stablize memory consumption as long as the parameters are only set once. ByteArrayalso has some totally broken unsafe code which I am unsure of how it was introduced.* Finish handling tvm-rt issues due to ArgValue lifetimeThis patch further refactors the bindings to better handle thelifetime issues introduced by detecting the argument memory leak.* WIP memory leak* There is issue using TVMCb function which is breaking refcount* Fix fallout from the lifetime refactor* Another tweak* Follow up work from the memory leak, attempt to clean up ByteArray* Add some todos for future work* Fix doc string* Clean up the changes* Format",0
[FLAKY] A small bug fix on the CmakeLists (#8826),0
[AutoTVM] Use PopenPool in XGBoostCostModel (#8820)* replacd multiprocessing.Pool with PopenPoolExecutor* add initializer func* static init func* address comments* linting* fix tests* address comments,0
Correct function signatures for CreateXPass functions in docs (#8829),2
Apply CPPLint to C++ Unit Tests (#8827)This change enables `cpplint` for the tests in `tests/cpp` and corrects any current linting errors. I had to use `NOLINT` in some of the PackedFunc tests due to a bug (see: https://github.com/cpplint/cpplint/issues/131) in CPPLint where `int(int)` is picked up as a cast rather than a nameless argument.,0
"[Hexagon] Remove uses of LLVM from simulator runtime (#8821)* [Hexagon] Remove uses of LLVM from simulator runtimeThe TVM runtime is not linked with LLVM libraries, so using LLVMin it carries a risk of referencing undefined symbols. This maywork for objects defined in header files, but it then relies onLLVM keeping them there.Replace uses of LLVM utilities in the Hexagon simulator runtime,with simple alternatives.* clang-format* Use dmlc::optional instead of implementing one from scratchMake detail::Optional be derived from dmlc::optional, and add some bitsto make it behave more like the C++17's std::optional. The goal is toreplace detail::Optional with std::optional, once the project switchesto C++17.",1
"Add link to docs and tutorials in the README. (#8832)Most project pages on GitHub have a README.md file with a clear link to installation or tutorial material for new users.While there is a link to Documentation, it's not that obvious, and adding a more explicit ""getting started"" link may behelpful for new TVM users trying to navigate the project.",1
"Better reflect allocator names in CRT tests (#8828)When the AOT executor was introduced, the Stack Allocator was associatedwith it by test name whereas the Page Allocator was left as justmemory_test.cc. This cleans that up a bit to clarify which tests whichallocator.",3
[M3a][Meta Schedule] Add Sampling Primitive SampleCategorical. (#8817)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>,1
[Community] @Lunderberg -> Reviewer (#8834),3
Update CONTRIBUTORS.md (#8837)TVM is no longer in the Apache Incubator; moving mentors to the end of the doc.,1
[Frontend] [Torch] [ONNX] GRU layer (#8781)* GRU cell was implemented in common.py. GRU was supported on pytorch frontend side* update GRU in common.py and onnx frontend* fix issue related to GRU accuracy in pytorch and ONNX frontend* small fixes and remove excess* common GRU was additionaly updated. tuned pytorch GRU was strongly accelerated* GRU cell in ONNX frontend was used from common.py. previous implementation was removed* small fixes in comments* fixes after review. GRU test was implemented for pytorch frontend* tests for RNN layers was unified for pytorch frontendCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
"Force CMake targets in top-level Makefile to run (#8840)This is a bug I introduced in https://github.com/apache/tvm/pull/8809, because the built binary is now named `build/cpptest` when `make` checks that artifact it finds it exists already and skips running `make -C build cpptest`. This ensures all nested `make` calls are forced to run from the top-level `Makefile`.",0
"[Hexagon] Reuse Hexagon SDK analysis across cmake files (#8822)* [Hexagon] Reuse Hexagon SDK analysis across cmake filesDifferent versions of the Hexagon SDK may have different directorystructures. Extract the directory identification code into a separatecmake module. Use that module in Hexagon.cmake and in the cmake filefor the FastRPC libraries.* Don't modify CMAKE_SHARED_LINKER_FLAGS, instead set target properties* Add quotes around ${...}* Add USE_HEXAGON_ARCH variable to cmake configuration* Restart build",1
[Pre-commit] Add pre-commit configuration to perform minimal checks locally (#8382)* [Pre-commit] Add pre-commit hook configuration file* [Pre-commit] Add header to configuratin file* [Pre-commit] Add basic configuration instructions* [Pre-commit] Extend pre-commit pipelines with C++ linting* [pre-commit] Add example usage comment for pre-commit hooks* [CI] Add in docker linting script mypy step* [CI] Use lint docker image for pre-commit checks* [CI][pre-commit] Minor cleanups on docker runners of pre-commit lints,1
Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger,1
[BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm,0
"[GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc",0
Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.,3
[Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.,0
Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a,2
[microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger,0
[AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci,3
"[Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to ""* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build",1
"[Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0",1
update gpu and cpu (#8853),1
VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings,0
[FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py,0
move rust lint script (#8726),4
[AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality,3
[TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests,0
extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>,5
Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.,4
Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>,1
"[Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>",0
"[Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.",1
"[UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.",0
Remove AOT Executor header from Arduino project (#8857),4
[Community] @mdw-octoml -> Reviewer (#8868),3
[TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,0
"[Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci",0
[Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials,0
[Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.,4
[TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue,0
[RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860),1
[Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI,1
[RPC] Fix ios_rpc build (#8864),0
"[Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. ""NVIDIA"", ""radv"", ""AMD open-source driver"") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `""unknown_driver_name""` if the driver does not supportquerying the driver name.",1
"[ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs""* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",0
refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling,0
"Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>",0
[TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>,2
[CI] make pre-commit hooks to run on every push instead of every commit (#8888),2
[TVMScript] Fix printing ForNode annotations (#8891),0
[1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055,1
[microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests,0
"[Docker] Re-enabled automatic --tty flag when running bash. (#8861)PR8382 split apart the --interactive and --tty flags, but only--interactive was set if the user opens a bash session.  This commitrestores the previous behavior of running `docker/bash.sh IMAGE_NAME`of opening a bash session with both --interactive and --tty.",5
fix error report on Store (#8895),0
[ROCm][TVMC] Add ROCm to the TVMC driver (#8896)* Add ROCm to list of RPC clients.* Add ROCm to list of TVMC devices.* Enable ROCm by adding session call.,1
[Onnx] Support Negative Log Loss (#8872)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,0
"Move to new style issue template system (#8898)* Move to new style issue template systemThis lets us have a template for each type of issue, notably this includes a template for requesting a CI image update.* Fix checkboxes* Codify the use of Discourse rather than raising issues* Change CI to CI Image and introduce CI Issue template* Fix poor english* Add more tags where we have them* CI Issue -> CI Problem",0
"[Vulkan][Topi] Parametrizing additional topi tests, marking vulkan failures (#8904)* [Pytest] Fixed TestTargetAutoParametrization in cases where LLVM is disabled.* [UnitTests][Vulkan] Improved robustness of test_tir_intrin::test_clzPreviously, would fail during build since support for Int64 primitives wasn'tdeclared in the `""vulkan""` target.  Now, uses `""vulkan -from_device=0""` targetand marks the test as xfail if the current target doesn't support Int64.* [UnitTest][Topi] Parametrized several unit tests, identify vulkan failures- Parametrized topi modules  - test_topi_conv1d_transpose_ncw.py  - test_topi_conv2d_nhwc.py  - test_topi_correlation.py  - test_topi_loss.py  - test_topi_math.py  - test_topi_reduce.py  - test_topi_softmax.py  - test_topi_sort.py  - test_topi_unique.py  - test_topi_vision.py- Unit Tests fixed  - `test_topi_loss::test_nll_loss`, failure due to `supports_float64`    not being passed from the target to the codegen.- Known Vulkan failures (tracked in https://github.com/apache/tvm/issues/8903)  - test_topi_math.py::test_ewise, [""tan"", ""erf"", ""isnan"", ""isfinite"", ""isinf""]    Unimplemented CallNode operations  - test_topi_reduce.py::test_reduce_map, [""sum"", ""any"", ""all""]    Fails during codegen, unexpected size of data type.  - test_topi_vision.py::test_proposal    Marked test_proposal as xfail on vulkan, currently has a type error    between bool/int8.  - test_topi_conv1d_transpose_ncw.py::test_conv1d_transpose_ncw    Incorrect numeric output, a few elements outside of allowed    tolerance, only occurs on vulkan backend.  - test_softmax.py::test_softmax    Marked float64 operations as xfail in vulkan, because GLSL.std.450    only supports 16/32-bit floats.",0
"Trivial uTVM -> microTVM ""spelling"" fix to align with branding. (#8905)* Embarrassingly trivial fix remove use of uTVM and replace with the proper microTVM naming convention.",0
[Community] @Hzfengsy -> Committer (#8908),3
Set default value of p in LpPool as 2 (#8866)* Set default value of p in LpPool as 2* Update test_forward.pyFix bug in test.* Update test_forward.pyupdate with correct shape.* Update onnx.py* Update python/tvm/relay/frontend/onnx.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>Co-authored-by: luyaor <luyaor@luyaordeMacBook-Pro.local>Co-authored-by: Wuwei Lin <vincentl13x@gmail.com>,0
"Enable python debug runtime for exported network libraries (#8793)* Add get_json method to graph_eceutor factorySigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Update Debugger runtime documentation for exported libraries* Fix cpplint* Change module get_json to get_graph_json, add test* Fix get_graph_json test* Change verificatino of llvm support in tet to decorator* Fix sphinx warning in debugger.rstCo-authored-by: Alexander Peskov <peskovnn@gmail.com>",0
[BUG] DataType Bug In SplitRel (#8899)* [BUG] DataType Bug In SplitRel* Add Test Case,0
"[UnitTests][Contrib] Enable contrib tensorrt/coreml unit tests (#8902)* [UnitTests][CoreML] Marked test_annotate as a known failure.The unit tests in `test_coreml_codegen.py` haven't run in the CIlately, so this test wasn't caught before.  (See tracking issue- Added `pytest.mark.xfail` mark to `test_annotate`.- Added `tvm.testing.requires_package` decorator, which can mark tests  as requiring a specific python package to be available.  Switched  from `pytest.importorskip('coremltools')` to  `requires_package('coremltools')` in `test_coreml_codegen.py` so  that all tests would explicitly show up as skipped in the report.- Added `uses_gpu` tag to all tests in `test_coreml_codegen.py`, since  only ci_gpu has coremltools installed.  In the future, if the ci_cpu  image has coremltools installed, this mark can be removed.* [Pytest][TensorRT] Mark the TensorRT tests with tvm.testing.requires_cudaPreviously, the tests had an early bailout if tensorrt was disabled,or if there was no cuda device present.  However, the tests were notmarked with `pytest.mark.gpu` and so they didn't run during`task_python_integration_gpuonly.sh`.  This commit adds the`requires_cuda` mark, and maintains the same behavior of testing thetensorrt compilation steps if compilation is enabled, and running theresults if tensorrt is enabled.In addition, some of the tests result in failures when run.  Thesehave been marked with `pytest.mark.xfail`, and are being tracked inissue #8901.",1
[Community] Add @Hzfengsy as TIR/Auto Codeowner (#8912),1
[AutoScheduler] Propogate global autotvm state to PopenPool workers (#8913)* [AutoScheduler] Propogate global autotvm state to PopenPool workers* Fix* lint,0
[ONNX] Add index_put operator (#8894)* onnx:add index_put* reformat code* add parametrize_targets* change slice to onnx_index instance* modify test_forward,1
[BUG] Shape Func of Split Op Error (#8887)* [BUG] Shape Func of Split Op Error* Convert Tab to Space* Add Test Case,0
[TensorIR][Minor] Allow Tuple/Array in TE lowering (#8916),5
Remove LoweredModule (#8886)* Remove LoweredModule* Clean up some comments* QEMU flaky tests* Don't add external functions to the LoweredFunctions module* QEMU flaky test* Respond to feedback* flaky test,1
"Sanitize names of input tensors in interface header (#8720)* Sanitize names of input tensors in interface headerChange-Id: I7f02a993887bf84316262cd2586a734a9079c338* Update tensor name sanitizer tests to parameterize them.Change-Id: I157d8d8d607de2904285e403893f146e97b510d5* Only test unpacked, C interface API, AOT caseChange-Id: I9082ae32079a1a3924c06c7f26c757aafa46dec2",1
"[Hexagon] Add trivial conv2d operator to Hexagon relay strategy (#8915)This is just to enable relay codegen for conv2d on Hexagon, not forperformance.",1
"[microTVM] Temporarily remove mps2_an521 from CI (#8927)Temporarily removing the mps2_an512 board from CI due to issue 8728.Possibly all tests can fail on mps2_an512, so removing the boardinstead of xfailing specific tests.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
"[UnitTests][Ethos-N] Mark unit tests as requiring Ethos-N (#8873)- Adds the decorator `tvm.testing.requires_ethosn`- Marks all tests in `tests/python/contrib/test_ethosn` as requiring  ethosn instead of directly checking `ethosn_available()`.  This way,  they show up as skipped rather than passing.- Marks test_compile_tflite_module_with_external_codegen as requiring  ethosn.",1
[Onnx] Turn off flaky nllloss test for now (#8919)* turn off flaky test* jostle ci* jostle ci,3
[ONNX] Add OpSet 13 implementation for Hardmax (#8924)* Add opset 13 impl for hardmax* Format,1
[microTVM] Remove Arduino aot code (#8869)* Fix Arduino DLDevice includes* microtvm_api_server fails if commands fail* Add regression test for microtvm_api_server not failing* Address PR commentsBreak error detection tests into separate fileAddress comments from MousiusRe-add necessary fixture,0
"[Relay, TOPI] Make Softmax op fusible with elemwise ops (#8909)* Change softmax op pattern to OUT_ELEMWISE_FUSABLE* Softmax is fused but x86 schedule is suboptimal* fusion properly done* Updating GPU schedule for fusion* update softmax warp shuffle schedule* fix compute_at* Bug fix in lower_thread_all_reduce when reduction storage is reused by storage_rewrite* Temp disable softmax warp reduction schedule when softmax is fused* Revert ""Bug fix in lower_thread_all_reduce when reduction storage is reused by storage_rewrite""This reverts commit 8aa340e23438a922905b1e247afa8f223f284a9c.* lint fix* try make diff smaller* fix tests* fixed another broken test* Fix flaky uTVM templating test* fix equality check on output opCo-authored-by: masa <masa@pop-os.localdomain>Co-authored-by: Gavin Uberti <gavin.uberti@gmail.com>",0
[CUDA] Improve adaptive and global pool schedule  (#8936),5
[TVMScript] Enhance printer (#8934),5
[TIR] Fixed LowerThreadallreduce not remapping Store buffer var (#8931)* Fixed LowerThreadallreduce not remapping Store buffer var* reenable warp reduction schedule for softmax with fused opsCo-authored-by: masa <masa@pop-os.localdomain>,0
"[Hexagon] Add support for linked-in model parameters (#8865)* [Hexagon] Add support for linked-in model parameters* Remove entry_func, since it's not used anywhere* Simplify linked-param codegen preparation a bit* Detect multiple linked-params functions* Add testcase to check for linked-param codegen* Empty commit to restart build",1
"[microTVM] Zephyr: Fix option name in PROJECT_OPTIONS (#8884)Fix option name for using an extra tarball when creating a new projectdirectory: it should read 'extra_files_tar', not 'extra_files'.This commit also removes the implicit C-style string concatenation inthe help strings for options 'nrfjprog_snr' and 'openocd_serial'. Italso adds a period at the end of some help strings, because some aremissing it plus help strings in TVMC use that form.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[Documentation] Document rewrite_once option (#8900),5
[Layout] Unify dense op input layout (#8921),5
[Bugfix] Add check to avoid calling back() on an empty container (#8930)* fix bug of calling back() on an empty container scope_[op->buffer_var.get()]* add test case for ConvertSSA* update the style to fix ci error* add annotation for function test_convert_ssa* update the style of test_convert_ssa to fix ci error,0
"[microTVM][Arduino] Fix Arduino Versions in RVM Build (#8938)* fix arduino cli version* fix release directory name* fix spresense release version* Revert ""fix release directory name""This reverts commit bba693212519f04005def55714b8b896f22dd9a7.",0
[CUDA] Improve local_response_norm schedule (#8946)* Improve cuda lrn schedule* fuse reduction and the next elemwise kernel* remove cpp schedule* fix* fixed unintended revertCo-authored-by: masa <masa@pop-os.localdomain>,0
"Fix printing of schedule operations (#8949)- Add printing of factor/nparts in ""split"".- Print correct operation name in ""fuse"".",0
[Relay] Add a non-recursive LetNode VisitExpr_ for LabelOps Pass to avoid stack overflow (#8917)* Add a non-recursive Let VisitExpr_ for LabelOps* fake commit to retrigger CI* fake commit to retrigger the CI* fix CI issue* fix CI issue,0
Add manupa-arm to CODEOWNERS (#8911),1
add platform to build directory (#8945),1
"Fix incorrect AOT Memory Planning (#8926)This change introduces a second memory planning phase in the AOT codegenerator once the storage rewrite pass has been completed, fixingincorrectly sized workspaces for a variety of models.It comes with accompanying tests so we can safely refactor this later.Also corrected a typo in the TE compiler regards the memory alignmentargument :smile_cat:Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>",0
"[UnitTest][Vulkan] Runnable relay unit tests on Vulkan (#8947)* [UnitTest] Added ids argument to tvm.testing.parametersThis matches the usage in `tvm.testing.parameter`, and allows forparameter sets to be referred to by a single name.* [Pytest] Fixed ordering issue of tvm.testing.parametrize_targets and known_failing_targetsIf an explicit list of targets is given, then the`known_failing_targets` decorator would fail to apply.  This commitresolves the issue, and cleans up all target-specific marks to applyin `tvm.testing.plugin._add_target_specific_marks`.* [UnitTest][Vulkan] Runnable relay unit tests on VulkanThis commit allows the relay test suite to be run targeting Vulkan with`TVM_TEST_TARGETS=""vulkan -from_device=0"" pytest tests/python/relay`.  Alltests that require a specific environment are skipped if that environmentisn't present.  All tests that are known to fail when running on Vulkanare marked as expected failure, and will be tracked inhttps://github.com/apache/tvm/issues/8903.- Failures during code generation  - Type mismatches, boolean vs int8    - tests/python/relay/test_any.py::test_any_reduce    - tests/python/relay/test_op_level3.py::test_sparse_reshape    - tests/python/relay/test_op_level4.py::test_reduce_functions    - tests/python/relay/test_vm.py::test_cond    - tests/python/relay/test_vm.py::test_simple_if  - Incorrect strategy selection, picks NCHWc implemenation for NHWC layout    - tests/python/relay/test_op_level2.py::test_conv2d_run  - Unresolved CallNode operation    - tests/python/relay/test_op_level1.py::test_unary_op[erf/tan/atan]    - tests/python/relay/test_op_level3.py::test_scatter_add    - tests/python/relay/test_op_level3.py::test_segment_sum  - Generates 64-bit calls to GLSL that have only 16-/32-bit support    - tests/python/relay/test_op_grad_level1.py::test_log_softmax_grad    - tests/python/relay/test_op_grad_level1.py::test_softmax_grad    - tests/python/relay/test_op_grad_level1.py::test_unary_op    - tests/python/relay/test_op_grad_level10.py::test_cross_entropy_grad  - Codegen raises error for variable size    - tests/python/relay/test_any.py::test_any_batch_matmul    - tests/python/relay/test_any.py::test_any_conv2d_NCHWc    - tests/python/relay/test_any.py::test_any_dense- Failures when running  - Numeric differences (observed on GTX 1650 with NVIDIA driver)    - tests/python/relay/test_op_level3.py::test_take    - tests/python/relay/test_op_level5.py::TestCropAndResize    - tests/python/relay/test_op_level5.py::TestResize1D    - tests/python/relay/test_op_level5.py::TestResize2D",0
[CMSIS-NN] code generator for softmax (#8833),5
[AutoScheduler] Fix custom build func in PopenWorker (#8939)* [AutoScheduler] Fix custom build func in PopenWorker* Add assertion,0
[ONNX] [Test] fix GRU modification and reduce tolerance for RNN tests (#8923)* fix high tolerance for RNN tests* random seed was added to GRU test reproductionCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
"[microTVM] Add method to query template info without creating a project (#8950)Add info() method to TemplateProject class so it's possible to query allavailable options for a given template project without creating a newone. This is necessary because TVMC will query the available options fora given template project to show them to the user so the user can usethem to finally create a new project dir.That is also useful in general to query the available options for anyproject type. For example, one can query all boards available on theZephyr platform with:import tvm.micro.project as project_apitemplate = project_api.TemplateProject.from_directory(ZEPHYR_TEMPLATE_DIR)boards = template.info()[""project_options""][8][""choices""]where 8 element refers to the ""zephyr_board"" option.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
[ONNX] Add support for QLinearConcat contrib op (#8907)* add qlinearconcat op* fix tests* Fix* lint* lint* review* boop ci* fix regression* noop* jostle ci,0
[Onnx] Pow support for other types (#8933)* update pow* update pow* remove duplicateCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
"[Hexagon] Fix VTCM allocation (#8954)Check if a buffer is in the `vtcm_buffers` list, before it's removedfrom it.",0
[Relay][Quantization] Per-Channel FQ2I (#8883)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Per channel fq2i (#8)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Fix bug in requantize dimension expansion.* Format.Co-authored-by: Josh Fromm <jwfromm@octoml.ai>* respond to review commentsrespond to review commentsCo-authored-by: Josh Fromm <jwfromm@octoml.ai>,0
[COMMUNITY] new committer -- giuseros (#8956),1
support slicing with out of order axes (#8959),5
[AutoTVM] Use popenpool in local_executor (#8851)* use popenpool in local_executor* move auto_tvm_common to tvm.testing* refactor* nit* remove LocalFutureNoFork* exception handling* handling two exceptions* handling error* add initiazlier,0
[2/6] Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op (#8795)* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D opThis commit adds mainly the relay passes and ethosu_conv2doperator to relay. The relay passes include the legalizationsand preprocessing of the relay graph in the paritioning.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* skipping the test if vela is not in the container.Change-Id: I68cc4259dc33e1473e460956978f364fbf6596d8* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* addressing Jared's commentsChange-Id: Ief669f788c6bd1a1be1004cbce5129ed06b63c3c* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* addressing Elen's commentsChange-Id: Iad6315bb63f12ba318deb9c5c9eff7459ff58c48* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* cleanup passesChange-Id: I8e1cbedd2c4d3d0cdff481d775d9eb0577e44456* Update TE commentsChange-Id: I7e65c2714d017c8a4b64986b111a6b51d128c963* Address ekalda's comments in TEChange-Id: I55cfbb3787c0aacdadf46c4859dff39287e65ddc* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*addressing chris's comments*addressing Nicola's commentsChange-Id: Id02788ddcdbc3679e0da37b2fa614cded0a4c1f5* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*addressing missed 'hidden' comments of Chris*addressing one missed comment of Nicola*adding type annotationsChange-Id: Iadf4907b311e195731dbbed571e95a266341db8f* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*further type fixes and one missed commentChange-Id: I6da69fd95d17dfeaf5940da4f8d8c8ea142b39d2* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*missed comment split_oChange-Id: I4a4b19ff2cd18e8f568a63ae827f44358ed85b8e* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* adding mypy checkChange-Id: Iaf58dbba2a9d8e1098a10c589d91b63c7efe646d* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op*removing premature insertion of get_accel_type utilityChange-Id: I210512e00a5eb46adf23d1d72eb16432db526d25* Arm(R) Ethos(TM)-U NPU Relay passes and Conv2D op* rebase fixesChange-Id: I06c9b536a7598646efce2b664fcc405aa6008203Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>,0
[Bugfix] Fix visit_attrs error if its function pointer is equal to nullptr (#8920)* fix visit_attrs equals nullptr on python container object* add a test a for python container object about function dir and getattr* change test_ir_container.py to the pytest style* update the style to fix ci error* update the style of ir container to fix ci error,0
"Set tvm.micro.project_api as a Python Module (#8963)* Add missing tvm.micro.project_api module file. The missing  __init__.py makes it impossible to import this module with  `import tvm.micro.project_api`.* This uncover 30-ish linting errors, which are also fixed here.",0
[Bugfix] Add a nullptr check to tir.Buffer to fix the illegal memory access (#8910)* fix wrong log of tir pass VerifyMemory* fix a typo of convention* add a nullptr check to tir buffer* add test case to trigger buffer nullptr bug* update the style to fix ci error,0
[TOPI] Fix CUDA pooling schedule (#8957),0
[PROFILING] Profiling over RPC (#8885)* [PROFILING] Profiling over RPCAllow for profiling over RPC by serializing the returned report beforesending it. Also remove collectors argument when profiling over rpcbecause it cannot be serialized.* lint* fixes* add comments,0
Add sse4/avx2 support for fast x86 int8 (vpmaddubsw/vpmaddwd/vpaddd) (#8897)* Add sse4/avx2 support for vpmaddubsw/vpmaddwd/vpaddd- Extend the list of different target for x86 topi- Extend tests for conv2d x86 int8 for fast i8 x86 platforms* fix code style* Change x86-64-v2 to nahalem in test to support llvm11* Change test target to get NCHW8c,0
[EZ] [ONNX] Remove unnecessary converters for greater and lesser (#8967)* simplify by removing unneeded conversions* remove uneeded* remove testCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,3
[ONNX] Support depth_to_space op for FQ2I  (#8966)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Per channel fq2i (#8)* WIP support per-channel quantization* more WIP* More WIP* fix issue with per-channel bias_add* Fix fake quantize tests (#4)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Add Relu* One more little one (#5)* Fixed fake quantize issues.* Formatting.* Cleanup unused imports* Fix real int8 tests.* Fix requantize shape bug.* Non-working Per-channel Dense* Fix legalization for non spatial operators. (#6)* Fix legalization for non spatial operators.* Fix axis checks for end2end functionality.* fix axis normalizationfix lintfix lint again* Fix bug in requantize dimension expansion.* Format.Co-authored-by: Josh Fromm <jwfromm@octoml.ai>* respond to review comments* start dtos* wip depth_to_space* dtos identCo-authored-by: Matthew <mbrookhart@octoml.ai>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>,0
change the doc to reflect previous code change (#8970),4
"[LLVM/CG] Sort PrimFuncs when creating LLVM module (#8958)* [LLVM/CG] Sort PrimFuncs when creating LLVM modulePrimFuncs are stored in a map where the order of iteration is notdeterministic. This can cause a different llvm::Module to be createdeach time, which can defeat debugging tools like -opt-bisect-limit.Add function CodeGenLLVM::AddFunctionsOrdered that takes a range ofPrimFuncs or objects convertible to PrimFuncs, and adds them to theLLVM module in a deterministic order.* Empty commit to restart build* Add testcase",0
[Bugfix] Fix div zero error in rewrite_simplify (#8961)* fix div zero error in rewrite_simplify* update the style to fix ci error* remove useless code and comment,0
flaky off (#8972),5
"[microTVM] Add support for AutoTVM (#8715)* Initial commit of API server impl.* initial commit of api client* Add TVM-side glue code to use Project API* Change tvm.micro.Session to use Project API* Rework how crt_config.h is used on the host. * use template crt_config.h for host test runtime; delete   src/runtime/crt/host/crt_config.h so that it doesn't diverge from   the template * bring template crt_config.h inline with the one actually in use  * rename to MAX_STRLEN_DLTYPE * Create a dedicated TVM-side host crt_config.h in src/runtime/micro* Modify Transport infrastructure to work with Project API* Add host microTVM API server* Zephyr implementation of microTVM API server * move all zephyr projects to apps/microtvm/zephyr/template_project* consolidate CcompilerAnnotator* Allow model library format with c backend, add test.* Update unit tests* fix incorrect doc* Delete old Zephyr build infrastructure* Delete old build abstractions* Delete old Transport implementations and simplify module* lint* ASF header* address gromero comments* final fixes?* fix is_shutdown* fix user-facing API* fix TempDirectory / operator* Update micro_tflite tutorial* lint* fix test_crt and test_link_params* undo global micro import, hopefully fix fixture* lint* fix more tests* Add session_constructor_args to tracker request() function. * Allows tracker clients to open non-traditional RPC sessions* Generate entry_func symbol in C host codegen. * Needed for AutoTVM.* print MeasureErrorNo enum value in MeasureResult repr* Add microTVM session constructor. * This constructor is to be called from the RPC driver to flash and   connect to the RPC server on the microcontroller.* add build_kwargs as a Builder constructor arg. * build_kwargs is derived from pre-configured args, the runner, and   now from the script. * user-supplied build kwargs override the other two, and a warning is   printed if any key is overridden.* Add do_fork option to Builder, to support stateful builders * When AutoTVM builder forks, any global state modified by the   build_func is lost between builds* Checkin module_loader used to build and flash microTVM for autotuning.* Import micro into top-level when enabled. * AutoTVM RPC server needs to load the micro session constructor.* Add tvm.contrib.random.random_fill to microTVM. * Allows autotuning with random data.* Move compilation to runner :O* Add a tutorial for AutoTVM with microcontrollers.* Fix si_prefix in autotuner callback* black format and git-clang-format* Switch tutorial back to qemu version* improve error reporting so CI will show test error* black format* autotvm is working* fix tutorial* fix dependencies* fix auto tune issue* lint* address comments* fix lint* test crt and zephyr added* fix func registery size* moved autotune test and fixed* fix crt test* address comments* change relay text* change relay in text_zephyr* class added* changed relay module in tutorial and cleanup* address comments* address TK comments* change fork* final comments* retrigger due to flahy test* fix tutorial* retrigger* fix changes due to mergeCo-authored-by: Andrew Reusch <areusch@octoml.ai>",0
[TIR][VM] Revert a change to lower_tvm_builtin.cc from #6126 (#8274)* revert a change to lower_tvm_builtin.cc from #6126* disable sim target on VTA tutorialfix bad refactortry again,0
[RPC][IOS] Add random to ios_rpc (#8935)random_fill is used in measure.py,1
"[TensorIR][M2a] Compute-At (#8943)This PR is part of the TensorIR upstreaming effort (#7527), which adds the following schedule primitives:* `compute-at`* `reverse-compute-at`Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",1
[microTVM] Refactor `platform` used as board name in microTVM (#8940),5
"[microTVM] Zephyr: implement 'west_cmd' server option (#8941)Currently Zephyr Project API server lists option 'west_cmd' as anoption available in Zephyr platform by advertising it in PROJECT_OPTIONSbut that option is not used by any API method.That commit adds that option to the server as a non-required option tothe build() interface method, allowing the user to specify analternative path to the west tool. If that option is not specified theZephyr build system takes care of searching for west as a module (sorelying on West being available on Python, i.e. relying on'python3 -m west').Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
[microTVM] Zephyr: Set 'choices' for ProjectOption 'verbose' (#8968)Set 'choices' tuple returned for ProjectOption 'verbose' as a doubleof True and False so TVMC and other interfaces can easily determineit's a boolean option.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,2
[microTVM][Zephyr] Hot Fix Bad Merge (#8980)* fix bad merge* Fix test to reflect change of simd size for ARM,0
add github issue template for docs (#8982),1
[CI][VitisAI] Update CI Vitis AI PyXIR version to v0.3.1 (#8814)* Update CI Vitis AI PyXIR version to v0.3.1* Add Vitis AI requirements to gen_requirements.py,1
[AutoScheduler] Fix task scheduler after 8478 (#8984),0
Clean up LowerTEPass and pass IRModule Attrs through passes (#8914),4
Update TVM VTA (VTA Chisel Wide memory interface) (#8973)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* Update to latest VTA,0
Make expressions in the DynamicToStatic pass tests more dynamic (#8989),3
[Relay][UnitTest] Removed redundant unit test. (#8993)test_op_level2.py::test_conv2d and test_any.py::test_any_reduce shouldhave been removed in the refactoring in #8947.  All functionalitytested by it is in test_op_level2.py::TestConv2D andtest_any::TestAnyReduce.,3
[Autoscheduler] Reduce task weight coercion overhead (#8995)Co-authored-by: Peter Salas <psalas@octoml.ai>,5
[Relay][Op] fix conv transpose weight dtype inference (#8962)Fixes type inference error when data and weight have different dtype,0
[DOCS] Update code review guideline (#8999)This PR updates the code review guidelineper community discussion.,1
[Onnx] Add Adagrad (#9001)* adagrad impl* passing tests* docstringCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
[microTVM][Zephyr] Fix board names (#8998)* fix board names* fix base_box test* fix tutorial,0
Add while node support in TVMScript (#9004)* support while* update synr version,1
[CMake] Corrected warning message about USE_GRAPH_EXECUTOR_DEBUG (#9006)Warning message about deprecated cmake flag `USE_GRAPH_EXECUTOR_DEBUG`referred to `USE_GRAPH_EXECUTOR` instead.,0
[Hexagon] Add contrib tests for blocked conv2d and maxpool2d (#8960)* Add hexagon contrib tests for blocked conv2d and maxpool2d* Restructure based on review comments,1
"[microTVM][AutoTVM] Fix autotvm bug and tests (#9003)* debuggging* cleanup and fix tutorial, zephyr and crt test* fix crt test* address comments",0
[TIR] Add conversion from FloatImm to float in Python (#9009)This method matches the IntImm method for converting from IntImm to int.,1
Move external codegen test helpers into utils (#9008)This is so they can be re-used as part of other tests which don't extend test_external_codegen.pyI've identified that `test_json_runtime.py` and `test_pass_partition_graph.py` use a very similar but slightly different variant of these functions for future iterations.,3
[Onnx] Add Adam (#9002)* add adam op* lint* remove tests* lint,1
[3/6] Arm(R) Ethos(TM)-U NPU TIR compiler with conv2d support (#8806)* Arm(R) Ethos(TM)-U NPU TIR compiler with conv2d supportThis commit adds the lowering passes necessary to loweran NPU Relay module down to a TIR module that can becompiled for the NPU. Conv2d is supported as the firstNPU operator. An intermediate TE stage between Relay andTIR allows support for scheduling the operators.Co-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>* Fix Conv2D TIR type sensitivityChange-Id: I3741f9dd8bb5952590ff8c586f6b96e5c3a03795* Arm(R) Ethos(TM)-U NPU TIR passes and TE for Conv2D*fixing testsChange-Id: Id4a4c80f72ce29b98fc8b3954a1413c1c7fda500* Fix import guards for testsChange-Id: Iaee06017bd125d3040ce42182c4ccdb80d7fc946* Fix typing failures with ignoresChange-Id: I81513f112a42b93cfdd3bcaf8e8852dd60ffe9e9* Remove unused importChange-Id: I6596b62ab56e4ca8b31ef08293686f53f38454d2* Reintroduce get_target_accel_typeChange-Id: I0aaf83fe0204c0db435692e9b92dee6e9d6997feCo-authored-by: Manupa Karunaratne <Manupa.Karunaratne@arm.com>,0
[Torch] Add an option to make imported models compatible with the Relay text parser (#9015)* [Torch] Add an option to make imported models compatible with theRelay text parser* py format,1
disable cuda int8 schedule for non-cuda gpu target (#9014),5
[Hexagon] `llvm-options` attribute is an array of strings (#9011)Change the type from String to Array<String> in the code that looksthe attribute up.,4
"[Runtime] Pipeline Executor Initial patch. (#8702)* [Runtime] Pipeline Executor Initial patch.This patch is one of serial patch for PR 7892 splitting.this is the initial partof the pipeline executor, this patch include the cmake change and python and C++interface for pipeline executor.* add pipeline config* add config connect logic.* fix build issue.* set output index start from 0* address review commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* address review comments.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* address review comments* add topology sort* add binding check logic.* fix plint error.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* address review comments.* fix plint issue.* address review comments.* trigger build.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>address review comments* address review comments.* polish doc and comments.* polish doc and address review comments.* address review comments.* doc change.* doc change.* Trigger build.* trigge build.* address review comments.* address review comments.* address review comments.* polish documents.* Polish the document.* address review comments.Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
"[Hexagon] Treat floats as float32 when passing args to offloaded kernels (#9010)`TVMArg` can hold a floating point value, but it's stored as `double`. InHexagon ABI doubles are passed in a register pair, but if the offloadedfunction was using floats (i.e. float32), it will expect values beingpassed in single registers. Since floats are much more common on Hexagon,assume all scalar floating point values are floats. This is only an issuewith offloading, and can be treated as a limitation (we do somethinganalogous for integers already).",2
[Relay] Remove memory planing from LowerTEPass  (#8974)* Clean up LowerTEPassAdd attrs to IRModule equal and hashMake LowerTEPass opt_level 0Copy IRModule attrs to per_target_modulesAdd ShallowCopy to IRmodule* Fix rebase* Remove comment* [TEC] Remove memory plan from LowerTEPass* Fix linting errors* Fix PR comments* Remove updated module with function info from LowerTe* Refactor UpdateMainWorkspaceSize to update func info independently from LowerTEPass* Fix aot failed tests* Revert whitespaces fixes* Remove obsolete function hoisting and minor cleanups* Address PR commentsCo-authored-by: electriclilies <lilyorthsmith@gmail.com>,0
"Add standalone_crt/ to be part of the wheel package, when available. (#9005)* When using a packaged TVM such as tlcpack, it is impossible to run  `tvm.micro.get_standalone_crt_dir()`, because the subtree  `standalone_crt/` is not available.* This patch adds `standalone_crt/` as `data_files`, so that they  can be picked up by _ffi.libinfo.find_lib_path() and therefore  be found when `tvm.micro.get_standalone_crt_dir()` is invoked.",1
[ONNX] Add Einsum converter (#8985)* einsum* address review* move files around* use generic topi op* TODO comment* jostle ci* jostle ci,1
[BYOC][TensorRT] Add TensorRT own int8 calibration support to TensorRT BYOC integration (#8808)* update trt* clean codes* tetsing running trt* clean data* clean codes?* remove env func* fix num_bings* add buildfromjson func* change condition* reset input and output func* re-config func* re-added trt version check* checking sanity* try to fix sanity issue* checking sainity* fixing sanity issue* fixing sainity issue* fixing sanity* clang format fixed* clang format fixing* clean trt cali* try to fix clang format* fixed some comments* remove double destroy engine codes* modify comments* add checking function* add trt int8 test* update trt int8 test file* Update test_tensorrt_int8_exp.py* update trt int8 fikle* change a little* upate trt int8 file* upate trt int8 file* fixing ci* fixing ci* fixing ci* fixing ci* fixing ci* fixing ci issue* fixing ci issue* fixing ci* fixing ci issue* fixing ci* fixing ci problem* fixing ci* upate trt python int8 test file* fixed ci* fixed ci* fix gpu build* fixed ci* update trt int8 test file* fix bug* fix bug* update trtint8 file* reformat* update trt int8 file* update* modify,0
[Relay][Pass] Add ExtractOperators pass (#8996)* add extractor* extract to array* add comments* lint* Update tests/python/relay/test_analysis_extract_operators.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* op freqs* add comment* Update python/tvm/relay/analysis/analysis.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* oops* mixedmode visitor* oopsCo-authored-by: Cody Yu <comaniac0422@gmail.com>,1
"[Hexagon] Implement model launcher (#8986)* [Hexagon] Implement model launcherThis implements a launcher that allows execution of ML models compiledinto a shared library on Hexagon DSP. It consists of two parts: theHexagon-side skel library and `launcher_android` to be used from`adb shell`.The launcher does not implement any performance-related optimizations,it's built on top of the `graph_executor` from TVM runtime, and so itexecutes a single layer at a time. This launcher should not be used tomeasure performance (because if will be highly suboptimal), its mainpurpose is to help in validating correctness.* Address review comments: explanations and elaborations in README.md* Rename cmake variables to be same as for TVM- `HEXAGON_SDK_ROOT` -> `USE_HEXAGON_SDK`- `HEXAGON_ARCH` -> `USE_HEXAGON_ARCH`* Address more review comments* Error out in cmake when USE_HEXAGON_SDK/USE_HEXAGON_ARCH are undefined* Change FATAL_ERROR to SEND_ERROR in cmake file",0
[Community] @AndrewZhaoLuo -> Reviewer (#9020),3
fix (#9021),0
[Onnx] Add momentum (#9000)* add momentum* make tests pass for momentum* blacking* lintCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
"[Hexagon] Allow undefined symbols in libtvm_runtime.so on Hexagon (#9024)The shared library libtvm_runtime.so (or any other shared library builtfor Hexagon) will not contain definitions of symbols from libc. To avoidundefined symbol errors, turn that check off when building shared libsfor Hexagon.",0
"[Hexagon] Disable `thread_local` on Hexagon (#9025)This is specific to running code on hardware: libc++abi can createTLS keys with destructors in the libc++abi library. Despite that,the library gets unloaded before the keys are destroyed, leadingto a crash. Turning off the use of `thread_local` is a workaroundfor this.",5
[ONNX] enable the onnx tests after PR #8274 merged (#9019)* enable the onnx tests after PR #8274 merged* fix lint,0
"[Bugfix] Fix other div zero errors also in rewrite_simplify (#8983)* fix div zero error in rewrite_simplify* update the style to fix ci error* remove useless code and comment* fix div zero error of mod, floordiv, floormod in rewrite_simplify* rewrite the test case of divison by zero to fix ci error* remove useless tab* retrigger ci* remove useless blank to retrigger ci",0
"[Onnx] Fix NLL Loss tests (#8971)* support negatibve indices in gather* move check to Tensor level indexing, gathernd* add test, update transform.h* remove unneeded gather* missing gather nd change* update tests* proper tensor comparison* blacking* lint* fix error* turn on test* missing test case* revert changes* add normalize_gather_indices* undo change* update* more removing diffs* more undoingCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",0
"Implementation of relay_to_tir target hook (#8423)This the first new hook proposed in the Additional Target Hooks RFC, longerterm the compilation should move to using `Target` proper but this unblocks our current work whilst illustrating the eventual interface via `Target` in `src/relay/backend/contrib/example_target_hooks/relay_to_tir.cc`Ideally the host target would be annotated onto the `IRModule` so as this `Pass` could use it instead of defaulting to C but this is fine for now.",1
[CUDA] Fix dense tensorcore legalize type error when units is specified (#9030)* Fix dense tensorcore legalize type error when units is specified* revert black change due to different version from CI,0
[ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op (#9017)* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op* Fix linter error for variable name and else after return* Separate quantized avg_pool impl and add TODO for global_avg_pool* Fix comment typo,0
Fix line break in `setup.py` (#9029),0
"[Onnx] Add SoftmaxCrossEntropyLoss (#8906)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger* [BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc* Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.* Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a* [microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger* [AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci* [Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to ""* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build* [Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0* update gpu and cpu (#8853)* VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* [FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* move rust lint script (#8726)* [AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality* [TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests* extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>* Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.* Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* [Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>* [Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>* [Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.* [UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.* Remove AOT Executor header from Arduino project (#8857)* [Community] @mdw-octoml -> Reviewer (#8868)* [TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* [Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci* [Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials* [Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.* [TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue* [RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860)* [Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI* [RPC] Fix ios_rpc build (#8864)* [Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. ""NVIDIA"", ""radv"", ""AMD open-source driver"") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `""unknown_driver_name""` if the driver does not supportquerying the driver name.* [ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs""* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>* refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling* Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>* [TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>* [CI] make pre-commit hooks to run on every push instead of every commit (#8888)* [TVMScript] Fix printing ForNode annotations (#8891)* [1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055* [microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests* sce loss example* add comments, remove other tests* lint* lint* jostle* lint up* jostle* uncomment some tests* proper return* clean up* lint* minor merge errorsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Michalis Papadimitriou <mikepapadim@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>",0
"[Hexagon] Don't use {} initialization with FastRPC structures (#9033)The data members in FastRPC structures aren't guaranteed to remainin the same order. Replace aggregate initialization with direct,member-by-member initialization.",5
[microTVM][autoTVM] Follow up fixes to #9003 (#9018)* fix test and cleanup* fix tutorial doc* fix verbose for tutorial* fix tune check* address comments* address comments,0
Add bindings for StaticMemoryPlan and DensePackAttrs (#9034)* add missing python binding for DensePackAttrs* make bindings for memory planning usable in python,1
[ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear (#9028)* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear* [ONNX][#8838] QLinearSigmoid contrib op and Bug Fix for DequantizeLinear,0
Support match pvar with dtype constraint (#9016),5
[3/10] Moved TIR generation from Python to C++ for CMSIS-NN (#8951)* [CMSIS-NN] Moved TIR Generation to C++* Deleted self import for cmsisnnChange-Id: I2cdcd7a90aa4749877c48bc6c7c4d27328856860* Reusing CodeGenC VistiExpr for softmaxChange-Id: Ie41b695fa06468cd3b0bfe428c360e98438a9180,4
[Meta Schedule][M3b] Builder (#9044)* [Meta Schedule][M3b] BuilderThis PR is part of the meta schedule project (https://github.com/apache/tvm/issues/8473)Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* add typing* unreachableCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,1
Add back missing __init__.py to unbreak CI. (#9052),1
[BYOC] Fix build with TensorRT 8 (#9047)* fix compile error missing noexcept in overwridden methods* remove depricated builder method call,0
"[Relay] VLOG for finer grained control of hyper-detailed logging (#9012)* [Relay] VLOG for finer grained control of hyper-detailed loggingI've been making very heavy use of DLOG and PrettyPrint to trace, understand anddebug Relay transforms. I've found myself deleting log statements to reduce theoutput verbosity, only to have to re-add them a few days later. Better would be tosupport leaving all those statements in place but have finer control over when theyare enabled.This PR introduces a 'VLOG(level)' macro to that end. The log is ignoredunless TVM_LOG_DEBUG is enabled (both as #define and an environment var), and thethe current verbosity level is >= level. The current verbosity level can be setglobally and/or overridden per source file (see 'VerboseLoggingEnabled').(Those familiar with the origin of the LOG and DLOG family will also recognize VLOG.)I also introduce a 'VLOG_CONTEXT' macro which pushes a string onto an internalper-thread stack. Each VLOG message includes that stack as its prefix, which is avery handy way to keep track of the (often recursive) program context in which eachVLOG is executed.I've rejigged some existing DLOGs to VLOGs to illustrate, but left most of themalone for now.  See the draft https://github.com/apache/tvm/pull/8788 for use in thewild.I noticed the DCHECK macros *disabled* instead enabled with TVM_LOG_DEBUG defined, sofixed that.I've also made changes to the Relay text printer to dump attributes in a humanreadable format rather than the indirect but machine readable 'meta' representation.This is gated by the show_meta_data_ flag, and I think this use is consistent with it'soriginal purpose.* [checkpoint] lints* [checkpoint] missing \n lintGotta get my docker setup going* [checkpoint] woops, we don't support gmock.h* [checkpoint] Address Hua Jiang's comments.* [checkpoint] strlen not avail on all toolchains?* [checkpoint] Rework TVM_LOG_DEBUG spec and parser* [checkpoint] woops, forgot the static modifier on map* [checkpoint] * -> DEFAULT for wildcard.Andrew pointed out *=9 suggests foo/*.cc=9 would work but it is not supported.* [checkpoint] constexpr length* [checkpoint] length is not constexpr on all targets, reverting* [checkpoint] minimize VLOG overhead when in legacy DLOG-only mode",0
"Migrate flake8 from workflow to lint script (#9062)Saw https://github.com/apache/tvm/pull/9055 adds `flake8.sh` to the `docker/lint.sh` and remembered I started doing this in https://github.com/apache/tvm/pull/8652, but never updated it after the Docker images were actually updated.",1
[Meta Schedule][M3a] TuneContext (#9053)* Add TuneContext class.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add tune context test.* Add meta_schedule to cmake.* Add type.* Rebase.* Disable MyPy for ethosu.* Add new line.* Remove duplicate line.* Minor fix.* Add comments.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
[ONNX] LessOrEqual and GreaterOrEqual ops (#9066)* recreate things* jostle,5
bump tolerances (#9054),5
[Support] Add `parallel_for_dynamic` with dynamic schedules (#9056)* [Support] Add parallel_for_dynamic with dynamic schedules* Update parallel_for.cc* Update parallel_for.cc,1
Add flake8 to docker/lint.sh (#9055),1
Complete docs. (#9070),2
[Meta Schedule][M3c] Argument Info (#9059)This PR is part of the meta schedule project (#8473) that adds metadata of each PrimFunc's argument.This feature is necessary for dynamic shape auto-tuning.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,1
[PYTHON][FFI] Speed Up get DataType (#9072),5
[iOS] Add tracker support into ios-rpc application (#7876)* [IOS-RPC] Missprint in flag valueSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] custom_dyld up commit id. Fix mem leakSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] build without schemeSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Add tracker support into ios-rpc appAlso containes:* Links with tvm_runtime.dylib* Minor improvements from UX perspective* Add cli args support* Add caching for url/port/key attributesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] lint fixSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Uniform serversAlso:- Disabled bit-code- Enabled ARC- Use custom DSO loader by default- Single button to connect/disconnect- Add verbose flagSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS_RPC] Min changes. Fix warningsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [IOS-RPC] Fix review commentsSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* Fix RPC connection to tracker* Fix typo* Fix build for local developer profile* Add tvmrpc xcode scheme* Revert unnecessary change* Remove old mechanism of reloading libs* Display ip and port for PureRPC mode* Update tests* Remove tvmLauncher from ios_rpc* Add updating tvm_build_dir in init_proj script* Update default bundle* Update README.md for ios_rpc* Fix lint* Apply comments* Rename PureRPC to StandaloneCo-authored-by: Egor Churaev <egor.churaev@gmail.com>,0
Ensure AOT passes all intermediary storages to function calls (#9064)* Ensure AOT passes all intermediary storages to function callsThis iterates over the return storage IDs rather than just using thefirst one to ensure all of them get passed to subsequent calls.Fixes #9036* Re-introduce multi sub graph AOT test,0
"[AutoTVM, Auto scheduler] Always use VM compiler for task extraction (#9069)* Always use VM compiler for task extraction* remove unused import* update task weights reference* Call op weights update callback from VM compiler* Revert ""update task weights reference""This reverts commit 61ca552da3c25ed239e8c84c599a3f0f7923c0ff.* add doc",1
"[ONNX][Relay] Add dynamic unsqueeze / expand_dims op (#9039)* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* working tests* add test, remove unneeded file* fix things* more lint* more lint* pick things* disable opencl tests* unsqueeze tests* clean up* dyn stuff* add num_newaxisCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",0
[Hexagon] Pytestify Hexagon unit test (#8955)* [Hexagon] Pytestify Hexagon unit test* Fix formatting* Convert linker registration into pytest fixture* Use yield in pytest fixture* Restart CI,0
[ONNX] Add Compress Support (#9067)* compress impl* unit tests* docstringCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
[ONNX] enable more `*_expanded` tests  (#9051)* enable more sce tests* more testsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,3
Update ubuntu_install_paddle.sh (#9082),1
[CUDA] Swap block x and z dimension for conv2d NHWC schedule (#9087),5
[microTVM][Zephyr] Add 'config_main_stack_size' option to API server (#9026),1
"BUG #8013: Remove register_alter_op_layout example from dev/use_pass_infra.py (#9076)* BUG #8013: Remove register_alter_op_layout example from dev/use_pass_infra.pyThis tutorial registers a global layout transformation for conv2d for alltargets which is not well-formed. Later uses of conv2d in the tutorialspick that layout up then assert fail in the conv2d type-relation.Better would be to register a transform for an entirely fake target, butthat is beyond my current level of expertise.In general our use of sphinx/sphinx_gallery for running and rendering thetutorials is highly suspect since there is no inter-example isolation: - Examples using tensorflow will gobble up GPU memory and not give it back. - Any examples which use any of the (many!) global registration mechanisms   need to ensure the registrant is safe across all tutorials.I recall seeing a thread with the sphinx_gallery where they said they'd prefernot to work on process-level isolation, but it's probably worth pinging again.While digging into this I noticed we had a slicing cast in AlterOpLayout dueto a derived class of ObjectRef introducing virtuals. I moved the virtuals tothe corresponding Node classes. In this case we got away with it since theObjectRef happened to not get copied but we were on very thin ice.* [checkpoint] Woops, forgot there was an extra AlterOpLayoutI should have run locally, there goes 6hrs of CI.",0
[Auto-Schedule][Fix] Fix hang while tune model through rpc (#9032)* [Auto-Schedule][Fix] Fix hang while tune model through rpc* Fix problem with hang by using deep copy* Fix with local args* Update python/tvm/auto_scheduler/measure.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>,0
[BYOC] Fix DNNL Conv2D in JSON runtime (#9043)* dnnl memory dim is wrong for conv2d* change test case for dnnl conv2d* trigger CICo-authored-by: sunway <sunwayforever@hotmail.com>,0
[Relay] Fix compiler warning in ExtractOperators (#9075)* [Relay] Fix compiler warning in ExtractOperatorsFix clang warning:  'OperatorExtractorWrapper::VisitExpr_' hides overloaded virtual functions* Restart CI,0
add paddlepaddle to python/gen_requirements.py (#9098),1
Support colons inside of TVMC input shape name arguments (#9080),2
"Add `extern ""C""` to C Interface API header (#9094)This is to provide the hint to C++ compilers that these functions are C linkage.New header looks similar to:```c++extern ""C"" {/*! * \brief Input tensor pointers for TVM module ""default"" */struct tvmgen_default_inputs {  void* y;};/*! * \brief Output tensor pointers for TVM module ""default"" */struct tvmgen_default_outputs {  void* output;};/*! * \brief entrypoint function for TVM module ""default"" * \param inputs Input tensors for the module * \param outputs Output tensors for the module */int32_t tvmgen_default_run(  struct tvmgen_default_inputs* inputs,  struct tvmgen_default_outputs* outputs);}```",1
Move the allocates of AoT codegen to be TVMBAWs (#9065)* Move the allocates of AoT codegen to be TVMBAWsThis commit introduces changes to aot_executor_codegen.ccto place tir.allocate to use storage_scope = 'global.workspace'.The lower_tvm_builtin pass is modified slightly to generateTVMBAW calls.Change-Id: Iba4ba437c1431c5197bf11abb826e03807bbcf66* Move the allocates of AoT codegen to be TVMBAWs*Adding more comments and descriptions*Modified the test case to use primitive relayChange-Id: Ia18a169d94bded3f81af7b3081c7d1ac29c669bc,1
[PYTHON][FFI] Skip numpy.ascontiguousarray if C_CONTIGUOUS == True (#9073),5
[TFLite] Fix padding calculation in Transpose Conv (#9089)* [TFLite] Fix padding caculation in Transpose Conv* [TFLite] Fix padding calculation in Transpose Conv* [TFLite] Fix padding calculation in Transpose Conv* remove unused variables,0
"[Relay] Prepare for merging context_analysis.cc and device_annotation.cc (#9077)* [Relay] Prepare for merging context_analysis.cc and device_annotation.cc- Improve construction and deconstruction of ""on_device"" and ""device_copy"" calls since they will be center stage.- Move ""device_copy"" support out of memory.h into own module to mirror ""on_device"".- Clearing out some DLOG -> VLOG changes I found helped me debug.- Clearing out some whitespace-only changes I accumulated.* [checkpoint] Address Christopher's comments.Some stray py formatting changes snuck in since I just run black . at the root.",0
[Frontend][TFLite] fix #9078 (#9099)Co-authored-by: sunway <wei.sun@hexintek.com>,0
[BYOC] Fix incorrect conv2d padding handling of `dnnl with c source runtime` (#9097)Co-authored-by: sunway <wei.sun@hexintek.com>,0
adding Jorn to reviewers list (#9105),1
[TensorIR][Bugfix] Disallow fusing loops with dependency (#9112)* check dependency for fuse* blank line,0
[Meta Schedule][M3a] SpaceGenerator  (#9079)* Add meta shedule space generator.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Clean up.* Minor fix.* Move utils.h.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
relu of dnnl json runtime only support 4-dims input (#9122),5
add `multiply` and remove `subtract` for dnnl json runtime (#9120),1
Fix the missing `dtype` attribute of `tir.Shuffle` in Python level (#9131),0
"[Relay] Register layout conversion function to more reduce ops (#9048)* Register layout conversion function to more reduce ops* bug fix for exclude=True case, the original code compute wrong axes* properly handle variance op, which has two inputs* update test expected output",0
fix annotation of tir generic (#9119),0
prevent casting handle to other types (#9114),5
"[LLVM] Refactor MakeCallPacked, NFC (#9118)Change the interface for `MakeCallPacked` in `CodeGenCPU` and in`CodeGenHexagon` to encapsulate the multiple returned values intoa single structure. This should help readability, but also it willmake the upcoming adoption of opaque pointers a bit easier.",4
Frontend: add onnx GlobalLpPool op (#8845)* Frontend: add onnx GlobalLpPool op* update* fix for testCo-authored-by: xp224797 <xp224797@alibaba-inc.com>,0
Arm(R) Ethos(TM)-U NPU TIR to CS for Conv2D (#8811)This commit introduces the TIR to Command Stream(CS)translation using Vela API calls for conv2D and copy operations.It will create Vela npu_op objects for each command.Change-Id: I906d2cb333652813142cc70fb39b8372ec498bd0,4
"Ensure google-mock is installed and setup (#9107)Google Mock is the mocking/helper framework that gets bundled with Google Test, it used to be separate but now isn't.  I ran into the issue of Google Mock not being configured fully in the i386 build of #9106, which uses the `HasSubtr` matcher. This PR aims to fully configure Google Mock for use, which is interesting in itself...The headers are installed as part of Ubuntu 18.04's `googletest` package:```shell$ dpkg -S /usr/include/gmock/googletest:amd64: /usr/include/gmock```But not the lib sources, that requires another package named `google-mock`:```shell$ dpkg -S /usr/src/gmockgoogle-mock:amd64: /usr/src/gmock```But in Ubuntu 16.04 the includes and lib sources are in the `google-mock` package:```shell$ dpkg -S /usr/include/gmockgoogle-mock:i386: /usr/include/gmock$ dpkg -S /usr/src/gmock/google-mock:i386: /usr/src/gmock```And excitingly, in Ubuntu 20.04 this will again be changed to `libgmock-dev` by thelooks of things, just to keep us on our toes.",2
"[CI] bash.sh, build.sh: add option to set the container name and hostname (#9110)This commit adds option ""--name"" to bash.sh and build.sh to enable the userspecify the name of the container and set the hostname inside thecontainer as well.This helps the developer idenitfy that they are inside the containerand which container they are working inside.",1
"[Codegen] Swap out analyzer when outlining (#9117)Problem: the `analyzer_` in `CodeGenLLVM` and derived classescan generate invalid code for outlined functions.Consider code like this:  let x = y in    // attr compute_scope    blah = xThen it gets outlined in codegen_cpu (for example):  let x = y in    call foo(x)  foo(x) {    blah = x  }Now, if `analyzer_->Simplify` was run on the body of `foo`, itwould produce:  foo(x) {    blah = y  }Because the `analyzer_` knows that `x` is same as `y` (becauseof the `Let` statemement), but doesn't know that `y` is no longeravailable in the outlined function `foo`.Seehttps://discuss.tvm.apache.org/t/compute-scope-issue-with-analyzer-invalid-simplification/11111",2
[Bugfix] Add nullptr checking for `AttrStmt` with `coproc_uop_scope` attr key (#9123),0
"[Meta Schedule][M3b] Database (#9061)This PR is part of the meta schedule project (#8473) that adds a genericDatabase interface of tuning records, as well as a default implementationof using two JSON-files to mimic the database.This feature is future-compatible with dynamic shape auto-tuning.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",1
"[ONNX] [Relay] Dynamic squeeze (#9095)* adding dynamic squeeze first steps* Matt B. implementing shape* squeeze implemented, dynamic_to_static and onnx importer next* add Squeeze op convert to onnx.py* dynamic to static* removed comments* removed comments* added comment* adjusted comment* black and lint* ran make format in root directoryCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>",1
[microTVM][Zephyr] Add MIMXRT1050 board support (#9068)* add target support* fix ci issue,0
"[Relay] Prepare for new plan_devices.cc (part II) (#9130)* Prepare for new plan_devices.cc (part II)These changes came from changing https://github.com/apache/tvm/pull/9038 to usetvm.parser.fromtext instead of manual AST construction.- Demote FunctionOnDeviceAttrs to just a pair of DictAttrs entries so  that the parser will understand them on Function definitions.- Connect some special operators to their attributes so parsing understands them  at call sites.- Don't silently ignore attributes during parsing.- Implement OptFunctionOnDevice so won't add device annotations for kUnknownDeviceType.- Allow the parser to be given an initial metadata map to support examples which  need constants.- More DLOG -> VLOG conversions to reduce debug clutter.* [checkpoint] Keep existing ParseModule ffi to simplify rust bindings* [checkpoint] Address Christopher's comments.* [checkpoint] Andrew's comments from #9038* [checkpoint] Jared's comments from #9038* [checkpoint] Woops, forgot rename.",0
[Torch] Support returning quantized weights and bias for BYOC use cases (#9135)* [Torch] refactored the way is bias quantization done* support returning 8bit weight* add test* add doc* pylint* return_int8_weight -> keep_quantized_weight* fixed for dynamic linear case* remove test function call* simplifying,0
"[UnitTests] Enable minimum testing on Vulkan target in CI (#9093)* [UnitTests] Enable minimum testing on Vulkan target in CI- Include the Vulkan runtime in the GPU build.- Run test_target_codegen_vulkan.py as part of the `python3: GPU` CI step.* [CI] Added a dummy task_config_build_gpu_vulkan.sh, to be removed later.The CI builds use the Jenkinsfile located in the ci-docker-stagingbranch, but the scripts in the PR that is being run.  Temporarilyadding back a task_config_build_gpu_vulkan.sh, which just calls therenamed task_config_build_gpu_other.sh.",1
add nn.global_avgpool to fq2i (#9137),1
[Relay][ConvertLayout] Support for qnn.conv2d_transpose (#9139),5
[BYOC] support arbitrary input dims for add/mul/relu of dnnl c_src codegen (#9127)* support arbitrary input dims for add/mul/relu of dnnl c_src codegen* fix lint* fixCo-authored-by: sunway <wei.sun@hexintek.com>,0
[OpenCL] Remove redundant visit statement in CodeGen. (#9144)Fixes regression with some models on which compilationdoesn't terminate.,0
"[UnitTest] Parametrized test_conv2d_int8_intrinsics (#9143)Parametrized it to get more detailed information while debuggingfailures in https://github.com/apache/tvm/pull/9091, but isn'tsemantically part of that PR.",0
[Frontend][PyTorch] support for quantized conv_transpose2d op (#9133)* [Frontend][PyTorch] support for quantized conv_transpose2d opPyTorch uses the same underlying function to pack andunpack the params for conv2d and conv_transpose2d ops.This change adds support for quantized conv_transpose2d opby reusing the ConvPackedParam and adding theoutput_padding param to it.This output_padding param will remain unused in case of conv2d.Also added test for above with specific condition fortorch v1.7.1 and below.* fix after merging main,0
[Meta Schedule][M3a] SearchStrategy (#9132)* Add c++ side SearchStrategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add python-side code & test.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Add docs.* Minor fix.* Add workflow.* Add docs.* Fix docs.* Add notes.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
fix things (#9146)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,0
[TIR] add loop partition hint pragma (#9121)* add loop partition hint pragma* fix unintialized var* fix to remove hint at last* use tir compare for loop partition testcase,0
"Fix Google Mock differences between Ubuntu 18.04 and 16.04 (#9141)I thought I got all of these, but turns out I didn't, there's another weirdism in how Google Mock and Google Test are packaged on Ubuntu where the `cmake` command fails due to directories outside of the build root.Double checked logs on Ubuntu 18.04 and Ubuntu 16.04 for this after enabling verbose copying:```shell$ ./docker/build.sh ci_cpu --net=host...'googlemock/libgmock.a' -> '/usr/lib/libgmock.a''googlemock/libgmock_main.a' -> '/usr/lib/libgmock_main.a''googlemock/gtest/libgtest.a' -> '/usr/lib/libgtest.a''googlemock/gtest/libgtest_main.a' -> '/usr/lib/libgtest_main.a'$ ./docker/build.sh ci_i386 --net=host...'libgtest.a' -> '/usr/lib/libgtest.a''libgtest_main.a' -> '/usr/lib/libgtest_main.a'...'libgmock.a' -> '/usr/lib/libgmock.a''libgmock_main.a' -> '/usr/lib/libgmock_main.a'```",0
"[Meta Schedule][M3b] Runner (#9111)This PR is part of the meta schedule project (#8473) that adds theasynchronous program runner interface, as well as a referenceimplementation of RPCRunner. LocalRunner will be implemented withPopenPool executor in a follow-up PR.Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Address commentsCo-authored-by: Cody Yu <comaniac0422@gmail.com>fix lint",0
"[CI] Split Integration tests out of first phase of pipeline (#9128)* [CI] Split Integration tests out of first phase of pipelineI took a look at the time taken by each stage in the Jenkins pipeline and what comprises the 6 hour CI build time. CPU Integration tests took `65` minutes of the `100` minutes of `Build: CPU`. By adding `python3: CPU` with just those Integration tests, it lines up with `python3: GPU` and `python3: i386` which both take a similar amount of time and takes roughly 60 minutes off the overall run time.Numbers copied from sample successful run (final time approx: 358 minutes):|Phase|ID                           |Job   |Minutes                                      |Start||-----|-----------------------------|------|---------------------------------------------|-----||0    |0                            |Sanity|3                                            |0    ||1    |0                            |BUILD: arm|2                                            |3    ||1    |1                            |BUILD: i386|33                                           |3    ||1    |2                            |BUILD: CPU|100                                          |3    ||1    |3                            |BUILD: GPU|25                                           |3    ||1    |4                            |BUILD: QEMU|6                                            |3    ||1    |5                            |BUILD: WASM|2                                            |3    ||2    |0                            |java: GPU|1                                            |103  ||2    |1                            |python3: GPU|66                                           |103  ||2    |2                            |python3: arm|22                                           |103  ||2    |3                            |python3: i386|70                                           |103  ||3    |0                            |docs: GPU|3                                            |173  ||3    |1                            |frontend: CPU|40                                           |173  ||3    |2                            |frontend: GPU|185                                          |173  ||3    |3                            |topi: GPU|110                                          |173  ||     |                             |      |                                             |     |Numbers predicted after change (final time approx: 293 minutes):|Phase|ID                           |Job   |Minutes                                      |Start||-----|-----------------------------|------|---------------------------------------------|-----||0    |0                            |Sanity|3                                            |0    ||1    |0                            |BUILD: arm|2                                            |3    ||1    |1                            |BUILD: i386|33                                           |3    ||1    |2                            |BUILD: CPU|35                                           |3    ||1    |3                            |BUILD: GPU|25                                           |3    ||1    |4                            |BUILD: QEMU|6                                            |3    ||1    |5                            |BUILD: WASM|2                                            |3    ||2    |0                            |java: GPU|1                                            |38   ||2    |1                            |python3: GPU|66                                           |38   ||2    |2                            |python3: arm|22                                           |38   ||2    |3                            |python3: i386|70                                           |38   ||2    |4                            |python3: CPU|60                                           |38   ||3    |0                            |docs: GPU|3                                            |108  ||3    |1                            |frontend: CPU|40                                           |108  ||3    |2                            |frontend: GPU|185                                          |108  ||3    |3                            |topi: GPU|110                                          |108  |* Fix typo in ci_cpu commands",0
Arm(R) Ethos(TM)-U NPU codegen integration (#8849)This commit integrates the codegen for Arm® Ethos™-U.* Adding Conv2D tests and a mobilenet_v1 conv2d offload test.Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Matthew Barret <matthew.barrett@arm.com>Co-authored-by: Grant Watson <grant.watson@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Matthew Barret <matthew.barrett@arm.com>,1
"[LLVM] Make changes needed for opaque pointers (#9138)* [LLVM] Make changes needed for opaque pointers- Pass value type to all Create.*Load and Create.*GEP functions.- Create type TypedPointer to keep both the address and the pointee's  type when buffer pointers etc. are created.- Eliminate calls to getPointerElementType, except one in creating  debug info (that seems necessary for the time being).* Fix typo in CodeGenCPU::CreateStructRefPtr* Fix type extraction in CodeGenLLVM::AddAliasInfo* Fix types in ramp-1 vector loads/stores* Fix getting intrinsic name in error message* Return valid pointer from PackClosureData when no data to pack",0
"[Relay] Merge analysis/context_analysis.cc and transforms/device_annotation.cc (#9038)* [Relay] Merge analysis/context_analysis.cc and transforms/device_annotation.ccCurrently LowerTEPass (backend/te_compiler.cc) is a 'special' pass because itdepends on a side-input DeviceMap. We'd like to remove that side-input, andinstead recover the Device (and, ultimately, Target) for each (fused) primitivecall from the AST alone.By doing so we also avoid needing to perform device planning twice: - It needs to be done before lowering so we know which primitives need   to be compiled for which devices. - It then needs to be re-done after lowering and optimization as a prelude   to memory planning.By baking the device plan into the AST we can simply do device planning beforelowering, and run memory planning later, both as ordinary passes.While working on that issue we realized we currently have 3 'device planners': - transforms/device_annotation.cc, which supports only a small subset of Relay   and uses a simple top-down algorithm to assign a device to every   sub-expression. - analysis/context_analysis.cc, which makes a galant effort to support most of   Relay, is based on unification rather than a top-down algorithm, but handles   higher order functions by ad-hoc and fragile inlining. - transforms/annotate_target.cc, which works on Targets instead of Devices, but   is otherwise like 'device planning'.We'd like to bring these together.In this PR we introduce a new transforms/device_planner.cc intended to replacetransforms/device_annotation.cc and analysis/context_analysis.cc. We don'tdelete those two just yet since we need to switch all users off of them in thenext PR. We also leave transforms/annotate_target.cc alone pending a proper RFCto bring devices and targets together sensibly, but have it firmly in oursights.transforms/device_planner.cc is based on analysis/context_analysis.cc, butis heavily reworked to: 1. Handle starting from existing ""on_device"" annotations as well as existing    ""device_copy"" calls. 2. Be idempotent, with the idea we'll probably need to re-run it to 'refine'    device planning to account for storge scopes. 3. Robustly handle all of Relay, particularly higher-order functions. For that    we replace the inlining approach in analysis/context_analysis.cc with a    higher-order unification domain. 4. Be a little more systematic with defaulting. 5. Capture the result of the analysis within the AST as new ""device_copy"" calls    at device boundaries, and new/replaced ""on_device"" calls wherever the device    for a sub-expression is not already 'obvious' from the sub-expression's    lexical scope. 6. Provide helper visitors for passes which need to ask for the device for    any sub-expression they are processing and/or preserve device information    on rewrites. Those passes include:     - backend/aot_executor_codegen.cc (AOTOnDemandAllocator)     - backend/graph_plan_memory.cc (StorageAllocaBaseVisitor etc)     - backend/te_compiler.cc (LowerTensorExprMutator)     - backend/vm/lambda_lift.cc (LambdaLifter)     - transforms/memory_alloc.cc (DialectRewriter)     - transforms/to_a_normal_form.cc (Fill)     - backend/vm/compiler.cc (VMFunctionCompiler)    However we won't change any of those in this PR.See the draft https://github.com/apache/tvm/pull/8788 for the end game.* [checkpoint] Use Relay script for all unit tests.* [checkpoint] Hoist out DeviceDomain and DeviceDomains.* [checkpoint] Hoist out visitors* [checkpoint] Woops, left debug-only code in",0
Fix flaky NMS test by making sure scores are unique (#9140),0
[TIR][LowerMatchBuffer] Fix lowering strides when source region has higher dimension than the buffer (#9145)* [TIR][LowerMatchBuffer] Fix lowering strides when source region has higher dimension than the buffer* use int instead of size_t,0
Update find cublas so it search default path if needed. (#9149),1
[Hotfix][Testing] Wait for RPCServer to be established (#9150),0
Fix typo (#9156)Co-authored-by: JoshuaZhong <joshuazhong@etekcity.com.cn>,0
"[microTVM] Add wrapper for creating project using a MLF (#9090)Currently there is already a wrapper function for creating a new projectdirectory based on an ExportableModule, but there isn't one for creatinga new project directory based on an existing MLF archive, which is alsohandy. Hence that commit adds a new wrapper for creating a project usingan existing model compiled and kept in a MLF archive.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>Reviewed-by: Christopher Sidebottom <chris.sidebottom@arm.com>Reviewed-by: Andrew Reusch <areusch@octoml.ai>",1
[6/6] Arm(R) Ethos(TM)-U NPU codegen integration with `tvmc` (#8854)* Add Arm(R) Ethos(TM)-U codegen support on tvmc* Include `ethos-u` as a new target for tvmc* Adds testing for the new targetCo-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>* Add Arm(R) Ethos(TM)-U codegen support on tvmc* move partition_for_ethosu from tvm.relay.backend.contrib.ethosu  to tvm.relay.op.contrib.ethosu* lazy load ethos-u-vela dependencies and show an appropriate error  message in case the dependency is not present* Adjust test casesCo-authored-by: Leandro Nunes <Leandro.Nunes@arm.com>* Add Arm(R) Ethos(TM)-U codegen support on tvmc* add missing importChange-Id: Ieefa0ee6e86bdc09ff93fcc632ed003b5f3f3a99Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>,0
[OpenCL] Add vectorization to cuda conv2d_nhwc schedule (#8636)* Add vectorization to cuda conv2d_nhwc scheduleAdding vectorization significantly improved performance. About 6-7xboost.* Apply comment* Move schedule to topi/gpu dir* Add vectorization to inner loop* Update values of vectorization factor,1
Introduce centralised name transformation functions (#9088)* Introduce centralised name transformation functionsTo address some of the concerns raised in https://github.com/apache/tvm/pull/8280 and https://github.com/apache/tvm/pull/8720 I've put together a series of functions to combine together names to re-use between these areas. These are meant to be a starting point to fix up the name generation to use the TVM C conventions and port the interface API header to C++.These functions will also be used for constructing the names in the C Device API (https://github.com/apache/tvm-rfcs/pull/31).* Improve error handling and error messages* Sanitize sanitise to sanitizeThis patch aims to sanitize uses of sanitise to the form of sanitize to be consistent with the overall codebases use of American English.,0
"[ONNX] support additional nllloss tests (#9045)* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* initial dyn unsqueeze example* simplify, properly unpack scalar* basic tests* squish bugs -- assign proper types* working topi* fix things* temp work* fix casting to int64* shape encoding method for axis* working shape encoding metric* add comment* move to non-rank encoded axis* failing regime* fix* it works!* add test* add comment on shape func* remove unused topi* undo some file changes* more cleanup* newline* clean up* clean up* enable multiple axis tests* move tests to dynamic op* Update docs* add converter* working tests* add test, remove unneeded file* fix things* more lint* more lint* pick things* disable opencl tests* unsqueeze tests* clean up* dyn stuff* add num_newaxis* add support* black* doc string* remove bad merge* fix default axis behavior* rebase* fix squeeze* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",0
"Issue8717 x86 dws conv2d schedule (#9092)* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Issue 8717 Add schedule for depthwise_conv2d_nhwc* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Formatting fixed.* Add default DepthwiseConv2D schedule in NHWC layout for arm cpu* Fixed micro model library test. Checking size reduced to 16 bytes from 2466816.* Revert ""Merge branch 'update-arm-simd-intrinsic' of https://github.com/sergey-grovety/tvm into issue8717-x86-DwsConv2d-schedule""This reverts commit e927567058403bcc9e4fdc3d24828b3dcd6a661b, reversingchanges made to 0ccb5a01495d02f521eea2af9efa6a3153c4f72b.* Revert ""fix test_export_model_library_format_workspace""This reverts commit 32ede712ada81242f435693403a78d98adf9afeb.fix formatmove schedule_depthwise_conv2d_nhwc to generic conv2d, add test for schedule_depthwise_conv2d_nhwcfix test_export_model_library_format_workspaceuse x86 depthwise_conv2d_nhwc schedule for arm_cpuAdd x86 schedule for depthwise_conv2d_nhwc# Conflicts:#python/tvm/relay/op/strategy/arm_cpu.py* move schedule_depthwise_conv2d_nhwc to generic conv2d, add test for schedule_depthwise_conv2d_nhwcfix formatRevert ""fix test_export_model_library_format_workspace""added a missing comma* Revert wrong merge changes* empty commit to force pipeline restart* Add condition to use compute_at for generic schedule_depthwise_conv2d_nhwcCo-authored-by: Sergey Smirnov <Sergey.Smirnov@mir.dev>Co-authored-by: Alex-grovety <Alexey.Yazev@mir.dev>",0
[microTVM] Update support for ARMv7m intrinsic (#8990)* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Formatting fixed.* [microTVM] Update support for ARMv7m intrinsic - Improved implementaion of gemm function for conv2d - Removed %4 restriction for channels - Added test case to verify SMLAD intrinsic speed accelerationSigned-off-by: Sergey Smirnov <Sergey@grovety.com>* Implemented discussed changes.* Removed unnecessary test files.* Formatting fixed.* Formatting fixed2.* Formatting fixed3.* Formatting fixed4.* Formatting fixed5.* Fixed test time result checking.* Check rebuild.* Formatting fixed.* Issue 8717 Add schedule for depthwise_conv2d_nhwc* Resolve merge conflict.* Resolve merge conflicts.* Fixed formatting.* From Issue 8717//Fixed micro model library test. Checking size reduced to 16 bytes from 2466816.* From Issue 8717.Removed changes.* From Issue 8717. Fixed typo.* Fixed import.* Fixed import and method call.* Added QEMU testing comment.* Fixed ZEPHYR_BOARD usage.* Fixed tests. Removed issue 8717 changes.* Formatting fixed.* Removed test call from base_box_test.sh,0
[Meta Schedule][M3a] TaskScheduler (#9154)* Add docs.* Add TaskScheduler.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI after hotfix.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
"[TIR] tir.transform.StorageFlatten refactor (#9091)* [TE] Improved flexibility of ArgBinder::BindDLTensorAllowed a compact DLTensor to bind to a Buffer object that definesstrides, if the strides defined correspond to a compact layout.* [TIR] Exposed ElemOffset as a member function of BufferNode.* [TE] Pulled shape determination out of StorageFlattenerPreviously, StorageFlattener would determine the shape of a physicalbuffer based on the extents of the BufferRealizeNode.  Pulled theseout into a separate BufferShapeLegalize pass.  After this pass, allbuffers have a shape that matches the buffer realization extents.* [TE] Refactor stride calculation out of StorageFlattenerPreviously, StorageFlattener would handle any attr::dim_alignannotations.  Now, this is pulled out into a separateBufferStrideLegalize pass.* [TE] Refactor thread scope propagation out of StorageFlattener.Previously, StorageFlattener would use the scope in IterVar to assigna scope to allocated buffers, where not otherwise defined.  This hasbeen pulled out into a separate ThreadScopePropagate pass.* [TE] Refactor buffer bind mapping out of StorageFlattener.Previously, StorageFlattener would look for `attr::buffer_bind_scope`to determine if a Buffer object is a view into another buffer, andwould apply that mapping while making the Allocate/Store/Load nodes.Now, the mapping of buffer binds is pulled out into a separateBufferStrideUnwrapper pass.This also resolves an issue in which BufferLoad/BufferStore nodes thatrefer to a Buffer defined through `attr::buffer_bind_scope` wouldgenerate Load/Store nodes that point to the linked buffer, rather thanthe actual buffer.* [TIR] Removed checks on buffer->shape.size()Even after BufferShapeLegalize, rank-zero tensors may have an emptyshape.* [TIR] Relaxed check on a bufferview's striding.Original refactoring requiring that a bufferview have no explicitstriding, and instead take the striding from the buffer that it isviewing.  Modified to allow bufferview to specify striding, so long asit is consistent with the viewed buffer's striding.  This reproducesthe behavior of StorageFlatten before the refactoring.* [TIR] Fixed StorageFlatten test for shape_legalize.AttrStmtNodes that contain rewritten Buffers need to be rewritten aswell.* [TIR] Assigned storage scopeThe earlier stage of the refactor left a buffer's storage scopeundefined if it's scope was not determined by the IterVar of a loopcontaining its allocation.  Now, these are explicitly set toStorageScope::kGlobal, to match the previous behavior ofStorageFlatten.* Updated ICHECK_EQ to CHECK_EQ for a test that depends on user-provideddata.* Added comments in storage_flatten.cc, indicating why buffer_bind_scopeneeds special handling.* Updated comment with a few examples of where compact buffers areassumed to have no strides defined.* Updated following @csullivan's comments.* Added fuzzy mapping to the BufferShapeLegalize.Maintains earlier behavior of StorageFlatten, which allows bufferviews to be mapped to higher dimension buffers, if the view extent is1 in each extra dimension.* Updated BufferShapeLegalize, asserts need to be inside the buffer_bind_scope.* Pulled all shape-dependent behavior into BufferShapeLegalize.Previously, BufferBindUnwrapper passed fuzzy_match=true toArgBinder::BindBuffer, which could change the number of dimensions.Now, all buffer dimensions should be updated prior toBufferBindUnwrapper, and it is an error to have mismatched dimensionsin BufferBindUnwrapper.* Added another pass to remove verifiable assert statements.ArgBinder::BindBuffer inserts these assert statements if they are notverifiable at the time of substitution.  Previously, with one giantsubstitution, the assertions were verifiable at that time.  After therefactor, with substitutions done in multiple stages forshape/stride/buffer_bind_scope, we need to clean up any assertionsthat are verifiable after all substitutions have occurred.* Minor cleanup- Removed StorageFlattener::BufferEntry::RelIndex, behavior already  handled by BufferShapeLegalize.- Improved comments and error messages.- Extracted duplicate behavior in BufferLoad/BufferStore handling in  BufferShapeLegalize.* Updated to handle BufferRealizeNode with no defined bounds.* Updated to be less aggressive when checking AssertStmtA true Assert statement can be removed, but a false Assert statementrequires CFA to give as a compile-time error.  Since we only need theremoval of true assert statements, skipping the CFA this time.",0
"[TEST] Move llvm import test away from minimum test (#9171)* [TEST] Move llvm import test away from minimum testThe llvm import relies on the same system clang and llvm version andmay be tricky to get right on all platforms.Given this is an advanced feature, and there has been some problemsin windows(could relates to clang version update).This PR moves away from minimum tests.* Update test_minimal_target_codegen_llvm.py* Update test_target_codegen_llvm.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>",1
add PaddlePaddle tutorial (#9124)* add paddle tutorial* fix some format issues* fix code style* clean* fix format* fix title underline* PaddlePaddle>=2.1.3,0
[BugFix][Meta Schedule] Fix meta_schedule.testing.local_rpc (#9172),0
Use a uint64_t to serialize primitive_attrs in the Relay VM to fix 32bit RPC (#9169),0
[TIR][LowerMatchBuffer] Fix lowering strides when source buffer has non-empty strides (#9166),0
"BUG: Fix core-dump in crt graph_executor.c (#9155)The JSON loader was missing a BeginArray for the ""device_index"" attribute.That's a 1 line fix. The rest is to add a unit test and make it build.The crt JSON handling is perhaps not our finest code.",0
Cleaning up Arm(R) Ethos(TM)-U codegen (#9147)This is a follow up commit to address thecomments of #8849Change-Id: I02d8de64f3bce0e7b544d652eee8737ec1ecbb80,1
[TVMScript] Script namespace changes (#9115)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Zihao Ye <zihaoye.cs@gmail.com>Co-authored-by: Tristan Konolige <tristan.konolige@gmail.com>,4
[Meta Schedule][M4a] Local runner (#9153)* [Meta Schedule][M3a]Local runner (#479)* localrunner* localrunner init* linting* address comments* exception handling* single run testcase* two more cases added* add exception case* one case with AddModule added* address comments* address comments* remove unused dependency* optional arguments* linting* add utils* linting* address comments* remove non-ascii commennt* add sanity check* address comments,1
"[CMSIS-NN] Initial operator support for Mul (#9163)This is largely as it says on the tin, it adds Mul support to CMSIS-NN",1
"[LLVM] Rename t_tvm_context_ to t_tvm_device_, NFC (#9176)Follow the change from DLContext to DLDevice.",4
Add cache flush for arm (#9170)* Add cache flush for arm* formattingCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>,1
[ONNX] [#8838] QLinearLeakyRelu contrib op  (#9063)* [ONNX] QLinearLeakyRelu contrib op* Add comment* jostle ci* jostle ci,1
[Unittest] Fixing unittest (#9180),0
"[Contrib][ONNX] Handle removal of onnx.utils.polish_model (#9178)Onnx 1.9 removed optimizers from the core repository (see discussionin https://github.com/onnx/onnx/pull/2834), includingonnx.utils.polish_model, breaking RelayToONNXConverter.  This PR addonnxoptimizer.optimize as a fallback method if onnx.utils.polish_modelis unavailable.Also updates tutorials/documentation to recommend installingonnxoptimizer when installing onnx, because the current PyPI versionof onnx is above version 1.9.",1
"[LLVM/CPU] Add comments with origins of various runtime/backend types, NFC (#9177)LLVM codegen defines a bunch of LLVM IR types that correspond to variousruntime entities. Add comments showing what entity a given LLVM IR typecorresponds to wherever applicable. Since there is no direct connection(e.g. using names, etc.) that could be extracted automatically (via ctagsor other mechanism), this can make the LLVM codegen a bit easier to read.",1
"[TensorIR][M2a] Decompose-Reduction (#9041)This PR is part of the TensorIR upstreaming effort (#7527),which adds the `decompose-reduction` scheduling primitive.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",1
[Test] Fix flaky LocalRunner test due to a small timeout (#9181),0
"[CI] Prevent the complete Jenkins pipeline to run when files commited only to  `/docs` (#9031)* Add script to look for changed in doc dir* Modify Jenkinsfile* Minor changes in scripts* Working Jenkinsfile on selective stages on docs* Pass groovy formater on Jenkinsfile* Implementation of relay_to_tir target hook (#8423)This the first new hook proposed in the Additional Target Hooks RFC, longerterm the compilation should move to using `Target` proper but this unblocks our current work whilst illustrating the eventual interface via `Target` in `src/relay/backend/contrib/example_target_hooks/relay_to_tir.cc`Ideally the host target would be annotated onto the `IRModule` so as this `Pass` could use it instead of defaulting to C but this is fine for now.* [CUDA] Fix dense tensorcore legalize type error when units is specified (#9030)* Fix dense tensorcore legalize type error when units is specified* revert black change due to different version from CI* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op (#9017)* [ONNX] QLinearAveragePool and QLinearGlobalAveragePool contrib op* Fix linter error for variable name and else after return* Separate quantized avg_pool impl and add TODO for global_avg_pool* Fix comment typo* Fix line break in `setup.py` (#9029)* [Onnx] Add SoftmaxCrossEntropyLoss (#8906)* nll loss v1* add converter* decode strings in byte form* decode variable length inputs* make shapes correct* unsqueeze* proper weight handling* simplify if statement* fix tests* add comment about tests* delete extra file* lint* so cool* Update CI Lint Image Version (#8841)* Update CI Lint Image Version* trigger* [BUG] ToBasicBlockNormalForm immutability (#8778)* ToBasicBlockNormalForm immutability* better comment on ToBasicBlock* refine comment of ToBasicBlockForm* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vm (#8807)* [GRAPH EXECUTOR,VM] Add benchmarking function to graph executor and vmThis new benchmarking function is just a convenience function forcalling time_evaluator on the underlying module. Hopefully this shouldmake it easier for users to get good benchmarks of their code.* formatting* import order* more test, more comments, more precision* fix tests* add seconds descriptions to doc* Apply CPPLint to CRT Tests (#8844)This one was a bit trickier as there was more usage of dynamic arrays and less safe casts. I've tried to minimise the changes to just those required to passing linting.* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost. (#8584)* [Relay][TOPI] Support of depthwise conv2d NHWC for Mali/Bifrost.Added initial tunable autotvm templates for depthwise conv2d withNHWC layout for Mali and Bifrost.* [Relay][TOPI] Misc fixes for depthwise conv2d Mali/Bifrost.- Fix assert for Bifrost.- Set reasonable default axis splits to avoid using tophub for NHWC.- Fixed typo: arm cpu -> Mali.* [Relay][TOPI] Fixed formatting in depthwise conv2d Mali/Bifrost.* Support for CMSIS-NN in Corstone300 Makefile (#8831)Change-Id: Ifc2305db4e11d1d15d45407287f8f0bea469100a* [microtvm][Zephyr] Increase timeout to fix flaky tests (#8846)* increase timeout* trigger* [AMP] Bump up tolerance on flaky test (#8850)* bumpy up tol* bumped tolerance up even more* jostle ci* [Hexagon] Rework tvm.target.hexagon() interface (#8823)* [Hexagon] Rework tvm.target.hexagon() interfaceMake the tvm.target.hexagon() function take most options as keywordparameters. This will allow adding additional parameters without changingthe interface.No changes are required to existing code, except for changing positionalparameters following the CPU version to keyword parameters, and updatingthe names of the keyword parameters:  sim_args  -> sim_options,  llvm_args -> llvm_options,although the old names will be accepted for the time being.* formatting* change ' to ""* Rename 'args' to 'config' for clarity* Use 'strip' instad of 'replace'* Restart build* [Pattern matching] Add an option to rewrite the graph only once (#8843)* [Pattern matching] Add an option to rewrite the graph only onceIf the graph returned from the callback consists of the originalpattern, the rewriter will run in the loop, which is not always desired.So this patch proposes an option to run the rewriter only once.Change-Id: I85cf0a055b8961d52394f21c1e4d7aad0a7e1d06* Make rewrite_once default to falseChange-Id: Idf6f01f254c403158883681e75c2a5978efbd2d0* update gpu and cpu (#8853)* VTA cmake change to include Verilator header for building tsim library (#8797)* VTA cmake file require Verilator include for tsim target. VTA module.cc uses svOpenArrayHandle to send wide data through DPI* Refactor Verialtor check conditions* Build TSIM only for CPU target. CPU target don't use -Werror to compile with Verilator. Jenkinsfile to have tvm_multilib_tsim defined for CPU build target.* remove build/libvta_tsim.so from non tsim targeting builds* Revert to enable TSIM build i386. Revert to -Werror in CPU config. Remove verilator CPP objects from cmake config for tsim and put them as include into vta module.cc to avoid Verilator compilation warnings* [FIX] Bug fix for a floormod rewrite simplify rule (#8852)* Update rewrite_simplify.cc* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* Update test_arith_rewrite_simplify.py* move rust lint script (#8726)* [AMP] Disallow fp16 conversion for summation-like ops (#8810)* [AMP] Disallow fp16 conversion for summation-like ops* test only structural equality* [TOPI] [Relay] Sparse Conv2d Implementation for 3x3 kernels (#8605)* [topi] add spconv2d_3x3 nhwc* [relay] sparse_conv2d: add kernel_size attr* [relay] add strategy for spconv2d_3x3 nhwc* [relay] pass to convert spconv2d with const args* [relay] convert sparse conv2d pass fixes* use array for sparse conv2d attr* fixup 1x1 tests; new 3x3 tests* extend repeat_interleave op for relay.Expr (#8839)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>* Change AOT from ExprVisitor to MixedModeVisitor (#8856)This should allow better scale-ability for AOT when targeting larger networks.* Add a PaddlePaddle Frontend (#8645)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black formatCo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>* [Runtime] add set_output_zero_copy (#8497)* Update graph_executor.h* Update graph_executor.cc* modify zero copy UT add set input zero copy* modify C style* add runtime test* realy build  generatr the jsonCo-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>* [Hexagon] Change declaration order of unique_ptr objects to fix crash (#8859)A crash occurs when automatically deleting an instance ofCodeGenHexagon because the LLVMContext object has already beenfreed. Objects of both types are created using unique_ptr, butthe object managed by the LLVMContext unique_ptr is passed toCodeGenHexagon object (not as a unique_ptr).This crash is fixed by moving the declaration of the LLVMContextobject before the CodeGenHexagon object. I'm not sure if thisis the best way to fix this, but it does fix the crash. Also,in other files, the LLVMContext object is always created first.Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>* [Graph Executor, VM] Add end to end benchmarking of models (#8858)Add benchmarking that includes ovearhead of transfering inputs andoutputs to and from the device. This should give an accurate measurementof the runtime a user would see when using the model. This isaccomplished by adding functions that run from inputs to return valuesinto the graph executor and the VM.* [UnitTests] Expose TVM pytest helpers as plugin (#8532)* [UnitTests] Expose TVM pytest helpers as pluginPreviously, pytest helper utilities such as automatic parametrizationof `target`/`dev`, or `tvm.testing.parameter` were only available fortests within the `${TVM_HOME}/tests` directory.  This PR extracts thehelper utilities into an importable plugin, which can be used inexternal tests (e.g. one-off debugging).* [UnitTests] Refactor the plugin-specific logic out into plugin.py.* [UnitTests] Moved marker definition out to global variable.* Remove AOT Executor header from Arduino project (#8857)* [Community] @mdw-octoml -> Reviewer (#8868)* [TIR] Fix opaque access in buffer locator pass and match_buffer in region detector (#8855)* init* fix* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update src/tir/transforms/plan_update_buffer_allocation_location.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* [Autoscheduler] Configurable workload keys (#8862)* change workload keys* remove binary string comparison* append the tuple not every integer* clean up* lint* dump workload keys to dags* fix things* change some strings* misc fixes, add tests* jostle ci* [Tutorial][Executor] Fix the usage of executors in tutorials (#8586)* fix: executor usage for keras tutorial* fix: executor usage for onnx tutorial* [Tutorial][Executor] Fix executors in tutorials* [Frontend][Onnx] Simplify onnx input since name accesses are not reliable. (#8867)* Simplify onnx input since name accesses are no longer supported.* move Celu importer.* [TIR] GetBlockReadWriteRegion (#8875)* [TIR] GetBlockReadWriteRegion* Fix black issue* Use constant reference for the interface* Fix lint issue* [RISCV] Add support for llvm parameter -mabi (-target-abi) (#8860)* [Community] @manupa-arm -> Committer (#8870)* adding Manupa to the contributors list* re-trigger CI* [RPC] Fix ios_rpc build (#8864)* [Vulkan][Target] Added the driver name to the vulkan target string. (#8882)Driver name (e.g. ""NVIDIA"", ""radv"", ""AMD open-source driver"") is readfrom the `driverName` property in[VkPhysicalDeviceDriverProperties](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/VkPhysicalDeviceDriverProperties.html),or is left as `""unknown_driver_name""` if the driver does not supportquerying the driver name.* [ONNX][TOPI] Support select_last_index for argmin/max (#8816)* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* support select_last_index for argmin/max* reverse conditions which made on accident* forward args in reduce.py* make proper nodes for reduction ops* remove complicated nested lambdas* fix lambda capture for conversion* forward more arguments* forward more args* enable onnx tests* wrapping casts to remove ambiguity* revert changes extraneous* correct incorrect attrs being used for ops* change attributes* remove old impl* register new attribute node* clean up test* reformat* reformat* coolio* stable comparison* casts to avoid ambiguity* casting more* correct arg passing* fix broken input* OneElementReduceAttrs-->ArgReduceAttrs""* reduce boilerplate* change names* remove log statement* jostle ciCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>* refactor optimize GEMM on CPU tutorial (#8825)* refactor optimize GEMM on CPU tutorial* fix lint errors* fix more lint errors* fix typo* fix problem with redefinition of `k`add TODO and comments around loop unrollingclarify note on the array packing figure* reword general description of array packing* grap kaxis from compute definition* remove duplicate comments on unrolling* Change target string to Target object in the TE compiler and interpreter (#8835)* # This is a combination of 2 commits.# This is the 1st commit message:Initial changes# This is the commit message #2:Ftarget string -> Target object works!* Fix remaining target strings* fix bad rebase* Fix typo* 1 more bad rebase fix* Lint* typo* Forgot to commit this* Add TargetStrHash and Map<Target... to std::unordered_map<Target... conversion fn* Passing most tests, yay* remove some comments* lint* target-str-to-target-object* Respond to change requestsCo-authored-by: Jared Roesch <roeschinc@gmail.com>* [TensorIR][M2a] CacheRead/Write (#8863)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>* [CI] make pre-commit hooks to run on every push instead of every commit (#8888)* [TVMScript] Fix printing ForNode annotations (#8891)* [1/10] CMSIS-NN graph partitioner for softmax (#8653)* cmsis graph partitioner for softmaxChange-Id: I80ecd7bc5351f241b4674ef53b36e4398c8adb83* Updated docstring in the partioning functionChange-Id: Ieb4b623e5929cfdb6aa0235db64c825fac8d7055* [microTVM][RVM] Add Arduino RVM (#8748)* Functioning Arduino Vagrant VMBegin building Arduino Vagrant VMMostly working Vagrant VMChanges for debuggingAdd ignored json fileFix venv path* Generalize parts of RVM for multiple platformscwd hackAdd unit tests from apps directory to task_python_microtvm.shGeneralize parts of RVM for multiple platforms* Add Vagrantfile lint exceptions* Address PR commentsAddress Mehrdad's PR commentsMore PR commentsDocumentation tweaksAdd dialout group to user* Rerun tests* Spresense fix* Rerun CI tests* Rerun tests* sce loss example* add comments, remove other tests* lint* lint* jostle* lint up* jostle* uncomment some tests* proper return* clean up* lint* minor merge errorsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Michalis Papadimitriou <mikepapadim@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>* [Hexagon] Don't use {} initialization with FastRPC structures (#9033)The data members in FastRPC structures aren't guaranteed to remainin the same order. Replace aggregate initialization with direct,member-by-member initialization.* Test* Minor checkstyle issue* Test* Test file* Revert changed in unit tests* Change script name* Test* Revert format on groovy file* Remove test file* Minor change in script* Minor formating changes* Revert logic in conditions for changed filesCo-authored-by: Christopher Sidebottom <christopher.sidebottom@arm.com>Co-authored-by: masahi <masahi129@gmail.com>Co-authored-by: Anirudh Sundar <quic_sanirudh@quicinc.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: AndrewZhaoLuo <andrew.zhao.luo@gmail.com>Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>Co-authored-by: Mehrdad Hessar <mhessar@octoml.ai>Co-authored-by: Jiawei Liu <jaway.liu@gmail.com>Co-authored-by: Tristan Konolige <tkonolige@octoml.ai>Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>Co-authored-by: Anastasia Stulova <38433336+AnastasiaStulova@users.noreply.github.com>Co-authored-by: Ashutosh Parkhi <86472128+ashutosh-arm@users.noreply.github.com>Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Anton Sorokin <anton.a.sorokin@intel.com>Co-authored-by: Chenfan <jcf94@outlook.com>Co-authored-by: Tantalus13A98B5F <jsl_713@live.com>Co-authored-by: Valery Chernov <black.chervi@gmail.com>Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>Co-authored-by: Jason <928090362@qq.com>Co-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Swift.Sun <sunjiwei@yeah.net>Co-authored-by: hwstaff <hwstaff@hwstaffdeMacBook-Pro.local>Co-authored-by: Cahoon, Brendon <bcahoon@quicinc.com>Co-authored-by: Lunderberg <Lunderberg@users.noreply.github.com>Co-authored-by: Yizhi Liu <liuyizhi@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@vip.qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Josh Fromm <jwfromm@octoml.ai>Co-authored-by: Alexander Pivovarov <pivovaa@amazon.com>Co-authored-by: Thierry Moreau <tmoreau@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Lily Orth-Smith <lilyorthsmith@gmail.com>Co-authored-by: Jared Roesch <roeschinc@gmail.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Gavin Uberti <guberti@users.noreply.github.com>",0
Support quantised RSQRT operator in TFLite (#9165)The commit tests _convert_unary_elemwise function for the quantised andnon quantized tensor for the RSQRT op.Other operators will be tested in future (separated )commits.,3
support Torch all and any op (#9185),5
"[Relay] Remove DeviceMap from LowerTE (#8788)* [Relay] Switch the graph, VM and AOT executors to use the mergeddevice_planner.cc from #9038, and finally remove DeviceMap from theLowerTE Pass.- We retire analysis/context_analysis.cc and  transforms/device_annotation.cc (and their tests). That  includes the CollectDeviceInfo, CollectDeviceAnnotationOps and  ContextAnalysis entry points. These are all subsumed by the  PlanDevices pass and the device aware visitors.- The following passes now use the new 'Device Aware' visitors to  recover the device for every Relay sub-expression:     - backend/aot_executor_codegen.cc (AOTOnDemandAllocator)     - backend/graph_plan_memory.cc (StorageAllocaBaseVisitor etc)     - backend/te_compiler.cc (LowerTensorExprMutator)     - transforms/memory_alloc.cc (DialectRewriter)     - backend/vm/compiler.cc (VMFunctionCompiler)- The following passes/utils must maintain the device information  encoded by the device planner within ""on_device"" annotations and  ""param_device_types""/""result_device_type"" function attributes:     - backend/vm/lambda_lift.cc (LambdaLifter)     - transforms/to_a_normal_form.cc (Fill)     - ir/expr_functior.cc (Bind)- Remove a lot ad-hoc 'homogeneous' vs 'hetrogeneous' conditionals  in favor of just asking for the device. Also removed a lot of ad-doc  encodings of the 'default' device.- We no longer need to run device-planning twice (before and after  lowering). Device planning is also decoupled from memory planning.- The LowerTE Pass no longer needs an expression-to-device side table  (which was the problem which kicked this series of PRs off in the first place).* [checkpoint] Revert unnecessary changes- Started down multi-target handling in interpreter but didn't finish- Some one-off debug stuff* [checkpoint] TODO's for default device logic",0
"[CMSIS-NN] Initial operator support for Add (#9167)This patch aims to add initial support for the `Add` operator to CMSIS NN, which was actually similar enough to the `Mul` operator that it shares quite a bit of code - exciting times.",1
[Hexagon] Fix compilation errors in Hexagon launcher (#9189)A few instances of `runtime::Device` were missed when `Device` was movedfrom `tvm::runtime` to `tvm`.Co-authored-by: Abhikrant Sharma <quic_abhikran@quicinc.com>,0
Add USE_ETHOSU for the config.cmake (#9162)This commit adds the cmake variable to enable/disablebuilding with Arm(R) Ethos(TM)-U NPU codegen support.Change-Id: I891a1cb0619dd24d749353fe7b156c45356d453f,1
correct error (#9193)Co-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,0
"Fix custom_address serialization in c++ tracker client. (#9192)* Make sure that if a custom address is passed then it is json serialized surrounded by quotes.* Better handling of custom address.* Revert ""Better handling of custom address.""This reverts commit 77935c53cfb22d7e75067c6218dca5e7deb6953b.",0
[Meta Scheduler] Add cleanup for localrunner (#9191)* add cleanup for localrunner* remove build results,1
"[UnitTests][CMSISNN] Mark CMSISNN with skipif they are missing libraries (#9179)* [UnitTests][CMSISNN] Mark CMSISNN with skipif they are missing librariesShow test as skipped, rather than failing test.* Added tvm.testing.requires_cmsisnn",1
[TIR] Fix FlattenBuffer computing size for buffer with strides (#9195),0
Update ci-cpu to v0.78. (#9199)* Close #9158,1
[VitisAI] Update Vitis AI integration to 1.4 release (#8815)* Update Vitis AI to 1.4 release* Parameterize Vitis AI codegen tests* Update Dockerfile.demo_vitis_ai,1
Fix end to end benchmark with rpc devices (#9175)* Ensure that device used in end to end rpc is a local device* fix vm; add actually failing tests* bump roi_align test tolerances,0
[cpptest] Reset op attributes before registering them (#9202)Prior registration of attribute XYZ can result in an error whentrying to register the same attribute again:```Check failed: (p.second != plevel) is false: Attribute XYZ of opis already registered with same plevel=10```Reset the attributes before registering them.,0
Address Christopher's comments from #8788 (#9197)We don't need the Optional<IRModule> on ToANormalForm and friends.,1
Add TVMC Frontend for PaddlePaddle (#9083)* fix some problems for matmul* fix some problems for matmul* add alpha parameter for matmul* remove unnecessary condition* add TranslatedLayer which support model loaded by jit.load* add mul operator support* Add padding mode support for conv/pool2d* support 4 two-tuples* add paddle test case* add paddle conv2d  case* update test_forward.py* fix paddle convert_matmul* add paddle multiply and matmul op test case* add test case and fix bug* delete import pandas* add paddlepaddle tests* modify the variable name of convert_reshape* formatting* formatting* use black to format python code* pylint check* Remove fluid api* black format* Add Paddle Frontend for TVMC* refine code format* add test case for tvmc* fix pylint check* gen_requirements add paddlepaddle* Trigger CICo-authored-by: root <root@bjyz-sys-gpu-kongming3.bjyz.baidu.com>Co-authored-by: wjj19950828 <wjjisloser@163.com>Co-authored-by: heliqi <1101791222@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[UnitTests][CMSISNN] Mark Binary Ops CMSIS NN tests as skipped (#9200)https://github.com/apache/tvm/pull/9179 and https://github.com/apache/tvm/pull/9167 both landed at the same time which resulted in new tests without the skipping.,1
"Contributing the STM32 port (#7742)* Contribute apps/stm32 application.* Removed a useless file.* STM32: Use Model Library Format in demo. Added test.* STM32: Added quantized mnist test.* STM32: Fixed apps/stm32/Makefile.* STM32: Added quantized models test.* STM32: Added tests to tests/scripts/task_python_microtvm.sh* STM32: Listed specific files with lint.* STM32: removed external copyright notices.* STM32: Added missing ASF copyright notice.* STM32: style fixes.* STM32: more style fixes.* STM32: fixed liny for C files.* STM32: Does extern C help with cpplint.* STM32: Fixed wrong LINT_C_FILE spelling.* STM32: Still some lint fixes.* STM32: more style.* STM32: More fixes lint+formatting.* STM32: cleanup.* STM32: style cleanup.* STM32: Moved ai_runner to the apps/stm32.* Alignment with PR 7742* lint cleanup.* STM32: Use crt_backend_api.c with standalone build.* STM32: Fixed the CI test.* STM32: style fixes.* STM32: Removed unused files.* STM32: Moved to crt_backend_api* STM32: style fix.* STM32: style fix.* Revert ""STM32: Removed unused files.""This reverts commit d72f8e5be3bae3c36da1758b6e61cd39ef7036c7.Undo changes to c_backend_api/c_runtime_api.* Revert ""STM32: Moved to crt_backend_api""This reverts commit 6c0e66672cb636d47244e99e8efd893004acc382.Undo changes to the c_backend-api/c_runtime_api.* stm32: aligned to micro TVM structure.* stm32: improved the python style.* stm32: cpplint fixes.* stm32: Fixed the test* stm32: style fixes.* stm32: style fixes.* stm32: style fixes.* stm32: style fixes.",0
Update ci_i386 to v0.74. (#9211)* Close #9158.,1
fix (#9205),0
Arm(R) Cortex(R)-M55 CPU and Arm(R) Ethos(TM)-U55 NPU Demo App (#8922)This commit adds a demo application that uses TVM to run a model on bare metal Arm® Cortex®-M55 CPU and Arm® Ethos™-U55 NPU. The demo demonstrates running Mobilenet_v1 TFLite model on the Fixed Virtual Platform (FVP) of the Arm(R) Corstone(TM)-300 reference package.,0
"Add dilation to MaxPool2DAttrs Rust bindings (#9215)Gets Rust bindings back in sync. Dilation must have been added to MaxPool2DAttrs on the C++ side, without it being added on the Rust side. There should probably be tests to catch this!",1
[PyTorch][Frontend] Semantic difference of 'bias_add' between relay and pytorch (#9204)* fix bias_add* add test* add test,0
Adding annotations for tir.allocate (#9168)* Adding annotation for tir.allocateThis commit is adding annotations for tir.allocatenode to be used as hints for future transformations.Change-Id: I02a3a875c38c3edd449385da5b741ef4958bb47f* Adding annotation for tir.allocate* adding tvmscript support* adding tir text printing supportChange-Id: Id0b6725b2e79c23f6b8ff192772f1ea4125a27c2,1
"[UnitTest] Removed vulkan from CI run of task_python_topi.sh (#9219)Vulkan unit tests were enabled withhttps://github.com/apache/tvm/pull/9093, but were only intended to runtests/python/unittest/test_target_codegen_vulkan.py.  Sincetask_python_topi.sh did not explicitly specify `TVM_TEST_TARGETS`, itdefaulted to `tvm.testing.utils.DEFAULT_TEST_TARGETS`, which includesthe vulkan runtime.This commit adds an explicit specification of `TVM_TEST_TARGETS` forthe topi unit tests, matching the targets enabled in the CI GPU build,but excluding vulkan.",1
"[TVMC] Treat invalid FILE arguments (#9213)Currently if an invalid FILE argument is passed to 'tvmc run' the userwill catch a traceback because exceptions are not treated, for instance:$ tvmc run /tmp/nonexistingfile will throw a FileNotFoundError trace.This commit catches the possible exceptions that can happen when aninvalid FILE argument is used and convert them to better messages forthe user so the invalid FILE can be easily spotted.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"Update virtual_machine.rst (#9222)Basic cleanup of language to replace ""TODO,"" which makes this section look very incomplete.",1
[Frontend][PaddlePaddle][Part1] Add 100+ operators for PaddlePaddle (#9126)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes,0
"Migrate C Interface API Generation to C++ (#9106)Using the new name transformations added in #9088, the C interface API is now generated in C++ rather than in Python. This is intended to be a no-op for the actual users of this change and thus I've undone some of my overzealous sanitizing to match that expectation.Follow up PRs will clean up any remaining name transformation inconsistencies.Fixes #8792",0
[BugFix] Fix a predicate bug in TIR schedule primitive `rfactor` (#9228)* Fix predicate bug of rfactor* Add regression test,0
"[TOPI] Fix compiing batch_matmul and dense when two args are the same tensor (#9207)* Add explicit copy stage for batch_matmul(x, x) case* do copy in relay strategy to avoid dup* add copy to dense op and schedules* black* add batch_matmul test* add dense test* fix cuda int8 dense test* remove need_copy flag* do not use tag to decide if tensors are same* rename to copy_if_identical and add comment* black* one more fix missed* add length check on input tensors* one more length check* fix variable name",0
"BUG #9216: Don't disable FuseOps pass since required by GraphExecutor (#9227)This tutorial disabled the FuseOps pass, but before #8788 that wasignored since FuseOps was applied directly.",0
Run full build when no files were changed over main. (#9221),2
"[cpptest] Use find_package to locate GTest files (#9208)* [cpptest] Use find_package to locate GTest filesThere is a standard CMake utility for finding GTest, use that insteadof doing a manual search.Thanks @tkonolige for the suggestion!* Use GTest:: targets instead of variable* Add USE_GTEST to cmake/config.cmake* Remove GTEST_INCLUDE_DIRS from target_include_directoriesCmake will automatically propagate the interface include directoriesto the dependends of (in this case) GTest.* Restart CI",1
[Frontend][PaddlePaddle] Fix bug for paddle frontend (#9236)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes* fix bug for paddlepaddle frontend,0
[BugFix] Fix to allow zero-copy between numpy and TVM NDArrays (#9230),0
"Documentation Refactor (#9203)* Documentation Refactor - Stage 1RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 1 of the documentation refactor reorganizes the docs structure,moving files (without content changes) and adding new scaffolding togenerate the proper document tree.It does not address naming, style, content, links, or other existingcontent in documents that were moved. State 2 will address fixing theseissues with existing content.Major changes include but are not limited to:* Dividing the existing tutorials into two sections:  * Tutorials  * How Tos* Moving all of the existing tutorials out of the `/tutorial`  directory and into the more general `/gallery` directory.* Breaking up how-tos into individual sections for more  flexibility and more consistent rendering.* Moving content into new classifications:  * `/docs/arch` for architecture guides  * `/docs/reference` for API guides and other reference material  * `/docs/topic` for topic specific guides such as microTVM and VTA  * Restructuring `/docs/dev`* Adding a table of contents to the doc index* Adding instructions on how to install using third-party tlcpack* Documentation Refactor - Stage 2RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 2 of the documentation refactor fixes naming and linksin the documentation to be consistent with the overall structure.Major changes include:* an update to how to contribute to docs.* several updated index pages with title changes to match  the organization style and bring consistency to the sections* expanded descriptions of some page collections* fixed links* Documentation Refactor - Stage 3RFC: https://github.com/apache/tvm-rfcs/blob/main/rfcs/0027-formalize-documentation-organization.mdTracking Issue: https://github.com/apache/tvm/issues/8987Stage 3 of the documentation refactor adjusts CI for the new structure.The CI build script takes into account the new gallery format. Italso prevents deleting existing documents, and takes advantage of the`_staging` and `_build` directories to clean out previous builds.",0
[CI] Use correct tag in Docker --cache-from (#9234)When I didn't have `:latest` available I saw that my image wasn't beingre-used between runs.,3
[DOCS] Fix installation from source link some text (#9238)Fix install from source link(pointed to matplotlib).Updated some wording.Move description of tlcpack to just a link so it can be kept from tlcpack side.,0
"Initial Implementation of TIRToRuntime Target hook (#9190)* Initial Implementation of TIRToRuntime Target hookThis is the initial implementation which wires in a test case for TIRToRuntime, in order to get this working I re-used `CodegenCHost` as it implements all of the `Op`s required from the lowered `PrimFunc`.Currently, the `IRModule` is non-unified but in future work it should definitely do so, I wanted to implement the basics here to get the infra in place.* Fix heterogeneous compute with multiple kDLCPU targets* Remove rogue te_compiler.h include",0
[TVM] Add importer for ONNX QLinearMatMul op (#8952)* adds importer code * enables `test_qlinearmatmul_2D` unit test,1
"Arm(R) Ethos(TM)-U NPU Depthwise2d operator support (#9209)* Arm(R) Ethos(TM)-U NPU Depthwise2d operator supportThis commit adds support for Depthwise2d primitive operator throughoutthe TVM stack including Relay legalization pass, operator definition,TE, TIR passes and translation into the command stream.Change-Id: If82b85f5d3b23cd214fe38babd724451bf95ef5b* Change depthwise2d to depthwise_conv2dAnd respond to other review comments.Change-Id: I58a9f28723750970d386b4d0ba62fa399c5c6181* Make a line shorter and add a commentChange-Id: Idf4c078bf65e7ed31fe82a92bf334295a82b6ead* Change the order of importsChange-Id: Ic6c77af30a5b9cb68dcc0c173b95490965359481* Whitespace changeChange-Id: I7318bd8cfa5985b33fc7d020cc19057cc9498197",1
[Relay] Gather op dynamic input support (#9240)* support gather op dynamic input* fix shape func and add test* remove constness check* fix shape func output rank* restore checkCo-authored-by: masa <masa@pop-os.localdomain>,0
[AlterLayout] Strided slice layout transform fix (disallow NCHW4c -> NCHW etc properly) (#9245)* prohibit propagating through packed to unpacked layout* add test,0
[RPC] Fix Server connecting to RPC Tracker through a Proxy (#9210),0
Fix typo in error message in CMakeLists.txt (#9251),0
add stage to log (#9249),1
"[TIR][USMP] Add a parallel to serial for loop converter pass (#8469)* [TIR][USMP] Add a parallel to serial for loop converter passThis is an optional pass to convert all parallel for loops in TIRto serial ones for different reasons such as executordoes not support parallel launch of for loops (e.g., AoT)or allocating space for parallel for loops might notbe desired.* Additionally adding FFI scaffolding for USMPChange-Id: Id5e8ccb90140d2d3ae113b20a3ca152a54497c45* [TIR][USMP] Add a parallel to serial for loop converter pass* remove unused importChange-Id: I29d5fdec92120418596f9dba1d6630f65620a603* [TIR][USMP] Add a parallel to serial for loop converter pass*moved the pass to tir namespaceChange-Id: I74720ca2f566066b3a4f22f504d8f0f684c99dc2* [TIR][USMP] Add a parallel to serial for loop converter pass* fixed docstringChange-Id: I73bb9867fe2ed6a86f65666493c5c6e3edf87b49* [TIR][USMP] Add a parallel to serial for loop converter pass* fixed mypy lint errorChange-Id: I226ef27d5536674fbe4b2d2c6ff47b8cb3b41431",0
[Relay] Improve reduction op layout propagation for packed input  (#9253)* wip* fixed packed dim size logic* fixed test* formatting* fix compile warning,0
"[microNPU] Enforce bias when pattern matching conv2d (#9244)Currently a conv2d pattern is matched when no bias is present.However, legalization expects a bias to be present, thereforecausing an error when this is not the case. For now, enforce abias when offloading conv2d to the NPU.Change-Id: I7f74b0f2c151f51ddc66ee1c5ebb77534238909b",0
Fix USMP parallel to serial loop transform test (#9254)Caused by https://github.com/apache/tvm/pull/8469 being stale on merge when https://github.com/apache/tvm/pull/9115 had changed the namespace for `tvm.script`.,0
[TVMC] Split common tvmc test file into more specific files (#9206)The `test_tvmc_common.py` file was becoming a bit of a mixed bag oftests and as we now want to extend the `Target` processing logic it madesense to split each out into its own file to make it clearer what eachdoes.`test_common.py` has also been renamed before we start using it for all thetests instead.,2
Address review comments on Arm(R) Ethos(TM)-U PR 3/6 (#9159)* Address review comments on Arm(R) Ethos(TM)-U PR 3/6Change-Id: I22961885a503be31f6a72622ae0b5f874cc6f463* Fix rebasing errorChange-Id: I3e2fde786096ea331fcb366080fa779ec4ea4a5d* Fix more rebasing problemsChange-Id: I1026e3ccee33a3fdec9ebbf6456bae244ad4f1d5,0
[Simplifier] Add printing of SplitExprNode and SumExprNode (#9262),1
fix docs (#9266),0
"[LLVM] Treat scalars as single-lane vectors in CreateVecConcat (#9264)LLVM differentiates between `<1 x ty>` and `ty`, while TVM does not.Make sure that a bunch of TVM scalars can be concatenated into avector when generating LLVM IR.",5
[TIR] Added PrettyPrint of ProducerStore/ProducerRealize nodes (#9259),1
"[TIR] Minor refactor to tir.transform.StorageFlatten (#9260)Expressed each step as a separate `transform::Pass`, so they can beused/inspected individually.",2
"[TVMC] Compose target options from target registry (#9218)* [TVMC] Split common tvmc test file into more specific filesThe `test_tvmc_common.py` file was becoming a bit of a mixed bag oftests and as we now want to extend the `Target` processing logic it madesense to split each out into its own file to make it clearer what eachdoes.`test_common.py` has also been renamed before we start using it for all thetests instead.* [TVMC] Compose target options from target registry[The RFC for this is still under discussion](https://github.com/apache/tvm-rfcs/pulls), but doing this before splitting the registries makes the most sense. This enables the `tvmc` driver to re-combobulate Target options from arguments:```tvmc --target=llvm \    --target-llvm-mcpu=cortex-m3```",2
"Hexagon conv2d full output slice  (#9198)* split h axis by constant factor 2; no cache write* enable cache_write, but not yet able to compute_at* cache_write with compute_at* cleanup, make loop split semantics more clear* parameterize height loop split* nhwhwc wiggling (needs cleanup)* added input channel splits for crouton depth* cleanup variable names and magic numbers* comments* add README* added 3x3 conv2d (no padding) case* add ASF header and RFC link* cleanup README",1
[Hexagon] Add hexagon launcher to apps and add to TVM's build system (#9220)* Add USE_HEXAGON_LAUNCHER cmake configuration to build the androidhexagon launcher along with a standard TVM build.* Update hexagon launcher README.md to includeinstructions on how to build the launcheralongside TVM.* Move Hexagon launcher into top-level apps directory.* Refactor hexagon launcher cmake directorystructure and group common code intocmake/HexagonLauncher.cmake.* Address CRs from @kparzysz-quic.,1
Propagate tvm target through graph tuning setup (#9248)* Propagate tvm target through graph tuning setup* Don't append -device if it is already present in tvm_target* Make sure target string has device tracing only* revert accidental reformat* Update per review comments* fix lint issue* Use string for tvm target* Update python/tvm/autotvm/graph_tuner/utils/traverse_graph.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* Cleanup testsCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[Topi] Fix direct SIMD conv2d schedule name (#9225)* feature tested* change name,0
Bumping up CMSIS-NN version to be in sync with TFLu (#9247)Change-Id: I51103632f6d41652d616857f987a846ea2b22a5c,4
"[Runtime] Pipeline Executor Second patch, configuration load and executor export/import. (#9108)* [pipeline executor] Add configuration load function and pipeline executor export,import function.* address review comments.* polish comments and doc string.* address review comments.* address review comments.* Change mod_idx start from 0, remove mod_idx - 1 logic.* address review comments.* polish documents.* adress review comments* address review comments.* address review comments.* polish the document.* address review comments.* address review comments.* polish comments.* Triger build.* address review comments.* address review comments.* fix grammar issue.* polish documents.* add single global binding check.* address review comments.* trigger build.",0
[CI] Pre-build Reference System Dependencies (#9270)Building these dependencies from scratch in each test was taking muchlonger than really necessary.Before:```$ time python3 -m pytest tests/python/contrib/test_ethosu/test_codegen.py::test_tflite_depthwise_conv2d[strides0-dilation0-SAME-kernel_shape0-relu-ifm_shape0-ethos-u55-256]real    0m19.982suser    0m13.255ssys     0m3.403s```After:```$ time python3 -m pytest tests/python/contrib/test_ethosu/test_codegen.py::test_tflite_depthwise_conv2d[strides0-dilation0-SAME-kernel_shape0-relu-ifm_shape0-ethos-u55-256]real    0m10.963suser    0m5.516ssys     0m2.232s```,3
"[Pytest] Sort unit tests before running. (#9188)* [Pytest] Sort unit tests before running.By default, pytest will sort tests to maximize the re-use of fixtures.However, this assumes that all fixtures have an equal cost togenerate, and no caches outside of those managed by pytest.  A fixturefor a `tvm.testing.parameter` is effectively free, while a fixturemaintaining a cache of reference data`tvm.testing.utils._fixture_cache` be quite large.Since most of the TVM fixtures are specific to a python function, sortthe test ordering by python function, so thattvm.testing.utils._fixture_cache can be cleared sooner rather thanlater.* Updated TestTargetAutoParametrizationWhen sorting the tests, the order of parametrizations may change.Therefore, the tests checking for automatic target parametrizationshouldn't depend on order.",0
"[ONNX] [Relay] Resize Opset 13 (#9265)* Fix handling of optional inputs.* Missed one test in the ignore list.* split 11 and 13* removed comments, adjusted for git reviewCo-authored-by: Josh Fromm <jwfromm@uw.edu>Co-authored-by: Matthew <mbrookhart@octoml.ai>Co-authored-by: CircleSpin <jocelyn@pop-os.localdomain>",0
Skip onnx test cases if no onnx (#9272)This was missing a guard which meant VS Code errored on test collection.,0
"Update TVM_LOG_DEBUG for IR tracing. (#9278)* Update TVM_LOG_DEBUG for IR tracing.Forgot to do this when I switched to VLOG, sorry.* Woops, remove src/ prefix.",0
[Core][Build] Move build module transformations and utilities to C++ (#9103)* Initial investigation* More progress!* More progress / notes* rewrite build_for_device mostly in c++* More progress* Initial split of transformations applied to device and host as post split action from mixed module* Combine duplicate passes after spliting mod on aot and vm flows* Minor cleanup* Move target mangling to driver_api.cc* Move more build utlities to cpp driver api* [Build][WIP] Moving build utilities to C++ from Python* [Build] Remove comments* [lint] Pass black* More formating* Move more build functionality into cpp* Remove comments* Remove unused defs and imports* Address PR comments* More PR comments* More comments* More comments* Add comments on the new split function* Fix PR comments on clarity* Test CI* Fix format* Refactor build* Expose splitted composite passes to python* Format files* Test fix* Fix for annotating entry funcs on code targeting CPU* Prevent entry funcs to be annotated when compiling for CPU with C runtime enabled* Guard for aot executor entry* Sphix format* Sanity fix* Sphinx fixCo-authored-by: electriclilies <lilyorthsmith@gmail.com>,0
[Tutorial] Fix vta vision detection tutorial 'sphinx' style error. (#9279)Issue:   Some bash code in this tutorial does not get syntax highlightingbecause of the format errors.Solution:   Fix the 'sphinx' 'rst' style error.,0
"Reset sphinx-gallery version to 0.4.0 (#9280)Seeing:```ERROR: Could not find a version that satisfies the requirement sphinx-gallery==0.4.1 (from versions: 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.10, 0.0.11.post1, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.8.0, 0.8.1, 0.8.2, 0.9.0, 0.10.0)ERROR: No matching distribution found for sphinx-gallery==0.4.1```This was changed in https://github.com/apache/tvm/pull/9115",0
"[Tests] Ensure MyPy type checks pass (#9284)* [Tests] Ensure MyPy type checks passThere's a few errors that come up when type checking that aren't triggering any failures:```Checking MyPy Type defs in the meta schedule package.python/tvm/meta_schedule/utils.py:23:1: error: Cannot find implementation or library stub for module named ""psutil""python/tvm/meta_schedule/utils.py:23:1: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-importspython/tvm/meta_schedule/search_strategy/search_strategy.py: note: In member ""__init__"" of class ""MeasureCandidate"":python/tvm/meta_schedule/search_strategy/search_strategy.py:59:13: error: Module has no attribute ""MeasureCandidate""python/tvm/meta_schedule/search_strategy/search_strategy.py: note: In member ""initialize_with_tune_context"" of class ""SearchStrategy"":python/tvm/meta_schedule/search_strategy/search_strategy.py:83:9: error: Module has no attribute ""SearchStrategyInitializeWithTuneContext""```To rectify this the `types-psutil` package adds type hints for `mypy` and `# type: ignore` stops `mypy` from trying to figure out types of `_ffi_api` resources.There's also a few places where variable type definitions are repeated even though they're only required once.Finally, I've ensured `task_mypy.sh` fails the build since it's stable right now, using `set -e`.* Add temporary # type : ignore for psutil",0
[TIR] Add support for 0-dim buffer (#9224),1
"[TFLite] Add option to overwrite OperatorConverter class in relay.frontend.from_tflite (#9256)* [TFLite] Relay Frontend: Add option to overwrite OperatorConverter classThis allows to overwrite the mapping from TFLite Operators to TVM Relay Operators from external python scripts. This has the following advantages:- Adding support for unsupported builtin or even custom operators by adding a hand-written convert function- Enables overwriting of existing convert functions for supported operators by alternative implementations (useful for currently unsupported edge cases)Example Usage:```class CustomOperatorConverter(relay.frontend.tflite.OperatorConverter):    def __init__(self, model, subgraph, exp_tab):        super(CustomOperatorConverter, self).__init__(model, subgraph, exp_tab)        convert_map_overwrite = {""SUB"": self.convert_sub_custom}        self.convert_map.update(convert_map_overwrite)    def convert_sub_custom(self, op):        ......relay_mod = relay.frontend.from_tflite(    tflite_model, shape_dict=shape_dict, dtype_dict=dtype_dict, op_converter=CustomOperatorConverter)```[TFLite] Make sure that even DETECTION_POSTPROCESS op can be overwrittenThis is desirable, because the current implementation of this CUSTOM op is incompatible with MicroTVM targets* Tests: added test case for overwriting op_converter in TFLite relay frontendKept the test as simple as possible by only comparing 2 differentimplementations of a SUB TFLite operator:1. Original: c = a - b2. Dummy: c = a + (-b)Comparison with TFLite reference output is not necessary because tis isalready covered by other test cases. Instead comparisons of the two TVMmodels are used.",1
[Frontend][PaddlePaddle] Remove unused parameters and fix doc string (#9283)* add part of operators* remove part of operators* add lookup* add test* Update paddlepaddle.py* modify error message for SAME padding* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* Remove some function and old version operator* add dot test* modify doc* remove unreviewed code* Update paddlepaddle.py* Update test_forward.py* Update paddlepaddle.py* Update paddlepaddle.py* Update test_forward.py* Update test_forward.py* add more cases for tests* add more cases for tests* remove annotation* reduce test case sizes* Remove unused parameters and fix doc string for paddle frontend* remove blank line* fix code error* modify test_forward.py,0
[Profiler] Do not aggregate frames with different devices (#9290),2
[Hexagon] Fix addressing TVMValue array (#9302),0
[Profiler] Sort columns in table and csv output (#9300)This should fix some problems around flakey profiler tests based onorder of columns.,0
"[TE] Light refactoring of TE -> TIR paths. (#9263)* [TE] Light refactoring of TE -> TIR paths.- Added ScheduleToPrimFunc, extracting out common behavior in  ScheduleToModule and auto_scheduler's feature extraction.- Added `tvm.driver.build_module.schedule_to_module`, to avoid needing  to 4-line boilerplate needed to do so.  Also makes deviations from  the usual path (e.g. `debug_keep_trivial_loop`) much more explicit.* Removed schedule_to_primfunc, replaced usage with schedule_to_module.* Returned C++ function ScheduleToPrimfunc to be inside ScheduleToModule.",0
[iOS] Fix build issues on the latest XCode and iOS (#9298)1. Specify target for compiled dylib2. Specify Metal Shader Language version when we compile metal library.,0
Rename build helper (#9297),5
[TVMC] Support dot inside of TVMC input shape name arguments (#9294)* [TVMC] Support dot inside of TVMC input shape name arguments* dot -> dots,2
fix typo (#9304),0
llvm 14 and above move TargetRegistry into MC (#9305)TEST=build with latest llvm,3
[unittests] Skip import of tvm.micro if micro-TVM was not enabled (#9301),3
"[microTVM][RVM] Always destroy the VM if all tests pass (#8739)Currently base-box-tool 'test' command will skip destroying the test VMif a single provider is specified (i.e. --provider virtualbox) even ifall tests pass. This is confusing (no warning is displayed to the user)and that will leave host resources (like USB devices necessary to runthe test) locked by the VM. So if the user tries to run a program thatuses the locked resource (e.g. openocd) cryptic failures might happen.Moreoever, even if all tests pass and more than one provider isspecified but the option '--skip-build' is set a VM will be left runningwithout notice.This commit changes that behavior by:1. Always destroying the VM if the release test pass2. Always keeping the VM up and running if a test fails1. guarantees no resource remains locked by the VM without necessity. Anew flag '--skip-destroy' is introduced in case the user still wants tokeep a VM up and running if the release tests pass. 2. guarantees the VMwhere the test failed is left running for further inspection of the testthat failed.Finally, for both 1. and 2. cases a proper message is displayed to theuser to inform if a VM was left running or not and about what actionsthe user can take next accordingly to the test result in the VM.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",1
update block syntax (#9286),1
Test run triage (#9308),3
"[Codegen][LLVM] Add ability to turn on fast math flags (#9223)* flags to turn off and on* turn fast math on always* llvm more opts* move to default codegen opt* TODO* add fast math options to llvm target* move to using new target attributes* llvm fast math target opt code* add -O flags* fix todo lint* support llvm 4.0, 5.0* use same opt level as target machine* revert TargetOptions* fix thing* prevent regression in llvm* togglable opt-levelsCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>",0
[Profiler] Add significant VM instructions to profiling report (#9292)Added a hooks to the VM execution loop to record runtime of certaininstructions with significant runtimes. Now the profiling report willinclude data allocation and transfer times.,1
"Fix direct and broken links (#9314)Updates links to use references instead of direct links, fixingbroken links and making all internal docs links more durable torefactoring",0
[Keras] Support return_sequences in LSTM (#9303),5
fix missing span arg (#9318),0
[Community] @elvin-n -> Reviewer (#9321),3
Adjust Hexagon conv2d schedule to split channel out (k) and move to outer loop (#9287)* Adjust Hexagon conv2d schedule to split channel out (k) and move to outermost loop* add missing reference data verify,1
Add conv1d support in BYOC TRT by converting conv1d to conv2d (#9324)Co-authored-by: ziyu.guo <ziyu.guo@bytedance.com>,1
"[Relay, TOPI] Add searchsorted op (#9184)* Add relay definition* 1D cpu test working* multi dim working* gpu version working* check shape in type rel* support side* use target specfic max threads* add relay boilerplate* relay test working* cleanup topi test* fix test* add torch converter* handle other cases* more topi test* support torch bucketize* update doc* fix tests* fix lint* rebase fix* make the test case smaller* add tests for edge cases* replace ""side"" attribute with boolean ""right""* add more descrition to binear_search IR gen params* return index from binary_search rather than update inplace* remove unused argument* format fix",0
[Error reporting] Replace runtime errors with LOG(FATAL) (#9311)* Replace runtime error with LOG(FATAL)* flaky test,0
Use variable in curl download url (#9330)* Use variable for curl download url* Replace qemu-5.1.0.tar.xz.sig with ${QEMU_SIG_FILE},2
[Relay] Remove FTVMCompute from TNonComputational ops (#9334)* remove FTVMCompute from noncomputational ops* Remove injective schedule registration for on_device since it is non-computational* lint,4
Specify argument to FastMathFlags setAllowContract (#9337),5
[microTVM][Arduino] Cleanup template directory (#9289)* restructure* readme* fix readme* trigger,0
[Op] Do not override specified layout in pooling (2nd PR) (#9328)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR)* [Op] Do not override specified layout in pooling (2nd PR),5
[ETHOSN] Match config for is-supported with compilation target (#9160)The Ethos-N variant configuration for the is-supported functionality is nowthe same as the variant configuration for the actual compilation,5
[Community] @ganler -> Reviewer (#9346),3
"BUG: Look through on_device annotations when looking for shape constants (#9345)https://github.com/apache/tvm/pull/8788 introduced a perf regressionsince a `shape.as<ConstantNode>` in `alloc_tensor` was always failingdue to the extra `on_device` annotation on the constant. Fixed that,and introduced some helpers to make this situation easier to deal with.(This is CORE-102 in OctoML JIRA).(Second try -- test_crp.py failure seems unrelated)",0
Disable Hexagon TestConv2dPackedFilter test (#9344),3
[Hexagon] Fix cmake files for Hexagon launcher (#9343)* [Hexagon] Fix cmake files for Hexagon launcherThe update to build the launcher automatically accidentally brokebuilding it separately. This patch fixes that.Also included are a few minor fixes and an update to the README.md.* Clarify support for Hexagon codegen,0
Support dynamic shape searchsorted (#9348),5
[microTVM][Zephyr] Enable RISCV Tests on QEMU CI (#9325)* add riscv32* add riscv64* fix url,0
[CORE][Relay] Swap and remove compile_engine with te_compiler followup of #8775 (#9282)* Remove compile_engine.h for real* Fix format* RM compile_engine.cc* Swap compile engine with TECompiler* Cleanup on compile engine py leftovers* [WIP] Exposing legacy compile engine capabilities through TE Compiler* Swap usages for depreciated compile engine with TE compiler* Track and replace usages of compile engine refactor them to TE compiler* [Docs] Log helper mod* Remove depreciated function for lookup compile engine cachce* Fix typos* Debug misc cleanups* Register global pass for using te compiler for auto scheduler* Fix tests using the legacy compile engine* Fix broken autotuner tests and minor cleanups* Swap compile engine with te_compiler in rst config* PR nits* Fix failed testCo-authored-by: Jared Roesch <roeschinc@gmail.com>,0
"[Code Style] Changed code to match the tvm code style conventions. (#9040)* [Code Style] Changed code to match the tvm code style conventions.[Issue]While reviewing the tvm code, I noticed some naming convention issuesin the diag_ctx_ and current_func variables.Variable current_func should be current_func_ because it is a classvariableVariable diag_ctx_ should be diag_ctx , because it is a public variable[Solution]Changed the variables to match the tvm code style conventions* addressed comments* removed debug logic* fixed plint issue* fixed building issue* fixed whitespace issue* fixed linting error in type_solver.cc",0
[Frontend][PaddlePaddle] Add autopad for conv/pool (#9295)* Add autopad for conv/pool* add autopad for conv/pool* fix pylint warning* add some annotations* add som annotations* add som annotations* Refactor autopad in the onnx.py and paddlepaddle.py to relay/frontend/common.py* add comment for conv2dCo-authored-by: heliqi <1101791222@qq.com>,0
"[UnitTest][Flaky] In test_report_serialization, compare csv. (#9275)* [UnitTest][Flaky] In test_report_serialization, compare csv.`str(report)` calls `ReportNode::AsTable()`, which includes aggregatevalues.  Otherwise negligible differences in the computed value can berounded differently after the round trip.  This was first [noticed inCI](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-9194/7/pipeline/#step-246-log-1217)for an unrelated PR.  Testing locally, this failure mode occurred 2times out of 3000 trials.Switching to `report.csv()` avoids this issue, as it does not includeaggregates.* Switched back to using AsTable(), but with column sums disabled.The .csv column headers are in arbitrary order, and do not testwhether the `device_metrics` field has been serialized/deserializedcorrectly.* Added explicit sorting of columns to Report::AsTable",1
"[Tutorial] Fix formatting, grammar, dead link (#9281)* tutorial: preprocess.py: Fix leading whitespaceThis fixes the indentation of metadata in `preprocess.py` in the TVMC tutorial, removing the leading whitespaces in the HTML rendering[^1].[^1] https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html#preprocess-py* tutorial: Add missing code block escapes* tutorial: Grammar fixup* README.md: Fix link to introductionCo-authored-by: Martin Kröning <martin.kroening@neclab.eu>",0
[Caffe Frontend] Add support for Embed layer (#9257)* [Caffe Frontend] Add support for Embed layer* [Caffe Frontend] Add support for Embed layer,1
[TIR] Add structural error printing for TensorIR (#9306)* add structural error printing* remove old code* address comments* address comments* add test* fix test case* fix nested loop* rm print* change simple loop cond* address comments* fix test* address comments* remove msg* add override* address comments* address comments,0
"Fix inconsistencies in graph_executor function names handling (#9255)* Clean up redundant code in graph_executor.ccHow did these lines ended up here?* Fix inconsistencies in graph_executor function names handlingUpdates value of `TVM_CRT_MAX_STRLEN_FUNCTION_NAME` from `80` to `120`Replace all occurences of `[120]` with `[TVM_CRT_MAX_STRLEN_FUNCTION_NAME]`to maintain consistency and make the array lengths user-configurable.Introduces `TVM_CRT_MAX_STRLEN_PARAM_NAME` used for parameter names onlyAdds comments to `kMaxFuncNameLength` variabe in src/relay/backend/te_compiler_cache.ccmaking sure that the values are kept ""in sync"". (sort of)See #8953 for more context. The actual bug reported there however canonly be fixed by increasing the TVM_CRT_MAX_STRLEN_FUNCTION_NAME to avalue larger than the maximum possible truncated function name length(including prefixes and suffices)Example:  6['tvmgen' prefix length]+ 7['default' model name length]+ 5['fused' fused function name prefix length]+ 80[truncated function name length]+ 19[length of appended hash]+ 4     [Number of '_' between components]= 121",0
"[TVMScript] Parser for Lambdas, Parser/Printer for `CommReducer` (#9358)* CommReducer Parser/Printer* update argmax unit test* update doc* lint fix* add unit tests with multiple reducers",0
[Fixbug] Report duplicated param names of relay function when bind params (#9350)* [Fixbug] Report duplicated param names of relay function when bind params* add test* lint,0
[BugFix][TIR] Fix primitive `Bind` for init-inside blocks (#9359)* [BugFix][TIR] Fix primitive `Bind` for init-inside blocks* fix python black error,0
[COMMUNITY] Xiyou Zhou -> reviewer (#9361),3
[microTVM] Add platform version check to template project (#9274)* add platform version to project template* fix arduino cli version on qemu* specify arduino/zephyr version everywhere* cleanup* address comments* fix warning message* fix typo* trigger* trigger* address comments* trigger* trigger,0
[TIR] Move UnifyThreadBinding to earlier stage (#9365)* Move unify thread binding to earlier stage* Unify thread binding support AttrStmt,4
[COMMUNITY] Mehrdad Hessar -> Reviewer (#9366),3
Fix Arm(R) Ethos(TM)-U55 NPU Demo app (#9323)* Change tvmc arguments to -executor=aot -interface-api=c -unpacked-api=1.* Enable the demo running on the CI.,0
[COMMUNITY] Mark Shields -> Reviewer (#9369),3
"[Relay] Introduce Executor and Runtime representations with associated registries (#9246)This is the underpinning Executor/Runtime objects for https://github.com/apache/tvm-rfcs/pull/29, this doesn't change the wiring as yet since that's a pretty big change in itself. Most of this patch is TVM boilerplate, which I've tried to minimize - there's maybe some future work to decide how to more easily define some of these things.",4
[Torch] Add aten::roll support for Swin Transformer (#9371)* add test* first impl* basic example working* all test cases working* support adaptive avg and max pool* cleanup* axes transpose logic fixed for roll* pylint* fixed roll dim indexing,0
Support runtime defined function wrapping of library module packed functions (#9342)* Expose DSOLibrary and add documentation.* Allow runtimes to specialize library module functionwrapping by providing a PackedFunctionWrapper objectat construction.* Apply clang formatting.* Use std::function.* Minimize DSOLibrary interface.* Add param and return documentation to PackedFuncWrapper type alias.,1
[Hexagon] Refactor directory structure to accommodate new runtime (#9354)* Move hexagon runtime files used for ARM offload to Hexagoninto runtime/hexagon/android and runtime files compiledfor hexagon into runtime/hexagon/hexagon. Use commonhexagon_module.h for both hexagon and android runtimes.* Apply clang formatting to hexagon files on main.* Apply cpp-lint,1
[Hexagon] Introduce new DeviceAPI (#9355)* Compile hexagon device api from runtime/hexagon/hexagon whenbuilding for hexagon and USE_HEXAGON_DEVICE is not set.* Add hexagon_common.h utilities including customtvm runtime logging for hexagon.* Introduce HexagonBuffer class to store hexagon allocation metadata.* Add HexagonDeviceAPIv2 for use on hexagon.* Add hexagon packed function wrapper.* Add custom linked param lookup for hexagonthat wraps params in an unmanaged HexagonBuffer.* Add custom hexagon module based of library module node.* Apply clang formatting* Apply cpplint changes.,1
[Hexagon] Launcher modifications to make use of the new device API (#9356)* Use custom lookup_linked_params function inapp/hexagon_launcher.* Use new device api in hexagon launcher.* Ensure app/hexagon_launcher uses kDLCPU for external allocationsso that CopyDataFromTo knows how to handle the provided memory.* Use loadfile_hexagon in app/hexagon_launcher.* Update hexagon launcher's get_output method toutilize NDArray's for copying in order to exercisethe DeviceAPI.* Apply clang formatting,1
"[PyTest] Sort by test location, but not parametrization (#9353)A follow-up from https://github.com/apache/tvm/pull/9188.  The`item.location` tuple contains `(filename, line_number, test_name)`,where the `test_name` includes a string representation of allparameters.  This change preserves pytest's sorting of parametrizedvalues within a parametrized test, rather than sorting by strings.",2
Fix typo in Git Usage Tips (#9377),0
Sync with upstream and use CreateDSOLibraryObject. (#9376),5
[TensorIR][Tutorial] Blitz course (#9315)* blitz course* update* black* update* update* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* Update gallery/tutorial/tensor_ir_blitz_course.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* typo* change linksCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,1
schedule_injective of arm_cpu should consider dtype itemsize (#9339)* schedule_injective of arm_cpu should consider dtype itemsize* trigger CI,5
[BugFix][Opencl] Explicitly cast min/max operands (#9374)* [BugFix][Opencl] Explicitly cast min/max operands* enable test_opencl_max,0
"[microTVM] Arduino: Fix MLF archive filename in generated project dir (#9320)* [microTVM] Arduino: Fix MLF archive filename in generated project dirCurrently generate_project API method is copying the input MLF archivefilename without renaming it to ""model.tar"" - hence not in accordancewith the specification. As a consequence when the server looks for thatfile to determine if it's a project dir or a template dir it alwaysdetermines it is a template dir since ""model.tar"" can never be found, soa TemplateProjectError() exception is thrown when instantiating aGeneratedProject class.This commit fixes that by correctly copying the input MLF archive tothe newly generated project dir as ""model.tar"" so the server can findit.It also takes the chance to change the MLF path returned byserver_info_query method: only if it's not a template dir the MLF pathis returned, otherwise an empty string is returned (it doesn't makesense to return a MLF path when it's a template dir because there isn'tany model associated to a template dir).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Retrigger CI",0
update ci-gpu to v0.78 (#9378),1
refactor Hexagon conv2d tests (#9333)* refactor hexagon conv2d tests* add nhwhwc schedule to logical filter tests* add gt_filter_block_shape function* turn on nhw8h8wc for logical filter and fix verification* cleanup* adjust command lines in the README,0
[OPENCL] Workaround for zero size allocation (#9379),5
[Frontend][PaddlePaddle] Add some activation、elementwise  and reduce operators (#9370)* add activations and unary operators* revert modify of slice* add test cases* disable signal capturing in paddle framework,1
"[iOS][RPC] Enable iOS simulation in public CI to cover basic tuning capabilities (#9212)* init class for launch of server with ios simulator* init infrastructure of tests* added functionality for automatic loading of the simulator* add error handling for simulator interaction* extend tests for connection configurations* add test for pure rpc connection* init test for remote call* add wrappers for connect configurations* remove duplicate code* add tests for simple remote call* change policy for tests of connect configurations* added tests to check basic functionality of rpc session* add test for remote graph executor* add test for auto schedule tuning* remove hardcode parameters* add success criterias for auto schedule tuning* fixing problems related to running tests through the pytest* expand the workflow for new iOS RPC tests* update GH workflow for iOS* update GH workflow for iOS: conda shell* add parser for iOS RPC console log* add depends for ios tests* set verbose flag for rpc server* changes related with main checkout* add context manager class for ios rpc server launcher* extend pythonpath* add watchdog for start ios rpc server* clean up GH actions workflow* clean up GH actions workflow* rename enum SimulatorSystem to OSName* fix python format black* fix bash syntax* add doc strings for API* skip tests, because this type of connection was broken* fix lint* add check that current environment has required environment variables* code review fixes* replaced call os.system with call subprocess.check_call* add description for messages from iOS RPC Server",0
[microTVM] Fix AOT/ARMv7m tests on physical devices. (#9364)* init* test on hardware* moved to testing.py* add option to set workspace size* changed model* fix memory issue* fix tests* conv2d test* fix simd test* format* fix exception* refactor workspace helper function* address comments* revert to fix the bug* address comment,0
"[BYOC] CUTLASS integration (#9261)* byoc cutlass* add cmake and fix build* test worked but accuracy is bad* fixed argument printing properly* moving files* moving contents of cutlass_profiler into python/tvm/contrib/cutlass* run black* remove irrelavant codegen code* clang format* tried replacing sm 75 with 80, didn't help improve accuracy* remove irrelavant code from generator* tried dense + bias fusion but generated cu file does not compile* dense + bias worked after adding Leyuan's patch, bias + relu worked too* tried adding sm80 generator but accuracy is still off* remove GemmUniversal generator* cleanup partition and build* moved partition, profile and build function out of test* turned out the result match's TVM non-cutlass result. Numpy fp16matmul is busted?* clean up test* LinearCombination can be reused for bias only epilogue* remove unsupported epilogues like gelu* removing deadcode* unify gemm templates for with or without beta scaling* supported gelu but accuracy is slightly off* gelu test passed with relaxed rtol* cleanup* remove unused stuff from library.py* move profiler template into its own file* removed gemm_profiler.py* move contents of compile_engine.py into gen_gemm.py* rename to profiler_template.cu to avoid CI issue* cleaning up trying to pass pylint* add missing asf header* run black* fixing many pylint issues except wildcard import* fixed wildcard warning* add missing CUTLASS.cmake file, restore gemm_profiler.py* pylint* minor fix* add license* start filling in TODO doc* rename GemmProfiler to GemmProfilerEmitter* more renaming and doc* add doc to the main compile API* refactored generator* run black* black fix* finish doc TODO* add test for 32 bit accum* fixed kernel generator to correctly handle fp32 accum* revise build-related API* add option to profile only one kernel* add option to enable parallel compilation* clean up gen_gemm* doc update* profile_cutlass_kernels -> tune_cutlass_kernelsCo-authored-by: leyuan.wang <leyuan.wang@bytedance.com>Co-authored-by: Masahiro Masuda <masahi129@gmail.com>",0
[ETHOSN] Add support for non-default Ethos(TM)-N78 configurations (#9386)- Updated tvmc with new Ethos-N78 composite target.- Added additional Ethos-N78 specific configuration options.Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>,1
[iOS][RPC] Enable tests for connection configuration: tracker via proxy (#9398),3
[COMMUNITY] New reviewer -- wrongtest (#9400),1
[Support] Fix StartsWith when the string is equal to the prefix (#9393),0
[CI] Add TVM_INTEGRATION_I386_ONLY for Integration Test on i386 (#9388)* add script* address comment,1
[CUTLASS] Fix hardcoded include path and logic for profile_all = False case (#9399)* fixed hardcoded cutlass include path* fixed profile_all = False case* add cutlass cmake option* check if cutlass path exists* improve err msg when cutlass is not found,0
[CUDA] Support memory reuse for dynamic shared memory (#9341)* reuse shared dyn* minor* format* format* address comment and fix* address comment* address comment,0
"[CUTLASS, Eazy] Cache profiling result and support compiling generated kernels in parallel  (#9402)",5
"[docs][bug] Add redirects for moved pages (#9394)The documentation refactor moved many pages that have outstandinglinks from search engines and other sources. Because we don'thave direct access to the .htaccess file for the TVM docs webserver,this patch automatically creates manual http redirects froma list of tuples that provide the old page with the relativelink to the new page.",0
Fix CallNode Rust binding (#9381)Was being constructed as VarNode.,0
Support quantized NEG operator in TFLite frontend (#9404),5
"[BYOC] [ACL] Update ACL to 21.08 (#9396)This PR changes ACL version to v21.08*ACL stands for ""Compute Library for the Arm® Architecture""",1
"[ETHOSU] Add early simplify to fix LoopPartition (#9387)* [ETHOSU] Add early simplify to fix LoopPartitionCertain loops aren't correctly partitioned if the loopcondition hasn't been simplified. This can happen whena copy loop is split by a non-factor. To fix this, anadditional simplify pass is added to the TIR pipelineprior to LoopPartition.Change-Id: Icd4ff14648ccaed41384da50c6d183a122b30048* Fix linting againChange-Id: I9c9dc2ee2c679861866b23531e88584b94198e51",0
[microTVM] Add microTVM Template Projects to tlcpack pip Package (#9309)* zephyr lib fixed* restructure* readme* add arduino* add project template to setup.py* fix lint* fix tutorial* address comments from PR9274,0
"[IR] Minor cleanup to tvm.ir.instrument.PassInstrument (#9392)Allowed users to subclass from PassInstrument directly.  Themotivating example was adding a `@classmethod` to a class decoratedwith `@pass_instrument`, but the class method wasn't passed through.",1
[VMCompiler] Support shape func lowering for nested function call (#9405)* Support nested function call in shape func lowering* add test,1
"BUG: Make sure FoldConstant can inline constants underneath on_device annotations (#9367)After device planning and conversion to ANF we can end up with:  let %x = on_device(constant, device_type=D)  ...  @f(..., %x, ...)where the device D is not the same as the device for the let-expressionitself. (eg D may be the CPU, %x a shape, and @f an allocation primitivethat requires shapes to reside on the CPU). That's all consistent with the conventionthe DeviceAware* visitors expect for recovering device information.However, it means folding constant into @f's call site must both 'see' theconstant underneath the on_device annotation and bring the on_device annotationalong for the ride:  @f(..., on_device(constant, device_type=D), ...)- Make FoldConstant be on_device aware- Clean things up a bit while I'm there.- Setup unit tests specifically for const folding with on_device annotations.- Replacing if __name__ == ""main"" drivers for units tests with official  incantation as I encounter them.- Don't create on_device(on_device(...))- Logging changes so A/B diffs can focus on just the pass of interest.- Revert Index->DLDeviceType changes in the vm in case they are the cause  of downstream problems.",0
fix (#9412),0
[TVMC] Keep quantized weights when importing PyTorch model (#9417)BYOC requires `keep_quantized_weight` be set to true when convertingPyTorch models using `from_torch`. Setting this to be True when usingTVMC.Change-Id: I8c183f9f802ea54d24679a4017e56481d84e5655,4
"[BYOC][NPU] Fix integration tests not running (#9415)As we were using `python` rather than `python3`, when ran in the docker containers it resulted in:```Exception: The minimal Python requirement is Python 3.6```This masked a further error of the wrong NPU configuration being usedfor the tests.",0
[BYOC][ACL] Update installation docs (#9426)Also updated CMake to proper casing,1
"BUG: alloc_tensor offset and reshape shape should be on the CPU (#9421)* BUG: alloc_tensor offset and reshape shape should be on the CPUThe VM ManifestAlloc pass was allocating constants in a few places Iforgot to tag with on_device for the host/cpu. As a result the runtimewould (silently) do the x-device copy, which destroys perf.To make this easier to spot in the future added a 'constants' propertyto the VM Executable to dump the shape & device for all VM constants.This is CORE-102 in OctoML JIRA.* [checkpoint] Older compilers can't handle << overload* [checkpoint] Woops, forgot requires_cuda",0
Bump the CMake version in ubuntu_install_cmake_source.sh to 3.14.7. (#9424)* This is required in platforms we need to build xgboost from source * Fixes #9414,0
Support quantized ABS operator in TFLite frontend (#9411),5
[Frontend][PaddlePaddle] Support more common operators (#9428)* update ci-gpu to v0.78* add some common operators* code format* add transpose and swish* add unitest,1
Fix GetQmin and GetQmax from relay.qnn (#9427),0
[DOC] Add tip on mitigation for symbol conflict with PyTorch (#9433),1
change id (#9431),4
add reviewer (#9430),1
"[CUTLASS] Initial support for dynamic shape dense (#9419)* dynamic dense branch importcommit ee790ad64dfaec9d28f27726074a53a1daf3cf3eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 21:32:32 2021 +0900    dynamic dense working on ref build and execcommit 2449b667727e247c5b04e5fce12fa9d284444c38Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 21:27:46 2021 +0900    add vm build APIcommit 57036806b0533e5b039cc17f8da81983fba3ef9cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:57:33 2021 +0900    fix testcommit 4a7a5033b1f8c34b7f08b0a2882d6fa8bb5f44abAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:55:33 2021 +0900    fixed profile_all = False casecommit fb915195c122e9036ea3283e7fc286fa19222709Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 19:34:15 2021 +0900    add export_lib compile kwargscommit 3ad61e5379b486a0af010143ae8c7065b1c649d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:41:45 2021 +0900    fixed hardcoded cutlass include pathcommit 1def1d34ad042f9859307fdbd1f732186115d556Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:36:19 2021 +0900    add vm build and execcommit 570f9dd2cdc9b366348043ecc9c52a593ae4e273Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Oct 29 18:15:01 2021 +0900    adding test* start new jit impl for updated signature* generate signature that takes DLTensor as input* choose default kernel, hit shape func problem* dynamic dense ran but the result is a bit off* the result agrees with cublas* support dynamic N* run black* clang format* add doc for build_cutlass_kernels_vm* add doc to gen_gemm.py* add sm75 dense kernels* black* replace print with logging, add sm check for DEFAULT_KERNELS",0
[ONNX][Converter] Add dynamic nodes support (#9380)* [ONNX][Converter] Add support for dynamic nodes* fix lint,0
[TIR] Fix VerifyGPUCode for vectorized halfx8 store (#9420),0
[Keras] Add l2_normalize support (#9383),1
"Adds SEScope (Storage/Execution Scope) for use as new unit of planning in 'device' planning. (#9313)[Target] Adds SEScope (Storage/Execution Scope) for use as new unit of planning in 'device' planningThis is the first step in https://github.com/apache/tvm-rfcs/pull/38 to bring devicesand targets together when doing device planning. I've gone ahead and also included amemory scope in this object since we will also need to propagate memory scopes acrossRelay expressions once this basic preparation is in place. In the meantime that field will beleft as """".Once device planning works in units of SEScopes it will be possible to directly read offthe device and target for any Relay sub-expression without the need for TargetMaps ortthe construction of default Targets.SEScopes also support 'Join' and 'Default' operations needed when constraint solving inthe device planner. You can see those in use in my scratchpad branch:  https://github.com/mbs-octoml/mbs-tvm/tree/mbs-scopesThis PR also brings some duplicated and the ad-hoc 'default target' handling logictogether into a CompilationConfig class. (Again, see the scratchpad branch for how thatwill end up being used). I've placed that next to SEScope since it's main purpose is to  a) establish the default SEScope for primitive ops  b) establish the SEScope for the 'host'  c) feed a definitive vector of Targets into device planning so it can resolve all     ""on_device"" and ""device_copy"" device references to their full SEScope form.* Reworked to avoid global SEScopeCache.Realized while working through unit tests in the sequel that it's reasonablefor folks to call build multiple times with distinct Target objects, in whichcase the global cache would grow without bound.So instead placed the cache in the CompilationConfig class. Since that classnow has everything the device planner needs to do its job, promoted it tobe an FFI-able Object, which is now in compilation_config.{h,cc}.I think we can do much better with CompilationConfig, but for now keeping itto the minimum I needed to prepare for device planning from all the executorcompilation codepaths.",1
[TVMScript] Use // and % for FloorDiv/FloorMod (#9437),5
Removed a manual file handler pitfall (#9435),2
Arm(R) Ethos(TM)-U NPU Pooling operators support (#9384)This commit adds support for the max and average pooling primitive operators for the Arm(R) Ethos(TM)-U NPU and includes a few minor rewording changes.,1
[CUTLASS] Support batch_matmul (#9439)* Import batched gemm changecommit cfacfa296e2487a189e52d189567b140c675ccc2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Nov 1 15:57:49 2021 +0900    change is_constant pattern to wildcard in gelu patterncommit 84da94306ca81209a8ccc44fd7d606cbce047082Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Nov 1 05:41:11 2021 +0900    fixed batch stride Ccommit 66e5779ee69dc0cd3969f268608b551ec549d79bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 20:47:16 2021 +0900    refactoring codegencommit 561daeafa66cddf6a565b537072a5efce0b0dbf1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 20:05:20 2021 +0900    generated kernel compiled and result matchcommit a5740bcf5287097b64dff8adb50f0cddc2c41349Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 19:36:53 2021 +0900    partitioning looks goodcommit 59112fdf78a4541905fad9b899737600e0ed9391Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Oct 31 19:01:47 2021 +0900    [WIP] cutlass batch matmul support* fixed test* refactoring* gelu test fixed* more refactor* batch_matmul fp32 accum working* dynamic batch matmul working* black* remove doc TODO,0
Fix several typos in pytest_target_parameterization.rst (#9447),0
[MetaSchedule] Task Extraction (#9382)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[MetaSchedule] Sample-Perfect-Tile (#9449)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[CI] Pin setuptools to v58.4.0 in CI to circumvent breaking change in v58.5 (#9446)* Pin setuptools to v58.4.0 in CI to circumvent breaking change in v58.5* Remove setuptools installation from ubuntu_install_vela.sh,1
Update TVM ci-cpu docker image to v.079 (#9454)* Requested in #9296 * Validated at   https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/169/pipeline/,1
[Bug][Meta Schedule] Fix Infinite Loop Caused When Calling Methods Not Overrided In PyClass. (#9451)* Fix Infinite Loop Caused When Calling Methods Not Overrided In PyClass.* Add new line.* Lint.,0
[PyTorch]Add PyTorchTVM: compile torchscript to tvm and export as pytorch_op (#8777)* add pt_op* add compile api* perf: support set_output_zero_copy* fix: cpu device_id mismatch* fix: pt_class test script* refactor: unify namespace to tvm.contrib.torch* add ASF header* build: set pt tvmdsoop default off* build: remove unset_log_macros.h* refactor: change header order* refactor: fix python code format* style: resolve pylint issues* style: add blank line* style: fix pylint invalid_name* trigger CI* test: add more test scripts* style: add empty lines* test: update test for trace tvm module* style: fix linting issues* style: remove single quote* style: disable pylint invalid-name* trigger CI* trigger CICo-authored-by: kongroo <imjcqt@gmail.com>,0
[TIR][Schedule] Add get-child-blocks primitive (#9434)* get child blocks* fix* lint* fix,0
Fix typo. (#9462),0
[TensorIR] Print TVMScript with prefix T instead of tir (#9422),0
"[TensorIR] GetProducer, GetConsumer (#506) (#9464)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>",2
Fix an infinite recompilation loop in tvm-sys. (#9450)Including the CMake `build` directory in `reun-if-changed` was leadingto `tvm-sys` recompiling on every invocation of `cargo build` (alongwith every crate which depended on it).,0
[TVMC] Re-enable PyTorch test (#9441)This test was originally disabled due to the issue documented in #7455affecting CI. I believe this has since been resolved by #9362.Note: This patch should not be merged until the changes inhttps: //github.com/tlc-pack/tlcpack/pull/81 are reflected in CI.Change-Id: Ib918595a1d9149e3c858ca761861304450cbfe13,3
Support quantised SQRT operator in TFLite (#9258),5
[Frontend][PaddlePaddle] Add operators of interploate/flatten and modify try_infer_value (#9459)* add interploate and flatten* fix spells* fix diff* rename unit test name* add parameters for common:try_infer_value* eliminate unnecessary diff* fix pylint problem* fix pylint problem* eliminate unnecessary diff,0
[TIR] Make compact buffer and get access region aware of conditions (#9372)* Support condition bound awareness in compact buffer and get block access region* remove intset difference usage* fix to visit match buffer's access region* change method to distinguish annotated opaque access regions,0
add is_entry_func tag for device function (#9436),1
Add back-to-back conv2d Hexagon test for stripe scheduling (#9390)* Add back-to-back conv2d Hexagon test for stripe scheduling* top level README with test specific links* test specific readmes* add ASF to README* add k_split and h_split case* add back deleted line from README* add 3x3 conv2d -> conv2d case* address code review feedback* skip conv2d -> conv2d test due to flakiness on i386,1
[Relay] Non-recursive Dtor for Let (#9461)* non-recursive let desctruction* fix serialization* Fix serialization* lint* lint* lint* add tests for serial/deserial* lintCo-authored-by: Haozheng Fan <hzfan@apache.com>,0
"[Frontend][TFlite] Cast MirrorPad paddings to int32 (#9468)* [Frontend][TFlite] Cast MirrorPad padding to int32 As an int64 paddings of MirrorPad would generate wrong TIR as result, try to cast to int32 for best compatibility.* [Frontend][TFlite] Add tests for MirrorPad with int64 paddings* Update test_forward.py",1
Fix function annotation (#9474),0
"[CMSIS-NN] Convert CMSIS-NN to use Target Hooks (#9397)* [CMSIS-NN] Convert CMSIS-NN to use Target HooksThis migrates CMSIS-NN to use Target Hooks instead of fully BYOC, whichmeans it will now go through any central passes the Driver API.* Mutated PrimFunc arguments in LowerTE so all functions are correctly lowered* Made Target `cmsis-nn` to match external code generator `cmsis-nn` to connect the Target with the external code generator* Modified Partition Graph to sanitise compiler names to generate them properly in C* Port tvmc fixes for hybrid targets* Update NPU tests with new sanitisation",0
"[TVMC] Add test for quantized pytorch model (#9467)As a follow up to #9417 and now that #9362 is resolved, this PR adds atest to check quantized pytorch mobilenetv2 is converted correctly.Change-Id: Iaf2d38ce71c008e0141a4a2536bd54c2c9f3fe3d",1
"[TIR] Add type hint for TIR  (#9432)* add init* get rid of span* afs header* update scope_handler* rm tir/__init__.pyi* fix linting* fix lint* new test case* add axis module* address comments* redefine ty types* lint* address comments* address comments* fix ci* add test cases* fix CI* address comments* add types* mypy --strict* comments* update test comments* linting fix* address comments* add pylint for tir type check* address comments* move doc string* comments* getter setter* add PrimExpr, IterVar and Var* add sequence* change for handle",0
[AOT][Tests] Use pre-built libraries in Reference System tests (#9271)This is a follow up to remove the rebuild of dependencies in each testrun from the AOT test utils and favouring those prebuilt into thecontainer.,3
"Better host handling in CompilationConfig & debug printing (#9460)(This is a bit of a grab bag in preparation for #9326which I'm trying to minimize)While switching the device planner to use SEScopes I had a lotof trouble with Target's not matching up.- If no explicit host target is given but the given  TargetMap has targets with hosts, try to use those  to establish the host_target.- Make sure both the 'legacy' TargetMap representation  and the newer representation agree to pointer equality on  their targets.- Make sure the Interpreter uses the target from CompilationConfig  since it's been normalized.To debug the above:- When in pretty printing with show_meta_data_ false give as much  detail on SEScopes, Targets and call attributes as possible.  That needed some rework in the relay_text_printer.cc.- Ditto for critical 'target' attribute on PrimFuncs.- Also added a Target::ToDebugString so I could see the  host fields along with everything else since a lot of problems  were caused by a mismatch of 'the same' Target with and without  a host. (Tried using that for the ReprPrinter but broken unit  tests.)Note that the codebase assumes Targets are compared by ObjectPtrEquality,yet CheckAndUpdateHostConsistency (I count 65 call sites) changes the targets.Ultimately CompilationConfig or it's ultimate replacement should ensure we mungetargets only once at the 'main' entry points.",0
[microNPU] Replace ICHECK with diagnostic context in type inference (#9470)[microNPU] Replace ICHECK with diagnostic context in type inferenceConvolution and depthwise convolution use the ICHECK format oferror checking during type inference. This PR updates these checks touse the diagnostic context.,0
"[MicroTVM][PyTest] Explicitly skip MicroTVM unittests. (#9335)* [MicroTVM][PyTest] Explicitly skip MicroTVM unittests.Refactor unit tests so they will show as skipped if `USE_MICRO=OFF`.* Updates following PR review.- Updated to avoid name shadowing of BaseTestHandler- Updated test_micro_transport to use fixture for setup.  Ended up  needing to refactor to use pytest instead of unittest, split up test  functionality during refactor.",0
"[CMSIS-NN] Assert correct amount of CMSIS-NN artifacts in MLF (#9480)Unsure why this wasn't picked up by the other PR, assuming some race conditions - there's now 4 artifacts in the archive:* Interface in lib0* CMSIS-NN in lib1* Linked params in lib2* Host code and executor in lib3",5
"[ETHOSN] Streamline Ethos(TM)-N cross-compile rpc usage (#9477)* When cross-compiling the runtime or rpc application, LLVM is not  required so don't insist on it being enabled for Ethos-N.",2
"Change Call with TIRCallAttrs to call_lowered op (#9312)* Introduce call_lowered opAdd op vm.call_tirChange from checking if CallNode has CallTIRAttrs to checking if the Op is vm.call_tirChange device_domains to use vm.call_tir op more explicitlyFixed issue in type checker, now have seg fault :(Fix typo -- most of VM tests pass nowInterpreter now deals with call_tir properlyFix typo in te_compilerUse InvokeTVMOp and CallTIRAdd some checks to graph_plan_memory.ccMake GetToken skip function typesC++ TESTS PASS WOOHOORemove printsformattingvm.call_tir -> call_tir and more comment removalscall_tir -> call_loweredfix lintclang formatRemove compute from non computational vm opsmissed some semicolons in prev commitFix warningMove call_lowered to relay/op/call/call.cc and rename util funcAdd helper fn that returns lowered_call opfix import orderclang formatAdd constraint to call_lowered type relclean up empty token vectorcommentMove CallTIRAttrs to include/tvm/relay/attrs/call.hRename TIRCallAttrs as CallLoweredAttrslintAdd helper for extracting func and args from call_loweredChange graph_executor_codegen to use helper functionUpdate interpreter to use helperFix device_domains.cc -- could still use cleanup, also I am not sure why there are still direct calls to primfns in DomainforCalleeClean up DeviceCopyProps and lintlintreturn CallLoweredAttrs with the extern funccommentnote in commentProgress & notes. Realized that I am not handling externs correctlynot sure why this ever worked before?Clean up CreateFuncCall signature, notescommentsFix extern function handlingextern_function -> extern_funcfix DeviceAwareVisitExpr_ -- now it handles both lowered and normal callsyay passes AOT tests!formatting and comment removalcleanupIntroduce call_lowered op* lint* Fix AOT to deal with externs* add const auto&* Fix aot crt test",0
[Support] Add libinfo into the runtime build (#9310)* Move libinfo into the runtime build* put libinfo back into libtvm* limit microtvm imports when we only have the runtime lib* fix lint* try conditional for micro import,0
"Fixed some warnings about lambda's closures that are bigger than necessary (#9481)* Fixed some warnings, including lambdas closure that are bigger than necessary, and some missing parenthesis around an assignment in a conditional (LLVM fears a mistake where one wanted to use the equality test ==)* Fixed formatting as two lines can now be merged while still being under 100 characters",0
"[HOTFIX][TARGET] Change LOG in compilation config to DLOG (#9486)CompilationConfig was merged in on the basis that it is an internal experimentalstructure that helps to group the target. Constructing the config should notemit messages for most cases. Change LOG(INFO) to DLOG(INFO)so users won't be overwhelmed by messages.There are a few warning cases that also changes to DLOG. Given CompilationConfigis still experimental, it would be better to respect the current default conventionand not trigger warnings that indicate an non-experimental suggestion.Warnings can be updated according with the convention.",0
Add default for split op (#9489)* split fix* add default split test case,0
[COMMUNITY] Junru's and Wuwei's PGP key for ASF release (#9488)* add PGP into KEYS* add wuwei's pgp as well,1
Arm(R) Ethos(TM)-U NPU BinaryElementwise operators support (#9442)This commit adds support for the binary elementwise primitive operators for the Arm(R) Ethos(TM)-U NPU and includes a few minor rewording changes.,1
[Relay] Use target_host determined at Relay level instead of recalculating it (#9499),2
Add LLVM-13 installation to Docker setup (#9498)* Installs LLVM 13 in for Ubuntu 18.04 Docker images * This is needed as a requirement for #9425,1
Fix repository URL in ubuntu_install_rocm.sh (#9425)* Fix repository URL in ubuntu_install_rocm.sh* ROCm dependency installation process was following outdated procedures.  This PR makes the installation script to point to the correct repository* Installation documentation is at:  https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html* Fixes #9413* Update docker/install/ubuntu_install_rocm.shCo-authored-by: Christopher Sidebottom <git@damouse.co.uk>Co-authored-by: Christopher Sidebottom <git@damouse.co.uk>,0
[Topi][Op][PyTorch][Vitas] Fix inconsistent kernel layout conventions for conv2d_transpose (#9336)* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* jostle* fix keras* fix another frontend test* fix things* jostle ci,0
[TARGET] Move target_host usage to new target style. (#9497)- Add deprecation warnings to functions with target_host parameters.- Update the build usage to new target style.,1
[BugFix] Fix divide by zero error in TIR pass lower_warp_memory (#9485)* fix factor divide by zero* add a test case in test_tir_transform_lower_warp_memory.py* fix type* make it more elegant,0
"[Conv2DTransposed] Fix wrong shape check and add new TOPI module to support groups (#9465)* f wrong type check in conv2d_transpose* add test case for conv2d transpose* add groups support for conv2d_transpose* add  naive implementation and schedule for conv2d with groups* enable tests for cpu and arm_cpu, raise error for cuda platform* revert the cuda and generic strategy* revert back the x86 strategy* revert back the arm_cpu strategy* revert back the arm_cpu strategy* revert back the arm_cpu strategy* fix EOF of x86* clang lint updated c++ code* update topi implementation* Revert test* Revert test* add generic/x86/arm specialization for conv2d_transpose with groups > 1* remove commentted codes* fix lint* fix lint* fix c++ lint* fix lint* fix python lint* remove comments and reformat* lint file* lint code* fix lint* update logging information in convolution.hCo-authored-by: Alicja Kwasniewska <alicja.kwasniewska@sima.ai>",0
"Switch PlanDevices pass to be w.r.t. SEScopes instead of DLDeviceTypes. (#9326)* Switch PlanDevices pass to be w.r.t. SEScopes instead of DLDeviceTypes.CAUTION: Breaking VM executable serialization change. I needed a new 'virtual devices' array in the executable so that instructions can continue to refer to devices by a simple index yet the VM can respect both the device type and id for runtime devices.Continuing from #9313, and as part of apache/tvm-rfcs#38, we switch PlanDevices to plan with respect to SEScopes instead of just DLDeviceTypes. Our ultimate goal is to be able to flow memory scopes between PrimFuncs by re-running PlanDevices after the LowerTE pass. This PR at least gets us to being able to flow the memory scopes, but the actual changes to PlanDevices to look inside PrimFuncs is still two PR's in the future.However, we get two nice side effects right away: - Since SEScopes contain Targets we can isolate all the device-to-target resolution machinery within PlanDevices (with the help of CompilationConfig). After PlanDevices has run we can retrieve the Target for any sub-expression directly from that sub-expression's SEScope. For now we retain the one-Target-per-DLDeviceType constraint since it baked into the public 'TargetMap' API, but the path to breaking that constraint is clearer. - Device ids are now respected all the way from annotation to executor. Previously though we had a bit of plumbing using Devices the device_id therein was ignored or defaulted to zero. The Python ""on_device"" annotation helpers still work w.r.t. devices. Thus though they now respect device ids, they do not allow the user to specify a Target or memory scope as supported by the underlying SEScope.* [checkpoint] Revert emitter.py, must have run 'black .' by mistake.* [checkpoint] Address PR commentsAlso add back SplitArgs pass in build_module.cc which somehow got lost in the shuffle.(try again -- flaky test_crt.py test_autotune?)* [checkpoint] Fix after rebase on CallLowered.",0
"[1/3][AOT][DeviceAPI] Connecting devices structure to relevant operators (#9395)* [AOT][DeviceAPI] Connecting devices structure to relevant operatorsThis patch adds support for passing the device context via the unpacked API in AOT, generating an additional struct if necessary:```c/*! * \brief Device context pointers for TVM module ""default"" */struct tvmgen_default_devices {  void* npu;};```Which is then added as an argument to the entry function:```c/*! * \brief entrypoint function for TVM module ""default"" * \param inputs Input tensors for the module * \param outputs Output tensors for the module * \param devices Device context pointers for the module */int32_t tvmgen_default_run(  struct tvmgen_default_inputs* inputs,  struct tvmgen_default_outputs* outputs,  struct tvmgen_default_devices* devices);```I've temporarily added the collection of external code generators to the TE compiler pending proper annotation of the eventual functions.Co-authored-by: Grant Watson <grant.watson@arm.com>* Correct ""use_device_api"" attribute name on TargetCo-authored-by: Grant Watson <grant.watson@arm.com>",1
"Make version.py to rely on repository metadata to generate version string. (#9472)This change removes the need for a hardcoded version string whengenerating a TVM release from a git repository. The long term goalhere is to be able to have reproducible release versions withoutmanual intervention on files such as version.py.It also removes assumptions of what the last valid tag should bein order for the version string to be generated. This informationshould be encoded in the repository, as we are using git tagsto retrieve the information.",2
[ONNX] Unique op should always return int64 indices (#9490),5
Followup from #9312 (Introduce call_lowered op) (#9491)* followups from #9312* remove unneeded moves,4
added tests for quantized tflite sin operator (#9478)* added tests for quantized tflite sin operator* removing unnecessary rsqrt code_block* resolving linting error,0
[TensorIR] Cross-Thread Reduction (#9360)* [TensorIR] Cross-Thread Reduction* Code revision on analysis and misc* Refactor TransformReductionBlock* Refactor code organization* Address comment* Use `std::make_tuple`Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[BugFix][TVMScript] Fix printer for dependent loops (#9506),0
[Topi] Cortex-M DSP support (#9233)Co-authored-by: Sergey Smirnov <Sergey.Smirnov@mir.dev>Co-authored-by: Ekaterina Bern <Ekaterina.Bern@mir.dev>Co-authored-by: Mikhail Trubnikov <Mikhail.Trubnikov@mir.dev>Co-authored-by: German Tretiakov <german.tretiakov@mir.dev>Co-authored-by: Ilya Gozman <Ilya.Gozman@mir.dev>Co-authored-by: Alexey.Yazev <Alexey.Yazev@mir.dev>Co-authored-by: Ilya Gozman <92577591+ilyag-grovety@users.noreply.github.com>,5
[TVMScript] Report error if add attr to implicit root block (#9507)* fix implict root block attrs* lint,0
fix compute inline not to over write annotated opaque accesses (#9509),0
"[ONNX][Relay] Support ""tf_crop_and_resize"" in relay Resize op. (#9475)* add fallback to opset 11* Support tf_crop_and_resize in resize op* change api use in the rest of the codebasereally fix the tests* respond to review comments, improve doc strings* fix docstring indentation* remove N anc C from resize roi",0
[2/3][AOT][DeviceAPI] Add Hooks for Activate/Deactivate/Open/Close (#9500)* [AOT][DeviceAPI] Add Hooks for Activate/Deactivate/Open/CloseThis adds the relevant hooks into their starting places in the codegeneration. As per the [C Device APIRFC](https://github.com/apache/tvm-rfcs/blob/main/rfcs/0031-devices-api.md)* Standardise on `lowered_ir_mods` and correct device_hook variable name,1
Add the Arm(R) Ethos(TM)-U NPU identity operator (#9457)* Add the Arm(R) Ethos(TM)-U NPU identity operator* Add the ethosu.identity operator which returns the input tensor* Add an opportunity to requantize the tensor* Add legalization for reshape and strided slice* Add a pass that puts an indentity op after a no-opChange-Id: I0adb5ca269f8529c79e0e7681ca4b5147d8f53c8* Fix the pylint errorsChange-Id: Icc9b6507f164681a5d6b1fcff2ae4a5051d44734* Changes in response to review commentsChange-Id: I63f30f84ad481789fc047ad8c2107f5313562f7f,0
"[ETHOSN] Cleanup of trademarks and registered trademarks (#9516)Simple cleanup of comments and documentation, mainly for Arm(R) Ethos(TM)-N NPU related code.",5
"[microNPU] Allow constants to be given as input to an operator (#9515)* [microNPU] Allow constants to be given as input to an operatorCurrently the expectation is that all constants need to be encoded,however, this is not always the case for scalar inputs. This PRensures that constants that don't need encoding are not treatedlike encoded constants by the EncodeConstants pass.Change-Id: I79cf4aa10d01c4ae9ce9cdafb6f21ebb2d028126* address commentsChange-Id: I67b61a2d2f67de25c47d2ace0e3a22c59ba8ea15",1
[4/10] Code generation for Conv2D via CMSIS-NN (#9331)This PR is for support of Conv2D via CMSIS-NN.,5
"[microNPU] Adding rounding mode attribute to operators (#9514)* [microNPU] Adding rounding mode attribute to operatorsAllows rounding mode to be specified for each supported operator.By default ""TFL"" is used, which matches that of the behavior of TFLite.Other rounding mode options include ""NATURAL"" which rounds to thenearest value and ""TRUNCATE"" which rounds towards zero.",1
Add a 'rolling_buffer' scheduling primitive (#9444)* Add a 'rolling_buffer' scheduling primitiveCo-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>* Fix lint problemsChange-Id: I5e27a66105fccca84327e41b4c68836ac2515126* Remove designated initializersChange-Id: Ic148264239eac7df7d976a6a3e15236935232792Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>,0
[ONNX] Normalize axes for Slice (#9517),5
"[PROFILER,VM] Fix timer device type for reshape_tensor (#9518)* [PROFILER,VM] Fix timer device type for reshape_tensorThe index of the host device was changed in the VM's `devices_` array,but we forgot to update this index in the profiler.* formatting",0
"cleanup Hexagon conv2d tests (#9473)* cleanup Hexagon conv2d tests* add back fixtures; skip tests only on i386* fix pylint error* fix maxpool failures* fix `skipif` statement and verify error + code review feedback* fix typo ""physical_physical""* retrigger ci* determining i386 processor string from CI* hopefully the correct test disable",0
Add Chris (#9532),1
[AlterLayout] Respect input layout for dense op if explicitly specified (#9535),5
"[TVMC][microTVM] Add new micro context (#9229)* [microTVM] zephyr: Make platform options comply with RFC-0020Make Zephyr platform options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] arduino: Make platform options comply with RFC-0020Make Arduino platform options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] crt: Make crt options comply with RFC-0020Make crt project options comply with RFC-0020 specification.Project options now need to specify the required metadata for everyoption, i.e. 'required', 'optional', and 'type'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM][Unittest] Adapt test to RFC-0020Adapt test to new metadata fields accordingly to RFC-0020 specification.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] Add info() method to GeneratedProject classAdd info() method to GeneratedProject class so one can use the ProjectAPI to query options for project dirs instead of only for templateprojects.This commit also adds for the sake of convenience a setter and a getterfor 'options' in case it's necessary to set or get 'options' after aGeneratedProject class is instantiated without initializing 'options'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [microTVM] Fix typo in python/tvm/micro/session.pyFix typo in comment.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* Allow multiple runs on micro targetsCurrently there is a limitation on microTVM / TVM which doesn't allowrunning a model multiple times in sequence without previously flashingthe model to the device.Root cause is that RPCModuleNode class destructor is called once a runfinishes. The destructor sends a RPCCode::kFreeHandle packet withtype_code = kTVMModuleHandle to the device which wipes entries incrt/src/runtime/crt/common/crt_runtime_api.c:147:static const TVMModule*registered_modules[TVM_CRT_MAX_REGISTERED_MODULES] when TVMFreeMod() iscalled when the target receives a kFreeHandle packet.Hence when one tries to re-run a model registered_modules[0] == NULLcauses a backtrace on the host side. Probably never before a model onmicroTVM was run without being flashed just before the run, so tvmc runimplementation for micro targets exposed the issue.This commit fixes it by not calling TVMFreeMod() for system_lib_handleon the target side when a session terminates so the pointer to thesystem_lib_handle is not flushed from 'registered_modules', allowingmultiple runs on micro targets.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] Pass main parser when calling add_*_parser functionsCurrently when a add_*_parser functions are called in main.py to buildand add the various subparsers to the main parser only a subparser ispassed to the functions. However if one of these functions need to builda dynamic parser it needs also to call the main parser at least once toparse once the command line and get the arguments necessary to finallybuild the complete parser.This commit fixes that limitation by passing also the main parser whencalling the subparser builders so it can be used to build the dynamicsubparses.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] micro: Add new micro contextThis commit introduces support for micro targets (targets supported bymicroTVM). It creates a new micro context under the new TVMC command'tvmc micro'. Moreover, three new subcommands are made available in thenew context under 'tvmc micro': 'create-project', 'build', and 'flash'.The new support relies on the Project API to query all the optionsavailable for a selected platform (like Zephyr and Arduino) and alsofrom any adhoc platform template directory which provides a customProject API server.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] run: Add support for micro devicesAdd support for micro devices using the Project API to query all optionsavailable for a given platform and open a session with an specifiedmicro device. Use of 'tvmc run' with micro device is enabled via the'--device micro' option in addition to the project directory.Once the project directory is specified 'tvmc run' will make all optionsspecific to the platform found in the project dir available as optionsin 'tvmc run'. They can be listed by '--list-options' and passed via'--options'.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[TEST] Fix duplicate definition error for gpu export mod testcase (#9538),0
[Docker][Onnx] Upgrade ONNX to latest version (#9519)* initial commit* jostle ci,3
"[TVMC][Relay] Introduce executor and runtime parameters (#9352)* [TVMC][Relay] Introduce executor and runtime parametersThis introduces `executor` and `runtime` into the various entrypoints but also into `tvmc` as `--executor` and `--runtime`. This touchs a lot of files and I've tried to update anywhere as necessary.Notable, executor code generators now accept the initial `IRModule` rather than creatingit themselves so it can be annotated once.Validated the demo application continues to classify the tabby cat withnew CLI options.* Correct Graph Executor Python API",1
[Relay][Frontend] Prune redundant logging (#9545)* [Relay][Frontend] Prune redundant logging* lint,2
remove compile warning complained by macos clang-13.0 (#9522),4
Expose workspace size in tvmgen_default.h (#9510)This PR exposes the workspace size as a macro TVMGEN_DEFAULT_WORKSPACE_SIZE in tvmgen_default.h (or TVMGEN_<MODEL_NAME>_WORKSPACE_SIZE in tvmgen_<model_name>.h in the case that the model name is not default).This functionality is useful for microTVM/AOT use cases where it's useful to know the workspace size at compile time.,5
"[CI.Lint.Black] Use ""en_US.UTF-8"" for Red Hat 6&7 Compatibility (#9537)",5
[microNPU] Add unary elementwise operator infrastructure with ABS (#9530)* [microNPU] Add unary elementwise operator infrastructure with ABS* Added unary elementwise ABS legalization support and tests* Added unary_elementwise Relay to TIR lowering and tests* Added TIR to Vela translation and tests* Added codegen testsCo-authored-by: Rishabh Jain <rishabh.jain2@arm.com>,1
[TVMScript][Fix] Add type hints for more uncovered cases (#9505)* add support for prevously uncovered cases* remove PrimExpr import* add exp test and mypy ignore* disable ling too long* resolve long line* nit* add dtype to unary ops,0
"Revert ""[CI.Lint.Black] Use ""en_US.UTF-8"" for Red Hat 6&7 Compatibility (#9537)"" (#9548)This reverts commit ecd8a9ce33991262f4184cb857f1088d9d8e1bb1.",5
[3/3][AOT][DeviceAPI] Wire up cpacked Device API context (#9501)* [AOT][DeviceAPI] Wire up cpacked Device API contextAdding the same functionality for the Device API to the cpacked calling convention. The MakePackedAPI pass now implicitly uses any variable named `kDeviceContextVar` as the `resource_handle` and this is then used in the `cpacked` calling convention which always expects some form of resource_handle to be passed.* Document calling conventions* Remove superfluous variable,1
[Frontend][ONNX] Support RandomNormal operator (#9493),5
"[Relay] Prepare DeadCodeElimination for running post LowerTEPass/ManifestAlloc. (#9542)* Prepare DeadCodeElimination for running post LowerTEPass/ManifestAlloc.As part of #9483 we need to prepare some critical Relay passes for running afterlowering and conversion to DPS. For DCE we need to make sure we never removeside-effecting let-bound expressions, such as for allocation or evaluation ofan external function with unknown effectfulness.Introduce a new purity pre-pass. It makes a half-hearted attempt at accountingfor functions by tracking both 'eval' and 'call' purity, but must fallback toassuming call-impurity in more difficult cases (eg calling a function passed asa parameter, calling a function projected from a tuple, etc). However it seemsplenty good enough.Purity must also be accounted for when determining the usage count of let-boundvariables, so reworked that. Collapsed the let-bound value accumulation pass intothe usage counting pass to make up for inserting the new purity analysis pass.A few tests assume DCE eliminates dead reference writes. The previousimplementation certainly did that, but by eliminating *all* writes.Filed CORE-118 to extend DCE to soundly elim dead writes (a very simple-mindedanalysis would probably do just fine and we don't need to get hung up on aliasanalysis). In the meantime, added an 'ignore_impurity' flag (default False)and set to true just in the few unit tests which rely on the unsound impl.* [checkpoint] Merge Lily's suggestions.",1
"[TIR][USMP] Added buffer info extraction pass (#8468)* [TIR][USMP] Added buffer info extraction passThis commit adds a pass that takes the main (call graph of operators)TIR PrimFunc and each operators also as TIR PrimFunc. The pass willtraverse through all TIR PrimFunc starting the from main. Thereafter,it will extract information from tir.allocates. Among the information,the liveness conflicts are reported.* Added test for a linear model* Added test for parallel/serial mixed for loops* Added test for a substructure of inception-style model.* Exposed buffer_info creation to python* Added member functions to update pool info* Unit tests to cover functionality of buffer_infoChange-Id: I5e163ac3e83c830629a5d34ed4407c9962701c60* [TIR][USMP] Added buffer info extraction passSwap key-value pairs of returned values of the buffer_infoextraction pass.Change-Id: Ia4f7289592bc776ef6189a41a7891038751bf31f* [TIR][USMP] Added buffer info extraction passUpdating the USMP utility tests to include teststhat test creation of PoolInfo and PoolAllocationObjects.Change-Id: I5d349d0ffcac6b0160072d832dd9d5418699228e* [TIR][USMP] Added buffer info extraction pass* Removing the unnecessary header : include/tvm/tir/usmp/analysis.h* Some nits and cleanupChange-Id: Iac3ddd9428c56cd8ef49cf643e797bf6fdf4e97a* [TIR][USMP] Added buffer info extraction pass* Change the class data members to have a trailing underscoreChange-Id: I71809b3c73b0bc0cd133fad1392ae8c17c895ee4* [TIR][USMP] Added buffer info extraction passAdding more documentation for data structuresand the approachChange-Id: Ide2bfffaeff9add86853b6992017264e5d796299* [TIR][USMP] Added buffer info extraction pass* Added more documentation* Added functionality to handle multiple calls  for the same PrimFunc with a test.Change-Id: Ib7c27b3cf17f415067a224f1e57d8b928f4c7c6f* [TIR][USMP] Added buffer info extraction pass* Attaching targets to PrimFuncs in the util test caseChange-Id: I82960512659a346f6242b2b5789ec1120f8ea2cf",1
"Fix Jenkins skip for CPU/GPU (#9549)Each step should be checking the docs-only build and marking itself as skipped if necessary, these two were skipping the i386 step instead.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[TensorIR][UX] Type annotation-based runtime type checking (#9559),5
[QNN] Fix order of operations in qnn.quantize slightly to prevent undefined behavior (#9558)* switch order of quantizations* add back in round,0
[TensorIR][Schedule] Inherit block anotation upon creating new blocks (#9573),1
[BUG][TVMScript] fix block range error (#9574),0
Improve the keras frontend to support tflite 2.6 (#9562)This code change keeps compatibility with tflite 2.4,4
[ONNX] Add MatMulInteger16 contrib op (#9186)* [ONNX] Add MatMulInteger16 contrib op* Fix formatting errors* Remove a code comment and do not set default value of nd* Move flatten_to_nd function outside matmul to be used across multiple functions* Add function docstring and describe the tests* Use max/min value of int16 as high/low while generating input vectors* Converge MatMul and MatMulInteger16 ops into a single op using output dtype* Fix indentation issues* Formatting changes* Fix CUDA batchmatmul strategy to allow mixed precision* Add test_matmulinteger to unsupported_onnx_tests,0
"Prepare for switching VM to LowerTEPass. (#9550)This is a grab bag of fallout changes from switching the VM to use LoweTEPasswhich can be easily split out of the main #9483 PR.- AnnotateSpans can be used from C++ (though, unfortunately, it didn't help  me with debugging since spans are universally dropped in most passes).- Can get a human readable dump of the VM's PackedFunc names and indexes for  debugging.- If TVM_LOG_DEBUG defined then include types and ids of GlobalVars. I had  a lot of difficulty tracking down where duplicate GlobalVars for the same  name_hint were getting created and propagated.- GetCallLoweredProps follows same API as GetDeviceCopy and GetOnDevice  where will return 'null' properties if call/expr is not of call_lowered  form. Mildly more convenient, though switching all the above to ICHECK  and push 'if (op == the relevant op)' into all use sites would also be just  fine.- Misc VLOG improvements made while tracking down issues in #9483.",0
[Target] enable -arch=sm_xx for assigning cuda target arch and deprecate autotvm.measure.set_cuda_target_arch api (#9544)* [Target] enable -arch=sm_xx for assigning cuda target arch and deprecate autotvm.measure.set_cuda_target_arch apiSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Format] fix format error in CISigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] add warnings to target.cuda and fix errors in ciSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] fix docstringSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [Target] amend warning conditionSigned-off-by: ZQPei <ziqiangpei@foxmail.com>,0
WithFields for Tuples (#9533),5
ignore 'training_mode' tag from onnx in batch_norm op (#9575)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>,5
"[FIX,PROFILING] Only check if ops duration is nonzero (#9568)We have seen intermittent failures in the profiling tests when some ofthe durations are not nonzero. This should fix it.",0
Update license file to note libbacktrace (#9579)Co-authored-by: Tianqi Chen <tqchen@users.noreply.github.com>,1
Update NEWS to include v0.8 change log (#9580)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[FIX][TIR] Remove unused code and fix typo in storage_align (#9583),0
Bump version to 0.9.dev0 (#9581),5
"[microNPU] Fix incorrectly calculated stride when converting NHWC to NHCWB16 (#9560)Fixes an issue that causes strides to be incorrectly calculated when thenumber of channels in the input is less than 16 and involves a conversionfrom NHWC to NHCWB16. This is due to TVM being 'too smart' when analyzinggenerated TE and removing compute that is deemed unnecessary. Consequently,strides over data are incorrectly calculated leading to an outputmismatch.The PR uses a reduce sum operation to trick TE's data dependencyanalyzer into looping over a whole block (16), rather than the numberof channels actually used (< 16). This causes the calculated strides tobe a multiple of 16 which is required for NHCWB16 format.Change-Id: Ibf76a94a12cebf51fa716fcac1de932a271c4a6d",0
[microNPU] Change weights and command stream section (#9523)Move microNPU weights and command stream to .rodata.tvm- Add unit test test_ethosu_section_name(),1
"[microNPU] Support binary elementwise with non-4D inputs (#9521)Reshapes non-4D inputs to become 4D, then reshapes the output back tothe non-4D input shape.",5
"[Relay] WithFields method for Call, Function, Var, TupleGetItem, If, Let, RefCreate, RefRead, RefWrite, Match, and Clause (#9569)* Implement WithFields for Relay exprs* lint",5
Add labels to each Jenkins step (#9556)Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"Introduction tutorial formatting fixes (#9539)* Introduction tutorial formatting fixesThis fixes some rST issues I noticed while going through the getting started tutorial. Some of these shouldn't be too controversial like using ` instead of `` and consistency fixes. I noticed lots of the `.. note:`s have titles even though they don't really render as anything special in Sphinx, but the base `.. admonition:` does render the title in the top line. This looks nicer IMO and wastes less space but it could go either way, I didn't change it in all places yet either.* Convert the rest of the tutorial `.. note::` directives to `.. admonition::`* Fix compilation for matmul tutorial + some random formatting fixes* Fix ambiguous links and remove namespaceCo-authored-by: driazati <driazati@users.noreply.github.com>",0
fix debug mask param check typo (#9586),0
[BugFix] fix nvptx not supported by device_enabled error (#9585)* [BugFix] fix nvptx not supported by device_enabled errorSigned-off-by: ZQPei <ziqiangpei@foxmail.com>* [BugFix] shared.dyn support for codegen_nvptxSigned-off-by: ZQPei <ziqiangpei@foxmail.com>,0
[CUTLASS] Refactor GEMM generator in preparation for conv2d (#9571)* split non-gemm specific generator code to gen_tensor_op.pycommit 250f915652e72e0012e9aa6ce0b6ef337d3da845Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:44:52 2021 +0900    remove conv2d stuffcommit 1a6b27c438472f13acd4a0f466d78f293415e076Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:41:31 2021 +0900    remove unused importcommit f7c3b5a191b8c73e8b178c32f6d3182fb0f697d6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:37:07 2021 +0900    add profiler boilarplate for conv2dcommit ca1ae274fb8f96a1dcde688deaf15339fe5604fbAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 06:22:06 2021 +0900    introduce gen_tensor_op.pycommit 37bb918e0873f04457c29479eb21a530b7052217Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 05:45:41 2021 +0900    more conv2d codecommit 5c00398892c99cb2a03be51f75878992663432ddAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Nov 14 05:13:30 2021 +0900    Begin conv2d support* fix* use functools.partial* remove unused import,0
Hotfix Jenkinsfile (#9592)* [CI] Use correct variable for image name in Jenkinsfile* Hotfix jenkins* Update JenkinsfileCo-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>,0
"[Frontend][PaddlePaddle] Support conv2d_transpose/rnn/fill_constant_batch_size_like (#9564)* add conv2dtranspose, rnn, fill_batch_size_like* fix conv_transpose and add RNN test case* Update paddlepaddle.py* Create paddlepaddle.py* fix scale attr of convert_interpolate* black codeCo-authored-by: heliqi <1101791222@qq.com>",0
[5/10] Code generation for Depthwise Convolution via CMSIS-NN (#9409)This PR adds support for depthwise convolution via CMSIS-NN.,1
[microNPU] Add support for unary elementwise CLZ (#9577)Add support for the CLZ (count leading zeros) operatorand the codegen test.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>,1
"[TVMC] Adds ethos-u-vela dependency in the ""tvmc"" set of dependencies. (#9590)The ethos-u-vela dependency is required for tvmc, given we support""ethos-u"" as one part of our tvmc targets, therefore, we need toadd this package to the optional set of dependencies that usersexplicitly ask when using tvmc.There is no impact for other sets of dependencies.",1
[ETHOSN] Update compilation defaults to Ethos(TM)-N78 (#9563)The default compilation target is now changed to target Arm(R) Ethos(TM)-N78 NPU.,1
Section names for TVM generated constants (#9524)This PR places the TVM generated static const arrays in a memory section named .rodata.tvm.The arrays default to 16-byte aligned but a constants-byte-alignment parameter can be added to the target to set alignment to a specific value.This allows the linker script to optionally place TVM generated constants in particular memory regions.,1
Update autotvm_relay_x86.py (#9601),1
[community] @areusch -> PMC (#9604),3
update the type of return value (#9603),1
Split GHA into 2 workflows (#9578)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
adding ArgReduceAttr to to inherit from Attrs (#9606)* adding ArgReduceAttrs as python class with register_object wrapper* cleaning,1
[ETHOSN] Add support for Ethos-N 21.08 driver stack release. (#9596)Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>Co-authored-by: Tristan O'Connor <tristan.oconnor@arm.com>,1
Add minimal forwarding RPC server for host driven python execution on Hexagon (#9526)* Minimal proxy RPC server for hexagon. Functions by routing fromAndroid to Hexagon via QTI FastRPC calls. Interim solution untilHexagon on-device RPC server is ready.* Apply clang-format.* Fix build to support building alongside Hexagon Launcher.* Add readme.* src/runtime/hexagon/rpc -> src/runtime/hexagon/proxy_rpc* Added small refactors to hexagon test_matmul.py.* Add skipif on additional env vars.* Fix IOS build.* Rename USE_HEXAGON_PROXY_RPC and add tvm_options entry.* Add NDArray::Container deleters.Co-authored-by: Eric Lunderberg <elunderberg@octoml.ai>,0
[microTVM][TVMC] Add TVMC test for Arduino and Zephyr (#9584)* Add TVMC test for Arduino and Zephyr* Address @gromero comments* address comments,1
"[Relay] Use LowerTEPass in VM (#9483)We replace use of the TECompiler::{Lower,LowerShapeFunc} methods from the VM'scompiler.cc with LowerTEPass. This clears the way for performing post-loweringIRModule->IRModule transformations which combine Relay and TIR analysis. In particular,it will allow us to use the PlanDevices pass to propagate memory scope constraintsacross PrimFuncs.We run LowerTEPass fairly early in the pipeline, which required quite a few passesto become 'post-lowering friendly'. In particular, ManifestAlloc is now run afterrather than before lowering, and so must now work in a mixed Function/PrimFunc world.The ""vm.shape_func"" operator has been removed since a) lowering has already generatedthe necessary dynamic shape function, and b) the call to that function can berepresented by an 'ordinary' vm.invoke_tvm_op call.We worked our way through the following glitches: - Dynamic shape functions are now given their true type (rather than the type of   the primitive function they are paired with). - Lowering was choosing definitional GlobalVars which were not pointer-equal to the   referential GlobalVars left behind in the rewritten Calls. We fixed that in   te_compiler.cc, though better would be to push GlobalVars deeper into the   lowering machinery. - device_copy was rewritten to a call to @__copy without any definition. Though we   tried adding it as a global this (obviously in retrospect...) won't typecheck if   there are multiple device_copies in the program. Instead leave device_copy unchanged   during lowering and update each executor codegen to look for them specially. - Calls to already-compiled BYOC functions were indistinguishable from calls   to (non-primitive) Relay functions. We move them into the call_lowered calling   convention, and leave behind a Function tagged with ""ExternalSymbol"". Better would   be a first-class representatn for externals in the IRModule but one step at a time. - Functions with dynamic shapes tagged for BYOC compilation were not tracking their   connection to their dynamic shape function. We now use exactly the same attributes   as for non-BYOC primitives. - VerilatorRuntime can legitimately be deleted before initialized. - IRModule attributes must be preserved. In particular, since LowerTEPass can   be invoked more than once we need to be careful to preserve any existing external   modules and other attributes gatherd from an earlier LowerTEPass. - GetUniqueName accounts for existing definitions in the module, but is not used   for external functions since their intended names are communicated to the codegen   toolchain via the already fixed ""global_symbol"" attribute.",0
[microNPU] Add the infrastructure for lookup table and TANH (#9547)Some activation functions like TANH and SIGMOID are implementedby calculating the values based on the QNN parameters andrecording the values into a lookup table (LUT).This patch adds the LUT functionality alongside with the TANHactivation function and the tests.,1
"[microNPU][1] Add affine analysis structures for the cascader (#9458)* [ETHOSU][1] Add affine analysis structures for the cascaderThe cascader relies heavily on being able to determinedata dependencies between operators. This is so that itcan calculate how stripes should be propagated through acascade.To do this, two data structures are defined: StripeConfigand Propagator. StripeConfig stores information for how atensor should be broken up into stripes and executed.Propagator transforms a StripeConfig using an affinetransform matrix, allowing an input StripeConfig for anoperator to be determined by 'propagating' the outputStripeConfig.By chaining together Propagators, we can analyse howdata dependencies vary throughout a cascade and thereforecalculate the memory requirements (and approximate theperformance).Change-Id: If7176fea961c631be4a6c195303da536030d957b* Add test guardsChange-Id: I1d7633e20daab33642fa5c4a12e474a4def4d8b8* Address review commentsChange-Id: Iff5f1effa08e0628de91f5577487d0cecebec824* Improve docsChange-Id: I508809d8c1a08d231e3a9b0fd9b3f2639cc2f0e3",1
"[microNPU] Move the compilation to use Target Hooks. (#9597)* [microNPU] Move the compilation to use Target Hooks.This commits moves the current compilation flowto use target hooks, so that the generated TIRis provided to unified module to for unifiedoptimizations.Change-Id: Ib3239a04ab201748e7f1b1ffa503cfe2aa7ccb7b* [microNPU] Move the compilation to use Target Hooks.*Fixing unpacked API tests*Adding use_device_api target attr to example target hooksChange-Id: I72c51caa57e9a0c2a538f40eb73939e28d4f112f* [microNPU] Move the compilation to use Target Hooks.* Modifed CLZ test case to support target hooks* Modifed reference TIR for test to include allocate annotation* TIR to CS translation tests are modified to run MakeUnpackedAPIChange-Id: I3a3d28777a6995e7f2b8789e14c5cb0f280dc763* [microNPU] Move the compilation to use Target Hooks.* Added a missed documentation to changes in source module* Skipping device api test for packed API as microNPU does not  support it.Change-Id: I6da1adcf8fdd3f972ec9b37ff530ff673e93058c* [microNPU] Move the compilation to use Target Hooks.* fixed tvmc test use unpacked-api for microNPU compilationChange-Id: Ib722d91ca3b3e4c6d13075ee0873acb86f487247* [microNPU] Move the compilation to use Target Hooks.* adjust target name.Change-Id: I862957324440705fb6093939b97b1a00fa1d4b46* [microNPU] follow up on using target hooks* Fixed few typos and cleaned up as per suggestionsChange-Id: I2a744a4bc4015e1884dbef4165252aa13aa30b31* [microNPU] follow up on using target hooksFixing some typos and change params toconst_dict as it seems more clearerChange-Id: Ia36a4635a68f6490bcc3eeaa72eeeeaadb6aa7f6* [microNPU] Move the compilation to use Target Hooks.Fixing up lookup table tests to use new runtime moduleimport structure resulted from using target hooks.Change-Id: I250aedef7cc73edad3812bb7e9aab013ed8bed5b",0
fix device on HandleCopyFromRemote (#9616),0
Fix cuDNN call for NHWC layout (#9600),0
[TVMScript] support kTarget func attr in tir script (#9594)* support kTarget func attr in tir script* fix variable redefine lint error* use Target::Export()* fix cr issues,0
[DRIVER] Specify name when build with primfunc (#9602),5
"[CUTLASS] Initial conv2d support (#9595)* Add initial conv generator* added conv2d pattern* profile by gemm profiler* remove conv2d profiler for now* remove unused code* add default* minor fix, profiling working* start codegen* generated code compiled* fixed layout initialization* matched with autotvm tensorcore result* test refactor* minor cleanup* remove iteration algo ""Analytic""* add test for dynamic batch conv2d* pass dl tensor as output too* support conv2d dynamic shape in codegen* test working* lint* simplify codegen* fix weird formatting* typo fix* check if cutlass is enabled in the test* simplify gen_conv2d.py",0
"[microNPU] fix the CMake to keep utils.cc (#9630)If the build does not use USE_ETHOSU ON,it will report that Object definitions are missing.This change will keep the file that has the Objectdefinitions.Change-Id: I64776f941bf0475e10397ff2cdbfd73a909611a0",0
[microNPU] Fixing imports in the entry point (#9624)This commit fixes errornous reporting that Velais missing if other import errors.Change-Id: I8db97be10018726cf5d9483508321a176c212516,0
[TIR][USMP] Greedy memory planning algorithm (#9214)This commit implements a greedy memory planning algorithmsusing the proposed USMP design.There are two greedy algorithmsintroduced here which use the size and number of conflicts as the criteria.- Adds few test cases checks for fan-out and linear structures.- Added a test case for ResNet sub-structure- Added a test case for MobileNet sub-structure- This includes a slight fix for buffer info  extraction where non-linear network buffers  owned by the main function should not show  sporadic liveness.,0
"[microNPU] Mean legalization support (#9576)Supports legalizing a Relay mean operation to an equivalent series ofNPU operations. Mean can be legalized given one of three cases:    - Case 1 (axis == [1, 2] and keepsdims == True):        depthwise_conv2d + binary_elementwise    - Case 2 (ifm qparams == ofm qparams):        pooling    - Case 3 (else):        depthwise_conv2dCo-authored-by: Rishabh Jain <rishabh.jain2@arm.com>",5
[microNPU] removing extra bytes for workspace (#9629)Given that microNPU codegen uses target hooksit undergoes the core compiler that updatesworkspace sizes. We dont need the additionalsizes anymore. This commit removes the sizes.Change-Id: Ifd2a01e5f9566ee0c37778f32392d6d1f8c7c091,1
Assume /Tools is part of the HEXAGON_TOOLCHAIN env variable (#9609),1
[microNPU] Add RequantizeArgs import to MeanParams (#9636)Fixes failing CI since CI on #9576 passed before #9624 was merged.Change-Id: I44f6aa0abe09d206f3e948e36e9799eaa12b3d14,0
"[TVMC] micro, run: Disable micro support when USE_MICRO=OFF (#9632)Do not generate 'tvmc micro' subcommand and disable micro device optionin 'tvmc run' when TVM is built without support for microTVM, i.e. withset(USE_MICRO OFF).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
[microNPU] Add NHWC -> NHCWB16 layout transformation pass (#9561)Adds a layout optimization pass that modifies the ifm/ofm layoutof an operation to NHCWB16 where possible. This can occur when theproducer or consumer of a tensor is also an NPU operator.,1
[RPC] Report correct port from C++ RPC to tracker. (#9642),5
Document CMSIS-NN Options. (#9647),5
[TVMC] run: Fix call to non-existing method (#9608),0
[Executor] Debug Graph Executor - Dumping output tensors include topographical ordering (#9557),0
[TVMSCRIPT] Misc error message improvements (#9543)* [TVMSCRIPT] Misc error message improvements* only prevent indexing into handles with multiple indexes* lint,0
[microNPU] Refactor codegen tests (#9623)* [microNPU] Refactor codegen testsChange-Id: I9c08520c9e03eb3fc32bd911b56c95981e851b4b* Fix paramsChange-Id: I8cea69ed3824c3a0417bb67abbabce460c17c4c6* Remove printsChange-Id: Iadf048e9590e724d73c2adac51bbe303de6f59a8* Address review commentsChange-Id: I56d647d86e3d495abe38b13cca349a71ec81cf4d,0
"[microNPU] Fix bug with re-reading in EncodeConstants (#9646)When a striping strategy that leads to weightsbeing re-read was deployed, the logic in EncodeConstantsfailed. This adds a test for that case and fixed thepass so it handles it correctly.Change-Id: I6f54cdb7be69428e49c3b4208271cd3e6c192e5d",0
[Dyn] Use SizeVar instead of Var in the GetShape function (#9650),2
"[microTVM] Remove prepare_options() (#9644)This commit removes prepare_options() helper function because it's nowsuperfluous since the default values for the project options are now setcorrectly at the server side, hence there is no need to force setttingthem at the client side.Also prepare_options() can incorrectly set the default value of Zephyr's'west_cmd' option when that option is not passed by the user. Hence byremoving that function this issue is resolved too.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
"[CI][microNPU]Running tests parallel using pytest-xdist (#9625)* [CI][microNPU]Running tests parallel using pytest-xdistThe microNPU tests runs safely, parallel using pytest-xdist.This commit introduces changes to run them in parallel onCI.Change-Id: Ia44d73203da320b81d7c8405ac4f201159654fec* [CI][microNPU]Running tests parallel using pytest-xdist* fixing a typoChange-Id: Iba8bed3d86b6aab64ba611a67e4751fa6b6b96c7",0
"[microNPU] Upgrade Vela to v3.2.0 (#9635)* [microNPU] Upgrade Vela to v3.2.0In addition to upgrading the version of Vela, this contains a bug fixand a fix due to an api change.The bug fix was found as a result of upgrading the version and ensuresthat block traversal mode is calculated before a set of block configsis found. As a result, valid block configs are calculated consistentlythroughout compilation.The fix for the api change is a cast to the datatype now expected byVela due to using strict casting rules.Change-Id: Icd40d11d37859f660527571d29cfeddd761d60da* adding version change to gen requirementsChange-Id: I45d2892951558e52c599af34e8d9d2d3fb5eb20b",0
[MetaSchedule] Fix comments in instruction traits (#9614),0
add einsum in pytorch frontend (#9651)* add einsum in pytorch frontend* add einsum in pytorch frontend,1
[ONNX]Support Opset 13 split IFF the split is a constant (#9643),5
"[7/10] Code generation for Pooling and Fully Connected via CMSIS-NN (#9531)Support for code generation of Maxpool, AvgPool and Fully Connected layers via CMSIS-NN",5
[microNPU] Add support for TFLite concatenate (#9589)* Add legalization pass and is_valid checks for concatenate* Add TIR pass for removing concatenates and replacing them with direct  writes to the final buffer* Add testsCo-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>,1
Document Project API server. (#9654)* Document Project API server.* address leandron comments* Update docs/arch/microtvm_project_api.rstCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>,1
[CI][MicroTVM] Disable autotune log check since microtvm autotune has erros  (#9639)* commented tune check* trigger,2
[TVMScript] Syntax sugar for reads & writes (#9634)* add test file* add syntax sugar support* add comments* cleanup* update stub* remove failed tests* update stub with overload* address comments,1
"[microNPU] Refactor section name test to remove relay_ir_builder (#9658)Since we're in the process of removing relay_ir_builder, I'm refactoring the test_ethosu_section_name test case to remove it from there.",3
[TVMScript] fixing block attr printing bug (#9667),0
Fix NormalizeError to allow colon inside CHECK (#9670),0
[Torch] Frontend update to support PyTorch 1.10 (#9664)* wip* fixed converting maskrcnn* fixed nll los* fixed linspace* fixed deformable conv2d* control flow and rnn test had no problem* swap more import orders* qmv3 test is having weird segfault* cleanup* black,0
[TIR][USMP] Augmenting the algo interface with memory pressure (#9649)This commit adds memory pressue to be an arugment tothe USMP algorithm interface as certain iterative algorithmscould use this as a guide determine the terminationcriteria.Change-Id: I3fb5eea3fe5ba43e68c23625d411e557f6dd89a3,1
[Target][TVMC][UX] Avoid creation of dummy Target objects in TVMC (#9662)* [Target][TVMC][UX] Avoid creation of dummy Target objects in TVMC* nit fix,0
Update gen_requirements.py with new onnx things  (#9541)* adds onnxoptimizer to gen_requirements.py,1
"[microNPU] Support different constant datatypes (#9626)Currently only uint8 datatype is supported for constants, as this isall that was necessary until now. This PR allows different datatypesto be used for constants, including different datatypes within thesame graph.A workaround was previously added for Mean legalization, this hasalso been removed and replaced with the expected datatype of theconstant.",1
[EZ][Autoscheduler] Log exceptions in topi lowering (#9615)* fix things* linting* remove format string messing things up* Update python/tvm/auto_scheduler/relay_integration.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* move to tvm errorCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[TVMScript] Add for loop syntax sugar (#9620)* add for loop syntax sugar* remove prints* better doc* finish thread binding* fix CI* fix CI* address comments* update sstub* fix CI* remove failed test* update stub* address comments* add decorator,0
"Add tvm-bot to triage role. (#9675)* Based on discussion in https://discuss.tvm.apache.org/t/rfc-ci-add-a-skip-ci-tag-to-shortcut-builds-and-tests/11589/10 * We will probably continue to leverage this bot for other, similar PR review feedback.",1
Options to create test directory and print commands in AOT Test Runner (#9638)* Options to create test directory and print commandsChange-Id: I381b4dfe870c9f6462681e77ebf0d3187749a535* Lint fixesChange-Id: I043d4e403df242ed304e10e98828fb1c982aac43* Addressed review comments: fixed incorrect docstringChange-Id: Ic0c7528986d87cb04ef5ccb03bf98ccf87750675,0
[microNPU] Add support for SIGMOID (#9627)Add support for SIGMOID activation function using the lookuptable mechanism in the NPU.,1
Add top level redirect from tutorials to tutorial (#9673),1
[TVMC][MicroTVM] Fix tvmc micro `project_dir` arg relative path (#9663)* Add fix for project dir path* address @gromero comments,0
[TVMScript] Add syntax sugar for T.handle and T.match_buffer (#9492),1
[Hexagon] Add RPC Mechanism for Hexagon (#9631)* Add Hexagon RPC* removed android remote and updated Readme* Add check for workspace size* Make libtvm_runtime consistent for Android* Remove root access* Fix some docstrings* Make stack remote size as parameter* add documentation* Refactor test conftest* clang format* Decoupled USE_HEXAGON_RPC* fix creation of test base directory on android* Address global variable* Fix format and Cleanup cmake* Fix build for other targets,0
Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 software (#9672)* Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 softwareChange-Id: I22685c117f3b6e9bc53c25ede14bb91c2b9f85f3* Upgrade to latest version of FVP based on Arm(R) Corstone(TM)-300 software- Allow CI tests to pass with existing FVPChange-Id: I67117de96c525fa01dae1d402a5f08743681a246,3
[Relay] Support nchwc layout in ConvertLayout pass (#9681),4
[Frontend][ONNX] Support ONNX Scan operator (#9438)* [Frontend][ONNX] Support ONNX Scan operator* fix lint* remove test_scan_sum in unsupported_onnx_tests* support scan opset 8* fix lint* fix negative axes bug* fix lintCo-authored-by: Matthew Brookhart <mbrookhart@octoml.ai>,0
"[microNPU] Update Conv2D Tests to Use TF API to Gen Test Cases (#9508)* Current conv2d tests compare the conv2d operator against tvm's execution of the default schedule of conv2d as defined in TOPI and that is not bitexact with tflite runtime's implemention. Therefore a tolerance of ""1"" in quantized 8-bit domain is used.* Converts the current conv2d tests to use TensorFlow APIs to create a test cases for conv2D and compare against TFLite runtime.",1
Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU (#9307)* Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPUChange-Id: If1e1134b56639021036862e8ea65a8e9d33dceb7* Tutorial for running TVM on Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU- Moved tutorials/micro/cortex_m_ethosu.py to gallery/how_to/work_with_microtvm/micro_ethosu.pyChange-Id: Ib554df6649b7313d4414187d5334ec5b03f35f33* [micronpu] Update and cleanup tutorials.- Moved tutorials/micro/cortex_m_ethosu.py to gallery/how_to/work_with_microtvm/micro_ethosu.py- Replaced full linker script document with a link to the linker script on githubChange-Id: Ic77648a4fc3dd76161d689774d21a3347a577b90* [micronpu] Update and cleanup tutorial- Replace Makefile code with a link to Makefile on github- Replace header files with a link to header files on github- Update demo.c with changes introduced by Device API- Update tvmc command line argumentsChange-Id: If6a254b368550c0f3effb8d1cb15f062279964e2,1
[TIR][USMP] adding the pass to convert to pool offsets (#9418)* [TIR][USMP] adding the pass to convert to pool offsetsThis commit adds a transform pass that consumesthe planned pool allocations using memory planning algorithmthat convertes them to pool offsets.* adds two test cases for a linear structure with two pools* adds test case with a single pool for residual structuresChange-Id: I9d31e854461b5c21df72d1452120d286b96791c0* [TIR][USMP] adding the pass to convert to pool offsets* Adding a toggle to produce TIR that is TVMScript printable for unittesting* Fixing the unit tests* Ensure deterministic pool variable ordering.Change-Id: I317675df03327b0ebbf4ca074255384e63f07cd6* [TIR][USMP] adding the pass to convert to pool offsetsFixing the references after changes in the memory planningalgorithm.Change-Id: Id7c22356fd5de43d10a2b4fc70e978af2c6d599d* [TIR][USMP] adding the pass to convert to pool offsets* fixing the lintChange-Id: I7ff920b92d14a9919c930a4b35a2169c77a57dd1* [TIR][USMP] adding the pass to convert to pool offsets* removing unnecessary defitinitions* remove global var map* adding explaination for let bindings to pointer typeChange-Id: I31bd1a9f3057ee7f06252263565b0f75c51e6d13* [TIR][USMP] adding the pass to convert to pool offsets* rebase changes* making imports absolute* fixing typos and removing unnecesary linesChange-Id: I4c94b9955b001513fecb39ca94f81b1ad99c7bfc* [TIR][USMP] adding the pass to convert to pool offsets* fixing typosChange-Id: I42c557fd394aefdf8c2e825c4e88770eb0732f9b,0
Fix TVMC micro import error (#9688),0
"[RVM] Fix AttributeError when action is not specified (#9683)Although an action (build, test, or release) is always required bybase-tool-box.py currently actions are not set as ""required"" in theparser, hence it doesn't complain when an action is missing and laterwhen 'args.platform' attribute (present in all actions) is referencedthe tool exits abruptly due to an AttributeError (because 'platform' isnot set), without giving any clue on what went wrong. For instance:$ python3 ./base-box-tool.py --provider virtualbox # no action is givenThis commit fixes it by setting action subparsers as required so theparser complains when no action is specified.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[FIX] Simplify during create prim func (#9691),0
Fix conv2d_transpose layout transform issue in trt (#9668)* fix* change default in teh transform instead,0
[ONNX][Converter] Fix when onnxoptimizer is unavailable (#9700),0
Don't requantize if bias or quantize scales are approximately equal (#9676),5
"[microNPU] Add support for SPLIT and SPLIT_V (#9621)Both, SPLIT and SPLIT_V get lowered to relay.split and in thelegalization the Relay split gets turned into strided slices. Thispatch adds the pattern and legalizer to enable offloading the TFLite'ssplits to the NPU.",1
"[TIR] Allow memory (aka storage) scopes to be retrieved/applied to PrimFuncs (#9689)* [TIR] Allow memory (aka storage) scopes to be retrieved/applied to PrimFuncsThis is in support of #9613 which allows memory scopes to flowout of already-lowered PrimFuncs into the rest of the Relayprogram. This means scope choices made during lowering canbe accounted for in the rest of the program, with device_copiesinserted as required.Somewhat more speculatively we also allow memory scopes to flowin to PrimFuncs. This is in preparation for when we can splitlowering into two phases: i) lower ""primitive"" fused Relayfunctions to TensorIR in a schedulable form roughly isomorphicto TE, and ii) actual scheduling down to traditional TIR. Oncethat split is made it will be possible to flow memory scopesout of one PrimFunc and into another so as to avoid unnecessarydevice_copies being necessary due to independently chosenmemory scopes.I also suspect we'll want to put our focus on layouts ratherthan memory scopes, but this at least sets up some of themachinery.* [checkpoint] Junru's comments.",2
"[ETHOSN] Allow Ethos(TM)-N testing without hardware (#9702)If no hardware is available, simulated tests allows executionof the Ethos(TM)-N - TVM integration. This patch allows thisfor the 21.08 release of the Ethos(TM)-N driver stack.",3
[TVMScript] Improve printer for TIR syntax sugar (#9680),5
"Add an option to FQ2I to fail soft or hard (#9660)add testfix lint, rework hard fail conditionalsapparently clang-format didn't take last time I triedI can haz overflowing comments",0
[RELAY] [AST] Add virtual_device as a first class field in Relay (#9641),1
[SimplifyExpr] Simplify consecutive adds with constants (#9671),1
"[Relay] PlanDevices supports 'free' on_device annotations (#9693)* [Relay] PlanDevices supports 'free' on_device annotationsThis is in support of #9613, which allows PlanDevices to be runafter lowering so as to flow memory constraints in andout of PrimFuncs. That requires a way to insert device_copieswhen the memory scopes chosen during separate lowering of fusedprimitive functions clashes, but otherwise avoid device_copies whenscopes can be chosen so as to avoid them.We support that by generalizing the ""on_device"" annotation toallow the device constraint to be independently controlled forits 'body' and 'result'.# Standard user annotation: body is constrained to Son_device(body, S)# Used by PlanDevices to 'fix' expression to S# (was is_fixed=True)on_device(body, S, constrain_result=True)# Used by PlanDevices to indicate a device_copy can be# inserted if necessary.on_device(body, S, constrain_body=False)# Supported, but currently has no use.on_device(body, S, constrain_result=True, constrain_body=False)A few extra odd's 'n ends collected along the way: - Some CallLowered cleanup which I found useful. - The usual extra debugging output needed as I debugged.   In return I removed some particularly verbose logging I'd   added while tracking down unexpected object copies. - Cleanup warnings from clang-12 as I touch files.* [checkpoint] unused var",0
"[Hexagon] Detect link-params via IRModule instead of target attribute (#9695)Additionally, restore the construction of target attributes in theHexagon target. This was erroneously removed in PR#9352.That PR deprecated target attributes, but also added transferringthese attributes to the new mechanism. Removing the attributesfrom tvm.target.hexagon eliminated them completely. Restoring thetarget attributes allow time to transition TVM clients to the newmechanisms.",1
Fix LLVM version for Hexagon (#9711),0
[CMSIS-NN] Fixed return data type from pattern callback function (#9682)This commit fixes the following issue: MergeComposite callback function from pattern table returns non boolean data type,0
"Add Hexagon VTCM and discontiguous allocation support (#9525)* WIP Allocation abstraction for VTCM and DDR.* Add Hexagon VTCM and discontiguous allocation support* differentiate between dimensions and allocations* remove change to llvm codegen* add integration test_add_vtcm to demo vtcm alloc* remove cmake change* forcing contiguous allocation in device API, for nowCo-authored-by: Chris Sullivan <csullivan@octoml.ai>",1
"[TVMC] run: Don't use static path to find model.tar (#9712)* [TVMC] run: Don't use static path to find model.tarCurrently 'tvmc run' when '--device micro' is specified looks for themodel in the project directory at <project_dir>/model.tar. That worksfor Zephyr but fails on Arduino because model.tar is actually located at<project_dir>/src/model/model.tar. As a consequence 'tvmc run' when usedto run a model on Arduino exists because model.tar is never found.This commit fixes it by using the MLF path returned by the Project APIinstead of using a static path.This commit also adds a project_dir attribute to TVMCPackage that can beset when a MLF archive is loaded/imported so the project dir can beconveniently found (similarly to package_path attribute).Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* [TVMC] test: Add test for importing a MLF with project_dirAdd test for TVMCPackage when importing a MLF archive and setting aproject directory too. Setting a project dir is only supported when aMLF model is imported, so it must fail on Classic format.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"Follow up from CMSIS-NN pooling failure (#9708)This commit fixes few comments in TIR2Runtime pass of CMSIS-NN target.These comments specify layout used by CMSIS-NN API for input and filter shapes.Another fix was done to the filter layout calculations. Instead of hard coded values for dimensions, filter_shape.find(""H"") was used to locate a particular value.Third fix was done to the padding API used by Conv2D and Pooling tests.It was made generic for TFLu's ""SAME"" padding type.",0
"Improve tvmc error message from lazy-loading frontend imports (#9074)When installing TVM from the python package, the Frontend frameworks dependencies such as TensorFlow, PyTorch, ONNX, etc, are not installed by default.In case a user tries to run tvmc using a model whose framework was not installed, it will be presented with a very raw Python exception in the output.The aim of this commit is to implement a better error messages for errors related to lazy-loading frontend frameworks in tvmc.Change-Id: Ida52fac4116af392ee436390e14ea02c7090cef0",0
[TOPI] Add generic batch norm (#9694)* Add topi batch norm and tests* Handle none values correctly* Return correct nun outputs for onnx* Use moving var/mean and update tests* Add a test for batch norm folding* Fix comment* Format with black* Re-order test args to match interface* Call fold constant manually,0
[TVMC] Add --opt-level to compile mode (#9722),1
[Relay] Non-recursive dependency graph (#9528)* fix recur* fix ci,0
"Add `make docs` and doc building instructions (#9534)* Update doc building instructions and pin dependenciesThis pins the dependencies for the docs and adds `pytest` as adependency which was missing when I built. Tested out therequirements.txt with a fresh `ubuntu:focal` Docker image to verify thatthe required depedencies work.* Address comments, add Makefile for docs and add to instructions* Use Python for running scripts* Fix lint, add lint command* Add option for cpu to only run the precheck, address comments* Fix 'make doc' usage, add some -x's* Fix bad condition on --cpu, add defaults for envs* Fix another 'make doc'* Fix for running on MacOSCo-authored-by: driazati <driazati@users.noreply.github.com>",0
Fix typos in runtime comments (#9726)Fix typos in comments about signal handling and entries in the runtimecode.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
[BugFix] Fix a wrong use of `std::move()` in cross-thread reduction lowering (#9728)* [BugFix] Fix a wrong use of `std::move()` in cross-thread reduction lowering* Remove,0
"[CUTLASS] More robust support for pattern matching and alignment (#9698)* bug fix in im2col encoding* skip legalize when batch size is dynamic* add sm75 kernels to sm80 profilings* add dtype and layout check in parttern match* use align1 kernel for unusual channel cases (IC = 3 etc)* test IC=3 convolution* fixed check functions for fused cases, run infer type before mergecomposite* check align on N dim* add comment on IC == 3 case* lint fix* do not offload depthwise conv2d* lint* trigger CI",0
"[Relay, BYOC] Make constant binding in PartitionGraph optional (#9721)* make constant binding in PartitionGraph optional* add test* Doc string update* remove default argument from set_body_typed",1
"Add CMake summary (#9696)* Add CMake summaryThis adds a printout at the end of CMake's configuration that prints out all the user options (anything declared via `tvm_option`) plus some extra metadata about CMake itself (version, system, etc). This makes it easier to see what CMake is doing without guessing from config files (i.e. is the build release or debug mode, is CUDA on, ...)* Add SUMMARIZE=ON to CI cmake configsCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[CUDA] Do not emit vector load on unaligned base offset (#9731)* [CUDA] Do not emit vector load on unaligned base offset* fix alignment check condition* black* improve test* improve the vectorization condtion to avoid error in yolo5 (thanks to vinx13)* replace coeff != 1 check by coeff % lane == 0,0
"Merge Java unittests into GPU unittests (#9732)These run on the same node but the Java unit tests take < 1 min (whereas the node setup alone takes 3-4 min), so it seems like it would be good to just run the Java tests as part of the general GPU unit testing rather than allocate and set up a separate node for it.Co-authored-by: driazati <driazati@users.noreply.github.com>",3
[microNPU] Update Arm(R) Ethos(TM)-U55 NPU demo README (#9725)- Update README.md to show config.cmake options when building from sourceChange-Id: Iecc36d8bc889541b8b890de37d58052a46928a83,1
"[Relay] Re-run PlanDevices after LowerTE to flow new memory scope constraints. (#9613)* [Relay] Re-run PlanDevices after LowerTE to flow new memory scope constraints.This PR: 1) Makes PlanDevices consider lowered calls when solving device domain constraints. 2) Connects the storage scopes on PrimFunc parameters (encoded in their Buffer data    Var type annotation PointerTypes storage_scope fields) to the memory_scope    fields of the SEScopes which PlanDevices unifies over. 3) Allows new device_copies to be inserted on the arguments and results of lowered    calls so as to acount for any memory scope mismatches which are now apparent.[device_planner.cc has main changes, rest is secondary.]In the short term we'd like to use this machinery to flow memory scope choices madeduring lowering back out into the overall Relay program. In the longer term we'dalso like to be able to use memory scopes to influence the lowering ofyet-to-be-lowered functions (or lowered functions which have yet to been scheduled,a distinction now possible with TensorIR). - Memory scope constraints can flow both out of and in to PrimFuncs   introduced by LowerTE. In TIR memory scopes are represented by   'storage scopes' on the PointerType type annotations on TIR Buffer data   variables.    - It is straightforward to extract memory scopes from PrimFuncs by      looking at the PrimFunc's buffer_map. We do this is 'phase 1' of      PlanDevices, which collects all the device constraints implied by    - However, pushing memory constraints in to PrimFuncs is more challenging      due to buffer aliasing. This aspect is still experimental. - Allow device_copies to be inserted for both arguments and   results of PrimFunc calls, on the assumption PlanDevices has   already established a consistent device assignment prior to   lowering and any new mismatch is required to match up memory scopes.   We use the new 'free' on_device annotations to implement this.Coming along for the ride: - To make unit tests of mixed Relay/TIR functions possible needed   to be able to supply a checked_type to GlobalVar since that's currently   the only way to give a Relay type to PrimFuncs. - Use GenSym to get unique var names in ANF & partial eval so easier   to diff debug output between passes and connect program fragments   back into the overall program. Relying on pretty-printing to   automagically unique-ify var names is certainly cute but until we   have better span support is very hard to work with. - Realized both dead_code.cc and fold_constant.cc would   happily move values into a different lexical virtual   device context since device_planner.cc was being   'clever' and eliding on_devices for let-bound values   when there's no change. Fixed so that every let-bound   value has an on_device. Will be much better after   https://github.com/apache/tvm-rfcs/pull/45 is implemented. - Make build -Werror clean for clang-12 (mostly move fixups). - Address post-submit comments from #9693.* [checkpoint] thread safe GenSym",0
[CI] Hotfix Jenkinsfile (#9739),0
"Fix for tvm.build()'s name warning (#9678)* change default name of tvm.build() to None* change lower name default to None* fix lint* lower() use ""main"" -> ""default_function""* remove default name's warning",0
"[CUTLASS] Add conv2d profiler (#9737)* Add cutlass conv2d profilercommit 1c0bbb297da43dab75ad995afbcacd59e9fe4c87Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:29:03 2021 +0900    fix lintcommit 463574ce087ca0444b23d4f47baf8066b8fbd3dfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:28:38 2021 +0900    fixed conv2d checkcommit 588c5abe15abbf0339a7972d81b8235bc7460620Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 15:05:27 2021 +0900    update testcommit a447b57ade7c99da4efd38e1bdfc213a64a80fd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 14:54:52 2021 +0900    speed up profiling by removing initializationcommit 93cd039ba04dd80e887ec1a71f358cd86a1e5221Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:26:29 2021 +0900    fixed nhwc cudnn depthwise convcommit 6db71727f553ee2009c9d3feb2b019c24459f4d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:39:05 2021 +0900    add cachecommit f7d17a116acd80c57dfa04aa86577fa30898908dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:05:38 2021 +0900    removed im2col profiling for conv2dcommit b724f446d07030e45f30474a1e3c124f1541b496Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:57:54 2021 +0900    blackcommit fe4687b9f6d41eff66742cdde694680538a3d5e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:49:13 2021 +0900    fixed cmd arguementcommit ab114f5c3e1a3086255caccd6c0f5df09d7c3755Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:22:19 2021 +0900    conv2d profiler workingcommit 49ee61f5583f73f0d9b2c18c1861c90fa028f64aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 20:26:15 2021 +0900    add conv2d profilercommit 49e2c8918a1a2632f60c700675f12f9d83adef24Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:03:36 2021 +0900    do not offload depthwise conv2dcommit cd8367768229720020d81d597d751bfa95ec014eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:20:01 2021 +0900    lint fixcommit 870823c6d54114896aa9db91eb72c132cdef762fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:54:38 2021 +0900    add comment on IC == 3 casecommit 6b780db7f8059a9a58bfc257fbb9cf7cb60b72a2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:48:33 2021 +0900    check align on N dimcommit 308c4dac39f761ac157f032b9fe7028820980294Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:34:42 2021 +0900    fixed check functions for fused cases, run infer type before mergecompositecommit 8d6a1bfee26e9dfdeb1ab001aa89b43b9cd33d74Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:59 2021 +0900    test IC=3 convolutioncommit ffce47de724398222f672b220156b19c8f48a700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:16 2021 +0900    use align1 kernel for unusual channel cases (IC = 3 etc)commit 6cdf205a3451b5fa7ea0cbff0228ae1da775573aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:06:56 2021 +0900    add dtype and layout check in parttern matchcommit 7743cc6dde442b45d66b4be320268ed24f352422Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:53 2021 +0900    add sm75 kernels to sm80 profilingscommit efceccb994ec71bc4ba847c791fc4f696eb7f242Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:42 2021 +0900    skip legalize when batch size is dynamiccommit 65fbc0a0813cb4e980e53240f34f3ba18754ab99Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:36:36 2021 +0900    bug fix in im2col encoding* minor fix* lint fix* allow autotvm NCHW depthwise conv2d schedule even if -libs=cudnn* Update python/tvm/contrib/cutlass/gen_conv2d.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* simplify processing profiler outputs* more simplify* fix runtime checkCo-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[TIR] Add 'global_symbol' and 'tir.noalias' as default attributes in script auto completion (#9744)* [TVMScript] Update default TIR prefix to T* [TIR] Add 'global_symbol' and 'tir.noalias' as default attributes in script auto completionCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,0
[TIR][Schedule] Analysis functions to check if compute_inline and com… (#9743)* [TIR][Schedule] Analysis functions to check if compute_inline and compute_inline is allowedCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>* Address commentsCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,1
[Relay] Support large constants saved/loaded outside of VM executable (#9734)* [Relay] Support large constants.This allows constant tensors at or above a given byte limit to be marked as'late bound' and saved/reloaded to a file independently of the overallexecutable. Since the executable is often embedded in the data segment ofgenerated runtime Modules this avoids problems with external tools which can'thandle multi-gigabyte data segments.[ACE-466 in OctoML JIRA]* [checkpoint] fix latent bytecode/code bug,0
Fix typo (#9740),0
[Frontend][PaddlePaddle] Enhance paddlepaddle frontend with more operators (#9724)* add operators for paddle frontend* add operators for paddle frontend* retrigger ci* retrigger ci* retrigger ciCo-authored-by: wjj19950828 <wjjisloser@163.com>,1
fix compact_dataflow (#9747)Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,0
"[TensorIR] Primitive ""SetScope"" (#9738)* Main code* Reorder steps* Unittests* Docstring* Check the input storage scope* Docstring for `CheckStorageScope`* Import header",2
Add unit tests for HexagonBuffer (#9736)* Add unit tests for HexagonBuffer* fix build error for signed / unsigned comparison,0
"[Relay] Add a unit test for structural equality (#9745)This is CORE-135 from the forums, which suggested structural equalitywas deeply broken. But unable to repro. No harm including unit test.(attempt 3)",1
"[CUTLASS] Support conv2d activation fusion (#9746)* Add cutlass conv2d activation (bias, relu, sigmoid)commit e4e273ae74a8e54ab1ae1414ce9b6bfcc2b3d530Merge: 0489d1418 77c938550Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:58:54 2021 +0900    Merge branch 'partition-constant-unbind' into cutlass-conv2d-fusioncommit 77c9385501595f804bd33b436aed0cc192059a10Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:58:18 2021 +0900    add testcommit ab01b3aae36ef88ec299ff5e9d1bbdf5fd7268f6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Dec 13 11:55:06 2021 +0900    make constant binding in PartitionGraph optionalcommit 0489d1418c5f49b95ad220b8f0e57ba48443c6a3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 21:52:29 2021 +0900    support sigmoid fusion (only fp32 accum for now)commit 3705bbd6b77ec8d8c416ab00738fe8b9c91e1535Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:50:58 2021 +0900    conv2d fusion test workedcommit 05b51c9f94c9a4f31afeb126c8749675be423892Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:34:10 2021 +0900    fix bias stridecommit 7cf40e719a8c1c464e2ee925018d8ed90b145dcdAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 20:01:21 2021 +0900    use nobetascalingcommit 274ec028845e296b012c23830a0be6f48cbee767Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 19:12:58 2021 +0900    adding fusion support to codegencommit 0de5ebdb2e318129a1a4d3a2e568849a993525c4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:39:08 2021 +0900    partition workingcommit c08bb38e3eea90168dc036cb4cbfbcdef288bac6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:24:42 2021 +0900    update testcommit 81bf9e600b1bbdbc27e0e8b54496fc09b33e0624Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:23:39 2021 +0900    add fused conv2d patterncommit 1c0bbb297da43dab75ad995afbcacd59e9fe4c87Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 18:29:03 2021 +0900    fix lintcommit 463574ce087ca0444b23d4f47baf8066b8fbd3dfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 17:28:38 2021 +0900    fixed conv2d checkcommit 588c5abe15abbf0339a7972d81b8235bc7460620Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 15:05:27 2021 +0900    update testcommit a447b57ade7c99da4efd38e1bdfc213a64a80fd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 14:54:52 2021 +0900    speed up profiling by removing initializationcommit 93cd039ba04dd80e887ec1a71f358cd86a1e5221Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:26:29 2021 +0900    fixed nhwc cudnn depthwise convcommit 6db71727f553ee2009c9d3feb2b019c24459f4d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:39:05 2021 +0900    add cachecommit f7d17a116acd80c57dfa04aa86577fa30898908dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Dec 11 15:05:38 2021 +0900    removed im2col profiling for conv2dcommit b724f446d07030e45f30474a1e3c124f1541b496Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:57:54 2021 +0900    blackcommit fe4687b9f6d41eff66742cdde694680538a3d5e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:49:13 2021 +0900    fixed cmd arguementcommit ab114f5c3e1a3086255caccd6c0f5df09d7c3755Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 22:22:19 2021 +0900    conv2d profiler workingcommit 49ee61f5583f73f0d9b2c18c1861c90fa028f64aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 20:26:15 2021 +0900    add conv2d profilercommit 49e2c8918a1a2632f60c700675f12f9d83adef24Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Dec 12 08:03:36 2021 +0900    do not offload depthwise conv2dcommit cd8367768229720020d81d597d751bfa95ec014eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 13:20:01 2021 +0900    lint fixcommit 870823c6d54114896aa9db91eb72c132cdef762fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:54:38 2021 +0900    add comment on IC == 3 casecommit 6b780db7f8059a9a58bfc257fbb9cf7cb60b72a2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:48:33 2021 +0900    check align on N dimcommit 308c4dac39f761ac157f032b9fe7028820980294Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:34:42 2021 +0900    fixed check functions for fused cases, run infer type before mergecompositecommit 8d6a1bfee26e9dfdeb1ab001aa89b43b9cd33d74Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:59 2021 +0900    test IC=3 convolutioncommit ffce47de724398222f672b220156b19c8f48a700Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:10:16 2021 +0900    use align1 kernel for unusual channel cases (IC = 3 etc)commit 6cdf205a3451b5fa7ea0cbff0228ae1da775573aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 12:06:56 2021 +0900    add dtype and layout check in parttern matchcommit 7743cc6dde442b45d66b4be320268ed24f352422Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:53 2021 +0900    add sm75 kernels to sm80 profilingscommit efceccb994ec71bc4ba847c791fc4f696eb7f242Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:40:42 2021 +0900    skip legalize when batch size is dynamiccommit 65fbc0a0813cb4e980e53240f34f3ba18754ab99Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Dec 10 10:36:36 2021 +0900    bug fix in im2col encoding* support batch norm fusion",0
[TIR][Schedule] Add Annotate/Unannotate primitive (#9742)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,1
[MetaSchedule] Add the missing HasWorkload interface to the Database (#9756),1
[TVMScript][FIX] Fix number of arguments for T.Buffer[...] (#9758)* fix number of arguments* make test clear* Update tests/python/unittest/test_tvmscript_syntax_sugar.pyCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>* only tuple for nowCo-authored-by: Wuwei Lin <vincentl13x@gmail.com>,0
Move Compute library to 21.11 (#9754),4
"[Relay] s/SEScope/VirtualDevice/g (#9759)* [Relay] s/SEScope/VirtualDevice/gNobody liked 'SEScope', and 'DeviceMcDeviceFace' is too verbose, so itseems 'VirtualDevice' has the popular vote.",5
"[Runtime][Pipeline Executor] Add the map logic of global input and subgraph input. (#9751)* [Runtime][Pipeline Executor] Add the map logic of global input and subgraph input.User can use ""global input name"" to feed input data for pipeline runtime. The name like""data_a"" will be mapped into a input interface of subgraph. In this PR, wecreate the related logic to do the following things. 1. building the input map configuration 2. in runtime c++ module, parseing the input connection configuration then    creating related data structure to record the said connection map. 3. providing the function to return the map information for verification.* address review comments.* addres review comments.* address review comments.",1
[MetaSchedule] Random Feature Extractor (#9760)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,2
"[Relay] Fix invalid shape function for ""copy"" operator (#9749)The 'script' for of the shape function was ill-formed,resulting in a TIR shape function which did not assignto it's output, which in turn caused either OOM orassert fails as uninitialized dimensions worked theirway downstream. That fix is in python/tvm/relay/op/tensor.py.Everything else is for testing and debugging as I trackedthis down.Special thanks to Lily for helping me with the scalar vstensor switch in the copy shape function.[This is CORE-112 in OctoML JIRA.]",0
"Remove CMake string REPEAT (#9771)According to https://cmake.org/cmake/help/latest/command/string.html#repeat this was added in 3.15, we can revert this if we ever use a minimum cmake newer than that. Until then we should just remove the alternate path to reduce complexity.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
[MetaSchedule] Misc improvement of the Measurer (#9757)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,5
[M3c][MetaScheduler] Add ScheduleRule class & PostOrderApply space generator. (#9761)* Add ScheduleRule class & PostOrderApply space generator.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix comments & docs.* Fix for mypy.* Retrigger CI.* remove get_hex_addressCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
[community] @lunderberg -> Committer (#9773),3
Clarify error message for missing libraries (#9710)Ran into this when importing tvm without a buildCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[TE][TensorIR] fix tensor attr in create_prim_func (#9764)* [TE][TensorIR] fix tensor attr in create_prim_func* Update src/te/operation/create_primfunc.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,0
Fix GLOBAL_SCOPE Shallow copy bug (#9718),0
[TIRScript] fix parse StringImm value in for loop annotations (#9755)* fix parse strimm value in for annotations* flatten buffer allow runtime.String attr value* remove unused import* rebase and ensure flattened attr order,0
"Include \0 terminating character in strncpy (#9775)I'm fairly certain that this bugfix is correct, but please check my code. I'm not sure how this hasn't caused more problems for people! Perhaps the malloced memory is usually filled with 0s? Either way, on my system, the malloced memory was NOT all 0s, and so when we would copy the `key.size()` valid characters of `key` into `*out_type_key`, it was likely that the next character was not 0, and thus, because no terminator was copied over, the string wouldn't be terminated correctly. This fix ensures the terminator gets copied over, by copying `key.size() + 1` characters.",0
fix int set analysis on negative scale (#9776),0
[microNPU] Added checks for out of range shifts (#9707)* [microNPU] Added checks for out of range shifts* Added testcase* Addressed comments,1
"[Hexagon] Account for objects being smaller than the allocated space (#9769)* [Hexagon] Account for objects being smaller than the allocated spaceIn particular, graph executor will reuse allocated buffers for varioustensors. These tensors may be of various sizes as long as the bufferis large enough to hold them.* Trigger CI* Add #include <algortihm> to hexagon_buffer.cc* Update cpptest, plus fix one CHECK in hexagon_buffer* Rename `offset` to `copied` for consistency",0
[microNPU] Fix incorrect comparison in schedulers (#9706)* [microNPU] Fix incorrect comparison in planning functionsAdded diamond graph test case to test_scheduler.py* Fixed copy luts test case,0
[microNPU] Upgrade to 21.11 version of Arm(R) Ethos(TM)-U55 NPU driver (#9777)Change-Id: Ide4d2a33a215b4f1367667ad0fa4a66913cf9ad4,4
"[Bugfix]  fix the bug that occurs when the test_pass_ctx_exception() is (#9774)tested separately.In the batch test ,cause a previous function will specify the value ofPassContext.current() as None,so tests the test_pass_exception() will pass.but when testing it individually ,the result of PassContext.current() isa [] instead of None,it will not pass, this may happen when using theoption of ""pytest -n 4""",0
Improve the frontend tflite _test_abs test to support tflite 2.6 (#9783)Updated the test input node name,1
[AMP] Disallow converting layer norm to fp16 (#9782)* [AMP] Disallow converting layer norm to fp16* black,5
[Autoscheduler] Task Extraction Raises Exception on Lowering (#9750)* forward messages* Update python/tvm/auto_scheduler/relay_integration.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* remove type annotation for consistency* lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>,1
[CodeGen] avoid crash if an exception is raised during llvm cpu codegen (#9786)* avoid crash if an exception is raised during llvm cpu codegen* use pytest.raises,3
[M3c][MetaScheduler] Add More Measure Callbacks. (#9780)* Add measure callbacks.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix comments.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
"[Docker] Update to Torch 1.10.1  (#9781)* update pytorch to 1.10.1* fix missing import test only on llvm and cuda* Revert ""[Docker][Onnx] Upgrade ONNX to latest version (#9519)""This reverts commit 3f5dca5f56da92942168bdf80ee8b7b24667ac1f.* skip testing if target is not enabled",0
"[CUTLASS] Conv2d activation fusion, part 2: Sigmoid fp16, SiLU and HardSwish (#9795)* [Torch] do not pad if pad widths are all zero* silu fusion supported* adding hardswish support* support fast_math sigmoid op* fixed type inference for yolov5 + silu fusion* use include_non_call_ops=False in AnnotateTarget* update cutlass* revert change in build.py* simplify codegen* lint",0
Update README.md (#9798),1
[microNPU] Re-enable LayoutOptimizer pass (#9793)* [microNPU] Re-enable LayoutOptimizer passIt looks like in #9597 the LayoutOptimizer pass was accidentally removed.Probably due to a race condition in PR's. Re-enabling this feature.Change-Id: I4fc16a440f90277c5fcd887715166332af052c6b* change pass orderingChange-Id: I6e7a22f46660029bbf4be3deb2be929cecf5d365Co-authored-by: lukhut01 (generated by with_the_same_user script) <lukhut01@e127400.cambridge.arm.com>,4
Redundant batch_flatten removed for 2D input matrix in Dense layer. (#9792)* Redundant batch_flatten removed for 2D input matrix in Dense layer.* Fix to follow code review. infer_type for input[0] is called once.,0
[Target] Fix device mask issue and typos (#9768)* [Target] Fix device mask issue and typos* Skip target hook,0
"[CMAKE] Automatically detect newly added source files (#9611)* [CMAKE] Automatically detect newly added source filesBefore this commit, newly added or removed source files were notdetected by cmake. This manifested either as file not found errors fromthe compiler (when files were deleted) or packedfuncs not being found(when files were added). This commit uses the CONFIGURE_DEPENDS optionof cmake's `file(GLOB)` function to ask the build system to check fornew files on every rebuild. Checking for new files adds a slight butnegligible overhead to each build, but is better than unexpected errors.* remove unnessesary configure_depends",0
"[TE] Support varargs in te.compute (#9796)* [TE] Support varargs in te.computeSupport varargs (`lambda x, *args: ...`) in te.compute. The varargs takeindices into the remaining dimensions of the outputs shape. Thisrequires using inspect.getfullargspec instead of `fcompute.__code__`.Also add checks that there are no keyword arguments.* implicitly broadcast to remaining dimensions",1
"[TIR] Affine utility support iter lowerbound and diagnostics (#9699)* Enable freevars, iter lowerbound and diagnostics in affine utility* fix lint issues and compare bug* update to use iter shift instead of itermark min for lowerbound* add testcase of fused iters sum with multiple lowerbounds* add more affine check testcases, fix bug for single iter and duplicate constraints on iter* add a newline to comment* forbidden predicate unmatchCo-authored-by: baoxinqi <wrongtest@intellif.com>",0
Update CONTRIBUTORS.md (#9804),1
[BugFix][TensorIR] Non-positive constant input factors for `split` (#9805)* Update docs of GetProducers/GetConsumers* Fix split for non-positive factors,0
fix a bug of instance norm. (#9806),0
"[Topi] fix get_pad_tuple3d bug, the conv3d kernel layout should be DHW. (#9788)",0
"[Frontend] Add Span filling for frontends to Relay (#9723)* [Frontend] Add Span filling for frontends to Relay* Add a common span filling feature for tf1/2, tflite and pytorch.* Add test case for Span filling in each frontend.* Expose Tuple and TupleGetItem to python end* [Frontend] Add Span filling for frontends to Relay* Fix lint errors* Change default string of scope_part in Pytorch* Reorder the span position for one to many conversion* [Frontend] Add Span filling for frontends to Relay * nit fixed * Add a bool flag to control print span * refactor pytorch get span to a birefer way* [Frontend] Add Span filling for frontends to Relay* Add one more condition for spanFller* Refine the format for those pytorch node without scopeName* [Frontend] Add Span filling for frontends to Relay* Fix lint",0
[TVM Basic] Extend generic func with get_packed_func() interface (#9784)add test_target_temp_strategy unittest.Co-authored-by: sqing <qing.siqi@intellif.com>,1
disable signal capture in unit test of paddle frontend (#9809)* disable signal capture in unit test of paddle frontend* code format,1
DNNL-BYOC enhancement (#9797)* add unit test for byoc-dnnl* add byoc-dnnl pattern and their test cases,1
[TensorIR] fix region cover check (#9810),0
[CUTLASS] Refactor cutlass kernel generation and selection (#9800),5
[TIR] For-kind inheritance in decompose-reduction (#9814),5
[Target][BugFix] Convert dict and str to TVM object (#9807)* [Target][BugFix] Convert dict and str to TVM object* Add tests,0
"[M3c][MetaScheduler] Update TuneContext, TaskScheduler & Search Strategy Design (#9789)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Minor fix.Fix mypy.Fix mypy.* Retrigger CI.* Minor fixes.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",0
Fix reduce NCHWc infer layout (do not keep reduced inner c when keepdims=false) (#9821)* Fix reduce NCHWc infer layout (do not keep reduced inner c when keepdims=false)* black* lint,0
[CUTLASS] Residual connection fusion (#9820)* [CUTLASS] Support residual block fusion for conv2dcommit d4a78a3e13530974e852b4c0480b7c8d0f792e68Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:33:41 2021 +0900    fixed residual block check conditioncommit 6ee5a3913333e8ba2d5d0ed6842a58fe37baa547Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:25:04 2021 +0900    minor fixcommit 8af8b3078f11ee293d2e22d9e37e715c617ffb75Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:18:50 2021 +0900    remove SimplifyExpr passcommit 20ae2d874917c69fabc6fcf03a3d47aff98eee91Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:16:46 2021 +0900    fix bad mergecommit 17eed222c5e69e7863c95563b638e5390c634b1bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:13:53 2021 +0900    blackcommit fda151b524cb28581256befa74575bbfa23efa4cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 16:09:45 2021 +0900    Support residual block fusioncommit ce9d52fd629d6119abdd471b00ff6a79223d6752Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 15:56:32 2021 +0900    Remove SimplifyExpr pass from the pipeline (makes DETR result nan)commit d3b681d95977b6fc0965a0a3ec8af3f866bd9e91Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 15:47:07 2021 +0900    fix no_beta_scaling valuescommit 87b36dbbb11adb582ffb628fc6ad62668dcdee7eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 14:59:40 2021 +0900    fill in TODO doccommit fd67595831c7b8741f30577bc91488bcce34a76aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Dec 23 14:31:06 2021 +0900    Refactor cutlass kernel generation and selection* do not try to support broadcast binary op* add comments* remove residual input shape check,0
Fix zephyr/test_zephyr_armv7m (#9684),0
[CI][Caffe Frontend] Change the caffe deps into SSD distribution (#9060)* Change the caffe deps into SSD distribution* update make flag* remove `rm -rf /var/lib/apt/lists/*`* install all python packages in one pip command* install latest package version* add caffe-frontend dependencies,1
[AMP][Pass][Typing] Add faster type inference (#9735)* reuse checked types* analogous subgraph* brr go fast* clean up src logs* clean up PR more* more clean up* more documenetation* clean up* formatting* rename fast --> local* more ocmments* jostle ci* type inference* change comment for SameTypedSubgraphExtractor* get_analogous_expression -> GetAnalogousExpression* comment in GetAnaalogousExpression* add comment* replace infer tests* jostle,1
Add Python representation for VirtualDevice (#9812)* Add Python representation for VirtualDeviceThis adds a Python class to represent the VirtualDevice so that thebehaviour for `device_type()` can be semi-replicated.These tests were actually not being ran and were broken so I've addedthem to the integration script.* Update other references to make_virtual_device,1
[TVMC] Split common tvmc file into more specific files (#9529)This follows from #9206 and splits common.py into multiple smaller and more focussed files.,2
[CMSIS-NN] Conv2D with equal paddings can be mapped to CMSIS-NN target (#9801),1
Update required cmake version in docs. (#9484)* The version used in CI is the minimum version. * docker/bash.sh ci_gpu cmake --version prints       cmake version 3.10.2 * Found in #9239,1
"[BugFix] shapeOfAttrs should be registered before ""vm.shape_of"" used (#9669)* [BugFix] shapeOfAttrs should be registered before ""vm.shape_of"" used[BugFix] DialectRewriter should not tranform scaler to let expr* retry tests* retry tests again* optimzie and add unit test* retriger test* add comment* fix lint",0
[Community] Bohan -> Committer (#9833)* [Community] Bohan -> Committer* Update CONTRIBUTORS.mdCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
"Calculate CMSIS-NN buffer size with respect to architecture extensions (#9338)This correctly calculates the buffer sizes for a variety of targetsbased on the `-mcpu` and `-mattr` flags passed to the `cmsis-nn` codegenerator.Added for Conv2d, Depthwise Conv2d and Average Pool.",1
"[microNPU][2a] Add CascaderGraph for cascading analysis (#9469)A CascaderGraph augments a TE graph with additionalinformation needed by the cascading algorithms. Thisincludes defining a strict ordering on the operatorsas well as including all the Propagators needed todo the affine analysis of cascades.The CascaderGraph consists of two object types, Partsand Tensors. A Part is an augmented operator whichincludes the Propagators and a Tensor is similar to aTE tensor but stores additional information likecompression ratio.",1
[Docker] Pin sphinx version to workaround sphinx-gallery bug (#9822)* [Docker] Pin sphinx version to workaround sphinx-gallery bug* also pin xgboost to suppress a warning from coremltools,0
"[M3c][MetaScheduler] Add ReplayFunc Search Strategy. (#9799)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Add ReplayFunc and EvolutionarySearch strategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix optional task name.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Remove extra files.* Fix things.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",0
Usability fixes to CI runner script (#9752)* Usability fixes to CI runner script* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,0
Add skip to flaky MacOS RPC test (#9753)* Add skip to flaky MacOS RPC test* Use flaky marker instead* link issue* trigger ci* trigger ciCo-authored-by: driazati <driazati@users.noreply.github.com>,1
This patch is to fix some minor typos in project. (#9852),0
[BugFix] resolve integer 32. ~ 64. mismatch by casting (#9582),0
[Torch] Better support in-place variant of ops (aten::relu_ etc) (#9851)* [Torch] Better support in-place variant of ops (aten::relu_ etc)* add warning* black,1
"[M3c][MetaScheduler] Add EvolutionarySearch Search Strategy. (#9836)* Modify TuneContext, TaskScheduler & SearchStrategy functions.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Add ReplayFunc and EvolutionarySearch strategy.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix optional task name.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Remove extra files.* Fix things.* Add evolutionary search.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>",0
"Generate compile_commands.json by default (#9763)* Generate compile_commands.json by defaultThis is low overhead (it only affects the make-generation step, not the actual build) and makes developer tooling like clangd work much better, so we should have it on by default instead of off (which is CMake's default). If necessary, it can be disabled with```bashcmake -DCMAKE_EXPORT_COMPILE_COMMANDS=0 ...```* Add skip when run in a subproject* Update CMakeLists.txtCo-authored-by: Andrew Reusch <areusch@gmail.com>Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>",1
[AMP] Register some new ops (#9849)* add new mixed precision ops* remove space* Update python/tvm/relay/transform/mixed_precision.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,1
[AMP] Fix IsMixedPrecisionType Edge Case (#9856)* fix ismixedprecisiontype* fix test* jostle* Update tests/python/relay/test_to_mixed_precision.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>,0
[TensorRT] Fix pad_value access (removed from PadAttrs) (#9858),0
[MetaSchedule] XGB-based Cost Model (#9859)* [MetaSchedule] XGB-based Cost ModelCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Fix lint* fix doc* fix mypyCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
Add Reference System to tvm.ci_qemu (#9853)This is added for upstreaming the CMSIS-NN demo from TVMCon which usesboth Zephyr and the reference system so I need an image with both to runit in CI.,1
Add FreeRTOS dependencies to ci_qemu (#9854)This enables us to build demo applications which use FreeRTOS,1
"[TIR]Show meaningful message when input shape size mismatch with expected size. (#9863)[Issue]When the transform function get a input data with incorrect shapesize, the error message for example ""src_shape.size() == src_axis.size()(6 vs. 4)"" will be printed out, but the said message not provide enoughinformation to help trouble shooting.[Solution]Add more meaningful error message.Co-authored-by: hua jiang <hua.jiang@xilinx.com>",0
Use shallow clone (#9864)to save bandwidth and docker image size,5
[LLVM] LLVM codegen debug utilities (#9857)* [LLVM] Set LLVM IR names to match TIR variable names* [LLVM] Added CreatePrintf and CreateLookupReturnAddress utilities.,0
"[microNPU][2b] Create CascaderGraphs from TE graphs (#9471)The first step in the cascader is to create aCascaderGraph from a TE graph. To do this, everyoperator in the TE graph must get 'matched' by aPart matcher. This converts TE operations intocascader Parts by augmenting them with Propagators.In this initial commit, we include basic Partmatchers for ethosu_conv2d and some transformoperators so that the graph creation can be tested.",3
"[TOPI] Support grouped conv1d (#9832)* [TOPI] Support grouped conv1dGeneralize the conv2d compute statement to a generic convNd thatsupports any layout and groups. Replace some existing conv2d and conv1dcompute statements with this generic compute. Also add a topigroup_conv1d compute that uses the generic convNd compute. Existingschedules for conv1d work with group_conv1d, so they are reused.* permute reduction axis order* formatting",1
[CI] Update to PyTorch v1.10 in GPU image (#9866)* apply PT vs LLVM symbol conflict mitigation* update ci-gpu to v0.79 with PT 1.10.1* disable quantized mv3 test due to weird segfault from torch,1
[MetaSchedule] Add Per-Store-Feature (#9860)* [MetaSchedule] Add Per-Store-FeatureCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* fix lint* fix lint* Update per_store_feature.py* address comments* fix lintCo-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,0
Fixed deprecation warning issue for pipeline executor. (#9770),0
[FIX] Fix bug in MobileNetV2 quantization (#8243)* fix test* fix bug* fix pylint* fix pylintCo-authored-by: wangyucheng <wangyucheng@sensetime.com>,0
[FQ2I] Support Conv2dTranspose FQ2I (#9347)* fix a lot of initial tests* make pytorch tests pass* lint* add test* fix bug with layout transform* change layouts for conv2d_transpose too* fix vitis tests* fix qnn conv2d transpose tests* fix fake quantization pass* add todo* lint* undo just formatting changes* remove formatting only change* remove f2qi for later pr* more frontend tests fixes* fix things* cool keras fix,0
[TESTS] Fix running tests without MICRO (#9867),0
"[PROFILING] Add ability to profile a single function_profiling (#9553)* [PROFILING] Add ability to profile a single function_profilingAdd a new function `tvm.runtime.profiling.profile_function` whichcollects performance metrics for a single function in an IRModule. Forexample, collecting performance counters using `PAPIMetricCollector`.This is helpful for optimizing kernels and schedules for a singleoperator.* fix docs* configurable number of warmup iterations. avoid allocating when stopping collectors",0
[Rust] Update Rust bindings (#9808)* Update Rust bindings* fmtCo-authored-by: AD1024 <dh63@cs.washington.edu>,1
remove downloaded source code archive (#9879),4
Change rust installation profile to minimal (#9878),2
"[TOPI] Print shape information when the input shape not compatible with (#9876)reshaped shape.[Issue]When the input shape not compatible with reshaped shape and not dynamicin topi reshape, the two shape sum value will get printed out as an errormessage, but such shape sum value is not helpful for the trouble shooting.[Solution]Print the shape detail, user can use such information to located relatedoperator for trouble shooting.",0
fix convert_pooling in caffe parser (#9828),0
Remove zephyr installer file after installation (#9883),2
[VM] Remove undesired arg to load_late_bound_consts (#9870)* Remove undesired arg to vm exec load_late_bound_consts* No-op for ci,4
[CMSIS-NN] Fixed the network hash to avoid type inference failure (#9887)Co-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>,0
te_compiler_cache: reduce name length without loss of information (#9787)This can mitigate issues described in #8953 without increasing memory requirements,5
Add support for aten::dot (#9893)* Add support for aten::dotThis implements dot product as a composite of of multiply + sum* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"Add a JSON converter for 0.7 -> 0.8 and 0.8 -> 0.9 (#9874)* Add default to serialization* revert changes in serialization.cc* update 0.6 converter* json updater working, except for cycles* clean up code* Fix tests* formatting* format:* nit",0
"[Docker] Update onnx to 1.10.2, ORT to 1.9 (#9882)* [Docker] Update onnx to 1.10.2, ORT to 1.9* restore disabled testsThis reverts commit b29a4438e217ed84f7ee8c40254756c604d90fa3.",1
add oneflow dependency in docker file (#9881)* add oneflow ci depend* fix comment,0
"Combine unit and integration test steps into one stage (#9733)This removes the barrier wait between test and integration tests in CI. This will increase capacity requirements and usage but, assuming we can meet that with autoscaler, should reduce CI times by an hour or two since we're doing all the testing in parallel.The slow path is CPU unit test -> GPU frontend tests, so kicking off the GPU frontend tests faster should help decrease CI runtime.Co-authored-by: driazati <driazati@users.noreply.github.com>",3
[ONNX] Use relay softmax op to convert Softmax if posssible (#9892),5
[CMSIS-NN] Support for asymmetric padding in Convolutions (#9886),1
Update CI to use tlcpack/ci-cpu:v0.80 (#9865),1
Add aten::mv support (#9894)* Rebase* Fix bad mergeCo-authored-by: driazati <driazati@users.noreply.github.com>,0
"[BUGFIX] Check that virtual device is unchanged in WithFields (#9826)* Add default to serialization* revert changes in serialization.cc* update 0.6 converter* json updater working, except for cycles* clean up code* Fix tests* formatting* format:* Check that virtual id is unchanged in WithFields* Set virtual_device_ to fully unconstrained in ctor* visit virtual device in the attr visitorFix serialization tests* Fix tests after bad merge* Change virtual_device() getter method* lint* ci failed* ci was broken",0
"Add sliding_window operator (#9816)* Add windows operator* remove TODO* Convert ICHECKs to CHECKs* Report errors using diagnostic context* Use more readable CHECKs* Remove example; move comments to test* Revert ""Remove example; move comments to test""This is a partial revert.This reverts commit c810c2db7637ce9537adc49d1016caddd5093d3a.* Add newline to fix Sphinx error* windows -> sliding_window* whitespace* fmt",0
"[FIX,TOPI] Fix issue when running conv2d in autoscheduler (#9900)conv_nchwc scheduling was missing a check on the type on inputs beforeit tried to apply pragmas to them.",0
[microNPU] Enable the codegen tests for 256 mac Arm(R) Ethos(TM)-U65 NPU (#9815)This patch has necessary changes to the Makefile and testinfra to run the FVP based codegen tests on a newer NPU variant.,1
Implement [skip ci] for Jenkins (#9554)* Rebase* Fix missing skipCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[TE] Support negative indices  (#9023)* initial change* more explicit api* switch to select* add support for negative indices* reduce things further* lint* to CamelCase* unit testCo-authored-by: Andrew Zhao Luo <andrewzhaoluo@system76-pc.localdomain>,1
[community] @AndrewZhaoLuo -> Committer (#9896)* Update CONTRIBUTORS.md* Update CONTRIBUTORS.md,1
[FoldScaleAxis] Support dense and bias_add op in fold scale axis (#9838)* [FoldScaleAxis] Support dense and bias_add op in fold scale axis* fix lint,0
[Caffe Frontend] Add support for Permute layer (#9157)* Add support for Permute layer* Add test for Permute layer* Fix alignment,0
[Caffe Frontend] adding Reduction op (#8015)* [Caffe Frontend] adding Reduction op* reformatting Reduction op test script* reformatting Reduction test script* [Caffe frontend] Reduction op- adding more test cases; handling '0 < axis < num_axes - 1' case to give the result equivalent to Caffe framework- skipping Relay multiplication if coeff is 1Signed-off-by: zotanika <zotanika@gmail.com>* linting test script* linting* typo fixCo-authored-by: sunchul.jung <sunchul.jung@samsung.com>,0
"[CUTLASS] Support more kernels: int8, tf32, and 3xtf32 (#9899)* add int8 type in library* wip* adding test and plumbing data and weight dtype* adding 3xtf32 support and refactor tile description enum* add 3xtf32 test* update gemm generator too* int8 test worked* 3xtf32 also works* int8 and 3xtf32 gemm works* clean up test* support int8 in sm75* refined int8 alignment constraints* black* support 3xtf32 in default kernel* remove log* refine dtype check* support tf32* leave TODO for alignment modification on int8 kernels* tf32 test working* fix default kernel for tf32* workaround for compilation failure* lint",0
Improve the frontend tflite _test_rsqrt test to support tflite 2.6 (#9888)Updated the test quantized graph creation to support tflite 2.6,1
[Object] Throw AttributeError if the object doesn't have a reflection table (#9919),0
min/max support (#9918),5
[Hexagon] Include Utils.cmake for tvm_file_glob used in HexagonSDK.cmake (#9903)Don't include it from HexagonSDK.cmake because the Hexagon SDK file canbe included from different directories.,2
fix pytorch frontend bug (#9884)* fix pytorch frontend bug* update* updateCo-authored-by: zhaojinxi <zhao.jinxi@intellif.com>,0
change install version of gpu to cpu (#9922),2
Run extract constants pass only for CMSIS-NN target (#9913)-Added checks for CMSIS-NN target in the pass extract_constant-Added a negative test with nested functions mapped to a combination of cmsis-nn and other compiler,1
[Relay] Add printer for op strategy objects (#9923),1
Add API `get_input_info` to graph_executor (#9889),1
[Fix Bug] fix the bug of pool_impl_nd when computing avgpool_nd whith ceil_mode and count_include_pad are True. (#9835)* Added the offset[i] for getting the correct  boundary* Added corresponding test case,0
[skip ci] Fix missing pack_lib in Jenkinsfile (#9924)This was inadvertently removed by #9554Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[Hexagon] Pass SDK information to launcher build for Android (#9902),4
std::string -> tvm::String for Conv1DAttrs (#9921)This is necessary to make the Rust bindings work.,5
[TIR][Schedule] Annotate allows array as annotaton value (#9920)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,5
fix icelake target for avx512 and vnni (#9928),0
Fix HexagonSDK.cmake (#9914),0
"Restore the use of ONNX_DEFAULT_CONFIGS[""use_nt_batch_matmul""] (#9925)",5
[CI] Fix pip cache config bug (#9933)* Update Dockerfile.ci_arm* Update Dockerfile.ci_cpu* Update Dockerfile.ci_gpu* Update Dockerfile.ci_i386* Update Dockerfile.ci_lint* Update Dockerfile.ci_qemu* Update Dockerfile.ci_wasm,0
dynamic to static use infer_type_local (#9869),5
"[USMP] Hill Climb allocator (#9704)* [USMP] Hill Climb allocatorThis PR adds HillClimb allocator ""tir.usmp.algo.hill_climb""to the memory allocation algorithm set.Change-Id: Ib7485df93757eb512da040528ec86c920db8d03b* requested changesChange-Id: I6700a24c1608d92f87be7dde33cc24f5de1f7063* Conda-related linter small fixesChange-Id: I0dac5c6d75ade8f813b077c8708aad59d2722933* Moved implementation from greedy.h to greedy.ccChange-Id: If8ed159eceef32d3f22b51e0252161d09222eb1e* Integrated into test_tir_usmp_algo.py unit testAdded ""hill_climb"" into test_tir_usmp_algo.pyAmended sorting to be consistent with ""greedy"" familyChange-Id: I8e9f5282f15baaab71d6d129aeb9643376b14763",0
[TEST] Remove `llvm -device=arm_cpu` and `cuda -libs=cudnn` from (#9905)default list,3
[Relay/Frontend][TFLite] Change the output shape calculation based on keep_dim option in fully connected (#9840)* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Support -> Change the output shape calculation based on keep_dim option* Change the output shape calculation based on keep_dim option in fully connected* TODO : Need to construct a fc op with (keep_num_dims == True)* TODO : Need to construct a fc op with (keep_num_dims == True),4
[TIR] Encode conditional accesses info into block read/write regions (#9880)* encode conditional accesses info into block read/write regions* compare ir after simplify,5
[Int8] Support cublas on e2e int8 models (also tried cudnn but doesn't work) (#9898)* fixed int8 dense offload for cublas* support OHWI kernel layout in qnn.conv2d* fixed reduction axis* add cublas int8 qnn test* lint,0
remove clang compile warnings (#9942),4
[ONNX] Fix onnx convtranspose error (#9938)* fix mix up of channels with conv2d-transpose* add grouped convtranspose tests* turn off groups for non-llvm test,0
"[Fix] relay onnx frontend bug when [A, B, M, N] * [1, B, N, K] (#9911)* [Fix] relay onnx frontend bug when [A, B, M, N] * [1, B, N, K]* fix lineCo-authored-by: tomoyazhang <tomoyazhang@tencent.com>",0
"[Caffe Frontend] supporting group > 1 cases for Deconv op (#8260)* [Caffe Frontend] supporting group > 1 cases for Deconv op- Handling group > 1 cases, assuming group == output channels- Simply decomposed into Relay split, conv2d_transposed, and multi-leveled concatenate ops- Added some test casesSigned-off-by: zotanika <zotanika@gmail.com>* [Caffe Frontend] amending a test case for Deconv opSigned-off-by: zotanika <zotanika@gmail.com>* explicit importing tvm.testing* changing split axis to 0, according to PR #9336",1
"[Caffe Frontend] extending Eltwise to handle multiple inputs (#8136)* [Caffe Frontend] adding Reduction op* reformatting Reduction op test script* reformatting Reduction test script* [Caffe frontend] Reduction op- adding more test cases; handling '0 < axis < num_axes - 1' case to give the result equivalent to Caffe framework- skipping Relay multiplication if coeff is 1Signed-off-by: zotanika <zotanika@gmail.com>* linting test script* linting* [Caffe Frontend] Supporting multiple grouped(channel-wise) Deconv op* Handling group > 1 cases, assuming group == output channels* Decomposed into Relay split, transposed conv, and multi-leveled concatenation.* Added some test cases.Signed-off-by: zotanika <zotanika@gmail.com>* [Caffe Frontend] supporting variable number of inputs for Eltwise* extra handling of rest inputs for PROD, SUM, MAX operations* extra testcasesSigned-off-by: zotanika <zotanika@gmail.com>* formatting fix* [Caffe Frontend] reverting codes related Reduction for splitting PR* Revert ""[Caffe Frontend] Supporting multiple grouped(channel-wise) Deconv op""This reverts commit 43e25e552b790ce9a38fdbcfb3ddf2075c253e20.* instant fix against docker format error* instant fix against docker format error* instant fix against docker format error",0
[MetaSchedule] Schedule Rule: Auto Inline (#9943)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"[microNPU] Remove remaining UnsupportedLayout checks (#9791)* [microNPU] Remove remaining UnsupportedLayout checksIn #9508 the decision was made to remove the UnsupportedLayout exceptionand the checks that throw it, this PR is cleaning up some that remained.Change-Id: I83bfe233381b83af886343c9569db753e33f9059* fix lintChange-Id: I67c1a5371f0b2e51b6cd39435ef4073d8d17af51",0
[microNPU][2c] Add performance modelling to cascader (#9778)* [microNPU][2c] Initial Performance Model* Added the pre-computed performance modelling per block.* Added the aggregation of cycles given a stripe config.* Implemented the op-specific performance code for conv2d.* Created a DeviceConfig class to hold constant performance related datathat is dependent on the accelerator configuration* Added generation of all valid block configs. This is pre-computed andgiven as an argument when constructing EthosuParts.* Implemented selection of the block config that gives the least amountof data read given a StripeConfig.* Add test guards* Extended block config testing,1
[MetaSchedule] random compute location (#9940)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[MetaSchedule] PostProcessor: Verify GPU Code (#9945),5
"[CUDNN] Refactor descriptor initialization, remove `cudnn.conv.output_shape_from_cudnn` (#9948)* Introduce SetConvdescriptors to refactor cudnn/conv_forward.cc* more refactor* remove cudnn get output* cpplint",4
"[microNPU] Add support for scalar values (#9794)* [microNPU] Add support for scalar valuesPR #9515 enabled support for scalar constants, but didn't consider thecase of a scalar value where the underlying constant data does not havea shape i.e. `constant.shape == []`. See the test case for a visualdifferece when the scalar value is 1.Change-Id: Id7a238cb5bf999dd5a8428c097202f9fb940a5f0* Fix failing test by removing constantBefore this PR scalar constants were handled differently so this testwas able to pass. Now that scalar constants are handled in the samemanner as tensor constants, the test fails since unexpected tir isproduced in the compilation pipeline. Since the relay used in this testcase is not expected to be produced by higher levels of the compiler,removing this constant for now.Change-Id: I4ea5155778809041339e6faac05af3f72c3e3ea5* clean up finding tensor from inputsChange-Id: Ideccf84f8c9149148ff23e2406229cf637c982a3",0
[HotFix] Skip the flaky MetaSchedule Auto-Unroll test (#9956),0
Enable NPU and CMSIS in ci_qemu (#9957)These are required for running the demos under ci_qemu in combination with Zephyr,5
"[Runtime][Pipeline executor] Global parameters group name and runtime modules parameters map. (#9846)* [Runtime][Pipeline executor] Global parameters group name and runtimemodules parameters map.Solution:To support on the fly parameters setting for each runtime modulein pipeline executor, we create a feature that use global parametersgroup name to map the runtime module parameter, after such maprelation get created user can do the on the fly parameters settingby using the parameters group name.trigger build.fix ut issue.polish comments.Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update src/runtime/pipeline/pipeline_executor.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update src/runtime/pipeline/pipeline_struct.hCo-authored-by: Cody Yu <comaniac0422@gmail.com>Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>address review comments.* Update python/tvm/contrib/pipeline_executor.pyCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix plint issue.Co-authored-by: Cody Yu <comaniac0422@gmail.com>",0
[CI] Upgrade ONNX (#9965)* jenkinsfile and one test* formatting* swtich to proper repo for docker* fix missing - with _* jostle* upgrade to latest images* jenkinsfile and one test* formatting* swtich to proper repo for docker* fix missing - with _* upgrade to latest images* jostle ci* update with official images* jostle ci,0
"[TOPI,x86] Improve performance on int8 conv2d on x86 (#9966)Appended fused operations in cov2d for int8 were computed in a separateloop from the main conv2d computation:```for i in ... parallel  for j in ...    accumulator = 0    for k in ..      vectorized_multiply_add(accumulator, data, kernel)    out = accumulator  for k in ..    out = out + fused subsequent ops```This patch moves the fused ops one more loop nesting inwards to get```for i in ... parallel  for j in ...    accumulator = 0    for k in ..      vectorized_multiply_add(accumulator, data, kernel)    out = accumulator + fused subsequent ops```On quantized mobilenetv2, this results in approximately a 30% speedup.",1
"[Hexagon] Return pathlib.Path from get_hexagon_rpc_path() (#9969)Type annotations don't do anything, the type conversion needs to beexplicit.",5
[Hexagon] Add missing #include <iterator> (#9968)This fixes compilation error with libstdc++.,0
[Doc][Fix] Fix qnn op parameters hint order (#9622)As the following parameters are both Expr.```zero_point : tvm.relay.Exprscale : tvm.relay.Expr```It's really get me confused when I followed the python arg hints to create a relay and without success.,0
"Propagate ssh-agent authentication socket (#9926)In case that SSH_AUTH_SOCK is defined, two items will be added to thedocker run command:1) A propageted ssh authentication socket value , to support   an underlying ssh calls inside the running contianer.2) A mounted volume for ssh channel.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>",1
"[TIR][USMP] Integrating USMP to AoT Executor (#9565)This commit integrates USMP with the AoT executor codegen. Additionally, this commit introduces two PassContext options to disable_usmp and disable_storage_rewrite.Moved PrintType from codegen_c.cc to codegen_source_base.cc to be accessible by source_module.ccMoved runtime::metadata to be ExecutorCodegeMetadata as it contains metadata produced by ExecutorCodegen for actual code generation (not a runtime component).",1
Disallow copy to/from external HexagonBuffer (#9930)* Disallow copy to/from external HexagonBuffer* change nbytes -> allocation_nbytes for clarity* retrigger ci,4
[Fix] Fix flaky test of #9952 (#9958)* fix to stablize the var orders when solve bounds in region analysis* change to std::find_if since num of vars is generally small,0
Add contribute page about CI (#9906)* Add contribute page about CIThis adds some docs with a description of the TVM CI and some usage instructions to both make contributing more friendly and educate existing developers about how CI runs. Bikeshedding on content is welcomeNote: The TODOs are blocked on some other PRs and will be done before landing* docker instructions* Comments* RebaseCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[bugfix] Fix the behavior of TVMScript printer (#9974)* upd* lint,0
Add sccache to docker images (#9844),1
"[Relay] Add `conv2d_backward_weight` op (without topi) (#9954)* python plumbing* add cpp def* legalize worked* clean up* layout conversion doesnt work* extract wgrad body* fix convert layout* black* fix kernel size* revert irrelevant change* add doc, clarify the meanings of parameters* update layout convert* test passed* fixed layout conversion* update convert layout* remove print* remove layout convert for now* minor fix* removed unused import* add wgrad python reference* add test stub* add doc* test other stride and pad* tweak* more pylint filter* fix typo in doc* swap arg order (data, grad) to be consistent with conv2d_transpose(dgrad)",0
[MetaSchedule] Schedule Rule: Add RFactor (#9975)* add rfactor* format* fix ci,0
Add Action to add cc'ed people as reviewers (#9934)* Add action to label mergeable PRsDevelopers often have to ping a committer once their PRs are both passing in CI and are approved. This helps facilitate this process by marking such PRs with a label `ready-for-merge` so committers can easily filter for outstanding PRs that need attention.* Fix lint and add tests* Add Action to add cc'ed people as reviewersThis provides a mechanism for non-triager/reviewer/committer PR authors to request reviews through GitHub. Anyone that is referenced by `cc @username` in a PR body will be added as a reviewer (GitHub will limit the reviewers to those with actual permissions to leave reviews so the script to add can be simple).* remove merge bot stuff* Fix target triggersCo-authored-by: driazati <driazati@users.noreply.github.com>,0
Add runtime.ModuleGetFormat method enabling export of BYOC generated sources which require a .cpp/.cc file extension (#9243)* Allow export of C++ kernels using correct file extension* [WIP] Set module_key=c for CSourceCrtMetadataModuleNode to temporarily fix failing testsI realized that the module format `cc` is currently already used by the `CSourceCrtMetadataModuleNode` declared in `src/target/source/source_module.cc`.This needs to be discussed first to decide if either the module_key should be changed or the test cases expecting the systemlib kernel (e.g. `default_lib0.c`) to have a `.c` extension.* Update Makefiles used by tests/python/relay/aot/ to support C++ file extensionsAOT: Add c++ support to aot_test.mkAOT: Add c++ support to corstone300.mk* Add missing definition of GetFormat to cmsisnn and ethosn codegens (WIP)* Resolve PR comments* lint python/tvm/runtime/module.py* fix EthosUModuleNode for CI* Fix: detect empty module.format* Add error message to assertion* Lint python/tvm/runtime/module.py,0
"Tvmc python tutorial (#9633)* finish rpc and shape_dict in tut* added more to rpc* tutorial edits* added tutorial to docs in howto* accidentally had two copies of tutorial* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Apply suggestions from code reviewCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* Update gallery/how_to/use_tvms_python_api/tvmc_python.pyCo-authored-by: Leandro Nunes <leandro.nunes@arm.com>* added Leandro's suggestions* added example model at top* added example model, blacked it* trying to get docs to build* underline too short for title* forgot Jetson info, added Chris H comments* reformatting text* black* hitting code block issue, trying to debug* added spaces after the python codeblock* black* changing formatting* touching up more edits'* more touchups* changed location of file to tutorial section* changing doc location* broke the order of the docs somehow* fixed it yayy* added additional indentation* black'dCo-authored-by: CircleSpin <jocelyn@pop-os.localdomain>Co-authored-by: Leandro Nunes <leandro.nunes@arm.com>",0
[CMSIS-NN] Separated symmetric and asymmetric padding tests for Conv2D (#9963),1
"[microNPU][2d] Add more Part matchers to cascader (#9785)* [microNPU][2d] Add more Part matchers for the cascaderAdds Part matchers for ethosu_depthwise_conv2d,ethosu_pooling and ethosu_binary_elementwise. Alsoadds additional testing for the CascaderGraphcreation.Co-authored-by: Jacob Bohlin <jacob.bohlin@arm.com>* Extended testing for block config* Add test guardsCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>",1
[CI] hot fix Sphinx (#9998),0
"[microNPU] Move optimization passes to be a module pass and ensure they (#9831)are runningMoves LayoutOptimizer and LUTOptimizer passes to be a module pass,rather than a function pass. This is because it was found that thesepasses were not running in the NPU compilation flow. In addition, atest for both LayoutOptimizer and LUTOptimizer has been added to checkthat the passes are running in the compilation pipeline of the NPU.Change-Id: I5145c6f02eeb0daea3cdba56198e0804ec32f351",1
[CI] Fix Rust path and remove wasmtime from ci_qemu (#10001)This fixes the Rust path to ensure `cargo` is accessible in thecontainer.As ci_qemu is targetted at microTVM it likely won't make use of wasmtimeas a dependency - it's used instead for the JS and WASM standalone applications asfar as I can see.,0
"[Relay] Fix a bug in tensor_array_scatter (#6890)* [Relay] Fix a bug in tensor_array_scatter  tensor_array_scatter constructs helper functions according to dtype  and shape of element. When there are multiple scatter operations with  same dtype and element shape but different indicies_shape, there will  be name conflict in prelude.* Refine get_name",0
[Minor] Typo Fixes (#10000)* Fix typos.* Missed funtion -> function.,0
Make cc bot skip errors (#9988)* [skip ci] Make cc bot skip errorsThis adds some better logging and ignores errors (this job shouldn't ever show up as a PR failure) so we can diagnose things like https://github.com/apache/tvm/runs/4873810315?check_suite_focus=true* Submit cc'ed reviewers one at a timeCo-authored-by: driazati <driazati@users.noreply.github.com>,0
Don't use std::move in WithFields (#10009)* Don't use std::move in WithFields* lint,4
[LLVM][Hexagon] Revert LLVM header change for version 14 (#10006)* Revert LLVM header change* Trigger,4
[MetaSchedule] Post Processor: Rewrite Reduction Block (#10013)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"[frontend][keras] Add support for TimeDistributed (#7006)* First pass on modifying Keras importer to handle TimeDistributed* Use squeeze inside TimeDistributed, add tests* linter fixes* More linting* Even more linting* Fix unused argument annotations* Forgot one pylint annotation* Forgot to set up data layout in _convert_activation* Decouple data_layout from etab* Linting fix* Forgot to set data_layout argument* Missed an etab.data_format, also test_conv1d was not in the test file's main* Rebase fixes* Linting fix* _convert_lambda needs a data layout argument too* linting fix too* Lint the test file too* Redundant variables* Simplify further* Another simplificationCo-authored-by: Steven Lyubomirsky <slyubomirsky@octoml.ai>",0
"Auto-discover C/C++ compiler instead of hardcoding g++ (#10007)Some platforms (e.g. FreeBSD) use clang as the default OS compiler,and there is no g++.",2
[Docker] Relax name check (#10011)Fix a issue that user name like aaa.bb can't be added to docker container,0
[MetaSchedule] Schedule Rule: Cross Thread Reduction (#9994)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"[TOPI,CUDA] Don't enable cudnn conv2d kernel if is not supported (#10021)* [TOPI,CUDA] Don't enable cudnn conv2d kernel if is not supportedSpecifically, check that layout is not NCHW if datatype is int8.* remove all conv2d_cudnn int8 support",4
[BugFix][TIR] Fix cross-thread reduction when single reduction loop with predicate (#10016),0
"Add user-configurable backtrace limit (#10025)A spin off of #9872, this adds an env variable `TVM_BACKTRACE_LIMIT` which can be set to an integer to limit the frames printed out on errors. This can make it easier to run interactive TVM scripts with errors since the stack traces are often long (70+ frames).```bashexport TVM_BACKTRACE_LIMIT=5python some_code_with_an_error.py```cc @tkonoligeCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[MetaSchedule] disallow_dynamic_loop (#9997)* [MetaSchedule] disallow_dynamic_loopCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Update src/meta_schedule/postproc/disallow_dynamic_loop.ccCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,1
"[CUDNN] Support gradient kernels (#9986)* Dgrad nchw, nhwc, fp16 workingcommit 426e5dca446a27da49270f45171b58f1bfa21fa9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:48:53 2022 +0900    blackcommit 211a58b80f4d0f0b5b0230720e41f35e50cb1eafAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:43:52 2022 +0900    fp16 also workscommit c2a34d473b063873628bff00e51a44cd8e4d0e4fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:36:36 2022 +0900    nhwc test also workedcommit c0609ab147fef30c230a94d16b6c1ba35f7dd9c0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 11:21:23 2022 +0900    nchw test workedcommit 2bf68c72763708151e9f49f09916a210b2547be8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 10:41:35 2022 +0900    add test stubcommit c86b1288d5e371f12cba4e1b1866966cb9264401Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 10:32:09 2022 +0900    add python definition stubcommit 3166952f9673376801bf4b5b39eeb6f89452f30aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:57:18 2022 +0900    bwd filter compiledcommit e311ba3d05c5f9424ecb952cb5a520ce81a0828aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:27:55 2022 +0900    dgrad compiledcommit 47f35beb5eeeb7cbf9f6ec7cf8f5c80c65e8da46Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Jan 18 06:16:43 2022 +0900    add dgrad stubcommit ebed032d15b1c3895f541c46ce5d80b6dd769034Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 17:01:56 2022 +0900    cpplintcommit 834f54a8c13512130e7d91ca0f54268dc06c5481Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 16:55:58 2022 +0900    remove cudnn get outputcommit dcbd9c95fdb8ffef9db9c2350430b270461a31c3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 16:28:07 2022 +0900    more refactorcommit 146464e8496fff972bdb1687c4e9d432fe3278d5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Jan 17 15:57:35 2022 +0900    Introduce SetConvdescriptors to refactor cudnn/conv_forward.cc* add python function for cudnn wgrad* adding wgrad test* black* wgrad nchw and nhwc worked* remove bwd algo name stuff* compute output shape properly* swap arg order in wgrad* add kernel size arg in test* black* cleanup* more fix* fix dgrad test* support running relay conv2d_backward_weight directly with cudnn* black* refactor reference function to support nhwc* removed unused function* lint* enable offloading conv2d_transpose to cudnn dgrad* relax tol* name fix, remove print",0
[Flaky] Skip test_qlinear_average_pool (#10030),3
[MetaSchedule] Mutator: Mutate compute location (#10028)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[MetaSchedule] Post Processor: Rewrite Unbound Block (#10027)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[MetaSchedule] Schedule Rule: Parallelize-Vectorize-Unroll (#10033)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"[microNPU] Add support for requantize (#9910)* [microNPU] Add support for requantizeAdds support for stand-alone requantize operation which is legalized toan identity operation on the NPU.Change-Id: Ie2450c5fc72f405eddf517593236074aa4716c3b* fix concatenate tests failing due to not being bit exactSince requantize is now offloaded, concatenate tests were failingdue a reference not being used.Change-Id: I44b26b5daecfefb776ca19e6646f3690f5570f52* test multiple requantize offloadChange-Id: I60a3283461a7a7083c05289e84f570698388077b* address commentsChange-Id: I7196a0fa468eb7c6a96f2b8a68f3a2dcf5a5693c",0
[PTX-MMA] Add full PTX MMA code generation support (#9909),1
[CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN (#9979)* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NNChange-Id: I6a3ba9db3e3cb2bd7c10383ebd52f9a1cdad74d0* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN* Addressing commentsChange-Id: I98fcdf95bf408700968827e1abd084a916b3b21c* [CMSIS-NN] Update microNPU demo to include offloading to CMSIS-NN    * Addressing comments    * Remove build folder before running demo to address #10020Change-Id: Ifa7ad3ff431f427f8afb8b3c9f06711b3b59ad62* Correctly filter tvmc TargetsFixed logic to check for >2 TVM Target to be based on none-hybridTargets onlyCo-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>,0
[QNN] Add qnn.rsqrt op (#9982)* Add qnn.rsqrt op* Add comment,1
"[Hexagon] Do not auto-build apps when building TVM (#9970)* [Hexagon] Do not auto-build apps when building TVMThe Hexagon cmakes have recently become unwieldy due to a complexnetwork of dependencies between various automatically built components.This was in large part because of trying to automatically build someapps, which then tried to build TVM runtimes again, but with theirown configurations.This patch removes the ability to automatically build any Hexagon--related apps from the main TVM build. The following cmake optionsare now deprecated:  - `USE_HEXAGON_LAUNCHER`  - `USE_HEXAGON_PROXY_RPC`In order to build the binaries needed for HexagonLauncher fromtvm.contrib.hexagon:  - Build TVM+runtime for x86, with codegen for Hexagon enabled.    This can be done via `USE_HEXAGON_DEVICE=sim` or `target`.  - Build Android runtime and tvm_rpc with `-DUSE_RPC=ON`,    `-DUSE_CPP_RPC=ON`, and `-DUSE_HEXAGON_RPC=ON`.  - Build Hexagon runtime with `-DUSE_HEXAGON_RPC=ON`, and    `-DBUILD_STATIC_RUNTIME=ON`.* Add README.md* Restart CI* Add optional variable to set output directory",1
"[Runtime][PipelineExecutor] Add Pipeline Executor Interface (#10010)Adding interfaces into Pipeline Executor to ""run"", ""stop"",""set input"",and ""get input"" from the pipeline executor,In this patch, we also implemented the ""BackendRuntime"" structure towrap the graph runtime interface in order to support  pipeline executorinterface and implement data copy method. This method is used totransfer data between two backend runtimes.",1
"[skip ci][Docker, CI] Update DGL installation, temp disable DGL tutorial (#10067)",1
[CUTLASS] Profile only the largest-possible alignment by default (#10036)* introduce profile_all_alignments option* add profile_all_alignment option to API* wip* fixed dynamic case* black* update gen_gemm too* minor improvement* fix* all tests work* add doc* fixed for sm = 75 case* fix typo* remove unused import* profile_all -> find_first_valid* fix,0
[Meta Schedule] Add `ApplyHisotryBest` Meta Schedule Context (#10049)* Add ApplyHisotryBest.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>* Retrigger CI.* Update integration.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>,1
[MetaSchedule] Mutator Rule: Mutate Unroll (#10045)* mutate-unroll* mutate-unroll,5
[TIR][Schedule] Blockize and Tensorize (#9871)* WIP* WIP* WIP* test cases* add examples* lint* Amend co-authors informationCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>* WIP* address comments and changed tensorized comparator* update* nit* fix example* lint* lint* lint* remove unused* trigger ci* clang-format* fix* rebaseCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,0
[microTVM][tutorial] Add ENV variable to enable testing on physical hardware (#9993)* Add env variable to micro tflite tutorial* Address @gromero comments* address @areusch comment* fix scope* trigger* trigger,0
"[microNPU] Refactor base address determination to codegen (#9929)This commit introduces BaseAddress ObjectRef to determinebase addresses in the codegen for microNPU. This isrequired when multiple memory pools become available. Thus,base addresses could not be statically determined in thesource module.",1
Add FP requantize flow. Set float32 flow by default for llvm x86 targets with (#9637)sse4.1 support,1
[Relay][DefuseOps pass] bug fix: To support function body types other than call node (#10069)Co-authored-by: pranav jonnalagadda-SJ1 Eng_ML <pjonnalagadd@sj1mach1.caveonetworks.com>,0
[Fix Bug]fix the bug of tensorflow frontend when parsing Range layer (#9999)Co-authored-by: wangjiuyang <wang.jiuyang@intellif.com>,0
[MetaSchedule][M4a] Schedule Rule: Multi-Level-Tiling (#10043)* multi level tiling* remove tensor core related code* pylint* fixCo-authored-by: Junru Shao <junrushao1994@gmail.com>,0
"Revert ""[Frontend] Add Span filling for frontends to Relay (#9723)"" (#10072)Because of the failure of LSTM conversion from Pytorch",1
"Improve the tensorflow frontend _test_spop_resource_variables to support tensoflow 2.6 (#9978)On tensorflow 2.4 the test is expected to fail as the generated graph is not forzen.On tensorflow 2.6 the generated graph is identified as frozen, therefore the test is not needed",3
[MetaSchedule] postproc: rewrite_parallel_vectorize_unroll (#10071)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
"Clear warnings when building with MSVC. (#10059)* Fix warning ""unsafe mix of type 'const int64_t' and type 'bool' inoperation"" occurring in tvm::tir::HasAnn* Suppress warning ""destructor never returns, potential memory leak""occurring in tvm::runtime::detail::LogFatal::~LogFatal",0
"[Makefile] Fixed error in ""make clean"" (#10048)The top-level makefile should delegate `make clean` to the cmakefolder of each enabled build, similar to the existing delegation of`make all` and `make runtime`.",0
"[Relay] QLinearMatMul allows 1D weight_scale, weight_zero_point inputs (#10047)* fix after cr* fix after cr 2* emptycommit* emptycommit 2nd try",0
"Don't explicitly link libgcc.a into libtvm_runtime.so on Android (#10052)Setting Android toolchain via CMAKE_TOOLCHAIN_FILE also causes necessaryflags to be added. Also, newer versions of the Android NDK no longer shiplibgcc.a, so this takes care of that as well.",1
"Change function constructors to WithFields (#9690)* Change function constructors to WithFieldsGet rid of std::moves, they were causing problems* Fix bad rebase* flaky* try to trigger ci* try again",0
Document missing qnn operators (#10077)The following qnn operators were missing from the relay documentation.,5
Add temp git dir to test_cc_reviewers test case (#10058)This decouples the test_cc_reviewers test case from the user's git configuration. The implementation reuses the TempGit structure from test_skip_ci to always use a fresh git environment.,1
"[CI] Fix Rust permissions for wasmtime and sccache (#10015)Previously this was ran as part of `ubuntu_install_rust.sh`, as we now have multiple scripts which write as the container build user we have to fix up each time to ensure future users don't break.",0
"[EZ][Typo] Correct gather, scatter type rel error message (#10023)",0
[microTVM][tvmc] Add TVMC Micro tutorial for Zephyr (#10024),1
"[CI][Fix] Remove additional qnn.op.transpose_conv2d from docs (#10083)Fixes CI after #10077, and replaces misuse elsewhere.Change-Id: I095fc8ea2b8d268b09538832cba1f5482a73a9d9",0
[PyTorch] Fix rsub type (#10090)* [PyTorch] Fix rsub type* fix,0
"[microNPU] Removing constant args from PrimFunc (#9951)Before this commit, microNPU creates PrimFunc as ifit accepts constants from the callee. This commitchanges the PrimFunc to remove the constants as anargument to PrimFunc as they are not provided fromthe main function.",4
[Relay] fix incorrect binding of Lets in ANF conversion (#10078)* fix incorrect binding of lets in ANF conversion* add test case* remove really weird auto-import from debugging* address comments,0
[microTVM] Update Zephyr to 2.7 (#10094)This supports the reference system added in #9853,1
"[Runtime][PipelineExecutor] Pipeline Executor Sequential execution (#10082)* [Runtime][PipelineExecutor] Pipeline Executor Sequential executionIn the first, adding the ""get output"" logic. Secondly, adding the the sequential executinglogic of pipeline executor. In the last, testing the pipeline executor interface andchecking the output data.* Address review comments.Co-authored-by: Cody Yu <comaniac0422@gmail.com>* trigger build.Co-authored-by: Cody Yu <comaniac0422@gmail.com>",1
[MetaSchedule][M4a] Mutator: Mutate Parallel (#10096),5
[Hexagon] Update hexagon API build instruction and cleanup hexagon_proxy_rpc (#10068)* Fix hexagon api build and Update Readme* Cleanup hexagon_proxy_rpc* Target Hack* Remove hack* address @cconvey comments* remove the rest of proxy rpc,0
[MetaSchedule][M4a] Mutator: Mutate-Tile-Size (#10092)* [MetaSchedule][M4a] Mutator: Mutate-Tile-SizeCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* Python 3.8 has no `math.prod`Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[TE] Fix Const Int bound analysis to handle uints for division (#10102)* case to handle uints* add unit test,0
"[Op][Topi] Gather, GatherND, Take can accept unsigned integers as indices (#10080)* take rel* gather and more tests* gathernd case* lint* remove test which invalidates take preconditions* re-add test* fix dumb test failure oopsie",0
[MetaSchedule][M4b] Testcases for TensorRT builder/runner (#10055)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,3
[MetaSchedule] postproc: rewrite_cooperative_fetch (#10081)Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,2
[Op][Topi] 5 ops can accept unsigned integers as indices (#10098)* tests passed* reformat* add uint test for unravel_index,1
"[MetaSchedule][M4a] User-API: Tune-TE/TIR/Relay (#10079)* Add tuning scripts for tir, te & relay.Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Minor fix.Nits.Add back tests.* slightly improve tune.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>",0
Update nn.rs (#10063),1
"[Fix Bug]fix the bugs of keras frontend when parsing LSTM, GRU, RNN layers. (#9850)* [Fix Bug]fix the bugs of keras frontend when parsing LSTM, GRU, RNN layers.* Reformat files with black formatter.Co-authored-by: AndrewZhaoLuo <andrew.zhao.luo@gmail.com>",0
[Caffe Frontend] Add support for Power layer (#9655)Co-authored-by: tangkun <kun.tang@hexintek.com>,1
Use ci.py explicitly in docs building instructions (#9971)This adds `ci.py` to the docs to make it more clear how to easily build the docs locally. This also re-arranges CI following the merging of all CI steps to run concurrently since there's no need to run the Sphinx precheck during GPU unit tests. This still preserves it though in the docs step as a way to quickly bail out if there are formatting errors so the full tutorials don't get built.Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[microTVM] Include standalone_crt dependencies in MLF (#10095) * Adds runtime to AOTExecutorFactoryModule * Standalone CRT files are added to MLF tarball if runtime is crt * external_dependencies info added to metadata.json for crt runtime * microNPU demo Makefile references standalone crt files from MLF tarball,1
[ETHOSN] Per-tensor support for int8 operations (#10018)* Per-axis quantization to follow,5
[CI] Update DGL in gpu image (#10111)* validating ci_gpu:20220128-070420-fa317edf7* remove gcn tutorial workaround* update ci-gpu image to v0.81,1
"[microNPU] Add support for nearest neighbor and bilinear upsampling (#9841)* [microNPU] Add support for nearest neighbor and bilinear upsamplingAdds support for 2x2 nearest neighbor and bilinear upsampling. In thecase of bilinear upsampling with align_corners set to true, theupsampling size must be `2*input_size - 1` (as opposed to `2*input_size`).Change-Id: I95d215eabfaac983629dcdedcda2b90efb8e0ddf* rebase and add support for no-upsampling case.Change-Id: I840d8ee3671a40c5c99f22119442c349dbed39cf",1
[Bugfix][Op] Fix shape inference of adv_index (#9717)* init* test* lint,0
[VirtualMachine] fix raw pointer using by VirtualMachine (#9980)Use ObjectPtr<Executable> instead of Executable* to solid saving of Executable object for correct work of VirtualMachine on c++ sideCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
[ETHOSN] Ethos(TM)-N 21.11 update (#10061)Minor update in inference buffer mapping due to changes in the driverlibrary,1
[CI] Use Python 3.6 variant of pypa.io (#10114)Using pypa.io directly now results in:```ERROR: This script does not work on Python 3.6 The minimum supported Python version is 3.7. Please use https://bootstrap.pypa.io/pip/3.6/get-pip.py instead.The command '/bin/sh -c bash /install/ubuntu1804_install_python.sh' returned a non-zero code: 1```Also see: https://github.com/apache/tvm/issues/9703,0
"[LLVM,TIR] Print LLVM intrinsic names instead of ids (#9964)* [LLVM,TIR] Print LLVM intrinsic names instead of idsThis makes it much easy to understand what is happening with llvmintrinsics.* add test, version llvm",1
[Ansor] Improve OpenCL support (#10108)* Support OpenCL in Autoscheduler tuning* add warning* Update src/auto_scheduler/search_task.ccCo-authored-by: Cody Yu <comaniac0422@gmail.com>* fix lintCo-authored-by: Cody Yu <comaniac0422@gmail.com>,0
"[AUTOTVM] Use opt level 3 when extracting tasks (#10065)* [AUTOTVM] Use opt level 3 when extracting tasksAutotvm was implicitly ignoring opt_level when extracting tasks becausepass opt_level is a thread local variable and extraction happens in anew thread. Not having opt_level 3 causes alter op layout to notfire, which in turn prevents tuning from finding all possible kernels.* disable alter op layout",1
[CI] Further open up Rust permissions (#10115)Tested this with `./tests/scripts/task_rust.sh` to ensure it builds.,3
"Update ci_qemu to v0.10 and Zephyr project generator for Zephyr 2.7 (#10117)This commit updates the ci_qemu image, in order to do this the Zephyrgenerator had to be updated to account for the newer version in theimage.",1
[CMSIS-NN] Moved test_cnn_small to the latest version (#9962),3
"[CUDA] Support float16 erf,tan,atan (#10122)* [CUDA] Support float16 erf,tan,atan* fix* fix* Update src/target/source/literal/cuda_half_t.h* Update src/target/source/literal/cuda_half_t.h",0
"[USMP] Add performance characteristics to PoolInfo (#10005)* [USMP] Add performance characteristics to PoolInfoScheduling algorithms that wish to optimize aroundmemory pools require further information about theperfomance characteristics of those pools. Thiscommit adds clock frequency, bandwidth, latency andburst length as optional fields to PoolInfo.Change-Id: I4cf3f35324d093fb38e874f0f2e587cb84d4ba1e* Remove unused importChange-Id: I1e2ef885425f4361b80c2bab9261ec129e61a756",1
[Relay][VM] Fix loading late bound consts when none exist (#10087)* Fix loading late bound consts when none exist* Simplify comment* -mFix skipping of verilator tests* Skip loading late-bound consts if none present* Remove semi-related fix to verilator test skipping* Remove more test-skip fixing for pr hygiene* No-op for ci* No-op for ci,0
[CUTLASS] Conv2d dgrad (#10110)* add conv2d transpose nhwc cudnn test* support conv2d transpose nhwc direct offload to cudnn* add cutlass dgrad support* remove unused arg* allow target none* fix beta initiaization condition* disable dynamic dense fp16 test since it fails on cuda 11.6,0
"[FIX,AUTOTVM] Add backtraces to tuning errors (#9901)* [FIX,AUTOTVM] Add backtraces to tuning errorsCollects tracebacks in LocalBuilder and LocalRunner and adds them to theerror messages.* formatting* correctly unpack traceback and exception* add assert* fix?* one remaining measureresult* formatting* fixed",0
Update ethos-u-vela for demo app (#10129)Update from ethos-u-vela 2.1.1 -> 3.2.0,1
[AutoScheduler] Allow device specification for AutoScheduler Runners. (#10123)* Changed the python api to support device.* Finished implementation and updated tests.* Fix typo.,0
[Relay][Pass] Add a relay pass to extract fake quantized ops (#10089)* add relay pass to collect fake quantized ops* add more tests* more tests* lint* lint* remove unused imports* update comment* lint* reuse SubgraphExtractor and update test assertions* remove print* lint* remove unneeded commentCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
Add more logging information to ReshapeLikeRel (#10125)* Update transform.cc* fix capitalize* fix lint* fix lint,0
[CMSIS-NN] Convert scalar constants to tensor constants (#10100),5
[Misc] typo and nit fixes (#10145),0
[onnx] fix onnx where broadcast (#10106)* fix onnx where bcast* jostle ci* jostle ci* jostle ci,0
Add bot to ping reviewers after no activity (#9973)* Add bot to ping reviewers after no activity* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[skip ci] Fix for ping_reviewers wait time (#10149)This was set to 1 day instead of 1 weekcc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>,0
Fix LayoutRewriter (#10118)* Fix layout pass* add unit test* fix lint* fix lint* fix lint,0
[BugFix] Linker: undefined reference to kTargetPoolReadWriteAccess (#10147),0
"[microNPU] Add support for transpose convolution (#9855)Adds support for legalizing transpose convolution toa microNPU conv2d operation for the case when strides==(2, 2),dilation==(1, 1) and no padding of the output is required.Change-Id: I485e2571913b3dcd7c75c46304f2f9a82f630ee0",1
[CMSIS-NN] Moved all asserts in tests under a single utils function (#10148),3
fix corner case when relay return empty tuple (#10128),0
[microTVM][Zephyr] Update RVM to Zephyr 2.7 (#10138)* Update to zephyr2.7 and Refactor* Temporary for testing* Update cmake version* fix import path and format* Fix test script* address comments* fix path* fix image name,0
[Runtime][PackedFunc] Bring `PackedFunc` into TVM Object System (#10032),5
Remove javah support (#10104)* Remove javah support* Remove unused compiler option* Osx pom.xml update,1
"[microNPU][3] Plan generation for the cascader (#9890)* [microNPU][3] Plan generation for the cascaderThe cascader creates 'Plans' which describe howto schedule subgraphs. As part of the cascadingalgorithm, it's necessary to explore a largevariety of Plans which are Pareto optimal (interms of memory usage and performance). This isdone by the Plan generation algorithm.This commit adds the TensorConfig and Plan datastructures which hold information on how to schedulethe tensors/operators. Additionally, it includesfunctions to calculate Pareto frontiers which areused to cull sub-optimal Plans.Change-Id: Ia358b2a1b29bd810df4441027752ced75812ad4e* Fixes to lint/testChange-Id: If4e083a3c96af75a8ffa72510704818d21a477d9* Improve python docsChange-Id: I831137f8235665bc20ab4c060cc7049ffd48088a* Fix enum hashing issue with old gccChange-Id: Ifbe97eb33b1ef313710f24c687a8155421a3c195",0
"[TVMScript] Support T.buffer_decl using data pointer from Let/Allocate (#10099)* [TVMScript] Added unit tests demonstrating desired functionality* [TVMScript] Implemented parsing of T.Ptr[...]These can be generated when exporting to TVMscript, but were notparsable after being generated.* [TVMScript] Updated buffer_var printingLetStmt and AllocateNode can both be used to generate handles that areused in Buffer objects.  In these cases, the Buffer declarations mustgo after the handle declaration, not in the function header.* Moved printing of var and buffer_decl into separate statements.* Updated following @shingjan's review comments.",1
"[CI] Update ci_arm and ci_lint (#10146)This includes sccache, and these are the less troublesome images.Also see #10120 for update issue.",1
"[microNPU] Fix layout assignment in layout optimizer pass (#10143)Fixes the layout optimizer incorrectly assigning layouts for graphs withmore complex topologies than previously considered. Specifically, thiscommit now ensures that intermediate layouts match (e.g. parent output =child input) and that all consumers are taken into account when alteringthe output layout - something not done previously due to an incorrecttraversal order.Previously, the input layout was always altered if the producer was anNPU operation without regard to the output layout of that operation.Additionally, is was possible for the output layout to be incorrectlyset due to a depth-first post-order of traversal of the graph, meaningit was possible for not all consumers to be taken into account whenaltering the layout.Now the `AnalyzeConsumers` pass is run before `LayoutOptimization` whichdetermines a mapping from NPU operation to list of boolean values thatrepresent whether or not each consumer is an NPU operation. Since thisis completed before `LayoutOptimization`, all consumers are guaranteedto be taken into account when altering the output layout. In turn, theinput layouts can correctly be determined by checking whether the outputof the producer will be altered.Change-Id: I04e9605da65fa9f12801109dd50c5e3f08cbc73c",0
[TIR][Schedule] Update compact_dataflow constraint (#10158),1
remove async tst (#10160),4
OpenCL debug runtime timer handler added. (#10140),0
"[microNPU] Enable network tests for U65 256 mac variant (#10159)Currently we run the network tests only on one NPU variant, so thispatch is to enable all the currently supported variants.Change-Id: Ic18054fb4ba19eb28b99ff4439e5a36e57199763",3
[ETHOSN] Drop back to Ethos(TM)-N release 21.08 (#10157)There is some clash with the int8 support that went in at the same time.,5
[Relay] Align strided slice shape functions (#10155)* fix static strided slice shape func for out-of-bounds negative stride slicing* Trigger CI* Trigger CI,0
[Pass] Simplify consecutive casts in Relay (#10133)* initial commit* initial commit* update test* jostle,1
[Meta Schedule] Allow Non-strict Population Size in Evolutionary Search (#10163),5
"[Torch] Experimental support for FX-quantized models (#10091)* works on resnet18 and deeplabv3* yolo5 conversion worked* fixed sigmoid* [Torch] Support clamp_min, clamp_max* fixed clamp_min* fixed quantize for 1 dim input* cleanup* improve inline_qparam impl* add clamp_min/max test* add fx quant test* cleanup* skip build in testing* black* improve clamp conversion* leave TODO on inf handling",0
[skip ci] Fail silently in ping_reviewers GitHub actions. (#10173),5
gitignore build-* folders (#10168)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,5
"[Relay][VM] Relay VM memory liveness/lifetime analysis (#10026)* WIP VM memory planning* tuple projection* support if* lint* remove old comment* WIP check in attempt at CFG analysis* rewrite CFG analysis in stages, support ADTs* lint* fix small bug in alias elimination, try fix VM profiler error* update DCE tests since allocations can be DCE'd* optimize worklist to reduce runtime* add docs, rename pass to ManifestLifetimes* add tests, more comments, proper VM profiler fix* lint* ci please* address nits* retry ci again* retry ci once again :)* fix sneaky memory leak due to cyclic refs* fix didn't work but retry ci anyway* slightly reduce size of large pretty printer test",0
Fix broadcast InferCorrectLayout (#10156)* Move function body to .cc file.* fix broadcast infer layout* add unittest* backward-compat: optimize for scalar layout* fix lint* fix lint and warning* Add newlines; Use std::vector* fix lint* jostle ci,0
"TVMC: Don't divide trials by zero tasks (#10164)If there are no tasks to tune, the number of trails is meaningless.Co-authored-by: Martin Kröning <martin.kroening@neclab.eu>",5
[MetaSchedule] Add target field to MetaScheduleContext (#10169)* Add target field to MetaScheduleContext* fix linter issues,0
"[skip ci] Fix scipy intersphinx link (#10181)Scipy changed their docs recently to be pinned to each release, so we need to update the URL we pass to intersphinx accordingly. Skipping CI on this to unblock other developers but local testing with```bashpython tests/scripts/ci.py docs```both showed the same error as CI before this change and no error after.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[CUTLASS] Initial support for conv2d wgrad (#10177)* [CUTLASS] Add wgrad support (without split-k)* run black* wgrad tests now work under pytest* dw conv2d properly supported for wgrad* all tests work* fixed for sm75* cpplint* fix conv2d grad test,0
"[TIR] Canonical simplify the intset before region cover proof (#9941)* canonical simplify the intset before region cover proof* add more rewrite rule for intimms to fix the issue after rebase* remove rewrite rules, there are existing rule can work after canonical simplify",0
"Support PyTorch grid_sample  (#10184)* [relay] Fix stack overflow in device_planner observed on windows due to recursive function calls.* Revert ""[relay] Fix stack overflow in device_planner observed on windows due to recursive function calls.""This reverts commit 70581364771e2415b37b202a9fa6a937f275cfc6.* [PyTorch] Add grid_sample with zeros and border padding mode for PyTorch.",0
"[MetaSchedule] bug fix ApplyHistoryBest. Previously, ApplyHistoryBest returned the incoming module without applying the tuning history. (#10183)",0
[BugFix][TVMScript] Use operator `is` when recognizing TIR Module (#10175)* [BugFix][TVMScript] Use operator `is` when recognizing TIR module* Test,0
[CUTLASS] Add parallel split-k support to wgrad (#10185)* [CUTLASS] Add split-k support to wgradcommit 60b73a91b79d644d8c95f682eedaf47a89abba0dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Feb 8 10:43:11 2022 +0900    pylintcommit ae2e7187256316c48c915c3c187feb5cd4d4dbd4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:51:52 2022 +0900    Add split-k support for wgrad    commit 43820d50055b0bd17b736f5c5830321c7509a20a    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sun Feb 6 10:07:34 2022 +0900        fix and add doc    commit 446a95b0aabc5ab69cdd2e414b812aab1c557f42    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sun Feb 6 09:48:38 2022 +0900        dw conv2d properly supported for wgrad    commit adc4e22d2e03a99f30ebb6a5e956a1749de693f0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:32:42 2022 +0900        fix overwriting template    commit 040eab000bc5f162c6e9aca70ae6d29378fe65bc    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:06:27 2022 +0900        black    commit e5a07c24b7463552b8e545710d25472159bcc127    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 16:03:10 2022 +0900        add reduction in profiler    commit be89334ab981d536d010dd765c9cf601dbdae5e0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Sat Feb 5 06:58:03 2022 +0900        adding split k reduction to conv2d profiler    commit ae09b0fbdc3a472eb320d866c054f73b3142f21c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 11:52:59 2022 +0900        fixed conv2d_backward_weight typerel for dw conv2d        commit 16fe5313fd1219e2e7d531ef9b36f64bb557e5e7        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Feb 3 12:59:22 2022 +0900            wip        commit 2167c2543340a285bb1985e8fe37e11aed51fb9b        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Feb 3 04:22:19 2022 +0900            fix conv2d type rel for depth wise and grouped conv2d    commit 14b12e5dd84fc34691d585213387198f091eefc5    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 05:01:03 2022 +0900        remove split_k.py    commit b14127179c43f71c3ce5ccc7b4ca678a099e5497    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 04:48:21 2022 +0900        workaround for invalid split_k_slice    commit 6e4c7e1d77d89f124abc77dbcdab69eff8a5d961    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 02:43:58 2022 +0900        support split k in profiler    commit 2eb1cf43c7f56f0537cf249855054b5cbd357b13    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Fri Feb 4 02:31:03 2022 +0900        improvement    commit 0bce8f3778a6bb05607232a0997d25681e55ce7c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 18:20:12 2022 +0900        fixed for fp16 output    commit 30df1bd5282a4d326856382726d4e63ee8c27e8e    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 17:50:33 2022 +0900        fp32 output works    commit 7a519956b8d103464dff83b4f01b75973f4a33b0    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 14:30:22 2022 +0900        fix    commit 4a383e2c7c37148a563e9cf34968fb7da3aaf91f    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 14:05:24 2022 +0900        update c++ codegen    commit 6206e388cc7062cbef0b3c8c47fcd228b44b6818    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 13:46:05 2022 +0900        wip    commit 0ece49b53e773ebc1ea71c7667abc0cbb29d91bf    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Thu Feb 3 03:05:21 2022 +0900        wip    commit 08a6147940d9911fd65a890a4d90beb68176fc03    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 13:10:21 2022 +0900        test worked with fp32 output    commit 084d5c47666df92ba6c2c1445d5a23de0193a119    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 12:35:18 2022 +0900        fix compile error for fprop    commit 31f25436c5aca1a75336fa1a8d1c8a25a4936ee8    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 12:18:06 2022 +0900        compiled    commit c2098e79ade47117f2c32132da864b1fa73fce4a    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Feb 2 11:11:43 2022 +0900        wipcommit a14585020151d0e09bb9bac549285dceb13e55e1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:46:16 2022 +0900    fixed for sm75commit 61515062ef4576bf5b4e7e9e800f7f705738809cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:32:46 2022 +0900    all tests workcommit 041c094b3646e0f521f5bd2c4f6f6b5b1cff7b97Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sun Feb 6 14:19:09 2022 +0900    dw conv2d properly supported for wgradcommit 2191918743a4e9ffb8254f3786d817be57ff49ccAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 09:14:05 2022 +0900    wgrad tests now work under pytestcommit 78f76df1eb1602f66cacb888a97b6b267f8600a7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 07:31:54 2022 +0900    run blackcommit 0a82149fe0586b0bf449fc7f3a1fa9809e9b38d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Feb 2 06:12:39 2022 +0900    [CUTLASS] Add wgrad support (without split-k)* pylint* add more doc* more doc clarification,0
[FQ2I] Add topk to FQ2I (#10170)* Add topk/dyn.topk to FQ2I* Remove dyn.topk* Add uint8 to sort,1
[TIR] Allow compute_at create block predicate for non-trivial bounds and support floordiv pattern (#9527)* allow generate block predicate in compute_at schedule* revert #9880 and add more testcases,1
[cleanup] Remove task_sphinx_precheck.sh (#10196)This is empty following #9971 and is no longer neededCo-authored-by: driazati <driazati@users.noreply.github.com>,4
"[ETHOSN] Add support for mean on Ethos-N78 (#10130)Adding the support of mean on Ethos-N78, which is based onan underlying pattern matching scheme.The operator is tested with 2 shapes: 4 and 3 dimensions.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>",1
[microNPU] Add support for pack and unpack (#9960)* [microNPU] Add support for pack and unpackPack is represented by a series of `expand_dims` operationsfollowed by a `concatenate` in Relay. Unpack is representedby a `split` followed by a series of `squeeze` operations inRelay. This commit legalizes `expand_dims` and `squeeze` toreshape operations while making use of existing legalizationtechniques for `split` and `concatenate` so that pack andunpack can be offloaded to the NPU.Change-Id: I3fbebb4ece5ca04598f8e587b9e6c0ddf280266d* rebase and add tests for expand dims and squeezeChange-Id: Ic6a9fd77b61368720328bfe82032490bcc66152c,1
"[microNPU] Refactor type inference data type checks (#10060)* [microNPU] Refactor type inference data type checksAims to improve readability, extendibility and error messageunification for data type checks across NPU operators.A follow up for the comments in #9576.Change-Id: I83fb89a56677003f7abebb7985ad60d92cfa8df1* unordered_set -> initializer_list and use new format for upscale checkChange-Id: Icf3d68d5cc7d5e1d5af42b1af193db89faea155e* remove unused header and use auto for initializer typeChange-Id: I10311b718c3abd0ed75dd88b5ec9de6e0742f047",0
[Torch] Run torch JIT pass lower_all_tuples before conversion. (#10186)* [Torch] Run torch JIT pass lower_all_tuples before conversion.* [Torch] Input containing tuples will disable lower_all_tuples.Co-authored-by: wenyuchi.wyc <wenyuchi.wyc@alibaba-inc.com>,4
[ETHOSN] Fix quantization parameters in test (#10178)The Ethos(TM)-N tests for addition had a poor choice of parameters forthe int8 case.,0
[ETHOSN] Per-channel int8 quantization for conv2d (#10131),5
[microTVM] TVMCon 2021 Zephyr Demo with CMSIS-NN (#10144)This is adds a Zephyr Demo showing how to integrate TVM directly into your embedded application. It runs a keyword spotting model with the Zephyr RTOS using CMSIS-NN with the Ahead-of-Time (AOT) executor and the stack allocation strategy.,1
"[TVMC] Add codegen args to tvmc (#10190)* [TVMC] Add codegen args to tvmcThis enables external codegen arguments similar to those for `Target`s:```tvmc compile --target=cmsis-nn,c --target-cmsis-nn-mcpu=cortex-m55```* Add CMSIS-NN decorator to dependent tests",1
[CMSIS-NN] Fix extension detection for CPUs (#10200),0
Add scripts for automated build and testing (#10194),1
[OpenCL] Fix vthread_extent for warp size 1 case (#10199),0
[USMP] Register hill climb algorithm (#10182)* USMP: Register hill_climb algo in algorithms map* USMP: Add missing space to assertion message* USMP: Integrate hill_climb algo in test_crt_aot_usmp,1
"[QNN] Lookup operations for hard to implement operators (#10053)* initial tanh impl* smalls error* support uint and int lookup into tables* reinterpret cast, working tanh tests* refactor relay func creation* basic casting tests* explicitly say do not handle multi-channel lookups* add example funcs* fix silent fail* fix some bugs with floating point funcs not working* add TODO* add tood* canonicalizations* refactor integer lookup ops into own folder* fq2i stuff* clean up existing tests* flesh out todo* more tests* test on keeping shape good* lookup table fix* replace canonicalization for rsqrt* remove canonicalization of rsqrt* add asf headers* topi tests* gather supports unsigned integer tests* fix things* move to legalization* jostle ci* linting* use take instead of gather* remove gather changes* undo changes* undo changes* undo changes* move thing in range* initial tanh impl* smalls error* support uint and int lookup into tables* reinterpret cast, working tanh tests* refactor relay func creation* basic casting tests* explicitly say do not handle multi-channel lookups* add example funcs* fix silent fail* fix some bugs with floating point funcs not working* add TODO* add tood* canonicalizations* refactor integer lookup ops into own folder* fq2i stuff* clean up existing tests* flesh out todo* more tests* test on keeping shape good* lookup table fix* replace canonicalization for rsqrt* remove canonicalization of rsqrt* add asf headers* gather supports unsigned integer tests* fix things* move to legalization* jostle ci* linting* use take instead of gather* remove gather changes* undo changes* undo changes* undo changes* move thing in range* lint* remove unneeded line* jostleCo-authored-by: andrewzhaoluo (generated by with_the_same_user script) <andrewzhaoluo@system76-pc.localdomain>",0
[microTVM][Tutorials] Add tutorials to run on ci_qemu (#10154)* add python files to script run* add python files to script run* fix issues with formating,0
Disable tensorflow v2 behavior in all unit tests (#10204)This should resolve the issue posted here https://discuss.tvm.apache.org/t/tensorflow-2-0-test-failures-while-running-the-tensor-flow-frontend-test-forward-py-function/11322 based on [this comment](https://discuss.tvm.apache.org/t/tensorflow-2-0-test-failures-while-running-the-tensor-flow-frontend-test-forward-py-function/11322/3?u=driazati).This is a necessary precursor to #10198 since it relies on invoking the tests individually.Co-authored-by: driazati <driazati@users.noreply.github.com>,2
resolve issue #10107 by setting eps larger (#10176)* resolve issue #10107 by setting eps larger* use numpy.testing.assert_allclose,3
"Add FreeRTOS variant of NPU demo (#10004)* Add FreeRTOS variant of NPU demoThis adds an extra flag to the existing NPU demo that runs it using theFreeRTOS kernel task scheduling and queues.* Add FreeRTOS notes to tutorial* Update FreeRTOS demo to run in demo scriptAlso, minor text fixes.* Minor text fixes* Fix docs formatting",0
"[RPC] Add Missing Command Line Option ""through-proxy"" of RPC Server (#10188)",1
"[ci] Add more details when showing node info (#10195)This adds some more information to help debug when there are infra problems with Jenkins, notably:* More Jenkins environment variables: https://www.jenkins.io/doc/book/pipeline/jenkinsfile/#using-environment-variables* EC2 metadata* System level information (disk space, CPUs, memory)Co-authored-by: driazati <driazati@users.noreply.github.com>",0
"Implementation of Common Subexpression Elimination for TIR (#9482)* Initial implementation of Common Subexpression Elimination for TIR (#703)The goal of this PR is to implement a Common Subexpression Elimination (CSE) pass for TIR, which aims at identifying redundant computations (both within statements and within expressions), and to replace them by a new fresh variable, introduced before the first occurrence of the redundant computation.Note that it does not only try to do commoning on full expressions, but it is also able to do it on subexpressions. For instance, if the program computes the expression (w+x) + (y+z) and the expression (w+x)+u, it will introduce the subexpression (w+x) into a new variable.If we want so, it will be easily possible in the future to make the notion of equivalence between terms more flexible, allowing for instance to identify expressions modulo commutativity (identifying for instance (x+y) with (y+x)), modulo associativity (identifying for instance (x+y)+z with x+(y+z)), etc. Replacing only the function bool EquivalentTerms(const PrimExpr& a, const PrimExpr& b) will be the only thing needed in order to do that. The typical way to rewrite it for such extensions would be to compute a canonical representant of a and a canonical representant of b and to then compare them with the strict syntactical equality.The main CSE pass is declared and implemented respectively in the files common_subexpr_elim.h and common_subexpr_elim.cc.The function Stmt CommonSubexpressionEliminator::VisitStmt(const Stmt& stmt) is a good entry point as it contains many comments about what the pass is doing.The general idea of this pass is that it tries to introduce at the current level (the current root) the computations that are redundant and which are possible to introduce there (they should only contain variables that are in scope). This notion of variables in scope is implemented with a context, which is a vector of pairs (var, MaybeValue). The context is not only used for checking that variables that appear in candidate computations are known at this point, but also for checking if a computation has already been introduced into a variable.For a greater flexibility in the future, there is a strong distinction already in place between :    - Syntactic computations, which are maintained in a hashtable which associates expressions (the computations already seen) to size_int (the number of times the computation has been seen).    - Semantic entities, which are obtained from the syntactic computations by merging equivalent computations (where this notion of ""equivalent"" is customizable). Semantic entities are stored into a vector of pairs (expr, size_int) where, again, the number is the number of times that expr or equivalent computations have been seen.The VisitStmt() method starts by computing the syntactic computations (implemented in an auxiliary analysis), then it merges equivalent computations to obtain the semantic computations. Then it sorts these semantic computations from biggest to smallest in order to always consider first the biggest computations. The rest will essentially be a loop over all these candidates, which will stay sorted.When dealing with a candidate computation, there are three cases that can happen:    1 - Rare case A variable in the context already contains this computation. This variable can't have been introduced by the CSE, as we would have performed the replacements at the same time (see case 2). So this is the case where the user himself (or the previous TIR passes) has written something like ""let x = A in ...A...A...)""    -> In this case, we simply perform the replacements of A with x in the current result. These replacements are done by an auxiliary transform/Mutator, declared and implemented in replace_expr_selected.h and in replace_expr_selected.cc.    2 - Case where we need to introduce the current computation inside a new variable This is the case where all the variables used by the current computation are within scope (i.e. are present in the context) and where our internal heuristic/predicate tells us to introduce this computation into a new variable.    -> In this case, a new variable new_var_i is generated, all the locations that use this computation in result are replaced by this fresh variable (using the same auxiliary Mutator mentioned in 1.), and the current result is replaced by let new_var_i = currentComputation in result.    3 - Case where we can't or don't want to introduce this computation inside a new variable This is the case where we either can't introduce the current computation inside a new variable (because it contains variables that are not yet in scope there) or because our internal heuristic/predicate did not want to introduce it.    -> In this case, we will compute the direct sub-expressions of the current computation (implemented by an auxiliary analysis), and we will add them to the vector of semantic computations so that they have a chance to be considered later. Note that they are added while still preserving the order.    Note that we do not add all the sub-expressions of the current expression but only its direct subexpressions given the fact that we always consider them from biggest to smallest, and given that some candidates are mutually exclusive. Otherwise it would be computationally more intensive and it would pose the problem of cleaning the vector of candidate computations when one of them gets introduced into a variable. Evaluating them lazily by only looking at the direct sub-expressions is at the same time more efficient and simpler.Once the entire vector of semantic computations has been tried, the main function VisitStmt() calls the general dispatcher , which will in turn call the appropriate handlers. The only specific task of overridden handlers will be to update the context appropriately as new variables are introduced into scope (via Let-In, via For loop, etc) or leave the current scope. Thus, they will update the context appropriately before and after the calls to VisitStmt() and VisitExpr() on the child nodes.* Added empty newline at the end of every new file* Rolled-back the pointer to the submodule vta-hw* Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too.* Spelling and comment* Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too.* Revert ""Improved the CSE by not commoning at the toplevel redundant computations that only appear in one of the possible execution path (for instance, only in the then/else branch of an IF statement). Redundant computations that appear only in a specific execution path are now being commoned at the entrance of their specific execution path instead of earlier at the toplevel. Introducing them at the toplevel was an anti-optimization as the redundant computation might not have been comptued at all. Added two additional tests for this too.""This reverts commit c4138d9afc28e79f107a4eccf988a6d93221eb5a.* Fixed reference used for no reason instead of normal variable.* Added comment explaning why we do not need the union/intersection over N tables at the moment (because we would only use it for N=3)* Did most of the changes suggested by upstream* Continued to work on the remarks given on the public repo.* Final remarks addressed, small formatting things, and fixing things reported by the linter* Last linter fix.* Fixing newline* Adding newline missing.* Minor commit for style fo conform with clang-format* Removed trailing space at end of line* And more minor style changes* Fixing style of the python test files* And one more for style in python tests!* This linter is very annoying to force the style of indentation in a comment, in a test file. It makes it harder to read in this case! And that incitates people to not write comments* Deactivate the CSE pass for the lowering tests as it would otherwise do some commoning, and improve the way the CSE recurse + test added for cascade commonings* Fixing new lint offenses* Removing debug statement* Restore other test file to its previous state* One more for the linter...* Linter again, this time for the new test...* again* again...* Deactivating the CSE pass for another lowering test as it does some commoning* Disabling the CSE for the a test for GPU too* Trying to fix a VTA test by disabling the CSE pass for it, as it probably does some commoning* Complying with the linter* Restarting the CI 1/2* Restarting the CI 2/2* Restarting CI 1/2* Restarting CI 2/2* Slightly reduce size of large pretty printer test, copied from https://github.com/apache/tvm/pull/10026/commits/ae98f9e7809cbf8d910fa16bfeac8364196e57d7* Trying to resolve the problems on the weird tests* Linter.* Restarting CI which has skipped the MacOS build for no reason 1/2* Restarting CI which has skipped the MacOS build for no reason 2/2* Commented buggy tests* Linter...* Restore the VTA tests, and use trick kindly given  by Masa to disable the CSE pass for the VTA tests, as vta.build() overwrittes the config* New fix, which this time does not break the doc (VTA uses a set with {} for the disabled passes instead of a list with [] for some reason* More VTA fixes* vta tutorial fixCo-authored-by: Masahiro Masuda <masahi129@gmail.com>",0
[Conda] Fix a compatibility bug in conda scripts (#10201)Remove `max_pin` requirements for `cudnn` library to be compatible with `cudatoolkit>=11`,0
"PackedFunction to return params from the .so module, show warning when no params are set (#9811)* PackedFunction to return params from the .so module, show warning when no params are set* Linter checkup* Autoset of params, tests for get_graph_params* Linter checkup* Check that inputs were set before run* Return the original implementation if Run* Fix RPC behavior",0
[ci] Invoke tensorflow tests individually (#10198)* [ci] Invoke tensorflow tests individuallyThis is another (simpler) attempt at #10151 to avoid CUDA issues with tensorflow tests. This should work by cleaning up any reserved GPU memory by tearing down the whole process that has imported tensorflow each time a test is run.* [ci] Invoke tensorflow tests individuallyCopy of #10197 but using a pytest plugin instead of manually grepping through test filesCo-authored-by: driazati <driazati@users.noreply.github.com>,2
[skip ci] Fix onnx/models URLs (#10218)These are broken in CI: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2500/pipeline/275The upstream changed their default branch from `master` -> `main` whichbroke the links used in these tests. This pins to a specific commit (thelatest one at the time of filing this PR).Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[microNPU] Use TFLite tests for strided_slice (#10165)Simply migrates the Relay style tests to TFLite for unification amongother tests.Change-Id: Iaab3ba27ad1534145fe94302be29f386f08af58e,3
fix an index out of bound problem of cache write block (#10203),0
Fix more ONNX URLs (#10220)PR #10218 was not enough for this fix so this should probably run through full CI to make sure it got everything.cc @mousius @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>,0
Fixes for follow up on PR #9631 (#10205),0
"Adding support for Hexagon User DMA Engine (#10217)* initial hexagon user dma impl* Hexagon User DMA descriptor, instruction and register headers* Synchronous 1D DMA working* HexagonBuffer unit tests passing with memcpy* cleanup* comments and orgnanize code* format and lint* init function + other code review feedback* add ifdef hexagon around inline asm",1
"[RPC] Take PageAllocator out of MinRPCServer, make it template parameter (#10219)* [RPC] Take PageAllocator out of MinRPCServer, make it template parameterThis way MinRPCServer can be instantiated with a custom PageAllocator.* Restart CI",2
"Skip tensorflow test `test_forward_ssd` (#10231)Since this test is launched in a thread, errors aren't propagated up to the main thread and thusly pytest doesn't detect / report them, so this test will always succeed. Given that this single test can take [upwards of 30 minutes](https://ci.tlcpack.ai/job/tvm/job/main/2499/testReport/cython.tests.python.frontend.tensorflow/test_forward/), this PR disables it until someone lands a proper fix (given that this test takes so long some consideration should be given to reducing its runtime before enabling it again).Co-authored-by: driazati <driazati@users.noreply.github.com>",0
"""Resolved deprecation issue in test_op_qnn_conv2_transpose.py"" (#10228)",3
Mark test_op_int8 as flaky (#10215)As per the docs [here](https://github.com/apache/tvm/blob/main/docs/contribute/ci.rst#handling-flaky-failures) this PR marks `test_op_int8` as a flaky test so it will run but failures won't be reflected in the Jenkins jobs. Once #10213 is addressed this PR can be reverted.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[PyTorch] add var_mean support (#10233)* [PyTorch] add var_mean support* update mean_variance,1
"[USMP] adding support for U2 and U3 usecases (#10193)This commit adds a MemoryPools argument forthe compilation flow according to RFC0029.Moreover, it is used to provide support forexternal pools from the application layerthat could be pinned for different memoriesand/or be reused between multiple inferencesof a model.",1
[TVMC] Add configuration `tir.add_lower_pass` to option `--pass-config` (#9817),1
"add a simplify rule for floordiv(x*8+7, 16) => floordiv(x, 2) (#10232)",1
"[Hexagon] Refactor Hexagon.cmake (#10227)This file is included every time TVM is build, regardless of whetherany support for Hexagon is enabled or not. This refactoring is meantto remove underlying assumptions about what features are enabled andwhat the compilation targets are. Now, when there is nothing neededfrom Hexagon, the script exits early (although it doesn't need to),and the rest of it is (and should remain) safe to execute regardlessof build configuration.Disable ""runtime.module.loadfile_hexagon"" from the offload runtime,since it conflicts with device_api.hexagon.v2.It was only used with offload on Android, which is being deprecated.",2
clean up conv2d type rels (#10236),5
[Torch] Fix conv2d transpose with group (#10235)* [Torch] Fix conv2d transpose with group* lint* wrong issue number* do not run test on cuda,0
Support sub warp reduction for CUDA target. (#10207)* upd* upd* upd* lint* fix* upd docstring* upd,0
"[Tir]Adding detail error messages when MatchCopyPattern function is failed. (#10244)There is an error message to show the body when 'MatchCopyPattern' is failed,but the error message not give the information why this function get failed.Adding the detail error information to help trouble shooting.",0
"Fix a lint issue. (#10245)lint.sh complain for an addtional space line in 'utils.cc', just fix it.",0
[TOPI] VNNI support for int8 dense (#10230)* wip* revert for now* simplify blocking* add bench script* update type rel* refactor tests* end to end compilation working* paralleize outer loop* add shape check* fused schedule first cut* restore original test* black* add vnni check* add relay test* skip on ci* check dtype* lint* make it tunable* minor cleanup,1
[Relay] Make DeviceAnalyzer a mixed mode visitor (#10248)* hack to ExpandDataflow* add test from mei* Update DeviceAnalyzer to inherit from MixedModeVisitor* indent,1
add back supported tests (#10116),1
[QNN] Register a bunch of unary elementwise ops (#10086)* 0;276;0cinitial commit* register a bunch of ops* unary ops* add a bunch of tests* 0;276;0crefactor tests* add tests to qnn* comments on macros* add back in log to pattern utils* update floating point func description* proper creating of calls to quantize and dequantize* fix lowering process for using dequantize and quantize ops,0
Fix JUnit failure reporting (#10121)* Fix spacing* Add try..finally everywhere* trigger ci* Fix pytest invocations* Remove junit collection where no files existCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[BUGFIX] Define kTargetPoolReadWriteAccess globally (#10262)* Fix bug* Fix whitespace* lint* Move the other consts out of PoolInfo,0
"[Hexagon] Don't use cmake glob for auto-generated source files (#10259)* [Hexagon] Don't use cmake glob for auto-generated source filesGlob treats inputs as patterns: if the file with a given namedoes not exist (is to be generated later), it won't be added tothe output.* Restart CI",1
Add one extra space to improve diagnostic messages (#10268),1
[RPC] Link in whole archive with BUILD_STATIC_RUNTIME (#10260)* [RPC] Link in whole archive with BUILD_STATIC_RUNTIME* Restart CI,5
Gitignore work items in jvm and android_rpc (#10253)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,5
fix RPC waiting for device (#10255)Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,0
Overload get() function for `Optional` type. (#9748)* upd* simplify* upd* fix* upd* fix docstring,0
Skip flaky tensorflow tests (#10276)See #10275cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>,3
[Cuda] Updated bfloat16 math defs. (#10258)Required to pass `test_cuda_bf16_vectorize_add` in `tests/python/unittest/test_target_codegen_cuda.py`.,1
[TVMC] Add an end_to_end benchmarking argument when benchmarking. (#10256)* Add an end_to_end benchmarking argument to TVMC run.* Add command line test.* Fix comment syntax.* Set device to cpu if end_to_end is on.* Tickle CI,0
"[VTA] Search for libvta_fsim.so in $TVM_LIBRARY_PATH (#10278)This adds `$TVM_LIBRARY_PATH` to the search directory of`vta.libinfo.find_libvta`, matching the behavior of`tvm._ffi.libinfo.find_lib_path`.",1
[BUGFIX] fix text printer when TVM_LOG_DEBUG is on (#10279),0
[microNPU] Add support for LeakyReLU (#10127)* [microNPU] Add support for LeakyReLUAdds support for offloading an int8 Leaky ReLU activation functionto the NPU by legalizing to a LUT.Change-Id: I63dd5b16a1a2a747b11f15a5b8124810e2ebf491* refactor LeakyReLUParams to inherit from LutActivationParamsChange-Id: I35b59200b16a7eff1915f771ab6b5d9181d4f3ab,1
update dnnl version from v1.5 to v2.2 (#10266),1
Use `/usr/bin/env bash` in shebang for all scripts under tests (#10277)* Use /usr/bin/env bash in shebangThis makes scripts executable on system without /bin/bash (NixOS)* Use `set -e` in script instead of `bash -e` in shebang,2
Add a conversion of individual operations in FQ2I pass. (#10239)* Add a conversion of individual operations in FQ2I pass.* apply review comments* apply review comments 2,1
"[CMake] add support for find_package (#10097)* removed include header path, which are invalid* 1. added target tvm in a cmake export group2. added cmake package config file* added Threads as public dependency* changed temp config file name for better understanding",1
[Docker][Hexagon] Add docker file and scripts (#10263)* Hexagon docker files added* trigger,1
[ci] Mark `test_autotune_conv2d` flaky (#10298)See #10297cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>,3
[TIR] Add software pipelining (#10066)* [TIR] Add software pipeliningCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* fix* fix* lint* fix* format* doc* remove print* lint* lint* doc* Apply suggestions from code reviewCo-authored-by: Junru Shao <junrushao1994@gmail.com>* address comments* address comments* refactor FragmentInfo::GetSize* remove unused* refactor* address commentsCo-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>,0
[FQ2I] Add support for some unary operators (#10273)* initial commit* lint,1
Add -i option to fix ASF headers to lint scripts. (#10284)* Add -i option to fix ASF headers to lint scripts.* address driazati comments,0
[Hexagon] Pass kDLHexagon device when allocating workspace pool on Hexagon (#10289),4
[Hexagon] Remember to add common sources when building TVMRT for Hexagon (#10290),1
Allow Ctrl+C during docker/lint.sh. (#10291),5
[ci] Mark some ehtosu tests as flaky (#10301)See #10300Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[microTVM] Add timeouts for CI tests (#10295)These shouldn't take longer than 5 minutes but since they have to poll they can end up running for a long while (e.g. this failure: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2534/pipeline).cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>,1
Add flaky test issue template (#10299)This adds a template so we can report (and label) flaky test issues separately from CI infra problems. This also helps others report flaky tests by pointing them to the relevant documentation.cc @areusch @denise-k @hpanda-naut @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[UnitTest] Disable ptx mma tests on unsupported nvcc versions. (#10229)* [UnitTest] Disable ptx mma tests on unsupported nvcc versions.- Modified `tvm.contrib.nvcc.get_cuda_version` to return a  `(major,minor,release)` tuple rather than a float.- Implemented `tvm.testing.requries_nvcc_version` decorator to specify  the minimum `(major,minor,release)` version needed to run a unit  test.- Applied decorated to unit tests in `test_tir_ptx_mma.py` that fail  on earlier nvcc versions.* Fix lint errors.* Updated a few of the cuda version checks.* More lint fixes.* Only compare major/minor in find_libdevice, not release version.",0
[TIR][Schedule] simpilfy compute_at static bound (#10307),5
[CI] Update CPU image to v0.81 (#10305),1
[Hexagon] Fix getting/setting DMA state (#10288)* [Hexagon] Fix getting/setting DMA stateThe bits [3:0] of the first word of the descriptor (both 16- and 32-byte)is the DMA state. It must be set to 0 before starting a DMA transaction.* Restart CI,0
update (#10306),1
"[TOPI] Add support for groupped conv3d (#9873)* [TOPI] Add support for groupped conv3dChange conv3d to use generic conv implementation which supports grouppedconvolutions. Also, remove support for non-float16 tensorcore operationsas they cause large degradation in accuracy. Generic conv now supportsautoscheduler.* correct none check* add tests for floordiv simplification* fixed incorrect test for autoscheduler* formatting* add groups to winograd* fix tensorcore* manually simplify index instead of relying on simplifier* formatting* add groups argument to conv3d_ncdhw_winograd_without_weight_transform* formatting",0
Parameterize test_link_params. (#9276),3
[Relay] [Virtual Device] Store function result virtual device in virtual_device_ field (#9848)* VStore function result virtual devices in virtual_device_ field* Address Mark's 'mega nit'* Promote function result virtual device to first class* Add kVirtualDevice* move kVirtualDevice* Fix annotation test* Progress on parsing & printing* Fix printing of virtual device attribute* flake,0
[ci] Disable flaky microTVM tests (#10313)See #10312cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>,3
[ci] Disable flaky cmsisnn tests (#10315)See #10314cc @masahiCo-authored-by: driazati <driazati@users.noreply.github.com>,3
[TE][Fix] Comparison of the output tensor (#9829)* [TE][Fix] Comparison of the output tensor* fix hybrid op issue* fix tensor replacement in schedule ops* fix compute inline,0
[TIR] add support for multi-blocking layout and their transformation (#9996)* add ceildiv and shapediv* add boundary checking in layout_transform* support multi-blocking and shape padding* refine the log for shape transform* add test for multi-blocking layout transform* delete unwanted comments* remove workaround* fix lint errors,0
[Arith] Support integer BufferLoad in IntervalSetEvaluator (#10327)* Deal with BufferLoad* Test for BufferLoad* Fix comments and CI,0
[docker] Update CI to Python 3.7 and Ubuntu 18 (#10247)This updates the docker image build to use Python 3.7 and Ubuntu 18 as discussed in #9703. The update is mostly straightforward except that the `apt` boost isn't built with 3.7 so we must now build it from source.,1
Generate correct output tensor names in C Interface API (#10191),5
"[ci] Add auto-updating `last-successful` branch (#10056)This adds a script that runs on a cron to discover the last commit where CI all passed (every job was successful and `tvm-ci/branch` is included) and updates a git tag `green` to point to this commit on `main`. This can be used for checking out the latest unbroken TVM, which can be useful for developers wanting a good changeset to base their changes on or for infra needing a clean, up-to-date TVM.",1
[microNPU] enable USMP (#10022)This commit enables USMP in the microNPU codegenand tests. The microNPU codegen is modified tosupport Let nodes that are produced as from USMP.,3
[Docs] Fix an irrelevant sentence in relay.reverse (#10331)It seems the sentence is from relay.repeat() and not related torelay.reverse().,0
"[Relay] Fix TFlite frontend for unpack, stridedslice (#10333)We found this while converting an RNN model.The relay tflite frontend use squeeze at converting unpack, but when theunpack.axis=0, `None` is passed to relay.squeeze(), which would squeezeall dimensions with length 1, causing different results from TFLite.A possible fix might be, assign the unpack.axis as-is to relay.squeeze()As for stridedslice, when the tflite frontend handles shrink_axis_mask,the wrapped `begin` should be used, instead of the original one whichcan be negative. It can cause errors athttps://github.com/apache/tvm/blob/d65ff6594d4d6db0062537a1d43c0504173b8e5c/include/tvm/topi/detail/strided_slice.h#L140Related cases are also added to the python test.",0
[TIR] Fix Ramp int32~64 mismatch in VectorizeLoop and NarrowDataType passes (#10172)[TIR] Fix Ramp int32~64 mismatch in VectorizeLoop and NarrowDataType passes,0
[BYOC-DNNL] add support for more ops and fusion patterns[BYOC-DNNL] add support for more ops and fusion patterns,1
"RelayViz interface and terminal ast-dump (#10085)* RelayViz interface and terminal ast-dump.This PR follows https://github.com/apache/tvm/pull/8668, with splittingout interfaces class and terminal ast-dump implementation.This visualizer is aimed for quick look-then-fix, so the interface issimple. Despite that, customization is still possbile throughimplementing interfaces defined in `interface.py` or overriding existentimplementations inside a renderer module, like `terminal.py`.A tutorial is also provided in this PR.A graphviz renderer will also be contributed after this PR.* lint and typo",0
[ETHOSN] Remove the compiler library from the runtime link (#10334)Due to some restructuring of the Ethos(TM)-N driver library it is nolonger necessary to link the compiler library (AKA Support library)into the runtime.,4
[Hexagon] Export `ir_lower_vtcm_pass` function in the init file (#10330),2
"[runtime] Add Metadata classes for AOTExecutor (#10282)* Add new Metadata classes and base implementation. * These were autogenerated in the original PR, but checking them in   as plain code until we can revisit the auto-generator approach.* address masa comments* Add documentation per Manupa's comments, and move kMetadataVersion namespace.* remove get_name function, used for debugging* clang-format",0
[ONNX] only broadcast matmul if the shape has changed (#10321)* [ONNX] only broadcast matmul if the shape has changed* fix copy-pasta mistake,0
"[TIR] Tir constants integration into compilation pipeline (#8509)* [TIR] Introduce tir.allocate_const to TIRThis PR is adding non-scalar constant representation in TIR. This is used toexpress constants (i.e., parameters) in the TIR instead of bypassing theTIR as it's done until now.Change-Id: Id3afc4d7197260cb43ecde60f05ccbce3fc42430Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: Id4a09a637c9c1fd7d49989c6c10f474a78569e18* [TIR] Integrate tir constant nodes in compilation pipelineThis PR integrates tir.allocate_const to the compilation pipeline to support --link-params.Change-Id: Ic8d0cb75d596299fcae7078b304598afbf0c5494Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>Change-Id: Id98cc682bbfacfe75c4d8b260fd41658f1f196b2* [TIR] tir.const extractionThis commit tries to implement an amendment to tir.constant RFCwith centralized storage of constant data within the IRModulePlease note that data and irmod_storage_idx are not mutual exclisivefurther more the irmod_storage_idx is valid only immediatly afterprim func addition to the mod or after update within the mod.If prim func is out of the the module scope then the index becomemeangless. irmod_storage_idx also is not used in calculation of hashfunction of the tir.constant node.Change-Id: I40742ed580468b0252ea3fec02184cba65e20871* unit test fixedChange-Id: Ied2186554d4cbad44b2346216c8be92449e55732* cmsis-nn codegen fixNow handled case when params of the functions came as constantsChange-Id: I5874e182e34ef94e23048eaf3c61b01a56d91131* Fixes for unittestsChange-Id: I5b82ee3f80337155706b5470973f494a301b5d90* Rebasing tests fixesChange-Id: I94ac87907081bab53c1dd1ab2db106ae057b4b19* Linter: added method param descriptionChange-Id: I2f8c4c8d244b74c794abaa6079c46cc593ffcbdb* Printing removal fixThis patch removes forgotten print in fuse_opsChange-Id: I4bb5934f3b4cd5fde19d36a8e3319aae136bce8a* BugfixFixed concurrent map update bug hereChange-Id: Ifec3bf5030086d9079b9e493096f17dfd82297ec* Reworked logic for not to introduce empty constant list to modue attrsChange-Id: I082c85b3b4b70c218f0d714f5613ef6e178bd020* Added support for tir builtin::tvm_access_ptrThis fixed unit tests for tests/python/integration/test_arm_mprofile_dsp.pyChange-Id: I10919f301ef9ddc3fd87f0e1a8414e9a52fc7938* Unit test fixFixes unit tests in torch frontendChange-Id: I6c179834f93dd202605d1ce5a7f07d987b9dc469* Addressed requested changesAddressed changes requested upstreamChange-Id: I741e52b89eb285732c23b1ac7ff277e757a088c3* Namespace usage changed to conform earlier C++ standardChange-Id: I1b29238cfe2a6bedb525f4f823a3a540f631d836* BugfixChange-Id: I57a44b714b307278a243817ec2864e53ad31366b* updated IRModuleNode::ExtractPrimFuncConstantsUpdated IRModuleNode::ExtractPrimFuncConstants as perrequest upstream.Change-Id: I35db0145fb5827efd0445ce665d0c99465274016* Minor changestypo fixdrenamed ExtractPrimFuncConstants to ExtractConstantsremoved getters/setters from FuseMutator and added parametrizedconstructorChange-Id: Ib2326805781779b88c963a8642ff683c8755956e* Moved LinkedParam/LinkedParamNodeMoved LinkedParam/LinkedParamNode from tvm::tir namespace to tvmnamespaceChange-Id: Ie3f0303bd4f7890c6d680268c91f2051977bc7f4* Addressed upstream commentsChanged BindParams argument to Array<NDArray>Removed 'name' argument from te.constSwitched to in-depth comparision of NDArrays in constant de-duplicationRemoved extra final comma from NDArrayToTIRChanged return type of ConstantAllocationSize to int64_tMade link_param a tvm.testing.parameter for test_fuse_take and test_fuse_gather_ndChange-Id: I4285099cc63756aa5ebe91a5bd207d4135499b41* Removed unnecessary forward declaration+linterChange-Id: I2a6c0d1f97773aeb1ae3f458da252a22079ccdb1* Constant extractor now is a separate passChange-Id: Ia4adca9d3315b26fbdc006ef7c115900c081e303* Added forgotten file + unit test fixChange-Id: Ice305f4fefd13fe95e97574e6d63ffeb664621df* Changed to IRModule passRefactored ExtractPrimFuncConstants to IRModule pass.deDup -> DeDupRefactored logic of Applicator supplementary classChange-Id: I6c120d175eb6790ba90f176c4f856bde8f0c7c94* bugfix after rebasingChange-Id: Ie3ee6ea2479476a30f486baef74f20070f117942* -v -> -vv to have more debug informationChange-Id: I12c63731663b9c9ea574b9ed5cb17311ba3cf701Co-authored-by: Giuseppe Rossini <giuseppe.rossini@arm.com>",0
Simple workaround for PyTorch symbol crash problem in meta schedule test (#10342)* Simple workaround for PyTorch symbol crash problem in meta schedule test* workaround for CI,3
add reading of nRF5340 DK product ID to determine which COM port to use (#10304),1
"[ARM_CPU] Conv2d int8 intrinsic for cortex-A72 (#10310)* [ARM_CPU] Conv2d int8 intrinsic for cortex-A72Add an intrinsic that performs a dot product of 8 4-element vectors atonce. Also conditionally inline fused operators into the mainconvolution loop depending on convolutions size. Small convolution = noinlining. Performance improves by ~20% on mobilenet on raspberry pi 4and ~30% improvement on performance for the individual convolutions.* ignore incorrect lints* fixup fstring* revert changes to conv2d_NCHWc (not int8)* remove error check, apparently tests rely on it* refactor alter op layout",0
[CI][Hexagon] Add Hexagon Tests to pipeline (#10302)* Add hexagon tests to CI Hexagon* Fix CRT libs* cleanup and fix Jenkins* Address @areusch comments,0
[TIR] Misc minor updates (#10335),1
[CUBLAS] Fix cublas batch matmul strategy plevel (#10351),0
"[CI] Re-introduce redirect follow and update hash for Boost download (#10343)Looks like we did need the redirect in (#10247), otherwise you get ablank redirect response and `tar` doesn't like that very much:```tar: This does not look like a tar archivegzip: stdin: unexpected end of file```",1
Add per channel quantization to QLinearConv and fix related bugs (#10354),0
[CI] Fix Flaky Test `test_task_scheduler_gradient` (#10360)* [CI] Fix Flaky Test `test_task_scheduler_gradient`A change to fix the issue of flaky test mentioned in #10356 by increase the `chain_rule` factor and avoid small gradient.* Retrigger CI.,0
[TOPI] VNNI support for batch matmul (#10332)* add test* compute added* schedule works* reuse dense_vnni schedule* try an alternative approach to scheduling layout transform* introduce a tunable knob to decide if compute_root* check transpose condition* support s8 + s8 input* pylint,1
[TIR] TIR Schedule Misc Update (#10341)* tir schedule misc update* Trigger Build,1
"[AOT] BugFix of workspace calculation (#10337)Following an investigation from #10022,it turns out, currently the workspacecalculation assumes there would be a singlelowered PrimFunc could be produced perprimitive Relay Function.However, the exception turned out tobe the CMSIS-NN codegen that producesmultiple calls/PrimFuncs in the placeof a single call to single relay PrimFunc.This commit adds changes to workspacecalculation to be done on lowered IRModule.Additionally, changes the test utils tonot to generate any stack allocator codewhen USMP is used to make the tests morestrict.This change also removes the confusing""run_model"" which has semantics identiticalto ""__tvm_main__"" in TIR.",0
"[runtime] Improved log information with function signature (#10326)This PR introduces a function signature printer in the `TypedPackedFunc` part, so that the log information in `detail::unpack_call` will be more complete. This PR allows users to obatin the original function signature when the `detail::unpack_call` fails.",2
refactored GraphProto.from_onnx into smaller functions (#10267)* refactored GraphProto.from_onnx into smaller functions* black formatted file* removed line that does not seem to make sense. Is there a purpose that I missed?* just to trigger CI pipeline,2
"[skip ci] Fix onnx frontend lint (#10363)This was broken in #10267, not sure how that commit passed CI (maybe some logic to figure out the PR diff in pylint is broken).Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[COMMUNITY] csullivan -> Committer (#10364),3
[BUGFIX][ARITH] Fix FloorMod Simplifier (#10336)* fix canonical simplifier* improve comments,0
[Lint] Fix Pylint Issues (#10358),0
[TIR][Transform] relax LoopPartition restriction that the intersection of all conditions can not be none. (#10340)Co-authored-by: sqing <qing.siqi@intellif.com>,5
[ETHOSN] Improved identification of driver library version (#10285),5
[ETHOSN] Stricter data type conversion checks (#10271)The 21.11 update for the Ethos(TM)-N driver is slightly more strict inaccepting various operator attributes.,1
[microNPU][4] Add the cascader Proposal generator (#9959)* [microNPU][4] Add the cascader Proposal generatorThe Proposal generator takes optimal Plans and combinesthem to find optimal 'Proposals' - sets of disjointPlans that cover every Part in a CascaderGraph. Itultimately produces a Pareto-frontier of 'optimal'Proposals in terms of estimated cycles and memory usage.Change-Id: Id42099819a596496a5769bae22f08eeb75ec69b6* FixesChange-Id: I4f5f2a298bd3bb379c7c8d179150358923b0dd66,0
[Runtime][Pipeline Executor] multiple threads management and the data forwarding notification mechanism. (#10234)* [Runtime][Pipeline Executor] multiple threads management and thedata forwarding notification mechanism.In this patch we create working threads for each runtime of pipeline.the threads would be terminated once the runtime class gets destroyed.We also add a notification mechanism derived from the 'binding configuration'of the runtime to forward the data notification.* address review comments.* address review comments.* fix typo.* fix typo.* trigger build.* address review comments.* address review comments.* address review comments.* address review comments.,0
"[Hexagon] RPC server/client for simulator (#10361)This is the C++ code for running Hexagon code on simulator via theRPC mechanism. It is intended to be integrated into the currentHexagonLauncher, although the integration will require further changesto the launcher python code.The final goal is to be able to run the same file.py on eitherhardware or simulator without needing to edit the python file, butsimply by changing the configuration of the execution platform(i.e. something like --exectute-on=simulator as a command line orin an environment variable). The exact details are still to bedetermined.",2
"[TIR, Relay] improve bfloat16 support (#10112)* update AMP table to enable ResNet50 conversion* add runtime datatype dispatch for BFloat16* skip asserts for uint16 for bf16 compatibility* add bf16 cast for the unary intrinsic operators* enable ""bf16<-->fp32<-->any dtype"" casting* support inconsistent input for bf16 BIOP legalize* add treatments for bfloat16 in if statements* add bfloat16 dtype casts in binary OP* delete unnecessary treatments for bfloat16* add test for bfloat16 building* code style* restore the modifications in .gitignore* restore the changes to AMP lists* fix typos* fix lint errors* fix typo",0
"[ci] Check more events before pinging reviewers (#10208)* [ci] Check more events before pinging reviewersThis was missing some events before (reviews without comments, PR updated from a draft -> ready for review) so these were being ignored when finding the latest event. This PR adds them and restructures the code a bit to make it more clear what is happening for each PR. This addresses some of the issues from #9983* fix testsCo-authored-by: driazati <driazati@users.noreply.github.com>",0
Lower cache_read and cache_write to Hexagon DMA via tensorize (#10365)* Lower cache_read and cache_write to Hexagon DMA via tensorize* rework test to be compatible with launcher* remove cpu device api mem_copy implementation and test,3
[microNPU] adding more tests with USMP (#10362)Adding a few tests to confirm memory usagewith and without USMP.- Supporting the toggle to disable storage_rewrite.- There is a slight change to tir_to_cs_translator to   add index of Load nodes associated with NpuAddressRange objects,1
[RELAY] [VIRTUALDEVICE] Change syntax for device planning and store parameter virtual devices in virtual_device_ field (#10352)* parent 33082e0032fb57b0516ad7e3eabd11fe0203437eauthor electriclilies <lilyorthsmith@gmail.com> 1643141097 -0800committer Lily Orth-Smith <lilyorthsmith@gmail.com> 1645560059 -0800Store function param virtual devices in virtual_device_ fieldFix test_annotation.py and change result_virtual_device to virtual_device* Change plan devices tests to use the new syntax for function parameters* Fix free var problem* Fix attribute parsing if there is virtual device; most device planning tests passgit status* fixed lambda lifting* Debugging high order functions -- right now FunctionOnDevice and Bind are mutually recursive. This needs to not be the case.* tests pass wootgit status* Remove FunctionOnDevice from device planner* Don't use MaybeFunctionOnDevice in VM compiler* Remove MaybeFunctionOnDevice from lambda lifter* Delete FunctionOnDevice and MaybeFunctionOnDevice!* Reomve GetFunctionResultVirtualDevice* Remove GetFunctionParamVirtualDevice* lint* lint* Python formatting* Remove FunctionOnDevice python test* Fix bug in binds & debug output* Fix text printer* lint* Remove function on device from fold constant tests* Mark nits* Revert behavior of bind* clean up debug* Make ExprBinder public interface and use instead of Bind* Fix lambda lift* This is broken but not sure how to fix* passes all device planning tests yay!* Add substitution helper and use in device planner* Remove unnecessary check* Respond to comments* Update comment,0
"[VirtualMachine] new method allowing to set one input tensor by its index or name (#10293)* set_input_with_index was implemented for VM* clean code* add getInputIndexFromName. add function descriptions. lint fix* fix lint* transfer comparison of parameter names number and assigned devices number to VMFunction constructor* add GetVMFunctionWithName to Executable API* clean code* add SetInputWithName (set_input_with_name) to VM API* join SetInputWithIndex and SetInputWithName to SetOneInputTensor (set_one_input) to VM API, the joined methods were removed* fix lint* some fixes after review* add set_one_input method to python API of VirtualMachine* pytests for set_input and set_one_input methods of VirtualMachine were implemented and checked* CI restart* construct simple model for pytests by relay instead of onnx tools (need for correct CI)Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>",0
"[Hexagon] Replace strlen in constant initialization with sizeof (#10381)Strlen is not constexpr everywhere, so replace it with sizeof.In C++ sizeof(""string"") works fine, since ""string"" has type""const char [...]"".",5
check to avoid crash in opt_level=0 vm build (#10347),5
[DOCS] Add how to contribute TVM docs with images. (#10287),1
[MetaSchedule] Update Tuning Interfaces. (#10367)This PR is further improvement of the meta schedule project (https://github.com/apache/tvm/issues/8473).Co-authored-by: Junru Shao <<junrushao1994@gmail.com>>Co-authored-by: Bohan Hou <<32121147+spectrometerHBH@users.noreply.github.com>>Co-authored-by: Ruihang Lai <<lairuihangdongdong@qq.com>>Co-authored-by: Hongyi Jin <<3231950289@qq.com>>Co-authored-by: Wuwei Lin <<wuwei@apache.org>>Co-authored-by: Siyuan Feng <<Hzfengsy@sjtu.edu.cn>>,1
"[Bugfix][TVMScript] Convert BufferSlice to BufferLoad when used as range/loop start and end (#10370)A quick fix of the parser issue mentioned in #10327 .Ranges and loops require `start` and `stop` to be PrimExpr, however, `BufferSlice` is not always scalar so it's not a `PrimExpr`.This PR performs the transformation.",0
"[FIX,PROFILING] Add extra precision to numbers when serializing to json (#10392)Numbers were serialized with too little precision when serializingprofiling reports to json. Deserialization can then sometimes round thenumber differently than if the full precision was available.Fixes #10382.",0
Fix plint error. (#10394)plint complain error in parser.py and test_vm.py just fix it.,0
meta schedule misc update (#10389),1
Fix tvmc run error message when inputs aren't found. (#10017),0
[Runtime][PipelineExecutor] Polish the name and comments of variable. (#10395)Polish comments and variable name,1
Enable groups argument for conv2d_transpose on the cudnn backend (#10396)* wip* reset conv2d_transpose topi conv_mode to 1* fix for 'Error: identifier “hfabs” is undefined'* address @masahi's comments in pytorch test_forwardCo-authored-by: Masahiro Masuda <masahi129@gmail.com>,0
"Fixed a bug in the convert_fully_connected() function (#10371)In case we need to change the output shape, need to convert the output_shape tuple to list before the change.",0
[TensorIR] Renormalize split pattern (#10401),5
[MetaSchedule] Arithmetic analysis (#10403)This PR changes the normal form of the affine detector and supports a single var predicate. It also enhances ModularSet detector to enable floor mod patterns.,4
"Add @slow decorator to run tests on `main` (#10057)* Add @slow decorator to run tests on `main`This adds the infrastructure discussed in https://discuss.tvm.apache.org/t/rfc-ci-skip-slow-tests-on-prs/11910, but without affecting any tests. As we investigate reasons behind [slow tests](https://gist.github.com/driazati/e009f09ff44c6bc91c4d95a8e17fd6f1) in CI, this decorator will allow us to move these to run only on `main` and not PRs after checking with all concerned parties.* cleanupCo-authored-by: driazati <driazati@users.noreply.github.com>",1
"[microTVM] Zephyr: refactor _find_openocd_serial_port (#10346)Refactor _find_openocd_serial_port() as a generic USB serial portfinder since other runners beyond openocd use it (e.g. jlink runner).Also instead of using redundant hardcoded values in BOARD_USB_FIND_KWdict, use idVendor and idProduct from boards.json. And don't use 'usb'module to first find the serial number of the port and then pass it to'serial' module to obtain the port path, instead search for the portpath directly via 'serial' module using the serial number (if provided)or use idVendor and idProduct values taken from boards.json.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",2
"[microTVM][RVM] Skip USB device attach if device is already attached (#8737)* [microTVM][RVM] Skip USB device attach if device is already attachedCurrently, when the VirtualBox provider is selected, if base-box-tool.py'test' command is used and a VM is already running with the USB devicenecessary to perform the tests already attached to it the command failsbecause it tries to blindly attach again the USB device without checkingif device is already attached.The failure can be reproduced by first running a VM for testing (thetests need to fail and leave the VM running):$ ./base-box-tool.py --provider virtualbox test --microtvm-board=stm32f746g_discothen one tries to re-run the tests without building the whole VM again:$ ./base-box-tool.py --provider virtualbox test --skip-build zephyr --microtvm-board=stm32f746g_discoThis commit fixes that error by checking and properly skipping the USBdevice attach if it's already attached to the VirtualBox VM.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* areusch review: Use --machinereadable for the outputUse 'showvminfo --machinereadable' output to parse for more robustnessto updates in VBoxManage.",0
Realize the function op during forward rewrite (#10410),5
"[ci][1/2] Shard `frontend: GPU` job into 2 jobs (#10413)This is the longest individual CI job by about an hour, meaning everything else is usually done and waiting on this job for a while before the entire build completes. This PR breaks it up into two roughly equal jobs (based on timings in https://ci.tlcpack.ai/job/tvm/job/main/2623/testReport/, both should take about 90 minutes). If capacity is available, this means CI jobs could potentially take 1 hour less. If not available, besides an insignificant queueing delay this PR has no effect.This is a two part PR since the Jenkinsfile changes cannot be bundled in this PR, so they will need to be in a follow up.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>",2
"RelayViz Graphviz renderer (#10400)Following https://github.com/apache/tvm/pull/10085, this PR adds agraphviz backend. It requires python `graphviz` package and `dot`executable in the PATH, similar to `tedd.py`.This implementation is much like a porting of `visualize` function inhttps://tvm.apache.org/2020/07/14/bert-pytorch-tvm, except that`node_attr_dict` is replaced with a callback `get_node_attr`.`get_node_attr` can be somehow used to emphasize a set of nodes.It might be useful if we encounter problems in inferencesand want to find nodes with certain types and attributes.An example is provided inhttps://github.com/chiwwang/tvm/blob/graphviz_renderer_example/test_viz.pyIts outputs are (conv2d with NCHW layout is green-colored):https://github.com/chiwwang/tvm/blob/graphviz_renderer_example/mod_with_subgraph.pdfhttps://github.com/chiwwang/tvm/blob/graphviz_renderer_example/mod_wo_subgraph.pdf",1
"[Runtime][ThreadPool]Refactor affinity function and support CPU affinity list setting. (#9802)* [Runtime][ThreadPool] Refactor affinity function and support CPU affinity list setting.Issue:1. There are multiple affinity function using ""LINUX"" and ""ANDROID"" macrocheck and the multiple check make the logic maintain and change becomecomplex.2. Current logic of tvm [Runtime][ThreadPool] assume all of the cpu resources are available fora single backend runtime to do the data flow computation. But such assumption may nottrue when user running multiple task on the system and not want tvm taskexhaust all of the cpu resource, or when user going to run multiple backendruntime of tvm on the system, each backend runtime of tvm should use different cpuaffinity settings to achieve best performance.Solution:1.Refactor the affinity functions to move the ""LINUX"" and ""ANDROID"" checkinto one function.2.In this solution, we introduce a new ""CPU AffinityMode type"" named ""kSpecify"", by using""kSpecify"" and the function named ""tvm::runtime::threading ::Configure"" user can specifythe cpu list for the cpu affinity of a backend runtime.This solution reused the existing per thread thread pool logic of [Runtime][Threadpool] thatcreated a worker thread pool for current thread which can running a particular runtime. for a multipleruntime use case, user can first launch multiple threads, then call ""tvm::runtime::threading ::Configure""with cpu list to create tvm data flow worker thread pool, after doing this the execution of the multipleruntime on the multiple threads will use different cpu resource list.* fix windows build issue.* fix build issue.* fix build issue.* fix windows build issue.* fix plint issue* polish comments.* address review comments.* address reivew comments.* address review comments.* address review comments.Co-authored-by: hua jiang <hua.jiang@xilinx.com>",0
"[CI][1/2] Update the Python version of pyxir (#10406)Currently the CMake file for pyxir is looking for things in Python3.6,so it needs to be upgraded to use 3.7 now that we have moved to use 3.7.Otherwise the build fails when the docker images are updated since the3.6 can't find the pyxir packages which have moved to 3.7.Additionally, there seems to be a problem with the newer version ofsetuptools installing the pyxir libraries, so reverting these versionsto the previous versions as a workaraound.Note that this has to be done in two patches for the changes to gothrough the current CI, this patch downgrades the pip and setuptoolsversions.",1
Modify debug output (#10372)1. Modify debug output to make it more readable3. Replace magic number with a variable `error_ct_threshold`3. Add function to set error counter threshold externally for debug purposes,0
Fix relative include path (#10402),0
[ci][2/2] Shard `frontend: GPU` job into 2 jobs (#10414),5
[TensorIR] Update VerifyGPU (#10405)* update VerifyGPU* address comments,1
[Bugfix][Arith] Fix TryFuseIter (#10427),0
Lily -> Committer (#10417),5
"Add group_conv2d_transpose_nchw to CUDA backend (#10423)* add group_conv2d_transpose_nchw to CUDA backend* simplify significantly, just add groups argument to conv2d_transpose_nchw",1
[MISC] Add miss Type2Str and remove compile warnings (#10430)* [MISC] Add miss Type2Str and remove compile warnings* fix lint,0
[cleanup] Log compile errors for AOT tests (#10214)* [cleanup] Log compile errors for AOT testsSee #10213* Update tests/python/relay/aot/aot_test_utils.py* removed the encode of msg that is already strCo-authored-by: lhutton1 <luke.hutton@arm.com>Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: Manupa Karunaratne <manupa.karunaratne@arm.com>Co-authored-by: lhutton1 <luke.hutton@arm.com>,0
"[skip ci][CI][Fix] Fixing lint (#10445)A linting issue was introduced in #10423, fixing this up.Change-Id: I06c518194e30dcaa755005f06b8b7280c237d386",0
"[CMSIS-NN] enable USMP with CMSIS-NN (#10224)This commit mainly enables the USMPwith CMSIS-NN codegen.In order to do that, CMSIS-NN functions neededto contain BufferMaps. This commit adds the necessaryBufferMaps as well.All the tests are modified to run with USMPwhile the networks tests run with and withoutUSMP.",1
Fix plint complain for some files. (#10433),0
"Fix a Uninitialized Variable Warnings. (#10436)There is a 'Uninitialized Variable' Warning in building process, just fix it.",0
[Frontend][TFLite] Added broadcasting to prelu alpha. (#10435)* Update prelu test cases* Add broadcasting to prelu alpha,1
[Relay] Fix shape func for strided slice (#10418)* fix dyn strided slice* add tests* remove stuff* jostle ci* jostle ci* jostle,0
[skip-ci][COMMUNITY] leandron to PMC (#10448),3
"[Hexagon] Allow execution on target or simulator from HexagonLauncher (#10454)Setting ANDROID_SERIAL_NUMBER=simulator will execute the tests onsimulator instead of a hardware device.This patch also introduces an environment variable HEXAGON_RPC_LIB_DIRto specify the location of the hexagon_api binaries. If unset, thecode will look for the binaries in the same way as before this patch.",2
[microNPU][5] Convert Proposals to te.Schedules (#10062)* [microNPU][5] Convert Proposals to te.SchedulesChange-Id: I6771578f1007b8fea02e2dec7d0c797a6ef6aa5e* FixesChange-Id: Id062ca7793656be4e870ac48ba41a34aa83276d2* Fix testChange-Id: Ib0fd55b99459c26425e1805df19d12367244e1b0,0
hot fix (#10464),0
"[ci] Add workflow to cc teams (#10322)As discussed in https://discuss.tvm.apache.org/t/rfc-remove-codeowners/12095/2?u=driazati, this adds a mechanism to auto-tag people based on PR/issue titles and labels. This should improve visibility across the project and make it easy for interested people to subscribe to various topics.Details on usage will be posted in the relevant issue: #10317Co-authored-by: driazati <driazati@users.noreply.github.com>",1
just a typo fixed (#10442)* minor typo fixed* to trigger CI* to trigger CI* fixed formatting issues* black formatted file,0
"[runtime] AOTExecutor implementation and c target code-generator (#10283)* Add memory pools to Metadata classes.* Move ShapeToJSON to utils.* Track returned TensorType from AOTExecutorCodegen.* Support calling Relay functions with Tuple.* Expand supported TIR calling conventions to work with C++ runtime.* Rename MetadataModule to ConstLoaderModule.* Add runtime AOT executor module.* Add AOT code-generation.* Add a runtime Module to mux between .text Metadata and live Metadata.* Move launch_param to namespace* Add test of c++ AOT.* Fix incongruity between kTvmRuntimeCrt constant* Expand ExecutorCodegenMetadata to include AOT runtime metadata.* commit cpp test* Make Metadata compile under C.* Ignore ephemeral metadata_module export_model_library_format. * This module does not need to be exported, since it is merely a C++   wrapper around get_c_metadata, and get_metadata is not used in C.* address manupa, kparszsyc, masahi comments.* further address comments* clang and python format* Fix broken test* Address lingering comments from masahi, kparszyzc",0
"[Runtime][ThreadPool] Handle the default value of affinity mode. (#10434)* [Runtime][ThreadPool] Handle the default value of affinity mode and acorner case of function 'SetMaxConcurrency'. 1. Handle the default value of affinity mode. 2. After calling the function 'SetMaxConcurrency' with a non-zero value,    if calling the function 'SetMaxConcurrency' again with a zero value ,    then the second setting can not correctly set the max_concurrency value    into zero.    use new logic to fix this issue.* address review comments.* polish the warning message.",0
[Relay] Fix output dtype for conv2d wgrad when the original one is void (#10459)* [Relay] Fix output dtype for conv2d wgrad when the original one is void* fix cpplint* also add out dtype information to dgrad* also use out_dtype for wgrad* remove redundant import,0
"[skip ci][ci] Remove -i from lint scripts (#10469)This was changed in #8509 to run without checking the file formatting, which would lead to pylint errors like we saw on `main` in https://github.com/apache/tvm/commit/0c836b73ffd9669bcc416515dce6436cbd7d7ebe.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
Modify Jenkinsfile to prevent builds from triggering on branch indexing (#10432)Co-authored-by: Noah <nkontur@octoml.ai>,2
[skip ci][ci] Skip actions on forks (#10468),5
[ci] Use available CPUs in builds (#10359)* [ci] Use sccache in builds* trigger ci* updateCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[ci] Fix slow test script permissions (#10457)This is failing silently, e.g.: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10359/4/pipelinecc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[runtime][Hexagon] AOTExecutor implementation for C Codegen (#10311)* Hexagon AOT tests work* fix and address comments,0
[microTVM] Zephyr: add B-U585I-IOT02A board support (#10416),1
"[MetaSchedule] Fix Cyclic Dependency in PyClass Family (#10368)Following the design of module_pass, we developed a mechanism, a decorator named derived_obj, to systematically allow derivation from TVM objects in pure Python and being passed into any language, without cyclic dependency. This PR introduces the new mechanism to all PyClasses in meta schedule.",0
[Hotfix] Black format (#10482),0
"[MetaSchedule] Keep Task / Trial / Iter / Postproc Number Consistent in Log (#10478)This PR fixes some inconsistency in log printing and make sure all numbers start from zero for tasks, trials, iters and postprocs. I think it's better for debugging if any task or trail went wrong in the future.",0
"[Torch] fix torch version check (#10481)old code checkout ""1.10.2"" greater_than ""1.5.0"" if false, fix it",0
[microNPU] Remove unused code from testing infra (#10462)Removing some legacy code from infra.py that is not called by anything.,3
[MetaSchedule] Enable AutoTVM-style template-based search space (#10461)* [MetaSchedule] Enable AutoTVM-style template-based search space* Fix lint* suppress mypy,0
[MetaSchedule] update misc parts (#10444)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[Arith] Handle mod/floormod in modular set analysis (#10453),5
Correctly enable architecture extensions in CMSIS-NN Zephyr Demo (#10458)* Correctly enable architecture extensions in CMSIS-NN Zephyr DemoWithout `CONFIG_FPU` being set the correct architecture extensions weren't being applied which means the buffer sizes didn't necessarily match up - this corrects it so that they align.* Fix memory allocation in demoThe stack allocator forcibly aligns memory by removing parts of it which causes there not to be enough memory and the CMSIS-NN integration uses more stack than the demo with pure TVM operators (we should look to remove some of our stack usage),0
[Minor][Build] Fix compiler warnings hidden overloaded virtual function (#10485),0
[Refactor] Expose meta-schedule related packed func in a header and call it directly (#10470)* [MetaSchedule] Expose CreatePrimFuncFromOutputs in a header and callit directly* add include guard* exposed ContextQueryInsideWithScope too* oops* add tir namespace for clarity* address comment,1
[ci] Skip CI based on globs (#10456)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
[skip ci][ci] hotfix Jenkinsfile (#10492)This was broken by #10456 since that did not have a case for ignoring the `main` and staging branches. This adds it at the Jenkins level so the further scripts won't even be run if not necessary. This is marked `skip ci` but since the changes all happen before lint this is being tested.commit-id:1655b8daCo-authored-by: driazati <driazati@users.noreply.github.com>,0
don't rely on cudnn for compilation (#10495),5
[ci] Disable flaky tuning test (#10490)See #10489Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[ci] Disable flaky ethosu tests (#10488)See #10487Co-authored-by: driazati <driazati@users.noreply.github.com>,3
"[CodeGen][CUDA] use hrint for cuda half rounding (#10460)When cuda c codegen generate `tir.round` for fp16, there is no function named `hround`, but `hrint` for cuda half arithmetics. https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH____HALF__FUNCTIONS.html#group__CUDA__MATH____HALF__FUNCTIONS_1gbbf7a989130edcbdbfbb4730f61c79b1Testcase to reproduce:```pythonimport tvmfrom tvm import relayfrom tvm.ir.module import IRModulex = relay.var(""x"", shape=[16], dtype=""float16"")y = relay.round(x)f = relay.Function([x], y)m = IRModule.from_expr(f)m = relay.transform.InferType()(m)relay.build(m, target=""cuda"")```",2
"[ETHOSN] Implement tanh operator (#10486)Adding compiler support for TANH operator, which is based onan underlying pattern matching scheme.One negative test is included as well.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>",1
[Hexagon] Fix scripts to enable automated testing on hardware (#10491)* Fix test and scripts for HW* revert port forwarding* address comments* address comments,0
"[TIR] Fix Tensorization IR-Comparator for Annotations (#10498)This PR fixes the way of comparison in which the tensorization IR-comparator deals with annotations.Prior to this PR, the comparator requires the annotation values from LHS and RHS to be exactly the same, which is, in fact, never possible. And this PR removes this comparison requirement (with a regression unit test).```c++bool TensorizeComparator::CompareAnnotation(const std::pair<String, ObjectRef>& lhs,                                            const std::pair<String, ObjectRef>& rhs) {  if (lhs.first != rhs.first) return false;  if (!lhs.second.same_as(rhs.second)) return false;  // <== The values would never be the same.                                                      //     Thus this line should be removed.  return VisitExpr(Downcast<PrimExpr>(lhs.second), Downcast<PrimExpr>(rhs.second));}```",0
[Hexagon] Enable running CI tests via simulator (#10473),3
[MetaSchedule] Refactor testing workloads (#10497),3
"[CI] Remove `llvm -device=arm_cpu` and `cuda -libs=cudnn` from the default test target list (#10500)After recent improvement in GPU frontend tests, I found that `topi: GPU` has become a bottleneck. From the log https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10391/14/pipeline/319, it is clear that topi tests are running on target `llvm -device=arm_cpu` and `cuda -libs=cudnn`, which I claim is completely redundant since we already run on `llvm` and `cuda` targets. In https://github.com/apache/tvm/pull/9905, I've already removed them from `DEFAULT_TEST_TARGETS`, but `topi: GPU` uses its own list of targets which still includes `llvm -device=arm_cpu` and `cuda -libs=cudnn`. I propose to remove them from topi test targets, which hopefully will cut topi GPU tests time by half.",2
[MetaSchedule] Update scripts for subgraph tuning (#10501),1
"[AOT] Rerun FVP test incase of first attempt failure (#10408)* [AOT] Rerun FVP test incase of first attempt failureIn light of the flaky failures seen when running these sorts of tests(issues: 10300 and 10314), adding a second attempt if the first attemptfails to try to help reduce the number of failures observed in CI.Change-Id: I25eb268555baab85fc7e70d2d70a37f3be49a54b* apply same logic to CMSISNN tests as wellChange-Id: I0afb54676e83af06fcd9ab2cc1fc2e87002031cd* rebase and print errors to stderr to capture in pytest logChange-Id: Ibedff4515050421d98462bfbd95d5cdf77b12412* reduce scope of retryReduces the scope of the retrial to only the portion that runs thetest module on the FVP.Change-Id: I4520f50aea5cc8e28c03b49da18612cc7f8b8045",0
"[CMSIS-NN] Only run ScalarToTensorConstants pass on CMSISNN external functions (#10375)* [CMSIS-NN] Only run ScalarToTensorConstants pass on CMSISNN external functionsEnsures the `ScalarToTensorConstants` pass only runs on functions withthe ""cmsis-nn"" compiler attribute. Previously, it was possible for partsof the pass to run on all of the input. For example,`ReplaceScalarWithTensorVariable` was running on all OpNodes withoutregard to the containing function. As a result, this change meansthat the pass now visits each CMSIS-NN function separately, rather thanvisiting the body of the main function.Change-Id: Ia4da2246952aa7e3514264f1ed1a77a0916deb95* Change variable names so main function is not mentionedSince the main functions is no longer accessed, the variable namesmentioning main don't make sense. Correcting this.Change-Id: Ic1f8170b5a73453be27e70d533a41ef7eb58f485",4
Fixed additional deprecation warning in file (#10318),0
"[TE][TIR] Implement layout transformations, non-flat memory buffers (#9727)* [TIR] Added BufferLoadNode::LegalizeDtypeWhen modifying a BufferLoad object, the return dtype must also beupdated.  This exposes the legalization function, so that passes thatuse `BufferLoad::CopyOnWrite` to modify the buffer/indices don't needto repeat the logic to update the dtype returned.* Replacing Store/Load in Stmt/Expr Visitor/Mutator* Removing Store/Load from optimization passes- UpdatePointerStorageScope- UnrollLoop- ThreadSync- LinearAccessPatternFinder- StoragePlanRewriter- VectorTypeRewriter- VectorTypeAccessChecker- NarrowDataType- IRConvertSSA- CompactBufferRegion* Removing Store/Load from examples- ConvertAddToSubtract* Replacing Store/Load in StorageFlattenNow, outputs BufferLoad/BufferStore with a flattened buffer object.temp commit, replacing Store/Load, BufferBindUnwrappertemp commit, replacing Store/Load, StorageFlattener* Replacing Store/Load in utility passes.- StmtSimplifier- IRSubstitute- BaseInliner- FeatureVisitor* Replacing Store/Load in analysis functions- StorageAccessVisitor- VarTouchedAnalysis- MemoryAccessVerifier- InplaceOpVerifier- GPUCodeVerifier- VarTouchVisitor- LCADetector- BlockReadWriteDetector- InstrumentBoundCheckers* Replacing Store/Load in lowering/legalization passes.- MakeCrossThreadReduction- CacheReadRewriter/CacheWriteRewriter- InjectVirtualThread- InjectDoubleBuffer- InjectCopyIntrin- LowerWarpMemory- LowerThreadAllreduce- LowerThreadAllreduce- LowerCustomDatatypes- LowerTVMBuiltin- CoProcSync- MergeDynamicSharedMemAllocations- VectorizeLoop- BF16Legalize* Replacing Load/Store in codegens.- Device code generators  - CodegenC  - CodegenLLVM  - CodeGenOpenCL- Utilities used during codegen  - ArgBinder  - MakePackedAPI  - ReturnRewriter  - SplitHostDevice- Execution environments  - CodeGenStackVM  - CodeGenHybrid  - AOTExecutorCodegen* [UnitTest] Add unit tests to test physical layout remapping.* Updated tvm::address_of() to hold BufferLoad instead of Load.* [TIR] Added IndexMap class.Holds a set of variables representing the input indices andexpressions in terms of those input indices.TODO:- Add validation, the index mapping should be invertible.- Add helper function, apply mapping to a set of indices.- Add helper function, apply mapping to bounds of input indices.* Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.StorageFlatten/FlattenBuffer passes updated to modify thebuffer/indices directly, rather than using vload/vstore.- Primary purpose of vstore/vload is to allow IR written in python to  define vectorized load/store.  This usage is maintained by returning  a BufferLoad/BufferStore node whose index is a Ramp.- Previously, vstore/vload was also used to compute the 1-d physical  index of a location within a N-d tensor.  This usage will no longer  be allowed, as it would not allow layout transformations to be  performed after a schedule definition, but any uses of the buffer  are flattened.* [TE] Added Stage::transform_layout to the C++ TE implementation.Adds an `Array<IndexMap>` in the stage to define the transformationsto be applied on the tensor's layout.  As of this commit, this mappingisn't propagated into the TIR graph yet.* Replace Store/Load with BufferStore/BufferLoad in ir_builder* [TE] Added Stage.transform_layout to the Python TE interface.Allows users to specify `s[A].transform_layout(mapping)`, andpropagate into the TE definitions.* Added pre_flattened_shape/pre_flattened_stride fields to Buffer.The shape and stride checks performed in ArgBinder::BindDLTensor(called from MakePackedAPI) require the tensor shape/strides prior toindex flattening.  Therefore, though it is no longer used by thelow-level code generators, we must maintain that information for usein MakePackedAPI.* [UnitTest] Test N-d indices exposed to low-level codegenWhen using te.AXIS_SEPARATOR in the call to .transform_layout, thisshould define groups of axes, each of which is flattened to a singleaxis, then exposed to the low-level codegen.* [TIR] Added PrimFunc attribute ""layout_transform_map"", filled from TE.Propagated the TE definition of the physical layout into the TIRgraph.* Added pre_flattened_type.If a boolean tensor is backed by an int8 buffer, the check on theargument buffer's type should be against the boolean type.When rebasing this PR, should be placed after the addition ofpre_flatten_shape/pre_flatten_strides.* [UnitTest] Added tests for loop iteration order.After transformation, the iteration order should follow the newtransformed axes.  In addition, the loop iteration variables should beexposed through the TE interface for further manipulation.* [TIR] Added BufferNode::axis_separators- Add axis_separators to represent divisions between groups  of tensor axes, where each group is flattened into a single  output axis, to be exposed to the low-level code generators.- Expose axis_separators to the python interface.- Update existing C++ calls to the Buffer() constructor.* [TIR] Added ApplyLayoutTransforms as part of StorageFlatten.For any buffers that have layout transforms defined in the""layout_transform_map"" attribute of a PrimFunc, rewrite access intothe buffer such that they use the updated ordering.* Update usage of ir_builder where necessary.* [TE] Implement te::TransformSimilar to Fuse and Split, this represents a modification to theexisting loop iterations.* [TE] Added Stage::set_axis_separators.In C++, this is implemented as an `Array<IntImm>`, specifyingpre-flatteneing axes after which a new post-flattening should bestarted.  The python interface uses a sentinel value`te.AXIS_SEPARATOR` in the call to `transform_layout`, which is thenused to define the array of axis separators.* [TIR] Expose tir.transform.ApplyLayoutTransforms for testing* [TE] Rewrite loop iteration orderAfter .transform_layout, rewrite leaf_iter_vars to follow the updatedorder.  Use the te::Transform iter_var relationship to track use ofthe transformed variable.* [TE] Fill BufferNode::axis_separators from StageNodeDuring ScheduleOps and SchedulePostprocToPrimfunc, the axis separatorsdefined in the stage must be passed through to the TIR BufferNode.* [TE] Return transformed iteration variables* Moved Buffer's pre-flatten information to PrimFunc.Since the pre-flatten information is only used for validating userinputs, it makes much more sense to store it alongside the buffer_map.* Updated ethos-u C++ unit tests to remove use of Load/Store.* Bugfix, layout transformation.Error occured during conversion from TE to IRModule, when layouttransforms were applied to a reader of a `cache_read`.* In test directory, replacing all instances of T.load.* Return buffer object from tvm.tir.script.scope_handler.AllocateNow that the load/store require buffer objects, allocation should alsoreturn a buffer object to be used.* Added .astype to tvm.script.tir.node.BufferSliceSince `buf[i]` returns a `BufferSlice`, this lets the TIR examplesthat use `buf[i].astype('out_dtype')` continue functioning.* Replacing all T.store TIR calls.* Added LOG(FATAL) in constructor of Store/Load nodes.* Updated tvmscript parser to report error for Store/Load nodes.* [TVMScript] Added T.preflattened_buffer stmtUsed to specify `PrimFunc::preflattened_buffer_map`. Takes an argumentof the postflattened buffer, so that it will work for both simpledeclarations and `T.match_buffer` statements without needing tointroduce a param handle.  All other arguments are identical to`T.match_buffer.`* [TVMScript] Updated TVMscript for BufferLoad/BufferStore- Use `T.preflattened_buffer` calls in TVMScript to represent  `PrimFunc::preflattened_buffer_map`.- Remove `T.buffer_decl` for return value of `T.allocate`, now that  `T.allocate` returns a buffer.- For buffer access as a different type, make a `T.buffer_decl` for  those accesses.* Updated test_tvmscript_roundtrip.py for BufferLoad/BufferStore.* Updated TIR reference in USMP pool allocation unit tests.Using let var handles as the data pointer in buffers, rather than justas `T.load`/`T.store` arguments, requires annotation as`T.Ptr[T.primtype]`, rather than as `T.handle`.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* fixup! In test directory, replacing all instances of T.load.* tir.ComputeInline, correct variable count.Previously, this metaschedule primitive relied on `tir::UndefinedVars`ignoring the data pointer of BufferLoad/BufferStore nodes.  When`tir::UndefinedVars` was updated to visit the data pointer, similar tothe previous behavior when visiting Load/Store nodes, this caused thecount of undefined variables to be unexpectedly high.* fixup! Replacing all T.store TIR calls.* fixup! Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.* fixup! In test directory, replacing all instances of T.load.* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* Expose Buffer index flattening function to Python.* Updated test_tir_buffer.py offset tests.Replacing calls to `Buffer.vload` with `Buffer.offset_of`, whentesting the index calculations.* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! Updated Buffer::vstore/vload to return BufferLoad/BufferStore objects.* fixup! Replacing Store/Load in lowering/legalization passes.* fixup! Replacing all T.store TIR calls.* fixup! Updated ethos-u C++ unit tests to remove use of Load/Store.* fixup! Replacing Store/Load in lowering/legalization passes.Fix linting for inject_double_buffer.cc* fixup! Updated ethos-u C++ unit tests to remove use of Load/Store.* fixup! Added .astype to tvm.script.tir.node.BufferSlice* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* fixup! Replacing all T.store TIR calls.* fixup! In test directory, replacing all instances of T.load.* fixup! Replacing all T.store TIR calls.* fixup! Replacing Store/Load in lowering/legalization passes.* [UnitTests] Added T.preflattened_buffer in expected result* fixup! In test directory, replacing all instances of T.load.* [UnitTests] Bound checker update, compare against N-d buffer bounds.* Fixup, bound checker vectorize test.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTest] Fixed breakage in InjectRollingBuffer test.Needed a bit more re-writing than usual, because the test wasexplicitly calling lowering passes, then calling `tvm.build`.  Fixedby using the standard lowering flow, with preprocessing stepsinserting with `tir.add_lower_pass`.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTest] Fixed breakage in flatten buffer unit tests.- Updated pass to allow BufferStore/BufferLoad nodes to be visited  before the block's alloc buffer.- Added `T.preflattened_buffer` annotations.* fixup! Return buffer object from tvm.tir.script.scope_handler.Allocate* [UnitTests] Fixed breakage in test_tir_buffer.py- Updated vload test for new behavior.- Added test for offset_of, testing behavior no longer in vload.- Added null check for buffer visitor.* fixup! Replacing Load/Store in codegens.* [UnitTest] ComputeInline, opaque access test updates* [UnitTest] Fixup, allow unit test to use `ib.pointer()[0]`.* fixup! Replacing Load/Store in codegens.The updated CodegenLLVM should use the BufferStore/BufferLoadconvention of indexing by `sizeof(dtype)`, rather than`sizeof(dtype.element_of())`.* fixup! Replacing Store/Load in lowering/legalization passes.BF16Legalize should also update the preflattened_buffer_map, since itis overwriting the `BufferNode::data` stored in the buffer_map.* fixup! Replacing all T.store TIR calls.* Fixed failing codegen c host unit tests.- Generated functions were making `uint8_t*` parameter arguments for  array handle for return value, rather than the earlier `void*`.- New parameter type was due to using  `PointerType(PrimType(DataType::UInt(8)))` as the type annotation, to  be usable as `BufferNode::data`.- Changing to `PointerType(PrimType(DataType::Void()))` still allows  usage as buffer, more appropriately expresses semantics.- Updated C codegens to allow `void*` types to be generated from  variables with type annotation, in addition to the previous behavior  of `DataType::Handle()` variables without type annotation.* Fixup, StorageFlatten when applied to post-StorageRewrite functions.Identified in a test that applied `tvm.lower`, then `tvm.build` on theresult.  If the result of an allocate node is used as the backingbuffer for multiple buffers, such as the output of the StorageRewritepass, then StorageFlatten would erroneously think that the secondoccurrence was an usage without earlier definition.* fixup, StorageFlattenWhen flattening a boolean buffer, the backing buffer should have typeint8, not the preflattened buffer.* Bugfix, correctly represent void* in LLVM IR.* Update, replace tir.Load with tir.BufferLoad* Added TVMScript error check for matching buffer/index dimensionalityNeeded for tests/python/unittest/test_tvmscript_error_report.py::test_high_dim_store* Bugfix, correct return type when lowering custom datatype.* Bugfix, removed unused primfunc from test_tvmscript_complete.py* Updated test_meta_schedule_postproc_verify_gpu_code.py TIRReplaced Load/Store with BufferLoad/BufferStore.* Allowed ramp nodes with buffer use analysis.* Updated tests in test_meta_schedule_postproc_verify_gpu_code.pyNeeded dummy writes to prevent buffer resizing, in order to triggerthe verification failure due to memory limits.* Updated TIR examples to be compatible with buffer dimension check.* Corrected section header in docstring.* Corrected indices size check in CogeGenC.* Fixed breakage in LowerThreadAllreduce.Since the AllocateNode is rewritten, any buffers that refer to thosevariables must also be rewritten.* [UnitTests] Replaced Store/Load in CUDA codegen tests.* Resolved breakage in C-based codegen for vectorized store/load.Needed to update to new convention of using the buffer's element typeas the stride.* Bugfix, incorrect LCA for buffer access in root scope.This had been present before the BufferLoad/BufferStore changes, buthadn't triggered on tests using Load/Store nodes.* Added docstrings for TransformNode member variables.* Added TODO for future removal of preflattened_buffer_map.* Fixup, transform layout + cache write tests.The correct sequence is to first apply any caching as needed, then toapply layout transformations, and finally to apply thread binds forthe computation step.* Bugfix, correct element type for scalarized access.* Bugfix, cuda buffer indexing when declared as different type.* Cuda codegen, update reference.* Bugfix, lower allreduceLoads of the output of the reduction should be replaced for allbuffers sharing a buffer pointer, not just for the buffer objectitself.* Removed obsolete comment.* Changed PrimFunc constructor preflattened_buffer_map to Optional* Removed flatten_buffer argument from T.match_buffer.* Correct call to VarUseDefAnalysis::VisitBuffer* Reverted unintentional testing change, lanes=2.* Updated lower_cross_thread_reduction to use buffer in allreduce* Updated transform_layout test to disable CSE* Updated CSE unit tests to use BufferStore* Replaced Store/Load for vta.transform and unit tests.* Updated unit tests for lower_cross_thread_reduction.* Updated arange to use scalar tensors.The start/stop/step tensors are declared as 0-d scalar tensors, butwere accessed as 1-d tensors.* Fix breakage in ethosu constant encoding.Buffers generated by ""ethosu_copy"" should have their buffer objectsrewritten, but shouldn't have their size updated in ethosu-specificCall nodes.* Fix breakage in ethosu call argument checks.Need to pull out indices from BufferLoad holders, not Load.* Resolve breakage from mismatched shape/index dimensions* Split out encoded parameters from preflattened buffer map.* Updated buffer shape/index dimensions to match in more ethosu tests* Fixed lint error* Removed debug code* Moved arith::Analyzer local variable to class member* Fixed SSA conversion of allocations.Can occur if allocation is inside an unrolled loop.  Added unit testto catch this failure mode.* Ethos-u index/buffer dimension updates.* Updated ethosu passes to handle buffer load/store.* Resolved bug in tvmscript printing of duplicate buffers.* Fix breakage in ethos-u test_assign_addresses, encode constants* Apply same changes to T.allocate_const as to T.allocateReturn a buffer when used in TVMScript, allow for aliasing buffers.* Fix lint errors.* Further updates for ethos-u tests.* Updated ethos.u buffer sizes in test.* Updated tir.BindParams to use BufferLoad instead of Load.* Updated topi.cuda.scan implementation to follow buffer dimensions.* Resolved breakage when flattening AllocateConst nodes.* Resolved breakages from latest merge with main.* Corrected error in merge.* Use empty indices for rank-0 tensor.* Added ir_builder workaround for 1-d indexing.* Consistent buffer access type in LLVM codegen, to match C codegen* StorageRewrite, update indices of modified buffers.* Dynamic relay nodes, access 0-d tensors with 0-d indices.* BFloat16 legalization, update buffer type.* Updated meshgrid to use 0-d index for 0-d buffer.* Corrected boolean handling in Allocate nodes.* Added workaround to unpack 1-d Tensor indices into N-d buffer indices.* Resolved a few more failures in relay tests on cuda.* Resolve linting* CI bump* Updated renormalize_split_pattern tests to use BufferLoad/BufferStore* Fixed cuda codegen checks for BufferStore/Ramp.* Simplify indices further, needed to avoid cuda register limit.* fixed dyn onehot shape func accessing 1d buffer with ()* Fixed codegen indexing for int4 scalar types.* Temporary workaround for incorrect constant folding.Need to further investigate vectorized LLVM constants* s/find_allocate_usage/FindAllocateUsage/g* Added buffer type consistency TODO.* Improved comment on address_of Op.* Rename LegalizeDtype to LegalizeDType, made private.* fix format and lint errors* Disable vectorization of AllocateConst buffer in StorageRewrite.* Pass buffer_map through to the PrimFunc in cmsisnn* try disabling problematic winograd test case* try different way of buffer mapping in storage_rewrite* Removed unnecessary ramp node in ir_builder.* Updated LLVM codegen for buffer indexing.TVM data arrays are always densely packed.  If the LLVM typecorresponding to a vectorized TVM datatype contains padding foralignment, the array location should be computed based on theprimitive element type.Co-authored-by: Masahiro Masuda <masahi129@gmail.com>Co-authored-by: adstraw <astraw@octoml.ai>",0
[BYOC-DNNL] Support DNNL optimal layout (#10421)* enable dnnl optimal layout for supported ops* verfied cv models with onednnv1.7* rebase to the latest main branch* fix format related comments* remove unnecessary layout transformation* change deconv into conv_transpose* rename some variables and functions* simplify query_layout* add checkes for query_layout* fix lint* move partition_for_dnnl from dnnl.py to test_dnnl.py* remove unnecessary model test* add more dnnl layout* rename flag in convolution.cc* enhance dnnl layout,0
support returned function in relay.build (#10502)* fix InferType bug* fix InferType related bug* support returned function in relay.build* support returned function in relay.build* support returned function in relay.build* support returned function in relay.build* add warning about function returning function,0
[Hexagon] Add default constructor to struct Optional in session.cc (#10517)It's needed for older compilers.,1
fix: select narrow dtype (#10519),0
"[vulkan] Add integer dot product (4xint8, 4xuint8) tensorization for the vulkan SPIR-V target. (#10391)* [Vulkan] Add cmake change for spirv dot product* add spirv and vk runtime change* add back conv2d int8 related change* add back dense and group conv2d change* add back test_topi_conv2d_int8.py change (but don't test on vk)* check dot prod availablity in batch matmul schedule* do not run uint8 tensorization on arm* add vulkan target to conv2d int8 test but comment out on CI* do not run vk batch matmul test* Fix performance regression due to missing of constant folding in the index expression.Co-authored-by: Masahiro Masuda <masahi129@gmail.com>",0
Automatically close open files to prevent ResourceWarning (#10526)Python will emit a `ResourceWarning: unclosed file` if filesare not properly closed. This removes this warning by enclosingfile-handling blocks in `with open...` that will automaticallyclose open files upon leaving the block.,2
[FIX] Only allow autoscheduler layout rewritting in conv2d_nhwc (#10522)Autoscheduler cannot handle layout rewritting for grouped convolutionsand non-nhwc layouts. Previously layout rewriting was enabled for allconvolutions causing errors where autoscheduler generated too largelayouts like `64N4n1n1n1H1W68C1n1h1w2c2n`. Autoscheduler is now onlyenabled on non-grouped conv2d_nhwc.,0
Add Check about negative uint constant (#10484)* fix InferType bug* fix InferType related bug* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative* check if uint variable is negative,0
[microNPU] modify the demo to use USMP (#10511)* [microNPU] modify the demo to use USMPThis commit changes the demo to use USMP.Change-Id: I87e85fdd9b2efea9559dda186c1644e4d63509ff* [microNPU] modify the demo to use USMP* removing a repeated TVMC CLI argumentChange-Id: I3e1ec328155bfc4c9ce813224a7f06b09e66232c,4
"[microNPU] Introduce a pass to remove redundant identity operations (#10254)* [microNPU] Introduce a pass to remove redundant identity operationsIntroduces a  pass that aims to remove identity operations,introduced during legalization, that are immediately followed by an NPUcompute operation e.g. Convolution.Change-Id: Ia3b2c7bebf8cba1f827af8e3f3335677ba8f6371* fix lintChange-Id: Idf9341ce757b849f8819944dab2fb3b1496a2caf* Addressing commentsChanges in test_identity_optimizer.py:* Fixed typo in docstring* Removed print* Fixed same output test to use correct input shapeChanges in codegen.cc:* Remove unnecessary constructorChange-Id: Ie4a053725110ce52d8be039ca1ce48084bc66545* skip tests when required packages are not availableChange-Id: I0a88d92dd31ca3dd07a2a495f18c10a2ebf2fc9e* support multiple output identities and add more testsChange-Id: Ib54031fe1c70159728876a23f96b72adb2ea17b0",0
"[microNPU] Fix stride bug in strided slice legalization (#10286)* [microNPU] Fix stride bug in strided slice legalizationTFLite slice is legalized to a strided slice operation with`strides=[1]`, but a similar TFLite strided slice operation islegalized with `strides=[1, 1, 1, 1]` which fails during compilation.Since we only support strided slice with no stride, hard-coding thisvalue to `[1]` during legalization.Change-Id: Ia34183c6984f3c4f7e88063bf8b17fc44f1eb7f9* add legalize strided slice testChange-Id: Icb9480f3c1ed8ce0a328fc7c079c492b33a62e79",0
[PTX] Support mma.sp to use Sparse Tensor Cores and refactor mma codegen (#10339)* init* upd* upd* lint* lint again* upd* add m16n8k32 testcase* format* use make_tuple instead of initializer list* add metadata offset* upd* docstring and sanity* add u8s8s32 back* improvement* compatible #9727,1
[TE COMPILER] Propagate structural hash from relay function to TIR function (#10475)The structural hash of each relay function is copied to the TIR functionso that users can associate relay functions with their lowered TIRversion.,5
"[skip ci][Bugfix] Allow constant folding of 0U - 0U (#10535)A check for unsigned integer overflow would throw an error if itencountered 0U - 0U.https://github.com/apache/tvm/pull/10484, which introduced the check,and https://github.com/apache/tvm/pull/9727, which introduced thisedge case, were in CI at the same time, and each was tested against amerge candidate that did not include the other.  The unittest failureonly occurred when both PRs were merged.",0
"Revert ""[TE][Fix] Comparison of the output tensor (#9829)"" (#10540)This reverts commit 73cf51b8246f1f0b5c0bff6fa206d154e053199b.",0
RFC: initial stab at TorchScript fallback (#7401)* initial stab at TorchScript fallback* make types more flexible* Easy review bits. Thank you @masahi,5
"[LLVM][TIR] Propagate variable names to parameters. (#10514)* [LLVM][TIR] Propagate variable names to parameters.To aid in debugging, generate the variable names of function/closureparameters based on their TIR names.  These function/closure names canthen be observed in the generated LLVM IR.* Use name parameter in CreateLoad.* Fixed unit tests that check the parameter name.",0
"[AOT] Introduce checks for return values from operators (#10424)This matches the lowering of `call_cpacked` which checks only for anoperator return of `0` in the main flow:https://github.com/apache/tvm/blob/bd14a4d36e0d364ef9bd34b2ee96cc09ce64d4b3/src/target/source/codegen_c_host.cc#L207-L231This replaces:```c(void)tvmgen_default_fused_add(x_buffer_var, y_buffer_var, output_buffer_var);```with:```cif (tvmgen_default_fused_add(x_buffer_var, y_buffer_var, output_buffer_var) != 0 ) return -1;```when AOT generates the C output.",1
[QNN] Add nn.adaptive_avg_pool1d to FQ2I (#10541)* add adaptive avg pool 1d* clean up* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
"[Hexagon] Resolve breakage in test_hexagon/test_cache_read_write (#10520)* [Hexagon] Resolve breakage in test_hexagon/test_cache_read_writeBreakage was caused by https://github.com/apache/tvm/pull/9727, whichdidn't account for the new `builtin::mem_copy()` when computing thestack size in `StackSizeChecker`.* Added comment indicating need for StackSizeChecker::MakeMemCopy.* Updated unittests to run all contrib/test_hexagon at CI.* CI bump* Fix lint formatting error.* Updated fix to remove StackSizeChecker entirely.* Bugfix, verify the precheck's allocations, not own.* Bugfix, pass context information to the precheck.",0
Add ONNX LinearRegressor operator support (#10477),1
"[AOT] Use python temporary directory for AOT tests (#10518)* [AOT] Use python temporary directory for AOT testsUses a python temporary directory with a context manager in an effort tosolve the flaky FVP tests raised inhttps://github.com/apache/tvm/issues/10300 andhttps://github.com/apache/tvm/issues/10314. Now that CI is becomingmore and more parallelized, the thinking is that the python temporarydirectory implementation might be more stable than `utils.tempdir`.Removing the XFail markings off the affected tests, but keeping thework around implemented in https://github.com/apache/tvm/pull/10408while we monitor with the above change.Change-Id: Id07869b51cd2278ec4885ef964bc1b23892ba235* alter context manager to make more readableChange-Id: Iba0644db14e50648f6dc99a4ed0f455641c31912",2
"[microNPU] Add support for TFLite FULLY_CONNECTED (#10345)* [microNPU] Add support for TFLite FULLY_CONNECTEDThis is primarily a legalization to an NPU Conv2d operator. Thelegalization target is Conv2d with 1 1 I O (HWIO)* [microNPU] Add support for TFLite FULLY_CONNECTEDTest TVM runtime against TFLite for codegen and operator legalization.* [microNPU] Add support for TFLite FULLY_CONNECTEDFix linting* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, update codegen test, fix linting.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress more comments, ensure qnn.dense is lowered to NPU, fix linting* [microNPU] Add support for TFLite FULLY_CONNECTEDFix linting, update legalization test and codegen test for completeness.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, fix linting. Certain legalization test assertions wereupdated.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>* [microNPU] Add support for TFLite FULLY_CONNECTEDFix assertion in legalization test.* [microNPU] Add support for TFLite FULLY_CONNECTEDAddress comments, fixing assertion on ifm and ofm shape.Co-authored-by: Rishabh Jain <rishabh.jain2@arm.com>",0
"[Runtime][ThreadPool] Remove a cout log output. (#10560)There is a debug std::cout logic left in thread_pool.cc, remove it.",0
[MetaSchedule] Bug Fix for Relay Integration (#10534)* Bug fix.* Fix tune relay script.* Remove debug info.* Retest CI.* Add regression test.* Remove comments.,0
"[FIX,HEXAGON] Gitignore generated Hexagon files (#10552)Ignore these files so they are not accidentally committed.",0
Interpreter call in FoldConstant now always uses graph executor with link-params=0 (#10465)Addressed issue https://github.com/apache/tvm/issues/10390Change-Id: I1a6b2dd27845f9292f1e07f9da1b9be722481f46,1
[CI] Enable TOPI tests in ci_arm (#10564)As part of this I had to disable some of the Target specific tests which didn't run under CI correctly,3
"[Bugfix] Simultaneous layout transform and axis separators. (#10553)Previously, SchedulePostProcToPrimFunc would first generate the mapfrom buffer object to layout transformation, then would update bufferswith the axis separators.  However, it failed to replace the bufferobjects in the layout transformation map, so the transformationwasn't applied.This PR correctly updates the layout transformation map, andadds a unit test to catch this failure mode.",0
[TVMScript] Add intrinsic to look up llvm intrinsic id (#10551)* [TVMScript] Add intrinsic to look up llvm intrinsic id* fix* fix,0
[PyTorch][BugFix] PyTorch-TVM Bridge Build Scripts (#10527),0
"[ci] Remove commit check on ci skipping logic (#10537)* [ci] Remove commit check on ci skipping logicThis makes it very hard to use an sometimes out of the submitter's control (e.g. when Jenkins decides to push a merge commit before running CI) for dubious benefit (the PR title is where people are looking after-the-fact anyways, so having it in the commit message doesn't make much sense). This removes the check for the commit message in order to make the process smoother.commit-id:dbd18808* Address commentscommit-id:ecd2be81Co-authored-by: driazati <driazati@users.noreply.github.com>",1
[CI] Upgrade Python dependencies as part of Docker image buildMake sure that Python package dependencies we install as part of the Docker image setup take precedence over previously Ubuntu installed packages that might be installed (e.g python3-***) via apt.,2
"[AUTO_SCHEDULER] Add feature extraction directly from PrimFunc (#10455)* [AUTO_SCHEDULER] Add feature extraction directly from PrimFuncAllow users to directly extract features from a PrimFunc. Extractedfeatures can be used to get an estimate of flops, memory load size, orarithmetic intensity from a PrimFunc.Also fix feature extraction to correctly measure the number ofarithmetic operations width vector datatypes.* fix param name* log scale in cc instead of python* rename functions, remove load/store* forgot rename in tests* forgot to commit rename",0
"Unit test for DFPatternRewriter on deeply nested sub-graph with attributes on call. (#10533)* Unit test for DFPatternRewriter on deeply nested sub-graph with attributes on call.* - newline, disaster averted",1
Fix TorchScript fallback build (#10556)This was missing a header `libtorch_runtime.h`. The test in `test_libtorch_ops.py` is also currently being skipped in CI since `torch` isn't available but that's left for a follow upcc @t-vi @masahicommit-id:f8998762Co-authored-by: driazati <driazati@users.noreply.github.com>,0
Remove CODEOWNERS (#10192)See RFC: <link tbd>Co-authored-by: driazati <driazati@users.noreply.github.com>,4
[Minor][MetaSchedule] Remove Unused Imports (#10577)Remove two unused imports.,4
"[TECompiler] Decouple TE compute and schedule lowering in ScheduleBuilder (#10561)* Decouple TE compute and schedule lowering in ScheduleBuilder* fixed merge conflict* removed create_schedule stuff* add public, fix include path convention* Forgot visiting arg in ScheduleBuilder CallNode vsit* fixed anchor impl selection",0
[CMSIS-NN] Include clip in the qnn binary op patterns (#10548)* [CMSIS-NN] Include clip in the qnn binary op patternsChange-Id: I3406c4ff90d26392b92675f09f9d8c872ddd596f* Removed redundancies in extraction of clip node in binary opsChange-Id: If6472a3fed6a3df6fbc55615982b8cc5eb40c310,4
[BYOC][TENSOORT] Add support for FP16 on TensorRT BYOC flow  (#10388)* FP16 support for TRT* Cleanups on tests* Fix for typing on output tensor* Fix icheck* Add TRT inference builder auto-convert precision flags as attrs in the config* Address PR comments* Fix bug on passing the new config attrs to codegen for tensorrt partitionCo-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,0
[TVMSCRIPT] Add type definition for preflattened_buffer (#10550)* [TVMSCRIPT] Add type definition for preflattened_buffer* argument should be buffer,1
"[Refactor] Reduced repetition in CodeGenLLVM's buffer access (#10567)* [Refactor] Reduced repetition in CodeGenLLVM's buffer accessPreviously, the majority of the BufferLoad and BufferStore visitorswere duplicate logic to handle the indexing.  After this commit, theshared logic is extracted out into a helper function.* Fixup, remove declaration of unused variable.* Bump to CI",0
[Hexagon] Add doc on TVM - Hexagon RPC flow (#10507)* [Hexagon] Add doc on TVM - Hexagon RPC flow* updated for the latest code* add TODO on removing rpc_local_session.cc,1
[CMAKE] Add option to enable custom logging (#10531)* [CMAKE] Add option to enable custom loggingThis option just passes -DTVM_LOG_CUSTOMIZE=1 to the compiler.* propagate compile defintions to tvm_allvisible* manually propagate compile definitions,1
"[TIR] Restrict Buffer indices, only last index can be multi-lane (#10513)* [TIR] Restirct Buffer indices, only last index can be multi-lanePart of tracking issue https://github.com/apache/tvm/issues/10505,restrict multi-lane indexing to at most one index per bufferaccess. This removes ambiguity as an expression such as`A[T.ramp(i,1,2), T.ramp(j,1,2)]`, which could be interpreted eitheras `[A[i,j], A[i+1,j+1]]` or as `[A[i,j], A[i,j+1], A[i+1,j],A[i+1,j+1]]`, depending on whether the implied iterators of the tworamp nodes are shared.* Improved readability based on review suggestions.* Resolve lint error.",0
"[ci] Build GPU libraries on CPU nodes (#10539)* [ci] Build GPU libraries on CPU nodesGPU capacity is more strained and expensive so we should stick to CPU when possible. This moves the GPU build to a CPU node (which is fine so long as the cuda libraries are present) and splits the C++ unit tests out to relevant areas (test steps where possible, otherwise it runs after the build)commit-id:d385b28c* Address commentscommit-id:dcb084daCo-authored-by: driazati <driazati@users.noreply.github.com>",1
"[ci] Delay pytest errors until all invocations have run (#10521)* [ci] Delay pytest errors until all invocations have runThis makes it a little easier to gather CI signal on a PR by ensuring that all pytest invocations run. Currently pytest runs through to completion for a single invocation so some failures are gathered, but not all. This is annoying for development since its hard to guage how a PR actually fared in CI without seeing the full picture. This will increase demands on CI since failures won't cause the skip the following pytests, but we can monitor CI to see if this has a big impact on queue times.This also also kind of a stop-gap since this wouldn't be an issue if we used a single pytest invocation, but that is difficult since we rely on loading `tvm` multiple times over the course of the test suite.* Don't use a file to stash info between runs* Fix exit code handlingCo-authored-by: driazati <driazati@users.noreply.github.com>",0
"[CMAKE,HEXAGON] Only enable Hexagon custom logging when building for Hexagon (#10587)Move custom logging flags behind `#ifdef defined(__hexagon)`.",2
[TE] Promote substituted variable to iter_var's dtype (#10571)* [TE] Promote substituted variable to iter_var's dtypeThis fixes a bug where an iteration variable and its associated loopvariable have a mismatched dtype.* add check to iter var constructor. fix two bad uses* proplem is more complicated then I thought* one more fix* remove old comments,0
[Arith] Support dtype promotion in TIR comparison expr creation (#10584),5
[QNN] unary op for quantized resize2d and test (#10589)* unary op for resize2d and test* renamed test,3
Upgrade Windows build to use windows-2019 runner (#10585)* Switch to windows-2019 build.* Use Visual Studio 2019 generator.,5
"[Fix] Refactor the roundtrip test. (#10592)This is a tiny fix on the roundtrip test, the case test I introduced in #10370 doesn't use `tvm.testing.parameter`.",0
"[Minor] fix redundant compute (#10580)we should bind axis in CS stage to threadIdx in each warp, otherwise awarp will compute all the tiles in a block.Co-authored-by: tom.hx <tom.hx@alibaba-inc.com>",0
[CMSIS-NN] Scalar to tensor constant pass to support only qnn.add and qnn.multiply (#10563)* Scalar to tensor constant pass to support qnn.add and qnn.multiply only.Co-authored-by: Luke Hutton <luke.hutton@arm.com>Change-Id: If9cb41d0dd3f56666b6a2c0d9903502d3f9e4eae* Created a function to check if an expr is worthy of passChange-Id: I67250a6214a2d54ef07d54d84eac4ce91474bb0eCo-authored-by: Luke Hutton <luke.hutton@arm.com>,1
[TFLite] Quantized unary elemwise ops (#10566)* [TFLite] Quantized unary elemwise ops* fix cos,0
[microNPU] Improve cycles estimates for memory transfers (#10508)Change-Id: Idadc5f354dce42c8dbcdcbe281d324adddb41ba3,1
"[CUDA] Various int8 fix (cublas, cutlass, etc) (#10596)* [CUTLASS] avoid tile size 256 for int8 + align1 case* allow selecting int8 dense strategy for vulkan* fixed cublas batch matmul for int8* fixed int8 dense tensorcore strategy* add cutlass conv align1 + int8 case* support int8 mixed precision cublas bmm* black",0
[FQ2I] Add leaky relu to FQ21 (#10378)* add leaky relu op + passing unit test* passing test* format* clean up* lekay relu qnn op* wip* qnn op* add comment* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
Deploy docs to tvm-site/asf-site on main (#10494)* Deploy docs to tvm-site/asf-site on maincommit-id:59241556* Use oauth* testing codecommit-id:6cc27fceCo-authored-by: driazati <driazati@users.noreply.github.com>,2
"[microTVM][RVM] Improve base-box-tool 'build' command (#8738)Currently base-box-tool.py 'build' command will fail with a 'packer'error message on the second run if it's run twice and the box for aprovider built on the first run is not removed manually before thesecond run.This commit avoids that failure by checking for the existence of a boxfor each specified provider and if a box already exists it will refuseto overwrite the box (since building a box takes a quite amount of timeto be done), exiting and warning the user. A new option '--force' isadded to the 'build' command that allows the user to explicitly rebuildthe box in case one already exists.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[TIR] Updated python docstring and parameter names for AllocateConst (#10602)The previous docstring referred to the non-existent `data` parameter,and passed the argument named `condition` in Python as the parameter`data_or_idx` in C++.  This commit matches the Python names anddocumentation to those in C++.",1
[Runtime][PipelineExecutor] Add the pipeline internal forwarding logic. (#10543)* [Runtime][PipelineExecutor] Add the pipeline internal forwarding logic.This patch use the SPSC lock free queue to forward the runtime output datainto the child runtime input interface.* remove debug logic.* address review comments.* correct a variable comments.* address review comments.,0
[BYOC][TENSORRT] Fix bug of Segmentation Fault  when loading engine file. (#10597)Co-authored-by: XuZhi <xuzhi.xu@alibaba-inc.com>,0
[TVMScript] fix print target's host (#10598)A followup fix for https://github.com/apache/tvm/pull/9594,0
[Arith] Improve floordiv / floormod rewrite simplifing rules (#10591),5
"[Bugfix][MetaSchedule] Fix over-simplification of Select (#10605)The feature extractor simplifies `Select` into a constant number, which overlooks the possibilitythat there could be buffer access inside Select.",0
"[Hexagon] Generalize builtin for Nd memory alloc with storage scope and add lowering for VTCM / Hexagon (#10558)* repurpose texture flatten for vtcm; TIR lowering correct* clean up remaining code in texture flatten pass* add Alloc and FreeTexture, but failing to run over rpc* test passing with malloc in the device api* cleanup* fails in very reliable way with memory corruption* working with non-HexagonBuffer vtcm alloc* cleanup* do not pass scope through mem_copy api* [Hexagon] Resolve breakage in test_hexagon/test_cache_read_writeBreakage was caused by https://github.com/apache/tvm/pull/9727, whichdidn't account for the new `builtin::mem_copy()` when computing thestack size in `StackSizeChecker`.* use HexagonBuffer in Alloc and Free packed funcs* Added comment indicating need for StackSizeChecker::MakeMemCopy.* add AllocVtcmWorkspace and FreeVtcmWorkspace* cleanup* Updated unittests to run all contrib/test_hexagon at CI.* create separate vtcm alloc lowering pass and transform* reset texture_flatten.cc* comments* CI bump* Fix lint formatting error.* Updated fix to remove StackSizeChecker entirely.* pass device and type to device api* Bugfix, verify the precheck's allocations, not own.* Bugfix, pass context information to the precheck.* pass order and shape to device api* working* fix up types and arg passing* pass scope to device api* common builtin for texture / vtcm* add scope to freend api* format and lint* fixed missed format error* restart ci* fix test random value issue + code review feedback* fix test hang* restructure lower vtcm pass per code review feedback (option a)* format error* global.vtcm + tvm_stack_make_shapeCo-authored-by: Eric Lunderberg <elunderberg@octoml.ai>",0
Fix bug check trt (#10600)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,0
"[CI] Pin numpy version in image build (#10611)Tensorflow `2.4.2` expects the version of numpy to be `~=1.19.5`,however pip installing numpy will attempt to install `1.21.5` bydefault. Therefore, pinning the version of numpy when building thedocker image to be `~=1.19.5` until the version of Tensorflow isupgraded.Change-Id: Ia44e183afb660cac67fc4274ff70b23d28fc3e3e",2
"[microTVM] Zephyr: add mps3_an547 board support (#10479)* [microTVM] Zephyr: add mps3_an547 board supportAdd mps3_an547 board support to microTVM.On Zephyr this board is supported by two emulators: QEMU and FVP. Thiscommit only enables the support for running mps3_an547 on QEMU, sincecurrently there isn't a FVP transporter on microTVM. The main differencebetween these two emulators is that FVP is provided by Arm as a closedsource emulator and it supports the Ethos-U55 accelerator.The mps3_an547 is an Arm reference board. For more details, please see:https://developer.arm.com/tools-and-software/development-boards/fpga-prototyping-boards/mps3Since there are already specific tests enabled on the CI to testEthos using the AOT executor, for instance, this commit will only addsupport for mps3_an547 using QEMU for now, also enabling the board to betested on the CI.The FPU is disabled on this commit (""fpu"": false, in boards.json). Thisis due to commit d4cc1c2196 (""target/arm: Enable MVE in Cortex-M55"")being absent in QEMU v6.1.1, so it's not available in any zephyr-sdkrelease yet. That commit enables MVE (M-Profile Vector Extension) and sofully enables the instructions to run the code generated when FPU isenabled on Zephyr. It's available from QEMU v6.2.0.This commit also adds support for the QEMU_BIN_PATH env variable so itturns easy setting an alternative QEMU version other than the one thatis available via PATH, when running / testing any virtualized board,either by the CI in the future or locally by the users/devs.Finally this commit fixes two typos in comments.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>* mehrdadh review: Add comment on why FPU is disabled if mps3_an547 supports it",0
"[Hexagon] Codegen for 2d Load/Store (#10586)* Added unit tests for codegen of 2d physical buffers in Hexagon.* Update IndexMap when buffers are updated.* Extended CodeGenLLVM::BufferAccessHelper to support N-dThis way, a subclass can override GetBufferPtr, without needing toreimplement all of the other indexing logic forBufferLoad/BufferStore.* Updated CodeGenHexagon to treat 2-d physical buffers as T*** Moved indices size check earlier.Previous location in `CodeGenLLVM::BufferAccessHelper` occurred afterpossible integer wrapping in `indices.size()-1` loop bounds.* Updated to use `llvm::ArrayRef` instead of `std::vector`.* Resolve lint error.* CI fix, contextlib.nullcontext not available on python3.6",0
[FQ2I] Add mean op to FQ2I (#10607)* add mean op* clean up* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
"[Hexagon] Deprecate SDK 3.x, rewrite HexagonSDK.cmake (#10612)* [Hexagon] Deprecate SDK 3.x, rewrite HexagonSDK.cmakeAvoid setting global state, instead implement get_hexagon_sdk_property,which will set a user-provided variable to the value of the requestedproperty.* Restart CI",2
"[Hexagon] Refactor tvm.contrib.hexagon, NFC (#10616)* [Hexagon] Refactor tvm.contrib.hexagon, NFCBreak it up into multiple files.* Restart CI",2
Add remaining targets to ci.py (#10425)* Add remaining targets to ci.pyThis adds build/test commands for all of the CI environments except ARM (that one will come in a follow up). Most of the invocations are similar and the scripts come straight from the Jenkinsfile. This improves the current situation by making it much easier to get CI environments locally. This also wraps pytest invocations in CI so that failures are parsed and a repro command is reported at the end of the failing CI run step alongside other logs to increase the visibility into this tool.This isn't perfect yet so some work (such as ARM support and certain tests that require pytest flags like in `tests/scripts/task_python_microtvm.sh`) is left for a follow up.* remove reporting changes* Clean up common functionality* Address comments* CommentsCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[skip ci][ci] Add missing guard to skip CI check (#10625)This was failing in the docker image validation since the `BRANCH_NAME` environment variable wasn't set. The part in question still runs even with the `skip ci`https://ci.tlcpack.ai/blue/organizations/jenkins/docker-images-ci%2Fdocker-image-run-tests/detail/docker-image-run-tests/62/pipeline/cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[Metaschedule] New relay backend for meta schedule task extraction (#10578)* New relay backend for meta schedule task extractioncommit 501fac65291c51710911ca49af1577ea1794bcb2Merge: 076fa33fc ce8c563d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:16:47 2022 +0900    New relay backend for meta schedule task extractioncommit ce8c563d09eaba2a6b03189d1d3452f7565f4c69Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:12:30 2022 +0900    fix cpplintcommit dfa4fb0c20c17049e8ac2c135200074b872ce1ecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 14:09:11 2022 +0900    update expected op list in    test_meta_schedule_integration_extract_from_resnet to remove dep on Ansorcommit a98182eed3b85e477c5f2527d5d21ce545bd5c18Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:56:35 2022 +0900    fixed test_meta_schedule_integration_apply_history_bestcommit 40d52a15b4c1ac9b8d4eac16f98ccec5e2a3e966Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:50:43 2022 +0900    uniquefy task namescommit dfaf4964bf3a0b542ead5f11f356c2ec592be725Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 13:45:30 2022 +0900    dedup taskscommit e49d500299c9c884497410046421853266b60cd2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:59:45 2022 +0900    return reversed listcommit 74636beae0878cdda7dd03aa2b09ab2821c86477Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:39:58 2022 +0900    refactorcommit 99f1701eb71d77a85bb0f8457841739dc586a168Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 12:34:14 2022 +0900    clean up integration.cc and Query interfacecommit 3f93a1e7645118c002aa10e5b7ff14b71b3f837aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 11:54:57 2022 +0900    check in minor vnni-related changecommit af3e98867f91f99522fee4da2e170dc87311466cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 07:36:35 2022 +0900    Removed TaskExtraction nodecommit 7b4d35eb00852db6397d43e0aa6b1fedabae3f63Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:42:56 2022 +0900    add doc to util functionscommit 3c5a3184fb42e69ef10619b05b9b9f128f7ea618Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:27:53 2022 +0900    rename to task extractioncommit 57f2882a5ed5615ef8eee96cd7284d495f908449Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:24:37 2022 +0900    fixed constant param bindcommit f099537d3630d268ad0700c75e93bbdc67831837Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:10:44 2022 +0900    remove unused stuff from python extract_tasks_from_relaycommit 4a5e4aae48a7bdc8c24c8f7ae7bd5484034837e4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 05:10:30 2022 +0900    move BindParams function to cc filecommit efecceaea3958e184de7ef0ff6cb5f3988640afaAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 03:56:05 2022 +0900    refactor param bindingcommit 109187fc0463728cd44171389e8fc91fb0ac8cf9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 02:21:58 2022 +0900    New relay backend for meta schedule task extractioncommit 6f019014a4614f43aefcf642981bfb15d64b09f3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 11:25:44 2022 +0900    fixed anchor impl selectioncommit be6c25893dd0546db71b8472415303fc5be9d67fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 10:57:02 2022 +0900    Forgot visiting arg in ScheduleBuilder CallNode vsitcommit 0c6d4a603335ae2cba2771e939eff1ddeb98fbe3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri Mar 11 10:45:08 2022 +0900    add public, fix include path conventioncommit 4cd3a1657c4e2e13abe7281b7cdef5dff73b37eeAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 18:43:15 2022 +0900    removed create_schedule stuffcommit eb1bc7e789b66eaf3d4fe01d5154c135ab275dc2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 18:13:42 2022 +0900    fixed merge conflictcommit 6e68fd9aff9f86412f8b7150b18ae1b374927f86Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Mar 10 14:27:34 2022 +0900    Decouple TE compute and schedule lowering in ScheduleBuilder* update integration.h doc* remove unused import* fix mypy check* use_meta_schedule restored, now extracts the same task as Ansor* python doc update* unused import* cache_ -> cache, suppres ""Cannot find workdload"" warning* Update src/relay/backend/task_extraction.cc and te_compiler_cache.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* removed unnecessary include* fixed build* drop relay.const on params* updated comment in integration.cc* update schedule_rule name to prepend ""metaschedule""* typo fix* more nit change* make the output of Query Optional* update py doc* remove TODO comment on parse_modCo-authored-by: Junru Shao <junrushao1994@gmail.com>",0
"[Hexagon] Remove double "".hexagon.hexagon."" from registered names, NFC (#10624)",4
[Arith] Fix floormod rewrite simplify rule (#10626),0
"[CI] Remove mps3_an547 from the CI (#10621)Remove for now the mps3_an547 board from the CI.Both mps2_an512 and mps3_an547 boards hit a hard to debug issue whentests run against them in the CI environment.The failure seems not tied to any specific test and usually consists inthe board stopping to respond the host that will show a generic tracedue to a timeout when trying to call a function on the device, like:tvm._ffi.base.TVMError: MicroSessionTimeoutError: failed to read reply message after timeout 10sThe issue is very hard to reproduce locally and its root cause can bein one or more of the stack components being stressed, like the QEMUemulation and its interaction with the particular CI environment,Zephyr's serial driver (the boards share the same driver), microTVM codeusing Zephyr's bufffer and serial APIs, and TVM RPC protocol over theserial line when and with a serial FIFO too small (1 byte in the case ofbooth board UARTs), or even a ""hiccup"" when running QEMU on the CIinside a container.Hence the most reasonable thing to do is to remove the boards from theCI until that issue is solved.This commit also removes the commented out lines for running themps2_an512 board in the CI so the script file don't get polluted. Themps2_an512 is currently disabled.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[Bugfix] Handled TransformNode in PassUpBitMaskOr/PassDownBitMaskOr (#10620)Previously, a layout transformation applied to a te.compute whosecomputation used a reduction axis would fail.",0
[Testing] Add model loader for int8 BERT (#10622)* add model loader for qat bert-base* add test* pylint* ignore mypy* Update python/tvm/meta_schedule/testing/tlcbench.pyCo-authored-by: Junru Shao <junrushao1994@gmail.com>* use a dedicated process for converting* return input info* encode batch size and seq_len information in cached file pathCo-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[Pytorch] Add `aten::fmod` and `aten::remainder` (#10613)* [Pytorch] Add `aten::fmod` and `aten::remainder`* update tests,1
"[Hexagon] Add support for Hexagon v69, deprecate v60 and v62 (#10623)* [Hexagon] Add support for Hexagon v69, deprecate v60 and v62- The tvm.target.hexagon() function now accepts v69.- Hexagon SDK 4.x does not support v60 and v62, so they have been removed.* Restart CICo-authored-by: Krzysztof Parzyszek <kparzysz@invalid>",1
"[Docker] Move psutil installation to ubuntu_install_python_package (#10615)Previously, psutil was installed as part of `ubuntu_install_redis.sh`,which was included as part of the ci_arm, ci_gpu, ci_gpu, ci_qemu, andci_i386 docker images.  However, it was not included as part of theci_hexagon docker image.  Since this is a more general python packageused in terminating rpc server/tracker children, moving it to the moregeneral location.",2
"[ci] Move pip dependencies to docker images, add ninja / shellcheck (#10257)Following on from #10246, this moves the `pip install`-at-runtime deps to the docker image install so they are baked in.",1
[microNPU] re-enable network tests (#10565)This commit re-enables tests thathad failed due to a interrupted downloadingof the testing data.,3
"[Runtime][PipelineExecutor] Setting CPU affinity for the Runtime of pipeline. (#10639)This patch add the function to set cpu affinity for the runtime of thepipeline. By using the said function, user for example can let the firstruntime of the pipeline to use the cpu [0,1] only, and the second runtimeto use the cpu [2, 3] only.",1
[USMP] bugfix workspace calculation (#10617)The workspace calculation should be doneafter memory is planned.,0
query rpc tracker - sort servers by key name (#10641)* query rpc tracker - sort servers by key name* fix black formatingCo-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,0
[Relay] Fixed VNNI batch matmul op strategy for meta schedule compatibility (#10637)* Fixed VNNI batch matmul op strategy for meta schedule compatibility* update bert task extraction test,0
[Relay][ONNX][Fix] Flatten in OnnxConverter (#10593)* fix flatten* fix: python tuple to list,0
[Lint] Ignore Hexagon generated files in CPP lint (#10609)* Ignore Hexagon generated files in CPP lint,2
[microTVM] Enable micro tvmc tutorial testing in CI (#10555)* Add script convertor* Address comments: added test* address comments,1
[Hexagon] Add flags to control floating point support in HVX (#10644)Add explicit parameters to `tvm.target.hexagon()` to control LLVMcode generation for floating point vector instructions.,1
[ci] Fix doc deploy folder (#10634)This was unpacking into `tvm-site/docs` instead of just `docs` at the top levelCo-authored-by: driazati <driazati@users.noreply.github.com>,0
"Add support for aten::__lshift__, aten::__rshift__, aten::__ior__ (#10631)* add support for aten::__lshift__, aten::__rshift__, aten::__ior__* lint* undo lint* lintCo-authored-by: Masahiro Masuda <masahi129@gmail.com>",1
"[microNPU] Setting a random seed for the codegen tests (#10640)Although a random seed is set while generating the input for the codegentests (`infra.py/generate_ref_data_tflite`), other sources of randomdata such as weight constants were not being deterministicallygenerated. This commit fixes that by setting a seed at the start of eachcodegen test.Change-Id: I25bb27a131cfc8aa318a8d808e78fb5ad628ad27",0
Remove unnecessary wasmtime from ci_lint (#10653),4
Bump all Docker image versions to update Python to 3.7 and Tensorflow to 2.6. (#10654)These updates are required due to Python 3.6 coming to EOL and also to updateTensorFlow to a newer version.,1
"[CI] Bump Python version from 3.6 to 3.7 in VitisAI.cmake (#10656)This is required because we migrated the default version in whichdependencies are installed, due to Python 3.6 coming to EOL.Co-authored-by: Elen Kalda <Elen.Kalda@arm.com>",2
Upgrade tensorflow to version to 2.6.x (#10084)Upgrade the following versions:keras - from 2.4.3 to 2.6tensorflow - from 2.4.2 to 2.6.2h5py - from version < 3.0 to version 3.1.0,5
[Meta Schedule] Refactor meta schedule testing utils (#10648)This PR moves some utility testing classes into `meta_schedule/testing/utils` and updated the following tests involved:- test_meta_schedule_integration.py- test_meta_schedule_measure_callback.py- test_meta_schedule_search_strategy.py- test_meta_schedule_task_scheduler.py,1
"[C Codegen] Remove global packed variables when interface_api=""packed"" and target=""c"" (#10645)* fix pack global variables* address comments",0
"[TIR] Bugfix in StorageFlatten, index flattening in PrefetchNode (#10657)This resolves a bug introduced inhttps://github.com/apache/tvm/pull/9727, and adds a test to catch thisfailure mode.  This bug occurred because StorageFlatten's visitor forPrefetchNode inserted additional pre-flattened `BufferLoad` nodesafter visiting the body of the Prefetch, preventing those `BufferLoad`nodes from being flattened.  Moving this visit to after the insertionof the `BufferLoad` nodes allows the usual buffer flattening to applyto the newly inserted nodes.",0
"[CI] Set default value for CI_NUM_EXECUTORS (#10642)* [CI] Set default value for CI_NUM_EXECUTORS`task_cpp_unittest.sh` relies on `CI_NUM_EXECUTORS` being set as anenvironment variable, which causes the script to fail if the value isnot set. To prevent this, setting the default as 1.Change-Id: Ie543998bc97c60cb2fb76a92831acc830e2805f5* move CI_NUM_EXECUTORS logic into task_build.pyChange-Id: I0c1a85032a688058c90d15964c0accf2b2510c36",2
rebase (#10525)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
Optimize the implmentation of swish operator (#10655),5
[TIR] CSE-TIR Pass - More deterministic behavior (#10663)* iterate through sorted keys* masa comments -- simplify iteration* test* tests* simplify vector construciton* jostle ci,3
[ci] Disable failing hexagon conv2d test (#10666)See #10665. This doesn't disable just the parameterized version that is failing since our parameterization is buried within TVM and shared among many tests.Co-authored-by: driazati <driazati@users.noreply.github.com>,3
Add order to functions in C Codegen (#10590)* Add function ordering to C Codegen* trigger* fix comment* address comments* add test* add unorder check* fix test* address comments,0
"[microNPU] changing region 'tvmbaw*' to 'dynamic*' (#10338)As a follow up to #10022, this is a follow PR toperform name change of the region as discussed inthat PR.",4
[CMSIS-NN] Fallback to native schedules for unsupported partiti ned functions (#10603),5
"Mark x86 specific test (#10672)Currently this test crashes pytest on any other architecture, this introduces a decorator which can be used to mark future x86 tests.",3
Add AArch64 frontend dependencies to Dockerfile (#10676)Preparing to run more of the frontend tests on AArch64 by installing all the dependencies in the container,1
[CI FIX] (Really) Skip test_conv2d in Hexagon (i386) (#10687)* [skip ci] Skip test_conv2d in Hexagon* properly skip hexagon test_conv2d on i386* skip entirely,0
Fix some tiny spell error (#10693),0
"[microNPU] Refactor Relay to TIR hook (#10599)* [microNPU] Refactor Relay to TIR hookRefactors the Relay to TIR python hook for the NPU so that optimizationscan be applied across the whole module and not just functions that willbe offloaded to the NPU. A pass `OutlineCompilerFunctions` is introducedto outline NPU functions, which now happens before optimization passesare run (this previously happened after the prim_func had been created).In addition, optimization passes that should only run on NPU functionsare now limited to running on outlined functions for the NPU (bychecking the ""Compiler"" attribute). To help avoid code duplication, ahelpful decorator `create_npu_function_pass` has been created for pythonpasses that should only run on NPU functions.This refactor helps move a number of passes in the microNPU codegen touse an IRModule -> IRModule philosophy.Change-Id: Icdea9ba43da0157d5ee17529d2b23b761396d112* add mixed compilers to testChange-Id: I3ca48738e096bb0f4dc362f0e9550317fc0d5afd* Address comments including renaming both npu_pass and RelayToTIRThis commit renames `npu_pass` -> `create_npu_function_pass`.It also renames the `RelayToTIR` pass created in Python to `LowerToTIR`,along with moving it to compiler.py to make it clear that this pass is awrapper around the `_lower_to_tir` function. In addition, to make itexplicit that the `lower_to_tir` func->func pass should not be useddirectly it has been renamed to `_lower_to_tir` - it is being maintainedsince it is used in many tests.Change-Id: I3a0a06801f029aeaa4a51c2d86d8703bb0d7afbb* address nit and small fix to exampleChange-Id: I44c64de15fa8680cc89ce0440ffa6c9e0ec62a50",0
"[microNPU] Remove xfail from test_clz (#10670)After #10640 we can re-enable the flaky clz test, mentioned in #10487.Change-Id: I35d474183239cad5f6c2eba35b55d4ca14869917",3
"[FIX,TOPI] Default to inlining fused operations for conv NCHWc int8 (#10682)Inlining fused operations used to be the default and performs better onx86.",0
[skip ci] Add generated docs file to .gitignore (#10701)cc @mehrdadhCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[MetaSchedule] Upstream the leftover changes (#10689)* [MetaSchedule] Upstream the leftover changes* Update driver_api.cc,1
[TIR] compact buffer region (#10557),5
"[ARM,TOPI] Allow auto scheduler layout rewritting in dense (#10699)* [ARM,TOPI] Allow auto scheduler layout rewritting in denseAuto scheduler was already rewritting the layouts of inputs to dense,but the dense operators was not passed the correct flag to takeadvantage of these inputs. This could cause a crash when the rewritteninputs did not match the size expected by dense.* formatting",4
[ci] Fix condition for skipping tests in i386 (#10698)The Python and base image update for the i386 container changed the results of the various functions in `platform` as found in #10687. This updates them to work correctly with the new container and updates the relevant parts of the codebase to use the new check.cc @masahi @mosiusCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[COMMUNITY] siyuan to PMC (#10688),3
[Hotfix] A line is accidentally removed in `Verify-GPU-Code`,0
[TIR] Tuple Reduction Support in CreatePrimFunc (#10671)* [CreatePrimFunc] Support multi-source ReduceNode (#64)* initial* assert structural equal test* Enhancement and tests* Fix dtype* DocsCo-authored-by: Andrew Liu <andrewlliu@gmail.com>,0
"[microTVM] Zephyr: Fix gdbserver_port option (#10678)Currently 'gdbserver_port' option is not working properly and if it'spassed to the API server it takes not effect, being ignored silently bythe server, hence no debug port for GDB is created when QEMU runs.This commit fixes it by correctly setting 'TVM_QEMU_GDBSERVER_PORT' envvariable so when Zephyr runs QEMU to create a virtualized board the GDBport is set correctly to the port passed to the Project API server.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[microNPU] Determine block configs using the cascader (#10695)The cascader needs to be able to choose the block config for operations in order to accurately model their performance. The cascader must attach the chosen block config to the te.Schedule. This is done using a pragma. The chosen block configis also added to the TIR spec. If the cascader hasn't set a block config, it defaults to the existing block configselection behaviour.Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>Co-authored-by: Matthew Barrett <Matthew.Barrett@arm.com>",1
[Community] Krzysztof Parzyszek -> PMC (#10703),3
"[Driver] Remove duplicate PreProcessModuleForBuild (#10530)* [Driver] Remove duplicate PreProcessModuleForBuild`PreProcessModuleForBuild` was nearly identical to the `build()`function in the same file.* Name change, `tvm::TIRToRuntime`.",2
"[build] Use mold or lld if detected (#10683)This looks for the lld or mold executables when configuring CMake.If found, it sets them as the linker for gcc/clang via -fuse-ld=.Fixes #10679Co-authored-by: Lite Ye <yelite@users.noreply.github.com>",0
"[ci] Fix diff condition for docker builds (#10684)Docker builds weren't triggering on main since it was diffing changes from `main` (and there weren't any), so this fixes it so the diff is checked against the previous commit for builds on `main`.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[skip ci][ci] Skip flaky test_auto_scheduler_layout_rewrite_networks test (#10709)See #10707Co-authored-by: driazati <driazati@users.noreply.github.com>,3
Update docs to clarify minimum Visual Studio version. (#10715),1
"[Relay] Remove DynamicToStatic pass from graph runtime build (#10691)Closes https://github.com/apache/tvm/issues/10692To solve this problem, we can either remove this pass from `relay.build(...)` pipeline or run `DynamicToStatic` in both VM and non-VM paths. I propose to remove it because  (1) usually `DynamicToStatic` is supposed to be applied after model import and (2) the only case running `DynamicToStatic` during `relay.build(...)` helps is when the input is entirely static but a frontend fails to produce a static mod AND a user forgets to run `DynamicToStatic` after model import. I hope the latter case happens rarely but if not, that's something we should fix in the frontend side. We should avoid relying on `DynamicToStatic` that runs during `relay.build(...)` since not all use cases of TVM use `relay.build(...)` (BYOC, for example).",0
"[AUTO_SCHEDULER] Only run rewrite layout tests on CPU (#10717)Set the layout rewriting tests to only run on ""llvm"" and ""llvm-device=arm_cpu"" as layout rewriting is only supported on CPUs.Currently, the arm test will not be run on arm CI because integrationtests are not enabled.",3
[Hexagon] Add Hexagon-specifc timer to enable using `time_evaluator` (#10714)* Add stub* Hexagon profiler is called* add HAP call* move to hexagon_common.cc,1
[fix] relax QnnConv2DTransposeRel constraint (#10716)* zp check* lint,0
[UnitTest] Fuzz based on seed rather than random value. (#10515)Some extensions to run tests in parallel (e.g. `pytest-xdist`) requirethat test collection be deterministic.  Using the random seed as thetest parameter instead of the random value makes the test collectionbe deterministic.,2
Use local complete block and local reduction block to identify compact dataflow (#10705)* inint* upd* upd* remove redundant print* upd* change the reads/writes region for argmin/val* fix wrong push,0
Fix shape func for Reshape (#10721),0
[Hexagon][CMake] Propagate build type to external cmake calls (#10711),5
"[TIR] Change the behavior of read/write region analysis for reduction blocks. (#10638)After discussion w/ @spectrometerHBH @Hzfengsy , we decide to exclude the buffer access from read regions if it's being written to inside a reduction block. In this way, the outer block would not find overlap between the region reads and writes simultaneously, thus solving the issue mentioned in #10420 .One tricky case is how to handle opaque memory access in `GetBlockReadWriteRegion`, where we have no hint about which buffer is being written to. And I keep the original behavior that the opaque access was added to both read and write regions of a block, no matter whether it's a reduction block or not.",1
[TIR][Schedule] Transform layout (#10538)* [TIR][Schedule] Transform layout* address commens* fix* doc* Address comments* remove unused* Use BufferIndexType enum* lint* support *args* lint* lint,0
"[Keras] Adjust Keras frontend for Keras 2.6 support (#10733)Add support for the Keras frontend to be tested and used withboth Keras 2.4 and 2.6, as we plan for migration.Co-Authored-By: Luke Hutton <Luke.Hutton@arm.com>Co-authored-by: Luke Hutton <Luke.Hutton@arm.com>",1
"[git] Ignore auto-generated micro_tvmc.py example. (#10729)This file is generated automatically by`tests/scripts/task_convert_scripts_to_python.sh`, added inhttps://github.com/apache/tvm/pull/10555, and can be generated whenrunning the `ci.py lint` script locally.",1
"[Hexagon][Codegen] Implement CodeGenHexagon::CreatePrintf (#10710)* [Hexagon][Codegen] Implement CodeGenHexagon::CreatePrintf`CodeGenHexagon` inherits from `CodeGenLLVM`, but debug messages sentthrough `printf` calls do not make it back across the RPC server.Instead, the `FARF` preprocess macro provided from the Hexagon SDKshould be used.  This implementation of `CodeGenHexagon::CreatePrintf`generates the same `HAP_debug_v2` function call as would be generatedby the `FARF` preprocessor macro, using the `ALWAYS` print level.* Updated following review comments* Updated from const llvm::ArrayRef& to llvm::ArrayRef.",0
"[AOT] Get input name from module/prim func (#10731)The input name generated in each of these test cases changes dependingon the version of tensorflow being used. v2.4 = ""x_int8"", while v2.6= ""x"". Making these tests agnostic of input name so that they work withboth v2.4 and v2.6.Change-Id: I843a655b3bf4e018624e5757c653b1d85058991e",3
Fix default pytorch divide behaviour (#10727)Co-authored-by: Aleks Knezevic <aknezevic@tenstorrent.com>,0
"[Runtime][PipelineExecutor] Getting the asynchronous output (#10723)This patch create a new GlobalRuntime to check whether the output dataready and poll global output of pipeline, it also removed the sequencepipeline execution logic as the asynchronous logic already done.",1
[skip ci][ci] Skip flaky test_cudnn test (#10747)See #10746Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[TVMScript] Enable assignment statement without type annotation  (#10736)* Add test* workaround mypy* replace assert with condition,1
"[FIX,AUTOTVM] Fix printing of measure results (#10647)* [FIX,AUTOTVM] Fix printing of measure resultsThe check for if the error_no was valid was wrong.* switch logic",0
[ONNX] Make `freeze_param = True` and run `DynamicToStatic` by default (#10750)* [ONNX] make freeze_params=True and run DynamicToStatic by default* remove convert_to_static in onnx test* fixed qlinearconv conversion for freeze_params=True* fixed assert msg placement,0
"[docs] Disable numpy intersphinx (#10752)These links usually 403 for us which `task_python_docs.sh` is set up to ignore, however sometimes it hits a 404 error which has been causing CI failures lately (e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/2832/pipeline/). This disables intersphinx links to numpy entirely until we can sort out a better fix.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[BugFix] Generate unique names for reduction blocks (#10726),0
[git] Remove duplicate .gitignore entry (#10760)The ignore for `gallery/how_to/work_with_microtvm/micro_tvmc.py` wasadded in both https://github.com/apache/tvm/pull/10701 andhttps://github.com/apache/tvm/pull/10729.  This removes the duplicateentry.,1
Fix link formatting. (#10730),0
[TIR][Schedule] Change BufferIndexType enum in python side to string (#10737)This is a follow-up of #10538 to use string instead of enum in python side.,2
ONNX Opset 14 Support - HardSwish (#10735)* ONNX Opset 14 - HardSwishAdded hardswish support to TVM CI and fixed unit test.- Add class HardSwish and added its reference to convert_map in onnx.py;- Removed test_hardswish entry from test_forward.py;* ONNX Opset 14 Support - HardSwishFixing onnx.py format.* jostle ci,0
"[NDArray] Expose NDArray::CreateView to python (#10712)Modifying the array view is needed for Hexagon targets, in order tofirst call `tvm.nd.array` with the physical dimensions, then update theshape to contain the logical dimensions.",1
"[NDArray] Update runtime.TVMArrayAllocWithScope to use ShapeTuple (#10728)`runtime.TVMArrayAllocWithScope` predates the introduction ofShapeTuple, and its use simplifies the `tvm.nd.empty` function.  Thetwo modified locations are the only occurrences of the string""TVMArrayAllocWithScope"" in the repository, so no other call sitesshould need to be updated.",1
Add missing Slice layout fallback check of `stride=1`. (#10690)* Add missing Slice layout fallback check.* Fix lint* jostle ci,0
[ci] Upload built Docker images to ECR (#10662)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
"[ci] Check existing reviews and requested reviews before cc-ing (#10734)This will re-request reviews from existing reviewers as in #10714 which is not intended, so this lists out existing reviews/requests and filters the new reviews to add based on those usernames.cc @areuschCo-authored-by: driazati <driazati@users.noreply.github.com>",1
[ci][docker] Add Jinja2 to images (#10741)This is needed for #10740Co-authored-by: driazati <driazati@users.noreply.github.com>,1
Onnx squeeze enabled with auto axis handling. (#10742)* fix squeeze when axis is absent* lint* tidy code* fix argument,0
"[TIR] Simplify final indices from transform_layout (#10761)The final indices returned from transform_layout when applied on a`te.compute` are not simplified. Thus the returned index ranges areharder understandEg: When applying NHWC to NCHWc transform_layout```python   iter_vars = s[B].transform_layout(lambda n,h,w,c: [n, c//4, h, w, c%4])   print(iter_vars)```iter_vars before simplification:```python[iter_var(axis0, range(min=0, ext=((w - 1) + 1))), iter_var(axis1,range(min=0, ext=(floordiv(((z_div*4) - 1), 4) + 1))), iter_var(axis2,range(min=0, ext=((x - 1) + 1))), iter_var(axis3, range(min=0, ext=((y -1) + 1))), iter_var(axis4, range(min=0, ext=4))]```iter_vars after simplification:```python[iter_var(axis0, range(min=0, ext=w)), iter_var(axis1, range(min=0, ext=z_div)), iter_var(axis2, range(min=0, ext=x)), iter_var(axis3, range(min=0, ext=y)), iter_var(axis4, range(min=0, ext=4))]```",5
[PyTorch] Add `aten::square` operator (#10766),1
[Hexagon] Set target architecture when compiling for Hexagon (#10771),5
[Hexagon] Guard UserDMA code with architecture check (#10770)UserDMA is only available in Hexagon V68 and later.This fixes issue https://github.com/apache/tvm/issues/10768.,0
[MetaSchedule] Misc update for e2e workloads (#10776),1
[ci] Skip flaky CMSISNN test (#10749)* [ci] Skip flaky CMSISNN testSee #10748* Properly disable testCo-authored-by: driazati <driazati@users.noreply.github.com>,3
Fix clang llvm 12.0.1 warnings (#10744)* Fix warning: zero as null pointer constant [-Wzero-as-null-pointer-constant]* Fix warning: private field 'first_for_' is not used [-Wunused-private-field],0
[ARM] Support NCHWc alter layout in the fallback mode (#10724)* [ARM] Support NCHWc alter layout in the fallback mode* remove fallback path* add test* fixed int32_lanes and add channel check* fixed schedule dispatch bug* add workaround fallback path for NHWC im2col based GEMM schedule* int32_lanes=4 by default* typo* update test,0
"Eliminate some compiler warnings for microNPU demo app (#10549)* Eliminates all ""control reaches end of non-void function"" warnings * Eliminates all ""unused variable"" warnings * Eliminates some ""implicit declaration of function"" warningsChange-Id: Icba390f3e821e42f37066a1e4522c26d50a92380",4
"[TVMC] compile: Check if FILE exists (#10608)Currently when a non-existing FILE is passed to 'tvmc compile' it throwsa traceback because a FileNotFoundError exception is not handled. Sincethere is no need for such abrupt exit, and the trace can also confuseusers, this commit fixes it by checking if FILE indeed exists, informingthe user about the non-existing FILE before exiting.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
Remove depreceated mointergration tests with mxnet zoo importers (#10772)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,3
"[microNPU] Remove identity operations between non-compute operations (#10411)Builds upon the work in #10254 to remove identity operations sandwichedbetween two non-compute operations (reshape/strided slice - concatenateis handled differently), under certain conditions. Specifically, anidentity operation is not removed when the dimensionality between thetwo non-compute operations is reduced, due to non-congruent valuesbeing accessed incorrectly. For example,```strided_slice(dims=4) -> identity -> reshape(dims=4)```becomes...```strided_slice -> reshape```but,```strided_slice(dims=4) -> identity -> reshape(dims=2)```remains as...```strided_slice -> identity -> reshape```Change-Id: Ie28ba384fcb3230d6f4651c0c19e2b9526ebcc42",2
[TIR][MetaSchedule] Estimate TIR FLOPs (#10782),5
[TESTING] Mark CMSIS-NN test in TVMC tests (#10674)Currently failing with `USE_CMSISNN` set to `OFF`,3
Check for toolchain when marking reference system tests (#10659)This mimics the behaviour of aot_test_utils.py to ensure the tests don't start running when the toolchain isn't available.,3
"[CMSIS-NN] Stop test generating 1x1 and 1xn Conv2d (#10784)I believe the flakiness in #10748 is the small chance of generating a1x1 or 1xn convolution which allows for a different buffer size:https://github.com/apache/tvm/blob/63461c0c97c307e581271708c3490f5275675a1a/src/relay/backend/contrib/cmsisnn/buffer_size.cc#L38-L47Therefore, careful selection of the distribution should alleviatethis issue.",3
"[Hexagon] Improved ergonomics of HexagonLauncher in unit tests. (#10581)* [Hexagon] Improved ergonomics of HexagonLauncher in unit tests.The goal of this commit is to reduce/eliminate common code requiredthrough unit tests that interact with Hexagon hardware.- New testing fixtures in `tests/python/contrib/test_hexagon`.  A test  running on hexagon hardware should only need to use the  `hexagon_session` fixture.  - `rpc_server_port`: Iterates through port numbers, selecting an    unused port for each unit test.  Avoids needing to explicitly    specify unique ports for each unit test.  - `tvm_tracker`: Starts a tracker on use, exits after test.  Avoids    needing to manually start a tracker prior to running the unit    test.  - `hexagon_launcher`: Starts a `HexagonLauncher` server on use,    stops server after test.  Avoids needing to call `start_server()`    and `stop_server()` in each test.  - `hexagon_session`: Starts a hexagon session using    `hexagon_laucnehr.start_session()`, exits after test.- Added `Session.upload` function, which delegates to  `HexagonLauncher.upload`.  Avoids needing to interact with both the  launcher and the session.- Allowed `tvm.IRModule` as argument passed to `Session.load_module`,  which will automatically save/upload the module, then load it.  Avoids needing to handle save/upload of temporary files in each unit  test.* Added default port for tracker if not already set.* Pass through None from hexagon_launcher to hexagon_session.* Updated launcher to use external tracker if specified.* Avoid setting up the local tracker unless required.* Declare previous_port as global, instead of list.* Corrected type hints.* Docstring updates",0
"[CI] Enable integration tests on AArch64 (#10677)As part of this any failing tests have been marked for follow up as part of https://github.com/apache/tvm/issues/10673.This depends on fixes in https://github.com/apache/tvm/pull/10659, https://github.com/apache/tvm/pull/10672 and https://github.com/apache/tvm/pull/10674 to scope other tests correctly.",0
[TIR][Analysis] Add SuggestIndexMap for layout rewriting (#10732)This PR added an analysis function `SuggestIndexMap` to analyze buffer access pattern and suggest index map for layout transformations.Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Junru Shao <junrushao1994@gmail.com>Co-authored-by: Xiyou Zhou <xiyou@octoml.ai>,1
"[microNPU] Add a pass to move allocate nodes to the outer scope (#10725)* [microNPU] Add a pass to move allocate nodes to the outer scopeAdds a pass called `HoistAllocates` to move allocate nodes to the topof the body of the main function. In doing so, it opens the door toother optimizations that need to swap the ordering of external calls.Pass illustration:(before)```allocate {    extern_call {        allocate {            extern_call {            }        }    }}```(after)```allocate {    allocate {        extern_call        extern_call    }}```Change-Id: Ibcfc3c75b15deebb5c6645a4923a6ddf683b37c4* address comments* uses prim func pass, rather than module pass.* adds error message informing user to run this pass with LowerToTIR()  pass for now.Change-Id: I57757b9dc5bff0208034a974a341c09cce0294bc* Support allocates when not followed by a sequence statementWith a test to back this case up.Change-Id: I670809f5ee53b583a15d9b783852dda3089756e9* Add new directory tir/contrib/ethosu to cmake buildChange-Id: I3e9f24adfe992ace4e03238a18a8378b03257e1a",0
"[ci] Generate Jenkinsfile from a template (#10740)* [ci] Generate Jenkinsfile from a templateThis uses `jinja2` to generate the Jenkinsfile. This is useful since it lets us both keep common functionality easy to define (i.e. iterate over all images and do something) while keeping the output easy to debug (you can look at the `Jenkinsfile` directly instead of trying to imagine what the Groovy interpreter will do). This will become more useful as we start to make CI more configurable, such as adding dynamic test sharding.This mostly introduces the infrastructure and makes some token changes to demonstrate the generation process, but already its use is shown since the parameters was missing an entry for the `ci_hexagon` image.* Address comments, fix CI with temporary workaroundCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[ONNX] fix reduce crash on scalar inputs (#10780)* fix reduce crash on scalar inputs* fix uncovered cases.* fix on different opset to pass ci,0
[skip ci][ci] Fix bad merge after moving to templated Jenkinsfile (#10792)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[Hexagon] Correct use of wrong cmake variable (#10769)The code should be checking DSPRPC_LIB_DIRS instead of REMOTE_DIR.,2
[TVMC] Fix wrong terminology in tvmc source (#10320)Renames variable from `runtime` with `executor` to betterreflect current terminology and reduce confusion.,0
"[TVMScript] Parser `int64` support (#10789)## ContextWhen dealing with end-to-end models, we note that some tensors may have large shapes. Thus, when designing graph-level IR, we sometimes use `int64` instead of `int32` for the shape. Below is an dense GeMM example which has `int64` input tensor shape:```python@tvm.script.ir_moduleclass Module:    @T.prim_func    def main(rxplaceholder: T.Buffer[(1, 512), ""float32""], rxplaceholder_1: T.Buffer[(T.int64(1000), T.int64(512)), ""float32""], T_matmul_NT: T.Buffer[(1, T.int64(1000)), ""float32""]) -> None:        # function attr dict        T.func_attr({""global_symbol"": ""dense"", ""tir.noalias"": True, ""op_pattern"": 3})        # body        # with T.block(""root"")        for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 4, 1, 25, 8, 1, 10, 64, 1, 1):            with T.block(""T_matmul_NT""):                i = T.axis.spatial(1, 0)                j = T.axis.spatial(T.int64(1000), i1_0 * T.int64(250) + i1_1 * T.int64(10) + i1_2)                k = T.axis.reduce(512, i2_0 * 64 + i2_1)                T.reads(T_matmul_NT[i, j], rxplaceholder[i, k], rxplaceholder_1[j, k])                T.writes(T_matmul_NT[i, j])                T.block_attr({""layout_free_placeholders"":[rxplaceholder_1], ""meta_schedule.tiling_structure"":""SSRSRS""})                with T.init():                    T_matmul_NT[i, j] = T.float32(0)                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]```## ProblemThough our TVMScript printer can easily print `int64` constants, the parser had poor support for `int64`. So this PR introduces some parser support for `int64`, basically about the data type of loop variables, block iterators and block read/write regions.Besides the parser, most of the TIR schedule primitives didn't take `int64` into account in their implementations. These schedule primitives will be fixed and updated in recent future, in followup PRs.",0
"[Hexagon] 2-d allocation cleanup (#10786)- Added device validity check in allocation. HexagonDeviceAPI should  only be called for CPU/Hexagon types.- Check for ""global.vtcm"" scope instead of ""vtcm"".  The ccope of N-d  allocations produced by `LowerVtcmAlloc` should be `""global.vtcm""`.  The previous check allowed unsupported scope such as `""local.vtcm""`.- Remove `vtcmallocs` entry after calling free. Previously, the vtcm  allocation map kept dangling pointers to `HexagonBuffer` objects  after they had been freed.- Rename N-d alloc and free packed functions.  Since most of the  similar device functions use snake case, renaming `*.AllocND` to  `*.alloc_nd` and `*.FreeND` to `*.free_nd`.Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Adam Straw <astraw@octoml.ai>",1
"[Runtime][PipelineExecutor] Fix CPU affinity setting issue. (#10781)Found the CPU affinity setting not work in pipeline executor, thesymptom is that there is no perf change after doing cpu affinitychange. the reason is that only the ConfigRuntime stored the cpuaffinity setting but the BackendRuntime class not.",0
"[BugFix][TIR] Fix construction of new IterVars in CacheRead/Write for non-int32 dtypes (#10795)_This PR is a follow-up effort of #10789, which enables the `int64` support for TIR schedule primitive Cache-Read and Cache-Write._Prior to this PR, the IterVars of the generated cache stage block are always `int32`-typed, which might conflict with the dtypes of the domains of the IterVars.In this PR, the dtype of new IterVars are constructed according to the data types of their domains, and thereby the possible conflicts are resolved. Meanwhile the data types of the read/write regions of the cache stage blocks are also constructed according to correct data types.",0
"added surpport for arg type of numeric float16 and testcase, fixed the (#10797)cierror",0
[PyTorch] Fix neg indexing issue for `aten::flatten` (#10796),0
use python3.7 install script in ci-qemu (#10799)* use python3.7 install script in ci-qemu* update pyton venv to 3.7* setuptools is just python3...* don't use apt-add-repository (breaks with python3.7 as python3 on ubuntu 18.04,1
[microNPU] Remove unused import and command stream printing (#10764)This is a follow up to https://github.com/apache/tvm/pull/10695.Change-Id: I7f2dc14826cefea81fe5ff69c6255cdb5dc7f5c0,4
"[LIBXSMM] Add libxsmm to tvm ci (#10179)* [LIBXSMM] add libxsmm to TVM CI.* Config ""make"" thread number in a more flexible way.Co-authored-by: Cody Yu <comaniac0422@gmail.com>* Empty commit to trigger github CI.* Update ubuntu_install_libxsmm.sh.* Trigger CI tasks.* Trigger CI tasks.Co-authored-by: wenxizhu <wenxizhu@tencent.com>Co-authored-by: Cody Yu <comaniac0422@gmail.com>",1
"[microNPU] Use TF reference kernels for codegen tests where possible (#10762)Uses reference kernels for the codegen tests when the version ofTensorflow is >= 2.5.0 (as this is the first version this functionalitywas added). Otherwise, fallback to running the tests without.Change-Id: I92b24ad259d2fda2fed497aa0fe6d7f11a0db85a",1
[ARM] Fix NCHWc int8 dot product schedule lowering (#10773)* [ARM] Fix NCHWc int8 dot product schedule lowering* fix arm task extraction test not running* skip test on i386,0
[Arith] Remove diagnostic ctx argument from DetectIterMap (#10798),4
Also strip prefix from TVM_LOG_DEBUG specs. (#10755),0
[AutoScheduler] Supported CSE (variable definitions) in feature extraction (#10686)Add supported for LetStmts in feature extraction. A stack of variable definitions is maintained and added to the arithmetic analyzer at the appropriate points. The buffer access analysis now creates a new arithmetic analysis context per set of loops to avoid redefining variables which is unsafe in the presence of let statements.,1
"[TIR] Properly initialize PRNG seed when copying schedule (#10806)* Make Schedule::Copy non-const, fork RND seed in Copy* fork seed in traced schedule copy toocommit eeb4a6d4b34909822ea5d56488afd11f254e53a9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Mar 29 06:39:38 2022 +0900    add more commentcommit 183b4cfe5d7938d5e440a9d77b7e8c3871544966Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Mar 28 10:04:12 2022 +0900    skip flaky vk testcommit c19ecc17afc8ee1b54aa2260bffb4e1d431ab429Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon Mar 28 07:34:25 2022 +0900    move intrin decl for vector typecommit 3dd7f045f791b805012227ab4ee866995cc5297dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 09:40:29 2022 +0900    disable default post processor, tuning now works with compactness checkcommit 2f6fdae675975e2bd95a086dce8a88d9f267746dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 08:08:35 2022 +0900    more commentcommit c7ebfa904367885442f928c0cecf875190341930Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:42:46 2022 +0900    add commentcommit 78400bad77f5201b3cebfb8a7fee0642adead060Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:40:28 2022 +0900    disable tuning test for nowcommit a33243fbf91863f0a834505cd4936c0fff228603Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:30:03 2022 +0900    remove annotation check in ir comparatorcommit 105f98cc76081d46dddbda47dba2578a25cfadb2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:28:36 2022 +0900    clean upcommit 8aa16f209ee709375d90f2c3a5883a47df6ce104Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat Mar 26 07:15:24 2022 +0900    Add test* add test case that hangs without forkseed",1
[ci][docs] Disable matplotlib intersphinx (#10808)This is failing in CI so let's disable it until we come up with a better way to deal with network failures like theseCo-authored-by: driazati <driazati@users.noreply.github.com>,2
tir printing (#10805),5
"[BugFix] Fix NeedsMultiLevelTiling by skipping trivial block iterators (#10804)This PR fixes a bug of `NeedsMultiLevelTiling`, which didn't consider the effect of trivial block iterators (iterators whose domains are `[0, 1)`). Such iterators impacts the following analysis by overlargely counting the number of iterators that are not used to index the block read regions, and might lead to the application of multi-level tiling where the rule is supposed not to apply.To fix the problem, we simply skip such trivial block iterators.",0
upd (#10809),5
[MetaSchedule] Extract task weights during task extraction (#10810)* [MetaSchedule] Extract task weights on task extraction* Update test_meta_schedule_integration.py,1
[MetaSchedule] Support grouping in the cost model (#10811),5
[COMMUNITY] David Riazati -> Reviewer (#10812),3
Fix compile error when AddRewrite gets additional args (#10669),0
[CI] Fix Vitis-AI tests when USE_VITIS_AI flag set to OFF  (#10802)* Register relay.ext.vitis_ai.available function* Fix vitis-ai tests when running with USE_VITIS_AI OFF* Replace skip_test with pytest skipif* Add a function to see if vitis_ai is available* Use requires_vitis_ai function for running tests,0
[ci] Clarify message in ping-reviewers bot (#10807)This makes the next actions more clear (i.e. convert the PR to a draft if you don't plan to address it soon) and also fixes a bug where `@`-ed users would get double-tagged.Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"[Pass][Bugfix] StorageFlatten, buffer var definitions in LetStmt (#10788)Previously, any buffers whose buffer var was defined in a Let orLetStmt would result in an error when running `StorageFlatten`,stating that the buffer var was undefined.  These are used forallocations in external calls, such as those produced by`LowerVtcmAlloc`.",0
[dyn.nn.pad] cast pad value to input dtype (#10818),5
[CI] Update `ci-qemu` to use python 3.7 (#10815),1
"[ci] Manually merge code after checkout (#10778)* [ci] Manually merge code after checkoutThis is step 1 of 2 to fix the issue where Jenkins schedules a PR rebuild on non-code changes like editing the title. When the title is changed, GitHub sends an `pull_request` webhook to GitHub. Jenkins would ordinarily look at the webhook, query the PR and checks if the commit hash for the build has already been built. However, since we have it set in the Jenkins job to merge with main before this check occurs, the commit hash is different nearly every time. Disabling that setting fixes the issue, but we still need to merge the code with `main` to ensure that the build is valid if it completes successfully.* Use the same commit everywhere to mergeCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[REFACTOR] Remove legacy nnvm folder (#10821)nnvm was the first generation IR that was maintained by TVM before the community moved to a newer generation.The community keeps it for a few more releases(v0.7 and v0.8).This PR removes the folder. The source code can still be found in past releases.,1
[skip ci][ci] Fix outdated Jenkinsfile (#10822)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[DOCS] Add background context for stack allocation in builtin lowering (#10632),1
"[microNPU] Tweak a layout transform matrix (#10763)* [microNPU] Fix layout transform matrixOne of the layout transforms currently causes the cascader to stripeacross B16 axis (which is not allowed), so change that and deal withthe implications to the get_valid_block_configs.Change-Id: I04199f9f35fcc31618581567483cfb80d3b5aad2* Reduce the duplication of layout transfrom matrices* Change the nhcwb16_to_nhwc matrix for binary and unary elementwise  such that it matches the other NPU ops* Reduce the number of places where the same layout transform matrices are  defined* Add documentation to the layout transform matrices",0
[ETHOSN] int8 support for tanh operator (#10813),5
[ETHOSN] int8 support for the mean operator (#10463)- Updated the test; no other changes necessary- Parameterized the tests for easier failure analysis,1
"[CUBLAS] Add cuBLAS as a Relay partitioning target (BYOC) (#10820)* [CUBLAS] Add cuBLAS as a Relay partitioning target (BYOC)This PR adds a partitioning pass for cuBLAS so thatsupported Relay patterns can be offloaded to cuBLAS.This initial commit only adds offloading supportfor nn.matmul.Although cuBLAS is already enabled in TVM by usingstrategy selection in TE, by exposing it explicitlyas a Relay partitioning target we can more preciselydescribe how to execute a model in Relay. This isdesirable particularly in the Collage effort toimprove multi-backend graph partitioning.* Refactor to remove boilerplate",1
"bump PyTorch version to 1.11 (#10794)* bump PyTorch version to 1.11* disable some caffe2 ci* Fix sub conversion in PyTorch frontend* use fuse_modules_qat if available, fallback to fuse_modules for older PyTorch* Re-Run CI",0
[skip ci][hotfix] Fix broken lint (#10827)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"[TE] Correctly generate buffer binds with axis separators (#10819)In SchedulePostProcToPrimfunc, when the axis separator attribute ismoved to the buffer properties, it doesn't update buffers that are inthe buffer bind scope.  This occurs if `Stage.tensorize` is called fora stage whose layout transformation includes `te.AXIS_SEPARATOR`.",1
"[Pass][Bugfix] Disable re-use of non-flat buffers in StorageRewrite. (#10787)* [Pass][Bugfix] Disable re-use of non-flat buffers in StorageRewrite.As a follow-up from https://github.com/apache/tvm/pull/9727,restricting StorageRewrite to only modify flat memory buffers.  Whenrewriting, the existing algorithm in StorageRewrite flattens N-dallocations into 1-d allocations, preventing them from being exposedto the codegen.* Bugfix, flattening of Allocate/AllocateConst extentsPreviously, these were ignored entirely.  This worked so long as allallocations were 1-d, as `StorageRewrite` erroneously flattened mergedarrays into 1-d.",0
"[Metaschedule] Add demonstration of selectively tuning relay ops with TIR schedules  (#10793)This demonstrates how to selectively extract and tune tasks from a whole relay mod, and apply the tuned schedule during the final `relay.build(...)`. This flow is entirely different from existing tests in `test_meta_schedule_tune_relay.py` where ALL ops are extracted and auto-scheduled by MS. My test extracts only int8 `dense` op, applies a manual TIR schedule on it, and leaves int8 `batch_matmul` to be scheduled by TE. This also serves as an example of autotvm style manual template + tensorization. The manual TIR schedule is equivalent to TE VNNI `dense` schedule in https://github.com/apache/tvm/blob/ce335c3a74185df6cc1152e53c60695d8a418d8e/python/tvm/topi/x86/dense.py#L366-L375",1
"[ci][docker] Remove nvidia ml repository before updating (#10828)`bash docker/build.sh ci_gpu` fails locally as well as in CI: https://ci.tlcpack.ai/blue/organizations/jenkins/docker-images-ci%2Fdaily-docker-image-rebuild/detail/daily-docker-image-rebuild/273/pipeline/57From [this post](https://forums.developer.nvidia.com/t/failed-to-fetch-https-developer-download-nvidia-com-compute-machine-learning-repos-ubuntu1804-x86-64-packages-gz/156287), so long as the build completes (meaning we don't use any images from this repo), it should be fineCo-authored-by: driazati <driazati@users.noreply.github.com>",2
[MetaSchedule] Fine-Grained Rewrite Unbound Block (#10823)In this PR we introduced more fine-grained loop spliting and reordering for Rewrite-Unbound-Block post processor based on given cuda target's attribute (`max_threads_per_block`). After this PR the performance of non-reductional kernels could improve by ~20%. Regression tests are also added.,1
"[Hexagon] Pass extra parameters to link_params via Map (#10830)There is no way to pass kwargs dictionary from C++ code, so the previousway never worked. Use TVM's Map instead, and pass the target architectureversion to the linker to use libraries specific to the architecture.",2
[Hexagon] Handle v69 in RPC launcher on simulator (#10829)There are a few tweaks that are needed related to directorystructure in the SDK.,5
[CUBLAS] Add support for nn.dense and nn.batch_matmul (#10826)* [CUBLAS] Add support for nn.dense and nn.batch_matmulThis commit includes a fix for cublas.batch_matmulwhen mixed precision is being used.* Specify args in dense,0
"prune dnnl subgraph, and add related test case. (#10835)",1
[MetaSchedule] Add Gradient Based Task Scheduler (#10366)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
[microNPU] Some housekeeping in the test_ethosu folder (#10824)* [microNPU] Some housekeeping in the test_ethosu folder* Move the utility functions from test_codegen.py into infra.py for  wider accessibility* Remove some unused code* Make the conv2d codegen tests more general* Update test_identity_optimizer.py* Update test_lut_optimizer.py,1
[TVMC] Support compiling and running with VM  (#10722)* introduce vm compile path* support vm in tvmc* cleanup + lint* add profiler + simplify vm case in tvmcpackage* address comments + parametrize testsCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
[BYOC-DNNL] Enhance GetRootCall function (#10836)* enhance dnnl codegen to support different topology of partition graph* fix lint* add test case,0
[Hexagon] Provide empty weak definitions of two missing functions (#10847),5
"[ARM] Fix int8 NCHWc compute and alter layout (#10839)This PR fixes a bug in TE ARM int8 compute for NCHWc conv2d, introduced in https://github.com/apache/tvm/pull/10310. The compute itself, not the schedule, is broken for the following reasons:* We are using `n_elems = 8` in https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_alter_op.py#L350. Thus, the innermost axis of the transformed kernel has extent 8: https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_alter_op.py#L375* In the TE compute, we iterate over the innermost axis `ic_s_inner` of the kernel at https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L577. `ic_s_inner` has extent `n_elems` according to https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L566. `n_elems` is 4 by default according to https://github.com/apache/tvm/blob/f6f252f0abc8f621a96506739f9534083d1fe213/python/tvm/topi/nn/conv2d.py#L478* The ARM code that calls this compute does not explicitly pass `n_elems`, according to https://github.com/apache/tvm/blob/e9091d6c68d5d70c28881e5c75bfe72e385c1f4d/python/tvm/topi/arm_cpu/conv2d_int8.py#L106-L108* Thus, even though the innermost axis of the kernel has extent 8, the TE compute only loops over `n_elems = 4` of the input channel dimension. Initially, I tried to keep `n_elems = 8` in alter layout and fix the intrinsic definition. But `n_elems = 8` breaks tensorization pattern matching, since now the compute is doing 4x8 innermost loop but this intrinsic is supposed to do 4x4 dot product, see https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/tensor_intrin.py#L467-L479. Setting `num_int8_elements = 8` there does fix the tensorize pattern matching, but the result was still incorrect.Rather than fixing the intrin implementation in https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/tensor_intrin.py#L492 to adapt for 4x8 dot product, I settled on setting `n_elems = 4` in alter layout. It turned out this change is enough to get the correct output. Moreover, `n_elems = 8` is simply wrong for the dot product path in https://github.com/apache/tvm/blob/7896108fc41663a1fecbb52345194a93278e9e28/python/tvm/topi/arm_cpu/conv2d_int8.py#L154-L155 which computes 4x4 dot product in one instruction. @tkonolige I suggest doing perf benchmark again, since the numbers in https://github.com/apache/tvm/pull/10310 are invalid.cc @mbrookhart @Mousius  @junrushao1994 @vinx13",0
"[PROFILING] Various fixes for profile_function (#10850)Check that the function to be profiled is actually defined.Check that the MetricCollector used actually can time the regionrequested.Default to using the module's entry_name instead of ""main"".",0
[Hexagon] Don't use alternative linker for non-x86 API binaries (#10854),5
[CI] Update GPU image for PyTorch 1.11 (#10849)* Update Jenkinsfile to point to the new GPU image* fix onnx test* fixed keras tutorial,0
"[Metaschedule] Add test case for multi-anchor subgraph (#10856)This adds a demonstration of extracting, scheduling, and e2e-compiling relay subgraphs with multiple anchor ops. Since task extraction is not associated with TE scheduling anymore, extracting a subgraph with multiple anchor TE compute just works.The test case manually creates a simple fused mod with two `relay.dense`. But in the future, an effort like https://github.com/apache/tvm/pull/9628 should make it easier to construct multi-anchor subgraphs.The extracted TensorIR block corresponding to two TE `dense` compute looks like this:```@tvm.script.ir_moduleclass Module:    @T.prim_func    def main(placeholder: T.Buffer[(128, 128), ""float32""], placeholder_1: T.Buffer[(128, 128), ""float32""], placeholder_2: T.Buffer[(128, 128), ""float32""], T_matmul_NT: T.Buffer[(128, 128), ""float32""]) -> None:        # function attr dict        T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})        # body        # with T.block(""root"")        T_matmul_NT_1 = T.alloc_buffer([128, 128], dtype=""float32"")        for i0, i1, i2 in T.grid(128, 128, 128):            with T.block(""T_matmul_NT""):                i, j, k = T.axis.remap(""SSR"", [i0, i1, i2])                T.reads(placeholder[i, k], placeholder_1[j, k])                T.writes(T_matmul_NT_1[i, j])                T.block_attr({""layout_free_placeholders"":[placeholder_1]})                with T.init():                    T_matmul_NT_1[i, j] = T.float32(0)                T_matmul_NT_1[i, j] = T_matmul_NT_1[i, j] + placeholder[i, k] * placeholder_1[j, k]        for i0, i1, i2 in T.grid(128, 128, 128):            with T.block(""T_matmul_NT_1""):                i, j, k = T.axis.remap(""SSR"", [i0, i1, i2])                T.reads(T_matmul_NT_1[i, k], placeholder_2[j, k])                T.writes(T_matmul_NT[i, j])                T.block_attr({""layout_free_placeholders"":[placeholder_2]})                with T.init():                    T_matmul_NT[i, j] = T.float32(0)                T_matmul_NT[i, j] = T_matmul_NT[i, j] + T_matmul_NT_1[i, k] * placeholder_2[j, k]    ```",1
"[Hexagon] Support both 1-d and 2-d VTCM allocations (#10846)* [Hexagon] Support both 1-d and 2-d VTCM allocationsPreviously, all VTCM allocations were assumed to be 2-d buffers.  Thiscommit extends `HexagonDeviceAPIv2::AllocVtcmWorkspace` to allow both1-d and 2-d VTCM allocations.  Matching the semantics used in`CodeGenHexagon::CreateBufferPtr`, allocation of 1-d buffers returns a`void*`, and allocation of 2-d buffers returns a `void**`.Co-authored-by: Adam Straw <astraw@octoml.ai>* [Hexagon] Distinguish between 1-d buffer and single-alloc 2-d bufferPreviously, HexagonBuffer represented 1-d buffers as 2-d buffers with`nallocs==1`.  Since this is used to determine the return type of thedata pointer exposed to the generated code, the ambiguity between`shape=[N]` and `shape=[1,N]` must be avoided.  This commit replaces`HexagonBuffer::nallocs_` with `HexagonBuffer::ndim_`, avoiding thisambiguity.* [Hexagon] Treat ""global"" scope allocations as 1-dThis updates `HexagonDeviceAPIv2::AllocDataSpace` to follow thesemantics of `DeviceAPI::AllocDataSpace`, to avoid breaking callerassumptions in `tvm.nd.array` or graph_executor/aot allocation.* Updated C++ unit tests for HexagonBuffer* Remove commented GetNumAllocs and unused GetBufferDimensionCo-authored-by: Adam Straw <astraw@octoml.ai>",1
Fix typo in comment about kill() (#10863)Fix typo in comment about kill method in PopenWorker class used to killchild processes created by the worker.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
[TVMC] tune: Use proper caps for AutoTVM and AutoScheduler (#10864)Use proper caps in help messages when mentioning AutoTVM andAutoScheduler tuners.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,2
"[Pass] Fix printer formatting for PassInfo (#10844)* [Pass] Fix printer formatting for PassInfoThe format of the printed PassInfoNode was difficult to read as therewas no separation between the different attributes of the nodeBefore this change, PassInfo is printed as:The meta data of the pass: pass name: tir.ApplyLayoutTransformsopt_level: 0required passes: []After this change, PassInfo is printed as:The meta data of the pass - pass name: tir.ApplyLayoutTransforms, opt_level: 0, required passes: []* Restart CI",0
[AOT] Re-enable AOT output name test on AArch64 (#10868)This was fixed in https://github.com/apache/tvm/pull/10731 as it was the mismatch of tensorflow versions in use by the different CI containers.,0
"[Pattern] add optional pattern to C++ syntatic sugar (#10872)Add an optional pattern syntatic sugar to the C++ pattern language to match python. While writing the tests, I noticed that the const definitions on the methods weren't quite right (I couldn't do nested syntatic sugar calls), so I fixed that as well.",0
"[Hexagon] Remove timeout on HAP_compute_res_acquire (#10713)* [Hexagon] Remove timeout on HAP_compute_res_acquireWhen run on the simulator, a non-zero timeout for this call willbusy-loop.* Added TODO for further investigation.",1
Don't use alternative linker for static libraries (#10870)Otherwise we may end up with things like this (make VERBOSE=1):/usr/bin/ar qc libtvm_runtime.a  -fuse-ld=lld CMakeFiles/tvm_runtime_ob.../usr/bin/ar: invalid option -- 'e'Usage: /usr/bin/ar [emulation options] [-]{dmpqrstx}[abcDfilMNoPsSTuvV]...       /usr/bin/ar -M [<mri-script]...,2
"[Hexagon] Updated incomplete docstring (#10879)As a follow-up from https://github.com/apache/tvm/pull/10846,completing a docstring that unintentionally ended in the middle of asentence.",1
[MetaSchedule][BugFix] Fix broken integration tests (#10885),0
[BugFix][MetaSchedule] Fuse only serial loops in rewrite-unbound-block (#10883),0
Fix a small timer bug. (#10875),0
Bump pyxir version tp v0.3.5 to avoid bad cleanup error with pyxir and tensorflow 2.6 (#10858),0
"[PTX] `ldmatrix` builtin to accelerate copying data from shared memory to warp memory (#10855)We already have PTX mma and mma.sp builtin support in #9909  and #10339 . However, we have not supported corresponding data movement builtins for these mma instructions, so the data movement would not be as fast as wmma.This PR brings the `ldmatrix` builtin, which is a native PTX warp-level instruction (https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix), and we can use it to load several (1/2/4) 8x8 matrices from shared memory to warp memory.",2
Optimize the implmentation of scale (#10884),5
[runtime-hexagon-rpc] more Hexagon/Android logging (#10767)- Alter `android_bash.sh` to meet the runtime conditions needed  for FARF logging.  (Note that FARF logging is also governed  by certain preprocessor definitions.)- Alter `android_bash.sh` so that any stdout/stderr emitted  by `tvm_rpc_android_server` is saved to a log file  (`tvm_rpc_android.log`). Previously that output was simply  lost.,2
Fix Arduino workspace alignment (#10886)* Fix Arduino workspace alignment* Fix linter error* Rerun tests,0
"Update Docker image with tag 20220404-055909-fcdf4636d (#10889)Updates docker image with tlcpackstaging 20220404-055909-fcdf4636dto update TensorFlow to 2.6 and solve Vitis-AI build fixes caused byan allocation error from TensorFlow 2.6.Also updates Keras, h5py and pyxir.",0
"[skip ci][ci] Fix black version (#10893)See https://github.com/psf/black/issues/2964, this is broken in CI now after the update in c2744704bec3cc914fa96dc4f4c9b0ccfaa8ace4. This rolls back the Docker change and includes a fix for when we update to `ci_lint:v0.71`.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
"[Hexagon] Select qaic executable based on Ubuntu version (#10891)* [Hexagon] Select qaic executable based on Ubuntu versionAllow users to override the selection via QAIC_PATH_OVERRIDEenvironment variable, for example on non-Ubuntu systems thatcan still run Ubuntu binaries.* Address review comments- Remove repeated call to _check_path_exists.- Return qaic_path-NOTFOUND is qaic is not found.- Change SEND_ERROR to WARNING when qaic is not found, since not finding  of other properties is not an error.",0
"[BYOC][TRT] Add DFPattern support for TRT backend (#10759)This PR adds DFPattern support for the TRT backend without removing the existing predicate registry.Adds and extends the following:In tensorrt.py: Add a pattern_table for all the supported ops and consumes the pre-existing op_registry checksAdds an additional pass as unmerge_composites.cc. This is required for the TRT backend as it expects a single primitive function to work with, while the MergeComposite and PartitionGraph will produce a single function for each Composite pattern.Adds test_inline_composites.py which tests the newly introduced pass.Both the pattern-based and predicate-based pass sequences produce syntactically equivalent IRModules.This is to ensure backwards compatibility.""",1
Fix submodule URLs. (#10888)- Add `.git` suffix.- Remove incubator prefix.,0
Handle uint8 in ConstantNode visitor in LowerToTECompute (#10894),5
[RUNTIME] Api to get number of runtime threads (#10896)* [RUNTIME] Api to get number of runtime threadsAdd `tvm::runtime::threading::NumThreads` and `tvm.runtime.num_threads`as a way to get the number of threads in use by the TVM runtime.* check if equal to hardware threads or hardware threads/2,1
[LLVM] Support CodeGenBlob for large >2GB models on x86 (#10882),5
[TIR] Fix int32 vs int64 mismatch in For construct. (#10595)* Respect dtype in Scalarize.* Add unittest.* Fix lint.* Promote dtype of IntImm to match loop_var in For.* Fix dtype mismatches.* Lint* Lint.* jostle ci* Match dtype in hybrid parser.,0
[TIR] Fix check for multiple axis separators (#10845)Fix check for more than 1 axis separators and add 3d test,0
"[microNPU] Flatten after allocates have been removed in HoistAllocates pass (#10890)Fixes a small issue which caused SeqStmts to get left behind in thebody of the inner allocate after hoisting the allocates. Flatteningnow happens after the mutation has happened, which consequently helpssimplify the pass.The `test_outer_seq_stmt` test case already had this behavior, so a newcheck has been added to catch this case.Change-Id: Ia9e8a12088bc87dbf931c2535b648c49e676ea20",0
[QNN] Add per-channel quantization to add/subtract/multiply (#10718)* Add per-channel quantization to QNN add/subtract/multiply* Add feedback* Add feedback - round 2* Fix for arm test* Add params to the test* Try again* Try int* Move lhs_axis and rhs_axis* Add as an attribute* Add quotes,0
"[COMMUNITY] Mehrdad Hessar -> Committer (#10901)Please join us to welcome @mehrdadh as a new committer to TVM.Mehrdad has greatly contributed to the Hexagon backend, tvmc bug fixes as well as microTVM implementation. He has been also very active in the PR reviews, community meetings and forum discussion to share his ideas.- [Commits History](https://github.com/apache/tvm/commits?author=mehrdadh)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:mehrdadh)- [Community Forum Summary](https://discuss.tvm.apache.org/u/mehrdadh/summary)",0
"Update rpc_module.cc (#10881)Sometimes our locally built module is wrapped as a submodule of the top module. In such cases, we want to fetch the symbol from the imported modules, other than only searching in the top level.",1
"[Hexagon] Generalized HexagonBuffer::CopyTo/CopyFrom (#10878)* [Hexagon] Generalized HexagonBuffer::CopyTo/CopyFromThis change operates on the allocation regions in a `HexagonBuffer`,rather than referencing the managed allocation owned by a buffer,handling copies between two sets of possibly discontiguous regions.This will be necessary to handle discontiguous buffers that cannot bestatically planned at compile-time, such as user-initiatedallocations, within a shared memory pool.Contiguous regions of memory are recognized and result in a single DMAcall.",4
"Handle float16 in ConstantNode visitor in LowerToTECompute (#10902)Load the bit representation of Float16 as uint16, and convert it to thecorresponding float32 value.",5
Add python installation script for Ubuntu 20.04 (#10841),1
[QNN] Fix qnn.dequantize scale and zp shape (#10880)* [QNN] Fix qnn.dequantize scale and zp shape* Rework* Add review feedback,0
"[MetaSchedule] Add utility API to ease using manual schedules  (#10876)As discussed in https://github.com/apache/tvm/pull/10856#discussion_r840324560, add a utility under `meta_schedule/testing/utils.py` to clean up the database boilerplate. Also using `DummyDatabase` instead of `JsonDatabase` for further clean up, as suggested by @junrushao1994 .",1
"Install gdb by default, sort packages (#10913)gdb is helpful to debug segfaults in CI problems and for local development. It's cheap, so I propose we just include it as standard.",0
[COMMUNITY] new committer -- gromero (#10911),1
"[ONNX] Add imports for Gelu, BiasGelu (#10898)As title. Adds imports for Gelu, BiasGelu from the com.microsoft onnx op domain.",1
[ci] Don't diff Python files when checking formatting (#10895),2
[ci] Look for any tags in issues before adding new tags (#10685)Previously this searched for a specific `cc @abc` line by itself before looking for people to tag. This led to a double-tag in #10679. The change here updates it to check for `@`-ed people anywhere in the PR/issue body and filters those out.,1
[CMSIS-NN] Aligned scale computation with TFLM to fix numerical mismatch (#10817)Fixes numerical mismatch in Conv2D layers byaligning order of output scale computationwith TFLM. Correct output scale is neededto calculate quantization parameters neededby CMSIS-NN.,0
[ETHOSN] Improved handling of 5d reshapes (#10860)Resolves an issue with 5d reshapes in the Yolo network and added atest case. Refactored the reshape tests to use parametrization.,1
"[Hexagon] Use single allocation to back 2-d arrays (#10903)* [Hexagon] Use single allocation to back 2-d arraysCurrently, each allocation allocates an entire page, so even arelatively small number of allocations can use very large amounts ofVTCM.  This commit changes calls to `AllocVtcmWorkspace` of shape`[N,M]` from performing `N` allocations of size `M`, to 1 allocationof size `N*M`.  Since `N` is usually much smaller than a page, thisreduces the total amount of memory required.This is an intermediate step, where the long-term solution is to usestatic planning for VTCM allocations.  This returns the same `void**`type as the static planning eventually will, but avoids excess memoryuse in the meantime.* [Hexagon] Maintain alignment of allocationsPreviously, when a single monolithic allocation is used to back a 2-dHexagon buffer of shape `[nallocs, nbytes_per_allocation]`, theallocation itself is aligned, but each individual region is not.  Thiscommit ensures that each individual region also followed the alignmentspecified.",4
[docs][ci] Add CI reproducability docs (#10912)This adds info about `ci.py` to the docs and also re-arranges things a bit to consolidate/de-duplicate informationCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[ci] Roll out teams-tagging to everyone (#10739)* [ci] Roll out teams-tagging to everyoneThis removes the check for opted-in users from #10317, making it so anyone can attach their names without having to also opt-in. Also included is a script to generate the list of owners from `CONTRIBUTING.md` which was used to update #10317.* Cleanup after discussionCo-authored-by: driazati <driazati@users.noreply.github.com>",1
[build] Update libinfo and add lint rule (#10774)* [build] Update libinfo and add lint ruleThis updates `tvm.support.libinfo()` to be in-line with the current tvm options. It also adds a lint rule to ensure these stay matched up in the future as well as a script to print out the options in more detail. This should add in communication when debugging (i.e. tell someone to run `python -c 'import tvm; tvm.support.describe()` to learn everything you need about their envrionment)* Fix pylintCo-authored-by: driazati <driazati@users.noreply.github.com>,0
"[ci] Add a tag to generated Jenkinsfile (#10825)This adds a timestamp to the generated Jenkinsfile that is ignored when `--check`-ing. This line should generate merge conflicts for updates that would not pass `--check` in CI on main, so PRs will need to be rebased and the Jenkinsfile regenerated.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
[skip ci][ci] Remove inplace flag from black script (#10918)This was erroneously added in #10895Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[Hexagon][LLVM][CodeGen] Make CodeGenHexagon a subclass of CodeGenCPU (#10908)* Initial pass at making CodeGenHexagon a subclass of CodeGenCPU* More cleanup of CodeGenHexagon* Remove unused func_handle_map_ from CodeGenHexagon* Remove CreateCallExtern and CreateCallPacked from CodeGenHexagon* Code style fixes* Run clang-format-10 over codegen_hexagon.cc,0
[skip ci][ci] Fix stale test in teams tagging (#10920)This is failing in `main` but as of 8e438683a4a815ae2d5b528360ae0f111501b607 it's not used anymoreCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[Hexagon] Refactor to keep HexagonBuffer private to the device api (#10910)* No longer return HexagonBuffer from device api.* fixup! No longer return HexagonBuffer from device api.,0
[TIR] Check dynamic shared memory in VerifyGPUCode (#10923),5
"[CI] Updated argument parsing of optional arguments in ci.py (#10906)* [CI] Updated argument parsing of optional arguments in ci.pyPreviously, optional arguments were identified by comparing the string`""typing.Optional""`.  This misses some cases, as `Optional[int]`expands to `Union[int, NoneType]`.  This commit updates the check toidentify `typing.Union` annotations where one of the types is`NoneType`.* Bugfix, correctly handle type annotations outside of `typing.*`",0
[TRT] Minor fixes on TRT python interface (#10917)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,0
[CMSIS-NN] Re-enabled skipped tests (#10928),3
[ci] Add sccache to remaining Docker images (#10751)* [ci] Add sccache to remaining Docker imagesThese just need sccache installed and available on the `PATH` to startusing it in CI* Remove docker/ from glob skip listCo-authored-by: driazati <driazati@users.noreply.github.com>,1
relax reorder primitive's  affineness check (#10887),5
[microNPU] Fix bug in microNPU demo app (#10930)* Fixes a bug in convert_image.py where the uint8 range image was cast to int8 without subtracting 128 first* Updates run_demo.sh to use an image that previously failed to be classified correctly but is now successully classified,0
"[microNPU] Set output tolerance of codegen and network tests to 0 (#10675)After the recent upgrade of Tensorflow to 2.6, we are now able to usereference kernels in order to verify the output. Thus, removing thetolerances previously added.Additionally, the network tests have been altered to use TFLite as areference, rather than TVM.",1
[MetaSchedule][Refactor] Clarify Integration Logic (#10927),2
"[Hexagon] Add unit tests executing 2-d VTCM usage (#10904)* [Hexagon] Add unit tests executing 2-d VTCM usagePreviously, the schedules in `test_2d_physical_buffers.py` werelowered and built into a `runtime::Module`, but were not executed.* Added a missing function definition in the PR branch.* Only run test_execute on simulator in CI",1
[Hexagon] Register basic strategies and schedules for common operators (#10919)These are just placeholders to enable building full models.,5
[TVMScript] Fixing T.buffer with typed positional arguments other than int32 (#10892)* workaround for T.buffer with typed positional arguments* address comments* fix linting,0
[TIR] VNNI and ARM dot product intrinsic for tensorization (#10925),2
[QNN] Fix per-channel broadcast with invalid axes (#10936)* [QNN] Fix broadcast for invalid axis* broadcast -> channel,0
[MetaSchedule][Minor] Fix Integer Overflow in Tuning Statistics (#10935)* [MetaSchedule][Minor] Fix Integer Overflow in Tuning StatisticsThis PR fixes the integer overflow when the flop count of a given workload is larger than `MAX_INT` during tuning statistics printing.* Fix linting.* Support printing int64_t.,0
[CI] Run frontend tests for aarch64 in CI (#10869)Using `task_python_frontend_cpu.sh` to begin with to match those usedin `ci_cpu` - can add more frontends after this initial set isfunctional.,1
"[CI][Docker]Update Hexagon docker image to Ubuntu 20.04 (#10932)* fix permission* update pip versionAdd ONNX, TFLite, Tensorflow and update SDK version",0
"[ci] Remove hardcoded test shards (#10743)This moves the sharding logic from being inlined in the Jenkinsfile to templated, so we can change just the number of shards and the test allocation in `conftest.py` and the Jenkinsfile will work to match. This also changes the test allocation from a manual balancing before to be random between shards. Each shard needs to know only its shard number and the total number of shards, then it hashes each test and skips it unless that hash falls within its allocated tests. This breaks up related tests across shards but has the downside that any change to the number of shards will shuffle around where the tests end up (but ideally this is rare as we settle on a good number of shards to use).This only does this for the GPU frontend tests but eventually we could expand it to more.Co-authored-by: driazati <driazati@users.noreply.github.com>",2
"[CUDNN] Add cuDNN as a Relay partitioning target (BYOC) (#10871)* [CUDNN] Add cuDNN as a Relay partitioning target (BYOC)This adds infrastructure to support offloading of Relaypatterns to cuDNN. In this initial commit, only softmaxis supported.* Refactor common TE BYOC code into separate file* Add test guard",1
"[Hexagon] Cleanup, remove obsolete comment (#10931)Should have been removed as part ofhttps://github.com/apache/tvm/pull/10581.",4
[FQ2I] Add abs to FQ2I (#10922)* add abs to fq2i* lint* special case for fq2i* np iinfo,1
"[BYOC-DNNL] enable conv3d->bn folding (#10837)* support conv3d bn folding* add test case for fold_scale_axis* modify lint* remove test cases* unify conv2d 3d impls, and add test cases.",1
"[Hexagon] Do not pass lookup_linked_params to graph executor (#10944)This function is no longer used or generated, so it comes from theregistry as an ""empty"" PackedFunc. If the lookup function is provided,the executor will expect it not to be empty, which leads to a failedassertion.",4
escape tvmscript's string literal (#10954),5
"[CI] Bump black version to 22.3.0 (#10960)* Make all required adjusts in the code to comply with the new version* Upadte ci-lint to v0.71, based on tlcpackstaging/ci_lint:20220411-060305-45f3d4a52",1
[ci] Don't diff when running clang-format  (#10933)* [ci] Don't diff when running clang-formatThis takes about 15-20 extra seconds but has the benefit of allowing users to replicate and fix clang format issues locally with ease.* format files* Add --fix flag* CommentsCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[ci] Install GNU parallel on lint image (#10951)This will make it so we can lint in parallel on a single machine and still get good output.Co-authored-by: driazati <driazati@users.noreply.github.com>,2
use ubuntu18 in docker android demo  (#10222)* use ubuntu18 in docker android demo + remove useless directives in script* remove redundant apt-get update* revert install python3.7 + use install_python3 for ubuntu1804Co-authored-by: pfk-beta <this_email_isnot_working@gmail.com>,1
[ONNX] Update onnx shape op with slice index support (#10947)* support shape op slice indices* lintCo-authored-by: Margaret Qian <mqian@octoml.ai>,1
"[ci] Break out test steps for Hexagon / microTVM (#10946)Since we gate all tests on all builds currently in Jenkins, the longest running build is a bottleneck for overall runtime. This moves them to their own test steps so that the longer-running GPU/CPU tests can start earlier. This should shave off another 30 minutes or so of CI time.As a follow up we can investigate per-platform parallelism, e.g. the CPU tests only wait on the CPU build, but Jenkins doesn't have good support for this so we might have to work on the UX a bit first.Co-authored-by: driazati <driazati@users.noreply.github.com>",2
[Hot Fix][Jenkinsfile]Fix Hexagon Parameter (#10966)* Fix ci_hexagon parameter* fix template,0
Update CONTRIBUTORS.md (#10972),1
Switch to CPU PyTorch (#10914),5
[CI][DOCKER] Add pytest-lazy-fixture to images (#10970)Install lazy-fixture pytest plugin. This is needed for PR #10865.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>,0
[HEXAGON] Split huge 1D DMA Transfers into smaller transfers with legal sizes. (#10971),5
"[Hexagon] Move aot/graph_executor interactions into launcher (#10907)* [Hexagon] Move aot/graph_executor interactions into launcherFollow-up from https://github.com/apache/tvm/pull/10581, applyingsimilar changes to the AOT and graph executor interactions.  Thismoves the file management and upload/download from the unit tests intothe launcher.* Added Session.test_executor to avoid duplication in graph/aot test.* Resolve lint errors* Moved link flags workaround out of session, into create_aot_shared* Separated Session.get_*_executor and Session.get_executor_from_factory* Updated to resolve lint error",0
"[Hexagon][LLVM] Enable/test tensorized Hexagon DMA on 2d transformed layout (#10905)* [Hexagon][LLVM] Enable/test tensorized Hexagon DMA- In the `CodeGenLLVM::CreateIntrinsic` handler for  `builtin::address_of()`, pass N-d indices to  `CodeGenLLVM::CreateBufferPtr`.  The base class implementation still  asserts that there is a flat memory space, while the  `CodeGenHexagon::CreateBufferPtr` override allows 2-d memory.- Enable tensorization in `test_cache_read_write.py`, using  `tir.address_of` to pass the lowered value.Co-authored-by: Adam Straw <astraw@octoml.ai>* [TIR] Allow buffer_bind_scope of N-d buffersPreviously, any `buffer_bind_scope` attribute that provides a viewinto a non-flat buffer would result in an error.  After this commit,`buffer_bind_scope` may be used for non-flat buffers, but use of`arg_buffer->elem_offset` within the body of the bind statement isstill an error.The `BufferNode::elem_offset` field represents the offset between thepointer of the backing allocation and the first element of the buffer.This offset is only well-defined for flat memory spaces.* update test to tensorize cache_read `y` (works) and cache_write `z` (fails)* add `split` to allow for tensorization of cache_write of `z`* fix typo and cleanup comment* add back original 1d test_cache_read_write* update comments* format errorCo-authored-by: Adam Straw <astraw@octoml.ai>",0
[CUDNN] Add partitioning support for conv2d and log_softmax (#10961),1
remove exception handling of autotvm xgboost extract functions (#10948),4
change Hexagon docker version (#10981),4
Support `qnn.conv2d` in FoldExplicitPading (#10982)* wip support pad + qnn.conv2d folding* works* Added test but structural equality is failing* fixed structural equality test using map_free_vars=True,0
[COMMUNITY] @guberti -> Reviewer (#10976),3
[ONNX] Add MatMulInteger importer (#10450)* implement matmulinteger* rm test* rm outdated comments* fix lint and review* wip* fixes* fix* alter tests* extra 4x4x4 step* comments,0
[TVMC] Allow output module name to be passed as a command line argument (#10962)* Allows module-name as a command line argument to tvmc * Updates microNPU graph partitioner to pass module name to PartitionGraph() * Updates CMSIS-NN graph partitioner to pass module name to PartitionGraph()Change-Id: I12a4a2eef2ddc7e3c4a6c0dd8fdcab009c975bac,1
[FIX] resolve int64/32 for AttrStmtNode (#10983)* resolve int64/32 for AttrStmtNode* rm debug header* refine* add test case* lint,0
"[Runtime][Vulkan] Add RGP support to TVM for vulkan device (#10953)RGP(Raedon GPU Profiler) is a tool used to analyze the applicationsrun on AMD GPU. RGP captures the data based on VKPresent and providesthe hardware specific information. Allowing the developer to optimizethe application. To add RGP support to TVM, debug labels ""AmdFrameBegin""and ""AmdFrameEnd"" need to be inserted into the vulkan queue.These Labelshelps the RGP tool to understand the start|end of frame when no presentis available. Thus enabling the RGP tool to capture and analyze the data.At runtime, set the envirnoment variable ""TVM_USE_AMD_RGP=1"" to startinserting the Debug Labels into the vulkan queue.Signed-off-by: Wilkin Chau <Wing-Ki.ChauWilkin@amd.com>Signed-off-by: Anurag Kumar Vulisha <AnuragKumar.Vulisha@amd.com>Co-authored-by: avulisha <avulisha@amd.com>",0
[CI] Update GPU image (#10992),1
[Hexagon] Remove HexagonBuffer external constructor and support (#10978),4
sort axes (#10985)Co-authored-by: Margaret Qian <mqian@octoml.ai>,5
"[ONNX] Add imports for BERT contrib operators (#10949)* EmbedLayerNormalization, Attention* fix Attention* SkipLayerNormalization* fix dtype bug in GeluCo-authored-by: An Wang <anwang2009@gmail.com>* missing parameterize_targets* lint* lint* comments* fix small thing* factor out layer norm computation* layernorm func* add optional args to test* upgrade onnxrt version* no upgrade onnx* fix tests* int32* fix testsCo-authored-by: An Wang <anwang2009@gmail.com>",0
"[Metaschedule] Make custom schedule_rule registration optional (#10975)See the discussion in https://github.com/apache/tvm/pull/10793#discussion_r837626566 for the context.Now I'm doing auto-tensorization on VNNI, I do need to be able to switch on / off `schedule_rule` freely.",5
[COMMUNITY] @yzh119 -> Reviewer (#10993),3
"[hexagon] 'add_hvx' test to explore HVX usage. (#10604)Add a unit test named 'add_hvx' to explore how variousscheduling choices, tensor sizes, etc. impact efficient usage of HexagonHVX units.",1
[Hexagon] Less aggressive adb state clean up (#10909)* Only remove port forwarding applied in a sessionto avoid affecting global adb state.* Send SIGINT to attempt to allow remoteserver to cleanup and undbind port indeconstruction* Only attempt to forward ports not in use byadb or the system.,4
"[Hexagon] Handle TCP server binding to unknown port (#10945)The server IP address will be obtained from the RPC tracker, but multipleservers must be distinguishable. To enable this, set a unique key whenstarting a server, and use that key when starting a session.",1
[BYOC][ACL] Fix list is not supported as an input node (#10801)* [BYOC][ACL] Fix list is not supported as an input node* fix clang lint error* fix compile warnning* fix python module import error* rename concatenate test file* fix always MakeACLTensor with same eid 0* do not offload concat default* fix concattnate test failure* fix test failure* fix lint error* fix lint* remove global var offload_concat* support concatenate with pattern table mechanism* disable pylint dangerous-default-value warningCo-authored-by: XuZhi <xuzhi.xu@alibaba-inc.com>,0
[Hexagon] Add top-level CMakeLists.txt for apps/hexagon_launcher (#11006),1
"[CUDNN] Add partitioning support for fused conv2d+bias+act (#10997)cuDNN has kernel support for the pattern conv2d+bias+act,although as of v8 only relu is supported as the activation.",1
Add driazati to triagers. (#11004)- This is to help with work on things like the merge bot and issue triage.,1
[Relay] Refactor inline composites transformation (#10995)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,5
[TensorRT][BYOC] Minor refactoring to handle constants in pattern-based ops for TRT (#10994)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,5
[CI] Update CI Images to include `pytest-lazy-fixture` (#10999)Closes #10984,0
"[ROCM] DP4A intrinsic support for TE/TIR (#11009)* [ROCM] Support dp4a on AMDGPU by sdot4 intrinsiccommit 0225f2bfe3f413cd4764c2dba6c922af2520146bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 08:56:10 2022 +0900    share op strategy between cuda and rocmcommit 762c7e8611c9ec3cca3321428e2362c81fe89b9bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 08:28:34 2022 +0900    fixed rocm batch_matmul strategy for mixed i8i8i32commit ce53e8d141f7f901303ec6a91674337cbf2b2384Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 06:17:30 2022 +0900    add rocm sdot4 TIR intrincommit f4562b991f9180b61be7339b2890de1584656c10Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 06:03:44 2022 +0900    rocm sdot4 workscommit 6cc62805f82dd884a18a1c4c0e9bae5866e00da0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 05:32:07 2022 +0900    more wipcommit 0602f4a3157d4cb5a3f280a3a3c514bb6535aac8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 14 03:47:37 2022 +0900    Squashed commit of the following:    commit 65b8bcf955f44540d6a52c8416e60f3047c8366c    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Apr 13 20:36:49 2022 +0900        [WIP] adding DP4A support to rocm    commit 4f8f308ab6bb85ef3bdcc2b8e846c2eea15f2167    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Wed Apr 13 14:03:25 2022 +0900        Squashed commit of the following:        commit 1711be38a17e3b6171350009f1da05824cd0b340        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 13:11:40 2022 +0900            fixed condition for real        commit 8a48fb5262e80e318cd81d5ff51bf95fd5eb576e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 09:57:42 2022 +0900            Revert ""Skip applying sch_rule when both ann and sch_rule are defined""            This reverts commit 4915c6a5a91ff87038e71f8aff9f31db684b4a95.        commit daea033d2cb06388ef27ddadb80fc5bce72181d2        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Mon Apr 11 09:31:05 2022 +0900            [Metaschedule] Support rocm and spirv        commit eb0cae2c779808cced074d189e8f487bf46ea89f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 07:25:04 2022 +0900            dp4a works        commit 4915c6a5a91ff87038e71f8aff9f31db684b4a95        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 06:13:45 2022 +0900            Skip applying sch_rule when both ann and sch_rule are defined        commit 7b3d71c6b21a9c5de9ef2b89d0a7db2800a5f3a2        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 13 04:40:31 2022 +0900            fixed intrin description        commit 7666cd7a5b0ce182791662673fbe45944c84d0ae        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 19:59:47 2022 +0900            add DP4A intrin        commit 7086bdb75546a2680d12dc8f80c040cea23f729a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 19:03:44 2022 +0900            works        commit db343974bfae86e51078e40e6170022a782d8e0a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Tue Apr 12 12:49:52 2022 +0900            more hack to tensorize loop mapping to make resnet50 e2e work        commit 2409674a7884a60beb50d7aa3345c4b907b8cd13        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Mon Apr 11 13:40:59 2022 +0900            wip support pad + qnn.conv2d folding        commit 613cb7ec33b6df41f1ebe0f0a0ac8eca7c73cff1        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 12:04:08 2022 +0900            hack to tensorize loop mapping to make conv2d work        commit 9e4f9df6a409396a8a4a20d967c4f51accf5d210        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 11:34:13 2022 +0900            wrap tensorize with try/catch        commit d4b496d858da0ae43063d47cb03a28b803d0269f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sun Apr 10 11:33:39 2022 +0900            revert change in task_scheduler.cc        commit 476129be7b286f5d109402280aea585e89f6dc1d        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Sat Apr 9 05:54:10 2022 +0900            try / catch in ThreadedApply        commit d8226ff26f25eba17d4000f25131822874bdc2cc        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 17:17:59 2022 +0900            filter out invalid candidate        commit 2632899a2759885d338e25f2a25ba0b2c555f0c3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 10:09:48 2022 +0900            try graceful exit in parallel_for_dynamic        commit 9d6741c3dd29c4dde861aa1d3b2ca85f560f5ac6        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Fri Apr 8 09:35:51 2022 +0900            [QNN] Fix broadcast for invalid axis        commit 6ccde0959343ce4246ef99505b4f54de469a1a5c        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 20:51:15 2022 +0900            refactor rewrite_tensorize        commit 2ce206699f10b03b9611c4683018f7e0c70c7eb5        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 20:48:17 2022 +0900            allow missing schedule_rule in post order apply        commit 3a69353a29abfc454e28d4e530d22a3e2043712e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 19:42:48 2022 +0900            refactor rewrite_tensorize        commit 43e0b2f7f98299679807aaf1ffb13cce2b5f5ce3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 18:25:14 2022 +0900            rewrite_vnni -> rewrite_tensorize        commit 823797e2627a9bfa812b72019468569ee79eb4c6        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 18:12:12 2022 +0900            VNNI -> WithIntrin        commit 4284a47e5933aa89c1c3362b15ad53b14782fc81        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:45:41 2022 +0900            introduce TileForIntrin        commit b87ef32e30e1e71b3f39789f7289976a8cba4ab4        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:34:04 2022 +0900            move TilingwithTensorIntrin to auto_tensorize.cc        commit 2fc118b3726586ba13f7de950beaa299b83a0af3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:28:45 2022 +0900            clean up headers        commit d8b2aa325c91b524bec22dc1ec2fc52c9f060fce        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:09:32 2022 +0900            clean up using namespace        commit eb05d25e2b71f4a1232a8796d1413011ec7629d3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 17:03:05 2022 +0900            refactored init        commit 5e6b0a08d447c0470c2c8a993e4bd62673e34fe3        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 16:57:14 2022 +0900            compiled        commit 2b8c430e2fec7ceb285eed7bc7aa73bb9a74a997        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 12:51:55 2022 +0900            wip MultiLevelTiling refactor        commit 7c21a9fea0511c88bd82f49f799b5198252df40a        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:58:33 2022 +0900            function doc string not supported by tvmscript        commit 40f9742bc9c3aa11e8c2c0551d1827ad47fc0f39        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:56:45 2022 +0900            update vnni intrin name        commit 4814f825a5315efd2a3da8c36d2ce6b5df5447cd        Merge: e0c5eb84b 07bbb38f7        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:44:47 2022 +0900            Merge branch 'tir-tensor-intrin' into auto-tensorize-vnni        commit 07bbb38f7fb52db4a2ecde3d5c87cf4d5cd000a1        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:24:56 2022 +0900            more lint fix        commit 15e60b42362cc64b1428b219c8eada414d1b8372        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:16:08 2022 +0900            black        commit 7a757fe53758e06418ea1367b348b47c8cd2dcf9        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:12:54 2022 +0900            pylint        commit 9a3e508b6f4529158e703b4617f2ddaa351a89eb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:58:52 2022 +0900            simplify import        commit d8e43ecf1c0a79a2c195ff31e1e699a447a11335        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:52:50 2022 +0900            use vectorlow/high in arm intrin        commit 625cd2774ec455307646b0c26bb3971d89613d1e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:34:57 2022 +0900            fixed offset factor        commit 69e72b6b612588e670937e003435afa647030ceb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:12:02 2022 +0900            Add ARM intrin        commit 1351fdea6b22f231a290a6c28e06732c9cf993cf        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:27:27 2022 +0900            use buffer syntax sugar        commit 0ced85fd097ed48aad8714912718d8735791e1fb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:17:43 2022 +0900            rename vnni.py to x86.py        commit 38a5aca87ec438446593a3af17760339211f5ad9        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:24:44 2022 +0900            add VNNI unittest        commit 88b763ec48c20cf68db8bc3bae3fa3ae78996ee8        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:10:06 2022 +0900            refactored existing test using VNNI intrin        commit 711a0076d9be2b9aa80ada67e1edda5ba1fdf1fd        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:04:58 2022 +0900            [TIR] Add VNNI dot product intrinsic for TIR        commit e0c5eb84bf6a0ad2ba0cddc4bdf22a799dc4b8a0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:42:26 2022 +0900            merge fix        commit b171748139e53f0cf75ff4b6fde436f9d8a5fe91        Merge: 71fe3bdf0 82e152a3c        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:33:59 2022 +0900            Merge branch 'tir-tensor-intrin' into auto-tensorize-vnni        commit 71fe3bdf02ae10ddbe090a4fd1020f545a05bb41        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:57:38 2022 +0900            move tensor intrin under tir        commit 0c51badef45af2a1025ab42fe38d1b3f07ab493e        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:12:39 2022 +0900            remove log        commit fed910e03eb94c169d4a160b8f3cad406d04c6aa        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:11:22 2022 +0900            more revert        commit 7150aff9fba167d88dbfb40d48727de8a144b9c0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:10:44 2022 +0900            revert stmt_functor change        commit 155107b98b09c5e5cc7f19afbd327b0557a02843        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 06:10:09 2022 +0900            refactored RewriteVNNI a bit        commit ca15255e3a882b89b05bb83079640c929fb63096        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 05:41:13 2022 +0900            add RewriteVNNI        commit dc9f71d5e3122b50fa8ae6a4462f959f13870b05        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 05:38:56 2022 +0900            vectorized init loop        commit fcc31ee20ddfafd47f566bf98ff40a9f684d12eb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 04:55:36 2022 +0900            tensorize worked        commit 2b534377a45b9ab84bf35c3d7c03ecae7616d17f        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 6 19:11:05 2022 +0900            TilingwithTensorIntrin works        commit 86baa31e773fc864f77dc113bc9a93b79f3fc652        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Wed Apr 6 08:58:27 2022 +0900            Ported auto-tensorization code        commit 82e152a3c91144041ade783116a50565ebb48b89        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:24:56 2022 +0900            more lint fix        commit 88d9bdd3b21302bc2dd068a990df15c375a1a8ef        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:16:08 2022 +0900            black        commit 31fe7eb8075445161d804d170772eac8e90d3425        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 11:12:54 2022 +0900            pylint        commit 7876754effc40ad089349534dacd75df19d38fc4        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:58:52 2022 +0900            simplify import        commit 56f2e9a85069426021e2872eb1da95bf134ac7e0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:52:50 2022 +0900            use vectorlow/high in arm intrin        commit 995cc8d6fcec70a3fadcfb1c6fee7b9f0b5a0951        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:34:57 2022 +0900            fixed offset factor        commit 86bbd4955b34257d68d957cb4a2536aea3ef9bac        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 10:12:02 2022 +0900            Add ARM intrin        commit 120fd96e80307b4301ee3fc93e6793e0b40485f0        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:27:27 2022 +0900            use buffer syntax sugar        commit 0f0682d00c3961afd1f492ae55f180c5b5502767        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 08:17:43 2022 +0900            rename vnni.py to x86.py        commit f88c31ead1fa6db4bfd2c88eeaf5f665e4c6dddb        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:24:44 2022 +0900            add VNNI unittest        commit 6cc80094adac398762924b0b31a4c741417ba9dc        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:10:06 2022 +0900            refactored existing test using VNNI intrin        commit 11a29c704cdaad96aeeca39c9c753ef006d27a50        Author: Masahiro Masuda <masahi129@gmail.com>        Date:   Thu Apr 7 07:04:58 2022 +0900            [TIR] Add VNNI dot product intrinsic for TIR* cleanup* black* update dot prod intrin* add mattr kind* conv2d topi test working* add dense and bmm test* add conv2d relay test* add tir intrin test* pylint",0
"[TIR] Ignore Allocate/AllocateConst in BufferAllocationLocator (#10998)* [TIR] Ignore Allocate/AllocateConst in BufferAllocationLocatorPrior to this commit, the BufferAllocationLocator mutator used in thePlanAndUpdateBufferAllocationLocation pass would erroneously insert anentry to `BlockNode::alloc_buffers` for buffers allocated using`Allocate` or `AllocateConst` nodes.  This error was introduced inhttps://github.com/apache/tvm/pull/9727, which deprecated `Load` and`Store` nodes, replacing them with `BufferLoad` and `BufferStore`nodes.  As a result, BufferAllocationLocator identified these asbuffers whose allocations should be moved to inner loops, rather thanas unmanaged allocations that should be ignored.This commit restores the earlier behavior by only operating on bufferallocations in `BlockNode::alloc_buffers`, and explicitly ignoring anybuffers whose allocation is done with `Allocate` or `AllocateConst`.* Only inject opaque block if managed buffers exist.Previously, all buffers found were managed buffers, so this checkwasn't needed.",0
"Fix broken CI when git-merge needs to create a commit. (#11007)- This fixes an error seen when CI needs to create a merge commit   to sync the PR. CI started doing this recently to ensure that all   CI branches use the same commit for their regressions. - The error looks like:        + git merge e370ed459739f5312e45a2fb3a446b120f8ec5d1        *** Please tell me who you are.        Run          git config --global user.email ""you@example.com""          git config --global user.name ""Your Name""        to set your account's default identity.        Omit --global to set the identity only in this repository.",0
"[RELAY][FRONTEND] Initial OneFlow frontend support.  (#8790)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.pyCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>Co-authored-by: BBuf <1182563586@qq.com>",0
[Metaschedule] Support tuning on rocm and vulkan target (#11017),5
"[Metaschedule] Enable continuing tuning after schedule application failure  (#10937)Currently, when there is a failure in schedule application during tuning (e.g. tensorize), the entire tuning session is killed with an error msg like `RuntimeError: parallel_for_dynamic error with ...`.  We should gracefully handle such errors and let tuning continue on other candidates.No test is added since I don't know how to get tuning to fail in a controlled manner.",0
"[Metaschedule, Refactor] Move MultiLevelTilingNode decl to a header (#11020)* [Metaschedule, Refactor] Move MultiLevelTilingNode decl to a header* cpplint* Update src/meta_schedule/schedule_rule/multi_level_tiling.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* Update src/meta_schedule/schedule_rule/multi_level_tiling.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* cpplintCo-authored-by: Junru Shao <junrushao1994@gmail.com>",1
Add Havisha to triagers and alphabetize. (#11005)- Havisha has offered to help with maintaining some of the roadmaps.,1
"[Arith] Updated arith::DetectIterMap to keep extent=1 components (#10980)* [Arith] Updated arith::DetectIterMap to keep extent=1 componentsPreviously, arith::DetectIterMap simplified the output expression byreplacing iteration variables with extent==1 with their value.  Thisprevented the return value from being used inarith::InverseAffineIterMap to solve for the variable, as it no longerexisted in the returned expressions.This commit changes arith::DetectIterMap to keep the iterationvariable even if extent==1, and adds a motivating unit test thatrequires this updated behavior.* Updated to retain default behavior of DetectIterMapTo avoid breaking existing test cases, updated to maintain the samedefault behavior, but a flag to maintain trivial iterators in theresult.* Updated FFI and Python API for DetectIterMap",1
[QNNParam] Refactor the implmentation of QNNParam (#11011)* The patch is to simplify the implmentation of QNNParam and make it more friendly to Python 2.x.* Empty-Commit* fix error about boolean value of Tensor with more than one value is ambiguous.,0
Use TVM log instead of hexagon_print (#11024),2
"[OpenCL] Fix type casting error (#11021)Faced situation when generated OpenCL kernel contained the following ifcondition:```if (uint4(...) && (int4(...) == int4(...)))```In this case, got the following error:""can't convert between vector values of different size ('uint4' and 'int __attribute__((ext_vector_type(4)))')""Added casts for binary ops. But it was necessary to modify `CastFromTo`and add new method `CastTo`. Because with `CastFromTo` the followingcode was generated:```if (uint4(...) && (convert_uint4(int4(...)) == convert_uint4(int4(...))))```But the OpenCL compiler still generated the same error.This is why added new method `CastTo`. In this method we don't check thecurrent type of op and just add cast to a new type.Finally the following code will be generated:```if (uint4(...) && convert_uint4(convert_uint4(int4(...)) == convert_uint4(int4(...))))```",0
"[Runtime][PipelineExecutor]Add forwarding queue logic for set input. (#10990)* [Runtime][PipelineExecutor]Add forwarding queue logic for set input.When the set_input function get called, a runtime of pipeline may notyet finish the former computation work then the new set_input call wouldbreak the current computation logic, to avoid such issue, we add theforwarding queue logic to guarantee the order of input data consuming.* polish the documents.",1
[VirtualMachine] Zero copy in set_input when input is DLTensor (#11003)* method of creating of NDArray from external DLTensor was implemented* set input without copying for DLTensor source* code clean up* update description and comments after reviewCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,1
"[COMMUNITY] @wrongtest -> Committer (#11028)Please join us to welcome @wrongtest as a new committer to TVM. The contributor has contributed to TensorIR schedule primitives, arithmetic analysis and TVMScripts.- [Commits History](https://github.com/apache/tvm/commits?author=wrongtest)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:wrongtest)- [Community Forum Summary](https://discuss.tvm.apache.org/u/wrongtest/summary)",1
"[ci] Always assume num executors == 1 (#11014)This is true now and we've seen problems like https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-10753/17/pipeline/. This could have arisen from the EC2 user data script that is supposed to set up this env variable failing or something, but we don't really need it in the first place.Co-authored-by: driazati <driazati@users.noreply.github.com>",2
"[BugFix][TIR] Fix narrower dtype of loop vars in CreatePrimFunc (#11030)This PR fixes a bug uncovered in end-to-end tests, where the dtype of loop variable could has fewer bits than the loop min/extent, which leads to a fatal error introduced by the recent #10595 which enforces more restrictive checks.",0
[BugFix][TIR] Fix rfactor when RF block becomes spatial (#11031)Should fix #10899,0
[BugFix][TIR] Error check: Inline Block with Init Stmt (#11033)Should fix #10900,0
[Frontend][Paddle] Fix pool2d op (#11029)* fix pool2d op* [frontend][Paddle] Fix pool2d Op* reformat files,0
"[MetaSchedule][Refactor] Introduce TuneConfig (#10986)This PR unifies the existing `EvolutionarySearchConfig`, `ReplayFuncConfig` and `ReplayTraceConfig` into `TuneConfig`, and refactored the logic in `meta_schedule/tune.py`",2
Fix typo in tutorial doc (#10974)Fix typo in the commented out code in TVMC Python tutorial.,0
"[Hexagon] Deprecate USE_HEXAGON_DEVICE, introduce USE_HEXAGON (#11025)The new cmake flag `USE_HEXAGON=[ON|OFF]` enables/disables Hexagonsupport in TVM and TVM runtime. It should be turned on _whenever_Hexagon support is required, even when compiling TVM runtime forHexagon itself.This is one in a series of commits intended to remove offloadsupport, and make the whole-model support the default mode ofoperation.With `USE_HEXAGON_DEVICE` deprecated, offload runtime is not builtanymore, so register `device_api.hexagon` to be same as `.v2`(presence of device API is taken as evidence of support for thedevice in TVM, so this step is necessary).",1
"[ci] Migrate all test steps to macros (#10968)This moves all the tests in the `Jenkinsfile` to use the `test_step` macros so they all get the same timeout/condition/skipping behavior. This also adds 2 shards for i386 and GPU unittests, the 2 remaining longest jobs.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
"[ci] Add branch protections to .asf.yaml (#10964)Moving these into the repo means we will be able to change them at-will.`tvm-ci/pr-merge` will change soon into `tvm-ci/pr-head` to fix anunrelated bug, but codifying it here means we can more easily coordinatethe change.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[COMMUNITY] New Committer -- lhutton1 (#11049),1
[Hexagon] Adjust RPC read buffer size from python  (#11022)* added buffer size* remove default size,1
Register PackedFuncObj with type registry. (#11039)* Fixes CHECK-fails when trying to use reflection dispatch with PackedFuncObj. * Triggered by trying to add PrettyPrint() to StructuralEqual checks.,0
"[AOT] Support LLVM backend with C++ runtime (#10753)* add get_c_struct_name() method to Metadata to distinguish struct type name in llvm* add metadata serialization support to llvm codegen* Organize MetadataQueuer into a separate file.* Add DiscoverArraysVisitor to metadata_utils* Fill DLTensor metadata in LegalizePackedCalls.* Improve error message from Call asserts* Pass non-String device_context down to codegen. * this is necessary to allow CodeGenCPU to emit calls that include resource_handle.* Scope usage of lvalue refs in LowerTVMBuiltin to avoid corrupt memory.* test fixes* Also fill preflattened_buffer_map (TODO, maybe don't do this)* Fix C codegen.* Set USMP elem_offset to 0.* Clarify calculation of byte_offset from elem_offset.* fix tests* Fix arm compile warning* Fix hexagon test. * previously I believe we required interface_api == ""c"", but   this really means to generate C API bindings, and we are generating   ""packed"" bindings. * I think ""c"" was chosen here because the distinction between   interface-api and use-unpacked-api is confusing. ""c"" interface-api   means to generate an entrypoint API for microcontrollers that   accepts bare data buffers. ""packed"" interface-api means to generate   a TVMBackendPackedCFunc entrypoint. use-unpacked-api forms the same   determination for the operator functions. * A further confusion here is that there are two ways to call   ""packed"" operator functions: tir.tvm_builtin_call_packed and   tir.tvm_builtin_call_cpacked. This distinction describes whether or   not to late-bind calls via TVMBackendGetFuncFromEnv. Right now, AOT   only ever requires call_cpacked because target_host == target, and   for all suitable target_host, we expect a single DSO-exportable   runtime.Module. When we move away from this by introducing   heterogeneous target support to AOT, we can use this as a condition   to help us choose between call_cpacked and call_packed (and   possibly add a compile-time option to assert it is call_cpacked,   for situations where we really don't want call_packed).* Document T.preflattened_buffer* Fix test_aot_legalize_packed_calls* Address manupa comments* Fix convert_pool_allocations_to_offsets test.* lint* Fix T.preflattened_buffer* Add preflattened_buffer_map to TIRTextPrinter* Fix tests* Fix BYOC* Fix invoking C device API.* remove comments* Address Mousius comments* lint* lint* Fix GMock linking on new CMake* address masahi commentCo-authored-by: Masahiro Masuda <masahi129@gmail.com>",0
"[TE][TIR] Enable CreatePrimFunc to properly handle 'layout_free_placeholder' (#11054)`layout_free_placeholder` is used to guide proper layout transformation on TE/TIR-level. However, previously it is not properly supported on upstream AutoTIR. This PR introduces legalization of this block annotation into function attributes.Note that this attribute is not useful on Relax end-to-end tuning, because in Relax @jinhongyii developed a set of more powerful mechanisms to handle these cases more effectively without introducing bugs like https://github.com/apache/tvm/issues/9476.",0
"Upgrad oneflow version (#11052)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* upgrad oneflow version to 0.7.0* fix* continue push* continue pushCo-authored-by: hhhfccz <hjk1938927583@163.com>",0
"Let remote RPCModule get function recursively (#11053)Same reason as we changed the get_function to recursive searching. This is useful when we treat the local module as a data segment and wrap it in another empty LLVM module to reuse the export_lib API. After de-serialization in remote, the data segment will be translated as an imported module. Thus we need to fetch the function recursively.",4
[TVMC] Add `--config` argument for config files (#11012)* [TVMC] Add `--config` argument for config filesCollecting common configurations for users of TVM and exposing them gracefully in tvmc using a `--config` optionas defined in https://github.com/apache/tvm-rfcs/blob/main/rfcs/0030-tvmc-comand-line-configuration-files.mdCo-authored-by: Shai Maor <shai.maor@arm.com>* Add correct test guardsCo-authored-by: Shai Maor <shai.maor@arm.com>,1
"Revert ""[OpenCL] Fix type casting error (#11021)"" (#11035)This reverts commit 8aafe5b1095b8c1024e826f6a8c2114606288182.",0
Add FlattenAtrousConv transformation (#10996),1
Hotfix CI (black check not caught by PR CI) (#11056),0
"[Hexagon] Pass stack size to simulator (#11046)Increase the default stack size to 256kB, since this is the minimummain thread stack size in QuRT on simulator.",4
[Hexagon] Refactor test scripts (#11026)* Refactor hexagon test scripts* rever removing the script,3
Attempt to prevent concurrent update in Map (#9842)* Attempt to prevent concurrent update in MapCalling Map::Set invalidates exising iterators to protect fromusing already deleted data due to re-hashingChange-Id: Ib6b580758e74c8b77ed560932d87b643bd6c9402* Migrated to using TVM_LOG_DEBUGNow uses TVM_LOG_DEBUGMap state_marker made atomicChange-Id: I090c4b33e6edaa977cccba11f8d1c6ff3fbca430* removed usage of atomicsChange-Id: I7bd930cb52d58ca10fd49a5fe8f5d48b3e955d0a,0
Add ekalda to reviewers. (#11061),1
[ci] Remove TensorCore node name (#11048),4
[OpenCL] Fix type casting (#11038)* [OpenCL] Fix type castingThe previous PR apache/tvm#11021 was reverted in apache/tvm#11035 dueto it affected performance of generated OpenCL code.This PR fixed the same issue but doesn't lead to performancedegradation. Tested on Resnet50_v2 network.* Implement using select built-in,0
"[TOPI] Fix nn.lrn result dtype on fp16 (#11032)The buggy script as below:```pythonimport tvmfrom tvm import relayfrom tvm.contrib import graph_executorx = relay.var(""x"", shape=[1, 3, 224, 224], dtype=""float16"")y = relay.nn.lrn(x)mod = tvm.IRModule.from_expr(relay.Function([x], y))lib = relay.build(mod, target=""llvm"")f = graph_executor.GraphModule(lib[""default""](tvm.cpu()))f.run()```The error I get is```Check failed: ret == 0 (-1 vs. 0) : Assert fail: (((tir.tvm_struct_get(arg.T_divide, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg.T_divide, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg.T_divide, 0, 7) == (uint16)1)), arg.T_divide.dtype is expected to be float32```",0
"[CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo (#11013)* [CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo- Downloads a quantized (int8) person detection model- Uses tvmc to compile the model for Cortex(R)-M55 CPU and CMSIS-NN- Downloads an image to run the model on- Creates a C header file inputs.c containing the image data as a C array- Builds the demo application- Runs the demo application on the FVP- Application reports whether a person was detected e.g. ""Person detected""Change-Id: If58d02ed0c4d2a85c0100398f65e6915a86f6546* [CMSIS-NN] Add Arm(R) Cortex(R)-M55 CPU and CMSIS-NN demo- Downloads a quantized (int8) person detection model- Uses tvmc to compile the model for Cortex(R)-M55 CPU and CMSIS-NN- Downloads an image to run the model on- Creates a C header file inputs.c containing the image data as a C array- Builds the demo application- Runs the demo application on the FVP- Application reports whether a person was detected e.g. ""Person detected""Change-Id: Ic20ceed80bc6e48d5c96ff0d5ca6c85e7f19174b",1
"[TIR] Utility function to decide loop mapping for auto tensorization (#11050)* [TIR] Add TensorizeInfo and GetTensorizeLoopMapping* expose PreOrderVisit to python* add test case* add conv2d nchwc test* add mma test* add arm nhwc conv2d test* Revert ""add arm nhwc conv2d test""This reverts commit eb147f33bb02d62a0eacc9cdfe777ac047ee1bc9.* refine* add doc* update* fixd condition* black* pylint* Update python/tvm/tir/schedule/analysis.pyCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>* run black* bring back logic in original code to support loop permutation* add comment* simplify* minor fix to testCo-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>",0
Fix While Node StructuralEqual and StructuralHash issue (#11073),0
[microNPU] Cascader performance model bugfixes (#10510)* [microNPU] Performance model bugfixes* Fixed incorrect num_blocks calculations for both BufferModes.* Fixed similar issues with Read/Write byte calculations.* Fixed an issue where the 'partkernel' flag was not propagated to  the performance estimation code.* Fixed single buffering check incorrectly used output shape and  block rather than the input shape and block.* Fixed block config not aligned to micro block for Elementwise.Change-Id: Ide6b231bc1a17c65bed20129d2179a215ada14b2* Address review commentChanged incorrect usage of 'max_width' to 'max_depth'.,0
"[TVMC] Add configuration json files to the Python package (#11063)Add the `configs` directory to be part of the installed version ofTVM in the setuptools configuration, and introduce a new functionto load the `configs` directory from the right paths both when TVMis locally installed for development, as well as, when it is installedas a package.",1
"Better version handling for Arduino (#11043)* Fix bug allowing microTVM to be used with Arduino version v0.20 and   above (see changes to _parse_connected_boards) and adds relevant unit   tests.                                                                                                                                          * Only perform version check when calling build or flash (things that   actually require arduino-cli), and adds relevant unit tests.                                                                                    * Only raise a warning if the arduino-cli version present is below the  min version (previously any version other than v0.18 would cause an     error).                                                                                                                                         * Change version comparison to use version.check, like the rest of TVM",0
[QNN] Support input scale and zp of 1-element vector in qnn.conv2d_transpose (#10952)* Support input scale and zp of 1-element vector in qnn.conv2d_transpose* Lint,5
[Frontend][ONNX]support  Pool2D layout is CHW (#11034)* support  Pool layout is CHW* fix lint test* change the if condition,0
Add FlattenAtrousConv pass into the default optimize pipeline. (#11077),1
Add two possible missing visit of let stmt in lowering (#11079)Refer to the issue in https://github.com/apache/tvm/issues/10831#issuecomment-1086287433,1
[TIR] Add TileWithTensorIntrin (#11075)Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>Co-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>,1
[RPC] Don't use existence of USE_HEXAGON_SDK as enablement check (#11080)* [RPC] Don't use existence of USE_HEXAGON_SDK as enablement checkUse USE_HEXAGON to check if Hexagon support is enabled or not.This fixes https://github.com/apache/tvm/issues/11059.* Restart CI,0
Restart popen pool. (#11074)Retrigger CI.Address issues.Retrigger CI.,1
[CI] Update GPU image for oneflow v0.7 (#11085),1
"[OpenCL Textures] Fix memory management in texture pool (#10938)Previously, the size of the memory which should be allocated wascalculated as multiplication width on height. It doesn't work well incase when one texture has big size in height and the next one big sizein width. We tried to reuse the allocated memory and every time whenthe next texture with big size was used we reallocated the previousone. It has huge impact on the performance.Now we check two dimensions independently. So, in this case we willcheck both dimensions and it helps us to avoid the situation withcyclic memory reallocation.",0
[FQ2I] Add log op to FQ2I (#10924)* unary op for resize2d and test* renamed test* added log in quantized form* black'd some files* changed suggested commentary,1
"[TVMScript] Allow `val = buf[index]` without type annotation (#11060)* [TVMScript] Allow `val = buf[index]` without type annotationOther instances of `var = expr` were previously allowed withoutrequiring a type annotation, by using the dtype of the expression asthe dtype of `var`.  This behavior didn't work for `buf[index]`expressions, which are internally represented as `BufferSlice` pythonobjects, and only converted to `BufferLoad` primexprs when used as anexpression.This commit adds a `dtype` property to `BufferSlice`, allowing`buf[index]` to be used in a let statement without a type annotation.* Reverted a wider changeAutomatically adding a type annotation to Var if it could bedetermined from the dtype let the unit test directly compare theannotated and unannotated versions of buffer load.  Unfortunately, italso broke 54 unrelated tests, so that change is removed from this PR.",1
"[TIR] StmtFunctor RenewDefs (#10843)* [TIR] StmtFunctor RenewDefsIn this PR, I introduce a StmtFunctor `RenewDefs` for deep copy all definition nodes in PrimFunc (including Var, Buffer, and IterVar). This functor can create a new PrimFunc with the same behavior as the old one but contains different Nodes.This Functor may help TIR fusion or inline multiple PrimFuncs* add ut* address comments* address comments* lint* lint",1
[microNPU] Integrate rolling buffers in Arm(R) Ethos(TM)-U (#10344)* [microNPU] Integrate rolling buffers in Arm(R) Ethos(TM)-UChange-Id: Iede5e68981a063f6eb1e118433cc2c92e175af52* Add documentation for create_tiles* Fix linter issues* Fix integration tests,0
STM32: add as a new target (#9385)* STM32: add as a new target* STM32: Target takes a board ID rather then a series.* STM32: target series.* STM32: Fixed lint issues.,0
[Hexagon] AoT with LLVM Codegen on Hexagon (#11065)* AOT with LLVM Codegen on Hexagon* Address comments,1
"[Hexagon] Delete offload runtime, move files to right places (#11090)Within src/runtime/hexagon- delete directory android,- move files from hexagon to ., delete hexagon,- merge host/hexagon_module.cc with hexagon_module.cc, delete host.Rename HexagonHostModuleNode to HexagonModuleNode.",2
[ci] Remove `Prepare` step (#11082)This step doesn't do much on its own and triggers another `CPU` node allocation. This PR combines it into the `Sanity Check` step and renames that to `Lint`Co-authored-by: driazati <driazati@users.noreply.github.com>,4
[COMMUNITY] ashutosh-arm -> Reviewer (#11101),3
Prevent IRSbustitute to create new buffer when buffer var is unchanged (#11103)* Prevent IRSbustitute to create new buffer when buffer var is unchanged* typo,1
"[Analysis] Exposed Analyzer::CanProveEqual to Python API (#11102)* [Analysis] Exposed Analyzer::CanProveEqual to Python APIChecking for `analyizer.simplify(lhs-rhs) == 0` was a frequent patternin Python unit tests, and already had a utility function in the C++public API.  Exposing this utility function to Python allowed thispattern to be cleaned up.* Replaced more cases of .simplify with .can_prove_equal",3
[CONTRIB] Add PopenWorker process recycling (#11094)* [CONTRIB] Add PopenWorker process recycling* Clarify docstringsCo-authored-by: Peter Salas <psalas@octoml.ai>,1
[Auto Scheduler]add task name during printing table info (#11098)* add task name during printing table info* address comments and fix lint* better look* fix linting* fix linting again,0
"Add oneflow fronted tutorials (#11036)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* fix bug when upgrade oneflow to 0.7.0* add tutorials* add tutorials* try to fix* fix bug* add test* fix bug* fix flowvision bug* Update test_forward.py* Update test_forward.pyCo-authored-by: hhhfccz <hjk1938927583@163.com>",0
[Hexagon] Add mobilenet test (#11104)* Add mobilenet test on Hexagon* Address comments* fix import and remove extra function,0
[microNPU] Integrate the cascader (#10862)* [microNPU] Integrate the cascaderIntegrate the cascader into the codegen and optionally enable itwith the enable_cascader flag. Includes placeholder MemoryRegions untilintegration with the PoolInfos provided by a user.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>* Fix linting and a docstring* Plumbing and testing improvementsPlumb the workspace memory pools into into the cascader and makethe tests to check for the memory reduction.* enable_cascader() -> is_cascader_enabled()* Check for the exact value of workspace size* Remove unused ACCEL_TYPES* Linting...Change-Id: If2d92846f05a7e8b21be767163841084538805a9* Rebasing...Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>,0
"[TVMC] compile/tune: Check if FILE exists (#10865)Currently when a non-existing FILE is passed to 'tvmc tune' it throwsa traceback because a FileNotFoundError exception is not handled. Sincethere is no need for such abrupt exit, and the trace can also confuseusers, this commit fixes it by checking if FILE indeed exists, kindlyinforming the user about the non-existing FILE before exiting.Add test for verifying if 'tvmc compile' and 'tvmc tune' commands handlecorrectly the FILE option when it is invalid (e.g. missing, a dir, or abroken link).A TVMCException will be generated by test_tune_rpc_tracker_parsing testbecause FILE will be set by pytest to a mock object, which is not avalid input. Since FILE argument is irrelevant for the test in question,circumvent the Mock hijack of FILE argument by setting it before usingmock.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
[ETHOSN] Roll CI forward to Ethos(TM)-N release 21.11 (#11099)The updated code is already in the repo; this merely switches the CI over.,1
update for using new functions (#11100),1
[microTVM] Bump versions in reference vm (#11067)* Update spresense sdk version to make hack unnecessary* Bump arduino SDK version* Fix Arduino RPC server test* Fix versions for all board libraries,0
[ci] Add local test re-run info (#11051),1
"Complete pytorch grid_sample (#10504)Pytorch's grid_sample() supports various interpolation options:(1) data dimension: 2D / 3D(2) interpolation method: nearest / bilinear / bicubic(3) padding_mode: zeros / border / reflection(4) align_corners: True / FalseHowever, TVM only supports a part of above options:(1) data dimension: 2D(2) interpolation method: bilinear(3) padding_mode: zeros / border(4) align_corners: TrueThis commit completes the options not supported by TVM, and keeps existinggrid_sample of onnx/pytorch uninfluenced.Co-authored-by: shukun.net",1
"convert full-width characters to half-width characters (#11112)* `）` -> `)`* `】` -> `]`* `、`* `，` -> `,`",5
[TIR] Enhance software pipeline validation and fix predicate of epilogue (#11106)* Fix pipeline validation* fix predicate* Update test_tir_transform_inject_software_pipeline.py* Update inject_software_pipeline.cc,0
[Hexagon] Add test for registered schedules (#11016)* add hexagon schedule tests* moved tests to sub-directories,1
[Hexagon] Clean up Hexagon device APIs (#11119)- Rename device_api.hexagon.v2 -> device_api.hexagon- Rename HexagonDeviceAPIv2 -> HexagonDeviceAPI- Rename hexagon_device_api_v2.* -> hexagon_device_api.*This concludes the removal of offload support from the Hexagon runtime.,5
"[USMP] Adding support for U4 usecase (#10785)* [USMP] Adding support for U4 usecaseThis commit adds support for placing I/Otensors within the workspace buffer.This is enabled using PassConfig optiontir.usmp.use_workspace_io. Once it is enabled,it will remove the I/O tensors from the TIRmain PrimFunc and replace them with Allocatenodes that is annotated to contain Input andOutput tensors.The USMP will plan memory for them accordingly.(i.e. it will re-use space used by them forintermediaries depending on the liveness).This will only be supported with C Interface API.Thus, this commit produces two functions to themetadata sources to obtain input and output structsthat points to location inside the workspace struct.Change-Id: I4c7e750ead9a880ba900602c17f53a125f97dbf9* fixup! [USMP] Adding support for U4 usecaseChange-Id: I78f03d36b12b4a5e8eae8d11701f51019489defc* fixup! [USMP] Adding support for U4 usecaseChange-Id: I857f3d0ba7bc192d56d750c44b232998b2876e7a",0
"[Python] Populate setuptools description with README.md (#11078)* [Python] Populate setuptools description with README.mdAdds the description metadata for the setuptools descriptor file`setup.py` with the contents of our existing README.md, which isa common practice.* Update python/setup.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Update python/setup.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Import pathlib and apply black formats.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>",1
allow constant value let binding in script (#11115),5
"[Metaschedule] Auto tensorization for CPU / GPU dot product (#11088)* [Metaschedule] Auto-tensorization for CPU / GPU dot productCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>* doc update* add vnni conv2d test* add dp4a test* adding tests for rewrite_tensorize* add rewrite_tensorize test* add missing pydoc* black* more doc* adding auto tensorize integration test* add dp4a test* fix target name* fix dtype in test* skip bert test* replace hard-coded llvm intrinsic id in test with look up* remove unnecessary include, add doc for the rest of params* update postproc.h* update doc* fix shape in te matmul workload* fix newline in cppdocCo-authored-by: Siyuan Feng <Hzfengsy@sjtu.edu.cn>Co-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>Co-authored-by: Ruihang Lai <lairuihangdongdong@qq.com>Co-authored-by: Wuwei Lin <wuwei@apache.org>",0
[onnx] Relax tolerance for qlinearleakyrelu test (#11042)This has failed on `main`: https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/3068/testsCo-authored-by: driazati <driazati@users.noreply.github.com>,2
[ci] Switch to requiring `pr-head` for merges (#11081)This is necessary to fix the bug where PRs rebuild when editing the description.sCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[skip ci][ci][actions] Hardcode Python version to 3.7 for miniconda setup (#11136)Fixes #11131Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[docs] Update publication list (#11137)* [docs] Update publication listThis PR updates some publications that use or built on top of TVM.* Fix CI,0
"[TVMScript] Support TVMScript template meta-programming over variables (#11097)This PR supports a simple meta-programming paradigm for TVMScript, which allows users to get access to var definition in the Python environment.",5
[CI][DOCKER] Install blocklint for identifying non-inclusive language (#11128)Installs blocklint in the ci_lint Dockerfile,2
[Frontend][ONNX] Update softmax calculation method when dimension > 2 (#11123)* update Softmax with uniform operator* add testcases for softmax,1
[ci] Split hexagon into 2 steps (#11132)This shards up the hexagon build since after #11016 it's the longest test step. This also drops the max runtime per stage down to 2 hours from 4 hours to make these kind of increases more obvious in the future.Co-authored-by: driazati <driazati@users.noreply.github.com>,3
fix incorrect pos ids generation in EmbedLayerNormalization (#11149),0
"[Graph Debugger] Expose way to benchmark individual nodes. (#11000)* initial* secondary commit* docs* match tests* fix test* use std::fixed, max precision, typed pack func, fix isnan* comments on docs* address tristan comments* add test* tristan comments* use skipif* empty commit* empty commit* jostle again* remove assert statement",0
[CMSIS-NN] Moved TFLite model making to common area (#10939)* [CMSIS-NN] Moved TFLite model making to common areaChange-Id: Ic4dbc1919ff0b481c05daf7e57cf9b055c714c9c* Fixed lint issues with tensorflow importChange-Id: I7a520beec9c244e9c790d3e82733c2fb476f7e5e* Resolved merge conflict with mainChange-Id: Iefe58dd321efae6eae26cd54a31c5923d0f1e32b* Made TFLite layer creation explicitChange-Id: I7fbf6a5a2163c1fada49477f86d84f1bc09bd57c* Lint fix: added a missing docstringChange-Id: If1fb8bb09c538c04e333ccab65a20cff247a504d,0
"[TIR] Get read/write access precisely for opaque access. (#11110)* [TIR] Get read/write access precisely for opaque access.When the opaque access is wrapped with tvm_access_ptr, we can get the access_maskfrom tvm_access_ptr in BlockReadWriteDetector and put this opaque access to read_regionsor write_regions according to access_mask.* [TIR] Add parameter extent for access_ptr.Co-authored-by: sqing <qing.siqi@intellif.com>",1
Bump cmake version for GPU build (#11156)The cmake version (3.10) in Ubuntu 18.04 does not cope well with themore advanced cmake use in libtorch surrounding the CUDA target.We switch to a self-built cmake 3.14 (already used by arm and i386 CI).The context for this is #10758 .,5
[Hexagon] Add test for depthwise conv2d schedule (#11138)* Add test for registered scheduales - depthwise_conv2d,1
fix apt install command text in check_arm_qemu() (#11153),0
"[hexagon][benchmark] Add workload with [1,?] shape (#11163)Re-enable a benchmark configuration that was failing becauseof a bug in TVM's new dimension-mapping code.",0
Include BUILD_NUMBER in rebuilt docker image. (#11165)* This allows folks to retrigger Jenkins runs through the Jenkins UI   rather than requiring the author to push an empty or amended commit.,2
"Move WrapTimeEvaluator from RPC to profiling, NFC (#11172)",4
[Analysis] Readability/Deduplication in Analyzer CanProve/Simplify (#11130),5
Update CUDA key to fix #11168. (#11170),0
Remove micro_dev (#11169),4
"[Arith] Simplify the output of InverseAffineIterMap (#11167)This PR simplifies the result of `InverseAffineIterMap` by assuming the `output` param has the same range as the output range of the affine transformation. For example, for iter map `i, j => i * 16 + j, i \in [0, 8), j \in [0, 16)`, after this PR, the inverse will be `m => m // 16, m % 16, m \in [0, 128)` instead of `m => (m // 16) % 8, m % 16`",2
[MetaSchedule] Allow optional params to be None (#11188),5
Make microtvm_template_projects available in tutorials. (#11164),5
[fix] vec * mat in matmul in onnx converter (#11174)* fix: vec * mat in matmul in onnx converter* fix: pylint* fix: vec-mat matmul* fix test* fix test,0
"[AOT] Return module list from AotExecutorFactory (#11191)* [AOT] Return module list from AotExecutorFactoryAdd a function ""module_list"" to AotExecutorFactory that returnsArray<String> containing names of all modules. At the moment therewill be only one entry in that list, most commonly ""default"".* Address review comments- Change assert in unit test to a simpler one.- Add custom name to the generated module in unit test.- Rename ""module_list"" to ""list_module_names"".* Remove obsolete comment",1
"Enable USE_CUSTOM_LOGGING for Hexagon Launcher (#11185)* Enable USE_CUSTOM_LOGGING for Hexagon Launcher* Fix clang-format error* Remove ""ERROR:"" from LOG messages",0
support round-trip for T.Ptr in tvmscript (#11179),5
[TIR] Bind iter domain in analyzer in CreatePrimFunc (#11187)* [TIR] Bind iter domain in analyzer in CreatePrimFunc* lint* fix test,0
[ONNX] Reshape op (#11047)* hitting bug while running the reshape unit test. currently trying to reproduce error in script* unit test passes* ran make format* removed print statements* edited commentary* moved the zero check outside of the ravel unravel and into the topi reshape defn* ran cpplint* changes from andrews comments* derp* black* ran black on test_forward.py* fixed test expected output* retriggering CI due to hexagon test failure,0
[CI] Update GPU image to use newer CMake (#11194)Requested in https://github.com/apache/tvm/pull/11156Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/257/pipeline,1
[Relay] Create header file for realize.cc (#11093)* Move class definitions to header file* Trim out unnecessary includes* Run clang-format-10* Remove unnecessary class declarations* Adjust grammar to trigger CI* Change comment phrasing again to trigger CICo-authored-by: Jonathan Sparling <jsparling@westus2-ml-vm-sg01.2xo54b0zdm3epgab0khwgzehke.xx.internal.cloudapp.net>,2
"Remove support for run-time linked-params from codegen (#11144)Linking parameters via a runtime lookup function no longer happens aftercommit b5f1dabce4 (PR#8509): ""Tir constants integration into compilationpipeline"". Now, in cases where the runtime lookup would have happened inthe past, the parameters are embedded into TIR, removing the need for aruntime lookup.There is still plenty of code around that implemented the original runtimelookup. This patch removes the unnecessary leftovers from TVM's codegen.",4
[FIX] Avoid stack overflow in TargetHookVisitor with large modules (#11135)Use MixedModeVisitor to not recursively visit let nodes.,0
[TRT] Add check to support split op with TRT 5.1.5+ (#11154),1
"[TIR] Reduced duplication in op.h (#11129)* [TIR] Reduced duplication in op.hPreviously, `is_positive_int`, `is_negative_int`, `is_const_int`, and`as_const_int` had nearly duplicate type-checking logic.  This allowedhandling of Broadcast nodes to be diverge between theimplementations.  (e.g. `is_const_int(Broadcast(4,1), 4)` returnstrue, but `is_positive_int(Broadcast(4,1))` returns false.)This changes `as_const_int` to contain the type-checking logic,including the handling of Broadcast nodes, with the other threefunctions implemented in terms of `as_const_int`.* Test case, removing BroadcastNode handling from as_const_intRather than extending it to apply in more cases, seeing if it is safeto extract this functionality out to a separate function.",2
[TRT] Add check to use setBindingDimensions in TRT 6.0.1+ (#11178),1
Run cpplint in quiet mode. (#10292),5
Fix make format (#11197),0
"[CI] Update GoogleTest (#11162)This updates GoogleTest across all our images which also has the sideeffect of using the same version for any host OS.Rather than updating to a fixed version, I've followed the best practiceadvertised by GoogleTest itself which is the Live-at-Head philosophy:https://github.com/google/googletest#live-at-headCloses #11002",0
[microNPU] Match requantize in min/max with activation pattern (#11010)* [microNPU] Match requantize in min/max with activation patternOptimizes a corner case where min/max + clip also produces a requantizeoperation. Previously the requantize was lowered separately as anidentity operation which is unnecessary. Now the quantization parametersfrom requantize will be used by the lowered min/max operation.Change-Id: Id740d975bd8ba2952f3444ce1061acef560d74d7* add random seed to legalization testChange-Id: Ic4ff78af94c3e8250dba8e3ce5c2775fcc7a17f6,1
[docker][RVM][microtvm] Refactor CMSIS installation to add to RVM (#11148)* Refactor CMSIS installation for RVM* Fix `ethosu_dir` existing directory* Address Andrew comment,0
[Hexagon] Add support for on-device unit testing using gtest (#11145)* link gtest to tvm runtime* first test running!* HexagonBuffer tests running in sim* move to new tests directory* use USE_HEXAGON_SDK* add python frontend for Hexagon unit tests* clean up after rebase* isolate cmake changes to Hexagon* add gtest init with arguments* add hexagon sources only if building for Hexagon; remove workaround* format & lint* fix Hexagon build error* remove x86 implementation and win32 code* check if hexagon gtest path exists before linking* make USE_HEXAGON_GTEST an optional cmake param* turn on Hexagon gtest in Hexagon CI* Hexagon unit tests should fail if run without proper gtest linkage* add tvm option; move Hexagon tests to test/cpp-runtime/hexagon* add libinfo* trigger ci,0
"[Runtime][PipelineExecutor] Refactor PipelineExecutor.py and Add cross compile support for pipeline executor. (#11133)* [Runtime][PipelineExecutor] Refactor PipelineExecutor.py add crosscompile support for pipeline executor.Current pipeline_executor and pipeline_executor_build stay in samefile, this caused that the the running of pipeline_executor need supportfrom tvm and relay that is not available on edge device in which a runtimelibrary only can get build.Pipeline executor used PipelineExecutorFactory to store the pipelineconfiguration and export the pipeline executor library, but the currentexport not support the cross compile, add related logic.* fix ci issue.* use runtime to replace relay and leave the export_library inpipeline_executor.py.",0
[Hexagon][Runtime] Add QuRT thread pool backend (#11018)* Initial take on adding QuRT thread support to TVM's thread pool. WIP; crashes* Allocate QuRT thread stacks automatically* Remove duplicate stack in QuRTThread* Add more logging to QuRTThread* Use QuRT mutexes and condition variables* Get QuRT thread pools working perhaps* Sleep for a little bit to let race condition bugs shine through* ayeee it works!* Remove custom hexagon implementations of std::mutex and std::condition_variable* threading_backend.cc code cleanup* Formatting changes* remove hexagon debugging* Initial take on adding QuRT thread support to TVM's thread pool. WIP; crashes* Allocate QuRT thread stacks automatically* Remove duplicate stack in QuRTThread* Add more logging to QuRTThread* Use QuRT mutexes and condition variables* Get QuRT thread pools working perhaps* Sleep for a little bit to let race condition bugs shine through* ayeee it works!* Remove custom hexagon implementations of std::mutex and std::condition_variable* threading_backend.cc code cleanup* Formatting changes* remove hexagon debugging* Add hexagon thread pool test* style fixes for tests/python/contrib/test_hexagon/test_thread_pool.py* Fix some style issues* Address some reviewer comments,0
Remove device type dependency (#11198),4
[Hexagon] Add schedule and test for conv2d_transpose_nchw (#11175)* Add test for registered scheduales - depthwise_conv2d* added more test to depthwise_conv2* adding new line at the end of the file* reformatted the file* resolve comments* add schedule and tests for conv2d_transpose_nchw* registering conv2d_transpose strategy and clean up test,1
"[PROFILER] Theoretical roofline models (#11066)`tvm.analysis.roofline_analysis` adds estimated roofline performance to aprofiling report. The roofline model measures how close an operator getsto best possible memory bandwidth or FLOP/s depending on whether it ismemory or compute bound. This computation uses the runtime of theoperator along with two numbers extracted from the TIR code: bytes ofmemory touched and number of floating point operations. Because thesenumbers are extracted from TIR, they may not be 100% accurate. The bestpossible memory bandwidth and FLOP/s are measured by running smallprograms that are memory and compute bound respectively.For now, this function only works with llvm cpu targets, but it shouldbe possible to extend to GPU targets.",1
"[FIX,AUTO_SCHEDULER] Handle manually unrolled loops in auto scheduler features (#11166)For multiple statements in a loop, add flops for each statement insteadof only using the last statement.",0
"[TIR] Fix an index out of bound issue of compact buffer region (#11201)After https://github.com/apache/tvm/pull/10557, the region extent after compaction is ensured to not exceed original shape. Now when the inferred region min is negative, the index remap rule `idx -> (idx - region_min)` would introduce out of bound accesses, which would cause crashes at runtime.The two updated cases in UT:- padding block inlined to poolingCurrent version results to out of bound accesses in `cache` block, since the H/W extents are compacted to no more than 224 but accesses are shifted by `- (-1)`.```python@T.prim_funcdef func(X: T.Buffer[(224, 224), ""float32""], Y: T.Buffer[(224, 224), ""float32""]) -> None:    cache = T.alloc_buffer([224, 224], dtype=""float32"")    for h, w in T.grid(224, 224):        with T.block(""cache""):            cache[h + 1, w + 1] = X[h, w]    for h, w, kh, kw in T.grid(224, 224, 3, 3):        with T.block(""compute""):            Y[h, w] = T.max(Y[h, w], T.if_then_else(T.likely(1 <= h + kh, dtype=""bool"") and T.likely(h + kh < 225, dtype=""bool"") and T.likely(1 <= w + kw, dtype=""bool"") and T.likely(w + kw < 225, dtype=""bool""), cache[h + kh, w + kw], T.float32(0), dtype=""float32""))```-  sparse access`A_data_local[A_indptr[i] + k]` is rewritten to `A_data_local[T.min(A_indptr[i] + k, 0)]` instead of `A_data_local[0]`. Compared to current version, interestingly, it keeps the semantic that negative sparse index result to oob behavior.",0
[MetaSchedule] Logging Interface Unification (#11157)* Implement new logging interface.* Major interface usage update.* Functionality fix.* Switch logging conditions.* Tweak logging interface.* Minor fix.* Feature updates.* Logging usage.* Linting.* Fix linting.* Fix handler type.* Fix issues.* Nits.* Address issues.* Add DEBUG level fall back.* Minor fixes.* Allow parameterized configuration.* Linting.* Polish interface.,0
[COMMUNITY] Altan Haan -> Reviewers (#11205)Please join us to welcome @altanh as a new reviewer to TVM. Altan has made contributions to relay language.- [Commits History](https://github.com/apache/tvm/commits?author=altanh)- [Code Review](https://github.com/apache/tvm/pulls?utf8=%E2%9C%93&q=reviewed-by:altanh)- [Community Forum Summary](https://discuss.tvm.apache.org/u/altanh/summary),1
[COMMUNITY] Xiyou Zhou -> Committer (#11206)Please join us to welcome @zxybazh as a new committer to TVM. The contributor has contributed to Meta-schedule a lot.- [Commits History](https://github.com/apache/tvm/commits?author=zxybazh)- [Code Review](https://github.com/apache/tvm/pulls?q=reviewed-by%3Azxybazh+)- [Community Forum Summary](https://discuss.tvm.apache.org/u/zxybazh/summary),1
[CI] Update GoogleTest in ci_wasm (#11207)Looks like I missed this one in #11162 due to missing the cpptest callin the Jenkinsfile,1
[Hexagon] Update launcher cmake flags for Android (#11213)Don't build graph executor for Android (it's not needed).,1
"[Relay] Support 'external codegen targets'. (#11173)* [Relay] Support 'external codegen targets'.(Part of Collage, https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md)This change prepares the VM and Relay target handling machinery to supportexternal codegen targets in addition to 'regular' targets. This allows usto configure the build with Collage as follows:```    host_target = tvm.target.Target(""llvm"")    targets = [tvm.target.Target(""cuda"", host_target),               tvm.target.Target(""cutlass"", host_target),               tvm.target.Target(""cudnn"", host_target)]    with tvm.transform.PassContext(...):        exe = tvm.relay.vm.compile(module, target=targets)```Four changes are required:1. I introduce four new target kinds for the external codegens currently supported   by Collage. Others can be added as they are vetted for use by Collage. These   are given a device type matching the external codegen's assumption (ie just CUDA   currently), and given a target kind attribute ""is_external_codegen"" of True. The   latter is needed by Collage to signal the target kind name represents and external   codegen 'compiler' name. See the RFC for specifics.2. I introduce the binary relation Target::IsExternalCodegenFor so that   external codegen targets can be related back to the 'underlying' targets   they are implicitly using in their codegen.3. I rework the VMCompiler and BuildModule interfaces to accept an Array<Target> of   'raw targets' instead of a Map<Integer, Target>. This more general representation   is needed because we may now have multiple targets of the same device type   active simultaneously. I add new static methods on the Python Target to   convert to this form in a way that mimics check_and_update_host_consist.4. I rework CompilationConfig to work from Array<Target> directly, to not depend   on the host_target argument (since dealt with on the Python side), and to   understand that if we have two targets for the same device type the non-external   codegen target takes precedence.The change to CompilationConfig seems neutral with respect to the recent discussionson compilation configuration representation and tvmc.I made a few attempts to remove Target.check_and_update_host_const entirely in favorof using CompilationConfig as the definitive target handling choke point but backedout once they became too large.* - Working on unit tests* - Fix two Debug-only failures* - Use Array<Target> in GraphExecutorCodegen/AOTExecutorCodegen ifaces instead  of CompilationConfig (don't want to bake it into any official APIs).- Started unit tests.* - Lints* - Moar Lints* - Fix some unit tests* - Fix last unit test failures* - whitespace* - Address Eric's comments.  CI likely to fail due to stricter FindPrimitiveTargetOrFail but let's see.* - Comment adjustments.- Unit test for new Target members.",0
[Hexagon] Removes directory after stopping the server (#11212)* removes hexagon directory,4
[Hexagon] Add AoT capability to Hexagon launcher (#11214)* [Hexagon] Add AoT capability to Hexagon launcher,1
Implemented rpc logging (#10967)Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,2
"[TIR] Fix printing enum in TransformLayout::AsPython (#11211)After this PR, `as_python` can handle `transform_layout` correctly. The result will be like```pythonsch.transform_layout(..., buffer_index_type=""read"", ...)```Previously `buffer_index_type` was printed unquoted.",0
Fix mixed precision output type to original type (#11142),0
"Revert ""Implemented rpc logging (#10967)"" (#11227)This reverts commit aa3bcd9d3374878c5e958b842f51bfd82f0ebd9e, because itfails on Windows CI as reported in issue #11220. PR #11223 tries to addressit but is is failing in the regular CI with testing issue on Hexagon.",1
[COMMUNITY] Nicola Lancellotti -> Reviewers (#11226)* adding new contributor* edit* Update CONTRIBUTORS.mdCo-authored-by: Nicola Lancellotti <nicola.lancellotti@arm.com>Co-authored-by: Nicola Lancellotti <nicola.lancellotti@arm.com>,1
[CI] Update all Docker Images to 20220505-060045-500703308 (#11219)This gives us GoogleTest for #11202 and blocklint for #11200 but most importantly it makes use of the new and improved tags from @leandron in https://github.com/apache/tvm-rfcs/pull/66Closes #11202Closes #11200,1
[Relay] Flexible shape dispatch transformation (#11199)* Added pass that creates a semi-dynamic dispatcher around a relay module.* Added automatic padding feature.* Output slicing working.* Multiple input support working i think.* Added test file.* Improve comments.* Fix lint.* Allow default values.* Fix docstring.* Improved documentation based on feedback.* Add extra check for record loading.* Improve variable names.* Add type inference to make sure things worked.* Added support for multiple outputs.,0
"[CI] fix docker group exists with different GID (#11184)This patch fixes an error while using the docker/bash.sh script toinvoke docker. If the same CI_BUILD_GROUP is present within dockercontainer but with a different GID, we try and fail to add the existinggroup This patch tries to fix this error",0
[TIR] Add schedule primitive SetAxisSeparator (#11225)* [TIR] Add schedule primitive SetAxisSeparator* remove unused include* Move ReplaceBufferMutator impl to cc file,1
"Update debugger.rst (#11231)Updating docs to reflect a way of using the debugger that works, see [TVM forum post](https://discuss.tvm.apache.org/t/runnig-a-model-with-tvm-debugger/9869/8?u=wheest)",0
"[AOT] Enable A-Normal Form in the AOT executor (#11091)* [AOT] Enable A-Normal Form in the AOT executorThe sequence of calls produced by the AOT executor codegen is arbitrary,especially in the presence of 'branchy' networks. This makes itdifficult to analyze memory usage for each call. By running theToANormalForm pass to insert a series of let bindings before thelowering and codegen stages, we can establish an ordering for theevaluation of the external calls, thus allowing reliable analysis ofmemory usage.Change-Id: Ic320b68cde83c96b228a8d1d2829a0e8ac7b768f* Maintain GetStorage(var) == GetStorage(value) invariant for letsChange-Id: Id40b70f67a3e37f75b8331aa89f1819072e4d48e* Add check to ensure ANF runs in AOTChange-Id: I8de2bd19c7c17057e2bc89f6a68595780c2e9433* Avoid let block traversal and don't visit var in let visitationChange-Id: I74c080e2a09e84a75400db5c3395d508697d5d0f",1
[Frontend][PyTorch] Add: Relay stft operator (#11190)* Add: Relay stft operator* fix doc* address PR comments* address addtional comments,0
[LLVM] Fix a possible tbaa issue (#11181)* fix a possible tbaa issue* Correct tbaa index unit by underlying buffer elemtype* always use byte as index unit in tbaa,0
fix two check typo in codegen (#11240),0
[TIR] Fix reverse_compute_at for trivial region with trivial block var (#11234)* [TIR] Fix reverse_compute_at for trivial region with trivial block var* Prevent handle arithmetics,0
[CI] Identify non-inclusive language in commits (#11230)* Adds a script blocklint.sh that checks for non-inclusive words  * Updates the task_lint.sh script to call blocklint.sh  * Replaces the terms Master and Slave where possible  * Replaces the terms Blacklist and Whitelist,1
[CMSIS-NN] Increase partitioning accuracy for pooling (#11229)This ensures that CMSIS-NN is only used when the batch size and layout are correct for the library calls.,5
[ETHOSN] Minor corner case fixes (#11218)Minor issues in corner cases from static analysis and a code standardviolation fix.,0
"[microNPU] Add support for conv2d running on two cores on U65 (#10251)* [microNPU] Add support for conv2d running on two cores on U65The 512 mac variant has two cores that processes the weights inparallel, so we need to split the weights and biases into twoand encode them separately.Change-Id: I53791f614288ac4df181b9462fc632d35b934a86* Changes due to rebase* Rebase, improve DivideConstants and expand testingMake the DivideConstants to operate on non-flattenedtensors to support two core execution in U65.",1
[Hexagon] Add mobilenet test with AOT (#11204)* add mobilenet AOT test* Add _serial_number to super class,1
[Hexagon]Disable hexagon gtest (#11236)* Disable hexagon gtest build,3
[tir] remove unused member variable (#11248)Remove unused member variable`tvm::tir::PackedCallLegalizer::tvm_value_index_`.This also fixes a GCC 7.5 compiler warning.,0
[microNPU] Remove spurious prints and improve documentation (#11247),4
[USMP] Change internal workspace section (#11246)This commit changes the internal workspace generationto be under .bss.noinit.* with NOLOAD behaviour as itdoes not need any form initialization.,4
[CMSIS-NN] Fix memory alignment bug in CMSIS-NN demo (#11221)* Updates convert_image.py to include memory alignment,0
[ROOFLINE] Calculate roofline from existing TIR PrimFunc (#11238)Refactor roofline_analysis to use a pass instrument to save TIR codefrom compilation for feature extraction. This should support differentcompilation pipelines and avoids recompiling the module twice.,2
"[LLVM] Make sure all functions have target-related attributes set (#11222)LLVM codegen create new function, e.g. the ""_compute_"" function fora compute_scope attribute, etc. These function did not have functionattributes defining the target properties, specifically ""target-cpu""or ""target-features"". Make sure this information is present on allfunctions created in CodeGenLLVM.",1
"[microTVM] Add support for host-driven AoT Executor (#11044)* Generate AOT Metadata when targeting C runtime and packed API.* Also copy metadata.h and metadata_base.h to standalone_crt.* add support for get_input_index as well as setting up get_input_info as unsupported* add support for tvm.aot_executor.create in C runtime* changes in-progress to unit tests* Include get_c_metadata in emitted function list* make CRT error codes generic for graph or AoT executor, fix AoT lib link order* add AoT executor creation and initializaion, as well as support for get_input_index()* add allocation of inputs, outputs, and pools; add get_input(), but shape encoded in metadata appears to be incorrect;* add support to test_aot_executor for get_input()* fix numpy array shape so that get_input() works properly* implement run(), get_output(), get_num_inputs(), and get_num_outputs(); test_aot_executor() is now passing;* fix up some issues from rebase with main* clean up logging and test_graph_executor()* lint clean-up* more lint clean-up* fix i386 build errors* first set of changes addressing PR feedback* more PR feedback: device pass-by-value, docstring entries, return variable name* add mangling of get_c_metadata() name to avoid function name collisions* only mangle get_c_metadata() when using C runtime* add static specifier to all kTvmgenMetadata variables to avoid namespace collisions* use TVM_IS_CPP_RUNTIME preprocessor define to deteremine whether or not to include metadata.h c++ code* add TVM_IS_CPP_RUNTIME define for cpptest* add TVM_IS_CPP_RUNTIME to apps/bundle_deploy* add TVM_IS_CPP_RUNTIME web/Makefile* update number of expected generated C files for AoT source files* break out metadata data structures into separate metadata_types.h header to avoid c/c++ issues and remove the need for the TVM_IS_CPP_RUNTIME define* remove TVM_IS_CPP_RUNTIME from web makefile* fix metadata.h include-order lint issue* correct error mask bits* address PR feedback* trigger build* trigger build* trigger build* trigger build* add alternate name for test_graph_executor() too see if it runs in CI* fix lint* revert alternate test codeCo-authored-by: Andrew Reusch <areusch@gmail.com>",0
[TENSORRT] Improvements and fixes for TensorRT (#11203)A number of small fixes and refactors to improve the robustness ofthe TensorRT integration.Co-authored-by: Mark Shields <mbs@octoml.ai>Co-authored-by: Mark Shields <mbs@octoml.ai>,0
[OpenCL] Change of OpenCL profiling logic (#11180)* Enable profiling only when it is used explicitly* Change logic of clCommandQueue create/destroy* Update comments* Linter fix* Refactor queue create* Move queue recreation logic to function* Replace profiling flag by the queue info request* Enhance readability* Fix linter errors,0
[ci][docker] Add Jinja2 to image (#11265)commit-id:207f3fb7Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[ci] Use r5.large nodes for builds and lint (#11258)This uses `r5.large` for linting and build steps and splits lint into 2 to keep runtime down. This is a subset split off of #11120. Once `task_cpp_unittest.sh` is fixed so it picks up sccache we can enable these smaller nodes there as well.,0
[USMP] Fix assert condition for TVMBackendAllocWorkspace (#11270)* Fix test condition* fix,0
[ETHOSN] Remove remaining support for the N77 variant (#11262)Specifically removes some TVMC tests that are no longer necessaryand some partitioning infrastructure.,3
[ETHOSN] Adding support for Leaky ReLU (#11261)* [ETHOSN] Adding support for Leaky ReLUChange-Id: Icad69b2ae6ed4b3f3949cf5673efe2571aa66f5f* add some missing error reportingChange-Id: I935054c4d19a939e122092fab3c6c77204d9ead8,0
Fix running gtest on Hexagon hardware (#11257),0
[ci] Add --docker-image option to ci.py (#11118)This allows users to specify what Docker image they want to use manually (i.e. for debugging if a user has built their own image)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[ci] Bump i386 shards (#11271)i386 is the longest running test step by a small margin:<graph incoming>This also only runs the C++ unittests on the first shard so we don't end up wasting compute minutes.Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[COMMUNITY] mikepapadim -> Reviewer (#11276),3
fix expand onnx conversion (#11278),0
[ARM][Strategy] Fix is_int8_hw_support check function (#11193)* Fix hw schedule condition* add warning messages to unoptimized schedules,0
[ci][docker] Use sccache everywhere by default (#11267)This adds `/opt/sccache` to the PATH of each of the CI docker images so when cmake looks for a C compiler it will pick up the sccache wrapper by default. This fixes some issues where compiler invocations weren't being run though sccache. With this approach the invoker doesn't need to do anything specific to set up sccache.This will require a follow up PR to update the Docker images and remove some of the sccache logic in `task_build.py`Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[Hexagon] capture gtest output and return over FFI (#11239)* [Hexagon] capture gtest output and return over FFI* rename to test_hexagon_unit_tests.py so it will run in CI* rename to run_unit_tests.cc* pass back gtest error code along with gtest output* skip Hexagon unit tests if gtest not enabled* pass gtest_args as pytest argument* change env variable to HEXAGON_GTEST* set HEXAGON_GTEST in the environment to enable Hexagon unit tests* add back try / except around get_function,0
[Frontend] [Paddle] fix testing problem (#11259)* fix testing problem* remove clear_executor_cache,0
Fix a case of linking to wrong OpenCL library (#11215)if explicit path to OpenCL SDK was pointed in USE_OPENCL option,0
"[microNPU] Adding a option to enable striping (#11263)This commit adds a cascader option to enablestriping explicitly.When doing so fixed a bug that is associatedwith block config selection, that will betriggered when striping is disabled.Co-authored-by: Elen Kalda <elen.kalda@arm.com>",0
[microNPU] Update existing microNPU tutorial for CMSIS-NN (#11285)* [microNPU] Update existing microNPU tutorial for CMSIS-NN * Added instructions to existing microNPU tutorial indicating how to offload operators to CMSIS-NN.Change-Id: I9faef1d92a2107e04cfc21b7bfd1b72dc1bd5489* [microNPU] Update existing microNPU tutorial for CMSIS-NN  * Reformat micro_ethosu.py with blackChange-Id: Id189f333b5bd891232781d4eb58522a240146c95,1
[rpc] Implemented rpc logging (#11232)* Implemented rpc logging* fixing windows build issue* triggerCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,0
[ci] Run docker prune directly in Jenkins (#11275)* [ci] Run docker prune directly in Jenkins* Inline scriptCo-authored-by: driazati <driazati@users.noreply.github.com>,2
"[ci][docs] Seed autotvm tutorial (#11147)* [ci][docs] Seed autotvm tutorialThe runtime on this one varies a lot, so this tries adding a seed to get it consistent* address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>",1
[ci][build] Use ninja instead of Makefiles (#10934)* [ci][build] Use ninja instead of MakefilesThis switches the CI build to use Ninja which has slightly nicer output and faster behavior in the face of re-runs. This also adds a `--verbose` flag to `ci.py` to control build output accordingly.* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[ci] Disable dependabot PRs (#11072)A bunch of these just got created (e.g. https://github.com/apache/tvm/pull/11070) and are clogging up CI with 2x normal number of builds since they push to a branch and make a PR.Co-authored-by: driazati <driazati@users.noreply.github.com>,2
[skip ci][wasm][ci] Fix WASM build and JS doc build (#11299)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"Fix json serialization for NDArray (#11303)When `NDArray` is being stored as `ObjectRef`, the serializer won't trigger the right path for storage. Under the new serialization mode, we need to be able to leverage the `repr_bytes` mechanism to save `NDArray`.This change is backward compatible -- ndarray saved in previous format will continue to work. And fixes the problem of serialization when `NDArray` is involved as part of `ObjectRef`. In the future, we can consider consolidate the `NDArray` save into the `repr_bytes` and remove the specialization as we evolve to newer versions",0
"Prevent simplifing unit IterVar in CreatePrimFunc (#11292)Simplifying unit iter vars in CreatePrimFunc changes semantics of the PrimFunc, which need different handling in analysis.This reverts commit 26cefab5df8f24af7dc43a3239dbfd0e858fd1a2.",3
"[microNPU] Add various options to the cascader  (#10509)* [microNPU] Added options to Cascader* Added option to toggle multi-dimensional striping, it is disabled by  default because it has a very high computational cost. Single  dimension striping shares most of the benefit with greatly reduced  cost.* Added multiple developer/debugging options prefixed with 'dev_'  Also added these options to tvmc.* Added cascader logging, if enabled it will dump information about the  cascader proposals to a 'cascader_log.json' file.Co-authored-by: Matthew Barrett <matthew.barrett@arm.com>Change-Id: I2ec59ae0bd84b73b2cc4bc56d39e3831b0aeec27* Updated memory_reduction testcasesAlso added enable_striping to plan_generator.hChange-Id: I496b30ed6af6f0730087329cd81a69c5040a5e4dCo-authored-by: Matthew Barrett <matthew.barrett@arm.com>",0
"Add Adreno GPU target and topi supporting textures with dynamically allocated textures (#11161)* Add Adreno GPU target and topi supporting textures- There are 5 compute/schedules: conv2d for NCHW/NHWC, depthwise_conv2d  for NCHW/NHWC, average pooling- Fix of dynamically allocated textures caching- Add texture-nhwc scope- Fix issue with codegen of vars having non acceptable symbolsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>* Address comments* Add vectorization into some adreno pool flowCo-authored-by: Li <quic_lih@quicinc.com>* Fix adreno tests for running on the opencl host platform* remove unnecessary kDriverVersion in DeviceAttrKind* Move utils adreno functinos to separate shared file* fix black hitsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>Co-authored-by: Egor Churaev <egor.churaev@gmail.com>Co-authored-by: Li <quic_lih@quicinc.com>",0
[Hexagon] Update Readme (#11283)* move conv2d readme* Update README,1
[logging] LOG(FATAL) calls [[noreturn]] functions (#11310)Ensure that `LOG(FATAL)` always resolves to calling`[[noreturn]]` code.  This has two benefits:- Helps developers more quickly understand the intended/required  behavior for `LOG(FATAL)` calls.- May eliminate spurious compiler warnings based on control-flow  analysis.  E.g. gcc's / clang's `-Wno-return` warnings.,2
"[Hexagon] Remove sim_options from tvm.target.hexagon() (#11293)We no longer run simulator automatically, so this is not necessary.Also, the only way to pass options to the simulator was by settingan environment variable. That variable (HEXAGON_SIM_ARGS) shouldbe set independently by the user from now on.",4
"[TIR][Arith] Implemented padded inverses in IndexMap (#11235)* [Debug] Error logging in DetectIterMap* [Affine] Allowed PrimExpr argument to NormalizeIterMapToExprThis allows it to be used for any expression containing an`IterMapExpr`, not just expressions whose top-level node is an`IterMapExpr`.* [Affine] Implemented DetectPaddedIterMapThe existing DetectIterMap tries to rewrite index expression as alinear combination of split/fused iterators, where the new iteratorscover the exact same indices as the original expression.DetectPaddedIterMap relaxes this condition, allowing the new iteratorsto cover a superset of indices that the initial index expressioncovered.  It uses the minimum amount of padding necessary to representthese transformations, and also a predicate that identifies anypadding that has been added.This is a utility function to be used for layout transformations ofbuffers, in cases where the pre-transformation shape of the bufferdoes not evenly fit into the post-transformation shape.* [IndexMap] Implemented IndexMap::NonSurjectiveInverseAllow non-surjective transformations, with DetectIterMap used todetermine the minimum padding to insert.  Returns the inversefunction, along with a predicate that identifies padding indices.  Thepredicate is in terms of the transformed variables.* [IndexMap] Exposed methods to python- `IndexMap::Inverse` exposed as `IndexMap.inverse`- `IndexMap::MapShape` exposed as `IndexMap.map_shape`- `IndexMap::NonSurjectiveInverse` exposed as `IndexMap.non_surjective_inverse`* [IndexMap] Extracted _assert_equal_index_map into class methodIn preparation for adding additional tests for the IndexMap class,which will require this functionality.* [IndexMap] Added unit tests for new behavior* Re-enabled divisibility check in CheckMappingInitially disabled as dynamic shapes resulted in padded lengths whosedivisiblity couldn't be proven.  Re-enabled along with asimplification rule to resolve it.* Fixed breakage in compute_at primitive* Corrected typos/examples in docstring",0
"[profiler] Skip i386 skip condition (#11280)See #10698 for some context, this test wasn't actually being skipped on i386Co-authored-by: driazati <driazati@users.noreply.github.com>",2
"[QNN] Enable constant folding for QNN operations. (#11228)* [QNN] Enable constant folding for QNN operations.This commit enables constant folding for QNN operations.This functionalty is disabled by default, use fold_qnn=True to enable.Co-authored-by: Alexander Peskov <peskovnn@gmail.com>* [NFC] Fixed comments* Added more unit tests for QNN opers in constant folding pass.* Address PR feedbacksCo-authored-by: Alexander Peskov <peskovnn@gmail.com>",0
[Relay] Fix a corner case of fused identity (#11217)* fix a corner case for fused identity in te compiler* make fallback code work with gpu,0
Consider pad value and input zero point in FoldExplicitPading (#11127)This commit adds the following:Do not fold `nn.pad` and `qnn.conv2d` if padding value is notequal to input zero point of qnn operation. Added unit testto check such behaviour.,1
Avoid use of MemoryInfo when undefined in StorageRewrite (#11254)* Check if the requested memory info is defined before using it.* Address review comment to add warning when MemoryInfofor scope is undefined.,1
[Hexagon][Docker]Add HEXAGON_SDK_ROOT ENV variable (#11291),1
"[ROOFLINE] Roofline analysis over RPC (#11252)* [ROOFLINE] Roofline analysis over RPCRun roofline analysis on remote devices if requested. Peak flops andpeak bandwidth estimation are done on the remote device.* allocate testing arrays directly on device and randomly fill* forgot to include remote* lower flops ratio, machine may be using multiple threads* forgot fill",2
[build][hexagon] Respect x86 C/C++ compiler choice (#11312)- Fix issue where `CMAKE_C[XX]_COMPILER` isn't propagated  into the build configuration for `x86_tvm_runtime_rpc`.,0
[Hexagon] Add USMP tests (#11279)* Add USMP tests* Address Chris comments* Address Chris comment on assert* trigger,1
[ci][docker] Update images to include sccache changes (#11314),1
[PYTORCH] [FRONTEND] torch.bool support for data type conversion (#11290)* [FRONTEND][PYTORCH] Support fo nn.SiLU added* torch.bool added to torch convert_torch_dtype_map,1
"Oneflow fronted support more model and fix bug (#11321)* add relay.f.frontend.fm_oneflow support cnns* support cuda* fix mobilenetv2 and reviews* fix: model without meta info* support eager and yolo, add test* fix: license* add: tutorials* fix: support new graph* fix some comments* refine* fix concat op convert bug* refine* refine* change cuda to cpu* fix bug* fix ci error in tvm* fix pylint check* delete useless file* add skimage package in docker* fix ci error* fix bug* add oneflow fronted test in ci* merge conflict* fix tutorial* try to find error in ci* revert* merge conflict* black oneflow* Delete from_oneflow.py* restruct oneflow fronted* support vision-transformer* black format* update black version and reformat* fix ci error* fix doc error* fix gpu fronted test failedCo-authored-by: hhhfccz <hjk1938927583@163.com>",0
[MetaSchedule] Allow Easy Logging Level Setting (#11305)This PR allowed users to set logging level without giving a logger config. Previous implementation hard-coded `logging.INFO` as the default logging level and requires a logger config to change it. Now the logging level and handlers can be inherited from the current `tvm.meta_schedule` logger setting.,2
"[TVMScript] Represent ramp as index slice (#11308)* support represent ramp as index slice in tvmscript* fix testcase's comment, check slice lanes instead of extent",0
[ONNX] Fix cast op to/from bfloat16 (#11171)* fix cast from bfloat16* fix cast to bfloat16 test as well* clean up comments* lint* add commentCo-authored-by: Margaret Qian <mqian@octoml.ai>,0
[BugFix][Topi] Fix 'duplicated iterator names in the compute definition' bug of roi_align (#11322),0
adding ramana to reviewers list (#11311),1
[frontend][ONNX]support ConvTranspose explicitly specified output_shape (#11076)* support ConvTranspose explicitly specified output_shape* fix unit test case* fix lint test* retest* fix code error* fix lint test* update test* retest* fix test onnx official tests,0
Missed out_layout field of conv1d attrs (#11325)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>,5
"[TIR] Propagate storage scope of undefined vars in SplitHostDevice. (#11255)* [TIR] Propogate storage scope of undefined vars in SplitHostDevice.* Test global.texture for input, output, and intermediate buffers.",3
Add vlogging for type-table registration. (#11041),1
[build][hexagon] fix several compiler warnings (#11245),0
[TIR] Support affine expressions as indices in reverse compute inline (#11317)* [TIR] Support affine expressions as indices in reverse compute inline* fix trivial iterators,0
[TIR] Simplify indices in layout transform (#11330)Co-authored-by: Yuanjing Shi <yuanjing@octoml.ai>Co-authored-by: Yuanjing Shi <yuanjing@octoml.ai>,5
[CMSIS-NN] Align CMSIS-NN in TVM to TFLu SHA (#11273),5
[CI] Update Docker images for new CMSIS-NN (#11336)Updates ci_cpu and ci_qemu to pull in #11273,1
[Hexagon]Use requires_hexagon instead of requires_hexagon_toolchain if running on hexagon target (#11294)* refactor requires_hexagon_toolchain* trigger* lint,2
[docs][microtvm] fix command path in microTVM Reference Virtual Machines Running Tests documentation (#11333),0
"[Runtime][ThreadPool] Enhance CPU Affinity configuration for OpenMP case. (#11343)This commit allows to pin threads to cores when we use OMP. It enhances`tvm::runtime::threading::Configure` method to work with OMP and ""kSpecify""affinity mode.",5
[BYOC] Threadsafe initialization of JSONRuntime module (#11339)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>,5
"[build] Fix/simplify `ccache` logic (#11189)- Remove TVM's `USE_CCACHE` option in favor  of CMake's built-in `CMAKE_C_COMPILER_LAUNCHER`  and `CMAKE_CXX_COMPILER_LAUNCHER` variables.  This eliminates a significant source of  complexity, especially:  - TVM's CI scripts, which use `sccache`    instead of `ccache`, and  - calls to `ExternalProject_add` in TVM's CMake logic.- Ensure that `CMAKE_C[XX]_COMPILER_LAUNCHER` variables  are passed through in all `ExternalProject_add` calls.- Update user documentation.",0
"[ci] Use r5.large nodes for hexagon build and some tests (#11120)* PR #11314 - [ci][docker] Update images to include sccache changes* [ci] Use r5.large nodes for less-intensive jobsThis uses the `CPU-SMALL` label for certain jobs in CI, which is backed by r5.large instances in EC2 rather than c4.4xlarge instances which are much more expensiveCo-authored-by: driazati <driazati@users.noreply.github.com>",1
[Hexagon][Docker] Update image version (#11332),1
logsoftmax reusing the softmax function (#11141)Co-authored-by: caizihua <978497756@qq.com>,2
[Relay] Bug fix when applying history using an iterator or records. (#11306)* Bug fix when applying history using an iterator or records.* I forgot strings are iterables.,0
"[skip ci][ci][docker] Pin Pillow version (#11348)A recent release depends on some things we don't have installed, so don't use it.e.g. https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-11319/5/pipeline/Co-authored-by: driazati <driazati@users.noreply.github.com>",2
[ci] Bump job timeout to 3 hours (#11350)This is intended to be temporary to avoid timeouts on jobs while we work on getting some things under control like artifact upload time and shards for various jobs.Co-authored-by: driazati <driazati@users.noreply.github.com>,5
"Improve error messages with TVM_LOG_DEBUG and add docs (#11344)* Improve error messages with TVM_LOG_DEBUG and add docs.* Fix requirement to prepend ""src"" with /.",0
"[TVMScript] Support inlined function call as a sugar (#11324)* [TVMScript] Support function call to help construct AST* add test* update test* more comment* fix for avoiding Buffer.vload(...) case* update parse error msg* wrap func call with try / catch, emit error msg* silence pylint",0
[Runtime]Considering DLTensor's byte_offset in ZeroCopy function (#11340),5
Fix eltwise alter op layout for broadcast axis (#11337)* Fix eltwise alter op layout for broadcast axis* Add tests on boradcast blocking over already blocked layout,0
[TVMC][ETHOSN] Improve target string to avoid duplication (#11272)* [TVMC][ETHOSN] Improve target string to avoid duplicationImproves the TVMC target string to avoid duplication of theNPU variant. The new target string will require the just the NPUname followed by -variant=n78. The old target string is deprecatedand will be removed in a subsequent version of TVM.Change-Id: I4638f36788df3f478435ac13d3531aad2b23f204* fix lintingChange-Id: I76a9da511899f24a163be669877605cd1a440022* fix make variant functions and update test error messageChange-Id: Iff553d4b255c0ce0b86bad42eaa94ee9b1c62508,0
[microNPU] Add a pass to reorder copy and compute nodes (#10959),1
[TFLite] Add support to int16 data type in TFLite frontend (#10915)* [TFLite] Add support to int16 data type in TFLite frontendAdd support for int16 data type and int64 biases/accumulators inthe TFLite frontend.Adjusts TFLite tests to cover int16 convolutions and element-wise;Fixes a minor typo negtive->negative in the element-wise tests.* Update src/relay/qnn/op/convolution.ccCo-authored-by: Elen Kalda <elen.kalda@arm.com>Co-authored-by: Elen Kalda <elen.kalda@arm.com>,0
"[microNPU] Fix bug in channels extraction in the matcher (#11335)* [microNPU] Fix bug in channels extraction in the matcherIf the input tensor layout is in NHCWB16, we were passing W valueinstead of the channels to get_valid_block_configs.* Add test for conv2d",0
"[TIR] IndexMap Simplification Constraints (#11342)* [TIR] Added optional arith::Analyzer argument to IndexMap methodsSimplifications done when applying a transformation may requireiteration bounds from the caller scope.  This is a C++ only feature,because `arith::Analyzer` doesn't inherit from `ObjectRef`, and cannotbe passed through the FFI.* [TIR] Pass analyzer from TransformLayoutRewriter to IndexMapAvoid needing to simplify twice, now that IndexMap can accept theanalyzer from the calling scope.* [TIR] Added BlockNode handling to IRMutatorWithAnalyzerIteration variables defined in `BlockNode::iter_vars` may be usefulfor simplifications.  This functionality was extracted from`TransformLayoutRewriter`.",1
fix matmul broadcast (#11242),0
"Fix function number datatype from char to uint16_t (#10014)rewrite the modified part to pass lint checkUse 2 bytes for func num in fun_registryFix errors in linterAdd the declaration of the helper functionsset 2 bytes for func num in func_registry test unitspass num_func by valueThis commit change the datatype of the number of the function from 1 Byte to 2 Bytes.Besides, I use some helper functions to access the number of function and the first function name.",0
[ci][docker] Conditionally link sccache to clang (#11316)This was causing errors with #11314 since it was making it appear as if `clang` was available when it was only the sccache wrapper.Co-authored-by: driazati <driazati@users.noreply.github.com>,0
[CI] Added message if test is running on another shard (#11331),1
[CI] update oneDNN to v2.6 (#11140)* enable CI to get and build latest oneDNN release* remove the source code after installed* fix wget error and improve naming* refine the cmake/make commandsCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* pinned to v2.6 by default* simplify the logic and install to /usr/libCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>,0
[Hexagon] Add unit tests for Hexagon Device API (#11319)* [Hexagon] Add unit tests for Hexagon Device API* add scalar alloc for Hexagon + cleanup,1
[Hexagon]Refactor Hexagon_SDK_PATH (#11282)* refactor HEXAGON_SDK_PATH and remove HEXAGON_GTEST,3
use libtorch c++ distribution with c++11 strings in gpu image (#11346)* use libtorch c++ distribution with c++11 strings in gpu image* libtorch path* don't activate libtorch before merging the image,5
[ci][actions] Add more HTTP retries for conda (#11360)Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[skip ci] Revert ""Fix function number datatype from char to uint16_t (#10014)"" (#11363)This reverts commit f34bd22ddc4e7064eabe9fac42c4c04f54ede399.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
"[bug fix] skip ""__nop"" functions in graph_executor_debug (#11353)* bug fix, skip __nop functions in running operation over RPCCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>",0
[Frontend] [PaddlePaddle] Add split operator (#11354)* suuport split op of paddlepaddle* black formatting,1
"[Relay] Support i16, f16 scalars in Relay text (#11224)While testing fp16 models for Collage discovered the Relay textformat did not support f16. While adding that cleaned up scalar handlingin general. However I left two inlined tests for 'is simple const'in place (fuse_ops.cc and memory_alloc.cc) since it's not clear whetherthey should remain specific to just {i,f}{32,64} or whether they canbe replaced with the support::IsSimpleScalar central predicate.",1
nn.batch_flatten is a reshape op (#11367),5
[Hexagon] moves conftest.py to tvm.contrib.hexagon so outside repos can access the testing fixtures (#11277)* adding pytest_plugin to python so other repos can access* import requires_hexagon_toolchain from tvm.contrib.hexagon.pytest_plugin,0
[ci] Use S3 for artifacts (#11349)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
[microTVM][ARM] Add Relay tests for conv2d registered schedules (#11250)* Added conv2d relay test for each schedule* Enable relay tests in qemu* split aot test utils,1
[Runtime][PipelineExecutor] Add graph manually splitting logic into the unit test. (#11334)* [Runtime][PipelineExecutor] Add graph manually splitting example intothe unit test.Current unit test create 3 seperate module then re-connect them torun the pipeline executor. And this is not a real use case for pipelineexecutor.Adding a manually graph splitting logic which split a full network into 3subgraph then run the pipeline executor and verify the result tosimulate the real use case.* address review comments* trigger build.* address review comments* address review comments* rebase and trigger build.,1
fix vec*mat in PyTorch converter (#11347)* fix vec*mat in PyTorch converter* Trigger CI,0
"[PTX] Intrinsics for async copy from global to shared (SM80) (#11368)* registor ptx builtin for async copy* add basic codegen* add test* update codegen* wip* codegen bug fixed, test working* add commit group* add doc",0
[ci] Disable flaky onnx tests (#11376)Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[ci][easy] Fix parameters for macros (#11377)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
Add Conv3D bindings (#11381),1
"Fix function number datatype from char to uint16_t (#11365)* Fix function number datatype from char to uint16_trewrite the modified part to pass lint checkUse 2 bytes for func num in fun_registryFix errors in linterAdd the declaration of the helper functionsset 2 bytes for func num in func_registry test unitspass num_func by valueThis commit change the datatype of the number of the function from 1 Byte to 2 Bytes.Besides, I use some helper functions to access the number of function and the first function name.* Fix aot_executor_module to unbreak CI.* Fix GraphExecutorModule.* Remove graph_json_to_c_func_registry. * No longer needed and not called anywhere. * Superseded by emitting the FuncRegistry directly in codegen.Co-authored-by: 嚴中璟 <a1245967@gmail.com>",0
"Fix array pointers releasing with `delete` operator (#11328)It may be safe to release POD-types array with `delete`operator, but `delete[]` is always better.",0
[Bugfix] Fix qnn.quantize type func with incomplete type (#11124),0
[CI] Update CPU and GPU image (#11369),1
"[Schedule] Allowed typing.Tuple in tir.schedule._type_checker (#11289)* [Schedule] Allowed typing.Tuple in tir.schedule._type_checkerPreviously, `typing.Tuple` annotations could not be used with`tir.schedule._type_checker.type_checked` annotations.  This allows`Tuple` type annotations to be type-checked.* Revert change, allow tuples input as List arguments* Suppress mypy errorsDirectly interacting with a type object would otherwise cause somefalse positives.* Corrected unit test for allowing tuples to be used as typing.List* Represent multi-type lists as List[Union[...]] instead of List[Any]This gives a better error message and plays nicely with _type2str,since `typing.Any` doesn't have a `__name__` field.",0
"[docs] Add lightweight docs image (#11045)* [docs] Add lightweight docs imageThis image includes everything necessary to build the docs without any tutorials and is just about 1.5 GB which is significantly less than the CPU/GPU images.* remove ci.py docs --cpu flag, imply it via a lack of --tutorials/--full so it is the defaultCo-authored-by: driazati <driazati@users.noreply.github.com>",1
"[TIR] Support tensorization using ldmatrix + MMA (#11355)* [TIR] Support tensorization using ldmatrix + MMAcommit 3218facf100b0dfc55715acfd1cee156764129baAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 14:04:56 2022 +0900    some clean upcommit 7a235b69dc2023b3098ed44d591edb63b20a8f4eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 13:55:11 2022 +0900    parameterize over storage scope in mma store intrincommit 827ea4c434c35607b241f8e0ae2efe3214ac2458Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 13:37:38 2022 +0900    properly handle floordiv/mod in codegencommit 42d4c6f42182c9fd79566c0955f99cc82abd5144Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 09:53:57 2022 +0900    update tuned factors for fp16commit 328d0aa36b2ea9ea1b051970d612bff82d2d20e6Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 08:43:30 2022 +0900    all tests workingcommit 5e086cf5fd1404ac38f85c4bfbe692687b45a16cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 07:48:43 2022 +0900    add doc for mma_fill and mma_store intrincommit 4f945c4116b6d3bdc965ecb2be2229bb46dc11abAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 18 06:39:01 2022 +0900    remove testscommit df7708f7f67761d9c18f9564bc15abd50c12ac69Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:52:14 2022 +0900    unified testcommit 754c83eeb8510b31fb9652b089177f9b8e642ec0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:36:24 2022 +0900    clean up LowerWarpmemorycommit 178c3dcee7bfa17d5d93fec02aa858dc62151670Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 19:15:04 2022 +0900    Use IndexMapcommit 07fb58910338c62847fd902b37801d09b8c673b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 17:51:44 2022 +0900    remove 16x8x8 testcommit 2b05b5a5470ac221d559f31a31a8e2ff753b2414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 17:31:35 2022 +0900    generate mma fill/storecommit bf23fc50f0ffa99e875d9247ca66acec0c36677fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 12:23:30 2022 +0900    mma intrin generation with meta programmingcommit 5afb5f00afd642cb1e39872edc7965f476dcdcb7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 17 05:26:14 2022 +0900    ldmatrix intrin generation with meta programmingcommit fb62abb3424b88ec48c697e306e05889a3ac306fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 20:30:49 2022 +0900    minorcommit 5a80adce24e84d3ec6bf931b60cb9c730d243394Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:55:57 2022 +0900    revert some changecommit e599a55078ee75f2480a721098341812db58cf6fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:54:18 2022 +0900    remove obsolete filescommit 4b13b85ff91d0d592a7e0c01924e0b49b82f35a8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:51:21 2022 +0900    wipcommit 848de63455539e25cd0d43e5a65fd048636ef0f7Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:44:29 2022 +0900    wipcommit b35bff97ed10c22559e2164eb7538db0f711ce7eAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:31:18 2022 +0900    update parse error msgcommit ad9b053ef865b1f91f03d7b15ed7aae3420ee213Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 19:26:51 2022 +0900    fix for avoiding Buffer.vload(...) casecommit 54c686443e370edbfae860d0809b1b6182d26414Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:59:55 2022 +0900    wipcommit 078060fe28d22f1db5f07b1c382dee438f02df60Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:57:34 2022 +0900    wipcommit 576f8415e65e0e8a8a7808885e219b3b53867950Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 18:52:15 2022 +0900    wipcommit 12a376ae2f44aa6660121e64e0358f2866624f7fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 17:54:58 2022 +0900    Squashed commit of the following:    commit 48eef4981d1a55aaf3b0ac935f2a10347cb1ac2d    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 17:40:48 2022 +0900        more comment    commit 8f67fc87038834e9f7e2c5cd3dfe61fabf442206    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 17:11:27 2022 +0900        update test    commit ad85036621c005b733763e67ceffae39c356ec99    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 16:54:01 2022 +0900        add test    commit 4a5dc3ffd5d0bb4a1700e57897c9e0f26e3d2a88    Author: Masahiro Masuda <masahi129@gmail.com>    Date:   Mon May 16 16:40:47 2022 +0900        [TVMScript] Support function call to help construct ASTcommit 76c1bcf0ade45d7433a0066236add8372b1cc547Author: Masahiro Masuda <masahi129@gmail.com>Date:   Mon May 16 16:30:07 2022 +0900    simplify iterator in layout transformcommit 936280324ea2c91429a6a85a1b8ee89c7b825928Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 11:31:39 2022 +0900    remove obsolet filescommit 2e119b422d72d726d5f2bd20fe48a1e62fcb0510Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 10:43:59 2022 +0900    calculate mma store dst index using inverse affine mapcommit 9489434ee52b546e2abb2ab28173eefd51525ba4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 10:01:12 2022 +0900    simplify storecommit 1adcb77b8bba8e5d91080fe6cbfc7add7f4365c2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 09:43:40 2022 +0900    simplified fillcommit 7b13c736d23e0eac94137aa918101d788e60d4f3Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 09:22:17 2022 +0900    simplify intrin desc using index map functioncommit bcf212dda0f94c51f55c48921f61d92fd3b83777Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 07:16:42 2022 +0900    seems to workcommit dd8ccf9ec2e48100158152e5d4590d141424e2e2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Sat May 14 07:11:57 2022 +0900    poking with the parsercommit 596582cbfbd08ebe23ea71aaf7a447472415ccd1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 20:04:59 2022 +0900    16x8x32 4k trans workingcommit 273f89a8a6ac34f7c79147563922d34d44bffd08Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:52:13 2022 +0900    add 16x8x16 fp16 transcommit 8e2066cc4c6e86616bc9751324e63ba81a3b02afAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:32:37 2022 +0900    16x8x16 4k trans workingcommit c2d0744051733e94f840d4517bcee9ca5d444c75Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 19:25:52 2022 +0900    16x8x16 trans workingcommit c2e314cdda1c3a931781e51a863901ea178dffecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 16:19:32 2022 +0900    tuned int8 4k, 91 TOPScommit 94d9d965f19ff1a2ebdd342079ef420fb537b16aAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 15:59:33 2022 +0900    int8 4k tune workingcommit 3ca8ca02593aff7540c9655aa831348246171752Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 08:43:57 2022 +0900    mma 16x8x32 int8 working with ldmatrix b workaroundcommit 54f1cb731d4b42a6cbc08baf144e74646400eef5Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 18:23:27 2022 +0900    wipcommit 9d2844db602dc65af4dbd06a73fdd815f486b8b9Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 16:38:53 2022 +0900    test tensorize without layout transformcommit 86ee6dabc801aeb8d6917bec6de97b42025dbdd1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 15:15:34 2022 +0900    int8 4k tensorize workscommit 39f9e32c9a64222c91daba2c32969b27207a31d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Fri May 13 12:44:39 2022 +0900    begin int8 4k tunecommit 6fa91e55b5ab2ba0f901d0d35be1b2fb3ab092b0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 18:53:20 2022 +0900    try fix ldmatrix b for int8commit 7a962cddc4799fa3df0c0fdf3c056146d3f2cbdfAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 18:28:34 2022 +0900    fixed warp_coeffcommit a0afb5698f307382147a38819e004a2db7f554b1Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 12:20:01 2022 +0900    wipcommit f70ccd09b07d5325454ffdc39a7619ea84aa7e06Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 12:09:57 2022 +0900    int8 tensorize workingcommit 20321fa4674dabc78fe55b5e0e2876c35b245d21Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 07:06:22 2022 +0900    starting 16x8x32 int8commit 441fd193c59cdc436d87ab35896cbb8c779ddf35Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu May 12 05:50:46 2022 +0900    adding fp16 accum casecommit c9d40b69b1b57bfaddffba09ea07624ae90ee465Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 17:04:29 2022 +0900    clean upcommit 5b2d48635e762c77c824d1c259ac8bcbcc949421Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 16:38:19 2022 +0900    16x8x16 4k tune workingcommit c3cb170d85600d03da5c3f4cda03552208ca0b8cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 16:20:27 2022 +0900    tensoriz fixedcommit 68039b081efcdd6aea1d132940b3745f50164974Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:55:25 2022 +0900    begin 16x8x16 4k tunecommit ced5d8d980cc267d4735957c25cb60d71ae977d2Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:50:11 2022 +0900    16x8x16 workedcommit 3d2c90d77c1bb2df2193e9af6cbaa2bd927a26d8Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:47:26 2022 +0900    fixcommit 403050b03ad6b4f0ee8d45088ffb324727bbae48Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 15:45:10 2022 +0900    add 16x8x16 testcommit 18e8d73661c99cd1c83021063b41a457afcb1638Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 06:50:32 2022 +0900    fixed mma store codegen for 16x8x16commit ec81250561195705122bccb9a2372f71de68121fAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 04:25:25 2022 +0900    add 16x8x16 mma store codegencommit e08df2a62a4809bcd39782949283c16e7703aa5cAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 03:47:47 2022 +0900    tensorized C_warp initcommit ae0678918929c1ceec73f2039467040c5bb7823bAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed May 11 03:06:06 2022 +0900    mma store codegen workingcommit deb4d6646cc93d4cdb4f2560ce723bee4d86e144Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 10 19:22:57 2022 +0900    update lower warp memorycommit 71fe5fe465300705fa94f9544a2e1a5070de6e0dAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Tue May 10 09:01:42 2022 +0900    tensorizing mma storecommit e80a1f148c47f2a3fac2363a733d8d4e2a2631d0Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 19:54:08 2022 +0900    clean upcommit a9640f4b7c3c9f22b87ca74a61003438dfd8f992Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 19:40:55 2022 +0900    add tunable 4k test, 36 TFLOPScommit b9f7eae7041d1a9b3e434c331c874e8347e89dc4Author: Masahiro Masuda <masahi129@gmail.com>Date:   Thu Apr 28 18:01:08 2022 +0900    fixed bug in LowerWarpMemory index splitting for ldmatrixcommit 00df30823f874910ed1ec1f74718100311764234Author: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 07:58:17 2022 +0900    fixed missing reverse_compute_atcommit 93f9fe7e5f7ad16c8d0e6240c16c0281a0e97decAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 06:55:12 2022 +0900    add 4k testcommit 3689ef712aa4b282a4818fa2fa2e7e349c3a5eecAuthor: Masahiro Masuda <masahi129@gmail.com>Date:   Wed Apr 27 06:54:09 2022 +0900    temp disable high dim base indices check in tensorizecommit 0c859c4f385ba0b6f9477b569b80cee80b5b7282Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 19:18:23 2022 +0900    clean upcommit f6aadbfcfbd73c1667a6de7aedc5894232b8e750Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 19:13:09 2022 +0900    Add 16x8x8 MMA + LDMatrix testcommit 4cf6b20c6ca415e967ab58d80e4a77c701ad7255Author: Masahiro Masuda <masahi129@gmail.com>Date:   Tue Apr 26 18:04:17 2022 +0900    testing 16x8x8 ldmatrix tensoriation* set measure_perf to False* add requires_gpu decorator in tests, always test build on non-ampere* skip cuda compile on old gpu",0
"[skip ci] Fix scipy intersphinx link (#11399)Follow-up from https://github.com/apache/tvm/pull/10181, as the URLhas changed again in https://github.com/scipy/scipy/pull/16221.  From[thiscomment](https://github.com/scipy/scipy/issues/14267#issuecomment-1034196161),the `html-scipyorg` portion wasn't intended to be part of the URL.This should resolve the HTTP 404 occurring in `Docs: GPU`step (e.g. [here](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-11269/13/pipeline/405#step-975-log-73)),by accessing `https://docs.scipy.org/doc/scipy-1.8.0/objects.inv`instead of`https://docs.scipy.org/doc/scipy-1.8.0/html-scipyorg/objects.inv`",0
[ci] Restructure Jenkinsfile (#11380)Co-authored-by: driazati <driazati@users.noreply.github.com>,2
"[Meta Schedule] Add Auto-Thread Binding Rule (#11177)The current meta-schedule uses a PostProc `RewriteUnboundBlock` to auto-bind blocks to threads. However, it's a post proc, which means there are no search opportunities, and always splits with `factor=1024`. This PR adds a new search rule called `AutoBind` to do a similar thing to bind threads with sampled factors. Also with a corresponding mutator. After applying this rule, we get some positive perf results (on RTX-3080):Element-wise: from 2.76 us to 2.48 usConv2d Winograd: from 29.45 us to 18.96 us (ansor 22.00 us)Resnet18: from  0.591 ms to 0.531 ms (ansor 0.565 ms)",1
"[FFI] Renamed __VisitAttrs__ and __fvisit__ to non-reserved names (#11392)All names beginning with two underscores are reserved for thecompiler, even if they occur inside a class or namespace.",2
[MetaSchedule] Enhance CPU auto vectorization (#11404),5
Fix int8 cuda kernels on older SM versions (#11389)* Fix int8 cuda kernels on older SM versions* Update target.py* Simplify initialiasation of do_tensorize* Simplify initialization of do_tensorize dense* Simplify initialization of do_tensorize in group_conv_nchw* Fix tensorize for conv2d_int8 as well.* Try to make linter happy* make linter happy* Fix wrong commit to auto_scheduler,0
[Tests] Replace the Relay interpreter with the VM in the op tests (#11386),3
[CMSIS-NN] Aligned buffer sizes for Conv2D post CMSIS-NN SHA update (#11359),1
[TVMScript] fix typo for block syntax (#11407),0
"Finish support for list-of-targets (#11382)* Finish support for list-of-targetsThis finishes the work started in https://github.com/apache/tvm/pull/11173 to support'external codegen' targets in the N build-like API surfaces. - It turns out it's ok if a build is given only a single 'external codegen' target, so remove that check   in CompilationConfig::Init. When Collage builds a 'candidate partition' it does so for a single target.   As far as Collage is concerned it does not care whether the target is regular (eg Target(""cuda"")), or   for a specific external codegen (eg Target(""cutlass"")), it just passes the target into the build. - Add CompilationConfig::FindPrimitiveTargetForKind which I'll later need to retrieve   the external codegen Target instance corresponding to a ""Compiler"" attribute value. - Target.update_target_host_consist was supporting three API styles:    - single target    - map from device type to target    - map from target to IRModule (for the ir_to_runtime API)   I replaced all those calls with a more specialized 'canonicalize' call:    - Target.canonicalize_target_and_host    - Target.canonicalize_multi_targets_and_host    - Target.canonicalize_target_map_and_host   In particular, all the tuning interfaces (task extraction, tuning, tuning records) all explicitly   *do not* support multiple targets since the underlying code just doesn't support that.* - Lints- Revert unintended changes* - more lints* - Fix model_library_format handling of target.- Improve comments in compilation_config.h* - Lints- Update target/target_host params documentation* - Fix micro library format tests- Rev micro library format from 5 to 6- Use Target.current() in a few places* - eta contract comprehension* - Woops, one more device: target map left- Handle host already being in Target* - lint* - lint* - Bug with append- Take device type from target* - Fix hexagon",0
"[CI] Revert #10181 / #11399, use non-versioned scipy intersphinx link (#11411)Follow-up from https://github.com/apache/tvm/pull/10181 andhttps://github.com/apache/tvm/pull/11399.  Thank you to @rgommers for[pointing out](https://github.com/apache/tvm/pull/11399#issuecomment-1133874138)that the non-versioned link is stable and working.  The use ofthe versioned link was only introduced to work around the breakage ofthe stable link, so this reverts to the pre-breakage behavior.",2
[tests] add utility to replace direct call to pytest.main (#11393),1
"[Bugfix][TIR] Removed passing of IterMapExpr into PrettyPrint (#11412)Follow-up from https://github.com/apache/tvm/pull/11235, all errormessages should be based on expressions that are not IterMapExpr.",0
[Topi][Relay] Support for FP16 ERF on CPU. (#11413)* Functionality and tests implemented* Formatting and lint.* Typo fix.* Reduce strictness for fp16 tests.Co-authored-by: Ubuntu <ubuntu@ip-172-31-53-187.us-west-2.compute.internal>,0
"[TIR] Regression test for PrettyPrint/IterMapExpr bugfix (#11418)Follow-up from https://github.com/apache/tvm/pull/11412, adding aregression test for the bugfix.",0
Fix typo in typing of space generator (#11424),0
[Hexagon] Use HEXAGON_SDK_ROOT in gtest path (#11421),3
"[Arith] Allow unused trivial iterators in bijective check (#11425)This PR extends `DetectIterMap` bijective check to allow unused zero-constant iterators (trivial iterators). For example, `i, j, k => i, j` are treated as bijective if `k \in [0, 1)`, even though `k` is not used in the mapping result.Previously, when trivial iterators are simplified, the above iter map can pass bijective check. When `simplify_trivial_iterators==False`, `k` will not be simplified and `DetectIterMap` will fail if `require_bijective` is set. This PR make the behavior of `DetectIterMap` consistent with different setting of `simplify_trivial_iteraotor`.Regression tests for `reverse_compute_inline` is also added.",1
"[ci] Add GitHub Actions bot to merge PRs on demand (#10833)This implements https://discuss.tvm.apache.org/t/rfc-allow-merging-via-pr-comments/12220. The bot can be invoked from a top-level review comment or via a regular PR comment. The text `@tvm-bot merge` anywhere in the body will trigger the bot. Right now it checks that the latest commit is reviewed and that all CI jobs that have run on that commit are successful. If it fails, it will leave a comment on the PR with the reason.This is just a start and some features are left for followups:* Various TODOs throughout the code* ""Scheduled"" merges that happen once CI finishes* Allowing committers to merge without getting a fresh review for changes after an approval",1
[ci] Add more shards (#11402)This adds a bunch more CPU shards and moves everything to CPU-SMALL.Some Java limitations required splitting up the logic in the templates abit as well.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[Arith][BoundDeducer] Forbid non-supported expr type in bound deducer (#11323),5
[ci] Add -x to all CI scriptsFixes #10316cc @Mousius @areusch,0
Fix type checking annotation for Union type (#11430)* Fix type checking annotation for Union type* Update _type_checker.py,0
[skip ci][ci][AutoScheduler] Disable flaky test_mutate_parallel test (#11441)See #11440Co-authored-by: driazati <driazati@users.noreply.github.com>,3
unify ssize_t definition (#11384)remove tvm_ssize_t type and unify the definition of ssize_t in Windows buildCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,4
[skip ci][ci][paddle] Disable flaky test_forward_group_norm (#11436)See #11435Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[Android] Update gradle version and other changes in android apps, CI modification to auto-build Android apps and upload artifacts (#11241)* Update gradle version in android_rpc app* Support latest gradle, bump versions, replace ndk build script with gradle tasks* [android_rpc] Fix linter errors, disable weird ones* [android_deploy] Support latest gradle, bump versions, fix linter errors, disable some of them* [android_camera] Support latest gradle, bump versions, rewrite readme* [android_camera] Fix linter errors* Fix sanity check errors* Add Android jobs for Github Actions* Add python requirements for TVM and android_camera, use preinstalled NDK* Revert to build with make* Add minrpc include (PR #11232)* Remove relative paths",0
avoid loop dependent allocation in buffer compaction (#11428),5
[skip ci][ci] Fix broken test skips (#11456),0
[Hexagon] Rewrite AllocateNodes with global.vtcm scope after FlattenBuffer (#11429)* Add test to ensure AllocateNodes for buffers with global.vtcmare rewritten.* Move test_cache_read_write.py out of topi testing directory.,1
[ci] Use smaller ARM nodes for build/test (#11445)This applies the new instances from https://github.com/tlc-pack/ci-terraform/pull/32Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[TIR][Schedule] Transform layout quality of life (#11269)* [TIR][Schedule] Added Schedule.transform_layout_sugared* [TE][TIR] Reduced duplication in TE/TIR layout transformationsPreviously, the implementations of `tir.IndexMap.from_func` and`te.Stage.transform_layout` had significant duplication to handleargument parsing.  This commit extracts the shared logic into`tir.IndexMap`.* Enabled *args in Schedule.transform_layout_sugared* Fix lint error* Allow Schedule.transform_layout_sugared to set axis separators* Merged transform_layout_sugared functionality into transform_layout* Fix lint errors* Fix lint error* Fixed docstring errors* Updated/tested TransformatLayoutTraits::UnpackedAsPython* Disabled exec-used check for running trace.as_python()* Updated SetAxisSeparatorTraits::UnpackedAsPython* Updated unit test that was added in merge commit* Fixed the argument name for TensorizeTraitsThis wasn't checked before, but was the only other issue caught by theupdates to verify_trace_roundtrip.* Re-enable type checks of transform_layout/set_axis_separatorDisabled while waiting for https://github.com/apache/tvm/pull/11289,which was required for the `Tuple` argument.* Updated a few additional transform_layout usages from main",0
[rust][ci] Disable rust nn tests (#11420)See #11419Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[BYOC] Enable bfloat16 in DNNL BYOC (#11111)* refine the code style (#10112)* support more data types in oneDNN BYOC* consider dtype when query layout* support more translation of blocked layout* refine log for invalid layout transform* reset N and C for the weights* support multi-blocking in TransDims2Plain()* add tests for bf16 oneDNN BYOC* unregister 'round' OP in oneDNN BYOC* restore the criteria for fp32 tests* disable test_prune_dnnl_subgraph for bf16* fix typo in dnnl.py* delete tag::format_tag_last* delete 'is_weight' in layout2tag()* reuse dtype_dl2dnnl()* fix lint errors* change to WARNING for invalid laytout transform* skip bf16 tests if AVX512 is unavailable,0
"[microNPU] Expose compute cycle annotations to TIR lowering (#11288)* [microNPU] Expose compute cycle annotations to TIR loweringAdds an AttrSttmt ""compute_cycles_hint"" to each NPU operation for laterpasses to consume.Change-Id: I09779bdab6de6ef2094db610bb20d6e052e68ee3* compute_cycles->compute_cycles_hintChange-Id: Iebd71e699522e92a28fd321ffdb41ed7924db4e0* add test to check annotations in compilation flowChange-Id: Idcdcc8c8b5536c4732f297246b71aa8378a2732c* add compute cycles hints for copy operationsChange-Id: I007ba19732e16081fa2ea9baca40c64a653c93cf* fixing annotations for copies and improving test coverageChange-Id: Ib812c4151fab03f4c1adcc016b4e798003a22e5e* rebaseChange-Id: I653101908706096ae25ad1ebf08e7b6c4f1196c7",0
[hexagon][testing] refactor benchmark-table code (#11400)Generalize the benchmark-table code to support arbitraryindependent values. This supports future changes to the benchmarkcode.,2
"[Runtime] Add 'static_library' runtime::Module (#11442)(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).This adds a new 'DSO exportable' runtime module representing the contents of a .o file. Itallows external codegen toolchains to yield a result which: - Like CSource modules, can be conveyed directly to the final export_library compilation   step for linking into the final .so and saved to a know location without risk the   underlying code artifact will be lost. - Like DSOLibrary modules, are self contained so that no additional compile-time arguments   need be conveyed from the CSource module to the final export_library command lineSince this is the third flavor of 'DSO exportable' module, add a Module::IsDSOExportable.Since adding the above, can't resist also adding a Module::ImplementsFunction virtual andcalling it from TEComplier to check if an external codegen function actually provided theimplementation it promised.Note: - I've left the existing implementation of runtime.load_module alone which   relinks .o files to .so files. - Though also contained in the .o metadata, I require static libraries to always   carry their list of exported function names.This is all pretty stop gap pending a good rework of TVM to supoprt the notion of artifactsand, perhaps, build rules.",1
"[TIR] Additional Stmt/Expr simplication rules (#11373)* [TIR] Additional Stmt/Expr simplication rules- Enabled simplification of `A[i] = A[i] + 0` into no-op.  This was a  bug introduced in https://github.com/apache/tvm/pull/9727, which  applied this rewrite only to `A[i] = A[i]`, and not to statements  which simplify to `A[i] = A[i]`.  Regression test added to prevent  reoccurrence of this bug.- Enabled simplification of `x - x` to zero for floating point types.  Previously, this simplification was applied only for data types that  could be used as buffer indices.* Updated to maintain separate int/float simplification paths* Updated to use tvm.testing.main* Remove duplicate rewrite rules",0
[ONNX] Add MeanVarianceNormalization op (#11444)* [ONNX] Add MeanVarianceNormalization op* Add pytest.main([__file__]),1
Minimal example of tuning on hexagon. Fails in fast rpcs currently. (#11395),5
[ci] Clean up mergebot commit messages (#11437)* [ci] Clean up mergebot commit messagesAdds both bullets and closes #11433* Fix error from pr #11442Co-authored-by: driazati <driazati@users.noreply.github.com>,0
correct doc (#11439),5
[RUST] Add conv3d transpose Rust bindings (#11471)* Add conv3d transpose Rust bindings* Fix typename* Add base,0
[skip ci][ci] Disable `test_solution_consistency` (#11460)See #11458Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[VM] Memory alignment check for `set_input` in Virtual Machine (#11391)* add memory alignment check* add accounting of byte_offset* transfer NDArray generation method to NDArray class instead of VM* describe conditions. check IsContiguous for external DLTensor* hide safeless method in private* fix lint* fix lint* check conditions for correct creation of NDArray from external DLTensor* lint fix* update API after review* empty commit. restart CI tests* empty commit. restart CI tests once moreCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
Add unidirectional sequence lstm (#11183)* UnidirectionalLSTM added* fixed missing import* fixed pylint warnings* black formatted tflite.py* corrections according to reviewer comments* fixed black formatting* just to trigger the CI again* assertion now tests that there are exactly 24 input tensors.* black formatted tflite.py* added explanatory comment regarding unused imports* removed unused import* nothing* nothing* added some details in a comment about the differences in unbind regarding to the version in common.py* improved comment on unbind* fix of black issue,0
fixed tuple error (#10216)Co-authored-by: suhail <suhail@expedera.com>,0
"[CUDA] Allow dynamic shmem of size > 48K in runtime (#11478)Currently, we have functioning dynamic shared memory support on cuda. But we haven't actually explored allocating more than 48KB of dynamic shmem. This PR updates the cuda runtime to support launching a kernel which wants to use dyn shmem of size > 48KB. This is already useful for manually rewritten schedules, but to integrate this feature into tuning requires more work (see the discussion on `VerifyGPUCode` below). I'll add a test which actually uses a big dyn shmem in the next PR (need to fix one bug in software pipelining transform). Reference in cutlass code:https://github.com/NVIDIA/cutlass/blob/master/include/cutlass/gemm/device/gemm.h#L479-L482",0
Fix structural error reporting on root block (#11477),0
"[FFI][CYTHON] Release GIL when calling into long running functions (#11461)Unlike ctypes, Cython by default do not release GIL whencalling into C API functions. This causes problems when thefunction is long running. As the particular calling thread willblock other python threads by holding the GIL.This PR explicitly releases GIL when calling into possiblelong running functions. It fixes the timeout issue inPopenPool which previously relied on another python threadfor timeout.Added a regression test-case by changing sleep to sleepin FFI, which previously will indefinitely block the popen tests.",0
[skip ci][ci][docker] Prune all non-relevant images (#11491)Before this would leave around any image that could be used in CI. ThisPR changes it so that the `docker rmi` knows exactly which image isbeing used in CI so all others (even those that are being used in thesame build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on andshould help keep disk usage down.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[skip ci] Revert ""[skip ci][ci][docker] Prune all non-relevant images (#11491)"" (#11496)",5
[ci] Use smaller ARM nodes for build/test (#11445) (#11457)This applies the new instances from https://github.com/tlc-pack/ci-terraform/pull/32Co-authored-by: driazati <driazati@users.noreply.github.com>Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[Meta Schedule] Fix testing issues for models with more than one inputs (#11298),0
Silence unnecessary 'host' deprecation warnings (#11499),5
"[Software pipeline] Fix hardcoded index in `access_ptr` rewriting, add a GPU test with depth 4 (#11495)* fixed hard-coded index in software pipeling* fixed three-stage pipeline test* add three stage pipelined gemm test* refactor mma test* use mma_4k schedule utility in test* apply pipeling annotation* black* require ampere in test",0
[FIX] Add braces to if-else statements (#11493)Some if-else statements were missing braces. They have been added as perour style guide.,0
"[Pass] Add MaxPool, AvgPool to FoldExplicitPadding (#11494)* fold first steps* spitballing* check pad is really optd away* new pool test passes* stuff* refactoring midway* things actually kinda work* complete tests* lint and complete tests* clean* fix comments",0
"[OpenCL] Avoid SelectNode ambiguous overloading (#11488)* [OpenCL] Avoid SelectNode ambiguous overloading* Revert ""[OpenCL] Avoid SelectNode ambiguous overloading""This reverts commit 60f68d2e7f750a0f8e62536da7b3327d1f5f29c1.* [OpenCL] Avoid SelectNode ambiguous codegen",5
[TIR] Add schedule primitive TransformBlockLayout (#11485)* [TIR] Add schedule primitive TransformBlockLayout* fixup! [TIR] Add schedule primitive TransformBlockLayoutFix doc,0
"[TVMScript] Allow T.Buffer[] arg annotation to use int as shape (#11454)* [TVMScript] Allow T.Buffer[] arg annotation to use int as shapeBoth the function `tvm.tir.decl_buffer` and the TVMScript`T.match_buffer` expression allow a `PrimExpr` to be passed as the buffershape, which is interpreted as a 1-d buffer of that size.  This allowsthe same behavior to be used in the `T.Buffer` syntactic sugar.(e.g. `A: T.Buffer[16, ""float32""]` instead of `A: T.Buffer[(16,), ""float32""`)* Fixed round-trip when buffer size contains an expression",0
[Pass] Add utility that asserts that IRModule is not mutated in a pass. (#11498),1
Canonicalize type annotation during construction of Var and SizeVar (#11443)* Canonicalize type annotation during construction of Var and SizeVar* Update tests/cpp/expr_test.ccCo-authored-by: Junru Shao <junrushao1994@gmail.com>* lint* fixCo-authored-by: Junru Shao <junrushao1994@gmail.com>,0
[microNPU] add E2E tests with cascader wo striping (#11410)This commit adds end-to-end tests using the cascaderw/o striping. It needed few adjustments to the orderin which the arugments are provided to the entry pointfunction in AoT when both memory pools and devicesare present.Change-Id: I37e04afd635add895e317586f628a62cae75f3fa,1
[Frontend][PyTorch][Bugfix] Ignore Cuda in PyTorch version number when comparing versions (#11511)* Do not consider cuda in the PT version number* Add docstring,0
[Frontend] [PaddlePaddle] group_norm adjusts test accuracy (#11450)* group_norm adjusts the check accuracy* remove test skip,1
"[ci][docker] Prune all non-relevant images (#11497)* [skip ci][ci][docker] Prune all non-relevant images (#11491)Before this would leave around any image that could be used in CI. ThisPR changes it so that the `docker rmi` knows exactly which image isbeing used in CI so all others (even those that are being used in thesame build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on andshould help keep disk usage down.Co-authored-by: driazati <driazati@users.noreply.github.com>* [skip ci] Revert ""[skip ci][ci][docker] Prune all non-relevant images (#11491)"" (#11496)* [ci][docker] Prune all non-relevant images(this is a re-do of #11491)Before this would leave around any image that could be used in CI. This PR changes it so that the `docker rmi` knows exactly which image is being used in CI so all others (even those that are being used in the same build but not currently on that node) are deletedThis also adds some more logging so we can see what's going on and should help keep disk usage down. Skipped CI since this runs during lint.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
"[Arith] Merge surjective/non-surjective iter mapping detections (#11287)* simplify (x * 96) % 64 to (x * 32) % 64* adapt merge mulmod opt for OffsetOf computation* merge DetectIterMap and DetectIterMapPadded* adjust related interfaces for IterMapLevel* - check incompatible left paddings- determine case like x % 16, x in [0, 5) to be non-surjective, since usages may treat the region extent as 16 by mistake.- skip second round of rewrite when there is no padding- fix some typo in comments* rebase upstream",0
[microNPU] Fix flaky compute cycle annotation test (#11510)Fixes non-deterministic test by disabling striping when runningthe cascader.Change-Id: Ib44f299f21fa0b41be4bfac3deb61a9c16818c58,0
[microTVM][ARM][Zephyr] Add CMSIS dependencies in Zephyr project build (#11362)* Test with CMSIS build addeddisabled conv2d_nhwc_dsp.arm_cpu for non integers workloadsadded debugging feature to TempDirectory* revert arm_cpu strategy changes* Address Andrew comments* change copy to include* add cmsis_path only as project option,0
[MetaSchedule] Enable Task Filtering (#11512)This PR allows `relay.backend.MetaScheduleExtractTask` to take an extra argument `filter_func` which filters out tasks that don't need tuning. The counterpart of AutoScheduler is `traverse_to_get_io_tensors`.,5
[BugFix] Add lock for ModuleNode::GetFuncFromEnv (#11467)* [BugFix] Add lock for ModuleNode::GetFuncFromEnv* [BugFix] Add lock for ModuleNode::GetFuncFromEnv,0
[microNPU] Add transform matrices and part matcher to identity op (#11453)* [microNPU] Add transform matrices and part matcher to identity op* Address comments* Enable cascader in identity tests* Address comments,1
"[microTVM][ARM]Add tests for arm schedules (#11472)* add more tests for arm_cpu schedulesconv1d_ncw, conv1d_nwc, conv2d_NCHWc, depthwise_conv2d_NCHWc, dense_dsp, avg_ pool and max_pool tests are added.Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>",1
"[Relay] Plumb external codegen target via Target.current() (#11432)* [Relay] Plumb external codegen target via Target.current() for all external codegen paths(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).We want both old-style (via relay.ext.$toolchain) and new-style (via ""RelayToTIR"" Passattribute on target kind) external codegen to be able to access the current 'external codegen'Target instance via Target.current(). - For old-style, plumb the true Target through TEComplier and push it on the context   stack before calling relay.ext.$toolchain. - For new-style, pass the CompilationConfig to the RelayToTIRTargetHook pass, make the jump from   ""Compiler"" attribute value to Target via the new CompilationConfig::FindPrimitiveTargetForKind   method, and push on the stack before invoking the custom ""RelayToTIR"" pass.While working on this discovered RelayToTIRTargetHook was incompatible with the VM's compilationflow since RelayToTIRTargetHook assumes all ""Compiler"" attributed functions are inlined. Generalizeit to support both inline and global function styles.Extend Target::IsExternalCodegen to recognize target kinds with ""RelayToTIR"" attributes asexternal.Update target hooks unit test to exercise new support for outline-style, picking up the current target,and compiling via the VM.* - A bit of polishing en passant.* - Add comment as per Josh's suggestionCan't repro tests/python/contrib/test_ethosu/cascader/test_scheduler.py::test_compute_cycles_annotation failure, flake?",1
[VM] check DLManagedTensor for conditions to construct NDArray (#11504)* check DLManagedTensor for contiguous and alignment to construct correct NDArray* correction from the reviewer* update error description for incontiguous DLTensors* small updateCo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,0
[skip ci][ci][docs] Add CI infra docs (#11403)* [skip ci][ci][docs] Add CI infra docsThis adds some documentation around CI infra and pointers to the guides to run a deploy.* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,1
[ci] Add conditionals for non-Python tests (#11438)These don't get sharded in any way so there's no point in running them multiple times.cc Mousius areusch,1
[TE] Optimized version of concatenation layer (#11341)* [TE] Optimized version of concatenation layer     1. Concat implemented using extern_op     2. New tests added.     3. Workaround to allow inline extern_op-s with other layers.* *test fix* test_any.py fix.* test_forward.py from tensorflow fix.* lint fix.* Fixes after code review.* New comment added.* Lint fix.* Another lint fix.* Comments added.* rebase issue fix.* Restored previous state.* Update after code review.* After code review changes.* lint review.* Change strategy for cuda to fix tests.* Rebase to main* Comments changes after review.* Some more comments fixes.* One more error fix in comments.* restart build,0
[COMMUNITY] driazati -> Committer (#11525),3
[ci] Add filter to teams (#11455)This improves the parsing to avoid issues like in #11454commit-id:53a06ab3Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[TE] Fix `te.CreatePrimFunc` for 0-dim computation (#11518)For 0-dimensional computation, `te.CreatePrimFunc` creates an opaque block with 0 block iters,which is mistakenly passed into TVMScript auto-completion that failed to add the root block properly.As an example,```python>> from tvm import te>> a = te.placeholder((), name=""a"", dtype=""int32"")>> b = te.placeholder((), name=""b"", dtype=""int32"")>> c = te.compute(a.shape, lambda *i: a(*i) + b(*i), name=""c"")>> f = te.create_prim_func([a, b, c])>> print(f.body.block.reads)[a[], b[]]>> print(f.body.block.writes)[c[]]```This PR fixes this issue by enforcing the consistency that `te.CreatePrimFunc`always creates scheduleable blocks with at least 1 block iter:```python@T.prim_funcdef func(a: T.Buffer[(), ""int32""], b: T.Buffer[(), ""int32""], c: T.Buffer[(), ""int32""]) -> None:    # function attr dict    T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})    # body    # with T.block(""root"")    with T.block(""c""):        vi = T.axis.spatial(1, 0)        T.reads(a[()], b[()])        T.writes(c[()])        c[()] = a[()] + b[()]```",0
Add ceil shape registration (#11533),1
"[Bugfix][TIR] Handle bool tensor in FlattenBuffer (#11532)This PR fixes an existing bug in TIR lowering where the TIR below triggers an error:```python@T.prim_funcdef func(a: T.Buffer[10, ""bool""], b: T.Buffer[10, ""bool""]) -> None:    T.func_attr({""global_symbol"": ""main"", ""tir.noalias"": True})    for i in T.serial(10):        with T.block(""b""):            vi = T.axis.spatial(10, i)            b[vi] = a[vi]tvm.build(func, target=""llvm"")```The error message is:```  File ""/root/Projects/tvm-dev/src/tir/transforms/flatten_buffer.cc"", line 173TVMError:---------------------------------------------------------------An error occurred during the execution of TVM.For more information, please see: https://tvm.apache.org/docs/errors.html---------------------------------------------------------------Check failed: store->buffer->dtype == DataType::Int(8) (bool vs. int8) : Expected int8 backing arrayfor boolean tensor```This PR fixes this behavior.",0
[DNNL] Add TensorRequisite concept (#11345)Allow to use DNNL runtime in multi instance mode.Thread safe execution of Run() method.Signed-off-by: Alexander Peskov <peskovnn@gmail.com>,1
[Frontend][ONNX] Fix softmax converter when input shape is dynamic (#11507)* [Frontend][ONNX] Fix softmax converter when input shape is dynamic* [Frontend][ONNX] mark dynamic softmax tests as xfailed with cuda,0
[Onnx] Round operator (#11446)* banker round op added based off tutorial* black'd onnx.py file* retriggering CI with empty commit due to autoscheduler test failure* removed youtube link in comments* retriggering CI due to test failure that passed locally,1
[MetaSchedule] No explicit for spatial PrimFunc (#11534),5
[ci][wip] Upload docs with folder structure to S3 (#11528)Keeping the files as-is lets us serve them from S3 + CloudFrontCo-authored-by: driazati <driazati@users.noreply.github.com>,2
Restore integration test on Mac and Windows (#11538)Signed-off-by: Alexander Peskov <peskovnn@gmail.com>,3
"[ci] Add @tvm-bot rerun (#11480)This adds a command to restart CI runs that have stopped (either from afailure, success, or abort) via GitHub comments addressed to tvm-bot:```@tvm-bot rerun```tvm-bot will then comment on the thread and send a request to Jenkins torestart CI. This does not restart GitHub Actions jobs though we may beable to add that in the future.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
"[TIR][Arith] Additional Simplifications Inside Conditionals (#11524)* [TIR][Arith] Use equality constraints in analyzerPreviously, constraints with inequalities were recognized and used forsimplifications by `ConstIntBoundAnalyzer` and `ModularSetAnalyzer`,but constraints with equalities were not.  This adds equality-basedconstraints.  (e.g. Inside the then-case of `if i==5`, the value of`i` is known to be 5.)* [TIR][Arith] RewriteSimplifier, apply literal constraintsPreviously, constraints were only checked within a `tir.likely`annotation.  After this change, constraints are used forsimplification of all boolean expressions.  (e.g. Within a conditional`if i==n`, the expression `(i==n) and (j==m)` can be simplified to`j==m`.)* [TIR][Arith] Do not apply literal constraints to BufferLoadIf a literal constraint relies on the contents of a buffer, theconstraint may not be assumed to hold.  This prevents the incorrectrewriting of `A[i]==n` to true within a `if A[i]==n` conditional, asthe value of `A[i]` may have changed.* [TIR][Arith] Use each independent constraints in RewriteSimplifierInside a constraint `if i==n and j==m`, both `i==n` and `j==m` may bereplaced with true, even in separate expressions.This commit uses a new internal utility function`tvm::arith::ExtractConstraints`, which breaks up a boolean expressioninto a list of true statements.  This may be used to reduceduplication elsewhere, such as `const_int_bound.cc` and`iter_affine_map.cc`.* [TIR][Arith] Check for negation of literal constraintsWhen inside a conditional of `i!=n`, in addition to the previousreplacement of `i!=n` with true, we can also replace `i==n` withfalse.* [TIR][Arith] Added unittests for new simplifications* Fix lint error* Fixed handling of negation of non-boolean types* Removed extra asterisk",0
[TIR] Add schedule primitive ReIndex (#11515),1
[PROFILER] Fix percent compute bound calculation (#11542)* [PROFILER] Fix percent compute bound calculationSomehow the runtime was dropped from the percent compute boundcalculation. Tolerances on the test we bumped a little bit higher to tryand catch mistakes like this in the future.* forgot print,0
Fix docker/lint.sh after #10933. (#11541),0
[FIX] Pad feature vectors to the same size in xgboost cost model (#11479)* [FIX] Pad feature vectors to the same size in xgboost cost model* add test* more test* explaination* formatting,0
"Unbreak CI image build (tensorflow 2.6.5, ci_gpu bugfix) (#11546)* Pin protobuf to 3.20.1 due to #11545.* Unpin and instead update to 2.6.5* attempt to fix gpu build* Revert to 2.6.3, pin protobuf for ci-arm.* escape bash char",0
"[hexagon][testing] add TIRScript elemwise-add (#11490)Replace TE-based elementwise-add benchmark witha TVMScript-based one.Update Hexagon target architecture from v68 to v69.As a result, the benchmark now requires a version ofHexagon SDK newer than 4.4.0.1.  Version 4.5.0.3 isknown to work.",1
"[ci] Fix action expressions for tvm-bot workflow (#11556)These weren't caught by `actionlint` for some reason but GitHub doesn't merge multiple `if`s, so this combines them into one.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
"[BYOC] Two helper passes for external codegen using RelayToTIR custom pass machinery (#11474)* [BYOC] Two helper passes for external codegen using RelayToTIR custom pass machinery(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).For reasons explained in the above thread I'm moving CUTLASS to be IRModule-at-a-time external codegenusing a custom RelayToTIR pass instead of the traditional function-at-a-time external codegen usinga relay.ext.cutlass registered function. This means some of the rewriing done on-the-fly by LowerTEPass nowneeds to be done by the custom pass directly. This PR supplies two passes which ease that burden: - Before starting the CUTLASS-specific processing, make sure all ""Compiler"" attributed functions have   unique global definitions (ie are outlined). Though functions start in this form after BYOC partitioning,   under Graph and AOT compilation flows those functions are then inlined to pass through the 'codegen' keyhole   which assumes the whole model is just one self-contained main function. This pass will undo that. (I gave up   trying to just remove the inlining in the first place.) - After the CUTLASS-specific processing the now compiled ""Compiler"" attributed functions need to marked as   'extern'. The te_compiler.cc uses the ""ExternalSymbol"" attribute for that, but since a) the symbol name   is never needed, on the presense of the attribute is significant downstream and b) ""ExternalSymbol"" is   easy to confuse with ""global_symbol"", I just replaced ""ExternalSymbol"" with ""Extern"" with an Integer(1)   (cf ""Primitive""). The outlining pass is a little more general than necessary because it (will also) be used by Collage to rewrite the IRModule into optimally partitioned form while making maximal reuse of partition functions. Hence the abstract GlobalSymbolCache.* - Andrew's comments",2
[Hexagon] Register strategy for concatenate (#11562)* [Hexagon] Register strategy for concatenate* Restart CI,5
[CI] Update to LLVM 14.0.0 for ci_hexagon (#11539),1
"[CI] Refactor of tvm.testing.requires_* annotations (#11313)* [CI] Improved skip messages when using @tvm.testing.requires_*Previously, the same message was given regardless of why a testcouldn't be run.  This has been split up into separate checks for TVMcmake options in `config.cmake`, enabled targets in `TVM_TEST_TARGETS`environment variable, and checks for available hardware.* Refactor to specify repeated feature marks, compile-only markers* Fixed lint errors* Import from contrib, not from a different import* Removed use of requires_llvm() as a list of marks* Corrected mark from requires_gpu to requires_cuda* Adding missing ""not""* Added USE_CMSISNN as a requirement for corstone300.",0
[TIR] Expose tir.call_cpacked in python (#11563),5
Fix Hexagon build using ci.py (#11304)* Add output directoryadd post build for hexagonfix -net=host for docker* remove --net by default,0
"[docs] microTVM model training tutorial with Colab support (#10921)* First draft of micro train tutorial* unit test code* Fix obvious formatting issues* Linting* Proof of concept showing that ""Open in Colab"" is possible* Make test Python script more readable* Fix formatting* Ready for review* Import pyserial only when neededChanges from code reviewUse official sphinx-gallery repoCorrectly specify versionImport pyserial only when necessary* Add warning to ignored listTry to avoid throwing warningFix linting, try verbosity filterTry adding to ignore fileRemove fix attempts* Grammar fixes* Address code review commentsInclude full git hashes* Rerun tests* Rerun again",0
[Bugfix][MetaSchedule] Auto-bind when there are no spatial loops (#11570),0
"[TIR] Schedule Primitive: Add-Unit-Loop (#11575)In TE, a unit loop could be introduced by fusing an empty list of loops on a stage. This PR adds its counterpart in TIR, while being a bit more explicit with a new schedule primitive which adds a unit loop without impacting any existing functionalities.",1
"[MetaSchedule] Use Add-Unit-Loop in Auto-Bind (#11581)Following #11575, this PR allows CUDA thread binding for TIR programslike```python@T.prim_funcdef zero_dim_add(    A: T.Buffer[(), ""float32""],    B: T.Buffer[(), ""float32""],    C: T.Buffer[(), ""float32""],) -> None:    with T.block(""C""):        vi = T.axis.spatial(1, 0)        C[()] = A[()] + B[()]```where there is no loop available to be bound to threadIdx/blockIdx.",1
"[TIR] Prevent loop binding over-simplification (#11578)@vinx13 @jinhongyii and I observe a recent regression on TVM mainline: over-simplification in`Schedule.split` leads to information loss that negatively impacts search space generation.**Impact.** This affects common operators like `softmax` and even simpler reductions.**Example.** Consider splitting a simple reduction loop:```python@T.prim_funcdef main(    A: T.Buffer[2, ""float32""],    B: T.Buffer[2, ""float32""],    C: T.Buffer[(), ""float32""],) -> None:    for i in T.serial(2):  # <= split `i` into `i_0` and `i_1`, where `i_0` is a trivial loop        with T.block(""C""):            k = T.axis.reduce(2, i)            with T.init():                C[()] = T.float32(1)            C[()] = T.min(C[()], A[k] / B[k])```Splitting loop `i`  by factors `[1, 2]`, we get:```python@T.prim_funcdef main(    A: T.Buffer[2, ""float32""],    B: T.Buffer[2, ""float32""],    C: T.Buffer[(), ""float32""],) -> None:    for i_0, i_1 in T.grid(1, 2):        with T.block(""C""):            k = T.axis.reduce(2, i_1)  # <= i_0 is not part of the binding,                                       # so the system cannot tell if i_0 is a reduction loop            with T.init():                C[()] = T.float32(1)            C[()] = T.min(C[()], A[k] / B[k])```In this case, loop `i_0` will be considered as a spatial loop, even it’s the outcome of splittinga reduction loop. However, if we change the factors from `[1, 2]` to `[2, 1]`, loop `i_0` becomesa reduction loop. This means the loop iteration property depends on the loop extent.**Why is it problematic**? MetaSchedule has an assumption: extremely seldomly, a loop extent wouldimpact the iteration property of the loop itself, i.e. no matter the extent is 1 or 2 or anything,the fact that the loop is a reduction loop should rarely change.As an example, `Auto-Bind` finds the outer `k` spatial loops, which are fused together and bound tothread axis. In the trace, the number (`k`) of the outer loops has to be a constant.However, if Auto-Bind thinks there are `k=3` outer loops to fuse during search space generation,where the last loop happens to be a reduction loop with extent 1, as shown below:```pythonfor spatial_loop_0 in range(...):  for spatial_loop_1 in range(...):    for reduction_loop in range(1):  # <= Auto-Bind mistakes this loop as spatial, because extent==1```During evolutionary search, the extent of reduction_loop will change and become larger than 1.In this case, the binding strategy will consistently fail because it considers fusing `k=3` loops- which means the entire search strategy will fail with almost no valid candidates.Thanks @MasterJH5574 for figuring out the root cause of the issue,and @jinhongyii for valuable pointers to the right fix!",0
"[Bugfix][TIR] compute-at/fuse/split dtype mismatch (#11582)The schedule primitives, including compute-at, fuse and split usuallygenerate loop variables with `dtype=int32` as default. However, in somemodels, there are usecases where int64 are part of tensor shapes, whichleads to unexpected behavior in scheduling. This PR brings the fix toexisting known issues.",0
"[MetaSchedule] exposed method: TuneContextNodeInitialize (#11576)I exposed the initialize() method for TuneContextNode on the C++ side and added a corresponding method to TuneContext class on the Python side, so that we do not need to call initialize_with_tune_context for every scheduling rule.",1
"[MetaSchedule] Fix Summary Format for Invalid Runs (#11584)Previously for invalid tasks, MetaSchedule prints a huge number inlatency which is aesthetically unacceptable. For example,``` 69 |  fused_cast_add_cast_3 | 16777216 | 2 | 0.0000 | 10000000000000000019156750857346687362159551272651920111528035145993793242039887559612361451081803235328.0000 | 20000000000000000038313501714693374724319102545303840223056070291987586484079775119224722902163606470656.0000 |     64 |```This PR fixes this behavior and turns the huge number into ""N/A"".",0
[CI][DOC] Fix incorrect commands in docs/readme.md (#11583)Fix incorrect commands in docs/readme.md,0
split test_forward_math_api function (#11537),3
fix bmm quantization realize (#11586),0
"[microNPU] Fix output mismatch in Leaky ReLU (#11397)* [microNPU] Fix output mismatch in Leaky ReLUAll codegen tests have been running with a representative datasetbetween 0,1 which masked an output mismatch in Leaky ReLU when comparedto TFLite kernels. This issue can be replicated by replacing therepresentative dataset range with something like -1,1.To fix this mismatch, we use the same implementation for calculatingLUT values as Vela which uses arithmetic constrained to quantizedvalues, rather than the previously used floating point calculations.Change-Id: I0ed52215acd27722873be609271971b6fc4aaef1* fix lintChange-Id: Ica7de0c000ee015e79fe10985b2ec7a9b341861f* fix lint againChange-Id: I005d90ad248bfff7090f99d161eefbdc962cba48",0
"[microNPU] Optimize separate padding operation for conv2d (#11468)Optimizes a case where padding appears as a separate nn.pad operation followed by a qnn.conv2d. If possible, the nn.pad will be partitioned and offloaded together with the qnn.conv2d operation, as opposed to separately. As a fallback, both operations will be considered separately.cc Mousius NicolaLancellotti ekalda manupa-arm",1
[PROFILER] Add configuration information to profiler (#11530)Configuration is a place to store extra information related to thespecific profiler run. Right now it is just the executor used and thenumber of threads. The roofline analysis also adds peak flops and peakbandwidth.,1
"[MetaSchedule] Evo Independence from TaskScheduler (#11590)Per discussion with @Kathryn-cat, we realized that the current APIdesign could be verbose if we only want to tune a single task, in whichcase a dummy task scheduler still needs to be established to supply`EvolutionarySearch` with proper `CostModel` and `Database`. This PRfixes this UX issue.",0
Refactor RewriteTensorize to prevent concurrent map updates (#11596),1
fix uint case (#11597),0
[TOPI] TE implementation of LSTM using scan (#11531)* TE implementation of LSTM in TOPI* docstring* lint* add injective tags where applicable,1
"[MetaSchedule] Add Testing Script with ONNX Support (#11587)This PR introduces 2 tuning script for meta schedule and auto scheduler tuning support with onnx files. Now we can easily introduce onnx models benchmarking with command line scripts. Sample tuning call looks similar to the following scriptFor Meta Schedule ONNX tuning:```python3 -m tvm.meta_schedule.testing.tune_onnx_meta_schedule \    --model-name   ""$MODEL_NAME""                             \    --onnx-path    ""$ONNX_PATH""                              \    --input-shape  ""$INPUT_SHAPE""                            \    --target       ""$TARGET""                                 \    --num-trials   $NUM_TRIALS                               \    --rpc-host     $RPC_HOST                                 \    --rpc-port     $RPC_PORT                                 \    --rpc-key      $RPC_KEY                                  \    --rpc-workers  $RPC_WORKERS                              \    --work-dir     $WORK_DIR                                 \    |& tee         ""$WORK_DIR/$MODEL_NAME.log""```For AutoScheduler ONNX tuning:```python3 -m tvm.meta_schedule.testing.tune_onnx_auto_scheduler \    --model-name   ""$MODEL_NAME""                              \    --onnx-path    ""$ONNX_PATH""                               \    --input-shape  ""$INPUT_SHAPE""                             \    --target       ""$TARGET""                                  \    --num-trials   $NUM_TRIALS                                \    --rpc-host     $RPC_HOST                                  \    --rpc-port     $RPC_PORT                                  \    --rpc-key      $RPC_KEY                                   \    --rpc-workers  $RPC_WORKERS                               \    --log-dir      $WORK_DIR                                  \    |& tee         ""$WORK_DIR/$MODEL_NAME.log""```",1
"[MetaSchedule] Resolve dependencies between header files (#11604)* [MetaSchedule] Resolve dependencies between header filesAfter PR11590 TVM stopped compiling with clang-14 and libc++. The problemswere caused by incomplete types used in contexts where complete types wererequired. To resolve this, some code had to be moved into .cc files. Alsothe MeasureCandidate classes needed to be added to their own include files(or otherwise there would be a circular dependency between headers).All headers from the meta_schedule directory were updated to include alltheir dependencies (forward declarations were left where appropriate).* Fix a typo: PySpaceGeneratorCode -> PySpaceGeneratorNode",0
"[Relay] IndexedGraph improvements in preparation for Collage (#11481)* [Relay] Odd's 'n ends changes to help Collage. - Complete the implementation of WithFields.   (Unfortunately they appear to be without unit tests and I continue this tradition...) - InferTypeExpr for InferTypeLocal but return the expression rather than the type. - Remove python binding of InlineComposites since C++ impl was removed some time ago. - Make IndexedGraph<Expr/DFPattern> more robust as stand-alone datastructure, and avoid unnecessary copies.   This will become a fundamental datastructure in Collage rather than just a helper for DFPatternMatcher. - Extend IndexedGraph with a notion of 'basic block' on every dataflow node. Needed by Collage to   avoid impossible partitions.* - Revert non IndexedGraph changes.* - Stick to 'Indexed graph' terminology- More tests* - Stick to 'Indexed graph' terminology- More tests* - Remove silly unit test",2
"[relay] add missing virtual d'tor (#11601)Add a default virtual destructor to`tvm::relay::transforms::GlobalSymbolCache`, so thatcorrect destructors run when destroyingsubclass instances.",1
"[Hexagon][CI] Re-enable Hexagon tests in CI (#11613)* [Hexagon][CI] Re-enable Hexagon tests in CIThese were enabled in https://github.com/apache/tvm/pull/11294, thenerroneously disabled in https://github.com/apache/tvm/pull/11313.This applies the same fix as inhttps://github.com/apache/tvm/pull/11294, checking the`ANDROID_SERIAL_NUMBER` to determine if Hexagon tests can execute atruntime, but using the refactored `pytest.skipif` messages introducedin https://github.com/apache/tvm/pull/11313.* Fixed circular dependency, but feels somewhat ugly",0
"[MetaSchedule] TuningRecord Optional Arguments (#11598)In some situations, such as before measuring the candidates, the arguments `run_secs`, `target`, and `args_info` in `TuningRecord` are not required. Per this request, the new `TuningRecord` API now accepts arguments in the order of `trace, workload, run_secs, target, args_info` with the last three being optional. Note that some tests might fail due to the change of argument order, so they might need to be adjusted accordingly.",1
[docs] Various content corrections (#11517)* [docs] Various content corrections* Fix underline title,0
[DNNL] Fix end of line in test_dnnl UT file (#11560),0
minor fix after loading trt engine from disk (#11614),0
"[Relay] Restore dominator check (#11616)It is ok to match a sub-graph which has dataflowoutside of the sub-graph, provided all such flowseventually come into the sub-graph.",5
"[Hexagon] Make local symbols visible to loaded modules in RPC server (#11611)The simulator library `libhexagon_rpc_sim.so` contains TVM runtime builtinto it, but since it's loaded as a ""local"" library these symbols are notvisible to shared libraries loaded by subsequent dlopens. (Same applies tosymbols from the C++ runtime.)To make these symbols visible, dlopen the defining libraries as ""global"".(Re-dlopeninig an already loaded library is a well-defined operation.)",2
"TVMC: Allow to overwrite TVM_CONFIGS_JSON_DIR via environment variables (#11623)If a non-default location for the build directory is used, e.g. set via TVM_LIBRARY_PATHwe need to provide the user a way to overwrite CONFIGS_JSON_DIR as well.",5
Patch replay trace. (#11621),5
[BYOC][DNNL] Enable layer normalization in DNNL byoc. (#11508)* Enable layer normalization in DNNL byoc.* Added unittest for layer norm and make code compatible after introducing TensorRequisite(PR-11345)* Fix lint issue* Fix clang format issue,0
[COMMUNITY] @tkonolige -> Committer (#11626),3
[Hexagon] Add random string to workspace name (#11593),1
[ONNX] Add ReduceSum opset13 support (non-dynamic) (#11606)* [ONNX] Add ReduceSum opset13 support (non-dynamic)* Add check* Add support for constant axis* noop* Rework logic,1
[OpenCL] Implement conv2d_winograd algorithm for Adreno (#11543)* Implement conv2d_winograd algorithm for Adreno* Implement gtest for OpenCL texture pool* Implement conv2d_nhwc_winograd for Adreno* Minor refactoring* Fix lint* Apply comments* Apply comments* Fix lint,0
[CMSIS-NN] Removed redudant arguments to CMSIS-NN wrapper function (#11431)Removed input_scale and filter_scale from CMSIS-NNwrapper function. These are not needed by CMSIS-NNAPI which gets called from the generated C wrapperfunction for Conv2D.,4
"[TIR] CSE pass : Restrict the equivalence to be decided by a normal form - avoids comparison of terms (#11574)The CSE pass had been designed for potentially allowing comparisons (and commonings) of equivalent terms (like (x+y)+z and x+(y+z)), where **the notion of being equivalent was customizable, and no assumption was made about it**. That means that the implementation of the equivalence test function `EquivalentTerms()` - which was at the moment just calling the syntactical equality test `EqualTerms()` - could be replaced later by a cleverer equality test.However, having such a generic way of comparing elements meant that in the function `SyntacticToSemanticComputations()`, where we were going from a hashtable of syntactical entities to what I called a vector of ""semantical entites"" (which are just canonical forms/representants of classes of equivalence of terms), **the only way was to compare each pair**.That resulted in a quadratic behavior of this function, but there was no way around it as in order to merge equivalent entities into their class of equivalence, we had to compare them.**This PR essentially does the following:**- When computing the classes of equivalences of terms (therefore transforming a ComputationTable (i.e. a hashtable) into a vector of classes of equivalence) : **instead of comparing each pair of terms, relies on a normalization procedure to obtain a normal form for each of them**.That transforms a small part of the algorithm that was quadratic to n.logn. However, it's difficult to see improvements in practice, in particular for average sized programs, as that part was a ""small"" quadratic to a ""big"" n.logn (finding things in a hash-table, copying it to a vector, etc).It was probably going from a complexity of ~O(((n²-n)/2) + n.logn) to a complexity of ~O(3n + n.logn), so potential gains would only be expected for very large programs.- Completely gives the user the possibility to turn ON/OFF the semantical comparisons of terms. It is turned OFF by default (as it's quite longer to compile with it ON, unsurprisingly), which means that by default, the equivalence coincides with the (syntactical) equality of terms.    As the pass was written with the possibility to do these additional commonings (like (x+y)+z and x+(y+z)), it was a good time to fully plug that completely, up to the Python user who can now turn that ON if he wants to. But again, it is OFF by default, so no real change on that.To run it ON, simply do:`with tvm.transform.PassContext(config={'tir.enable_equiv_terms_in_cse_tir':True}):`before calling `build()`- When this boolean is set to ON, it uses a simple implementation of the normalization function with equivalences that uses `arith::Analyzer::Simplify` as noted by in https://github.com/apache/tvm/pull/10544 . Note that this is not a real normalization procedure as it is incomplete (i.e., it is not guarantee to converge to the normal form), but it is correct, and it works well with most properties : associativity of +, distributivity of * on +, etc.- Clarifies and enhance the test base for the pass. In particular, it adds the tests that were written in https://github.com/apache/tvm/pull/10544 but which did not make it through.- Also add the test ( https://github.com/AndrewZhaoLuo/TVM-Sandbox/blob/19284ddbd6bb28af61c0c2aa8bb334c5c53731a7/tir/test_inconsistent_tir_lowering.py#L1 ) demonstrating the (older) non-deterministic lowering and put it into a proper test, as I found it useful for making sure that this does not happen again. It has been copied from https://github.com/apache/tvm/pull/10663 and only slightly adapted (in particular for doing the comparison of hashes automatically instead of printing them and relying on a human to compare them).",1
[ci] Add guards to pytest_wrapper (#11553)This should fix #11544 and adds some more logging in case the issue persists. Unfortunately it is difficult to test for real since the case data in that PR is thrown away after Jenkins is done (Jenkins does store test data but it marshals JUnits into its own format)Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"[PASS] Refactor a couple of TIR passes - BindTarget, AnnotateEntryFunc, Filter, LowerInitBlock (#11628)This PR fixes a few inconsistent pass registration and add testcases for them. - `LowerInitBlock` had mismatch between its pass name and ffi key.- `BindTarget`, `AnnotateEntryFunc`, `Filter` were not following the name convention of tir passes and they were not registered in FFI registry.",0
[microTVM] Remove microTVM RVM version suffix (#11629),0
[Frontend][TFLite] Improve support for half_pixel_centers in resize (#11521)* add resize_nearest_neighbor op test* Improve support for half_pixel_centers in resize,1
Making CMSIS-NN tests pylint compliant (#11625),3
"[TIR][Schedule] Allow named block and buffer arguments in Schedule (#11624)* [Schedule] Allowed string argument as block argThis has previously been implemented for `Schedule.transform_layout`in https://github.com/apache/tvm/pull/11296, extending to allow forblock arguments in all `Schedule` methods.This change was only made for arguments that must be a `BlockRV`.  Forarguments that may be either a `BlockRV` or anothertype (e.g. `Schedule.get_child_blocks` accepts either `BlockRV` or`LoopRV`), this sugar is not implemented, to avoid ambiguity.* [Schedule] Allowed string argument to Schedule.reindexSimilar to https://github.com/apache/tvm/pull/11269, which added thisfunctionality to `Schedule.transform_layout`.* CI test update",1
"[ci] Rebuild Docker images if necessary (#11329)This rebuilds Docker images and uses them in later stages in the same build. If the build is running on `main`, then the images are uploaded to Docker Hub automatically once the run is complete. Images are always rebuilt, but Docker Hub functions as a cache. If there have been no changes to `docker/` since the last available hash on Docker Hub, then the build will just use the images from Hub.",4
[Hexagon] Fix gtest flag in apps/hexagon_api/CMakeLists.txt (#11652),0
[microTVM] Update pyproject to python3.7 (#11634)* Update to python3.7 and add poetry.lock file,1
[microTVM] Add support for Arduino Portenta H7 (#11636)* Add support for Portenta H7* Add Portenta H7 to supported boards in README* Rerun tests,1
adding vvchernov to contributors file (#11649),1
[COMMUNITY] Alexander Peskov -> Reviewers (#11648)* adding ramana to reviewers list* adding apeskov as reviewer* fix,0
"[TVMSCRIPT] Improve tvmscript type hints (#11654)* [TVMSCRIPT] Improve tvmscript type hints- Change numeric types to classes so they work as function arguments.- Add var as a class.- Add floordiv, index, and mod to PrimExpr.* use Union",1
[CRT runtime] Added functions TVMPlatformBeforeMeasurement and TVMPlatformAfterMeasurement (#11244)* Added functions with weak links before and after TVMFuncCall in the TimeEvaluator* Fixed lint* Clang changes* Added clang proposal* clang-format proposed changesCo-authored-by: Federico Peccia <peccia@fzi.de>,0
"[BUG] Disable second PlanDevices pass (#11662)Though started with the best of intentions, the secondPlanDevices pass to account for memory scope's introducedby lowering is buggy and not ready for prime time. Ithas caused an ICHECK fail since for some reason the newconstraints are not flowing into device_copies.",0
"[TIR, CUDA] Add pass to replace global to shared memory copy with cp.async (#11658)* [TIR, CUDA] Add pass to replace global to shared memory copy with cp.async* add missing doc* black* missing src* clang format* clang format* check against nested async scope",1
"[BYOC] RelayToTIR custom codegen passes can still depend on dynamic shape functions (#11619)In #11474 I got ready to switch CUTLASS from function-at-a-time to IRModule-at-a-time compilation.However my approach didn't handle dynamic shape functions, so I adjust it here.The idea is still that such passes will leave behindcalls to 'extern' functions. However, converting thosecalls to 'call_lowered' form inMarkCompilerFunctionsAsExtern is too soon since onlythe TECompiler knows how to capture all the attributesnecessary to support dynamic shape functions.So stop doing that in MarkCompilerFunctionsAsExtern andinstead support this case properly in the TECompiler.While there try to chip away at the chronic lack of structure in te_compiler.cc. Every little bit helps.Add a basic unit test.",1
"[MetaSchedule] Developer Ergonomics Enhancement (#11622)Per discussion with @Kathryn-cat- [x] Move `initialize_with_tune_context` as private API `_initialize_with_tune_context`, andencourage using `TuneContext.initialize`- [x] Instead of using bunch of import statements, encourage using `ms.xxx` as the prefix(e.g. `ms.database.MemoryDatabase`) to organize things better- [x] Move `DefaultLLVM`, `DefaultCUDA` to a separate file and make them more discoverable- [x] Move `DummyDatabase` to `tvm.meta_schedule.database.MemoryDatabase` given it's actually useful- [x] Delegate class members' methods in `TuneContext`, for example, having`TuneContext.generste_design_space` from `TuneContext.space_generator.generste_design_space`Next PR:- Allow using a string `""default""` in `TuneContext` as well as `tune_relay/tir/te` to quicklyspecify a set of target-specific rules- Add `TuneContext.tune` to allow directly tuning without task scheduler.- Enhance detection of `ScheduleFn` in `TuneContext` to make it easier for users to quickly try outtemplate-driven scheduling on TIR.Co-Authored-By: Kathryn (Jinqi) Chen <65606304+Kathryn-cat@users.noreply.github.com>",0
[Bugfix] GetReduceAxes accept empty axis (#11643)* emptycommit 2nd try* codeCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>,0
"[DNNL][Relay extern-schedule] DNNL Conv2D Kernel enable by assigning ""-libs=mkldnn"" (#11571)* enable oneDNN conv op by using -libs=mkldnn* add channel last format support and let oneDNN chose blocked format.* remove unnecessary changes* reformat 3 files* reformat 1 file* change the argument name* change the argument name* rename the arguments* fix cpp lint issue* fix cpp lint issue* fix cpp lint issue* clang reformated* adjust .py import for testing* function existence check in test",0
Add assert message (#11665)Change-Id: I88f19c7105cce048d2f52d50450a551fb12162dc,1
[CI] fix ci_gpu dockerfile (#11644),0
[microtvm] Add mxnet importer and update pyyaml to fix poetry error (#11668),0
"[ci][docs] Don't delete old versions when checking out docs (#11612)We don't have a good way to tell if a file was deleted or not in a docs update, so currently we delete the entire `docs/` folder and replace it from the build. However, this includes old version docs that aren't build in the normal docs build. This excludes them from the deletion so they stick around between updates. We'll have to revisit this list at each release but it should be a simple update.Co-authored-by: driazati <driazati@users.noreply.github.com>",1
[Hexagon] Run single RPC server on Android in each testing session  (#11547)* Reuse hexagon launcher in test session* separate random name generation* revert get_aot_executor* Fix launcher for simulator case* add stop server for simulator,0
[BYOC][DNNL] Improve performance of DNNL BYOC dense operator (#11513)* Enhance dnnl byoc dense operators performance by 1) introducing gelu fusion and 2) introducing alter dense weight layout.* fix lint issue* add unittest for dense pack* Make code compatible after introducing TensorRequisite(PR-11345)* Fix comments & refactor code* Fix lint* Fix partition graph unittest case* Fix comments* Fix comments* Fix lint,0
[DNNL][CBLAS][BYOC] Unifles all MKLDNN/DNNL  to DNNL (#11638)* unifies all MKLDNN/DNNL_CODEGEN to DNNL* translate -lib=mkldnn to -libs=dnnl in target* type check added before* rebase and update conv2d from mkldnn to dnnl,1
"[FIX,METASCHEDULER] Fix tune_te (#11676)`tune_te` was broken because it passed a primfunc to `tune_tir`. Now itis wrapped in an IRModule. Also the test is re-enabled.",0
"[MetaSchedule] Generate MetaSchedule Dataset (#11641)In order to build a dataset for improving the cost model for MetaSchedule, I added several filesincluding importing models to TVM, extracting tuning tasks, and sampling measure candidates.Meanwhile, I exposed some methods in C++ to the Python side to assist the process.",1
"[BYOC] Make CUTLASS BYOC integration 'Collage friendly'   (#11631)* [BYOC] Make CUTLASS BYOC integration 'Collage friendly'(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).Currently CUTLASS has four entry points: - The usual 'partition_for_cutlass' partitioning function, using the   standard pattern table and pass machinery (see cutlass/build.py). - A 'tune_cutlass_kernels' function which augments CUTLASS partition   functions with the results of building and running test kernels (see cutlass/build.py). - A 'relay.ext.cutlass' external codegen function which inspects the   turning results and generates a CSourceModule for each partitions   (see cutlass/codegen.cc). - A 'build_cutlass_kernels_vm' function which runs 'export_library' with   all the nvcc compiler options needed to build all the CSourceModules   (see cutlass/bild.py).For Collage we'd like CUTLASS to have only two entry points: 'partition_for_cutlass',and 'relay.ext.cutlass' or equivalent. This makes the CUTLASS external codegen integrationcomposable with other integrations, which in turn helps Collage avoid having to understand anyexternal codegen APIs other than the global pattern table and the custom compilation function/pass.Collage also tends to end up requiring multiple partitions for the same backend since it ismore aggressive at mixing-and-matching smaller sub-graphs between backends. Thus we'd also liketo make sure all tuning, generated code and compilation overhead is shared between all such CUTLASSpartitions.So, in this PR: - We add all the CUTLASS-specific tuning and compilation options as new Target   attributes for the 'external codegen' ""cutlass"" TargetKind (cutlass/target.cc).   The user now has one place to provide those settings, and we've already done the   legwork to plumb the target instance. - We replace 'relay.ext.cutlass' with a 'RelayToTIR' custom pass hook   'CompileForCutlass' (see cutlass/codegen.cc). This pass obviously can see all   the CUTLASS partitions in the IRModule, so we can now share tuning results   between them all and can be sure to generate a single CSourceModule. The pass can   also invoke the compiler to yield a StaticModule, which we've also already done the   legwork to support. In this way all CUTLASS-specific steps are handled at once. - For convenience we supply 'finalize_modules' and 'finalize_modules_vm' which   invoke nvcc for final linking (using export_library as usual). However, there's now   nothing CUTLASS specific in those helpers other than their overriding of the 'compiler' to   be nvcc. - test_cutlass.py is updated to use the new API. Though this is a breaking change for existing users of the CUTLASS integration the change is pretty minor, as shown in test_cutlass.py.* - Masa's comments* - Remove unnecessary save.",1
[TIR] Register CUDA WMMA tensor intrinsics (#11677)* Register CUDA wmma tensor intrins* Meta programming to generate wmma intrin* format* fix* fix wmma_store* lint* Update cuda.py,0
Fix typos in target warn of dnnl (#11678),0
[MetaSchedule] JSONDatabase Utilities (#11680)This PR adds some utility to JSONDatabase to accelerate its loading/saving time.,1
[Relay] Finish implementations of WithFields (#11674),5
[Bugfix] Shape inference of weight for grouped `nn.conv3d` (#11681)* Fix `nn.conv3d` weight shape inference.* Add test for conv3d type inference with groups.,0
fixed cutlass byoc build break (#11686),0
[docs] Add links to v0.8.0 docs (#11647)This uses the new code from https://github.com/tlc-pack/tlcpack-sphinx-addon/pull/5 with a link to the v0.8.0 docs. We can update this in the future as we add more releases.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[UnitTests] Parametrized test_topi_argwhere.py (#11651)Refactored while debugging breakage of tests inhttps://github.com/apache/tvm/pull/11646.  Submitting as a separatePR, as it isn't necessary or related to the primary changes in thatPR.",0
Added a docstring to missing CMSIS-NN test (#11690)* Made CMSIS-NN tests pylint compliantChange-Id: I6bc536a80a24a1603e9f75f8ee9a26d0d88f10df* Removed comments that disabled pylint checksChange-Id: Iee513a4a5bef1db5b78e1d25a30ac7202f8b0e92* Fixed pylint issue in the generate_constants testChange-Id: Icd341cf524b331ced1fc7ef282b67296583b0fa4,0
"[Hexagon] Tighten requirements on inclusion of runtime sources (#11635)* Tighten requirements on when Hexagon runtime sourcesare included in the runtime build. Specifically only include themwhen building for hexagon rpc on hardware and do not include themfor x86 (host, simulator) or android builds.* Remove device_api.cpu binding to hexagon in simulator rpc session.Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Karl Koscher <kkoscher@octoml.ai>* if(BUILD_FOR_HEXAGON)Co-authored-by: Adam Straw <astraw@octoml.ai>Co-authored-by: Karl Koscher <kkoscher@octoml.ai>",4
[MetaSchedule] Add Profiler Support For Tuning Efficiency Optimization (#11486)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,1
"[TE] Support schedulable TIR compute definitions in TOPI (#11589)This PR adds `te.extern_primfunc` which provides the interface around TE ExternOp that allows a TVMScript defined schedulable TIR PrimFunc to be inlined into a TE compute graph. The result is that TIR can be used for compute definitions in Relay OpStrategies and, paired with meta-scheduler support in relay as introduced in #10578, these compute definitions can be scheduled and tuned as demonstrated in the attached tests.  Prior to this, compute definitions were limited to those definable in TE only. As a consequence of this patch and ongoing improvements to TVMScript meta-programming (#11097), TOPI can be extended to include compute and scheduling functions targeting schedulable TIR uniformly.",1
Updated install from source docs to include additional instructions for M1 macs. (#11675)Co-authored-by: Noah Verke <noahverke@nverke-MBP.local>,1
Fix onnx round import with float64 inputs. (#11685)* Fix onnx round import with float64 inputs.* Fix lint and optimize dtype mapping.,0
"[Hexagon] Add HexagonThreadManager (#11653)* Adding initial threadmanager class* Fixed compile errors* Moving constant defines inside class* Updating qurt includes* use default scope for hexagon buffers* Updating buffer allocations* Fixed bug where array of pointers treated as array of structs* - Updated HexgonDeviceAPI to use HexagonThreadManager- Updated HexagonThreadManager interface to use TVMStreams- Added second `Dispatch` interfce in thread manager to use PackedFuncs- Updated thread manager to use vector for dynamic semaphore allocation- Added ""#if defined(__hexagon__)"" in several places to prevent compilation errors* Bug fixes + interface addition + basic thread tests - Fixed GetStreams not returning the streams properly - Added missing semaphore cleanup to prevent qurt kernel resource leakage - new interface functions:   - Start() : now all worker threads are blocked on initialization until ThreadManager->Start() is called   - WaitOnThreads() : blocking call which waits until all worker thread queues are empty - added extra debug logging - Two new basic thread tests working* Adding initial ThreadManager tests* HexagonThreadManager tests and refactor* remove stack / pipe size member vars* init pointers in the header file* move all mem allocs to SpawnThreads* start_semaphore as object instead of pointer* fix bug with WaitOnThreads deadlock + Wait/Signal off by one error* add / refactor Signal / Wait tests* add SyncFromTo test cases* add Dispatch test cases* add pipe fill and overflow cases* Updating dispatch to return bool and fix pipe overflow problem* change around min / max values for stack / pipe* integrate pipe fill / overflow tests back into HTM test suite* use HexagonBuffer* assert if stack / pipe sizes fall below min* Changed semaphore vector to store pointers, not structs (fixes vector capacity adjustment invaliding in-use addresses).* add producer consumer, thread order test cases* change to unordered_map for semaphores and remove PreallocateSyncs* tests running on device* code cleanup for compile warnings* remove #if defined(__hexagon__) guards* copyright, format, lint* add hexagon buffer map class* remove redundant thread manager tests* revert Hex Dev API changes for threading* add comments; remove untested code to dispatch / wrap a packed func* pass pipe address and not HTM pointer to thread context* rename to HexagonBufferManager* cleanup ahead of PR* use DLOG(INFO)* refactor GetStreamHandles to return a vector by value* adjust HexagonBufferManager methods; use thread_manager file names* style guidelines and debug prints* reinterpret cast for TVMStreamHandle* end member variables with underscoreCo-authored-by: Joseph McMahan <jmcmahan@octoml.ai>",0
[AutoTVM][Autoscheduler] Default build funcs inherit PassContext (#11632)* init commit* lint* empty commit* test results* reset progress* lint* fix,0
"[WIP] [CI] Bump CI GPU image version (#11637)* Bump CI GPU image version* Run generate,py",5
[Hotfix][MetaSchedule] Importing from test foldeer (#11695)A concurrent merge breaks the unittest which imports directly from`meta_schedule.testing`.,0
[BYOC-OpenCLML] OpenCLML integration with TVM. (#10243)* [BYOC-OpenCLML] OpenCLML integration with TVM.* [BYOC-OpenCLML] Cleanup and review.,5
cleanup (#11659),5
[tests][hexagon] Fix `allocate_hexagon_array` bug. (#11709)Fix bug where `allocate_hexagon_array` in`tests/python/contrib/test_hexagon/infrastructure.py` wasn'trespecting the caller-specified `memory_scope`.,0
Move FlattenAtrousConv before AlterOpLayout in the default opt pipeline. (#11706)Co-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>Co-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>,4
[Hexagon] remove #if defined(__hexagon__) where it is no longer needed (#11708)* remove #if defined(__hexagon__) where it is no longer needed* format and lint,4
[CI] [Hexagon] Update docker tag in jenkins (#11588),1
[microTVM][zephyr] Add support for host-driven AoT execution on zephyr (#11650)* - add support for host-driven AoT execution on zephyr;- add initial version of reference counting to prevent python code from inadvertently freeing tensors during garbage collection;- add support for numerical indices to host-drive AoT get_input();- add two initial tests for host-driven AoT execution on zephyr;- rename existing zephyr AoT exec. test;* address PR feedback* increase stack size to accommodate qemu_riscv64 stack usage,1
[LLVM] Update uses of AllocaInst::getAlign[ment] (#11718)Today's LLVM main branch removed AllocaInst::getAlignment.,1
Fix 1d-softmax schedule. (#11719),0
"[MetaSchedule] Apply-History-Best Task Filtering (#11692)This PR enables task filtering in Apply-History-Best, which is used inRelay/Relax integration. Previously, even though a task is ruled outduring task extraction, it still shows up in Relay compilation due tothe lack of filtering on `Apply-History-Best`. However, TE-to-TIRconversion `te.CreatePrimFunc` doesn't support all cases with hybridoperators involved, which leads to post-tuning failure affectingmultiple models.",5
"[Bugfix][TIR] Narrow-Datatype for thread axis (#11725)This PR fixes a bug in the pass Narrow-Datatype in TIR, where dtype ofcertain IterVar and loop variables are adjusted to narrower ones.The bug occurs when the dtype of thread axis is int32, while its extentis int64, where the original behavior will not narrow the extent toint32, which causes an assertion thrown in IterVar's constructor. Analternative approach is to re-dtype IterVar to int64, however, thesubsequent passes do not actually respect int64 thread axes, which leadsto even more issues in lowering.This bug prevents AutoTIR in tuning Huggingface DistilBERT.",0
[CI] add GH workflow to comment with link to docs (#11594),1
"[CI] Apply linting rules to AOT tests (#11657)This enables pylint against the AOT test cases.One issue I found was with the `tvm.testing.parameter` which breaks the naming convention rules in pylint (constants are upper case and function parameters are lower case). It may be worth a syntax similar to:```tvm.testing.parameter(""enable_usmp"", [True, False])```",2
[CMSIS-NN] Fixed error in finding input's dtype in maxpool (#11701),0
"[TVMC] Fix error while compile paddle model with tvmc (#11730)The tvmc command will throw a error while the passed path of model is not exist, But for PaddlePaddle model, it contains 2 file model_name.pdmodel and model_name.pdiparams, we only pass the prefix like inference_model/model_name.This pr is same with https://github.com/apache/tvm/pull/11108 Since the origin PR didn't update for a long time, I send this new PR",0
"[MetaSchedule] Include te/tensor.h instead of forward declaring te::Tensor (#11731)ApplyHistoryBestNode declares an Array of Tensor. There are type traitsused in Array that require that the element type is complete at the timeof the declaration. With only a forward declaration compilation fails(clang 14.0.3, libc++).",2
[Hexagon] Implement avg_pool2d slice op (#11417)* Implement avg_pool2d slice op* Address review comments and fix the STIR schedule* Fix formatting issues* Address pylint errors* Additional formatting issues* more pylint fixes* Changed arch version to v68 for now* Changing arch version back to v69* Move the test to tests/python/contrib/test_hexagon/topi,0
[ci] Skip failing tests in wheel (#11705)Some python tests are failing in the wheel. This PR skips them if the environment variable `WHEEL_TEST` is set.This PR is related to https://github.com/tlc-pack/tlcpack/pull/115.,3
"[MetaSchedule] Developer Ergonomics Enhancement II (#11727)Follow-up of #11622, per discussion with @Kathryn-cat- [x] Allow using a string `""default""` in `TuneContext` to quickly specify a set of target-specificrules- [x] Enhance detection of `ScheduleFn` in `TuneContext` to make it easier for users to quickly tryout template-driven scheduling on TIR.Next PR:- Add `TuneContext.tune` to allow directly tuning without task scheduler.Co-Authored-By: Kathryn (Jinqi) Chen <65606304+Kathryn-cat@users.noreply.github.com>",1
rename aot_demo to aot_standalone_demo for clarity vs. host-driven aot (#11723),5
[MetaSchedule] Modify Profiler Timers (#11735)Minor modification to scoped timers to cover 99% of all the time cost during MS tuning. Allow `ApplyHistoryBest` and `TaskExtraction` time to be counted during tune_relay.,2
[microTVM] Add support for the Raspberry Pi Pico via Arduino (#11694)* Add RP2040 support,1
[microTVM] [docs] Point micro_train tutorial links to official repos (#11715)* Point micro_train tutorial links to official repos,2
[Pytorch] Add quantized::leaky_relu (#11729)* emptycommit 2nd try* add operator and test* example output* lint with black* register param index* remove assert as it is a warning in torch* fix algo bugCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>,0
[TIR] Add preserve-unit-iters (#11585),1
[TVMScript] Support roundtrip of LetNode (#11742)Just a missing support for `tir.LetNode`,5
[Relay] Implement `SoftmaxRel` for softmax operators. (#11728)* Implement `SoftmaxRel` for softmax operators.* Print better error message for wrong axis.,0
"[Bugfix][MetaSchedule] Filter out dynamic extents (#11747)Previously only static shape computation is allowed in our tuningsystem. However, one special case is overlooked: the reduction iter varscould still have dynamic iteration domains which depend on other dataparallel vars. This PR rules out this case by carefully checking all theloop extents during task extraction.Related issue: https://github.com/apache/tvm/issues/11746.",0
[ci] Remove apt cache from the docker images (#11470),4
[microTVM] Refactor RVM scripts and fix DNS network issue (#11741)* refactor scripts* address comments,0
upgrade ci lint docker file (#11734),2
Fix CI break due to concurrent merge. (#11753)* #11470 and #11741.,0
[skip ci][microTVM] Update Arduino RVM name and box version (#11743)* update* Fix version* readme* Update README.md,0
[skip ci][microTVM] Update Zephyr RVM name and box version (#11655)* Use new Zephyr RVM version* fix box name* fix version to match zephyr* update version* Update README.md* Update README.md,0
[MetaSchedule][Minor] Organize Testing Scripts (#11751),2
Add optional mem_scope parameter to tvm.nd.array and tvm.nd.copyto (#11717),1
[ci][docker gpu] Install dnnl in docker GPU. (#11744)BYOC related tutorial may use dnnl  and such tutorial run at docker gpuwhich need to install dnnl to prepare the environment.,2
"Constant name prefix added (#11509)This is a proposal to fix the bug reported here: https://discuss.tvm.apache.org/t/problem-with-allocateconstnodes-in-cmsis-nn-code/12806Bug report: #11394A prefix has been added to the ""constant_"" in te_compiler_cache.cc to distinguish from the constant naming generated in aot_executor_gen.cc",0
Enable QNN primitives for DNNL runtime (#11642)* [DNNL] Enable QNN primitivesSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* [DNNL] add qnn testSigned-off-by: Alexander Peskov <peskovnn@gmail.com>* typo fixSigned-off-by: Alexander Peskov <peskovnn@gmail.com>,0
"[hexagon][testing] add test-skip logic; fixes (#11737)- Skip Hexagon benchmarks whenever the env. var `ANDROID_SERIAL_NUMBER`  has the value `simulator`.  This is a temporary hack to prevent the CI pre-commit hook from  running benchmarks, due to the extra time required.- Fix a bug where the elementwise-add benchmark code was broken by  an earlier change to the `HexagonLauncherRPC` class.- Rename `benchmark_elemwise_add.py` to `test_benchmark_elemwise_add.py`  so that it's noticed by the CI test infrastructure.  (CI tests are sometimes run in contexts _other than_ the pre-commit  hook.)- Miscellaneous small changes to  `tests/python/contrib/test_hexagon/benchmark_util.py`.",0
[CMSIS-NN] Fixed the case with repeating operands in the QNN binary ops (#11732),0
"[MetaSchedule] Distributed Measurement (#11683)This PR includes the distributed measurement of tuning candidates using builder and async runner, as well as some auxiliary functions. It enables multiple builders and multiple runners with a tracker connecting in between. The hierarchy of files in the database can be further compacted to make the database more concise.",2
"[hexagon][testing] add max_pool2d benchmark (#11720)- Add benchmarking framework for Hexagon maxpool-2d kernels,  and one (simple) kernel.",1
[MLF] Add support for multiple modules in Model Library Format (#11464),1
[Runtime][PipelineExecutor] Added Interface to Track Number of Global Inputs (#11315)* [Runtime][PipleineExecutor] Added Interface to Track Number of Global InputsAdded a feature to PipelineExecutor to track number of Global Inputs.* Fixed CI Error* Fixed remaining CI Error,0
"[MetaSchedule][Minor] Fix EvaluatorConfig Argument Description (#11766)Pointed out by @sunggg that the description of `number` and `repeat` for evaluator configuration is not accurate, updated to a version more consistent with `TimeEvaluator`.![TimeEvaluator](https://user-images.githubusercontent.com/3203174/174385966-74d3dbf6-dcca-43ea-9c0b-a91b4a281687.png)",0
Add tool to clear stale images. (#11772),1
"[MetaSchedule][Minor] Add Describe Function For Tuning Scripts (#11754)This PR is based on #11751 and adds `describe` function for `tune_relay` and `tune_onnx` script on both AutoScheduler and MetaSchedule. It prints out very useful information for reproducibility as follows:```Python Environment  TVM version    = 0.9.dev0  Python version = 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0] (64 bit)  os.uname()     = Linux 5.15.5-76051505-generic #202111250933~1638201579~21.04~09f1aa7-Ubuntu SMP Tue Nov 30 02: x86_64CMake Options:  {    ""BUILD_STATIC_RUNTIME"": ""OFF"",    ""COMPILER_RT_PATH"": ""3rdparty/compiler-rt"",    ""CUDA_VERSION"": ""NOT-FOUND"",    ""DLPACK_PATH"": ""3rdparty/dlpack/include"",    ""DMLC_PATH"": ""3rdparty/dmlc-core/include"",    ""GIT_COMMIT_HASH"": ""3b872a0adae07b0cd60248346fd31b158cba630c"",    ""GIT_COMMIT_TIME"": ""2022-06-15 11:27:59 -0700"",    ""HIDE_PRIVATE_SYMBOLS"": ""OFF"",    ""INDEX_DEFAULT_I64"": ""ON"",    ""INSTALL_DEV"": ""OFF"",    ""LLVM_VERSION"": ""11.0.1"",    ""PICOJSON_PATH"": ""3rdparty/picojson"",    ""RANG_PATH"": ""3rdparty/rang/include"",    ""ROCM_PATH"": ""/opt/rocm"",    ""SUMMARIZE"": ""OFF"",    ""TVM_CXX_COMPILER_PATH"": ""/usr/lib/ccache/c++"",    ""USE_ALTERNATIVE_LINKER"": ""AUTO"",    ""USE_AOT_EXECUTOR"": ""ON"",    ""USE_ARM_COMPUTE_LIB"": ""OFF"",    ""USE_ARM_COMPUTE_LIB_GRAPH_EXECUTOR"": ""OFF"",    ""USE_BLAS"": ""none"",    ""USE_BNNS"": ""OFF"",    ""USE_BYODT_POSIT"": ""OFF"",    ""USE_CLML"": ""OFF"",    ""USE_CLML_GRAPH_EXECUTOR"": ""OFF"",    ""USE_CMSISNN"": ""OFF"",    ""USE_COREML"": ""OFF"",    ""USE_CPP_RPC"": ""OFF"",    ""USE_CUBLAS"": ""OFF"",    ""USE_CUDA"": ""/usr/lib/cuda-11.2"",    ""USE_CUDNN"": ""OFF"",    ""USE_CUSTOM_LOGGING"": ""OFF"",    ""USE_CUTLASS"": ""OFF"",    ""USE_DNNL"": ""OFF"",    ""USE_ETHOSN"": ""OFF"",    ""USE_FALLBACK_STL_MAP"": ""OFF"",    ""USE_GRAPH_EXECUTOR"": ""ON"",    ""USE_GRAPH_EXECUTOR_CUDA_GRAPH"": ""OFF"",    ""USE_GTEST"": ""AUTO"",    ""USE_HEXAGON"": ""OFF"",    ""USE_HEXAGON_GTEST"": ""/path/to/hexagon/gtest"",    ""USE_HEXAGON_RPC"": ""OFF"",    ""USE_HEXAGON_SDK"": ""/path/to/sdk"",    ""USE_IOS_RPC"": ""OFF"",    ""USE_KHRONOS_SPIRV"": ""OFF"",    ""USE_LIBBACKTRACE"": ""ON"",    ""USE_LIBTORCH"": ""OFF"",    ""USE_LLVM"": ""llvm-config-11"",    ""USE_METAL"": ""OFF"",    ""USE_MICRO"": ""OFF"",    ""USE_MICRO_STANDALONE_RUNTIME"": ""OFF"",    ""USE_MIOPEN"": ""OFF"",    ""USE_MKL"": ""OFF"",    ""USE_MSVC_MT"": ""OFF"",    ""USE_NNPACK"": ""OFF"",    ""USE_OPENCL"": ""OFF"",    ""USE_OPENCL_GTEST"": ""/path/to/opencl/gtest"",    ""USE_OPENMP"": ""none"",    ""USE_PAPI"": ""OFF"",    ""USE_PROFILER"": ""ON"",    ""USE_PT_TVMDSOOP"": ""OFF"",    ""USE_RANDOM"": ""ON"",    ""USE_RELAY_DEBUG"": ""OFF"",    ""USE_ROCBLAS"": ""OFF"",    ""USE_ROCM"": ""OFF"",    ""USE_RPC"": ""ON"",    ""USE_RTTI"": ""ON"",    ""USE_RUST_EXT"": ""OFF"",    ""USE_SORT"": ""ON"",    ""USE_SPIRV_KHR_INTEGER_DOT_PRODUCT"": ""OFF"",    ""USE_STACKVM_RUNTIME"": ""OFF"",    ""USE_TARGET_ONNX"": ""OFF"",    ""USE_TENSORFLOW_PATH"": ""none"",    ""USE_TENSORRT_CODEGEN"": ""OFF"",    ""USE_TENSORRT_RUNTIME"": ""OFF"",    ""USE_TFLITE"": ""OFF"",    ""USE_TF_TVMDSOOP"": ""OFF"",    ""USE_THREADS"": ""ON"",    ""USE_THRUST"": ""OFF"",    ""USE_VITIS_AI"": ""OFF"",    ""USE_VULKAN"": ""OFF""  }```",0
[MetaSchedule][Runtime] Enhance Runner RandomFill (#11758),5
[COMMUNITY] Denise Kutnick -> Reviewer (#11778),3
"[TIR, analysis] Add GetAutoTensorizeMappingInfo to generate transforms for auto tensorization (#11740)This PR added a utility function `GetAutoTensorizeMappingInfo` to propose mapping from workload block iters to the iters in the tensor intrin. An example usage is conv2d, where the computation block has more iters than the matmul tensor intrin.",1
"[TVMC] Fix tvmc run when using rpc (#11757)* [TVMC] Fix tvmc run when using rpcAs described in #11707, the RPC mechanism does not supportobjects of type Map which breaks the use of tvmc run when usingRPC after #9889. This commit intends to workaround this issue byproviding a fallback to the old implementation when RPC is beingused. Further, a test has been provided to help prevent thisregression in the future.Change-Id: I70c1863d00098270e27c08ba834a3587e9132d69* fix lintChange-Id: I958cf4e19988d047bdd2e02f6475b9f70afe80c8",0
[ci][docker] Remove Docker image upload prefix (#11769)These should go to the same tag as in `tlcpack` so the only difference is the user,0
"[MetaSchedule] Enhance parsing in JSONDatabase (#11791)Originally, when failed with `std::stoi` and `std::stod`, the parserdisruptly stops and throw an incomprehensible error message, forexample, ""stoi"". This PR improves the user experience by detailing whichstring causes the parsing issue.A minor fix: out-of-range integers in parsing will now automatically fallback to floating point numbers.",0
"[MetaSchedule][Minor] Update CPU Flush ArgParse Type (#11792)Previously `cpu-flush` option existed as a boolean or integer argument, which is a bit counter-intuitive because for argparse, any non-empty string such as `False` will be parsed to `True` when using as a boolean and integer a little bit vague here IMHO. This PR used a function from `distutils` to directly parse input string to boolean, which makes the usage more stragiht-forward like `--cpu-flush True` or `--cpu-flush False`. Meanwhile it still supports usage of `0/1` and made sure the argument is always required.",1
add layerNormal infer layout (#11784),1
"[docs] Fix incorrect command (#11630)build/html get moved into _docs, update related document.",0
[microTVM] [Fix] reboot include for Zephyr version >=2.6.0 (#11790)* Fix reboot include for Zephyr version >=2.6.0,0
Fix apt install (#11781),0
"[HEXAGON] Slice ops added - add, subtract, multiply (#11529)* [UPSTREAM][HEXAGON] Slice ops added - add, subtract, multiply* Change to v68* Change transform_numpy function call* Do not disbale pylint errors and fix them* Fix variable names* Move the test file to topi* Resolve conflict* Modify init",0
"[Arith] Simplification of ceil, log2, and left_shift (#11646)* [TIR] Simplify expressions using tir.ceil and tir.log2These expressions are introduced in `topi.math.ceil_log2`, and canotherwise be propagated through to the generated kernel.* [Arith] Added left shift handling to ConstIntBoundsAnalyzerPreviously, only right shift was handled.  These left shifts areused in the `cuda.sort` implementation.* Update to avoid left shift of negative numbers* Updated rewriting of log2(x) to only occur in ceil(log2(x))Per @wrongtest's request, to avoid rounding differences betweendifferent devices.* Avoid assumptions made of negative arguments to left-shift* Recognize bounds of int(ceil(log2(arg)))",1
"[CI Image] support CSI-NN2 in ci_qemu (#11689)* [CI Image] support CSI-NN2 in ci_qemu* build CSI-NN2, download related toolchain and qemu* using fixed csinn2 branch",0
[TE Schedule] Fix broken 2D softmax TE schedules when axis=0 (#11803)* Support arbitrary reduce axis in softmax schedule.* Fix lint.,0
[CI] Update GPU image to add DNNL (#11786)Requested by https://github.com/apache/tvm/issues/11774Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/260/,1
"[LLVM] Retrieve entire target string from LLVMModule (#11802)The blob-embedding code creates a new LLVM module for which is needs moreinformation than just the target triple. The `_get_target_triple` functionin LLVMModule returned the triple with additional options appended to thestring. Instead of piggy-backing those extra options on top of the triple,replace `_get_target_triple` with `_get_target_string`, which will returnthe entire target string.",1
[TIR] Add SHash and SEqual to IndexMap (#11798),1
[skip ci] Use Stanford Cars mirror to fix CI docs build (#11812),0
Change new concat (#11800)* changed x86/concat to use lists of ints instead of te.tensor.Tensor for loop extents and array offsets* typos fixed* removed unused import* fixed micro model test* fixed micro model test,0
[Relay][Op] MetaSchedule layout in TypeRel (#11819)Co-authored-by: Junru Shao <junrushao1994@gmail.com>,5
[TFLite] Support quantized EQUAL op in TFLite frontend (#11520)* [TFLite] Support quantized EQUAL op in TFLite frontendSupport EQUAL quantization operation conversion as part of issue #9187* [TFLite] Support quantized EQUAL op in TFLite frontendUpdate elementwise quantized test for EQUAL opChange-Id: I3897d1ac07051ebfc10356ad45397117b592f878,1
"[CPP-RPC] Fix command line argument capture (#11801)There are a couple of instances where command line options use a ""-"",however when capturing these values a ""_"" is used, meaning theydon't get captured and the default value is used instead. Fixingthis by renaming instances of ""_"" -> ""-"".Change-Id: I9e083e25c5cc273298cd15df85a5862ee5f6722c",0
[microTVM][RVM] Reuse QEMU installation config and fix bug in RVM testing (#11808)* refactor* add llvm installation* fix testing,0
[lint] CHange docker lint message (#11767),4
[TOPI][Relay] New Op: MetaScheduleLayoutRewrite (#11826),1
[ONNX] Add more dynamism to Eyelike (#11615)* add dynamism-okness to eyelike onnx importer* add dynamism to eyelike* add more dynamism robustness to eyelike onnx importer* noop,1
[Fix] int32/64 mismatch of buffer elem_offset at HandleBufferBindScope (#11755)Yet another int64/32 mismatch at TIR level. `ArgBinder::Bind_` requires `elem_offset` of arg & view to have the same dtype while `int64-broadcast-concat` produce int64 `elem_offset`,0
"[docs] Fix the error in install/from_source.rst file (#11796)#10755 changed the TVM_LOG_DEBUG separator from ';' to ','.  This PR changes installation guide file accordingly.",0
skip mps2_an521 for host-driven AoT zephyr tests (#11833),3
Sort functions (#11814),5
"[LLVM/String] Remove conversion operator of String to llvm::StringRef (#11807)* [LLVM/String] Remove conversion operator of String to llvm::StringRefWe should not be declaring LLVM data structures in headers unrelatedto LLVM. There are only a handful of places where such a conversionwas used, it was replaced with a more local solution.* Rebase to restart CI* Restart CI",4
[USMP] Adding support for U1 usecase for constant pools (#10189)* [TIR.Constant] U1 usecaseConstants are now aggregated into one struct and initialized in default_lib0.cfileChange-Id: I34d61f8139c8a92c06944fe990ba892a660476fdUnit test fixedChange-Id: I436e7b6d6b3064b3f8bbfbb048d4296b63a6b69c* RefactoredAddressed:* PoolInfo splitted to WorkspacePoolInfo and ConstantPoolInfo* workspace_byte_alignment moved to ExecutorCodegenMetadata* getModuleAlignment -> GetModuleAlignment* GenerateInternalWorkspaceBuffers refactored* reverted format change of src/tir/transforms/legalize_packed_calls.cc* addressed comments for src/tir/usmp/analysis/extract_buffer_info.cc* removed commented code from include/tvm/tir/usmp/utils.hChange-Id: I7d1b32884b0e5992e2e00c7838c85e425d9c25fd* more unit test fixesChange-Id: I573a05fa1cb4037ae83691f7dff2c2724b1d7700* More refactoring and unit test fixesAdded ConstantMemoryPoolsChange-Id: If1e391c631575980564bca790ba33748c82d907f* bugfixChange-Id: Iacc7a9d734a505dfa0d8d32d23ea3f57e6de8582* refactoring. added constant_alignmentadded constant_alignmentunit tests updatedChange-Id: I378193cb9e675e352c61d96ff4e09655090053e1* unit-test bugixChange-Id: Ia4411d59c4a376c01326fed366cdb196a432899e* unit test fixChange-Id: Ia2077bdeb1d2c6c9827eeef90ab410ae31b8c4a4* Added support for c++ runtime* refactored* renamed pools and constsrenamed pools and consts to workspace_pools and constant_pools* addressed upstream comments* addressed upstream comments-2* addressed upstream comments-3,0
[microTVM][CMSIS] Add CMSIS libraries/sources to Zephyr CMake file (#11835)* Add CMSIS libraries to cmake build model with CMSIS,1
[CI] Amend docs bot comment (#11836)This PR fixes the docs bot comment message.,0
[TOPI] Layout Rewriting in TE (#11844),5
[TIR] Fix dtype mismatch in UnifyThreadBinding (#11843)This PR fixed dtype mismatch in UnifyThreadBinding when multiple thread axes with the same thread tag have different dtype.,0
[Bugfix][Minor] Avoid re-inference for MetaSchedule layout (#11842),0
[Relay][Pass] Meta-Schedule-Layout-Rewrite (#11845),4
"[CI][arm] Fix tensorflow-aarch64 repository URL (#11829)Update the custom repository URL used to pull TensorFlow-aarch64.The mechanism of installation is also changed to a file based, as atemporary workaround before we update to a newer version that canbe pulled from the official PyPI repository.Change-Id: Ic55abc9a9cd373c1db6b0322e7323dffbf2c12c8",0
[BugFix] IndexMap.Inverse for unit iters (#11841),0
"Revert ""upgrade ci lint docker file (#11734)"" (#11787)This reverts commit 7bfbc74c65684d1e25e235335da41c94372a561a, as itgenerates near 500 code violations when PyLint was updated from 2.4.4 to2.9.3.Issue #11785 for details.",1
"[LLVM] Remove `using llvm::BasicBlock`, NFC (#11850)There are a few places in CodeGenLLVM and CodeGenCPU that have thisdirective. There is no other `using` directive for any other LLVMtype anywhere. Remove it for consistency with the rest of the code.",4
"Fix `std::locale("""")` in profiling.cc (#11846)std::locale("""") would failed  in some env like: LC_ALL=zh_CN.UTF-8 `tvm._ffi.base.TVMError: locale: :facet::_S_create_c_locale name not valid>`locale has its default constructor function, no need to set input to empty string `""""`",0
[python][docs] fix docstring / comment typos (#11608),0
[OpStrategy] Support MetaSchedule Layout (#11848),5
"[LLVM] Remove PrintModule (defined in llvm_common.cc) (#11851)* [LLVM] Remove PrintModule (defined in llvm_common.cc)The only use of that function is commented out. `llvm::Module` can beprinted directly to `llvm::raw_ostream` via <<, so it's quite easy toinsert printing code when needed:```  std::string s;  llvm::raw_string_ostream os(s);  os << module;   // s (or os.str()) has the LLVM IR text```* Restart CI",2
[ci][docker] Fall back to tlcpackstaging if images don't exist (#11775)See #11768. This adds a script to check if Docker images exist in `tlcpack` and switch to `tlcpackstaging` if not (the tags must match though). There is also a feature flag for this in jenkins in the `DETERMINE_DOCKER_IMAGES` env variable (which must be set to `yes` for this change to work),1
[ci][docker] Send a PR to bump the Docker images nightly (#11813)See #11768 for detailsThis adds a GitHub Action to check for the latest images on Docker Hub via the Docker API and update the `Jenkinsfile` accordingly. It sends this in as PR for a committer to review and merge.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[ci] Enable pylint for tests/python/ci (#11666)This fixes up the pylint issues as part of #11414 for the CI tests,0
add split infer shape with convert op layout pass (#11825),1
add topK FInferCorrectLayout attr (#11849),1
fix flaky test (#11663),0
"[LLVM] Register factory function for CodeGenCPU (#11852)* [LLVM] Register factory function for CodeGenCPUAny target that has its own subclass of `CodeGenLLVM` must registera factory function that constructs an object of that class. Thisfactory will then be looked up and used in `CodeGenLLVM::Create`,which is the generic interface to create an LLVM code generator.However, there is no factory for `CodeGenCPU`, and so the creationof a `CodeGenCPU` object is done inside of `CodeGenLLVM::Create`.To make this happen, codegen_llvm.cc includes codegen_cpu.h, whichmakes the base class implementation depend on the derived class.This backwards dependency can be resolved by registering a factoryfor `CodeGenCPU`.* Add missing factory functions for other targets* Add cpp tests for codegen factories",1
add glu (#11865),1
[MetaSchedule] Introduce ArgInfo::FromEntryFunc (#11866),5
[Minor][MetaSchedule] Suppress warning for using `None` (#11868),5
[Target] Add a few AWS C5 instances in target tag system (#11869),1
[fix] quantize op consistent with python description (#11872)* move round op before `add expanded_output_zero_point`* consistent with python description `(round(input_tensor/output_scale) + output_zero_point`,0
[TIR][Pass] Remove-Weight-Layout-Rewrite-Block (#11870),4
[Arith] Update BufferDomainTouched to support vector access. (#11722)* [Arith] Update BufferDomainTouched to support vector access.* Add test checking that domain touched works on IR containing RampNodes.,1
[Hexagon] Softmax slice op initial version (#11559)Resolve merge conflict in utils.py,5
update (#11838),1
[ci] Add manual workflow to upload files to CI bucket (#11856)This adds a `workflow_dispatch` only GitHub Action that committers can use to upload files to the CI bucket for use like in #11839,1
[CI][Lint] Disable no-else-return check in pylint (#11327)* [CI][Lint] Disabled no-else-return check in pylint* Line breaks and alphabetical order for readability* Added description of reasoning/style in the code_guide,1
[Relay] [Pytorch] Add aten::maximum and aten::minimum (#11864)* add maximum and minimum* cleanup,1
add instance infer layout (#11871),1
[Arith] Fix DetectIterMap floordiv when IterSum only contains base expr (#11887),0
[PROFILING] Catch any errors while setting locale for printing (#11860)Change profiling::Report printing to catch any errors when setting thelocale (used to add separators to large numbers). This avoids issuesaround misconfigured locale.,0
Delete `from __future__ import annotations` since it requires Python 3.7+ (#11889),5
"[TIR][Arith] Avoid assigning range of possible values to integers (#11859)Previously, in `ConstIntBoundAnalyzer`, entering a conditional such as`if 2==0` could result in the expression `2` being treated as having aknown value of zero within the body of the conditional.  Evaluatingthe range of expressions using `2` in the body of the conditionalcould result in exceptions being thrown, such as evaluating `expr / 2`while setting `2` to its maximum value of zero.This issue was present for conditions with inequalities for some time,but was introduced for conditions with equalities inhttps://github.com/apache/tvm/pull/11524.  Both types are resolved inthis PR.",5
[docs] Update tlcpack-sphinx-addon (#11891)This integrates the fixes from tlc-pack/tlcpack-sphinx-addon#7,0
"[TIR] HoistExpression, generalization of HoistIfThenElse (#11592)* [TIR][Arith] Use non-inlined bindings when proving conditional* [TIR][Arith] Recognize Var when used as a literal constraint* [TIR][Arith] Added simplification of constrained if_then_else opThis feels like it should definitely be part of RewriteSimplify, butthat will require making CanInlineLet be a virtual function.* [TIR] Implemented HoistExpression transformationThis is a generalized form of HoistIfThenElse, which can also hoistLet bindings, or portions of conditional expressions.  This will beused in upcoming changes to separate compute loops into a slow loopthat handles edge cases and a fast branchless loop.* [TIR] Expressed HoistIfThenElse as special case of HoistExpression* Lint fixes* Fixed breakage in tvmc unit test that relied on pass type* More accurate handling of kUsingBlockVarDidn't correctly reproduce previous behavior.  In addition topreventing hoisting of expressions that use a blockvariable (e.g. threadIdx.x), should also prevent hoisting ofexpressions across a ""thread_extent"" AttrStmt.* Updated comment for HoistExpression pass* Fix linting error",0
"[Relay][Frontend][Onnx] Add support for onnx sequence operators. (#11894)This PR adds support for Onnx sequence operators introduced in opset 11. Specifically I've added converters for `SequenceConstruct`, `SequenceInsert`, and `ConcatFromSequence`, which we found sometimes show up in models exported from Pytorch. For simplicity, I handle these cases by just using Tuples. We may want to consider using the TensorArray ADT eventually instead.",1
[MetaSchedule] Postproc: Rewrite-Layout (#11884),5
[Relay] [PyTorch] Add aten::broadcast_tensors (#11863)* add aten::broadcast_tensors* add entry* fix test,0
"[LLVM] Include LLVM headers in files that use them, not in llvm_common.h (#11888)This is following the same principle we use everywhere else in TVM, thatis, every source file includes headers that it depends on. While includingunnecessary LLVM headers (which may happen by including llvm_common.h)is not actively harmful, it makes the header dependencies much less trans-parent.",2
"[AOT] Calculate used memory at the callsite of primitive functions (#11208)* [AOT] Calculate used memory at the callsite of primitive functionsIntroduces a new pass in the AOT executor called ""AnnotateUsedMemory""which applies liveness analysis to the callsite of each primitivefunction in order to calculate the total size of the live tensors atthis point of execution. The result is provided as a function annotationcalled ""used_memory"", which can be consumed by later stages of thecompiler (e.g. external codegens) to provide more information about thecurrent memory consumption. This can be useful for some optimizations.Change-Id: I8d6b7447498f19260358bbefe34029ddd86b9c89* small fix to file descriptionChange-Id: I0e460f6cf43f9b12ffa5fc66fcb68e55304daeb2* Various improvements addressing commentsIn addition, a new ""io_used_memory"" annotation is added to the mainfunction which refers to the total size of the IO tensors in theprovided module, enabling these to be discounted from memory pressurecalculations where necessary.Change-Id: Iafe9c85d7fc69c77a2115ed4efe7645160387c86* addressing commentsChange-Id: I00f5ba80d5e004076e4c27d39bec143178b3b1dd* add note for dynamic shapesChange-Id: If6409e2953addfc880bcc6d95083b78bdf5a23d0",0
make injective ops's opt schedule applied to every output tensor (#11820),5
"[TIR] Improved error message if tir.Schedule passed to lower/build (#11913)Previously, if a TIR Schedule is passed to `tvm.lower`, the errormessage is returned `ValueError: ('Expected input to be an IRModule,PrimFunc or Schedule, but got, ', <class'tvm.tir.schedule.schedule.Schedule'>)`.  This can cause userconfusion, as the expected class name in the error message does notdifferentiate between between a `tvm.te.Schedule` and a`tvm.tir.Schedule`.  Updated error message to explicitly state thatthis should be a `te.Schedule`.",0
[HEXAGON] Change arch and do not disable assert (#11858),4
[HEXAGON] Add op resize2d for hexagon (#11834)* [HEXAGON] Add op resize2d for hexagon* Reformat* Change to v69,1
Fix curand. (#11901),0
[MetaSchedule] Misc minor fix (#11904),0
[microTVM][zephyr] Increase stack size for zephyr host-driven AoT tests (#11777)* set zephyr stack size to 4096 for qemu_* and zephyr_board targets* use smaller stack size for HW targets,3
[HEXAGON] Initial clip operator for Hexagon (#11549)* [HEXAGON] Initial clip operator for Hexagon* Changes to utils and infra for pylint* Remove unused import* Use tvm.testing.main()* Address pylint error* Fix incorrect function call* Changes to calls to transform_numpy* Add newline at end of file* Add requires_hexagon and rename under topi* Whitespace fix and reduce input size* Remove te tensor arguments* Correct call to tvm.build* Run black formatting,0
fix unit8 in _convert_dtype_value (#11924),0
"[BYOC] InlineCompilerFunctions helper pass (#11923)* [BYOC] InlineCompilerFunctions helper passThe TensorRT BYOC integration needs to 'undo' partitionings in some situations. Add anInlineCompilerFunctions pass to make that robust. In particular, it must undo both the'partitioning' (ie separating out the ""Compiler"" function) and any 'compositing' (ie separatingout small sub-graphs as ""Composite"" functions).Fix misspelled nn.bias_add while there.Note that the current implementation is broken but untested in CI. I have all the testsfixed in a follow-up PR.* - Lints* - Only AOT compilation paths ensure ""executor"" is provided as a Target attribute.",0
"Add missing headers to llvm_module.cc/.h, NFC (#11925)",1
Fix clear-stale-images.sh with multiple worktree. (#11921)* Process substitution doesn't error out in bash.,0
[PyTorch] [Relay] Add aten::pad (#11922)* add aten::pad* fix* fix CI,0
[DNNL] Add bfloat16 type support for dnnl conv2d kernel (#11902),1
[Hexagon] Skip test_avg_pool2d_slice because of segfault on hardware (#11929),3
[TIR][BugFix] Do not bind non-index type value of lets in CompactBufferAllocation (#11828),0
[OpenCL] Change winograd priority and extend split (#11908),4
[ci] Allow skip tag anywhere in PR title (#11714),5
Concatenation corner case fix. (#11907)* Concatenation corner case fix.* lint fixes.,0
[usmp] U3 use case (#11015)* U3Change-Id: Ibc088f19ad1dc9466fc368f8523baa30ee88b7d0* addressed upstream comments* Unit test addedAdded unit test for InterfaceCNode::EmitConstantPool method,1
[USMP] Improve algorithm extensibility (#11880)* [USMP] Improve algorithm extensibility* [USMP] add option for custom_algorithm to avoid PackedFunc on default path* [USMP] add test for custom algorithm* [lint] fix wrong line length* [USMP][test] fix PoolInfo for latest tvm,0
"[microNPU] enable striping for network tests. (#11883)This commit enables the striping for network tests.Currently it requires, storage_rewrite to be run ifstriping is enabled to produce correct results.Change-Id: I12b976bb77d339771f8b5a554817d192e7c99723",3
Add cooldown interval logic for the profiling functional (#11465)* Add cooldown interval logic for the profiling functional.* Remove string serialize hack from RunIndividual functions* Update src/runtime/graph_executor/debug/graph_executor_debug.ccCo-authored-by: Tristan Konolige <tristan.konolige@gmail.com>,0
[ci][docker] Regenerate Jenkinsfile on each run (#11886)This makes it so the generated PRs pass CICo-authored-by: driazati <driazati@users.noreply.github.com>,2
[QNN] Add hardswish int8 impl using table lookup (#11700)* v1* [QNN] Add hardswish int8 impl using table lookup* format* format* fix* fix utest* fix ci error* jostle ci* triggle ci* remote nn* jostle ci* fix,0
"[MetaSchedule] Enable Adapative Training For XGBoost Cost Model (#11892)CostModel retraining is a time consuming part for MetaSchedule tuning, similar to AutoScheduler, we can alleviate it with an adapative way of increasing waiting period between each retraining. This PR introduced an argument called `adpative_training` in `TuneConfig` and the constructor of `XGBoostModel` to enable the capability. Testing tuning scripts are also updated.",1
[Relay][VirtualDevice] Expose WithFields to Python to do proper copy in ExprMutator (#11882)* [Relay][VirtualDevice] Expose WithFields to Python to do proper copy in ExprMutator* [Relay] give FunctionWithFields optional arguments* [lint] fix wrong line length* [lint] missing newline* [doc] add doc string to FunctionWithFields,0
"[Relay] CaptureIndexInSpans debugging pass (#11926)* [Relay] CaptureIndexInSpans debugging passThis pass will update (most) expression nodes to capture their post-dfsindexes. That makes it easy to connect pretty-printed fragments back tothe overall model, and is very handy for Collage which uses post-dfs indexesextensively.* - rename- add header decl",0
Move jenkins/ dir into ci/jenkins and spread docs around. (#11927),2
"[MetaSchedule] Refactor MultiLevelTiling state to allow subclassing (#11931)This PR made `State` in `MultiLevelTiling` inherit `Object`, to allow future subclassing of `State`. Making `State` an `Object` allows instances of `State` and its subclasses to be stored in `std::vector<State>`.",2
[CI] Docs bot now edits previous comments (#11909)This PR improves the docs bot to edit a previous comment instead of making new comments.Fixes #11837,0
[MetaSchedule] Improve Error Message in JSON Database (#11940),0
[microNPU] increase workspace sizes for network tests (#11943)The network tests with striping were reported to be flaky.This commit increases the workspace size to be generous andalso repeats the test case to make sure its not flaky.Change-Id: I134f504250c8fa0bbbcf5f673acec7ffa2ec2f55,3
[PyTorch][Relay] Add aten::cross_entropy_loss (#11935)* add cross entropy loss* fix cross entropy args* fix typo* add class indices* fix CI* fix naming* fix typo,0
export VirtualMachine for Windows (#11947),5
"[testing][hexagon] Better subproc errors (#11853)When a subprocess completes with a non-zero exit code, includeits stdout and stderr text in the Python exception's error message.",0
[TOPI][Hexagon] Implement Argmax Slice Op (#11847)* [TOPI][Hexagon] Implement Argmax Slice Op* run through black* Address initial review comments* Fix variable names in tests* Fix lint issueCo-authored-by: arangasa (generated by with_the_same_user script) <arangasa@hu-arangasa-hyd.qualcomm.com>,0
[RPC] Add Data & Time For RPC Tracker / Server Logging (#11950),1
[Relay] Handle memory scope during lowering from relay level (#11874)Relay expressions can have assigned virtual devices with certainmemory scope. This change landing of memory scope information fromRelay level to tir,4
"[Relay][Pytorch] Add aten::new_ones, aten::new_full, aten::fill_, aten::pad, aten::reshape_as and atem::empty_like (#11896)* add new ops* fix pad* fix pad* remove pad* fix CI* remove doc* fix fill_* add tests",0
[MetaSchedule] Handle 'warp_execution' implied extend of threadIdx.x in VerifyGpuCode (#11949),5
support any shape and axis for log softmax (#11951),2
"[MetaSchedule] Tuning Script Upgrade (#11797)* Support uint8.* Modify tuning functions.* Follow legacy setting, use int32 for uint8.* Add vm support.* Fix vm usage.* Use vm in rpc run module.* Fix lint & stuff.* Fix backend.* Fix ftimer.* Fix lint.* Limit backend choice.* Add try catch.* Display name in rpc try catch.* Support ahb from tune_relay.* Modify scripts.* Fix typo.* Minor fix.* Fix try catch & func name.* Fix utils.* Move utils to tune_utils.* Fix tune_utils.",0
typo fix (#11958)Co-authored-by: Terrance Liang <tailin.liang@outlook.com>,0
fix print attr of null node (#11959),0
"[microNPU] Fix offloading incompatible average pool (#11469)Fixes offloading a few corner cases of average pooling. Specificallynot offloading nn.avg_pool2d when:* The attribute count_include_pad=True* Padding exceeds the dimensions [3, 3, 4, 4]* The pool size is greater than [8, 8] when the pool uses paddingChange-Id: I7be546e28ebe1f17482f3ed3cee56996a71bfcd1",0
[TOPI] [Hexagon] Batch flatten slice op initial version (#11522)* [TOPI] [Hexagon] Batch flatten slice op initial version* Fix lint errors* Fix more lint errors* Fix lint warnings* Fix review comments* Update tests to use util functions* Update __init__.py* Fix review comments,0
[VM] class Executable does not export symbols to dll (#11963)* class Executable of VM exports symbols to dll* restart CICo-authored-by: Valery Chernov <valery.chernov@deelvin.com>,2
"[ETHOSN][CPP-RPC] Link NPU runtime in CPP RPC build (#11946)When building the CPP RPC package with the NPU enabled,`link_directories` fails to find the NPU runtime libraries. This ispresumably because the TVM runtime is linked with the PRIVATEoption in: https://github.com/apache/tvm/blob/main/CMakeLists.txt#L601.Therefore working around this by following the precedent of otherlibraries such as Hexagon and Open CL.Change-Id: Iba2fbc245df18147e3b564ba807ca78c9cc8461d",4
[ci] Redirect sphinx-gallery URLs to S3 (#11839)Co-authored-by: driazati <driazati@users.noreply.github.com>,5
[ETHOSN] Use partition_for_ function when running tests (#11945)Keeps the tests in parity with the partition_for_ function so anychanges are reflected in the tests.Change-Id: I580cc381d382c777484e8251c609867a69da8e67,3
"[BYOC] Handle constants in IRModule-at-a-time external codegen (#11770)I tried to do to the TensorRT integration what #11631 did to the CUTLASS integration, viz: - Make sure all compilation options are passed in Target instances. This helps Collage. - Use a custom pass invoked via RelayToTIRTargetHooks instead of the relay.ext.$toolchain mechanism.   This helps use decouple external codegen from lowering.This PR collects the prep for that change: - TensorRT uses the JSONSerializer visitor to encode each partition function. Previously, when the   visitor encountered a Constant it simply generated and recorded a name for the constant. Then,   completely separately, and via a callback in TECompiler, the function is visited again in the   same order and with the same name generation convention by a ConstantUpdater to actually collect the   bindings, which are then encoded into a ConstLoaderModule to be made available at runtime.   However if all TensorRT compilation is to be done by a stand-alone pass there's no TECompiler callback   hackery available. So I've added a ""const_name_to_ndarray"" attribute to the IRModule of type   Map<String, runtime::NDArray> so that named constants can be accumulated throughout compilation by   any pass which needs to do so. Then the Graph, AOT and VM executors are all updated to merge those   constants into the final runtime artifact   (Compare with ""Constants"", the equivalent attribute for extracting TIR AllocateConsts.) - The TensorRT tests use the create_executor interface but it wasn't quite ready for the   new more general form of passing list-of-targets. - I want TensorRT compilation to work out of the box without the need for any special targets if   all the default options should apply. Go back and make the CUTLASS integration I did follow the   same convention. - To test this I also switched the 'demo' ""ccompiler"" external codegen target to IRModule-at-a-time   style. This means we can test most of external codegen machinery in one place without depending on   any target which may not be enabled in CI (eg TensorRT):     - Target instances are plumbed correctly so compile-time options are available.     - External modules are conveyed to the final export library.     - Constant bindings are conveyed to the metadata module.",1
[skip ci] Disable flaky test `test_empty_like` (#11968)See #11967Co-authored-by: driazati <driazati@users.noreply.github.com>,3
[CI] Skip some additional tests that are failing in the wheel (#11969)This PR skips some additional tests that are failing in the nightly wheel.,1
[ci][docker] Nightly Docker image update (#11857)This bumps the Docker images to the latest versions from Docker Hub.Co-authored-by: tvm-bot <95660001+tvm-bot@users.noreply.github.com>,1
[Hexagon] Disable broken test on physical device (#11960),3
[MetaSchedule] Handle 'warp_execution' in RewriteCooperativeFetch (#11955)Updated `RewriteCooperativeFetch` to handle 'warp_execution' annotation when the extend of `threadIdx.x` is not specified,1
[MetaSchedule] Fix Task Extraction (#11954),0
[PyTorch] [Relay] Add l1 and mse loss function for pytorch frontend (#11978)* add l1 and mse loss function for pytorch frontend* fix CI,0
"[MetaSchedule] Extract workload embedding (#11975)This PR enables extracting the embeddings of the workload in a tuning context, which further strengthens the feature extracting process. Workload embeddings are extracted based on names of each block in the IR module. If `extract_workload` is enabled, the extracted feature vectors will have length 164 + 8 = 172.",5
Further clarify CI docs (#11980),2
[docker] Fall back to tlcpackstaging in bash.sh (#11976)This uses #11775 to make local builds work if they're run in the meantime before CI tags over a new image to tlcpackCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[tests] Fix changed var name from 'target_str' to 'target_names', NFC (#11982)",0
"[Hexagon] Fix use of subprocess.run in _check_call_verbose (#11985)It uses parameters that are not present in Python 3.6, plus itcatches generic exception, which may not have `stdout` or `stderr`members.",0
[Hexagon] Enable int8 vlut codegen for Relay take (LUT) operator (#11693)* Working 8 bit vlut for relay take operator* Formatting* More formatting* clang-format on codegen_hexagon.cc* Update for llvm api* Add return to VisitExpr(BufferLoadNode) function* different llvm api,1
Couple patches to docker/bash.sh after #11976. (#11988)* Use python3 to run determine_docker_images.py * Properly detect presence of CI env var with + expansion.,5
"[ci] Don't skip index-triggered builds (#11915)This code was there to stop Jenkins restarts from doing a repository scan and scheduling a ton of builds. However, I haven't noticed this happening during restarts lately, and repository scans are useful to patch up PRs that didn't get CI run properly (i.e. while Jenkins was down or something).For example in #11914 since this code is there all the messed up PRs needed their CI to be manually re-triggered even though they were detected during the scan.",2
"[BYOC] Switch TensorRT BYOC integration to IRModule-at-a-time using RelayToTIR hook (#11979)* [BYOC] Switch TensorRT BYOC integration to IRModule-at-a-time using RelayToTIR hookThis does for the TensorRT integration what #11631 did for the CUTLASS integration.- All compilation options are captured within the attributes of a Target of  kind ""tensorrt"" (instead of the ""relay.ext.tensorrt.options"" attribute in  PassContext). This means all BYOC configurations options needed by Collage can  be captured uniformly by a list-of-Targets. It also means RPC boundaries (as used  internally at OctoML) only need to worry about maintaining the fidelity of the  Target instance(s) rather than reaching into the PassContext.- Compilation is switched from function-at-a-time (relying on the TECompiler) to  IRModule-at-a-time (using the RelayToTIR target-specific hook mechanism). Though  not strictly necessary for Collage I want to check the path is now clear to  deprecate the support for BYOC in TEComplier.- Get all the TensorRT tests going again, except for a few I've disabled with  x-link to a new issue #11765. CAUTION: The TensorRT runtime is not supported in  CI so many of these tests are cosmetic.- While trying to track down a 'free(): invalid pointer' error in test_tensorrt_int8_exp.py  made the TensorRT allocs/frees more robust, but turns out its also broken in main.  No harm leaving these changes in though.* - Lints* - Woops, fix test* - lints* - Use default tensorrt target if none given in targets list* - fix free error* - accidentally introduced 'transforms' namespace- can't use default Target(""tensorrt"") arg* - D'oh! Include ended up #if protected* - restore mark for test_dynamic_offload- handle missing runtime in versioning- turn test_maskrcnn_resnet50 back on now that we have the  import-torch-first workaround.* - wibble",0
"[LLVM] Remove use of deprecated PointerType::getPointerElementType() (#11984)With LLVM switching to opaque (typeless) pointer types, some functionsrelated to handling typed pointers are being deprecated (and will beremoved).The DWARF debug information does express pointee type. When constructingthis information from opaque pointers in LLVM IR, the pointee type needsto be obtained from somewhere else (not the pointer).Change the debug info generation to use the original PrimFunc to obtainthe necessary type information. This will work with older versions ofLLVM as well.",0
[Relay] [PyTorch] Add aten::tril and aten::triu (#11890)* add trilu* update triu and tril; fix empty* fix lint,0
add missing narrow down of index within conditions (#11942),1
"[MetaSchedule] Enhance AutoInline for Spatial Task (#11996)Previously, Auto-Inline on CPU will only inline according to strictconditions, for example, ordered index mapping. This is generally goodpractice to do so, but on the other hand, there is no much benefit tostop inlining only due to some restrictive conditions for pure spatialsubgraphs. By doing so, we also save some search trials on pure spatialsubgraphs so that more can be allocated to more important ones.",5
[COMMUNITY] Hongyi Jin -> Reviewer (#11998),3
"[TIR] Add sugar method `Schedule.work_on` (#11999)This PR introduces `Schedule.work_on`, which instructs`Schedule.get_block` to find the correct PrimFunc to retrieve fromwithout having to specify `func_name` in every time if the PrimFunc'sname is not `main`.",1
"Enhancement for fold_scale_axis and dnnl_json_runtime (#11815)* enhance WA in dnnl_convolution, support crop for tensor with mismatched groups and OC* add missing param checks for conv2d, conv3d* fix lint",0
[Adreno] Modify default AutoTVM params for conv2d (#12005),5
[Frontend][TFLite] Add support for NonMaxSuppressionV5 op (#12003)* add nms_v5 op for TFLite* add a test for the TFLite nms_v5 op,1
[BYOC-DNNL]rewrite downsize blocks for rensetv1 to get better performance (#11822)* rewrite downsize blocks for rensetv1 to get better performance* fix lint,0
[microTVM] Autotuning performance tests (#11782)* Common autotuning test* Autotuned model evaluation utilities* Bugfixes and more enablement* Working autotune profiling test* Refactoring based on PR commentsBugfixes to get tests passingRefactor to remove tflite model for consistencyBlack formattingLinting and bugfixesAdd Apache license headerUse larger chunk size to read filesExplicitly specify LRU cache size for compatibility with Python 3.7Pass platform to microTVM common testsBetter comment for runtime boundStop directory from being removed after session creation* Use the actual Zephyr timing libraryUse unsigned integerAdditional loggingTry negationTry 64 bit timerUse Zephyr's timing libraryFix lintingEnable timing utilities,0
add aten::randn (#11994),1
[TIR] Make conversion from Integer to int64_t explicit (#12010)* [TIR] Make conversion from Integer to int64_t explicit* Fix compilation errors* Fix compilation issues in cpptest* Fix SPIRV compilation errors,0
Fix infercorrect layout in Layoutrewrite and improve naming. (#12007)* Fix infercorrect layout in layoutrewrite.* Compatibility issue.* Fix lint.* Better naming and detailed comments.* Add unittest.,0
"[CI] Allow command-line argument or TVM_BUILD_PATH for C++ unittests (#12011)* [CI] Use command-line argument or TVM_BUILD_PATH for C++ unittestsPreviously, the `ci.py` script would execute all C++ unit tests in the`""build""` directory, regardless of the docker image being used.  Thischange allows a caller to specify the build directory to be used by`task_cpp_unittest.sh`, either by the command line or by using thesame `TVM_BUILD_PATH environment variable as used by the top-levelMakefile, and passes this argument from `ci.py`.  To preserve theexisting behavior for the pre-commit CI, if no argument is passed andif the `TVM_BUILD_PATH` is undefined, `task_cpp_unittest.sh` defaultsto the `""build""` directory.Python unit tests executed through `ci.py` used the `TVM_LIBRARY_PATH`environment variable, and were not similarly affected.* Remove `name=name` in format scriptCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>* Fix lint error* Use default expansion of TVM_BUILD_PATHOtherwise, `set -u` rightly errors out for it being undefined.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>",0
[USMP] HillClimb stability patch (#10547)This patch increases stability of the hill climb allocation algorithmChange-Id: I56414ae661fa856baeddce00f4717a9f5a9e2954,4
[Topi] [Hexagon] Conv2d slice op initial version (#11489),5
"[microNPU] Calculate memory pressure for microNPU external functions (#11209)* [microNPU] Calculate memory pressure for microNPU external functionsDuring the microNPU compilation stage, the ""used_memory"" annotations onexternal microNPU functions are read to determine a memory pressurevalue. This value is passed to the cascader to better approximate thememory available for the optimization.Change-Id: I11a311b0005e785637014cb451f4aed96edcda26* fix get size from memory regionChange-Id: I41acfc83f05b2204075edb99f86a0eecaba00f71* add test case for full offloadChange-Id: If3e672d402ab237fa82e34761bb972d2e9483ba9",0
[Arith] Allow constant values in InverseAffineIterMap (#12026),5
[TVMScript] Doc Base Class & DocPrinter Scaffolding (#11971)This PR addes:- Doc base class- DocPrinter base class- PythonDocPrinter- LiteralDoc and its support in DocPrinterTracking issue: #11912,1
"[Pytorch] add aten::rnn_tanh, aten::rnn_relu (#12017)* emptycommit 2nd try* dev* comments* format* formatCo-authored-by: yuanfz <42092999+FZYUAN-1@users.noreply.github.com>",1
[TIR] Revert #11428 and move loop dependent alloc extent check after region union (#12019),4
"[MetaSchedule] Support ApplyHistoryBest Direct Dispatch (#12016)This PR introduced a new argument for `ApplyHistoryBest`'s `Query` interface to allow direct dispatch without querying the database, would be useful for debugging and benchmarking without interference.",0
[TOPI] [Hexagon] Reshape slice op (#11983)* Reshape slice op. This patch adds the initial python implementation reshape slice op for hexagon.* Add tests for reshape op,1
[Fix] fix python setup.py file bug (#12000)* fix setup.py bugSigned-off-by: Zhengqiang Yin <codle@outlook.com>* remove data_files field* keep a init setup_kwargs,0
[MetaSchedule][Minor] Stability Improvements (#12014)* Fix tuning util for uint8.* Change to check runner_result.* Revert change to let cost model learn.,0
[MetaSchedule][Testing] Test search space of conv1d (#12032)* [MetaSchedule][Testing] Test search space of conv1d* Add checks for trace roundtripping,1
[Pylint] Pylint integration_tests folder (#11672)* add folder to pylint* add init py* lint test_arm_mrpofile_dsp.py* one more change to tests/python/integratoin/test_arm_mprofile_dsp.py* add test_dot* test_ewise_fpga.py* test_ewise.py* test gemm* test_lower.py* test_meta_schedule_auto_tensorize.py* test_reduce.py pt1* test_reduce.py pt2* test_scan.py* test_tuning.py* test_winograd_nnpack.py* final test pass* comments* clean up test_lower more,1
[TIR] fix crash when comparing IntImm to None (#12034)* [TIR] fix crash when comparing IntImm to None* [TIR] raise ValueError when comparing IntImm to None* fix: add test for non-pytest run,0
[MetaSchedule][Testing] Add unittests for C1D search space (#12036),1
"[TVMC] Updates TVMC tutorial with input shape information (#12031)The tutorial is currently broken, probably because updates in themodel, so we now need to pass input shape information.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>Co-authored-by: Liam Sturge <Liam.Sturge@arm.com>",1
[microNPU] Test averge pool partitioning (#11965)Follow up for #11469.Change-Id: I474b1d43d3abc6b66d35ebcf3ad6fea50becfb97,3
[TIR] Avoid unnecessary dtype escalation in loop splitting (#12035)This PR introduces a type check to cast loop split decisions (sometimes given as `int64`) back to a smaller datatype when the loop variable's data type is smaller. This issue usually happens during reloading a trace from disk using JSON database and causes the failure of `CompactBufferAllocation` pass.,4
[MetaSchedule][Test] Add unittests for C2D (#12043),1
[Texture] Add memory scope entity into graph JSON/runtime (#11875)This PR is a split part of origin PR #11357Co-authored-by: Chris Sullivan <csullivan@octoml.ai>,1
[MetaSchedule][Test] Add unittests for C3D (#12046),1
[MetaSchedule][Test] Add unittests for CAP (#12047),1
[BYOC-DNNL] support more post-ops (#12002)* support post-op swish* support post-op clip* enhance get_shape and get_dtype in dnnl.py to support efficientnet* add checks for with_eltwise whether in supported list* fix lint* fix test,0
"Several type mismatch fixes and checks (#12041)* Compute common type for shape elements in BroadcastHelperThe corresponding dimensions in the input/output tensors in a broadcastoperations may have the same value, but different types (e.g. int32 vsint64).When the broadcast helper tries to unify the dimensions it also needsto compute the common type to hold the dimension.* Cast and simplify both members of `Range`Only the `min` member was type-casted, which could lead to ranges withdifferent types for `min` and `extent`.Move the casts to the argument of Simplify, so that they can be eliminatedif they aren't needed.* Type-check iv domain ranges, use cast only if needed in MakeLoopNestIn some cases the domain ranges had the `min` and the `extent` valuesbe of different types (e.g. [(int64)0, 32)). This is an error, and itcan lead to compilation failures later on. Add a check for equal typeshere to catch this early.Also, only add the cast operation when the desired type differs fromthe current one to keep the expressions simpler.* Check that variable and substituted expression have same typesAdd a check to IRSubstitute to detect when the type of a variable andthe type of the expression to replace it with have different types.* Add testcase* [TVMScript] Use void for lambda parameters, allow mismatch in SubstituteWhen the script parser deals with lambdas, it creates Var objects for eachparameter. Their actual types are not known at the time, and the properlytyped variables are subtituted in the body later. Since the default dtypeof a Var is ""int32"", this could lead to a type mismatch in Substitute.To deal with this scenario, use ""void"" for newly created Vars in theparser, and add an exception to Substitute to allow replacing void Varswith expressions of any type.* Fix type error in test_reduce_combiner_simplify* Restart CICo-authored-by: Jiawei Liu <jaway.liu@gmail.com>",0
Add xgboost version restriction (#12050)Co-authored-by: jiabeizhao <jiabeizhao@tencent.com>,1
enable bmm (#12018),5
"[MetaSchedule] Added a cost model (#11961)In this PR, I added a cost model based on SegmentSum MLP, which can be used for pre-training or integration with TVM.",1
[Frontend][TFLite] PreLU alpha can be an expr (#11879)* [Frontend][TFLite] PreLU alpha can be an expr* [Frontend][TFLite] handle both cases of PreLU alpha param,5
[microtvm][RVM] Refactor Arduino/Zephyr into one RVM (#12023),5
[CMSIS-NN][Perf] Converted Relay Conv2D into CMSIS-NN Depthwise (#12006),5
"[Collage] SubGraphs (#11981)* [Collage] SubGraphsSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.Collage works in units of 'sub-graphs', which are potential partitions of theoverall Relay model. This PR introduces SubGraph (an arbitrary partitioning, withoutany implication about how it is to be represented), it's companion SubSubGraph(implying a representation as a function), and some supporting odds 'n ends.* - make Integer <-> size_t conversion explicit- make 'Compiler' name explicit* - fix namespace ambiguity* - review comments",0
Fix node.func to node.funcs on parser.py (#12053),0
[ci][docker] fix the path of custom toolchain in ci_qemu for csinn2 (#11905),0
[relay] Changed 'name' field to 'registry_name' for Executor and Runtime (#10466)* [relay] Changed Executor and Runtime 'name' field to 'registry_name'Changed 'name' field to 'registry_name' for Executor and Runtime pythonwrappers as it clashed with tvm object attribute 'name' which made the latterinaccessible from PythonChange-Id: I917755753549edfe1d3090ca9ca4512de552c4bdchanged name to registry_nameChange-Id: I9feb5b33b7b6f6f8421902e5721167f585cc4193* more fixed unit testsChange-Id: Ie2e96297fda119e1b726b196a59deae95b263a07* typo fixedChange-Id: Id579c50ab58dfb25fa18436265e0701ebbd9d554* renamed registry_name to flag_registry_nameChange-Id: Iabbd81069959f05c073f9dbc8d10fb31dd05f7a3* bugfix,0
"[LLVM] Fix build errors in CodeGenCPU::AddDebugInformation (#12054)This code is guarded by TVM_LLVM_VERSION >= 50 and < 70, so the errorswere not detected in local tests or in CI.",0
[AOT][BUG] Only include extra headers if the constants array is needed. (#12061),0
[microNPU] Add MergeConstants pass (#12029)* [microNPU] Add MergeConstants passChange-Id: I1ff51d8147fba8c66d442a370b9f058e9b2758d8* Fix errors and warningsChange-Id: I29f68f83a73fa00ca34ed0ab2321c53c6b761137* Address commentsChange-Id: Iad59107d5abdec6b079c6fd4ab48c6bffbb5e0bb* Fix lint errorChange-Id: Ie5caf506337de01e169d6f422e4682eefbd93241,0
"[Collage] PartitionRule (though without CombinePartitionRule) (#11993)* [Collage] PartitionRule (though without CombinePartitionRule)See https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.(Special thanks to Matthew Barrett for authoring partition_rule_test.cc and suggesting a PRpartitioning strategy.)Collage uses a small 'combinator library' of PartitionRule to decribe how candidate partitionscan be extracted from a model for measurement and comparison. This introduces most of thatmachinery, however we defer the all important 'CombinerPartitionRule' for the next PR. Thusthe rules at this stage can only express the sorts of DFPattern-based rules we find in mostBYOC integrations, and cannot describe rules more traditionally associated with operator fusion.Based on #11981.* - Backport improvements to partiton_rule_test.cc* - Oops",3
[Frontend][TFLite] respect out type of Shape op (#11877)* [Frontend][TFLite] respect out type of Shape op* tests: update for changes to tflite shape handling* lint fix,0
[QNN] Replace nn.leaky_relu with qnn.leaky_relu (#11930)* [QNN] Replace nn.leaky_relu with qnn.leaky_relu* jostle ci* fix typo,0
[QNN] Use sigmoid Lookup Table method instead of fallback to fp32 (#12038),2
[docs][tvmc] Fix ResNet50 model URL (#12040)Fix the ResNet50 Models in both tvmc tutorials so that the commandssuggested will work fine.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>Co-authored-by: Liam Sturge <Liam.Sturge@arm.com>,0
fix some typo in conv2d.py (#12067),0
[MetaSchedule][Test] Add unittests for DEP (#12071),1
[Topi][Hexagon] Implement Cast F32ToF16 and F16ToF32 Slice Op (#11561),5
"[Relay] Move TOpPattern registration for nn.* to C++ (#12072)* [Relay] Move TOpPattern registration for nn.* to C++Some of the Collage machinery is best tested from C++, butrequires Relay ops to have their ""TOpPattern"" registered.However since the nn.* ops register on the Python side testscan't rely on those ops.The easy fix is to just move the registration to theRELAY_REGISTER_OP block. However since kOpaque is thedefault I did not preserve those registrations.There's still a few dozen more exotic ops still registeredon the Python side. I've left them be.* - D'oh! Even kOpaque ops must be registered.",0
[MetaSchedule][Test] Add unittests for DIL (#12077),1
[Hexagon] Enable broken tests (#12073),3
[COMMUNITY] Add driazati key for release (#12076)As per https://tvm.apache.org/docs/contribute/release_process.html#id3,1
"[LLVM] Update creation of llvm::DebugLoc, remove TVM_LLVM_VERSION < 70 (#12069)* [LLVM] Update creation of llvm::DebugLoc, remove TVM_LLVM_VERSION < 70* Properly deal with ""handle"" type* Emit correct subroutine flags* Fix llvm testcase to account for presence of debug metadata",0
[CMSIS_NN] Align CMSIS-NN in TVM to TFLu SHA (#12030)* [CMSIS_NN] Align CMSIS-NN in TVM to TFLu SHAChange-Id: I7bb3b92196ad9f1a22eee87d704545e72b79ca0b* Updated CMSIS SHA to CMSIS TOTChange-Id: I0fec18e823478da991d49aa782f58f1c2f6212ba,1
"[TOPI, x86] Properly handle fused ops in TE softmax schedule   (#12015)* fix x86 softmax fusion* properly handle the case where softmax and fuseed op having different layout* add test",0
fold const or empty iter partition (#12080),5
[TIR][Schedule] Refactor Tensorize (#12070)* Refactor blockize* Refactor tensorize* Address review comments* typo* rename variables according to review,1
"[Arith] Updated BufferDomainTouched to use IRVisitorWithAnalyzer (#11970)* [Arith] Allow binding of Var in IntSetAnalyzerThe other four subanalyzers in `arith::Analyzer` can each be providedwith variable bindings/constraints that are remembered internally.This adds the same capability to `IntSetAnalyzer`, rather thanrequiring users to independently track and maintain a `Map<Var,IntSet>` containing the domain of each variable, and appliesbindings/constraints alongside the other subanalyzers.* [Arith] Updated IRVisitorWithAnalyzer to mimic IRMutatorWithAnalyzerPreviously, `IRVisitorWithAnalyzer` did not allow subclassing, andcould only be used to collect bounds of variables along an entirestatement, and could not be used to perform scope-dependent analysis.This commit removes `final` from `IRVisitorWithAnalyzer` and providesthe same scope-based constraints/bindings during iteration as areprovided by `IRMutatorWithAnalyzer`.* [Arith] Moved IRVisitorWithAnalyzer to tvm::arith namespaceChanging for consistency, since `IRVisitorWithAnalyzer` it is part ofthe `src/arith` directory and the analogous `IRMutatorWithAnalyzer` isalready part of the `arith` namespace.* [Arith] Updated BufferDomainTouched to use IRVisitorWithAnalyzerThis used the earlier changes to allow subclasses of`IRVisitorWithAnalyzer`, and to expose binding/constraints to`IntSetAnalyzer`.* Avoid accidental Bind with dynamic Range* [Arith] Do not visit SelectNode in IRVisitorWithAnalyzerBecause both sides of a `Select` node are visited regardless of thecondition, the `SelectNode::condition` should not be treated as aknown value.* [Arith][IntSet] Track global and scope-dependent bounds separatelyResolves a bug that was found in CI, where an earlier scope-dependentconstraint was treated as a conflict by a later global bound.* [Arith] Recovery function for each subanalyzerThis way, if a subanalyzer throws an exception during`EnterConstraint`, the other subanalyzers are still appropriatelybacked out of the constraint.* [Arith][IntSet] Use CanProve instead of CanProveGreaterEqualThe `min_value - max_value` in the `CanProveGreaterEqual` argument canresult in an exception being thrown for unsigned integers wheresubtraction would wrap.* [Arith] Allow vector expressions in IntSet::operator(PrimExpr)Since these are tracked when lowering expressions, should allowpost-vectorization expressions.To maintain previous behavior, this only applies when using theautomatically tracked `Map<Var, IntSet> dom_map_`.  If an explicitdomain map is passed, the previous behavior of raising an error forvectorized expressions still occurs.* Avoid comparisons between integer and handle datatypes* [Arith] IntSet, Combine() extensionPreviously, the Combine() method didn't handle values without a knownlower bound, for boolean operators.* Added docstring* Naming consistency of `IntSetAnalyzer` methods.To be consistent with other subanalyzers, using ""Update"" whenproviding the analyzer with the same data structure as is usedinternally, and ""Bind"" used when providing it with something that mustbe converted to the internal data structure.",0
"[Collage] CombinerRule and CandidatePartition::EstimateCost (#12078)* [Collage] CombinerRule and CandidatePartition::EstimateCostSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.We complete the PartitionRule sub-class hierarchy with the addition ofCombinePartitionRule, which allows disjoint candidate partitions to beunioned based on simple rules. - By TOpPattern kind, eg a kOutElemwiseFusable and kBroadcast. - A tuple argument with injective fields. - The projection from an injective group (obviously of tuple type) - Combinations of the above.These let us mimic many common fusion strategies, including TVMs, so thatthe candidates explored during Collage search are as large as possible toexpose possible fusion opportunities but no larger.Also completes CandidatePartition with the EstimateCost method, which isused during search to construct a stand-alone IRModule for latency estimation.Finish units tests for PartitionRule and CandidatePartition.* - fix relay.collage ffi prefix.",0
"[TVMScript] Add ObjectPath class (#11977)Motivation:Same IR node object can be referenced in several different contexts inside a larger IR object. For example, a variable could be referenced in several statements within a block.This makes it impossible to use an object pointer to uniquely identify a ""location"" within the larger IR object for error reporting purposes. The `ObjectPath` class addresses this problem by serving as a unique ""locator"".Tracking issue: https://github.com/apache/tvm/issues/11912",0
[TOPI] Allow conv definition to have custom kernel layout (#11936)* [TOPI] Allow conv definition to have custom kernel layout* add tests* fix* fix,0
"[Relay] Add RecoverVirtualDeviceMap helper (#12085)* [Relay] Add RecoverVirtualDeviceMap helperDevice planning is halfway through the transition to using the virtual_device_field on every expression node to capture device/target/etc info. In the meantimeit is necessary to derive from a 'device aware' visitor so as to track deviceinformation. In Collage this is not feasible, so as a stop gap allow the mapfrom expression nodes to virtual devices to be reconstructed as a stand alonemap.This code can be removed once expr->virtual_device() is the canonical representation.* - review comments",1
Add tensorflow Einsum op converter (#12064)* Add tensorflow Einsum op converter* fix lint* fix lint,0
[WIN] export void Configure(...) symbol for Windows (#12091)Extends TVM API for clients of Windows. It helps configure thread pool on native side on Windows OS.Co-authored-by: Valery Chernov <valery.chernov@deelvin.com>,5
[BugFix] Use shape dtype on ArgReduce to determine return type (#12083)Fix ArgReduce automatic return type inference by forcing it to use thedatatype of the shape of the Tensor instead of the fixed Int32.Including additional tests.,0
[ci] Override Request in pytests (#11974)This follows on #11839 to apply it outside of docs and to tests running under pytest insteadCo-authored-by: driazati <driazati@users.noreply.github.com>,2
[ci] Add a manual retry for conda setup (#12058)This makes it so the conda setup will re-run entirely in case of failures like in https://github.com/apache/tvm/runs/7287493088. [This issue](https://github.com/conda-incubator/setup-miniconda/issues/129) has some more context but there doesn't seem to be a better way to do a retry than re-running the whole thing since the settings in `conda/condarc` are picked up but they don't help for this particular issue.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[ci] Re-run failed tests on failure (#12055)* [ci] Re-run failed tests on failureThis uses [pytest-rerunfailures](https://github.com/pytest-dev/pytest-rerunfailures) to retry failed tests. This should help alleviate flakiness for issues like segfaults in ethosu tests or flaky numerics. This obviously isn't as good as fixing the tests themselves, but it's an easy way to fix the most pressing issue (the cost of developer time to wait for, check, and re-run CI) while keeping the signal from the times the tests do fail consistently.* Remove changes to pytestCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[Releay] Fix on_device call for explicit virtual_device (#12088),0
adding Alexey (#12090),1
[MetaSchedule] Add MultiLevelTilingTensorCore rule for auto-tensorization on CUDA (#12059)* [MetaSchedule] Add MultiLevelTilingTensorCore rule for auto-tensorization on CUDACo-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>* address comments* update intrin registrations* fix tests* address comments* add warning when storage align doesn't work* remove printCo-authored-by: Bohan Hou <32121147+spectrometerHBH@users.noreply.github.com>Co-authored-by: Hongyi Jin <3231950289@qq.com>,0
"[Collage] CollagePartition pass (#12086)* [Collage] CollagePartition passSee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.This adds the main CollagePartition pass, which: 1. Inspects all the targets in the CompilationConfig and builds    PartitionSpecs describing how to generate speculative CandidatePartitions    for them. 2. Runs the above rules on the model to collect all the candidates. 3. Eliminates candidates whose target contradicts any constraints already    imposed by, eg, device planning. 4. Eagerly estimates the cost of each candidate. 5. Performs a shortest path search to chose an 'optimal' set of candidate    partitions so as to minimize estimated model latency, such that every sub-expression    node is contained in exactly one candidate partition. 6. Coalesces adjacent optimal candidates which ended up on the same target. 7. Rewrites the model according to the chosen optimal partitioning.As for the existing partition_for_<external codegen name> methods, the result ofCollagePartition can then be built using regular TVM.Very special thanks to @mbaret for authoring test_pass_collage_partition.py.Logic to prune the candidates after step 3 will be in a follow up PR since itdeserves its own testing. A demonstration driver will also come as a follow up.* - lints* - more lints* - use the _ffi_api properly",1
Fix bug that disabled cuda integer dot product. (#12099),0
[TVMSCRIPT] Make ceildiv available from tvmscript (#12096),5
Enable conv family fused with gelu (#12106),5
"[Relay, Op] Add conv2d generic layout op strategy when meta schedule is enabled (#12104)",1
"[Relay] Allow Primitive functions to carry virtual device annotations in PlanDevices (#12095)* [Relay] Allow Primitive function to carry virtual device annotations in PlanDevicesPreviously Primitive=1 functions not analyzed and calls to such were completelyunconstrained. With this change at least any virtual device annotation on the functionare respected and accounted for in calls, even though the body is not analyzed.This may help with piggy-backing on PlanDevices for doing memory scope analysis, sinceit is now possible to express cross-scope functions on Primitive functions. HoweverI believe there are other issues to deal with in addition to this one.* - comments* - also canonicalize targetsWhen including virtual device annotations in test relay programs theannotation will typically use a target which was used as an input tothe make_compilation_config helper, but due to various canonicalizationmake not be pointer equal to the final structurally equal target which endsup inside the constructed CompilationConfig. However VirtualDevices usepointer equality when comparing their target field.So make sure the notion of CanonicalVirtualDevice also accounts for canonicaltargets.* - update unit test to reflect the Ardreno example* - trivial cleanup",1
"Add member object accessors to With<> (#12100)* Add member object accessors to With<>Currently the With<> template constructs an object, but gives no accessto it, so it's only applicable to situations where we rely on the side-effects of creating the object.* Restart CI",1
[Collage] PruneCandidates and demo_collage_partition.py (#12105)* [Collage] PruneCandidates and demo_collage_partition.pySee https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md.This completes our checkin of our Collage 'sketch' branch into main. Special thanksto Matthew Barrett for his help getting this over the line.The only C++ functionality added here is for 'pruning' candidates. This is a somewhatspeculative algorithm (and I've called that out in the comments) which tries toelide candidate partitions which will 'obviously' not contribute to the final optimalpartitioning. For largish models such as GPT2 this can significantly reduce the number ofcandidates we need to actually measure latency on. I beefed up the MockCostEstimator tomake it possible to assert pruning occured from within the test_pass_collage_partition.pyunit test.The rest of this PR adds the demo_collage_partition.py driver file we've been usingto test and measure perfomance differences against various baseline (though onlyfor the CUDA ecosystem). To eliminate loading time the models of interest are directlyexpressed in Relay text form in menangerie.py.* - lint,1
"[Relay] Allow partial virtual device annotations. (#12107)* [Relay] Allow partial virtual device annotations.Previously CompilationConfig::CanonicalVirtualDevice requiredthe argument virtual device to contain a device type. Howevernow that virtual devices may contain memory scopes that'sunnecessarily strict.With this change it is possible to write virtual deviceannotations with just memory scopes, and let PlanDevicesflow those constraints along with the usual device constraints.* - Make sure CanonicalVirtualDevice reuses FullyUnconstrained",4
[MetaSchedule] Allow MultiLevelTilingTensorCore rule to specify multiple tensor intrin groups (#12113),5
[TOPI] [Hexagon] Uint8 Reshape and batch flatten slice ops (#12037)* [TOPI] [Hexagon] Uint8 Reshape and batch flatten slice ops* Fix documentation,0
fix typo (#12115),0
[TIR][BugFix] Fix a wrong use of T.exp in test_compute_inline_opaque_access_with_tvm_access_ptr. (#12117)Co-authored-by: sqing <qing.siqi@intellif.com>,0
"[TIR] Moved PrimExpr operator overload from op.h to expr.h (#11973)* [TIR] Moved PrimExpr operator overload from op.h to expr.hIf a compilation unit includes `<tvm/ir/expr.h>`, but does not include`<tvm/tir/op.h>`, the operator overloads for `ObjectRef` are declared,but the operator overloads for `PrimExpr` are not.  In this case, anyuse of `expr_a == expr_b` would use `ObjectRef`'s implementation andcompare reference equality of the two expressions, rather thanreturning a `PrimExpr` that represents the comparison.  By having theoperator overloads in the `<tvm/ir/expr.h>` header file, directlyadjacent to the `PrimExpr` declaration, the correct overload must beavailable whenever the `PrimExpr` can be used.Even though this would only impact `operator==`, `operator!=`, and`operator<`, the three operators defined for `ObjectRef`, this PRmoves all operator overloads to `expr.h` for consistency.The named version of the operators (e.g. `tvm::add`) do not haveoverloaded variants, and so they are intentionally kept in`<tvm/tir/op.h>`.* Explicitly convert TVMRetValue to bool in target.ccNeeded to avoid ambiguity between `TVMRetValue -> bool` conversion and`TVMRetValue -> int -> PrimExpr` conversion.* Used vector/unordered_set to track BufferInfoExtractor::call_order_Use of `std::set<Call>` had ambiguity between `operator<` by`PrimExpr` or by `ObjectRef`.The comment for `call_order_` implied that the previous usage of`std::set<Call>` was intended to have a de-duplicated list in theorder of occurrence.  However, the `std::set` was ordered by`ObjectRef::operator<`, not by insertion order.  Switching to using a`vector` for ordering and `unordered_set` for de-duplication resolvesthis issue, and also removes the use of `operator<`.* Remove C-style cast to fix lint error",0
"[AUTOSCHEDULER,FIX] Calculate arithmetic intensity without log scale (#12079)* [AUTOSCHEDULER,FIX] Calculate arithmetic intensity without log scaleIn autoscheduler's featurization, arithmetic intensity was incorrectlycalculated as log(FLOPs) / log(bytes). This change removes the logs soarithmetic intensity is FLOPs / bytes.* slog arith inten",0
[ci] Re-run flaky tests on failure (#12108)This is a follow up to implement the library added in #12055Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[Target] Add target_parser to TargetKind (#12119)This adds the `target_parser` as described in https://github.com/apache/tvm-rfcs/pull/71, which parses an incoming `TargetJSON` and produces a new configuration for generating the final `Target` object from.Marks `set_attrs_preprocessor` as deprecated and errors if both `set_attrs_preprocessor` and `set_target_parser` exist together.",0
[Texture] Add 2d memory support into static memory planner (#11876)* [Texture] Add 2d memory support into static memory plannerCo-authored-by: Chris Sullivan <csullivan@octoml.ai>* Add test verifying GraphPlanMemory work for 2d memoryCo-authored-by: Chris Sullivan <csullivan@octoml.ai>,1
[microNPU] Add support for hard swish (#12120)Adds support for hard swish by populating a LUT similar to Vela'simplementation.Change-Id: I7ca15a3e21bc91c1b41cdd4547fabaa00de96e90,1
[CMSIS-NN] Fix typo in EmitPool2D (#11702)Co-authored-by: Philipp v. K <phvankempen@gmail.com>,0
"[Target] Add ""features"" property to Target (#12121)This adds a generated property ""features"" to the `Target` which cancontain a read-only list of available features in line withhttps://github.com/apache/tvm-rfcs/pull/78.Features are re-generated upon parsing into a `Target` object rather than beingattached as `attrs`. The `Target` JSON is therefore stored without theinferred `features` attached.",1
support overlapped itersum (#12039),5
[Pylint] Making hexagon tests pylint compliant Part 1 of N (#12082),3
"TVMC: Add new text/relay frontend (#10941)* TVMC: Add new text/relay frontendThis feature enables passing a textural representation of a relay module to the tvmc command line.Example: `tvmc compile relay.txt --target c --runtime=crt --executor=aot --executor-aot-unpacked-api=1 --pass-config tir.disable_vectorize=1 -f mlf`Currently it is not possible to supply parameters as it is mainly intended to be used for testing certain relay functions or operators. In the future (with minor changes to the tvmc frontend api) params could be passed via an additional i.e. `params.bin` fileThis commit also adds minimal unit testing of the added feature.Resolve PR commentsTVMC: add warning if relay frontend is used* [TVMC] populate parameters with random values instead of ones* [TVMC] Relay frontend: do not populate input tensor buffers if --input-shapes is providedThis prevents that the constants inputs are used for Constant folding,thus changing the complexity of the model.If there would be a way, to distinguish between model inputs and parameter thisworkaround would not be required.* [TVMC] Relay frontend: check provided file contents before calling tvm.parser.fromtext()",1
[Fix] post-fix incre/decre should not return reference (#12128),0
"[MetaSchedule, Testing] Generalize in/out dtype of testing te workloads (#12122)* [MetaSchedule, Testing] Generalize in/out dtype of testing te workloads* Fix tests",0
[microTVM] Make Arduino API server obey timeout (#12074)* Make Arduino API server obey timeout* Pass arm_cpu as default option to micro testingSyntax fixIncrease Zephyr default stack size for create_aot_session* Set write_timeout when appropriate* Fix unit tests and lintingCheck whether arm-cpu flag is breaking testsUpdate tests for arm-cpu flag,0
[Hexagon] Slice op relu (#11449)* Add support for relu slice op.* Format code* removing out_shape in relu def and lint issues* removing out_shape in relu def and lint issues* Changes as per the new formatCo-authored-by: Venkat Rasagna Komatireddy <89959097+rasagna-quic@users.noreply.github.com>Co-authored-by: Venkat Rasagna Reddy Komatireddy <rasagna@hu-rasagna-hyd.qualcomm.com>,1
[microTVM] Arduino retry on flash failure (#12114)* Retry on flash failure* Add unit test* Style improvements for Arduino api server,1
"[LLVM] Clarify the status of pointers to llvm::Module in LLVMModule (#12123)LLVMModule has two members that refer to llvm::Module:1. llvm::Module* mptr_, and2. std::unique_ptr<llvm::Module> module_.The mptr_ member is always valid and it points to the llvm::Module.The unique pointer takes care of the ownership of the llvm::Module, anddeletes it when no longer needed. However, once JIT is initialized, ittakes over the ownership of the llvm::Module, and the module_ memberbecomes null.To avoid checks for null, use the raw pointer whenever accessing thellvm::Module, and only use the unique_ptr as the owner/deleter.The member names are changes to reflect their roles:  module_ -> module_owning_ptr_  mptr_   -> module_",4
"Revert ""support overlapped itersum (#12039)"" (#12137)This reverts commit 3e7a2ad9568a79fb775c0ca9d09a3fa2f51f792f.",5
Fix #12039‘s broken cases (#12143),0
[TIR][Schedule] fix tensorize example (#12146),0
[UX][TIR][Schedule] enhance function annotation for tir primitive (#12147)* [UX][TIR][Schedule] enhance function annotation for tir primitive* lint* fix mypy* fix pylint,0
[CMSIS-NN] Support for passing cpu flags to Arm(R) Corstone(TM)-300 software (#12132)Change-Id: I12312dc0fb27ac991f1a25544f226cd00b5f9281,4
"[ETHOSN] Supply output tensor to issupported checks (#11944)Some operations were being offloaded when they are not supportedby the NPU, for example mean could get offloaded with differentquantization parameters for the input and output which is notsupported. Consequently, this meant that there would be a failureduring compilation or an output mismatch at runtime. Fixing this bysupplying the output information to the issupported checks thatdetermine whether an operation should be offloaded.Change-Id: I8896f83dad3d1c837fbb85bf2836fc9325f9dec9",0
[arith][BugFix] Fix simplify input PrimExpr of DetectClipBound (#12150),0
fix T.Ptr[T.void] for packed api roundtrip (#12118),0
[Adreno] Fix winograd schedule to support prime shapes > 4 (#12157),0
[HEXAGON] QCOM hexagon library (qhl) (#12149)* qcom hexagon library (qhl)* fix lint errors* fix lint errorsCo-authored-by: aakaverm <aakaverm@qti.qualcomm.com>,0
[hexagon][testing] Better pytest ID strings (#12154)- Add utility functions to allow more human-readable pytest test IDs.  Helpful when ID strings become too large for humans to easily read.- Update the `test_avg_pool2d_slice.py` unit test to use this mechanism.,1
"[Runtime][PipelineExecutor]  Tutorial of using pipeline executor. (#11557)* [Runtime][PipelineExecutor]  Tutorial of using pipeline executor.Tutorial of using pipeline executor including the byoc use case.* fix ci issue* document change.* triger build* fix doc issue* fix ci issue* doc issue* fix ci issue* fix ci issue.* fix __file__ not found problem.this is a known issue of sphinx-galleryhttps://github.com/sphinx-gallery/sphinx-gallery/issues/211* fix byoc with dnnl issue* enable dnnl and pipeline executor* trigger build* trigger build* fix build issue* trigger build* oneflow cause crash, do test with change* add sphinx skip* plint* remove from_oneflow change test.* remove pipeline executor change for test* plint* enable DNNL and pipeline* disable DNNL* enable DNNL without pipeline* remove dnnl and add cutlass* use cutlass with byoc* change into cutlass* fix doc convention issue* remove duplicate variable* fix plint issue.* address review comments.* address review comments* fix bug.* polish the document* fix plint issue* address review comments.* address review comments* address review comments",0
[QNN] Support different qnn params between in/out tensor in leaky_relu (#12116)* [QNN] Support different qnn params between in/out tensor in leaky_relu* format code* format code* fix bug* fix format* fix format* fix,0
[hexagon][testing] nonrandom tests (#12164)Add support for populating unit-test input tensorswith values other than random.,1
tanh float16 (#12165)Co-authored-by: aakaverm <aakaverm@qti.qualcomm.com>,5
[TIR] Well-Formed Verifier (#12166)* tir_well_formed_verifier* fix typo* lint* fix testcase,0
[BYOC-DNNL] suppport more dnnl ops (#11823)* support dnnl.global_avg_pooling2d* fuse pad-avg_pool2d* fix lint,0
"[TVMC] Workspace Pools Parameters (#11427)* [TVMC] Workspace Pools ParametersAttributes from tvmc are now passable into the created PoolInfo objectsinside WorkspaceMemoryPools. This is passed in to relay.build that getattached to IRModule attribute.* [TVMC] Workspace Pools ParametersAddress comments, fix linting. Testing improved.Change-Id: Iea79329b6b9ec1cbc51e5c293449bf6dd43b00c5* [TVMC] Workspace Pools ParametersUpdate workspace pools test namingChange-Id: Ib698d6248be1e6f44340f27db3641c985bc5c5d8* [TVMC] Workspace Pools ParametersAdd test for parameter overrides.Change-Id: I67d5470dcfbfbc9ab27f34e20a9269d2070193ca* [TVMC] Workspace Pools ParametersRebasing over #10189Updates to the way a WorkspaceMemoryPool object is createdChange-Id: I1f0e1d240343af311ddb3ed5c564cc1ab329f463* [TVMC] Workspace Pools ParametersFix linting, fix CIChange-Id: If75f8709ac4ad925655eca54b3e5c1bb09d025e8* [TVMC] Workspace Pools ParametersAdd mcpu and mattr to target registry for cmsis-nnChange-Id: I15257b8d01624c071c738cab6d12ecb84ed6cb16* [TVMC] Workspace Pools ParametersAdded test for override on single pool when multiple pools are presentUpdated functionality of parsing multiple attributesChange-Id: I2c0745051b7a923dd7f75040bfb89bbc99376a11",0
[hexagon][testing] sequential input tensors (#12168)Provide mechanism to let unit tests initialize input tensors with sequential element values.,3
[PyTorch] Add aten::numpy_T (#12179)* add numpy_T* add warning* fix linting,0
[CI][docker] Add comment (#11953)* add comment* address comments,1
fix typo (#12183)* fix typo* fix typo,0
[Doc] Fix link error in pipeline executor tutorial (#12185)Fix the link error.,0
TVM Vertical Integration with PyTorch (#11911)* optimize_torch & as_torch* split files* code formatting* optimizing optimized_torch* scrap your boilerplate* as_torch polished* configuration fixed* Apply suggestions from code reviewCo-authored-by: Lite Ye <liteye859@gmail.com>* more document* file deleter* optimize deleter* drop how-to guides* clang-format-10* formatter changes* reformat* reformat* reformat* reformatting* fixed* auto setting* fixed* split long string* tune_tir* upgrade as_torch* optimize as_torch* as_torch* fixed typoCo-authored-by: juda <yzhou@octoml.ai>Co-authored-by: Lite Ye <liteye859@gmail.com>,0
"[Fix] Fix some errors in unittests (#12170)* unittests fix 0* fix unittests* fix unittests* fix unittest* fix unittest* fix unittest* Revert ""fix unittest""This reverts commit 09b6b410bc51e91ca256e888d380e5648739d257.* fix unittests* fix",0
[ci] Skip broken android_rpc failures (#12192)See #12191Co-authored-by: driazati <driazati@users.noreply.github.com>,5
[TIR Pass] decouple flatten buffer to lower opaque block pass and flatten buffer. (#12172),4
Update to 0.10.0 (#12190)This updates the version numbers after the v0.9.0 release and adds a version selector option for the v0.9.0 docs.Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[TVMScript] ExprDoc (#12048)This PR addes:- All ExprDoc subclasses- Their Python bindings- Support of ExprDoc in PythonDocPrinter- Unit tests for ExprDoc in PythonDocPrinterTracking issue: https://github.com/apache/tvm/issues/11912,1
[microTVM] Fix timeout of -1 breaking Arduino transport (#12189)* Remove warning from Teensy boards* Use a real timeout* Skip assertion of whether a functional schedule exists* Don't specify least significant digits for Teensy boards,0
TIR Schedule primitive - decompose_padding (#12174)Co-authored-by: baoxinqi <wrongtest@intellif.com>,1
fix pooling documents (#12201),0
[runtime][hexagon] improved file-copy logic (#12194)- Add `tvm::runtime::CopyFile` function.- Change `HexagonModuleNode::SaveToFile` to use new function  instead of a shell `cp` invocation.  This fixes a problem where the `cp`-based implementation  couldn't handle certain valid filenames.  This also fixes a bug where `SaveToFile` simply skips the  file-copying step on Mac OSX.,0
"[hexagon][testing] filesystem-friendly test IDs (#12195)- Change the formula used to compute pytest test-ID strings.  The previous formula included '(' and ')' characters, which  can cause the filename to require escaping / quoting on the  Bash command line.",2
[docs] Update tlcpack-sphinx-addon (#12188)This includes https://github.com/tlc-pack/tlcpack-sphinx-addon/commit/545450acaf0ee4e2932d8c5d9ab6e321d0bc86c8 which fixes the sphinx-gallery cards and closes #12156,0
[OpenCL] Fix profiling hang for OpenCL device (#12173)* Fix opencl timer logic to support nested calls to start and stop functions* Fix profiling for __nop functions that caused the profiler to hang. Now __nop functions are skipped for profiling* Fix linter* Add a test that checks the correct time for nested timers* Fix linter* Fix work with memory object in test for opencl timers,0
"[JIT] Force finalization of JITed code, expose sf/hf runtime functions (#12187)* [JIT] Force finalization of JITed code, expose sf/hf runtime functionsCode that handles fp16 and fp32 may end up calling builtins that do theconversions between these types. LLVM emits calls to __truncsfhf2, and__extendhfsf2, which are not present in TVM or TVM runtime. This createstwo problems:- Problem 1: JITed code that does the conversions will fail because it  calls non-existent functions.Adding these functions to libtvm.so/libtvm_runtime.so solves this part,but there is another issue:- Problem 2: JITed code may still not call these functions, because thegenerated object may not be fully resolved.To force full resolution, try to obtain an address of a non-existentsymbol.* Restart CI",1
Use std::move to avoid warnings on clang-13 (#12196),4
[ci][docker] Use RFC image tags only (#11938)This ignores image names like `123-123-abc-validated`Co-authored-by: driazati <driazati@users.noreply.github.com>,5
"[Target] Improve string interpretation in Target creation (#12152)- SplitString now preserves escape sequences, but still observes  quote characters.- Added function Interpret that transforms given string according  to interpretation rules:  - outermost quotes are removed (if present),  - escape sequences inside quotes are preserved verbatim,  - unquoted escape sequences produce the escaped character (the    escape character (\) is removed.- Interpretation happens every time a value of any type is to be  parsed from a string, e.g. Array<String> will first be parsed  as an array, then substrings of the input will be parsed as  individual elements of that array. In this case, some parts of  the initial input will be parsed (and interpreted) twice.- Implement corresponding stringification functionality.This new scheme enabled encoding nested arrays of string with anydegree of nesting. For example  ""-field='\\'foo0\\',\\'bar0,bar1\\'','\\'zing0,zing1\\',\\'fred\\''""would correspond to the target kind attribute  Array<Array<Array<String>>>(""field""))and have the value { { {foo0}, {bar0, bar1} }, { {zing0, zing1}, {fred} } }",1
[ci][lint] Consolidate image lookup logic (#12206)This makes `run_docker` just pass through the image name to `bash.sh` without preprocessing it at all so that `bash.sh` can do its own determination of which image to actually use.,2
[TVMScript] StmtDoc Definitions (#12111)This PR addes:- All StmtDoc subclasses- Python bindings for StmtDocTracking issue: https://github.com/apache/tvm/issues/11912,1
[Relay][PyTorch] Add aten::lerp (#12167),1
[Adreno] Fix winograd tests and accuracy (#12202)* [Adreno] Fix winograd tests and accuracy* Fix lint* Fix test on cpu,0
[TFLite] Fix _test_tflite2_quantized_depthwise_convolution is unused (#12145)* Update test for tflite2_quantized_depthwise_convolution* fix input_node name error* Add entry for test_quantized_convolution* reformatted by black,0
[TVMScript] StmtDoc Printing (#12112)This PR addes:- StmtDoc Printing in PythonDocPrinterTracking issue: https://github.com/apache/tvm/issues/11912,1
[MetaSchedule] Integration test for CUDA AutoTensorization (#12142)* [MetaSchedule] Integration test for CUDA AutoTensorization* cleanup* fix,0
"[ci] Fix docker post-merge builds on main (#12210)* This patch addresses #12097 by referencing the branch from the `BRANCH_NAME` environment variable, which is also used in the Jenkinsfile* This avoids git rev-parse, which assumes the local repo contains a git branch that matches the name of the branch being merged. I think this is in spirit of what the script was trying for.",0
"[TIR] Asynchronous stage in software pipeline (#12171)* [TIR] Support asynchronous stages in software pipeline transform* Support interleaved async producers separated by a consumer* clean up* adding doc* adding doc* simplifying* make wait count computation a two pass process* commit_stage -> commit_queue, wait_stage -> wait_queue* make async_commit_queue special scope stmt* codegen async_commit_queue in cuda* clean up* clean up* Move block predicate outside of commit_queue* updating test* test updated* changed async_wait to an annotation* update doc* update meaning of software_pipeline_async_stages* update test* fixing codegen* more fix* remove one of tests that have async and sync ops in the same stage* format* lint and other fix* Define attr::software_pipeline_async_stages* populate wait count in a separate function* fold variabel consumed into AsyncStateLocal* introduce CompletePipelineLoopStatements function for further refactor",0
[microTVM][tutorial] AOT host-driven tutorial with TFLite model (#12182)* Add aot tutorial,1
[Adreno] Update conv2d_nhwc test to use winograd (#12214),1
"[FIX,ROOFLINE] Handle mismatched compiled and TIR hash (#12219)Fix a bug where roofline analysis would crash if the provided tirfunctions do no match the profiling report.",0
Removing header arm_math.h which has disappeared after CMSIS upgrade (#12217)Change-Id: I51a852321296aa6ac3e74eb44f37e8e693cb3f8f,4
Fix comments. (#12220),0
[TIR] Disallow unused rhs vars in GetAutoTensorizeMapping (#12225),5
[microTVM][Zephyr][projectAPI] Minimize project build commands (#12209)* move cmake args to generate project* remove zephyr board from flash and run,4
[UX] highlight tvm script (#12197)* feat(ux): highlight tvm script* resolve dependency* refact(tvmscript): highlight fallback as plain text with warning; put ansi colors as default terminal style* refact(tvmscript): make terminal style close to the default notebook style* fix(pylint): disable=import-outside-toplevel* fix(ci-dep): Pygments>=2.4.0 to support ansicolors w/o #* refact: making Pygments versioning most robust and user-friendly* fix: pylint var naming,0
[TOPI][HEXAGON] Implement depthwise conv2d slice op. (#12218),5
[ci] Fix build android rpc failure (#12216)android_rpc build problem: https://github.com/apache/tvm/issues/12191The problem with the build appeared due to the fact that the `ANDROID_NDK_HOME` environment variable was removed in the current version of github actions. https://github.com/actions/virtual-environments/blob/main/images/linux/Ubuntu2004-Readme.mdBut this variable is used here:https://github.com/apache/tvm/blob/ee319d9d23c80091da9c4fb764b1e6d49d462714/.github/workflows/main.yml#L122-L127Now only `ANDROID_NDK_LATEST_HOME` is available for ndk.,0
[ci] Reinstall junintparser after zephyr deps (#12226)Fixes #11749,0
[ci][docker] create Dockerfile.ci_riscv (#12230)Move RISC-V related content from ci_qemu to ci_riscv,2
Deploy the Pretrained Model on Jetson Nano  (#11037)* Create deploy_model_on_nano.pyadd deploy_model_on_nano.py* Update deploy_model_on_nano.py* fix doc build bug* Update deploy_model_on_nano.py* fix ci error* Update deploy_model_on_nano.py,0
remove duplicated cast op when lowering qnn.requantize op in float mode (#12234),4
[AutoSchedule] Fix misusage of an already-moved object (#12239),0
[Relay][VM] Fix an ICHECK which never fails in ctor of VMFunction (#12241),0
"[UX][TVMSciprt] Use HTML formatter in notebook environments (#12240)Previously we use ANSI color sequences to highlight TVM script. In jupyter notebook environments, such color sequence will be recoginized and translated to corresponding HTML to display things. This works fine for most notebook environments (including the jupyter notebook and the VS Code plugin). Recently, thanks to @tqchen, we found that Google Colab does not well support ansi color sequence for 24-bit colors (`JupyterLight` and `VSCDark`) that all its displayed colors are unexpectedly black/gray/white. To also bring highlighting in Colab, in this PR, we directly render the highlighted code with HTML when a notebook environment is detected.",2
[ROOFLINE] Add CUDA support to roofline analysis (#12205)* [ROOFLINE] Add CUDA support to roofline analysisAdd functions to estimate peak flops and bandwidth for CUDA. Add a newregistration mechanism to the roofline analysis to support adding anytarget. This mechanism uses generic functions with overrides. Newtargets only need to add `estimate_peak_bandwidth` and`estimate_peak_flops` functions.Also fix cuda codegen and tensorcore_infer_fragment.cc to supportfilling matrix_a and matrix_b fragments.* formatiing* move statement back inside loops* print out report for debugging* default to avx2* review comments,0
[TVMScript] Doc Definition (#12244)This single-file PR is automatically generated by a script that describes the Doc AST.,2
[FQ2I] fix unary op output affine type in fq2i (#12224)* fix unary op output affine type in fq2i* better names* add option to force to positive values for ops that are undefined on negative values,0
[MetaSchedule][Test] Add unittests for GMM (#12243),1
"fix bug: KeyError, can't find some parameter key (#12211)Co-authored-by: woobinw <Wubin.Wu@imgtec.com>",0
[Fix] Fix some errors in unittests (#12245)- test_aot_legalize_packed_call.py: `T.preflattened_buffer` returns `void`- test_tir_intrin.py: `type` here should be `buffer_type`- test_tir_transform_flatten_buffer.py: `extents` should be `list`- test_tir_transform_hoist_expression.py: change `tir` into `T` and register `Let` expression in `script/tir/intrin.py`- test_tir_transform_storage_flatten.py: `T.allocate` has no argument named `strides`,0
[MetaSchedule][Test] Add unittests for GRP (#12246),1
[Fix] Minor modification in unittests (#12247)Update unittests to align with the expected behavior,0
[MetaSchedule][Test] Add unittests for T2D (#12249),1
[BYOC-DNNL] add post_sum pattern (#12151)* add post_sum pattern* add checkers for sum pattern* fix lint* fix error in test_pass_partition_graph* fix lint error,0
[MetaSchedule][Test] Add unittests for NRM (#12250),1
[TVMScript] Python Expression Precedence (#12148)This PR:- Handle expression (operator) precedence during Python code printing (`(* 1 (+ 2 3))` prints as`1 * (2 + 3)`)- Addresses remaining feedback from previous PR #12112- Reformats Python import with isortTracking issue: #11912,1
[MetaSchedule][Test] Add unittests for SFM (#12251),1
Enable conv family fused with mish (#12228),5
"[FIX,TIR] Handle LetStmt in EstimateTIRFLops (#12138)* [FIX,TIR] Handle LetStmt in EstimateTIRFLopsAdd flops for the right hand side of let statements to the total flops.* assert flops",0
[MetaSchedule][Test] Add unittests for CBR (#12252),1
[CPP-RPC] Fix GetPath to use relative file path (#12242)* [CPP-RPC] Fix GetPath to use relative file path* change file_path to file_name,0
[CI] Add setup-pytest-env.sh call to task_demo_microtvm.sh (#12260)task_demo_microtvm.sh fails when running on a container because it can't find TVM. Adding a regular `source ...task_demo_microtvm.sh` will make the proper test environment setup.Co-Authored-By: Liam Sturge <Liam.Sturge@arm.com>,1
[microTVM][ARM] Enable tests that were skipped unintentionally (#12223)* fix bug to enable tests* fix import issue,0
[MetaSchedule][Test] Add unittests for TBG (#12262),1
[Fq2i][ fix output type on fq2i binary ops with constant inputs (#12236)* fix output type on fq2i binary ops with constant inputs* allow off by one on test,0
[docs] Update installation instructions (#12269)This cleans up the installation instructions and adds some info about`apache-tvm` on PyPi.,1
"[UnitTest][TIR] Testing utility for before/after transform tests (#12264)This PR adds `tvm.testing.CompareBeforeAfter`, a generalization of the `BaseBeforeAfter` utility previously used in `test_tir_transform_simplify.py`, which performs unit tests that perform a transformation on a TIR function and compare the results to an expected TIR output.  This arose when minimizing the boilerplate required for unit tests in the implementation of https://github.com/apache/tvm/issues/12261.",1
"Hide registration errors in `test_myfloat` (#12268)This should fix the error from #11580, the test will still get run butthe registration failing will be ignored if it happensCo-authored-by: driazati <driazati@users.noreply.github.com>",0
[Arith] Handle bitwise_and with power of 2 in modular set (#12272),5
[MetaSchedule][Minor] Fix Median Number (#12273)The previous median number calculation util function used the wrong index which could cause out of bound issue as mentioned in #12199 . This PR fixed this issue.,0
[Pylint] Making hexagon tests pylint compliant Part 2 of N (#12176)Second set of **hexagon tests** modified to be pylint compliant as part of #11414 tracking issue. The files supported in this patch are:* [X] tests/python/contrib/test_hexagon/test_autotvm.py* [X] tests/python/contrib/test_hexagon/test_cache_read_write.py* [X] tests/python/contrib/test_hexagon/test_launcher.py* [X] tests/python/contrib/test_hexagon/test_maxpool2d_blocked.py* [X] tests/python/contrib/test_hexagon/test_models.py* [X] tests/python/contrib/test_hexagon/test_run_unit_tests.py* [X] tests/python/contrib/test_hexagon/test_thread_pool.py* [X] tests/python/contrib/test_hexagon/test_usmp.py,2
[ci][docker] Update GPU image (#12265)This includes a fix to `tlcpack-sphinx-addon` to fix broken card linksCo-authored-by: driazati <driazati@users.noreply.github.com>,0
[TensorFlow] Disable failing tests on AArch64 (#12257)* [TensorFlow] Disable failing tests on AArch64Change-Id: I7a53ce2d0dfff682b2953a26cace53fc5cc5d388* Fix lintChange-Id: If60d6e6ad24230f3c7f17c7ec55a7febcbba90fd,0
[ONNX] Disable failing tests on AArch64 (#12256)Change-Id: I170d2a8032dcb19d6ba3f67d9b0441944def84b8,3
[Pytorch] Disable failing tests on AArch64 (#12255)Change-Id: I234a4486a4a5ffac03d478e65cbfcee3efe30df1,3
"[ETHOSN] Upgrade NPU driver stack to v22.05 (#11759)* [ETHOSN] Upgrade NPU driver stack to v22.05In updating the driver stack to v22.05 some additional things neededchanges:* Prevent split being offloaded to the NPU which is not supported in  v22.05.* Removes compile algorithm configuration option since this was removed  in v22.05. Versions before v22.05 will use the default option.* Managing some API changes.* Updating network compile hashes.* Updating expected error message for overall scale bounds check.Change-Id: I09343c398a1f47dec44e135ff8252a6315a9b63f* fix decorator evaluation orderChange-Id: Ib1a34093b4011bdc20fca47d474eb1786218de98* Return none if version doesn't existFor some reason PyTest evaluates the second skipif decorator evenif the first one marks the test to be skipped. Thus, meaning testcollection fails when we want to check the version. The workaroundis to return None when the version is not available.Change-Id: I7cdd8cc70a9ee3c193e9a900f1011829538d975b* Update resnet hash after rebaseChange-Id: I7555c4a4d7db4f6c7aa8d476e39277fc5cba2f0d",0
"[TVMC] Only load extra targets when there are workspace pools (#12253)After #11427, `tvmc compile` wouldn't work for external codegens thatdon't have a `Target` registered by `TVM_REGISTER_TARGET_KIND`. Suchexternal codegens can be expected to have no workspace pools and may notalways have a target associated as their implementation predates thismechanism. While it is likely a `Target` is specified for all externalcodegens in the future, we should still support external codegenswithout an associated `Target` until this is enforced.Co-authored-by: Chris Sidebottom <chris.sidebottom@arm.com>",5
"[Adreno] Add markup pass of relay tensors for static texture planning (#11878)* [Adreno] Add static texture markup relay passCo-authored-by: Chris Sullivan <csullivan@octoml.ai>* lint check* Remove hardcoded texture limit, check through target options* fix cpplint* Add winograd into annotation pass* fix clang* Remove extra call of PlanDevice in OptimizeImpl* Remove one more extra call of PlanDevice in OptimizeImpl* Fix/add scopes for static texture planning tests* Remove test_2conv2d as duplication of test_plan_device_issue* remove comments in test_residual_block* address review comments* fix black hits* Add textures test descriptions* Address PR commentsCo-authored-by: Chris Sullivan <csullivan@octoml.ai>",0
[Docker] Update onnxoptimizer to 0.2.7 (#12278)Co-authored-by: Christopher Sidebottom <chris.sidebottom@arm.com>,1
[CI] Shard Qemu python tests (#12258)This PR shards the QEMU python testsFixes #12180,0
[MetaSchedule] Check auto tensorization applicability in MultiLevelTilingWithIntrin (#12263)* Check auto tensorization applicability in MultiLeveltilingwithintrin* fix qbert loader* add MultiLevelTiling rule in integartion test* unused import* fix cpp format* add more test* Check for tiling failure,0
[Relay][Op] Trilu operator implementation (#12124)* Added topi trilu implementation* Implemented and tested full Trilu op.* Fix test type.* Add tril zero tests.* Add pytorch trilu integration.* Clean up torch integration.* Readded skip for zero tests.,0
"Revert ""[CI] Fix build android rpc failure in CI"" (#12277)Reverts apache/tvm#12216Addition to my previous changes.After the previous changes, the android camera build failed because by default `PYTHONPATH` is empty, and after `set -eux` it is fails:https://github.com/apache/tvm/blob/759a648cd5237885a8205b1ee4508dabcc3af2d5/.github/workflows/main.yml#L152-L156This error was not noticed because the flag `continue-on-error` is true.",0
[release] Add script to gather PRs for a release (#11987)Co-authored-by: driazati <driazati@users.noreply.github.com>,1
[microTVM]Fix dense_dsp schedule in autotuning (#12271)* dense layer used in autotune* format,0
[ci] Specify permissions for tvm bot (#11937)This adjusts some error reporting for tvm-bot and manually specifies the permissions it should run with to hopefully alleviate the 403 errors when merging PRs,0
[ci][docker] Fix deploy to tlcpackstaging on Docker Hub (#12282)This was previously broken since it wouldn't pick up the new images names and there was an errant `'` floating around,0
"[TVMScript] Add object path tracing to StructuralEqual (#12101)Motivation: when two IR objects fail a structural equality check, currently there is no easy way tofind out which part of the IR caused the mismatch. In this PR, we modify the `StructuralEqual`infrastructure to also optionally return a pair of `ObjectPath` objects that point to the mismatch.(See https://github.com/apache/tvm/pull/11977). In the upcoming PRs, we will pass these paths to theTIR printer, so that it could highlight the mismatch location nicely.Tracking issue: https://github.com/apache/tvm/issues/11912",1
"[ci] Add retries to S3 uploads/downloads (#12221)`aws s3 cp` sometimes segfaults, so retry it when the command fails",1
[microTVM] Refactor pytest fixtures (#12207)* Refactor micro test fixtures* fix error* fix scope* address @guberti comments* fix help message* rename tvm_debug and added .gitignore* fix bug* fix bug,0
"[TIR][CUDA] Fix sub-warp reduction using ""max"" (#12275)* upd subwarp unittest* fix range check in sub-warp reduction* upd: sub-warp max unit test",0
[MetaSchedule] Enhance Conv2d NCHW Winograd Schedule Rules (#12127)* Update winograd schedule rules.* Remove extra part for setting local storage scope.* Fix bgemm schedule.* Add winograd tile size to annotation.* Finish winograd schedule rules.* Process add relu.* Modify to nchw rules.* Add missing nchw output rules.* Add winograd conv2d nchw search space test.* Fix lint.* Leave consumer of output to autoinline.* Remove bgemm rules.* Remove bgemm schedule rule annotation.* Update unit test.* Fix test case.,0
"[LLVM] Create LLVM scope object for use with LLVM libraries (#12140)This implements RFC 80. See https://github.com/apache/tvm-rfcs/pull/83.Summary of changes:- Created an `LLVMInstance` class. Uses of LLVM functions and data struc-tures should be contained within the lifetime of an object of this class.LLVMInstance object contains LLVMContext, and implements member functionsto deserialize an llvm::Module.- Created an `LLVMTarget` class. Once an LLVMInstance object has beencreated, an object of LLVMTarget class can be created from TVM targetstring, or Target object for ""llvm"" target. Once LLVM command line flagsare added to the ""llvm"" target, one of the goals of this object will beto save/restore relevant LLVM global state. Another objective for theLLVMTarget object is to be a single location for all LLVM-relatedcompilation structures and options (such as TargetMachine, FastMathFlags,etc.)",1
fix type bug about topi test unitest (#12285),0
"[microTVM] Arduino: Fix f-strings on flash warning/error messages (#12175)This commit fixes two f-strings on flash timeout exception and runtimeerror so the proper variables (like port, number of retries, and timeoutvalue) are correctly printed to the warning / error messages.Signed-off-by: Gustavo Romero <gustavo.romero@linaro.org>",0
"[ETHOSN] Only use mock inference when whole graph is offloaded (#12296)The mock inference functionality is only supported when the wholegraph is offloaded to the NPU, otherwise it can result in undefinedbehaviour. This patch makes sure the mock inference functionality isnot run on test cases where some parts of the graph are not offloadedto the NPU, while ensuring the module is still built as a sanity check.Change-Id: I27052d118ff976f9adbfc3f5b5b96185318e1573",3
"[ETHOSN] Get buffer sizes from the compiled network (#12160)The NPU support library compiler sometimes adds padding to inputtensors which means the buffer sizes calculated at runtime cansometimes be smaller than necessary. Instead, buffer sizes are nowcollected at compile time and passed to the runtime so that they matchthe sizes expected by the compiled network. This was seen when runninga fully connected operation with an input that is not a multiple of1024, so testing has been added to cover this case.Additionally changed the fully connected test case to use pytestparameterization as part of a general cleanup, and fixed an issuewith specifying a different output shape and weights with more than 1output channel.Change-Id: Iad319d75326b9ac41950de982603660a084dc27b",0
[ci][tvmbot] Enable re-run for GitHub Actions (#12295)This adds the right permissions so anyone associated with the repo can trigger a re-run (GitHub hasn't flagged all committers as repo `COLLABORATORS` for some reason so it's difficult to determine from a username who has commit rights) and makes it so `@tvm-bot rerun` also re-runs all the Actions on a PR.,1
"[TVMScript] Make classes derived from ObjectPath non-nullable (#12304)`ObjectPath` is marked as non-nullable, which causes it not to have adefault constructor. The derived classes, on the other hand, are notmarked as such, thus getting an explicitly defaulted default constructor(via TVM macros). This constructor can't actually be called since itends up being deleted, so the derived classes are effectively non-nullable.",5
[Relay][Frontend][Onnx] Add RNN operation for ONNX frontend (#12213)* Add RNN operation for ONNX frontend.* link checks* rm test_rnn_batchwise in unsupported_onnx_tests* merge similar codes to class methods* implement opset 14 and refactor test_forward* reformat verify_rnn_helperCo-authored-by: 张亦驰 <zhangyichi1@corp.netease.com>,1
initial commit (#12301),5
[FQ2I] Add attrs to adaptive_avg_pool1d (#12290)* Add attrs to FQ2I adaptive_avg_pool1d* Fix failing test and add param,0
[QNN] Add qnn op for abs to fix wrong scale on quantize (#12287)* [QNN] Add qnn op for abs to solve wrong scale on quantize* Fix for pylint to allow redefine absCo-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,0
[ci][docker] Add retries for docker pull (#12306),1
"[TVMScript] TracedObject class that simplifies tracing ObjectPaths (#12299)Motivation: when printing a piece of TIR, we need to track an ObjectPath from the root TIR object to the currently printed object. This means that we need a convenient way to maintain an ObjectPath whenever we access a sub-object, e.g. via an attribute.Tracking issue: https://github.com/apache/tvm/issues/11912",5
[CI] Increase CPU Intergration tests shards to speedup runtime (#12316)Co-authored-by: Michalis Papapdimitriou <mpapapdimitriou@octoml.ai>,3
"[TIR] Add tir::builtin::assume (#12267)* [RemoveAssume] Implemented T.assume in TVMScript, RemoveAssume* [UnitTest] RemoveAssume, initial functionality tests",1
"[TVMScript] Add source_paths to Doc (#12324)This PR:- Add the source_paths attribute to Doc base class.- Add the corresponding Python binding for it.This PR is depended by multiple tasks, including the diagnostic output in DocPrinter, VarTable and IRDocisifer.Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>",1
[microTVM][CI] Rename ci_qemu to ci_cortexm (#12281)* rename files* ci script* demo* RVM files* jenkins* more ci* Jenkins* fake name for ci_cortexm* merge with main* add cortexm config file,1
[TIR] Add DeclBuffer IR node and functors (#12300)* [TIR] Add DeclBuffer node* [TIR] Add IR functors for DeclBuffer* [TVMScript] Add printer and parser for DeclBuffer* Update printer* Update printer* Add test case* lint* fix,0
[microTVM][ARM] Keep microtvm testing only in QEMU Image (#11809)* Move scripts* Address comments* move ethosu tests* move cmsisnn tests to qemu,1
[TIR][Schedule] Support annotate dict typed value (#12288)* tir schedule support annotate dict typed value* fix lint* fix comment issues,0
"[TIR] Add tir::builtin::undef (#12266)* [UnitTest] RemoveStoreUndef, simplest behavior* [RemoveStoreUndef] First implementation* [UnitTest] RemoveStoreUndef, stores that depend through LetStmt* [UnitTest] RemoveStoreUndef, LetStmt handling, error on illegal usage* [RemoveStoreUndef] Added error checking for illegal T.undef() usage* Fix lint error* Use const ref for list of stores to remove* Verify that removed expression has no other side effects* Fix lint error",0
"Pass that removes reshapes post LowerTE (#12215)Introduces a Pass for removing intermediate reshapes postLowerTE() in AOT compiler. This commit adds pass specifictests and updates usmp generated workspace pools due toreduction in number of allocations post reshape removals.Note: this pass at present does not support first reshapeappearing in the graph. If seen as a useful case, it can beadded in the future.",1
[OpenCL] Use size_t instead of int64_t for OpenCL timer count (#12328)Resolves a few gcc warnings for comparing signed and unsignedintegers.,2
[CI] Increase the number of shards for Cortex-M from 4 to 8. (#12334)Co-authored-by: Florin-Gabriel Blanaru <fgb@system76-pc.localdomain>,5
"[CI][Docker] Removes Dockerfile.ci_qemu as it was moved to Dockerfile.ci_cortexm (#12329)Removes the - now deprecated - Dockerfile.ci_qemu, as it was moved to be specialized in different environments such as Dockerfile.ci_cortexm, to support specific environments for tests in different platforms.",2
"[ETHOSN] Fix output tensor ordering (#12317)During compilation of an NPU subgraph, the input and output tensorordering is determined by creating a mapping of Support Librarybuffers to TVM buffers. The runtime had been using this mappingincorrectly by instead interpreting it as a mapping of TVM buffers tosupport library buffers. This can result in undefined behaviour. Theruntime now interprets the mapping as Support Library buffers to TVMbuffers.A test has been added to check the correct ordering of outputs isprovided correctly in the runtime.",0
[Relay][Op] Multinomial (#12284)* Add multinomial operator.* Implemented Pytorch integration with multinomial.* Fixed test paramatrization and added onnx integration.* Add statistical testing.* Make get_type more flexible.,0
"[CI] Deduplicate and clean XML test reports (#12332)This PR does the following:- When running on a shard,  removes the tests that will not run on the current shard from the suite. We'll no longer get the message `Test running on shard X of Y` in the XML reports. This will result in cleaner test reports.- Adds the shard index or `no-shard` to the generated XML report file name. Currently, the reports generated when sharding is present are overwritten in S3 by the last shard to finish in the CI pipeline. This change is needed as part of #11670.- Since the same tests might run on different configurations (CPU, GPU), it uploads the result of each configuration in a subdirectory in S3 (e.g `/pytest-results/frontend_aarch64`).",1
[ci] Fix aws s3 cp command in the Jenkinsfile (#12341)Looks like #12332 broke [the CI](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/3992/pipeline/389) due to a misuse in the `aws s3 cp` command. This PR fixes that.,0
"[microTVM][Zephyr] Fix missing BOARD in CMakeCache file (#12338)This bug was caught in the microTVM hardware in the loop CI, becauseonly in the HW in the loop CI it's possible to test 'open_transport' andZephyrSerialTransport() method, i.e. run a model via the serialtransporter.                                  This commit changes the microTVM Zephyr template project to read BOARDfrom cmake file instead of cmake cache file.",0
"[BYOC-DNNL] Bug Fix (#12314)* add bias_add checker, check op's order in catched pattern* fix wrong return in legalize_pad_avg_pool* add check for pooling, ceil_mode=True has not been supported by onednn currently.* fix lint* fix test error",0
[Topi] add x86 schedule for batch_norm (#12321)* format black* format black* docstring* typo,1
"[FIX,ROOFLINE] Only save tir functions for roofline (#12339)Only collect TIR PrimFuncs in roofline's SaveLoweredTIR. SaveLoweredTIRwas saving the full Relay main function leading which could beexcessively large. Also improve the logic to only save functions rightbefore MakePackedAPI.",0
[MetaSchedule][Bugfix] Feature: Strides of buffer access (#12331)I fixed a bug in MetaSchedule per_store_feature.cc,0
"[TIR] Simplify indices in InjectVirtualThread (#12259)If the injected index expressions can be simplified to Rampnodes (e.g. `Ramp(0,1,4)` resulting in `Ramp(vthread*4, 1, 4)` insteadof `Ramp(0,1,4) + Broadcast(vthread*4, 4)`), these can be identifiedas vector access in later passes.  Simplifying at the time ofsubstitution avoids requiring all downstream passes to perform thesimplification.",2
[COMMUNITY] Yuanjing Shi -> Reviewer (#12345),3
"[hexagon][topi] add sliced max_pool2 (#12169)Add TOPI implementation of sliced max_pool2d,with basic scheduling.",1
"[UMA] UMA v1.0 (#12087)* Add minimal working structure for generic interface* Separate target definition from codegen* Update file structure to support multiple NPU targets* Add scheduling and pass support to codegen* Update schedule function and pass registration* Add generic partitioner for relay graph partitioning* Add pattern-based relay graph partitioning and AOT codegen* Update API* Add UltraTrail relay passes and schedule function* Update UltraTrail relay passes* Add tir_to_runtime hook for UltraTrail* Add operator strategy registration to lowering* Add option to pass constants as attributes* Refactor naming: Generic to UMA* Change API to single user-facing backend class UMABackend* Add initial codegen API* [UMA] add a generic packed function to register targets* Restructure files and add initial codegen* Minor code cleanup* Add UMA config and MergeCompilerRegion example* Move UMA configuration to init parameters* Add python hooks for C-codegen. Still has known restrictons* Fix relay_to_tir hook to keep virtual device in main function* Remove register schedules, scheduling is moved to passes for now* Remove extract constants since non-scalar constants are now supported by TVM* API documentation and some code fixes and cleanup* Fix typo* Fix UMA lowering* Prototype for UMA-based target attribute registration* Add default option and type deduction to register_target_attr* Change pass phases to enum* [Relay] Plumb external codegen target via Target.current() for all external codegen paths(See https://discuss.tvm.apache.org/t/byoc-supporting-cutlass-byoc-with-collage/12796/6 forcontext, which in turn is part of Collage (https://github.com/apache/tvm-rfcs/blob/main/rfcs/0062-collage.md).We want both old-style (via relay.ext.$toolchain) and new-style (via ""RelayToTIR"" Passattribute on target kind) external codegen to be able to access the current 'external codegen'Target instance via Target.current(). - For old-style, plumb the true Target through TEComplier and push it on the context   stack before calling relay.ext.$toolchain. - For new-style, pass the CompilationConfig to the RelayToTIRTargetHook pass, make the jump from   ""Compiler"" attribute value to Target via the new CompilationConfig::FindPrimitiveTargetForKind   method, and push on the stack before invoking the custom ""RelayToTIR"" pass.While working on this discovered RelayToTIRTargetHook was incompatible with the VM's compilationflow since RelayToTIRTargetHook assumes all ""Compiler"" attributed functions are inlined. Generalizeit to support both inline and global function styles.Extend Target::IsExternalCodegen to recognize target kinds with ""RelayToTIR"" attributes asexternal.Update target hooks unit test to exercise new support for outline-style, picking up the current target,and compiling via the VM.* Use current target in lowering* Use attr:kRelayToTIR* Remove erronousely commited quick fix* Towards test cases for uma* Add test_uma* Initial UMA structure for version 1* [UMA]: conv2d unit test* [UMA] update of tutorial* [UMA] update of pass format, still issue with conv2d c code* [UMA] refactoring of test_uma_lowering_with_umalower.py* [UMA] refactoring of test_uma_lowering_with_umalower.py* [UMA] Adding backend, codegen, patterns, strategies and run file for MyAiHw* [UMA] update towards my_ai_hw usecase* [UMA] working testcase for conv2d with uma* [UMA] testcase* [UMA] uma lower.py: replaced outdated function create_prim_func_from_outputs to be compatible withe latest content of ""main""* UMA: Move torch import to top to avoid free(): invalid pointer error* Add stub files for targets* Add tests for ultratrail codegen* Adopt my_ai_hw accelerator for new target definition* Add unit test for target attributes* Test string arguments* Extend target test* [UMA] tutorial first versin* [UMA] moved unit tests to contrib* [UMA] renaming interfaces* Fix umalower_tests in ci* make uma a python module* [UMA] Update of UMAv1 API + added testcases + tutorialV1* [UMA] UMAv1* [UMA] cmake file updated* AOT test infrastructure adapted* UMA: add __init__.py for uma.api* Finish uma tests* Use upstream version of dmlc-core* [UMA] tir_to_runtime documentation update* [UMA] cleanup* [UMA] fix for test_partition* [UMA] lint fix* [UMA] lint fix* [UMA] lint fix* [UMA] lint fix* [UMA] fix of build scripts for arm and i386* Fix remaining linter errors* [UMA] CMakeLists.txt added UMA tvm_option* [UMA] added UMA tvm_option* [UMA] guard against multiple registrations* [UMA] fixed comments as pointed out in PR 12087* [UMA] fixed comments as pointed out in PR 12087* [UMA] skip uma tests if uma is not available* [UMA] added UMA rst* [UMA] Moved tutorial to RST file in gallery* [UMA] moved uma cli to apps* [UMA] change requests according to PR-12087* [UMA] update and sync of uma_cli and tutorial* [UMA] update of template passe: remove Pad block of Conv2D* [UMA] lint updates* [UMA] Test updates* [UMA] fixes according to comments from PR 12087 discussion* [UMA] lint updates* [UMA] moved UMA _template file to apps* [UMA] lint* [UMA] Remove exceptions when dispatching over targets* [UMA] vanilla pattern update* [UMA] added mobilenet integration test* [UMA] clang lint* Remove tir to runtime* [UMA] Use sequential for UMA relay passes* Use comparison against BYOC flow in test_partition* [UMA] tutorial update: moved code blocks to RST* [UMA] tutorial update and lint fixes* [UMA]  removing UMA from i386 build, as there is a fail in the CI pipeline due to missing CLANG for i386* [BYOC-DNNL] covered case for sum node without attr* [UMA] pylint* [UMA] pylint* [UMA] aot fix* [UMA] Changes PR review* [UMA] cc lint* [UMA] cc lint* Use better function name for te_lowering and annotate current target at TE functionsCo-authored-by: Paul Palomero Bernardo <paulpb@outlook.com>Co-authored-by: Christoph Gerum <christoph.gerum@uni-tuebingen.de>Co-authored-by: mbs-octoml <mbs@octoml.ai>Co-authored-by: Christoph Gerum <gerum@informatik.uni-tuebingen.de>",0
"[AutoTVM][Fix] Fix wrong axis names of data_vec (#12303)This PR is trying to fix the wrong axis names of data_vec. As the data_vec is nchwc format, the axis names should be batch, ic_chunk, ih, iw, ic_block, but not batch, ic_chunk, ih, ic_block, iw.Although the following code does not use these last two axises, so it does not cause some bugs for now. But I think we should fix this.",0
"[Hexagon] Minor changes/fixes in codegen_hexagon.cc (#12308)1. Change calls to inherited functions to use CodeGenCPU:: instead of   CodeGenLLVM::.2. Fix #if guards for #include's.3. Remove InitContextPtr and GetContextPtr, use the ones from CodeGenCPU.",0
[Node] fix typos in include/tvm/node/functor.h,0
[MetaSchedule] Extend tune_tir to support tuning of specific blocks. (#12342)* Added optional target blocks.* Checkpoint for debugging.* Building with packedfunc filter.* Extended tune_tir API to support named blocks.* Remove accidental import.* Improve integration test.* Change names for more consistency.* Update integration test.,0
Infer the value of shape expr to avoid dynamic (#12313),5
"[FIX,STORAGE REWRITE] Rewrite buffers in let statements (#12349)Storage rewrite was missing a visitor for let statements so buffersadded in them would still refer to the pre-rewritten version. This errorwas originally noticed when using `global.vtcm` buffers which getchanged to let statements by LowerVtcmAlloc.Implementing the test for this change also required adding support forvectorized datatypes to tvmscript. The solution included is a littlehacky and involes adding the datatypes to the `global()` table of eachmodule they need to be defined in.",0
[TIR] Add int8 CUDA tensor core intrinsics (#12354),1
[TIR] Minor fix to tensor intrin description (#12356),0
"[TVMScript] Printer Registry (#12237)This PR:- Adds the registry of printing function (traced_object_layered_functor.cc)Compared to the prototype version, this:- Consolidates the implementation into a single class, since this class is only for the TVMScript printer.- Deduces the TObjectRef when calling set_dispatch.Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>",1
[ci] De-duplicate retry functions (#12325),5
"[ci][tvmbot] Ignore irrelevant Actions jobs (#12351)GitHub associates the automation run on PRs for user tagging and reviewsas CI jobs on that PR, and since these often get re-run, skipped, or mayfail in the background they prevent merges. This PR ignores all of thesejobs so only the ones defined in `main.yml` matter, which are the onesthat actually build / test TVM.This is a simpler version of #11569",3
[DOCS] Fix tvm.build API doc layoutThe newline in the pydoc breaks the layout of parameter inputs in API `tvm.build`,0
[skip ci] [CI] Re-generate Jenkinsfile (#12360)Timing of merges resulted in the Jenkinsfile being out of sync,2
[BYOC-DNNL] add partition test on sum pattern (#12357)* add partition test on sum pattern* fix lint,0
"[ci][tvmbot] Fix authorization filtering (#12310)There was a level of unwrapping missing in the check for who is allowed to trigger re-runs causing it to always fail. This also uses a different actions API endpoint to re-run only failed GitHub jobs. This also fixes the text fixtures to match the GitHub API response, also tested live in driazati#34.Co-authored-by: driazati <driazati@users.noreply.github.com>",0
[testing] Remove wrapper from @slow (#11566)This makes it a normal pytest decorator so it doesn't incur test set up / tear down. This also makes the PR body the source of truth for skipping slow tests or not since it can be confusing sourcing it both from the PR and commit message.,3
[ci] Test pytest-forked boxing (#12312)This wraps all tests in pytest-forked (with n=1) so that segfaults and otherprocess-terminations are properly reported by pytest. With this reporting on issues like #12311 should be much better (segfaults become like any other test failure)Co-authored-by: driazati <driazati@users.noreply.github.com>,3
"[TVMScript] Text underlining in DocPrinter based on Doc's source_paths (#12344)This adds an ability to print a ""diagnostic marker"" based on a given ObjectPath. For example, say we are printing a fragment of TIR like```for i in T.serial(10):    a[i] = 5```and we would like bring the user's attention to the bound of the loop:```for i in T.serial(10):                  ^^    a[i] = 5```In this case we would give the doc printer an object path that represents this loop bound, i.e. something like `path_to_underline=ObjectPath.root().attr(""extent"")`Tracking issue: https://github.com/apache/tvm/issues/11912",1
Unify name mangling in TVM (#12066)* Add NameSupply and GlobalVarSupply* Build GlobalVarSupply from IRModules instead of having it attached to an IRModule.* Pass GlobalVarSupply when lowering shape funcs* Partially replace instantiations of GlobalVar with GlobalVarSupply* Construct GlobalVarSupply from IRModule* Add tests for supply* Add documentation for NameSupply and GlobalVarSupplyCo-authored-by: Florin-Gabriel Blanaru <fgb@system76-pc.localdomain>,1
"Build and test TVM under minimal configuration (#12178)This PR builds and tests TVM (running the CPP and unittests) under minimal configuration with some debug flags enabled:- `USE_RELAY_DEBUG=ON` in TVM- `-Wp,-D_GLIBCXX_ASSERTIONS` in TVM- `-DLLVM_ENABLE_ASSERTIONS=ON` in LLVMIt also adds this configuration to the CI. `tests/python/unittest/test_meta_schedule_task_scheduler.py::test_meta_schedule_task_scheduler_multiple_gradient_based` results in an array OOB access and a segfault due to `D_GLIBCXX_ASSERTIONS`. I disable this test for now and will open an issue to solve it ASAP.It should fix #11932 and address [this discussion](https://discuss.tvm.apache.org/t/pre-rfc-new-ci-container-ci-cpu-asserts/12536/9).",0
[Hexagon] Add skip option for RPC server initialization (#12368)* add hardware parallelism in hexagon* fix name* lint issue* better description,0
[skip ci][ci] Fix Jenkinsfile (#12387)This got out of date after merging #12178Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"Update C++ standard to C++17 (#12337)* Update C++ standard to C++17LLVM has switched to C++17 in its development branch. Follow suit to beable to compile LLVM headers.* Clang 8.0+ also supports -faligned-new* Make make verbose* Use CMAKE_OSX_DEPLOYMENT_TARGET to set minimum macOS version (10.12)* Update llvmdev version in conda to >= 11Something seems to be wrong with the llvmdev 10.0.0 packages, since theLLVM unit test fails on Windows. It works fine when LLVM 10 is compiledfrom sources.",1
"[ci][docker] Tag tlcpackstaging images to tlcpack (#11832)See #11768, this PR changes the deploy workflow so that after a successful build with fallback images (see #11775), they get moved over to tlcpack automatically. Since the images are moving repositories we can't just rename the blobs in docker, so there needs to be a full `pull -> tag -> push` for each image",4
[BYOC] [DNNL] enable in-place post-op sum in dnnl json runtime (#12371)* enable inplace post-op sum in dnnl byoc* add inplace post-op sum test,1
[Adreno][OpenCL] Get rid of extra memory copy (#12286)* Add annotation pass for device_copy where we get buffers but expecttextures* Fix issues with running device_copy* Get rid of extra memory copy* Fix build after cherry-picking* Fix lint* Fix CI* Apply commentsCo-authored-by: Andrey Malyshev <elvin.nnov@gmail.com>,0
"[TVMScript] Printer Frame (#12366)This PR:- Implement Frame for the TVMScript Unified PrinterCompared to the prototype version, this:- Removes the dependency of VarTable (SymbolTable) from Frame- Adds a callback array to the Frame base class so that VarTable can add callback to clean variable when Frame goes out scopeTracking issue: https://github.com/apache/tvm/issues/11912",1
Add Python function to get type index by class (#12393),1
"[Target] Fix C5 Target Tag to Include CascadeLake Archs (#12385)>The C5 and C5d 12xlarge, 24xlarge, and metal instances feature custom 2nd generation [Intel](https://aws.amazon.com/intel/) Xeon Scalable Processors (Cascade Lake) with a sustained all-core turbo frequency of 3.6GHz and maximum single core turbo frequency of 3.9GHz. The other C5 and C5d instance sizes will either launch on the 2nd generation Intel Xeon Scalable Processor or the 1st generation Intel Xeon Platinum 8000 series (Skylake-SP) processor with a sustained all core Turbo frequency of up to 3.4GHz, and single core turbo up to 3.5 GHz using Intel Turbo Boost Technology.>The C5 and C5d 12xlarge, 24xlarge, and metal instance sizes enable Vector Neural Network Instructions (AVX-512 VNNI*) which will help speed up typical machine learning operations like convolution, and automatically improve inference performance over a wide range of deep learning workloads.According to [introduction to AWS EC2 C5 targets](https://aws.amazon.com/ec2/instance-types/c5/), the C5 12x and 24x machine are `cascade lake` architecture instead of `skylake`, and enable Vector Neural Network Instructions (AVX-512 VNNI*) which can be useful for AutoTensorization.This PR fixes the target definition of the 2 above mentioned C5 machine to support VNNI.",0
[PyTorch] Fix pad_common for float pad_value (#12134)* fix pad* fix constant padding and handle float infinity* revert change to pad_width* fix constant pad value,0
[PyTorch] Fix all_any_common with no default input (#12395)* fix all_any_common with no default input* work around* better naming,0
Add needs-triage label to CI Problem template (#12386)These issues are reported as part of the CI monitoring rotation andusually reflect something contained and actionable. As such we shouldtrack and monitor these issues and make sure that a proper mitigation orfix is merged soon. This label helps us do this since we can easilyfilter by untriaged issues and make sure each one has an assignee.Co-authored-by: driazati <driazati@users.noreply.github.com>,0
"[microNPU] Reorder copies and computes based on the cycle count (#11591)If the cascader is enabled and the ops in TIR have the cyclecount annotation, enabling the reorder_by_cycles option willreorder to copies and computes based on a cycle count.If reorder_by_cycles is enabled, max_copy_movements is ignored.This pass is currently not part of the TIR pipeline since itassumes that weights and bias of a compute op are merged intoone constant (which is WIP).",4
[Target] Add Target Parser for Arm(R) Cortex(R) M-Profile CPUs (#12319)This implements an initial Target Parser which uses the same logic asthe CMSIS-NN compiler flags to update the features and keys of the `c`and `llvm` `Target`s.Refactoring of the CMSIS-NN logic will be in a separate patch.,1
"[ci] Default to n=2 for test parallelism (#12376)This decreases the test times for most of the tests except a few that did not run under pytest-xdist with 2 worker nodes. This also doesn't decrease overall runtime since CI is still bottlenecked on other jobs. However, this could lead to savings in compute which makes CI more sustainable so this is still worthwhile, though we should revert this if we start seeing ""weird"" errors like OOMs more often.",0
[Profiler] Fix graph_executor_debug hang (#12382)For some operations such as `__nop` or `__copy` the measured inferencetime is equal to 0. In this case we are in infinite loop and we won'texit from it. Added new parameter `limit_zero_time_iterations ` which specify themaximum number of repeats then the inference time is equal to 0. Whenwe exceed this value then we will exit from a loop.,0
[docs] Update minimum compiler requirements for building from source (#12405),1
Add sort_by_time flag to debug_executor.run method (#12402),0
"[LowerVTCMAlloc] Move LowerVtcmAlloc to after StorageRewrite (#12364)* [LowerVTCMAlloc] Move LowerVtcmAlloc to after StorageRewriteVtcm allocations were being moved inside loops even if they wereoriginally allocated outside of the loops. NormallyPlanAndUpdateBufferAllocationLocation moves allocations as close to useas possible and then StorageRewrite moves them back out as far aspossible. However, with Vtcm allocation,PlanAndUpdateBufferAllocationLocation would move the Vtcm allocationclose to the compute, then LowerVtcm would convert the allocation to aLetStmt. StorageRewrite would not move this LetStmt as it only handlesallocations. Moving LowerVtcmAlloc to after StorageRewrite ensures thatthe vtcm allocations are in their final spot before converting them to aLetStmt.* fix issues with tagging and storage rewrite",0
Update hexagon max_concurrency to be at most equal to the number of HVX units available. (#12394),1
"[skip ci] Revert ""[ci] Default to n=2 for test parallelism (#12376)"" (#12413)This reverts commit 369e8b283083a3440c59431a9438ca17afb73e4e.There are certain tests that need to be serialized first before this canmerge or else failures likehttps://ci.tlcpack.ai/job/tvm/job/main/4040/display/redirect will happenbased on which tests happen to be run together or notCo-authored-by: driazati <driazati@users.noreply.github.com>",3
"[WIP][Pylint] Making frontend tests pylint compliant (#12028)* [CI] Apply linting rules to tf tests* [CI] Apply linting rules to tflite tests* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to caffe2 tests* reformat by black* reformat by black* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to darknet tests* Update tests/python/frontend/coreml/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/tflite/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/coreml/test_forward.py* Update tests/python/frontend/tflite/test_forward.py* fix test errors* fix ci test errors* [CI] Apply linting rules to keras tests* [CI] Apply linting rules to oneflow tests* [CI] Apply linting rules to onnx tests* replace with tvm.testing.main()* fix conflict* Update as @areusch suggest* pylint pytorch/test_forward.py* Update as @areusch suggest* reformat by black* fix redefined-outer-name lint errors* remove rules* remove rules & fix pylint errors* Remove all unused_var and fix other pylint errors* reformatted by black* [CI] Apply linting rules to onnx tests* Disable unused-argument is for some unused-argument in function can not be removed* Fix ci errors* reformatted by black* Fix ci errors* Fix invalid-name/unused-variable,redefined-builtin* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to darknet tests* [CI] Apply linting rules to pytorch tests* [CI] Apply linting rules to tflite tests* reformat by black* [Pylint] Making frontend tests pylint compliant Part 1 of N* Fix ci errors* Fix invalid-name pylint errors* Fix invalid-name pylint errors* Disabale temporarily* Fix dangerous-default-value* Fix typo errors* Fix ci errors* [CI] Apply linting rules to tensorflow tests* Fix dangerous-default-value* Fix ungrouped-imports* [CI] Apply linting rules to tf tests* [CI] Apply linting rules to tflite tests* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to caffe2 tests* reformat by black* reformat by black* [CI] Apply linting rules to coreml tests* [CI] Apply linting rules to darknet tests* Update tests/python/frontend/coreml/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/tflite/test_forward.pyCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>* Update tests/python/frontend/coreml/test_forward.py* Update tests/python/frontend/tflite/test_forward.py* fix test errors* fix ci test errors* [CI] Apply linting rules to keras tests* [CI] Apply linting rules to oneflow tests* [CI] Apply linting rules to onnx tests* replace with tvm.testing.main()* fix conflict* Update as @areusch suggest* pylint pytorch/test_forward.py* Update as @areusch suggest* reformat by black* fix redefined-outer-name lint errors* remove rules* remove rules & fix pylint errors* Remove all unused_var and fix other pylint errors* reformatted by black* [CI] Apply linting rules to onnx tests* Disable unused-argument is for some unused-argument in function can not be removed* Fix ci errors* reformatted by black* Fix ci errors* Fix invalid-name/unused-variable,redefined-builtin* [CI] Apply linting rules to caffe tests* [CI] Apply linting rules to darknet tests* [CI] Apply linting rules to pytorch tests* [CI] Apply linting rules to tflite tests* reformat by black* [Pylint] Making frontend tests pylint compliant Part 1 of N* Fix ci errors* Fix invalid-name pylint errors* Fix invalid-name pylint errors* Disabale temporarily* Fix dangerous-default-value* Fix typo errors* Fix ci errors* [CI] Apply linting rules to tensorflow tests* Fix dangerous-default-value* Fix ungrouped-imports* Fix boolean value of Tensor with more than one value is ambiguous* Fix 'NoneType' object has no attribute 'shape'* Fix boolean value of Tensor with more than one value is ambiguous* reformatted by black* [CI] Apply linting rules to caffe tests* Fix dangerous-default-value* Fix conflict & fix pylint error* restoreCo-authored-by: Sebastian Boblest <sebastian.boblest@etas.com>Co-authored-by: Andrew Reusch <areusch@gmail.com>",0
[MetaSchedule] Filter vector_load_lens based on buffer dtype (#12408)This makes the same config generic to work across workloads with different types.,5
[TIR] Add pass ManifestSharedMemoryLocalStage (#12355)Added a pass to insert local (cache) stage for the shared memory. It's similar to cache read but bypasses the limitation of int set analysis for compacting buffer region by inferring the buffer shape from the loop extents.,1
"[TVMScript] Printer VarTable (#12336)This PR:- Adds VarTable for the new TVMScript PrinterCompared to the prototype version, this:- Removes unnecessary public methods.  - GetObjectName  - GetUniqueName- Add Frame parameter for `Define` methods. VarTable will add callback to Frame to remove variable when Frame exits.- Changes DocFactory from `ExprDoc(ObjectPath)` to `ExprDoc()` to simplify var definition.Tracking issue: https://github.com/apache/tvm/issues/11912",1
[TIR] Expose ScriptComplete in header (#12419),5
[TOPI][OP]change float multiplication of resize op to integer division (#12315)* [TOPI][OP]change float multiplication of resize op to integer division* add unittest.* 1. add docstring2. rename param,1
[Fix] Fix precision issue in FFI converting `int/float` to `PrimExpr` (#12417),0
[Fix] Fix errors in error checking and reporting (#12423),0
Add `operator()` to `support::With` (#12418),1
[TIR] Fix assert for tensorcore int8 intrinsics (#12365),0
[TIR] Allow converting `BufferRegion` to vectorized `BufferLoad` (#12420),5
[Fix] Fix `dtype` in Cache-Read/Write (#12421),0
[TIR] Avoid `import *` in TIR tensor intrinsic registration (#12424),2
[TIR] Allow `tir.Buffer` converted to `BufferLoad/BufferRegion` with `__getitem__` (#12422),5
"[TIR] Expose: `call_packed_lowered`, `call_cpacked_lowered` (#12425)Added the following operations in TIR:- call_packed_lowered- call_cpacked_loweredCo-Authored-By: yongwww <yongcale@gmail.com>",1
[TIR] Expose Stack-related TVM builtins in Python (#12429)Added the following operations in TIR:- `tvm_stack_alloca`- `tvm_stack_make_shape`- `tvm_stack_make_array`Co-Authored-By: yongwww <yongcale@gmail.com>,1
[CI][AArch64] Enable ONNX installation in ci_arm image (#12438)This patch enables ONNX and dependencies installation on ci_arm asa way to enable ONNX and Torch testing.Change-Id: I818db28dea2a3d4ae66e775aa15f7ed2f059d673,2
"Add ci_riscv image, update all to 20220810-060142-fae79bbc3. (#12369)",1
[TIR][UX] allow override when register TensorIntrin (#12439)* allow override when register TensorIntrin* lint,5
"Zephyr: Add support for FVP (#12125)adds corstone300 FVP to the platforms supported by the zephyr. We use the Iris debugger to communicate with the emulator via semihosting, due to the FVP serial port's faulty behavior.also changes the generated micro-projects build system from make to ninja.Co-authored-by: Andrew Reusch <areusch@gmail.com>",0
[MetaSchedule] Add logging of usage of tensor intrinsics (#12445)* [MetaSchedule] Add logging of usage of tensor intrinsics* fix,0
[TIR] Expose Misc TIR operations to python (#12435)This PR exposes the following TIR operation in python:- `assume`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_transform_remove_assume.py#L34)- `undef`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_transform_remove_undef.py#L63)- `likely`: tested [here](https://github.com/apache/tvm/blob/bb513866ad70fa20eb0c37ca339d330d6a76c747/tests/python/unittest/test_tir_schedule_compute_at.py#L849)Co-Authored-By: yongwww <yongcale@gmail.com>,3
[ETHOSN] Add support for Requantize (#12384)This commit adds support for the requantize operator for the Arm(R) Ethos(TM)-N NPU.,1
"Use std::optional instead of dmlc::optional, NFC (#12443)* Use std::optional instead of dmlc::optional, NFC* Fix linter* Set deployment target to macOS 10.13Otherwise std::optional<T>::value() is ""unavailable""...* Fix linter again* Update Hexagon apps to use C++17 as the C++ standard",0
[HotFix] Op is not bound to any variables (#12401)* [HotFix] Op is not bound to any variablesAfter PR #12349 inference of some Adreno networks was broken. In theoutput it was the following error:```TVMError: Not all Vars are passed in api_args: 'compute' is not boundto any variables```,0
"[Arith] Parse > and >= bounds in ConstIntBoundAnalyzer (#12457)Previously, only `<` and `<=` bounds were parsed, as these are thecanonical form produced by the `RewriteSimplifier`.  However, theconstraint may also be supplied by the user through the Python API,and may not be canonicalized prior to parsing.",5
Expose `Struct/Tuple`-related TVM Builtins (#12452)This PR exposes the following TIR operation in python:`tvm_tuple`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L554)`tvm_struct_get`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L200)`tvm_struct_set`: tested [here](https://github.com/apache/tvm/blob/c477c763c37adf29b34528ca52d231d622719b3e/tests/python/unittest/test_tvmscript_roundtrip.py#L2432)Co-Authored-By: yongwww <[yongcale@gmail.com](mailto:yongcale@gmail.com)>cc @junrushao1994,2
"Change tir::GetPointerType to return std::optional<DataType> (#12458)It was returning a std::pair<bool, DataType> to emulate the behaviorof std::optional.",4
[TVMScript] Printer IRDocsifier (#12396)This PR:- Adds IRDocsifierThis PR is in draft state because it's branched off from a pending PR #12336Tracking issue: https://github.com/apache/tvm/issues/11912Co-authored-by: Greg Bonik <gbonik@octoml.ai>,1
"Use std::make_unique instead of std::unique_ptr(new ...), NFC (#12459)",1
"Remove uses of std::iterator, NFC (#12461)std::iterator is deprecated in C++17. It only defined some member types,and the new recommended solution is to define these members directly.",1
"Use std::string_view, remove experimental or pre-14 variants, NFC (#12460)",4
[TVM PyTorch Integration] libstdc++ CXX11 ABI Compatibility & boolean tensor support (#12232)* first commit* rename* cmake* deprecated* newline* config* config* typo* skip tvm_class* rename* delete ptr* delete ptr* save progress* boolean support* cmake file* polish code* compile config* improving the codes* format* doc&errormsg* zero-cost copy* one step* to ndarray* extra output* delete extra codes* update test* boolean support* strong test* decrease memory copy* polish* reformat* polish* remove redundant importCo-authored-by: juda <yzhou@octoml.ai>,0
Expose Missing TIR Builtins to Python (#12466)This PR exposes the following TIR operation in python:`address_of`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L3247)`lookup_param`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tir_usmp_analysis_extract_bufferinfo.py#L171)`infinity`: add new unittest`reinterpret`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L2991)`isnullptr`: tested [here](https://github.com/apache/tvm/blob/d2f9f254d275df256dbcbc5a9f8b3a07cee1d81f/tests/python/unittest/test_tvmscript_roundtrip.py#L260)Co-Authored-By: yongwww <yongcale@gmail.com>,1
[TIR] Expose TVM Backend API-related Builtins and Misc (#12468)This PR exposes the following TIR operation in python:`tvm_thread_allreduce`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_type.py#L135)`type_annotation`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L718)`tvm_access_ptr`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L717)`tvm_throw_last_error`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L343)`TVMBackendAllocWorkspace`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L340)`TVMBackendAllocWorkspace`: tested [here](https://github.com/apache/tvm/blob/bcc7cde95c1e84b85f18c07110489350865b8cfe/tests/python/unittest/test_tvmscript_roundtrip.py#L465)Co-Authored-By: yongwww <yongcale@gmail.com>,0
[skip ci][microTVM] Add pytest-xdist to pyproject.toml (#12478),1
[docs] Add instructions for uploading CI resources to S3 (#12476)These were missing the final step to use the uploaded resources,1
"[Frontend][Pytorch] Add axis N when maxpool3d layout is (C,D,H,W) (#12467)* Add axis N if input is (C,D,H,W) layout.* Add (C,D,H,W) test case.",1
[MetaSchedule] Handle deserializing empty string RVs in trace (#12481)* trace.cc* add tests* remove assert* add proper test* lint* lint,1
[HEXAGON][TOPI] This PR adjusts schedules so >64 length vector loads/stores are not generated at LLVM level. This is a workaround for an instruction selection issue in current version of llvm for hexagon (#12471),2
[COMMUNITY] Adam Straw -> Reviewer (#12480),3
[TIR] Disallow vectorization with strides in VerifyGPUCode (#12477),5
"[TVMScript] IRBuilder, IRBuilderFrame base class (#12482)* [TVMScript] IRBuilder, IRBuilderFrame base classThis PR introduces basic data structures of the generic IRBuilderacross the codebase.IRBuilder is a general-purpose IRBuilder that can be used in TIR, Relaxand any other vendor-specific dialects; IRBuilderFrame is where contexualinformation as stored in the IRBuilder.* fix linter* Update include/tvm/script/ir_builder/base.hCo-authored-by: Junru Shao <junrushao1994@gmail.com>",0
Fix memset of memory pool in PageMemoryManagerCreate (#12437),0
Add RISC-V build/test pipeline to Jenkins. (#12441),1
[HEXAGON] Auto-vectorization (fp16) for v68 (#12397)* Auto-vectorization (fp16) for v68* use tvm.testing.main in fp16 test of tanh_slice op,3
[TIR] [bfloat16] add bfloat16 promotion for CallNode (#12370)* add bfloat16 promotion for CallNode* add softmax to bfloat16 build test,1
"[CMSIS-NN] Re-use CPU Target Parser (#12320)Previously `CMSISNNFlags` was derived using logic specific to the external code generator, this converts the external code generator options into a `Target`.",2
"[Target] Only append default keys if target doesn't have any yet (#12474)* [Target] Only append default keys if target doesn't have any yetThis allows target parsers to provide their own target keys. Without thischange, the default keys would always be appended, which may or may notbe desirable.* Add ""cpu"" to ARM CPU keys* Add ""cpu"" to the keys in the mprofile target parser* Restore the mprofile cpptest, since the ""cpu"" key is back* So the -device attribute is actually needed...",1
"[ci][tvmbot] Search more users when checking usernames (#12491)To figure out a user's association with the repo this code beforesearched the associations in the repo filtered by the relevant username.GitHub doesn't return the exact match only though, so we have to insteadcollect many results and search through all of them.Co-authored-by: driazati <driazati@users.noreply.github.com>",2
[COMMUNITY] MichaelJKlaiber -> reviewer (#12501),3
Fix test_autotune to support schedules with no tuning space (#12484),0
[microTVM] Add config space to dense_dsp schedule (#12444)* add config space* lint* lint,1
[TOPI]fix scatterND large shape problem (#12200)* fix scatterND large shape problem* fix thread pool alloca* add scatternd unit test* update with comment* EmptyCo-authored-by: wrongtest <wrongtest0@gmail.com>,0
fix group_conv3d caculate error (#12500),0
[Fix] Fix some typos (#11503)Fix some typos in src/.Co-authored-by: driazati <driazati@users.noreply.github.com>,0
fix pytest (#12483),0
[Relay][Layout] Add FInferCorrectLayout for L2 norm layout transform. (#12497)* [Relay][Layout] FInferCorrectLayout for L2 norm layout change.* [Relay][Layout] Test for L2 norm layout transform.* [Relay][Layout] Re-edit test to add multi-dimensional axis list.* Fix cpplint errors* Use clang-format-10 rules.* replace uint with size_t.,0
fix temp array object reference in manifest_shared_memory_local_stage (#12516),0
"[TIR][Schedule][UX] Beautify TIR Trace Printing (#12507)Following https://github.com/apache/tvm/pull/12197, this PR introduces`Schedule.show()` which convenience the user experience in the followingtwo aspects:- Python syntax highlighting- Outputs a schedule function instead of standalone instructions so thatit's easier to follow.To demonstrate this change:- Before `Schedule.show()` is introduced:<img width=""555"" alt=""image"" src=""https://user-images.githubusercontent.com/22515877/185713487-03722566-1df7-45c7-a034-c1460d399681.png"">- After this change:<img width=""583"" alt=""image"" src=""https://user-images.githubusercontent.com/22515877/185713564-c54f3a9d-cd52-4709-a8b8-d8a61361e611.png"">",2
[MetaSchedule] Implement ScheduleFn as a C++ class (#12513),5
"[MetaSchedule] Migrate MemoryDatabase to C++ (#12514)This PR migrates the existing MemoryDatabase, which is implemented inpython at the moment, to C++. The original intent of having an in-memorydatabase that does not persist on disk is merely for testing, but astimes go on, we found it useful in production workflow, and thus decidedto migrate it C++ for potentially better performance.",3
[COMMUNITY] An Wang -> Reviewer (#12517),3
[TVMScript] Printer entry point (#12462)This PR:- Adds an entry point for the TVMScript Unified Printer- Adds a helper object class `RootNodeContainer` to provide an injection point for the actual printer implementation to add specialized logic on the root node to print.Tracking issue: https://github.com/apache/tvm/issues/11912,1
[TVMScript] Printer: add boolean operators to OperationDoc (#12518)This PR adds boolean operators to OperationDoc. This is needed by the TIR expression printing because it has `tir::And` and `tir::Or`.Tracking issue: #11912,1
fix group conv3d pack kernel shape error (#12523),0
"[ETHOSN] Remove support for older versions of the driver stack (#12347)Removes support for driver stack versions older than 22.05(semantic 3.0.1). Additionally, changes the integration to makeversion checks using semantic versioning rather than the previousyear.month versioning method.",1
[TIR] Support AllocateConst nodes in TensorIR scheduling flow (#12489)* [TIR] Support AllocConstantNode in CreatePrimFunc* Handle AllocConstantNode in LeafBlockRemovalPlan* Properly handle AllocConstNode in BufferAllocationLocator* handle AllocateConst in EstimateFlops* remove NDArray printing* doc update* add test* cpplint* Removed dependency on link-params attribute from target* Restored NDArray printing to unbreak test,1
"[ONNX] Fix test to disable default ONNX frontend constant folding (#12532)In TVM ONNX frontend, constants are folded by default, which makes `test_load_model__onnx` to fail because it is looking for ""params"" that were already converted into constants.This patch fixes the test to disable constant folding so that we can assert that ""params"" in the model are present as expected.",0
"[CI] Set test dependency on ""transformers"" package with pytest.importorskip (#12528)`test_meta_schedule_integration_extract_from_bert_base` depends on the `transformers` package, which is not currently installed in our Docker images.When running this test currently, it fails with an ImportError. This patch makes this dependency explicit and will make the test to be skipped when the dependency is not installed.`test_meta_schedule_integration_extract_from_bert_base` is part of the integration tests, which is currently only running on AArch64 and CPU image (both not at the moment with torch installed in the live CI system), so this is another issue to be understood/fixed.",0
[MicroTVM] expose project options in autotuning (#12479)* expose project_options in autotuning* address comment* address commentCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,1
[TIR][Schedule] Support for specific consumer block targeting in cache_read (#12505)* Add optional consumer blocks to cache_read.* remove comments* Fully functional* Add test for consumer targetting.* Formatting.* Add missing parameter comment.* Fix comments* Simplify type of consumer_blocks in python.* Change how consumer_blocks is printed in python.,0
[FIX] Fix bug in resize2d unittest func name (#12498),0
[ci] xfail failing ethosu codegen tests (#12508)This adds a testing utility so we can mark parameter combinations asxfail without having to manually match each parameter from the name intothe code. The param strings here come directly from CI logs as inhttps://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12389/5/pipelineCo-authored-by: driazati <driazati@users.noreply.github.com>,1
"[CI] Add alexnet and googlenet caffe model to request hook (#12510)This PR intends to move the alexnet and googlenet caffe models from the old link to s3, therefore getting rid of the flakiness in `caffe/test_forward.py` introduced by external url timeouts. Fixes #12465",0
"[LLVM] Add ""cl-opt"" attribute to target_kind ""llvm"" (#12440)* [LLVM] Add ""cl-opt"" attribute to target_kind ""llvm""Add LLVMTargetInfo class that can be used to query the LLVMconfiguration without forcing an LLVMTarget to be created.There is no programmatic way to obtain the actual type of an LLVMoption. The type is necessary to obtain the value of the option,hence it must be provided as a part of the option string.See src/target/llvm/target_kind.cc for more information about thesyntax.* Fix lowercasing of bool value string* Use std::optional instead of std::pair<..., bool>* Treat malformed options as fatal errors* Fix linter* More unit tests for option parsing, have one case per test* Remove ""option ignored"" from fatal error messages",0
"[BugFix][UMA] Fix order issue in uma_lower  (#12447)There was a flaw in uma_lower (see issue #12410) that lead in some case to a different argument ordering of the cached_func and the Relay function. This results in an incorrect lowering of the primfunc and eventually a wrong result of a run-time error, in some cases.This commit adds code to correct the described misbehavior and a unit test case to check this end-to-end functionality with a TFLITE model.",0
[TIR] Add pass to check for out of bounds memory access (#12352)* [TIR] Add pass to check for out of bounds memory accessThis is a conservative static analysis that checks to see if any out ofbounds array access occurs. It is not enabled by default.* formatting* manually construct local irmodule* update comment* fix bug in int_set,0
Remove mutable defaults in mlp_model (#12546),2
check for CMSIS_PATH in project generation (#12547)Co-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,5
[microTVM] Rework evaluate_model_accuracy into a more generic helper function (#12539)* Add workaround for #12538* Rework evaluate_model_accuracy into predict_labels_aot,1
[microTVM] Replace static fixtures with parameterization (#12530)* Replace microTVM static fixtures with parameterization* [microTVM] Only perform parameterization when fixture is present* Reformat with black* Fix Cortex-M tests* Add docstring to pytest_generate_tests* Remove trailing space from docstring,0
"[docs] Add CI contribution instructions (#12551)This PR documents the steps to introducing a new CI docker image, which we've been doing a lot lately.",1
"[ACL] Adjust mobilenet test for Keras 2.9 (#12541)In Keras 2.7, one ""reshape"" operator was removed fromthe Mobilenet model, making our test which verifies thenumber of operators to be incorrect.This patch adjusts the operator count so that it is in linewith the changes in Keras. For reference, the change inkeras repo was done in hash b6abfaed132 ""Remove unnecessaryreshape layer in MobileNet architecture"".",1
[COMMUNITY] @konturn -> Reviewer (#12543)Co-authored-by: Leandro Nunes <leanun01@e123855.arm.com>,3
Fix TFLite 2.9 tests (#12130)This pr fixes the tests that will be broken when we will update TFLite tothe 2.9 version.We will update TensorFlow and TFLite versions to 2.9 so that we canbenefit from improvements in packaging to support multiple platformsand Operating Systems.,0
[CMSIS-NN] Pad fusion with QNN Conv2D (#12353)Pass that fuses nn.pad and qnn.conv2d for CMSIS-NN target.,4
"[CI][AArch64] Skip libgomp failures in integration tests (#12554)Some integration tests are failing when running in CI machines thathave torch installed (validated only in AARch64 for now), with anerror message related to libgomp, similar to the one above:OSError: /.../dist-packages/torch/lib/libgomp-d22c30c5.so.1: cannotallocate memory in static TLS blockAs part of enabling the integration tests in AArch64, I'm marking thistests as skipped, so that tests can start executing and don't regresswhile we take time to investigate these specific failures.",0
[ETHOSN] Fix requantize output conversion (#12540)Fixes a small issue when converting the output information to the support library API. The `requantize_info` output datatype needed updating with the output datatype from the relay function to ensure the graph is compiled correctly by the support library. Included a test to prevent regression in the future.,0
[Relay] Add Rsqrt to SimplifyExpr (#12363)* Add Rsqrt to SimplifyExpr* fix unit tests,0
"[AutoTVM] Add support for text buffers to ApplyHistoryBest (#12521)Currently, AutoTVM's ApplyHistoryBest class does not support loading tuning logs from memory. This is a pet peeve of mine, as it requires you to work with a tempfile whenever writing autotuning tests. This is also just strange, as the rest of AutoTVM has support for text buffers (e.g. tvm.autotvm.callback.log_to_file supports passing in a text buffer, letting us write to but not read from them).Additionally, ApplyHistoryBest handles input arguments very unintuitively. Before this PR, it allowed users to pass string filepaths, a list of string filepaths, or an Iterable (such as a list) of input and result tuples. However, it did not support taking in StringIO objects as mentioned above, nor pathlib.Path objects, nor combinations of a filepath and an Iterable of tuples.In a perfect world, we would change ApplyHistoryBest to take as input a path-like object, file-like object, or an Iterable of input and result tuples (similar to what ApplyGraphBest takes as an argument). However, this would break the existing functionality to take as input a list of filepaths.To be backwards compatible, while fixing this issue, this pull request defines a new type inside dispatcher.py:Records = Union[    Union[str, bytes, Path],  # Path-like objects    TextIOBase,  # File-like objects    Iterable[Tuple[MeasureInput, MeasureResult]],]It then rewrites ApplyHistoryBest.load so it takes the following arguments:def load(self, records: Union[Records, Iterable[Records]]):This PR also adds unit tests for this new functionality, and fixes a relevant bug in tests/micro/common/test_autotune.py in which a StringIO object was passed to apply_history_best, causing it to appear to pass but not actually read any data.",0
"[skip ci][ci] Mark more ethosu tests with xfail (#12560)See #12511 for context. Since more parameterizations are popping up asfailed, this disables whole tests rather than specific combinations ofparameters.",3
"[CI] Remove Vela from ci_cpu (#12533)While the dependencies for microNPU and CMSIS-NN moved into ci_cortexm,Vela is still installed in ci_cpu. As a result, we have some of the microNPU tests outside oftest_ethosu folder failing since they use precence of Vela to decide whether to skip thetest.This change will* remove Vela from ci_cpu* remove unnecessary PATH update",1
[ETHOSN] Add support for special indices of Reshape (#12556)This pr adds support for the special indices values of the reshape operator for the Arm(R) Ethos(TM)-N NPU.,1
[MicroTVM] add heap-size to project options (#12390)* heap-size is added to project options* change stm32l4r5zi recommended heap size* change stm32l4r5zi recommended heap size* addressing comments* addressing comments* addressing commentsCo-authored-by: Mohamad <mkatanbaf@users.noreply.github.com>,1
"Replace std::result_of (deprecated in C++17) with std::invoke_result, NFC (#12562)",5
"Add using directives for otherwise hidden virtual functions, NFC (#12561)This silences warning```warning: 'foo' hides overloaded virtual functions [-Woverloaded-virtual]```typically caused by overriding only some overloads of `VisitExpr_` froma set defined in the base class.",1
[Target] Remove deprecated parameters from target (#12416)* remove depricated parameters in target* lint* fix cpp testsfix* remove more configs in test files* address comments* fix error* fix hexagon* fix micro tutorial* fix integration tests* fix hexagon* lint* fix unittest* fix readme* fix assert executor in target* address comments* fix tutorials* fix hexagon target* fix tutorial* fix for tutorials* hexagon,0
[PyTorch][Fix] Fix for numerically unstable logsigmoid (#12563)* Fix numerical instability for log sigmoidFix numerical instability for log sigmoid in pytorch frontend* update* add test for overflow check* merging two tests,0
"[microNPU] Force compute_cycles_hint to be interpreted as an int64 value (#12558)`compute_cycles` can be the size of an int64 value, however it seemsthat when that value is attached to the IR as a pragma from Python,it is interpreted as an `int`, rather than `int64_t`. This commit addsan explicit cast to ensure the value is interpreted correctly.The reason these values started appearing very large and randomly isstill yet to be solved, although the hope is that this fix will unblockCI.Change-Id: Idcdd7d37af1acd665590c87624446a025b50eb3d",0
[CI][CMSIS-NN] Running tests parallel using pytest-xdist (#12557)Introducing -n auto for CMSIS-NN tests to run them inparallel with pytest-xdist. This is needed because ofadditional parameterization done over cpu variants.Change-Id: I02e1b37ead0b0a562b5b1b2dacfeb3fdd7cc1ce3,1
[ETHOSN] Add support for resize (#12535)This commit adds support for the `resize` operator forArm(R) Ethos(TM)-N NPU.,1
"[TIR][CompactBufferAllocation] Improve upperbound estimation of buffer compaction (#12527)Hi, this change wants to add some minor updation to region estimator used by buffer compaction:- Add and clearify among `EstimateRegionStrictBound`, `EstimateRegionLowerBound` and `EstimateRegionUpperBound`     Originally we have `EstimateRegionLowerBound`, actually it implements strict bound estimation IMO. Now add `upper` and `strict` version for where we actually want them.- When estimating upperbounds (eg. in buffer compaction), try estimate each dimension independently when they are dependent accesses where `EstimateRegionLowerBound` is expected to fail.   Eg, `A[i, i], 3 < i < 16`  fails via `EstimateRegionLowerBound` who check indices be independent. But we can still try best to invoke strict bound analysis on each dimension individually.- If range->extent == 1 for `EvalSet(range, dom)`, invoke `EvalSet(range->min, dom)` instead.    Eg, `EvalSet([k*k, k*k+1), dom_k)` results to [-inf, +inf] due to current algorithm limitation but  `EvalSet(k*k, dom_k)` results to a range which makes more sense.",1
[Target] Replace IsaAnalyzer with Target Features (#12322)This is clean up to use the new `target.features` instead of `IsaAnalyzer`.,1
"[CI] Set test python.contrib.test_onnx.test_resize as xfail (#12568)`python.contrib.test_onnx.test_resize` is failing due to a numericalaccuracy issue, reported in #12567. This patch marks that test asan xfail, so that other tests can be enabled, while this one isinvestigated separately.",3
"[ETHOSN] Support multiply conversion to depthwise (#12403)Multiply can be supported when offloaded to the NPU by a conversion to a depthwise convolution operation. This is only supported when the multiply operation has a single single variable input with the other being a constant of shape [1, ..., C]. This commit adds a new pass ""ConvertEquivalents"" (name subject to change) to handle this conversion before codegen.",1
[TIR] Expose Vector-related API in Python (#12571)This PR exposes the following TIR operation in python:- `vectorlow`: tested [here](https://github.com/apache/tvm/blob/592148abf6866a41eefa736efca067d42f5aea86/python/tvm/tir/tensor_intrin/arm_cpu.py#L62)- `vectorhigh`: tested [here](https://github.com/apache/tvm/blob/592148abf6866a41eefa736efca067d42f5aea86/python/tvm/tir/tensor_intrin/arm_cpu.py#L79)- `vectorcombine`: add new unittestCo-Authored-By: yongwww <yongcale@gmail.com>,1
[Hexagon] Add support to run on multiple devices (#12504)* working in parralel using worker* creating launchers per test and clean up* clean up* ci change to distrube tests* ci work with any number of devices* fix running on simulator* adding function docstring* fix android_serial_number to always return a list of string* lint issue* fix internal error when skipping tests while androideserial number is not set* lint issue,0
[Hexagon] Fix missing pytest import (#12565)* Add pytest* lint,0
"[TOPI][Hexagon] Implement quantized avgpool (#12340)* [TOPI][Hexagon] Implement quantized avgpool* Fix pylint errors* Needed to adjust input padding for int8 buffer layout* Fix formatting issue* Add unit test for fixed-point conversion utility functionAlso, address review comments.* Remove pytest.skip for test_avg_pool2d_slice.py to enable on-target testing* Fix formatting issue* Update python/tvm/topi/hexagon/utils.pyCo-authored-by: Christian Convey <christian.convey@gmail.com>* Update comments and error messages* Address review comments* Import Tuple from typing* Address pylint errorCo-authored-by: Christian Convey <christian.convey@gmail.com>",0
[microTVM] Fix `build` directory exists error (#12575)When you build a project from existing project directory using `tvm.micro.project.GeneratedProject.from_directory` it would show up error if build directory previously existed.,0
"[MicroTVM] fix compile error when the compiler implements char as unsigned (#12519)When compiling tvm with micro on the compiler which implements char as unsigned(such as arm-linux-gcc), there is an error:`src/runtime/crt/graph_executor/load_json.c:218:12: error: result of comparison of constant -1 with expression of type 'char' is always false [-Werror,-Wtautological-constant-out-of-range-compare]``    if (ch == EOF || ch == '\r' || ch == '\n') {`The reason is because the implementation of char is undefined, so it's better to specify here that it is signed.",0
[TIR] Expose `shift_left` and `shift_right` to Python (#12584)This PR exposes the following TIR operation in python:- `shift_left`: tested [here](https://github.com/apache/tvm/blob/1afd0593956066635ee49297b731726c9218c91c/tests/python/unittest/test_tir_transform_simplify.py#L487)- `shift_right`: add new unittestCo-authored-by: yongwww <yongcale@gmail.com>,1
[MetaSchedule] Add software pipeline in CUDA tensor core auto tensorization (#12544)cc @Hzfengsy @junrushao @junrushao1994 @masahi @spectrometerHBH,1
[TIR] Expose WMMA-related TensorCore builtins (#12589)This PR exposes the following TIR operation in python:`tvm_load_matrix_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L711)`tvm_store_matrix_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L913)`tvm_mma_sync`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L860)`tvm_bmma_sync`: add new unittest`tvm_fill_fragment`: tested [here](https://github.com/apache/tvm/blob/cd8fd9121deb22b078c9fe73cd8a554e6e7a0e15/tests/python/unittest/test_tvmscript_roundtrip.py#L571)Co-authored-by: yongwww <yongcale@gmail.com>cc: @junrushao cc @Hzfengsy @junrushao1994Co-authored-by: yongwww <yongcale@gmail.com>,1
[PyTorch] Add aten::new_empty (#12591)This PR intends to add `aten::new_empty` which is used for model like `hf_Longformer`.cc: @masahi,1
[CI] Install xgboost in Hexagon image (#12592)Needed for https://github.com/apache/tvm/pull/12587@mehrdadh cc @Mousius @areusch @driazati @gigiblender,2
[microTVM][Zephyr] Add recommended heap size for NRF and qemu_x86 (#12585)This PR sets recommended heap size for qemu_x86 and NRF board to fix memory size with models like VWW using AoT host driven executor.,0
"[CI] Assert some unittests are not skipped in CI (#12436)This PR adds a script that does a diff of skipped tests between the latest successful build on the main and the current branch. Then, it posts a comment with the report on the open PR. #11670",1
[DOC] fix code-block error in debuggging TVM part (#12597)The code block in part Debuggging TVM is not showing up. Just fix it.,0
"[CI] github_cc_reviewers: Catch all exceptions so all reviewers can be processed (#12578)In a recent change, `github.post` throws `RuntimeError` instead of `HTTPError` when the requested reviewer isn't a project collaborator. This prevents other reviewers to be added to the PR, for example, https://github.com/apache/tvm/runs/8001367110?check_suite_focus=true.This PR changes the caller to catch any exception so the execution won't be interrupted.Co-authored-by: driazati <9407960+driazati@users.noreply.github.com>",0
[microNPU] Remove xfail from tests relating to #12511 (#12570)Removes tests previously marked as xfail since the issue has nowbeen resolved.,3
"[ETHOSN] Support conversion of add to depthwise (#12531)In similar fashion to the conversion of mul to depthwise, this commitconverts add when one input is a constant of shape [1, ..., n] to adepthwise convolution. If neither input is a constant, the add isoffloaded naturally like before.The addition testing has been improved to use pytest features.",1
[F2QI] Fix a rounding error on AvgPool when input and output affine scales differ (#12577)cc @sfvaroglu @AndrewZhaoLuo,0
[CUDA][CodeGen] Fix cuda codegen's fp16 inf literal (#12581)* Fix cuda codegen's fp16 inf literal* add relay testcase,0
"[ci] Default to n=2 for test parallelism (#12414)* Revert ""[skip ci] Revert ""[ci] Default to n=2 for test parallelism (#12376)"" (#12413)""This reverts commit 478b672f2b7bb37f529fa6477b3c4ac353217b7a.* [ci] Default to n=2 for test parallelismThis is attempt #2 of #12376 which was reverted in #12413. The changesin `plugin.py` should keep all the tests on the same node so sporadicfailures don't happen due to scheduling.Co-authored-by: driazati <driazati@users.noreply.github.com>",2
[Runtime] Change default alignment to 64 bytes. (#12586)* Change default alignment to 64 bits.* Run dlpack test a few times.* Update alignment in tests.* Revert mma alignment change.* Change default printing of buffer.* Change crt runtime default allocation.,1
[COMMUNITY] @cconvey -> Reviewer (#12598),3
[skip ci][Community] Wuwei Lin -> PMC (#12605)[Community] Wuwei Lin -> PMC,3
[TOPI][Bugfix] Make semantics of empty `axis` in `squeeze` consistent with Relay (#12596)* Fix empty axis of `squeeze` in TOPI.* Add test case for `squeeze` with empty `axis`.* Add LLVM target for `test_squeeze`.,0
[TIR] Expose Memory Copy-Related PTX Builtins (#12611)* Expose Memory Copy-Related PTX BuiltinsThis PR exposes the following TIR operation in python:`ptx_ldmatrix`: tested`ptx_cp_async`: tested`ptx_commit_group`: tested`ptx_wait_group`: testedCo-authored-by: yongwww <yongcale@gmail.com>* apply code review suggestionCo-authored-by: yongwww <yongcale@gmail.com>,2
"[TIR][Schedule] enhance compute_at and reverse_compute_at primitive to choose possible position (#12450)Current TIR ""compute_at"" primitive will compute at it's closest consumers. When a block has multiple producers, whoever compute at later who is behind. But for some special hardware, we usually hope keep the a certain order whatever it's compute at early or late.eg: block A and block B are producers of block C. block A compute at block C first and block B compute at block C later. We hope the result is block B->block A->block C under some loop var.",5
"[SimplifyExpr] Add simplify for dq->arg funcs (#12580)* add simplify for dq->arg funcs* add comments, fix lint* move comments to the right spots",0
"[Hexagon] Initial support for meta schedule tuning (#12587)Enables AutoTVM-style, template-based tuning for Hexagon.To run compiled code on Hexagon, we need to use Hexagon `Session` object https://github.com/apache/tvm/blob/dc522a6ff65b68532cd1bba43827cd981114df2c/python/tvm/contrib/hexagon/session.py#L35 in the metaschedule `RPCRunner`. But for RPC ""session"", `RPCRunner` expects an instance of `RPCSession`, https://github.com/apache/tvm/blob/53fe5966823eee4e011d7228bceab3c82c1d9caa/python/tvm/rpc/client.py#L32,  to be created and used by various customizable functions. Since `RPCSession` and Hexagon `Session` have slightly different API, we cannot use `RPCRunner` with customizable functions directly. So I introduced an alternative implementation of `RPCRunner` for Hexagon.The test is disabled for simulator since `HexagonLauncherSimulator` is not pickle-able due to its `multiprocessing.Process` attribute: https://github.com/apache/tvm/blob/c97895e0ffb512e73c89de7cdee9846f052244fc/python/tvm/contrib/hexagon/build.py#L614Output log from tuning `vrmpy` dense (included in the test)``` ID | Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated--------------------------------------------------------------------------------------------------------------  0 | main | 150994944 |      1 |       380.3399 |     397.0000 |              397.0000 |     32 |--------------------------------------------------------------------------------------------------------------```",2
"[TIR] More hygenic TVM_SREF macros (#12607)Previously, the `TVM_SREF_TO_BLOCK`, `TVM_SREF_TO_FOR`, and`TVM_TYPE_AS` macros required both the input and output variables.The input variable name is useful for improving the error messagereturned, but the output variable name isn't necessary for thisfunctionality, and prevents the macro from being used as part of anexpression.* Generate an immediately-invoked lambda expression to allow for an  independently-scoped `result` variable.* Use parentheses around the input argument, in case the sref is  the result of an expression.* Update all call sites to remove the macro argument providing the  first argument.",0
"[CI] Update Hexagon image to install boost (#12613)The new image has xgboost installed, which I need for https://github.com/apache/tvm/pull/12587Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/279/pipeline",1
"Replace '> >' in templates with >>, NFC (#12615)The problem with greedy lexing of >> as an operator was solved inC++11, and now templates no longer require spaces between >'s.",5
"[Hexagon] Asynchronous DMA support (#12411)Adds adds asynchronous DMA support through the Hexagon User DMA engine with unit tests to validate basic functionality. Asynchronous DMA support here means the ability to ""kick off"" asynchronously a number of DMAs using the Copy API and then to Poll for or Wait on a number of ""in flight"" (not done) DMAs. Enables future testing and development for asynchronous memory copy on Hexagon. For now, Hexagon DMA support remains synchronous in nature through existing hexagon_user_dma_1d_sync interface which uses asynchronous capable HexagonUserDMA class in a synchronous way --- calling Copy and Wait back to back for each request.* use ring buffer to store DMA descriptors* add RingBuffer class; used by HexUserDMA to store descriptors* add test to overflow the HexagonUserDMA ring buffer",1
"[MetaSchedule][UX] Make `Database` with-able (#12520)`ApplyHistoryBest` right now plays a role as the database adaptor to query inside the database.In fact, the logic could be simplified and users only have to deal with `Database` instead of thisextra object.- [x] Add `EnterWithScope`/`ExitWithScope`/`Current` to Database- [x] Migrate `te_filter_func` => ""tir_filter"" in Relay's pass context- [x] Migrate `f_take_tuning_record` => ""Database.query_tuning_record""- [x] Migrate `TECompiler` to use `Database`- [x] Remove apply-history-bestNext PR:- Migrate `f_direct_dispatch` (potentially unify with `apply_fixed_schedule`?)",0
[TIR] Expose MMA-related PTX builtins (#12623)Expose MMA-related PTX builtinsThis PR exposes the following TIR operation in python:`ptx_mma`: tested`ptx_mma_sp`: tested`mma_store`: add new unittest`mma_fill`: add new unittestCo-authored-by: yongwww <yongcale@gmail.com>Co-authored-by: yongwww <yongcale@gmail.com>,1
"[MetaSchedule] Introduce `ScheduleFnDatabase` (#12626)Following #12520, this PR introduces `ScheduleFnDatabase`, a mockeddatabase to allow injecting handcrafted schedules provided by a schedulefunction.The schedule function comes with the following signature:```pythondef schedule_fn(  sch: tir.Schedule,) -> bool:  task_name = sch.mod.attrs[""task_name""]  # ^^^ provides an optional name of the task queried  ...```This mocked database helps incorporate the existing testing utility`apply_fixed_schedule` more formally into the MetaSchedule-Relay buildpipeline, and allows further extension to Relax with the same interface.Next as another follow-up, we will introduce ConcatDatabase that allowsmixing multiple databases, including the mocked and ones from JSONfiles.",0
"[Refactor] Replace std::tie with structured bindings (#12610)* [Refactor] Replace std::tie with structured bindingsWith C++17 enabled in https://github.com/apache/tvm/pull/12337, usingstructured bindings to replace cases where `std::tie` is used todefine local variables.* Added missing header for <optional>* Silenced unused variable warnings after structured bindingsThis is a bug in gcc version 7, resolved in gcc 8.  While gcc version7 is used for CI, we'll need to silence unused variable warningsresulting from using only part of a structured binding.More information: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81767",0
[QNN] Align output_scale/zero_point of sigmoid to Torch (#12624)* [QNN] Align output_scale/zero_point of sigmoid to Torch* [QNN] Align output_scale/zero_point of sigmoid to Torch,5
[microTVM][Zephyr] Disable test_armv7m_intrinsic since it's broken (#12620)add xfail,1
"[ci] Don't update Jenkinsfile timestamp on image updates (#12621)The timestamp in the Jenkinsfile is there to prevent post-mergeconflicts from different PRs that edit the templates mergingnon-sequentially. This is not an issue when a line is edited in placethough, which is often the case when Docker image tags are updated. ThisPR makes it so the timestamp is not updated in these cases which shouldreduce merge conflicts on these types of PRs.",1
"[Utils] Handled Callable in tir.schedule._type_checker (#12633)Previously, `Callable` was handled as an atomic type.  This workedwhen it was included as last element of a `Union[]` annotation with nosubtypes, but raised an error for other use cases, including`Optional[Callable]`.This commit adds explicit checks for `Callable` type annotations tovalidate whether the argument is callable, but doesn't recursivelyvalidate the signature of the callable object, because lambdafunctions cannot have typeannotations. (https://peps.python.org/pep-3107/#lambda)",0
"[TIR] Improved error messages for PrimExpr operator overloads (#12638)Previously, type-checks in boolean operators on `PrimExpr` wouldstate that the type is incorrect, but further investigation would berequired in order to determine what expression caused the error.After this commit, error messages for these type checks include theexpression that was used, and the dtype of that expression.",0
"[ci] Move non-task CI scripts into ci/ folder (#12609)[CI] Update Hexagon image to install boost (#12613)The new image has xgboost installed, which I need for https://github.com/apache/tvm/pull/12587Validated in https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/ci-docker-staging/279/pipelineCo-authored-by: masahi <masahi129@gmail.com>",1
"[TVMScript] support float inf, -inf and nan in TVMScript parser and printer (#12618)* support float inf, -inf and nan in TVMScript parser and printer* address comment and fix lint* use type_extensions.Literal* address comments* fix win build* remove template",0
"[microTVM][ARM-DSP] Fix pool schedule  (#12653)When I built keyword spotting ONNX model, there was an issue with the pool schedule because certain schedules like broadcast and elemwise do not have input tensors.",0
[microTVM]Fix test util functions (#12641)* Fix test utils* Update python/tvm/micro/testing/utils.pyCo-authored-by: driazati <9407960+driazati@users.noreply.github.com>,0
[Hexagon] Expose gtest output through runtime exception (#12502)Expose Hexagon gtest output in CI by raising it as a runtime exception rather than printing it to stdout.,3
"[microTVM][Zephyr] Add missing CMSIS-NN source files to cmake file (#12642)This PR adds missing CMSIS-NN source files to Zephyr cmake template file for models like keyword spotting, anomaly detection, VWW and image classification.",1
"[ci] Add mechanism for trust on certain CI scripts (#12604)This makes it so changes to certain files from users not listed in`CONTRIBUTING.md` are not tested in CI. This is necessary since thesescripts run on the baremetal EC2 instances and not inside Dockercontainers, so they can affect other builds and potentially grab Jenkinssecrets. This checks out the version from the upstream for the listedfiles after running `git checkout`. Tested in CI: [positive](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12604/6/pipeline/) and [negative](https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/PR-12604/9/pipeline/)",1
[MetaSchedule] Complete NCHW Conv2D Winograd Kernel Scheduling (#12648)* Complete winograd scheduling.* Fix test.,0
[TIR] Preserve annotations after lower opaque block (#12572),5
[Testing] Allow NCHW layout in `relay_workload` (#12656),3
"[ETHOSN] Improve inferring new shape of the Reshape operator (#12594)Fixes the case when reshape is > 4 dims. While this cannot be offloaded to the NPU, the check was previously producing an error preventing further compilation. The correct behavior is to ensure the check returns False and not offload the reshape.",0
"[TIR][TVMScript] Update printer / parser to make T.allocate return buffer var (#12412)* Updated TVMScript syntax of `T.allocate` to return buffer var.* Added syntax sugar for `T.decl_buffer`. When `data` field is not  specified, `data` will be implicitly created via `Allocate` stmt.  * Updated the existing test cases. Most test cases can be updated by  changing `T.allocate` to `T.decl_buffer`. `T.allocate` in some tests  are updated to `T.allocate` + `T.buffer_decl`, to maintain the  legacy behavior of allocation and implicit buffer declaration (will  be followed up in future PR to adopt `T.decl_buffer`).",1
"[Torch][AArch64] Skip test_load_model___wrong_language__to_pytorch (#12660)This patch makes test_load_model___wrong_language__to_pytorch to beskipped in AArch64 due to a bug that can be reproduced when enablingIntegration Tests in machines with Torch installed in TVM.```The error message seen is:OSError: /usr/local/lib/python3.7/dist-packages/torch/lib/libgomp-d22c30c5.so.1: cannot allocate memory in static TLS block```While the test needs further investigation, it is being set asskipped so other tests can be enabled and not to regress and allowtime for the investigation to be made.This relates to the issue described in #10673.",0
[ci] Add linter for PR title and body (#12367)* [skip ci][ci] Fix Jenkinsfile (#12387)This got out of date after merging #12178Co-authored-by: driazati <driazati@users.noreply.github.com>* Address commentsCo-authored-by: driazati <driazati@users.noreply.github.com>,0
"[TIR] Allow string/buffer arguments to Schedule cache_read/write (#12661)Previously, the argument needed to be an integer specifying the indexinto the read/write regions of a block.  Now, the argument can be astring specifying the name of the buffer, or the Buffer object itself.This is a follow-up from https://github.com/apache/tvm/pull/11624.",5
[ETHOSN] Fix tests pylint errors (#12649)This pr fixes pylint errors in tests/python/contrib/test_ethosn as reported in issue #11414.,0
[Relay] Extract intermediate node by its expression ID (#12646)[Relay] Extract Intermediate Expr by relay expr ID for analysismodify doc commentsCo-authored-by: Bin Li <binli1@amd.com>,5
"[Hexagon] Implement fixed_point_multiply op through intrinsics. (#12659)This commit adds high-performance implementation of fixed_point_multiplyoperation based on Hexagon intrinsics for vmpye/vmpyo instructions.Benchmarking of 'fixed_point_multiply' op with (1,8,56,56,32) inputtensor on Qualcomm SM8350:  * default implementation: 10.06 ms  * optimized implementation: 1.42 ms  * speedup: 7x times (!!!)Please note that this is introducing a small round-up error for somecorner cases with negative shift argument (The same as for ARM CPU, seePR#5980). This is because we are rounding twice instead than only once:  * original q_multiply_shift: round(x*y*2^-s)  * hexagon q_multiply_shift: round(round(x*y)*2^-s)",0
[MetaSchedule] Fix autoinline for single const consumer block (#12668)fix autoinline and add test,0
Add methods to get and set late-bound constants. (#12664)* Add methods to read and restore late-bound constants on Executable.* Add bindings for new functions* Cleanup* Fix function name* Add tests for python API to access new load/save functions* Add another tests for python API to access new load/save functions where there are no constants,0
[Adreno] Change compute/schedule for ToMixedPrecision pass (#12537)* [Adreno] Change compute/schedule for ToMixedPrecision pass* Address CI fails* address PR comments* Fix AutoTVM flow,0
[hexagon][tests] re-enable maxpool hardware test (#12676)- Re-enable test_max_pool2d_slice.py when run on Hexagon  hardware (as opposed to hexagon-sim).  This is now safe because https://github.com/apache/tvm/issues/11928  has been fixed.,0
[HEXAGON][TOPI]Slice Op Argmax uint8 (#12472),5
"[MetaSchedule] Introduce `Union` and `OrderedUnion` in Database (#12628)Following up #12520 and #12626, this PR introduces two database classes:`UnionDatabase` and `OrderedUnionDatabase`, both of which allow users toorganically compose multiple databases together, so that the high-levelIR (Relay, Relax) could select the best tuning records according torunning time or a preferred order given by users.To each query, `UnionDatabase` returns the best record among all thedatabases given; Instead, `OrderedUnionDatabase` returns he record fromthe first database that responds to the query.Used together, users may specify complicated dispatching patterns likebelow:Examples below demonstrate the usecases of and difference betweenUnionDatabase and OrderDatabase.Assumption:* db1, db2 do not have tuning records for the target workload.* Each of db3, db4, db5 has tuning records r3, r4, r5 for targetworkload respectively.```python#### Case 1. `UnionDatabase`:merged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    db4  # has r4)# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)### Case 2. `OrderedUnionDatabase`merged_db = ms.database.OrderedUnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    db4  # has r4)# returns r3merged_db.query_tuning_record(..., target_workload)### Case 3. Mix-use scenariomerged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    ms.database.OrderedUnionDatabase( # returns r4        db4,  # has r4        db5,  # has r5    ))# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)### Case 4. Another mix-use scenariomerged_db = ms.database.UnionDatabase(    db1, # no record    db2, # no record    db3, # has r3    ms.database.UnionDatabase( # returns the better one between r4 and r5        db4,  # has r4        db5,  # has r5    ))# returns the best one among r3, r4 and r5merged_db.query_tuning_record(..., target_workload)### Case 5. Yet another mix-use scenariomerged_db = ms.database.OrderedUnionDatabase(    db1, # no record    db2, # no record    ms.database.UnionDatabase( # returns the better one between r3 and r4        db3, # has r3        db4, # has r4    )    db5,  # has r5)# returns the better one between r3 and r4merged_db.query_tuning_record(..., target_workload)```Co-authored-by: sunggg <49998730+sunggg@users.noreply.github.com>",2
[TIR] Handle DeclBuffer in ToSSA (#12679),5
"[COMMUNITY] Yaxing Cai -> Reviewer (#12683)Please join me in welcoming Yaxing Cai (@cyx-6) as a new reviewer in TVM. Yaxing has brought the PackedFunc into TVM object system ([RFC-051](https://github.com/apache/tvm-rfcs/pull/51)), designed and implemented the new parser infrastructure for TVMScript and meta-programming ([RFC-079](https://github.com/apache/tvm-rfcs/pull/79))- [Commits History](https://github.com/apache/tvm/commits?author=cyx-6)- [Code Review](https://github.com/apache/tvm/pulls?q=reviewed-by%3Acyx-6+)",1
[PyTorch] Fix aten::arange for pytorch (#12681)fix arange for pytorch nightly 20220815,0
"[MetaSchedule][UX] Convenient Object Creation (#12643)This PR introduces a set of `.create` methods making it easier to createMetaSchedule objects.For example:```pythonms.database.JSONDatabase(...)ms.database.create(""json"")ms.runner.RPCRunner(...)ms.runner.create(""rpc"")```Besides, this PR allows `JSONDatabase` to be created via `work_dir`:```pythondb = ms.database.create(""json"", work_dir=""/path/to/db/"")db = ms.database.create(work_dir=""/path/to/db/"")  # or even simpler```",5
[ETHOSN] Fix some more pylint issues (#12675)Fixing a few more pylint issues caught when using pylint==2.9.3.Change-Id: Ie7ca61e1a8083a40e0ffccf1418192966884707a,0
"[ETHOSN] Add support for concatenate with negative axis (#12686)Supports offloading concatenate with a negative axis to the NPU. In addition, parameterized the concatenate unit tests.",1
"[ci][tvmbot] Trigger GitHub Actions after merging (#12361)This fixes the issue where merging from GitHub Actions (i.e. with the default `GITHUB_TOKEN`) doesn't trigger post merge GitHub Actions on the commit it creates in `main`. Instead these jobs are triggered manually by a call to the Actions API after the merge has taken place.This also updates the tvmbot testing code (and by extension some of the other CI testing code) to remove the fixtures for each test in favor of constructing them from a single sample at runtime, this makes it a lot easier to add new tests and see what is different between each data sample and clean up the testing anti-patterns that were there before (e.g. `run()` instead of `pytest.mark.parameterize`, but none of the tests in `test_ci.py` have changed)Tested in https://github.com/driazati/tvm/pull/36 which ran https://github.com/driazati/tvm/actions/runs/2881047903",0
"[AutoTVM][Testing] Add `tune_relay` scripts (#12685)Example:```bashpython -m tvm.autotvm.testing.tune_relay  \       --workload bert_base               \       --input-shape '[1,64]'             \       --target ""llvm""                    \       --num-trials 800                   \       --rpc-host 192.168.6.66            \       --rpc-port 4445                    \       --rpc-key 3090ti                   \       --work-dir /logs/autotvm-bert_base \       --cache-dir /cache-workloads       \       --graph-tuner True                 \       --cpu-flush True                   \       --backend graph```",1
[ci] Add tests for PR linter (#12680)This adds some checks for the current usages of the PR linter and fixes the case where the script would error uncleanly when a PR body was `null`.,0
[Adreno] Define memory_info for global.texture* (#12647)There are now many warnings in the tuning process about undefined memory information when using textures. A definition is required as textures* are tagged.,5
"[Web][Emscripten] Update EMCC C++ standard to C++17 (#12693)As a follow-up to https://github.com/apache/tvm/pull/12337, updatingthe EMCC flags from `-std=c++14` to `-std=c++17`.",1
"[ETHOSN] Use pytest parameterization for integration tests (#12688)Using pytest parameterization helps identify the particular parameter combinations that are failing for a given test. Additionally, it can be useful when parallelizing the tests. This commit makes sure that ""trials"" have been replaced by parameterization as well as completing a general cleanup.",1
"[Apps] Pin android_camera TensorFlow/Keras dependency version (#12710)At the moment, android camera is installing latest TF and Keraswhich is causing the following issue in CI:```  File "".../keras/dtensor/lazy_variable.py"", line 26, in <module>    from tensorflow.python.trackable import base as trackableModuleNotFoundError: No module named 'tensorflow.python.trackable'```This patch fixes the versions in the last known working versionsof both: TF 2.9.1 and Keras 2.9.",0
"[Hexagon][Runtime] Better support for 2-tier memory (#12574)- Introduce 'global.ddr' memory scope:  - Like 'global', this allocates memory from the Hexagon SoC's    DDR memory.  - Like 'global.vtcm', the specified tensor shape must be 1d    or 2d, where 2d indicates Hexagon's ""indirect tensor""    (i.e., discontiguous) allocation scheme.- Change memory-alignment strategy to always be 2048-byte aligned  on Hexagon.  (This can be refined in the future, but for now it  ensures all allocations meet the strictest alignment requirements  for any Hexagon operations.)",3
"[TIR][StorageRewrite] Allow in-place buffer reuse of non-flat memory (#12655)* [TIR][StorageRewrite] Allow in-place buffer reuse of non-flat memoryPreviously, shared buffer use was entirely disabled for non-flatmemory, since the existing checks for shared memory assume flat 1-dspaces.  This was enforced in `FindAlloc` and validated in`PrepareNewAlloc`.  The validation in `PrepareNewAlloc` could trigger,if the buffer sharing was due to an in-place operation, and notthrough the `FindAlloc` function.In-place operations do not require N-d packing, nor do they introduceambiguity in how different code generators may interpret non-flatphysical indices.  Therefore, this commit relaxes the validation in`PrepareNewAlloc`, allowing buffer reuse of non-flat buffers forin-place operations.* Update new StorageRewrite with correct allocate/buffer_decl usage",1
[COMMUNITY] ekalda -> Committer (#12715),3
"[Hexagon] Add optimized schedule for nn.pad (#12714)Motivation:In case of quantized models nn.pad operation typically is not fused with QNN opsand lives as a standalone operation. In this case it uses default injectiveschedule for Hexagon target and it is not optimized very well (based onanalysis of real models like ResNet50 INT8).What was done:New schedule for Pad operation was implemented instead of default injective schedule.For Hexagon target injective schedule does fusion of all axis and vectorizationon 128/64/32 (depends on dtype). It works fine for Add, Sub, etc... but not for Pad.New optimized schedule does these steps (fusion+vectorization) only if last tensordimension is divisible by 128/64/32 (depends on dtype). It was done only for Hexagon,for other targets (x86, cuda, etc.) there is no changes and it uses default injectiveschedule.Benchmark results on Snapdragon 888:4d NHWC layout with ((0, 0), (1, 1), (1, 1), (0, 0)) padding, ""uint8"" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 112, 112, 32)  |         10,03        |           0.2          |    50.1x times    |(1, 56, 56, 128)   |         0,099        |          0,085         |  ~1x (no speedup) |---------------------------------------------------------------------------------------|4d NCHW layout with ((0, 0), (0, 0), (1, 1), (1, 1)) padding, ""uint8"" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 128, 56, 56)   |         10.96        |          1.38          |    7.9x times     |(1, 32, 126, 126)  |          1.66        |          1.58          |  ~1x (no speedup) |(1, 32, 128, 128)  |         13.98        |          2.66          |    5.25x times    |---------------------------------------------------------------------------------------|5d NCHWc layout with ((0, 0), (0, 0), (1, 1), (1, 1), (0, 0)) padding, ""uint8"" dtype:shape              | default schedule, ms | optimized schedule, ms |      speedup      |-------------------|----------------------|------------------------|-------------------|(1, 4, 56, 56, 32) |          6.39        |          0.29          |     22x times     |(1, 56, 56, 128)   |          0.15        |          0.15          |  ~1x (no speedup) |---------------------------------------------------------------------------------------|Summary:For some input tensors we get up to 50x times speedup, for other performance is the same.No performance degradations were detected.",1
"[TVMC] Run module once by default (#12713)* [TVMC] Run module once by defaultCurrently executing `tvmc run module.tar` will run the input modeltwice. For benchmaking this is to be expected as the first run is usedto prime caches etc before taking a measurement. However, this seems abit unintuitive to have as default, especially when benchmarking is notalways intended. In this sense, this commit aims to amend thenumber of runs for the default: `tvmc run module.tar` to a single run.After inspection, this seems to be down to the use of the `.benchmark()`method which runs (1 + repeat * number) executions in total. This meansthat at least two runs are required (i.e. when repeat=1, number=1). Italso seems that it is only necessary to benchmark the model when`--print-time` has been set from the CLI POV. From the python interfacePOV, benchmarking is always run, but this may not always be necessary.This commit makes use of the `.run()` method to singularly execute themodel by default. From the CLI this will be used when `--print-time` isset to False whereas from the python interface this will be used when`benchmark=False`. Otherwise, the `.benchmark()` method will be usedas before. Complementary to this change `repeat`, `number` and`end_to_end` parameters are only used when either `--print-time` or`benchmark` are set to True - and the documentation has been updated toindicate this.Change-Id: I18a38a9d430d660264f7fce5caf0779aa059fed3* improve documentation with number of exectuions when benchmarkingChange-Id: Iecf557594420fcc9f3abcec5ce7d952db2c94271",1
"[Docs] Add Commit Message Guideline (#12689)This commit adds the Commit Message Guideline text to Apache TVMdocumentation in ./docs/contribute/pull_request.rst, under section'Submit a Pull Request', below subsection 'Guidelines', as a subsectionnamed “Commit Message Guideline”. The text in the second-last item insubsection 'Guidelines' that mentions PR tags is also updated to referto this guideline.This documentation will help guide contributors on how to write goodcommit messages when submitting code / creating Pull Requests, inaccordance with RFC-0088:https://github.com/apache/tvm-rfcs/blob/main/rfcs/0088-commit-message-guideline.md",1
"[TIR] Fix pragma_loop_partition_hint attrs should check it's value (#12699)Current LoopPartition doesn't check the value of attribute key ""pragma_loop_partition_hint"". Whatever I set pragma_loop_partition_hint to True or False, the result is same, which is confused for debug.This PR fix pragma_loop_partition_hint attribute key should check it's value.",0
support false-positive fast math (#12702),5
[ETHOSN] Add support for transpose convolution (#12674)Adds support for offloading transpose convolution with an optional biasto the NPU.Co-authored-by: Samuel Panijel <samuel.panijel@arm.com>Co-authored-by: Leo Blonk <leo.blonk@arm.com>,1
[microTVM][Zephyr] Enable -O2 optimization on build by default (#12718)* add spped optimization flag* trigger* add exception for qemu_riscv64,1
[HEXAGON] [TOPI] Dequantize (#12677)dequantize op hexagon,5
"[Build] Update C++ standard to C++17 for AOT, iOS, VTA (#12712)Follow-up from https://github.com/apache/tvm/pull/12337 andhttps://github.com/apache/tvm/pull/12693, updating a few additionallocations that specified C++14.",1
[TVMScript] IRBuilder methods for `IRModule` (#12694)* IRBuilder methods for `IRModule`This PR introduces IRBuilder methods for `IRModule`.Co-authored-by: yongwww <yongcale@gmail.com>* apply code review suggestionCo-authored-by: yongwww <yongcale@gmail.com>,5
"[TFLite][CI] Update TensorFlow dependency to 2.9.1 (#12131)This updates the TF version to be used in TVM CI to 2.9.1,which brings improvements so that more platforms are supported byofficial packages.When building TFLite, an update to CMake was also required,which is updated now to 3.18.4.ethos-u-vela dependency is also updated, from version 3.2.0 to 3.4.0so that it is closer to the TensorFlow version being proposed here.This PR updates the Docker images scripting to install TF and TFLite.Change-Id: I290085f0c018ad57606f1295494c19ff6e1af2dd",1
[ci] Add onnx model to S3 (#12716)Addresses this CI failure on `main`:https://ci.tlcpack.ai/blue/organizations/jenkins/tvm/detail/main/4235/pipeline/Co-authored-by: driazati <driazati@users.noreply.github.com>,1
"[ci] Re-balance shards (#12473)Replace '> >' in templates with >>, NFC (#12615)The problem with greedy lexing of >> as an operator was solved inC++11, and now templates no longer require spaces between >'s.Co-authored-by: Krzysztof Parzyszek <kparzysz@quicinc.com>",5
[TIR] Add unroll_loop_with_partition_hint_no_interval attr in LoopPartitionConfig     to unroll loop (#12631)[TIR] Add unroll_loop_with_partition_hint_no_interval attr in LoopPartitionConfigto unroll loop,1
[OpenCLML] CLML Profiling fixes corresponding to OpenCL Timer recent … (#12711)* [OpenCLML] CLML Profiling fixes corresponding to OpenCL Timer recent changes.* [OpenCLML] Review comments.* * review comment,0
Add Arm DSP implementation of Depthwise Conv2D (#12448),1
"[Relay] Change when int8 operations are converted to int16 on Arm (#12671)Currently, Relay QNN uses its `helper_no_fast_int8_hw_legalization` to convert most `int8` convolution and dense operations into `int16` ones on Arm. This currently occurs on ARM chips except for `v8.2a` chips with `dotprod` support.However, this behavior means that `int8` operations are replaced with `int16` ones on Cortex-M chips. On these chips `int16` is substantially slower, as while it saves a few sign extension operations, it doubles the amount of memory loads we need to perform. This PR changes when `helper_no_fast_int8_hw_legalization` is used on Arm, and instead makes **not** doing this replacement the standard. We will only do this replacement if we are on a chip with ASIMD support but without `v8.2a` and `dotprod`. This ensures that Cortex-M microcontrollers do not have `int8` operations turned into `int16` ones.I have also verified that this does, in fact, improve performance for some common models. For example, MobileNet_v1_0.25 on the Cortex-M4 saw a 10% performance improvement, compared to before this change. Accuracy does not seem to be affected.",2
"[CI][AArch64] Mark tests to be skipped due to torch crash (#12730)Some integration tests are not being run on CI due to theconfiguration of the machine with onnx and torch not callingthe integration tests script.This patch skips two more tests failing with the error messagebelow:```""OSError: /.../torch/lib/libgomp-d22c30c5.so.1:cannot allocate memory in static TLS block""```",0
[MetaSchedule] Mark two tests as xfail (#12733)This patch marks two tests as xfail for further investigation:* test_meta_schedule_integration_extract_from_resnet_with_filter_func* test_meta_schedule_integration_extract_from_resnet,3
"[Test] Add tvm.testing.requires_libtorch (#12737)Create a specific test dependency to map to USE_LIBTORCH, whichis disabled by deafult, and is independent from torch beinginstalled on the underlying machine, so it causes problems inmachines that have torch installed but TVM is build withUSE_LIBTORCH OFF.Mark tests.python.contrib.test_libtorch_ops.test_backend withthis new decorator.",1
"[TIR] Handle axis_separators during FlattenBuffer (#12652)* [TIR] Moved tir.FlattenBuffer to occur before tir.LowerOpaqueBlockFor buffers with more than one physical axis, the `axis_separators`are required in order to know which groups of logical axes to fuseinto each physical axis.  The implementation in `tir.FlattenBuffer`assumed that all buffers were being flattened to a single physicalaxis.  Because `tir.LowerOpaqueBlock` replaces the`BlockNode::alloc_buffers` with `Allocate` nodes, `tir.FlattenBuffer`no longer has access to the axis separators and performs inconsistentflattening for `Allocate` as opposed to `BufferLoad`/`BufferStore`.This was introduced in https://github.com/apache/tvm/pull/12172, whichdecoupled the lowering/flattening steps.The commit reorders the `tir.FlattenBuffer` to occur before`tir.LowerOpaqueBlock`, to make use of the axis separators.  Any`Allocate` nodes that exist at that point (e.g. from hand-writtenschedules) are still flattened to 1-d physical buffers, but the`BlockNode::alloc_buffers` are flattened according to the axisseparators.* Add unit test to validate non-flat memory after tvm.lower* Explicitly write T.reads for test on BufferRegion updates* Update incorrect docstring for test* Use DeclBuffer information in FlattenBufferThe DeclBuffer node can be inserted during LowerOpaqueBlock, thenprovide the missing Buffer information required to flatten theallocation.* Use T.allocate in unit testsWith the insertion of `DeclBuffer` nodes, `LowerOpaqueBlock` no longerneeds to be before `FlattenBuffer`, and has been moved back to itsoriginal position.  Revering the tests to use `T.allocate` instead of`T.alloc_buffer` more closely represents the functions as they arebeing lowered.* Fix usage of T.decl_buffer in updated tests* Update LowerOpaqueBuffer to expect the DeclBuffer nodes* Strip DeclBuffer annotation in FlattenBufferThe DeclBuffer annotations aren't yet supported in all passes.  Thisrestricts them to being introduced in LowerOpaqueBuffer, thenimmediately removed in FlattenBuffer.* Strip out all DeclBuffer nodes in FlattenBuffer* Update unit tests to remove expectation of DeclBuffer nodes",0
"[TIR] Update region min/extent in ReplaceBufferMutator (#12725)Prior to this commit, `ReplaceBufferMutator` only checks`BufferRegionNode::buffer` to determine if a `BufferRegion` needs tobe replaced, and doesn't check the `BufferRegionNode::region`.  As aresult, updating `T.reads(A[B[i]])` would fail to replace `B`.This commit checks `BufferRegionNode::region` for buffer usage toresolve this issue.",1
Move static array initialization into a function go avoid link errors (#12678)* Move static array initialization into a function go avoid link errors* Fix line length,0
"[TIR, Schedule] Check consumer in-bound and covered in reverse_compute_inline (#12717)* [TIR, Schedule] Generate consumer-in-bound predicate after reverse_compute_inline* Check consumer block iters are covered* fix lint",0
[ci][docker] Use CMake 3.20.0 for cortexm (#12744)The Zephyr project builds require 3.20.0 to work correctlyCo-authored-by: driazati <driazati@users.noreply.github.com>,5
[TF] Add DenseBincount support (#12728),1
"[CI] Update Docker images to bring TF 2.9 and integration tests (#12738)[CI] Update Docker images to tag 20220908-060034-62bdc91b1Updates all Docker images to tag 20220908-060034-62bdc91b1, toupdate TensorFlow/TFLite/Keras to 2.9, and cascaded dependenciessuch as numpy. Updates ethos-u-vela to 3.4.0.It also brings ONNX and PyTorch to ci_arm, to enable Integrationtests to be run in CI.Standadises the minimum CMake version required in CI to be 3.18.4,fixing apps/microtvm/zephyr_cmsisnn to require this version.Finally, adds a new import error in the tutorials documentationwhich doesn't affect the final result. The new warning added is'absl:Found untraced functions such as _jit_compiled_convolution_op'",0
Aligned CMSIS-NN SHA in TVM to CMSIS top of tree (#12723)Aligned CMSIS-NN SHA in TVM to top of tree of CMSIS.-Aligned buffer size APIs to CMSIS implementations.-Updated the tests to match new CMSIS context buffer sizes.-This change needs updates to cortex-m docker image.Change-Id: I13f1ad29fe0ef02f08660eca4c818b5d66145ffc,1
[microtvm][Zephyr] Add project overlay to overwrite device tree configs (#12741)* add nucleo overlay,1
[TVMScript] Base IRBuilder methods for `PrimFunc` (#12745)Base IRBuilder methods for `PrimFunc`This PR introduces base IRBuilder methods for `PrimFunc`.Co-authored-by: yongwww <yongcale@gmail.com>Co-authored-by: yongwww <yongcale@gmail.com>,5
"[TVMScript][TIR] Clarify scope of BlockNode::iter_vars (#12726)Previously, it was ambiguous whether `BlockNode::iter_vars` werein-scope for `BlockRealizeNode::predicate`.  `ConvertBlocksToOpaque`treated them as in-scope, and applied a mapping from `iter_vars` to`iter_values`.  Similarly, TVMScript printing places `T.where`statements below the `T.axis` statements, where `T.axis` definitionsare in scope.  However, `BlockRealizeNode::SEqualReduce` and`BlockRealizeNode::SHashReduce` do not visit the block and `iter_vars`until after visiting the predicate, placing the `iter_vars` out ofscope.This commit updates the printing of `T.where` to be above `T.axis`,and updates `ConvertBlocksToOpaque` to report an error if thepredicate contains references to `BlockNode::iter_vars`.  After thiscommit, these three usages all consistently treat`BlockNode::iter_vars` as out of scope for`BlockRealizeNode::predicate`.",0
[OpenCL] Enable OpenCL for GPU tests (#12490)* Add opencl target in test build script* Fix fp16 test and compile test for opencl* fix lint* Fix relay OpenCL texture tests* Fix lint* Enable relay OpenCL tests* Fix opencl relay texture tests* fix lint* Remove OpenCL gtest variable* Fix unbound variable* Skip tests that are not supported in CI* fix lint* Add path for opencl gtest directory* Fix opencl gtests include directory* Enable OpenCL googletest. Fix bug in opencl timer test* testing fix for build cpp tests* update googletest git version for opencl tests build* update cmakelist* Update CMakeList* Update CMakeList* Disable opencl googletests* update Opecnl.cmake* fix Opecnl.cmake* Apply comments. Remove xfail decerator for opencl tests. Now specific tests are skipped in the environment script* minor code changes* apply comments* apply comment* skip test in ci by decorator* fix pytest skipif warnings* Fix skipif for opencl gtests,0
[Frontend][Paddle] Fix op in paddle did't transmit layout information (#12658)[Frontend][Paddle] Fix adaptive_avg_pool2d in paddle did't transmit layout information,0
[TIR][Arith] Add more strict checking in imm construction and folding. (#12515)* Add more strict check in tir imm construction and folding.* fix bool-compare compile error* fix some illegal imm construction in testcases* do not test i64 overflow behaviour because it is not consistent on cython and ctypes* fix float32 testcase* auto-inferred dtype should be int64 when value exceeds int32 range* add floatimm range check for fp16 and fp32* add more folding testcases and fix store fp32 folding result to double* fix i386 fp16 cases,0
[TOPI][Hexagon] Add test and schedule for uint8 resize2d (#12559)* [TOPI][Hexagon] Add test and schedule for uint8 resize2d* Fix correctness issue* Reformat* Remove cubic from testing* Remove unnecessary else,0
[TOPI][Hexagon] Implement quantized elementwise for hexagon (#12606)* [TOPI][Hexagon] Add test and schedule for uint8 resize2d* Fix correctness issue* Reformat* [TOPI][Hexagon] Implement quantized elementwise* Reformat* Address review comments* Reformat* Revert* Address review comments,0
"[ETHOSN] Update driver stack version to 22.08 (#12650)Updates the driver stack used by the NPU to the latest released version(semantic version 3.1.0), while maintaining backwards compatibility forthe previous version 22.05 (semantic 3.0.1) during the migration period.In addition, support for split is re-introduced as this is now supportedin 22.08.Change-Id: I86bce3469f0b8ad52e66461ae055dec6717b3527",1
