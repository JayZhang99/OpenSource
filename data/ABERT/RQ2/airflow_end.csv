commit_msg,labels
First official commit,5
Moving TODO list out of README.md,2
"Lint, fixed typos",2
Adding screenshots,2
Cropped screenshots,2
Fixing screenshot,2
More fixing the screenshots,2
Typo,2
Pickling dags and transfering pickles through the database,5
Moving folders around,4
"Added new ExternalTaskSensor operator, other minor fixes",0
Improved the tutorial,1
Fixing a few links in the README,2
First stab at the sphinx documentation,2
"Getting setup.py working, moving folders around",4
Improving setup.py,1
Bug fix + added initdb as a command line option,5
Improving the documentation,2
Forgot to git add in previous commit,1
Fixing bug,0
"Fixing the initdb script, adding to docs",2
Working on HiveHook,1
Minor touchups,0
Renaming project from Flux to Airflow,5
Changing DEFAULT_EXECUTOR to be an instance instead of a reference to the class,4
Fix the link to the docs,2
Changing the ascii header,4
Adding HiveOperator,1
Adding PrestoHook,1
"Improving PrestoHook to use REST, DatabaseConnection model",5
"Better docs, tweaks",2
HivePartitionSensosr,5
"Adding max_partition macro, fixing bugs",0
Better docs settings,1
Fixing the macros,0
Adding missing dependency,1
Fixed a failing example,0
Added missing init file,2
first version of the celery executor,5
breaking up base_executor into multiple files,2
moving celery settings to settings.py,1
Merge pull request #2 from mistercrunch/celery_v1first version of the celery executor,1
Implementing run method in presto_hook (for view creation),1
Merge branch 'master' of github.com:mistercrunch/Airflow,1
Merge branch 'master' of github.com:mistercrunch/Airflow,1
Fixed dag view to not repeat edges,2
"Tree view, repeated nodes are now collapsed by default",5
Merge branch 'master' of github.com:mistercrunch/Airflow,1
Adding different directional layouts for graph view,1
Linting with flake8,5
Queries,5
Merge branch 'master' of github.com:mistercrunch/AirflowConflicts:airflow/www/app.py,5
Fixing HiveHook to be picklable,1
"Making a generic SqlSensor, works for all db_ids",5
Adding set_dependency method to DAG class,2
Making Jinja templates as files work,1
Syncing,5
Hooking up hostname and port to defaults and command line,1
Moving to setuptools,1
UI: nicer task details view,5
Merge pull request #5 from mistercrunch/minor_uiUI: nicer task details view,1
Minor UI fix/tweak,0
Adding BashLexer,1
Merge pull request #6 from mistercrunch/bash_lexerAdding BashLexer,1
"Providing a way to specify the executor to use while constructing the DAG objectadding host=0.0.0.0 to ensure the service is accesible from outside localhostRevert ""adding host=0.0.0.0 to ensure the service is accesible from outside localhost""This reverts commit ce5436bc7dcd70dcb129c1dfa46f9fd7ce743e43.",4
Merge pull request #3 from mistercrunch/executor_fixProviding a way to specify the executor to use while constructing the DAG object,2
integrating config parser to read config values from a file,2
Addressed the comments on shorter import and space in configuration.py,5
adding a sed comand to replace airflow_home automatically in default airflow.cfg,5
Merge pull request #4 from mistercrunch/config_parserintegrating config parser to read config values from a file,2
Minor tuneupts,5
"Merge pull request #7 from mistercrunch/minor_touchupsMinor fixes, utility functions",1
Fixes,0
Fixing conf,5
Merge pull request #8 from mistercrunch/fixesFixes,0
Denser graph view,5
Merge pull request #9 from mistercrunch/fixesDenser graph view,0
Making the sqlachemy db connection a config param,2
Merge pull request #10 from mistercrunch/db_connMaking the sqlachemy db connection a config param,2
A few tweaks while running core_cx,1
Merge pull request #11 from mistercrunch/tweaksA few tweaks while running core_cx,1
health check + reload dags,2
Fixes,0
Merge pull request #13 from mistercrunch/tweaksFixes,0
hive_hook: Using -f with a temp file instead of -e,2
Merge pull request #14 from mistercrunch/tweakshive_hook: Using -f with a temp file instead of -e,2
Using a decorator to specify operators default args,1
Merge pull request #15 from mistercrunch/default_argsUsing a decorator to specify operators default args,1
Merge pull request #12 from mistercrunch/reload_Fixhealth check + reload dags,2
Reviewing the master command's logic,2
Merge pull request #17 from mistercrunch/tune_masterReviewing the master command's logic,2
moving to tornado server + consistent num_runs across all pages,1
Merge pull request #16 from mistercrunch/tornadomoving to tornado server + consistent num_runs across all pages,1
Minor,5
Making code metadata win over what's in the DB,5
Merge pull request #18 from mistercrunch/tune_masterMaking code metadata win over what's in the DB,5
a task needs to have start_date; either from the dag or the task,2
Adding state logging to backfill,2
Merge pull request #20 from mistercrunch/backfill_loggingAdding state logging to backfill,2
tMaking the gantt chart clickable,2
Merge pull request #21 from mistercrunch/gantt_clickMaking the gantt chart clickable,2
Bringing back the debug flask webserver as an option,0
Merge pull request #22 from mistercrunch/debug_webserverBringing back the debug flask webserver as an option,0
"Waiting longer between master runs, BaseSensor",1
"Merge pull request #23 from mistercrunch/longer_waitsWaiting longer between master runs, BaseSensor",1
"Adding --only_failed to clear, debug calls to master",0
"Merge pull request #24 from mistercrunch/longer_waitsAdding --only_failed to clear, debug calls to master",0
Adding a link to the task instances list,2
Merge pull request #25 from mistercrunch/task_instancesAdding a link to the task instances list,2
"Debug master scheduler, added file timestamp based dagbag update",5
"Merge pull request #26 from mistercrunch/refresh_dagbagDebug master scheduler, added file timestamp based dagbag update",5
Using the dagbag to create the view instead of the Flask Admin,1
Merge pull request #27 from mistercrunch/dag_dagbagUsing the dagbag to create the view instead of the Flask Admin,1
Merge pull request #19 from mistercrunch/start_date_fixa task needs to have start_date; either from the dag or the task,2
fixing a couple of corner cases that were causing crashes,1
Merge pull request #28 from mistercrunch/fixesfixing a couple of corner cases that were causing crashes,1
Moving templates to where they belong,4
Merge pull request #29 from mistercrunch/move_files_outof_adminMoving templates to where they belong,4
Logo,2
Merge pull request #31 from mistercrunch/move_files_outof_adminLogo,4
Minor display bug around filepath,2
Merge pull request #32 from mistercrunch/fixesMinor display bug around filepath,2
"Icon that looks better on retina screens, favico",2
"Merge pull request #33 from mistercrunch/move_files_outof_adminIcon that looks better on retina screens, favico",2
minor fix for the file paths,2
using scoped session and adding teardown for session,1
Merge pull request #30 from mistercrunch/session_fixessqlalchemy session fixes,0
Various improvments/fixes,0
Merge pull request #34 from mistercrunch/fixesVarious improvments/fixes,0
Action to redirect where you come from,5
Fixed the get_task_instances to receive the session as a param,2
Merge pull request #35 from mistercrunch/action_redirectThe clear action now redirects where you came from,1
Beautifying,5
Merge pull request #36 from mistercrunch/formatAdding a few links in Task Instance view,2
Pessimistic pool connection handling for master and run,1
Merge pull request #37 from mistercrunch/pessimisticPessimistic pool connection handling for master and run,1
Master fix,0
Fixes for master rerun,1
Merge pull request #38 from mistercrunch/fixesFixes for master rerun,1
Display bug for duration,0
Merge pull request #39 from mistercrunch/fixesDisplay bug for duration,0
"Charting, may not belonng in Airflow on the long run...",1
"Merge pull request #40 from mistercrunch/chartCharting, may not belong in Airflow on the long run...",1
Fixing bug in LocalExecutor,0
Fix backfill expunge_all,0
Merge pull request #41 from mistercrunch/fix_backfillFix backfill expunge_all,0
Per line bash logging,2
Chart improvements,1
A few basic chart improvements,1
Merge pull request #42 from mistercrunch/chart_impA few chart improvements,1
enabling pandas,0
More chart improvments,2
Merge pull request #44 from mistercrunch/chart_impMore chart improvments,2
Fix chart date type is different depending on maching,5
Yet more improvement to charts,2
Charts,2
Making the chart load async,2
Celery fix,0
Chart improvements,1
More chart improvements,1
More Chart ipmrovements,2
Merge pull request #47 from mistercrunch/chart_asyncChart async,2
Fixed a backfill/depends_on_past bug,0
Merge pull request #48 from mistercrunch/chart_asyncFixed a backfill/depends_on_past bug,0
Spinner loading wheel on tree view,5
Merge pull request #49 from mistercrunch/master_jobSpinner loading wheel on tree view,1
"Making master job a derivative of BaseJob, improving BaseJob",1
Adding unixname to Job and TAskInstance models,1
"Merge pull request #50 from mistercrunch/master_jobMaking master job a derivative of BaseJob, improving BaseJob",1
command line: prompting with list of tasks to clear before actually clearing,5
Merge pull request #51 from mistercrunch/clear_listcommand line: prompting with list of tasks to clear before actually clearing,1
Stacked and percent area charts,2
Merge pull request #52 from mistercrunch/master_jobAdding stacked and percent area charts,2
Merge pull request #43 from mistercrunch/per_line_bash_loggingPer-line logging in BashOperator,1
Merge pull request #46 from mistercrunch/celery_fixCelery execution fixes,0
Modified logging,2
Merge pull request #45 from mistercrunch/modified_loggingMinor logging changes,4
Adding a way to alter the hiveconf from the DatabaseConnection,5
Merge pull request #53 from mistercrunch/master_jobAdding a way to alter the hiveconf from the DatabaseConnection,5
"UI - clear asks for confirmation, shows a list of task instances",5
"Merge pull request #54 from mistercrunch/clear_uiUI - clear asks for confirmation, shows a list of task instances",5
Going more generic on DatbaseConnection's extra json field to support multiple hiveconf statements,5
Merge pull request #55 from mistercrunch/hive_confGoing more generic on DatbaseConnection's extra json field to support mu...,1
Adding wait_for_downstream as BaseOperator attribute,1
Adding email functionality,1
Merge pull request #57 from mistercrunch/emailAdding email functionality,1
Merge pull request #56 from mistercrunch/wait_for_downstreamAdding wait_for_downstream as BaseOperator attribute,1
[airflow] chart now using the Airbnb colors!,1
Merge pull request #58 from mistercrunch/chart[airflow] chart now using the Airbnb colors!,1
Adding macros support to charts,2
Merge pull request #59 from mistercrunch/chartAdding macros support to charts,2
Adding macro ds_add,1
Chart link + email bug,0
Merge pull request #60 from mistercrunch/chartChart link + email bug,0
depends_on_past backfill fix,0
Merge pull request #61 from mistercrunch/backfill_dep_pastdepends_on_past backfill fix,0
Fixes,0
"Fixing email and datepicker, upgrading to latest flask-admin with bootstrap 3",3
Improvements to the query tool,1
Merge pull request #62 from mistercrunch/query_aceImprovements to the query tool,1
Supporting list of emails,1
Adding datatables and bootstrap 3 support,1
Merge pull request #63 from mistercrunch/datatablesAdding datatables and bootstrap 3 support,1
Integrating flask_login,2
Making AUTHENTICATE optional,1
Merge pull request #64 from mistercrunch/flaskloginFlasklogin,5
Improving charts,2
Another set of improvments for charts,2
Merge pull request #66 from mistercrunch/chartsCharts,2
Simplifying CeleryExecutor,5
A few adjustments on CeleryExecutor,5
Fixing the checkbox's look,0
Linting,5
Merge pull request #67 from mistercrunch/chartLinting and polishing charts,2
Merge pull request #65 from mistercrunch/celerySimplifying CeleryExecutor,1
Adding cache support for charts,2
"Adding to reqs, handling connectino pessimistically",1
Adding missing files,2
Compressing / caching improvments around charts,2
"Moving functions into www/utils, documenting the chart admin form",2
Cosmetic polish,1
Merge pull request #68 from mistercrunch/chartChart - more cosmetic polish,1
Adding a proper primary key to db connections,5
Merge pull request #69 from mistercrunch/fix_dbidAdding a proper primary key to db connections,5
Quick fix,0
Merge pull request #70 from mistercrunch/minorQuick fix,0
"[email alerts] Fixed alerts for multiple recipientsFixed the case where smtplib.sendmail would interpret a single stringas a list with a single address as per RFC 822. Fixed ""To"" header aswell",0
Merge pull request #71 from mistercrunch/email_fix[email alerts] Fixed alerts for multiple recipients,0
Addding a few TODO items,2
"Improvments to command line tools: test, list_dags, list_tasks",2
"Merge pull request #73 from mistercrunch/cmdlineImprovments to command line tools: test, list_dags, list_tasks",2
More charts improvements,1
Merge pull request #74 from mistercrunch/chartMore charts improvements,1
Fetching logs remotely when building the log page,2
Log file dispatch service,2
Merge pull request #75 from mistercrunch/logsLogs. Solved.,2
Improving remote logs,2
Merge pull request #76 from mistercrunch/logsImproving remote logs,2
"Making concurency a conf param, stting CELERYD_PREFETCH_MULTIPLIER=1",2
"Merge pull request #77 from mistercrunch/celery_paramMaking concurency a conf param, stting CELERYD_PREFETCH_MULTIPLIER=1",2
Added missing flask-login to req file,2
Attempting to fix the 'mysql has gone away' by passing the factory,4
Merge pull request #78 from mistercrunch/fix_poolAttempting to fix the 'mysql has gone away' by passing the factory,4
Rename DatabaseConnection to Connection,5
Merge pull request #79 from mistercrunch/mv_db_connRename DatabaseConnection to Connection,5
Bug fix on chart modelview override,2
Merge pull request #80 from mistercrunch/bug_fixBug fix on chart modelview override,2
Chart bug fix,0
Merge pull request #81 from mistercrunch/bug_fixChart bug fix,0
Improving the test command line subcommand,3
Merge pull request #82 from mistercrunch/test_cmdImproving the test command line subcommand,3
Tunning celery settings to avoid reserved (as in stuck) tasks,1
Merge pull request #83 from mistercrunch/bug_fixTunning celery settings to avoid reserved (as in stuck) tasks,1
Adding lib +setproctitle for celery to name processes,1
Missed a reference to db_id when renaming to conn_id,5
Minor bug fix on chart params,2
Adding item in TODO.md,2
Bug fix around db_id ranme to conn_id,5
[presto_check_op] PrestoCheckOperator with conn_idCleaning up previous PR mess with a clean branch.,4
Merge pull request #84 from mistercrunch/presto_check_op[presto_check_op] PrestoCheckOperator with conn_id,1
New email operator,1
Merge pull request #86 from mistercrunch/email_opNew email operator,1
REmove to from templated_fields,4
"Merge pull request #87 from mistercrunch/email_opREmoving ""to"" from templated_fields",4
[PyHiveHook] Hook to access Presto through PyHiveTested on jn. @mistercrunch,1
Adding on to PyHive presto switch,1
Merge pull request #88 from mistercrunch/pyhivehook[PyHiveHook] Hook to access Presto through PyHive,1
Changing configuration scheme,5
Merge pull request #89 from mistercrunch/configChanging configuration scheme,5
Fix docs,2
Merge pull request #90 from mistercrunch/configFix docs,2
Fixing packaging of templates and static files,2
Merge pull request #91 from mistercrunch/configFixing packaging of templates and static files,2
Fixing docs automodule command,2
Improving docs and packaging for pypi,2
Including reqs to build,5
Hard coding install_requires,1
Polishing docs and packaging,2
Update setup.py,1
Merge pull request #93 from msabramo/patch-1Update setup.py,1
Adding librabbitmq as it is recommended in the Celery docs,2
Removing sql field from being searchabel to fix bug,0
Setting celery workers optimization to fair,1
Adding base macro ds_nodashand flaking,1
Cosmetics to conf file,2
Pickling the content of files referenced instead of file locations,2
Merge pull request #94 from mistercrunch/hql_picklePickling the content of files referenced instead of file locations,2
Adding HdfsSensor operator,1
Merge pull request #95 from mistercrunch/hdfs_sensorAdding HdfsSensor operator,1
Adding Hive2FtpOperator,1
Removing ftp related files,2
Merge pull request #85 from mistercrunch/Hive2FtpOperatorHive2SambaOperator,1
Fixed typo in description in setup.py #grammarpolice,1
Fixing merge disapearance in samba PR,7
Altering TODO.md,2
Version 0.2,5
Specifying version numbers in deps in setup.py,1
Fixing Text type issue,0
"New RunTaskJob that runs async, implements kill signal",1
Adding --local option to run command,1
"Merge pull request #96 from mistercrunch/async_ti_jobNew RunTaskJob that runs async, implements kill signal",1
Adding template_searchpath attribute to DAG object,2
Merge pull request #97 from mistercrunch/template_searchpathAdding template_searchpath attribute to DAG object,2
Packaging 0.2.1 for Pypi,5
closest partition macro,5
fixed indent + return obj for get_partitions,1
"[macros] added macro closest_ds_partitionAdded a hive macro to get the closest ds partition. It can get theclosest right before, the closest after, or either way.@mistercrunch",1
[closest_ds macro] lintingFollowing flake8 recs + changing return type of helper function,1
"Importing airflow in setup.py wan't a good idea, rolling back",1
Improving the examples dags a little,2
Version 0.2.2,5
Fixing bug around templating,0
Merge pull request #100 from mistercrunch/fix_templatingFixing bug around templating,0
Version 0.2.3,5
[presto_check] Changed the return for presto_checkI think this make more sense. It will fail if any cell evaluates toFalse or on empty records.,0
Merge pull request #99 from mistercrunch/change_presto_check_fail_condition[presto_check] Changed the failure condition for presto_check,0
Merge pull request #98 from mistercrunch/closest_dsClosest ds,1
"Version 0.2.3.1 for airenv (internal, not pypi)",5
Kill running jobs when cleared,1
Simplifying the approach,5
"Merge pull request #101 from mistercrunch/clear_killClearing running jobs now kills them properly, regardless of the executor",1
Adding task_instance_key_str to default template macros,1
Merge pull request #102 from mistercrunch/ti_keyAdding task_instance_key_str to default template macros,1
Fixing task page when template location isn't relative to DAG,2
"Merge pull request #103 from mistercrunch/template_wwwFixing the ""Task Details"" page when template location isn't relative to DAG",2
Version 0.2.3.2,5
"[tree view] when expanding a node, other instances of this node collapses",2
A few changes around templating to make hivepp templates work,1
Merge pull request #105 from mistercrunch/template_hookA few changes around templating to make hivepp templates work,1
Version 0.2.3.3,5
Merge pull request #106 from mistercrunch/template_hookVersion 0.2.3.3,1
Bug fix +v 0.2.3.4,0
Make tasks adhoc,1
"Merge pull request #108 from mistercrunch/adhocTask ""adhoc"" parameter",2
Dilling instead of pickling to allow for functions and modules,1
Merge pull request #109 from mistercrunch/adhocDilling instead of pickling to allow for functions and modules,1
[Airflow] [IntervalCheck] Presto Interval Check opThis is an implementation of the PrestoIntervalCheck operator similar tothe one in the internal data-eng library at Airbnb. This operator willallow to test if variations are within bounds over a period of timeon a week over week or YoY basis.@mistercrunch,1
[PrestoHook] Change logging level to INFOSimple modification to the logging level for thePrestoHook,1
Merge pull request #107 from mistercrunch/presto_interval_check[Airflow] [IntervalCheck] Presto Interval Check op,1
Massive refactore + adding rendered template to the UI,1
Merge pull request #110 from mistercrunch/renderedMassive refactor + adding rendered template to the UI,1
Minor fixes,0
Making sure DAGS_FOLDER is in the PYTHONPATH,2
Detail,5
"Merge pull request #104 from mistercrunch/tree_nodes[tree view] when expanding a node, other instances of this node collapses",1
v0.2.3.5 and a few very minor tweaks,5
"[PrestoValueCheck] New operator for value checkingPrestoValueCheck allows to define the value the SQL query should returnin order to pass the check. This can enable more readable SQL checks.PrestoValueCheck can accomodate string values or numeric values with orwithout a tolerance, which allows for close results.@mistercrunch",1
Bug fix on master double trigger,0
Version 0.2.3.6,5
0.2.3.7,5
Flower shortcut added to the client tools,1
Merge pull request #112 from mistercrunch/flowerFlower shortcut added to the client tools,1
Now caching/pickling jinja template objects,5
"Giving up on serializing templates, trying just the Env",1
Fixing the web ui to point to the new template methods,1
Passing the pickle through,4
Merge pull request #113 from mistercrunch/pickle_soupPickle soup,1
Merge pull request #111 from mistercrunch/presto_value_check[PrestoValueCheck] New operator for value checking,1
Getting a full stack trace in the logs,2
Merge branch 'master' of github.com:mistercrunch/Airflow,1
Fixing markup logs,2
Allowing to pause,1
Merge pull request #114 from mistercrunch/pauseAllowing to pause DAGs,2
"Impoving the docs, adding autodocs for command line",2
"Merge pull request #116 from mistercrunch/docsImpoving the docs, adding autodocs for command line",2
Showing the number of task instance by state,2
Merge pull request #117 from mistercrunch/colorsShowing the number of task instance by state,1
v0.2.3.9,5
v0.3 pushing to pypi,5
"Impoving the docs, adding autodocs for command line",2
Adding profiling to docs,2
Much more docs improvments,2
Merge pull request #118 from mistercrunch/docsDocs,2
More docs improvments,2
Fixed up the Tutorial section in the docs,2
Merge pull request #119 from mistercrunch/docsFixed up the Tutorial section in the docs,2
Adding hiveconf_jinja_translate param for HiveOperator,1
Merge pull request #120 from mistercrunch/docsAdding hiveconf_jinja_translate param for HiveOperator,1
Making hiveconf_jinja_translate happen at the right time,5
Refreshing TODO list,2
Yet more documentation improvments',2
Easter egg,5
Merge pull request #121 from mistercrunch/easterEaster egg,1
Fixing task regex getting disregarded,1
Merge pull request #122 from mistercrunch/fixesFixing task regex getting disregarded,1
v0.3.0.1,5
"Making sure task_ids are in range, disabling template cachingAnd some flaking...",1
Adding --local option to backfill,1
Merge pull request #123 from mistercrunch/local_backfillAdding --local option to backfill,1
Adding the tutorial.py file,2
Merge pull request #124 from mistercrunch/tutorialAdding the tutorial.py file,2
Misc improvments,5
Merge pull request #125 from mistercrunch/tutorialDealing with bad templates,1
A set of minor improvements,1
Adding light colors to the graph view,1
Merge pull request #126 from mistercrunch/colorAdding light colors to the graph view,1
"Adding PythonOperator, and changing the way context is passed to the Operator execute method",1
Merge pull request #127 from mistercrunch/python_operatorAdding PythonOperator,1
v 0.3.0.2,5
Unit tests!,3
Breaking downt initdb into initdb and resetdb,5
Insuring we're using the unittest cnfig,3
Merge pull request #128 from mistercrunch/testsUnit TESTS! 56% Coverage and counting.,3
default_args at the DAG level,2
Merge pull request #129 from mistercrunch/default_params_dagdefault_args at the DAG level,2
More unit tests,3
Merge pull request #131 from mistercrunch/testsMore unit tests,3
Copying the dict to be safe,5
Merge pull request #132 from mistercrunch/default_params_dagCopying the dict to be safe,2
[s3] Add S3 connection type for S3 sensor,1
Merge pull request #133 from mistercrunch/S3_connection_type[s3] Add S3 connection type for S3 sensor,1
Fixed a broken link,2
v0.3.1,5
Adding version info to the Admin->Configuration view,5
A few minor changes,4
[Tutorial] Put full script first and correct a few typos@mistercrunch lmk,2
Adding DAG level params,2
Merge pull request #136 from mistercrunch/dag_paramsAdding DAG level params,2
"[S3 sensor] S3 Hook and sensorThis PR is a first step to more advanced S3 operators.The hook can currently check for buckets, prefixes and keys,and will parse boto, AWS sdks or s3cmd config files if theyare defined in the connection in the Airflow connection db.The sensor uses the Hook to check for a particular file(or key to follow amazon's nomenclature). I also added wrappersfor boto methods to list keys and prefixes.@mistercrunch",0
v0.3.2 + bug fix caught in unittests,3
Merge pull request #134 from mistercrunch/S3sensor[S3 sensor] S3 Hook and sensor,1
Merge pull request #135 from mistercrunch/docs_fixes[Tutorial] Put full script first and correct a few typos,2
0.3.3,5
Removing hooks defaults from config,5
Merge pull request #138 from mistercrunch/remove_hook_defaultRemove hook default,1
Improving the unit tests,3
Adding TimeSensor (operator),1
Merge pull request #139 from mistercrunch/time_sensorAdding TimeSensor (operator),1
"Hive2Samba to use hiveserver2, refactoring HiveHooks",1
"Merge pull request #140 from mistercrunch/hive_server2Hive2SambaOperator to use hiveserver2, refactoring (breaking down) HiveHooks",1
Adding missing TimeSensor to docs,2
Improving the Hive2SambaOperator,1
v0.3.2.1,5
Adding only_running flag to clear subcommand,1
Merge pull request #141 from mistercrunch/only_runningAdding only_running flag to clear subcommand,1
More precise logging around dependencies not being fulfilled,2
Adding filters to jobs CRUD,1
Making Chart connection non nullable,2
"Reverting overlogging on dependencies, was printing more than intended",2
Parallel coordinates chart_type,2
Misc Improvements,1
Merge pull request #142 from mistercrunch/bubbleFirst iteration on parallel coordinates chart_type,2
Health checks for PagerDury,5
Merge pull request #143 from mistercrunch/health_checksInstumenting statsd for graphite / datadog / pagerduty,5
0.3.2.2,5
v0.3.2.3,5
"Fed up with readthedocs, moving docs to PythonHosted",2
Fixing bad default configuration,5
Adding missing Hive hooks to dag,2
"A few improvements, bug fixes, v0.3.2.4",0
"Adding an endpoint to get the raw config, will be used to automate setting up a sandbox",5
Making cfg leaner,5
Config simplification v0.3.2.5,5
Endpoint for suggested sandbox config,5
Only import DAGs that contain obvious references to Airflow,2
Changing perms,4
Adding root param to tree and graph view,2
Brutaly disabled jinja caching and solved deepcopying issues. BOOM.,0
v0.3.2.6,5
Fixing a bug in get_flat_relatives,1
Adding postgres operator and hook,1
Error message cleanup,4
Adding postgres dependencies,1
Improvements to max_partition,1
Merge pull request #144 from mistercrunch/max_partsImprovements to max_partition,1
Adding postgres operator,1
Not allowing master scheduler to process beyond end_date,5
Allowing retry_delay to be set as int,1
Adding tomorrow_ds and yesterday_ds to default macros,1
Adding line numbers to code views,1
Cleaning up Postgres identity,4
Adding indices,1
Adding autocommit setting for Postgres operator,1
Adding back example_dags,2
Making master scheduler more resilient,1
Merge pull request #145 from mistercrunch/master_safeMaking master scheduler more resilient,1
Adding postgres setAutocommit,1
Passing self to setAutocommit,1
Default for autocommit,5
Adding a CONTRIBUTING.md,1
Merge branch 'master' of https://github.com/mistercrunch/Airflow,1
[S3] This enables the use of IAM roles and fancy stuff with AWS creds,1
Modernizing MySqlOperator + unit test,3
Adding last_modified to Chart model,2
Merge pull request #146 from mistercrunch/artwr/IAM_support_for_s3_hook[S3] This enables the use of IAM roles and fancy stuff with AWS creds,1
Merge branch 'master' of https://github.com/mistercrunch/Airflow,1
Postgres operator unit tests,3
Merge pull request #147 from kerzhner/masterAdding Postgres operator and hook,1
Making hiveserver2 work in adhoc queires,1
Merge pull request #148 from mistercrunch/hiveserver2_hookMaking hiveserver2 work in adhoc queries,1
Changing name of menu label from Tools to Data Profiling,5
Adding loading spinner on adhoc query form,1
Switching to Apache license,5
Adding support for .airflowignore files in DAGS_FOLDERs,2
Adding a navigation bar at the task instance level,1
Getting the datetimepicker to navigate on change event,4
Fixing the table layout for code,0
Known events,5
Merge pull request #149 from mistercrunch/known_eventsKnown events,1
Removing a line from task_instance template,4
Minor CSS fix,0
Bug fix around code view,0
Base time for tree view,5
Yo dawg. Sub dags,2
Merge pull request #151 from mistercrunch/subdagYo dawg. Sub dags,2
Fixing caching issues on DAG,2
Merge pull request #152 from mistercrunch/fix_cacheFixing caching issues on DAG,2
Merge pull request #150 from kerzhner/tree_view_baseTree view base time fix,0
Better handling of DAG level params,2
Making master scheduler disregard subdags,2
Don't pickle when running subdags,2
Fix to allow deepcopying of DAG with subdags,2
Making clear recurse through subdags / refactor,4
Merge pull request #153 from mistercrunch/clear_recurseMaking clear recurse through subdags / refactor,4
s3 sensor fix logging,2
Merge pull request #154 from mistercrunch/s3sensor_logfixs3 sensor fix logging,2
[setup] selective features and imports for hooks and operators,1
Merge pull request #155 from mistercrunch/modularizing_airflow_setup[setup] selective features and imports for hooks and operators,1
Docs,2
v0.4,5
v0.4.1,5
"v0.4.2 , fixes around hooks conditional imports",2
v0.4.3,5
Documenting the now broken down packages,2
Fixing bug in clear subcommand,0
Circle fill color in tree view,5
Minor touchup to operator constructor decorator,1
Fix link on task_instance page,2
enable sending email by directly imputing MIME message,0
[Bash] minimal amount of isolation in a self cleaning temp directory,4
[hive_hook] add with statement to ensure nice cleaning in case of an error,0
Merge pull request #159 from mistercrunch/artwr/hive_hook_temp_file[hive_hook] add with statement to ensure nice cleaning in case of an error,0
Merge pull request #158 from mistercrunch/artwr/isolate_bash_op[Bash] minimal amount of isolation in a self cleaning temp directory,4
Mark tasks as successful (false positive) from the UI,5
Merge pull request #160 from mistercrunch/mark_successMark tasks as successful (false positive) from the UI,1
Making collapsed nodes stand out,1
Fixing the datepicker on the graph view,5
Making login module generic / overridable,2
Merge pull request #161 from mistercrunch/login_generalMaking login module generic / overridable,2
seperated sending of mail according to your suggestion,5
delete the end spaces,4
Merge pull request #162 from mistercrunch/emailenable sending email by directly imputing MIME message,0
Bug fix in utils.send_email,0
v0.4.4,5
Fixing some unit tests,3
v0.4.5,5
More conditional imports,2
Fixing a unit test,3
Merge pull request #163 from mistercrunch/conditionalConditional,1
v0.4.6,5
Cleaning up the methodology for conditional imports,2
Fixed a bug in headers endpoint,0
New ascii logo,2
MySQL 2 Hive operator,1
[s3 pickle] should fix the pickling bug,0
Merge pull request #167 from mistercrunch/arthurw/fix_s3_hook_pickling_bug[s3 pickle] should fix the pickling bug,0
Docs entry,1
Y labels on charts,2
Merge pull request #168 from mistercrunch/chartsY labels on duration and landing times charts,2
Minor linting,5
Improvments to the docs,2
Merge pull request #165 from mistercrunch/mysql2hiveMySQL 2 Hive operator,1
Changing default delimiter value for HiveCliHook.load_file to a comma,2
Fix field ordering in mysql_to_hive,0
Support for dicts and list in operators template_fields,1
Fix small typos in docs + constructor,2
Merge pull request #170 from mistercrunch/template_list_dictsSupport for dicts and list in operators template_fields,1
Merge pull request #171 from mistercrunch/arthurw/fix_mysqltohive_docsFix small typos in docs + constructor,2
v0.4.7,5
Adding blur url params for screenshot purposes,2
Merge pull request #172 from mistercrunch/blurAdding blur url params for screenshot purposes,2
template the table name to load into datestamped staging table,5
Merge pull request #173 from mistercrunch/artwr/template_transfer_dest_table_nametemplate the table name to load into datestamped staging table,5
0.4.7.1,5
Documented BaseOperator's wait_for_downstream attribute,1
Organizing gitignore,2
Fixing crash with default_login,2
Merge pull request #175 from kerzhner/devCrash in default_login,2
Bugfix: allowing None value in templated fields,1
Adding csv export from adhoc view,1
Making operator constructor leaner,1
Merge pull request #177 from mistercrunch/ops_cleanopsMaking operator constructor leaner,1
Hive stats collection operator,1
Addressing @artwr comment,1
Merge pull request #178 from mistercrunch/statsHiveStatsCollectionOperator!,1
Adding a legend to tree and graph view,1
Merge pull request #179 from mistercrunch/legendAdding a legend to tree and graph view,1
lengend color fix for dark ops,0
Bugfix in HiveStatsOperator,1
S3 to Hive operator. New PR for cleaner history,4
Bumping poke_interval to 180 seconds for HivePartitionSensor,5
Bugfix end_date format,5
Merge pull request #180 from mistercrunch/artwr/s3_to_hive_transferS3 to Hive operator. New PR for cleaner history,4
Adding unixname to task instance view,1
Legend improvments,5
Adding search funcitonality to graph view,1
Debugging scheduler lags,0
Merge pull request #181 from mistercrunch/debug_schedulerLogging master scheduler lags,2
Removing requirement that is not needed (flask_bootstrap),1
Improving search,1
Merge pull request #1 from airbnb/search_improvementsImproving the search functionality in the graph view,1
Bring in more resolution to hivestats,5
debug,0
Allowing to blacklist columns through the assignment function,1
Merge pull request #2 from airbnb/hive_stats_resolutionBring in more resolution to hivestats,7
Disabling caching for chart data,5
Bugfix hive_stats_operator,1
Refreshing DAG definition of the master scheduler every N runs,1
Merge pull request #182 from mistercrunch/refresh_masterRefreshing DAG definition of the master scheduler every N runs,1
Found what was slowing down the master scheduler as it ran longer,5
Merge pull request #183 from mistercrunch/fix_masterFound what was slowing down the master scheduler as it ran longer,0
fixing small typos,2
Merge pull request #184 from mistercrunch/artwr/fix_doc_typosfixing small typos,2
HiveToMySqlTransfer!,5
Merge pull request #185 from mistercrunch/hive2mysqlHiveToMySqlTransfer!,1
Heartbeat in the wrong place,0
"Hiding pasword, hooks for rendered doc attributes",2
New boostrap theme,1
Merge pull request #186 from mistercrunch/cssNew boostrap theme,1
Chart with data table only improvments,5
"HiveStatsCollectionOperator was throwing deadlocks, this should address it",1
"Merge pull request #187 from mistercrunch/stats_nodeleteHiveStatsCollectionOperator was throwing deadlocks, this should address it",1
Added new operator S3FileTransformOperator,2
UTC Clock,5
Merge pull request #189 from mistercrunch/clockUTC Clock,1
Updating TODO list,2
made changes suggested by arthur,4
changed suggested by Max. Removed the transform_executor arg. Expecting an executable,4
handling headers with field matching and streaming the file,2
Merge pull request #188 from mistercrunch/S3FileTransferOperatorAdded new operator S3FileTransformOperator,2
add variables kv-store model,1
Scheduler,5
Adding hostname as filter criteria for task instances,1
Allowing sensors to retry,1
Switching debug to false on log server,2
Improving the 'root' filter,1
adding support for headers and header checking,1
move view to admin and fix print statement,0
Update README.md file in /templates/variables,2
Merge pull request #190 from mistercrunch/add_variables_modeladd variables kv-store model,1
removed unused seek + flaking,1
Adding schema to HiveOperator,1
Merge pull request #191 from mistercrunch/artwr/add_header_handlingArtwr/add header handling,1
Tweaks on Variables model,5
Default sorting on start_date DESC for TI and known events,5
Update utils.pySmall typo to get on the Airflow repository.  Nitpick FTW!!!,1
Merge pull request #192 from mistercrunch/altertUpdate utils.py,5
added code for ds_format,1
changes max suggested,4
Merge pull request #193 from mistercrunch/ds_format_macroadded code for ds_format,1
Expiring DAGs,2
v0.5.0,5
Clarfying the docs,2
Allowing multiple schedulers to work together,1
Fix polymorphic identity of hive_to_mysql,0
Merge pull request #195 from mistercrunch/schedulerScheduler,1
Removing dummy User class,1
Hard coding 4 subprocess for the webserver before parameterizing it,2
Bugfixes,0
Debugging,0
Merge pull request #196 from mistercrunch/schedulerScheduler,1
Fixing minor bug,0
Passing a proper env to subprocess,4
Make root sticky when navigating across days,1
Merge pull request #197 from mistercrunch/stickyMake root sticky when navigating across days,1
Cosmetics,5
Query view crash,5
Merge pull request #198 from kerzhner/devQuery view crash,7
Better logging for MySqlHook.insert_rows,1
s3_to_hive: Move init of hooks to execute method,1
Merge pull request #199 from mistercrunch/fixs3_to_hives3_to_hive: Move init of hooks to execute method,1
Using CRUD for DAGs view,2
Getting the tests to run,1
Linting,5
Merge pull request #200 from mistercrunch/dag_crudDag CRUD,2
Bugfix,0
Pointing to fileloc from database,5
Bugfix around dag navigation on DAG view,2
fix a bug in case of date equality,5
Merge pull request #201 from mistercrunch/fix_closest_date_logicfix a bug in case of date equality,5
Fix bug where fileloc didn't trickle into subdags,2
Dissociating DagModel object from DAG object,2
Testing / tuning,3
Merge pull request #202 from mistercrunch/dag_refactorDissociating DagModel object from DAG object,2
Bugfix,0
Hardest bug to find of the year,0
Merge pull request #203 from mistercrunch/fucking_bugHardest bug to find of the year,0
D3 magic when hovering over status legend,5
Looking for airflow.cfg in your home first,5
Wrapping Tornado in a thread to help with runit stop,1
Fixing missing DAG issue,0
error -> failed,0
Using CRUD for main view,1
Merge pull request #204 from mistercrunch/modelview_filterUsing CRUD for main view,1
Switching to a single threaded server,5
Adding button to filter on node in modal popup,1
Making depressed button look more depressed.,1
Hidden endpoint for the list of local DAGs,2
Merge pull request #206 from mistercrunch/local_dagsHidden endpoint for the list of local DAGs,2
"Individual conn_id can have multiple entries, client randomly picks",5
"Merge pull request #208 from mistercrunch/connsIndividual conn_id can have multiple entries, get_connection randomly picks one",1
add hdfs hook and update hdfs sensor to work with HA configuration,5
Prioritization and concurency limitation on executor queues,5
Fixing file timestamp read race condition,2
Merge pull request #210 from mistercrunch/connsFixing os file timestamp read race condition,2
add task_state command and correct help messages for misc options,1
Merge pull request #209 from mistercrunch/hdfs_hookadd hdfs hook and update hdfs sensor to work with HA configuration,5
Fixed collision on self.queue,0
Showing paused DAGs in DAGs view,2
Merge pull request #211 from mistercrunch/cli_task_stateadd task_state command and correct help messages for misc options,1
Fix 2 ongoing issues,0
Removing unique constraint on conn_id,4
add sqlite hook,1
Merge pull request #207 from mistercrunch/priorityPrioritization and concurency limitation on executor queues,1
Bugfix,0
Adding shortcut to filtered dag view  on TaskInstance view,2
fix docstring,2
BaseExecutor bugfix,0
add doctests,3
Merge pull request #214 from mistercrunch/add_sqlite_hookadd sqlite hook,1
add sqlite default connection,1
Adding logging entry in CeleryExecutor to help debug,0
More debug logging for the Scheduler,2
Allowing different executors for the SubDagOperator,2
Merge pull request #215 from mistercrunch/add_sqlite_default_connadd sqlite default connection,1
Fixes around HDFSSensor,0
New main page,1
Merge pull request #218 from mistercrunch/main_pageNew main page,1
One more debug logging message in the scheduler,2
Only one heartbeat after all DAGs are enqueued,2
Minor UI polish,1
Merge pull request #219 from mistercrunch/main_pageMinor UI polish,1
Preventing odd RabbitMQ failures,0
Catching celery's REVOKED state,5
Running HiveCliHook execute in a temp working directory,1
Merge pull request #220 from mistercrunch/tmpfolder_hive_opRunning HiveCliHook execute in a temp working directory,1
Adding redis to reqs.txt,5
Adding wildcard matching for S3 hook and operators,1
Merge pull request #221 from mistercrunch/artwr/s3_fuzzy_key_matchingAdding wildcard matching for S3 hook and operators,1
Adding a config param to not load example dags,2
Merge pull request #222 from mistercrunch/load_exmamplesAdding a config param to not load example dags,2
Smoother loading on main page,5
Added an entry for the Scheduler in the docs,2
Merge pull request #223 from mistercrunch/docsAdded an entry for the Scheduler in the docs,2
Forgot to add the file...,2
Merge pull request #224 from mistercrunch/docsForgot to add the file...,2
Docs tweaks,2
Remote run won't pickle by default anymore,1
for consistency with list_dags and list_tasks,2
Merge pull request #226 from mistercrunch/remove_log_to_stdout_for_task_statusfor consistency with list_dags and list_tasks,2
Merge pull request #225 from mistercrunch/run_remote_worksRemote run won't pickle by default anymore,1
Adding a preoperator to HiveToMySqloperator to allow idempotence,1
Merge pull request #227 from mistercrunch/mysql_preoperatorAdding a preoperator to HiveToMySqloperator to allow idempotence,1
Enabling queue management for Celery,0
BEdugging,5
Debugging,0
Run individual tasks from the UI,1
Merge pull request #229 from mistercrunch/run_contextRun individual tasks from the UI,1
Fixing pip complaining about dupplicate reqs,5
Default,5
Adding slot pool management to Airflow,1
Bugfixes,0
0.7 + migrations,5
Merge pull request #216 from mistercrunch/celery_queueEnabling queue management for Celery,0
fixin spellin,0
Merge pull request #230 from mistercrunch/artwr/spelling_policefixin spellin,0
Queue & prioritize,5
Showing used slots,1
"Merge pull request #232 from mistercrunch/prioritizePrioritization, phase 2",1
Bugfix,0
Making queued squares gray,1
Adding item to TODO,2
Fix an iterate on None issue,0
Merge pull request #233 from mistercrunch/artwr/fixed_S3_wildcard_checkFix an iterate on None issue,0
Adding SqliteOperator,1
Improving the Chart list view,2
Docs and docstrings edits,2
Blur mode,5
Adding pools entry to docs,2
Adding a doc entry for Connections,1
Commenting the default cfg file,2
Merge pull request #238 from mistercrunch/docsNew doc entry for Pools and Connections,1
Merge pull request #239 from mistercrunch/blur_modeDemo mode - blurs task_ids and other sensitive-ish information,5
Moving EmailOperator docstring from constructor to class,2
Fixing referential integrity,0
Merge pull request #240 from mistercrunch/fix_fkRemoving problematic foreign key from Chart to Connection,2
Merge pull request #237 from mistercrunch/artwr/amending_example_dag_docs_and_typosDocs and docstrings edits,2
BaseOperator to not be a sqlalchemy model anymore,1
Merge pull request #241 from mistercrunch/de_orm_baseopBaseOperator to not be a sqlalchemy model anymore,1
v1.0.0\!,5
Merge pull request #242 from mistercrunch/v1v1.0.0!,1
Logging which host is being usedUseful when using multiple hosts with same conn_id and one is defective,1
Bug fix on logging hostname in BaseHook,1
Insures that we keep the same client connection through the sensor's lifecycle,2
Merge pull request #243 from mistercrunch/sensor_sticky_connInsure keeping the same connection through the sensor's lifecycle,1
Pointing the link to the new Github location,1
Added an entry for Queues in the docs,2
Improving mysql loads to support numpy.datetime64,5
Merge pull request #244 from mistercrunch/mysql_loadsImproving mysql loads to support numpy.datetime64,5
v1.0.1 points to the right download_url,5
Changed script to be the existing script name.,4
Misc spelling changes and change table name from OVWERWRITE to OVERWRITE,4
Random spelling updates.,5
Reverting spelling error,0
Merge pull request #5 from eerwitt/documentation_spellingMisc spelling updates,5
Bugfix for when authenticate=false and airflow_login is present,2
Merge pull request #10 from airbnb/fix_authenticateBugfix for when authenticate=false and airflow_login is present,2
Bugfix when airflow.cfg boolean had inline comments would eval to none,5
Merge pull request #11 from airbnb/bugfixBugfix when airflow.cfg boolean had inline comments would eval to none,5
"add env parameter to BashOperator, allow for passing env mapping to subprocess",4
Wrapping scheduler main file loop in try statement,1
Reducing flake warnings where it made sense,2
Adding prefix cfg for statsd,5
Minor improvment to tutorial example,5
Setting the default queue in BaseOperator,1
Merge pull request #19 from airbnb/default_queueSetting the default queue in BaseOperator,1
Undocumented width and height url params for larger graph view,2
Adding a note about using python2.7 in docs,2
Clarify the term constructor,5
Define a constructor,5
Backfill start_date to override the tasks's,5
Merge pull request #26 from airbnb/backfill_start_dateBackfill start_date to override the tasks's start_date,5
The term task is more clear than constructor,5
Typo,2
Using more plain language and making the reference to a task consistent,1
Small typo and clarity fixes,0
Made more consise and attempted to improve readability,1
Centralizing logging level into settings.py file,2
Merge pull request #28 from airbnb/logging_levelCentralizing logging level into settings.py file,2
Defining AirflowException in place of generic ones,5
Merge pull request #29 from airbnb/exceptionsDefining AirflowException in place of generic ones,5
Merge pull request #25 from statwonk/improve-tutorialTutorial improvements.,1
adding option to remove previous run data instead of manual rm,5
Merge pull request #33 from airbnb/artwr/fixing_coverage_clearing_previous_runadding option to remove previous run data instead of manual rm,5
Carry ignore_dependencies from backfill to run commands,1
Merge pull request #34 from airbnb/ignore_depsCarry ignore_dependencies from backfill to run commands,1
Adding nose to reqs,1
Removing explicit relationship between Connection and Chart models,2
Merge pull request #35 from airbnb/conn_widgetRemoving explicit relationship between Connection and Chart models,2
Bugfix around graph's width/height,0
Making sure conn_id are unique in Query view dropdown,4
Adding utility function to get to models.Variable,1
Adding an entry for Variables in the docs,2
Adding support for  option to PrestoHook + refactor,4
Merge pull request #36 from airbnb/presto_catalogAdding support for catalog option to PrestoHook + refactor,4
Allowing HiveServer2Hook to work with empty resultset,1
A simple plugin system for Airflow,5
Merge pull request #32 from airbnb/pluginsA simple plugin system for Airflow,5
v1.1.0,5
Fix default cfg file plugins_folder setting,1
v1.1.1,5
Updating to the right license (apache2) in setup.cfg,5
Bugfix for HiveServer2Hook.get_results with empty datasets,5
Adding -sd (--subdir) to list_dags,2
Adding a timeout Context object and using it when importing dags,2
Merge pull request #44 from airbnb/timeout_importsAdding a timeout Context object and using it when importing dags,2
Take 2 on more explicit plugins,2
Adding an endpoint to refresh all DAGs,2
Patching ExternalTaskSensor for non mysql DBs,5
"Oops, fixing patch",0
Merge pull request #49 from airbnb/patch_external_taskPatching ExternalTaskSensor for non mysql DBs,5
Passing context to sensor poke method,4
Merge pull request #43 from airbnb/pluginsTake 2 on more explicit plugins,7
Adding get_databases to HiveMetastoreHook,1
Merge pull request #53 from airbnb/poke_contextPassing context to BaseSensor poke method,4
Type in CONTRIBUTING.md,5
Merge pull request #54 from akuhn/patch-1Typo in CONTRIBUTING.md,2
Adding doc reference to Celery broker setup doc in installation instructino,2
Making HiveCliHook.run_cli return stdout,1
Merge pull request #60 from airbnb/stdoutMaking HiveCliHook.run_cli return stdout,1
Hive Metastore Browser plugin,2
Added missing import in the plugins doc example,2
Merge pull request #65 from gregorymfoster/fix_docsAdded missing import in the plugins doc example,2
Adding pre and post execute hooks to BaseOperator,1
Adding priority_weight to TaskInstanceModel view,1
Fixing queue prioritization,0
[doc] clarification around the scheduler,2
Merge pull request #68 from airbnb/fix_queueFixing queue prioritization,0
add a simple index view to extend,1
Merge pull request #67 from airbnb/pre_post_executeAdding pre and post execute hooks to BaseOperator,1
remove modal css override,4
Merge pull request #69 from airbnb/index_view_to_override_cssadd a simple index view to extend,1
Merge pull request #64 from airbnb/metastore_browserHive Metastore Browser plugin,7
Fixing HiveCliHook test,3
Adding secret_key to cfg,5
Bugfix in pool prioritization,0
Improvement in task documentation capabilities,2
Minor exception message spacing fix,0
HiveServer2 hack to run multi-statement in one session by passing a list,4
Merge pull request #76 from woodlee/message-fixMinor exception message spacing fix,0
Merge pull request #74 from airbnb/task_docImprovement in task documentation capabilities,2
HiveServer2 improvements,1
Merge pull request #71 from airbnb/hs2_connHiveServer2 improvements,1
Show hostname when clicking the clock,5
Clarify the function of wait_for_downstream,1
Resolve 404s when trying to click through to the task instances view,1
Adding doc_md feature to dag object,2
PR feedback,5
Merge pull request #78 from woodlee/doc-clarifyClarify the function of wait_for_downstream,1
Merge pull request #79 from woodlee/taskinstance-404sResolve 404s when trying to click through to the task instances view,1
A better 404,1
Use internal table. Using external was leading to duplicates.,1
Adding the upstream_failed state to allow the scheduler to move forward,4
Bugfix on log scale in charts,2
Improved backfill progress logging info,5
Merge pull request #85 from airbnb/backfill_loggingImproved backfill progress logging info,5
Various documentation spelling and grammar edits,2
Merge pull request #86 from woodlee/docs-editsVarious documentation spelling and grammar edits,2
Merge pull request #83 from airbnb/flag_upstream_failedAdding the upstream_failed state to allow the scheduler to move forward,4
Conform the tutorial to the code at the beginning,5
fixing dates,5
fixing parameter,2
Merge pull request #89 from airbnb/fix_args_in_tutorial_docsFix args in tutorial docs,2
Fix sort order in connection list on query page,0
Update tutorial.rst,5
formatting issue,0
Merge branch 'master' of https://github.com/airbnb/airflow,7
Removing QUEUED from runnable states list,1
Merge pull request #91 from airbnb/queue_fixRemoving QUEUED from runnable states list,1
Deleting queued up task where the task or DAG is gone,2
Merge pull request #94 from airbnb/clean_queueDeleting queued up task where the task or DAG is gone,2
Rare bug fix when around task instance duration,0
Cosmetics on TaskInstance list view,2
Merge branch 'master' of https://github.com/airbnb/airflow,7
Update tutorial.rst,5
Merge branch 'master' of github.com:gtoonstra/airflow,7
Sketch for a BranchPythonOperator,1
Adding documentation entry for BranchPythonOperator,1
Merge pull request #96 from airbnb/branch_python_operatorNow supporting branching using a BranchPythonOperator,1
Merge pull request #82 from airbnb/fix_hive_hook_to_use_internal_tableUse internal table. Using external was leading to duplicates.,1
Adding a link to Mark Success directly from the failure email,0
Merge pull request #98 from airbnb/mark_success_emailAdding a link to Mark Success directly from the failure email,0
Rare bug fix for deleted pipelines scripts,4
Removed print statement from view,4
Merge pull request #101 from 0x68/miscRemoved print statement from view,4
Merge pull request #90 from gtoonstra/masterExplanation about the meaning of date parameter,2
Deleting queued jobs for tasks and dags that have been removed,4
v1.2.0,5
"SLA can be set at task level, email notifications get sent",1
"Merge pull request #100 from airbnb/slasSLA can be set at task level, email notifications get sent",1
HTTP Operator and sensor,1
Bugfix in SLA management,0
Changes after review in pull request #103,4
"Task instances could be double triggered when using the --force, not anymore",1
Merge pull request #104 from airbnb/fix_double_triggerTask instances could be double triggered when using the --force,1
Priority based on priority_weight + time in queue,5
Merge pull request #105 from airbnb/prioPriority based on priority_weight + time in queue,7
Moving from 100 to 1000 tasks per page,4
Merge pull request #106 from airbnb/page_1000Moving from 100 to 1000 tasks per page,4
Making sure macrosfrom plugins get integrated,1
Forcing task instances to get prioritized,1
Fixing the magnifying glass icon pointing to the wrong place,0
Fixing a docstring typo,2
Better docs for PrestoCheckOperator,1
Merge pull request #107 from adamhaney/masterFixing a docstring typo,2
Passing desc sort priority_weight in pool queue view,4
Adding support for beeline as part of HiveCliHook,1
Added unit test and review changes,4
Adding a policy hook to allow setting up system-wide policy,5
Timeout and initdb,5
Use a bare raise so the original exception gets propagated.,1
Merge pull request #109 from airbnb/policyAdding a policy hook to allow setting up system-wide policy,5
Fixing a bad attr name in HiveToMySqlTransfer,0
Merge pull request #114 from mistercrunch/fix_h2mFixing a bad attr name in HiveToMySqlTransfer,0
Fixing up Http*,0
Merge pull request #117 from airbnb/http_operator_sensorHttp operator sensor,1
Render python_callable source code in Task Details for PythonOperator,1
Merge pull request #118 from mistercrunch/python_op_codeRender python_callable source code in Task Details for PythonOperator,1
Fixing recursion max_depth bug in deepcopying DAGs,2
Fixing missing duration bug,0
Merge pull request #111 from jbalogh/patch-1Use a bare raise so the original exception gets propagated.,1
Merge pull request #108 from airbnb/beeline_opAdding support for beeline as part of HiveCliHook,1
Editing TODO,2
Cleaning up dead item in pool queue,4
"Carrying root filter through the DAG views, adding buttons to hide and show all series",1
Merge pull request #126 from airbnb/rootCarrying the ROOT filter through the DAG views,2
Removing the second DAG entry from menus,1
Better datetime formating in list views,5
Merge pull request #127 from airbnb/format_dttmBetter datetime formating in list views,5
Adding headers to CRUD views,1
Merge pull request #129 from airbnb/headersAdding headers to CRUD views,1
Queue management improvements,1
Fix edge case in queue management,0
Adding a druid hook and hivetodruid operator,1
Adding a bit more meat to the skeleton,1
adding druid hook and operator,1
Debugging hive_to_druid,0
Fix edge case in queue management,0
SLA bugfix,0
Merge pull request #134 from airbnb/sla_miss_fixSLA bugfix,0
Adding timeout param to BaseOperator,1
Renamed timeout to execution_timeout as it was conflicting with BaseSensorOperator,1
Docs improvements,1
Merge pull request #141 from airbnb/timeoutAdding timeout param to BaseOperator,1
fixing copypasta issue with prefix sensor + docs,2
Merge pull request #143 from airbnb/fix_prefix_sensorfixing copypasta issue with prefix sensor + docs,2
Merge branch 'master' of https://github.com/airbnb/airflow,7
Uniform colors and more circle on the dashbaord,5
Merge pull request #145 from airbnb/unsuccessfulUniform colors and more circle on the dashbaord,7
"make a baseSqlHook and apply it to mysql, postgres, sqlite",1
refactor the presto check operator into a generalized check operator,1
move isinstance check outside of loop,4
Merge branch 'master' of https://github.com/airbnb/airflow,7
Adding event callback hooks to BaseOperator\!,1
check for baseOperator inside of task_or_task loop,1
change base_hook_sql to be dbApiHook and do some more refactoring,4
Fix for templated dicts,0
clean up default_conn_name and conn_name_attr,4
add the dbapi_hook,5
rename get_first_hook to get_db_hook,5
Fixing minor templating issue in HivePartitionSensor,0
"Bugfix, calling the wrong one...",0
Merge pull request #151 from airbnb/callabacksAdding event callbacks to BaseOperator (all tasks)!,1
Merge pull request #147 from smarden1/minor-nitpickmove isinstance check outside of loop,4
Added logic to allow for an embed parameter in the URL to strip everything from the charts view except the chart itself.,2
Merge pull request #154 from jeremyclover/embed-viewAdded logic to allow for an embed parameter in the URL,2
Remove some debug prints that snuck in with b61dc2fd,0
Merge pull request #131 from airbnb/hive_to_druidHive to druid,7
Merge pull request #156 from jbalogh/drop-printsRemove some debug prints,0
Making sure that SKIPPED task state don't run,1
Merge pull request #158 from mistercrunch/skippedMaking sure that SKIPPED task state don't run,1
fix submodule label for Druid,0
Merge pull request #159 from robbwagoner/druid-setup-fixFix submodule label for Druid,0
Merge pull request #153 from airbnb/hivesensor_templateFixing minor templating issue in HivePartitionSensor,0
add jdbc extension,5
Merge pull request #160 from wooga/af-1.2.0-jdbcadd jdbc extension,5
Removing leftover comment in hive_to_druid.py,4
Add smtp_starttls flag to config,5
Fixing an edge case in SLA email aggregation,0
Merge pull request #162 from seibert/no_tlsAdd smtp_starttls flag to config,5
Improving the README.md,2
Adding Lyft to company list,1
README cosmetics,5
Adding Agari to the list of companies using Airflow in the readme,1
Merge pull request #164 from agaridata/masterAdding Agari to the list of companies using Airflow in the readme,1
Fix context bug,0
Merge pull request #166 from airbnb/contextFix context bug,0
Adding conf dict to context,5
Merge pull request #169 from mistercrunch/conf_contextAdding conf dict to context,5
Adding Wooga to the list of companies using Airflow in the readme,1
Merge pull request #172 from james-woods/masterAdding Wooga to the list of companies using Airflow in the readme,1
Resolving conflict and adding refactor,4
Merge branch 'master' of https://github.com/airbnb/airflow,7
Merge pull request #173 from airbnb/smarden1-generalize-check-operatorSmarden1 generalize check operator,1
Bugfix on refactored PrestoCheckOperator,1
Merge branch 'master' of https://github.com/airbnb/airflow,7
Fixed typo,2
Adding Yahoo! to list of users,1
Fixing the conn_ids,0
Merge pull request #177 from airbnb/fix_conn_idA fix to make the conn_id handling backward compatible,1
Merge pull request #175 from syvineckruyk/dict_fixDict fix,0
Hide configuration,5
"[cli] improved datetime parsing for the ""clear"" command",5
airflow/models.py: collect_dags default to self.dag_folder instead of DAGS_FOLDER,2
Fixing missing navbar in charts following addition of embed param,2
Fixing bug around NaN in chart json + now exposing the js error,0
Setting autocommit default,1
Merge pull request #180 from mistercrunch/fix_autocommitSetting autocommit default,0
Fix edge case that took the scheduler down,0
Merge pull request #179 from storpipfugl/collect_dagsMake scheduler adhere to specific folder,2
"Merge pull request #178 from hominot/master[cli] improved datetime parsing for the ""clear"" command",5
add jdbc settings to extras to allow storage and usage of jdbc connectionsConflicts:airflow/www/app.py,5
add StringField to imports,2
use dbapihook as base for jdbchook,5
use css to toggle visibility depending on connection type,1
make jdbc operator consistent with other db operators,1
Stripe uses Airflow,1
Merge pull request #186 from jbalogh/stripe-uses-airflowStripe uses Airflow,1
Correcting wrong reference in docs from  to,2
Merge pull request #189 from airbnb/retry_delayCorrecting wrong reference in docs from `retry_interval ` to `retry_delay `,1
Bugfix on postres_hook,1
move autocommit to own method for easier overwriting,4
remove unnecessary overrides,4
fix instance to local vars,0
Proper exception handling on plugin import,2
Merge pull request #185 from wooga/af-jdbcExtend jdbc functionality,1
"Add DAGS_FOLDER to sys.path earlier, to allow plugins to use it",1
"Merge pull request #194 from airbnb/syspathAdd DAGS_FOLDER to sys.path earlier, to allow plugins to use it",1
mssql_hook,1
mssql operator,1
mssql_to_hive,5
mssql requirements,1
Added SQL Server to connection admin drop down,4
Added SQL Server to connection admin drop down,4
corrected invalid port location in connection defaults ... was meant for mssql_default not http_default,5
Moved connection type specific top/limit logic from app view to limit_sql utility function. Removed top_sql utility function.,1
Updated documentation with mssql hooks and operators,1
Forcing order in operators imports,2
Merge pull request #183 from syvineckruyk/mssql_dbapi_hookMicrosoft SQL Server hooks and operators,1
v1.2.1,5
Picking up the connection's schema in hive operator,1
Merge pull request #200 from airbnb/hive_schemaPicking up the connection's schema in hive operator,1
"Fixing jdbc bug, generalizing custom connection forms",0
Bumping pickle_hash to BIGINT,5
Improved DAG object docstrings,2
Adding logging to hive_to_mysql,2
Added mysql_preoperator templated fields in hive_to_mysql,1
"Merge pull request #201 from airbnb/custom_connFixing jdbc bug, generalizing custom connection forms",0
Altering TODO.md,2
Making external task sensor more versatile,1
Enhancements to ExternalTaskSensor,5
Making SlaMissModel read only,1
Adding Cotap as a user of Airflow.,1
Merge pull request #210 from cotap/cotap-is-a-userAdding Cotap as a user of Airflow.,1
v1.3.0,5
Docs touchups,0
"Clarifying extras_requires in the docs, moving snakebite into an hdfs category",4
Merge pull request #209 from airbnb/task_sensorEnhancements to ExternalTaskSensor,7
TODO,2
Applying policy on UI,5
"Applying policy while populating the dagbag, #dagbagging",2
Fixing logout page redirect,2
Better logging for the scheduler,2
Merge pull request #218 from airbnb/log_schedulerBetter logging for the scheduler,2
Docstring fix HiveToMysqlTransfer,0
Preventing mssql from breaking the build,4
Setting 10 seconds timeout on HiveServer2Hook connect,1
Merge pull request #223 from airbnb/hs2_timeoutSetting 10 seconds timeout on HiveServer2Hook connect,1
"Revert ""Setting 10 seconds timeout on HiveServer2Hook connect""",1
"Merge pull request #226 from airbnb/revert-223-hs2_timeoutRevert ""Setting 10 seconds timeout on HiveServer2Hook connect""",1
Fix argument in Variable.get()The correct argument is `deserialize_json` not `deser_json`. See #220,5
Adding an FAQ entry to the docs,2
Merge pull request #230 from airbnb/faqAdding an FAQ entry to the docs,2
Allowing to set a number of runs for the scheduler,1
Merge pull request #231 from airbnb/sched_killAllowing to set a number of runs for the scheduler,1
Merge pull request #228 from jlowin/patch-1Fix argument in Variable.get() docs,2
An operator to post messages to a Slack Channel,1
Removed print statements from execute method,4
Merge pull request #233 from airbnb/slack_operatorSlack operator,1
remove snakebite from requirements,1
Gracefully fail HDFSHook,1
futurize stage 1: print_function,1
futurize stage 1: print function,1
futurize stage 1: absolute imports,2
futurize stage 1: remove redundant exception,4
snakebite can remain in extras,5
add requirement 'future',1
explicitly import object for class definitions,5
Airflow.tree schedule_interval offset fix,0
import str from builtins,2
import object explicitly,2
unicode -> str,5
import range,2
xrange -> range,5
import input,2
import basestring,2
import zip,2
import next,2
import chr,2
import standard library and create aliases,1
import configparser,5
urlparse -> urllib.parse,5
Use list to allow modification in-place,1
use list to allow indexing,1
use list to be safe,1
use list to modify in-place,1
use list to be safe,1
use list to allow indexing,1
use list to be safe,1
use list to be safe,1
use list to be safe,1
preserve int division,5
iteritems -> items,5
use list to be safe,1
cStringIO -> io,5
StringIO -> BytesIO,5
Adding a TimeDeltaSensor,1
Merge pull request #237 from airbnb/timedeltaAdding a TimeDeltaSensor,1
Clarifying how to set the schedule_interval in the docs,2
fix indent typo,2
Merge pull request #238 from airbnb/sched_intClarifying how to set the schedule_interval in the docs,2
Removing noisy logging call,2
Changing the TimeDeltaSensor to set the delta relative to the start time,1
"Merge pull request #234 from jlowin/Py3Python 3 compatibility, Stage 1",7
Merge pull request #239 from airbnb/fix_timedeltaChanging the TimeDeltaSensor to set the delta relative to the start time,1
Debugging py3 stage2,0
Merge pull request #241 from airbnb/Py3_stage_2Py3 compatibility - stage 2,7
Better error message for untemplatable types,0
import basestring for py3 compatibility,2
ensure writing works for BaseOperator,1
Warning on borken files in DAGS_FOLDER,2
adding a fix for unicodecsv,0
Merge pull request #243 from airbnb/fix_unicode_delimiter_issueadding a fix for unicodecsv,0
Making all tests point to the same DAG_ID,2
"Use coalesce to protect against NULLWithout this coalesce, func.sum() can return None (if there are no rowsto sum over). Then the value of successes becomes None, and in Python 3comparing None to an int raises an error (as in line 690 of models.py)",0
update docs,2
Bugfix unicode delimter issue in python2.7 when using csv,1
fixing redundant condition introduced by 2to3 with a compatible correct test,3
using past.builtins instead of try except,1
Merge pull request #247 from airbnb/fix_string_testing_in_email_utilfixing redundant condition introduced by 2to3 with a compatible correct test,3
Merge pull request #242 from airbnb/brokenWarning on unparsable files in DAGS_FOLDER,2
Merge pull request #235 from microblag/masterAirflow.tree schedule_interval offset fix,0
Fix conflict with utils.pyConflicts:airflow/utils.py,5
update PR -- merge with master,7
Adding a UTC clarification to UI clock,1
Bugfix edge case around recent time related fix in tree view,0
"Typo in config docschanging ""you"" to ""your""",4
Merge pull request #250 from JackDanger/doc-typo-you-yourTypo in config docs,2
Setting up necessary dependencies for tests,3
Merge pull request #252 from JackDanger/automatic-test-setupSetting up necessary dependencies for tests,3
Improving coverage by 5-6%,3
Adds the ability to use environment variables to get databaseconfigurations instead of storing it.,5
XCom class,5
TI push/pull,5
Operator push/pull,1
store XCom from returned values,5
example,5
Merge pull request #253 from airbnb/coverageImproving coverage by 5-6%,3
Improving the docs,2
Update README.mdAdded Xoom,2
Merge pull request #1 from gepser/gepser-patch-1Update README.md,2
Merge pull request #258 from gepser/masterAdded Xoom,1
"check DAGs for None before the loop, to allow [None] escape",1
Check that visible_on isn't in the past,5
Drop duplicates,4
documentation,2
Py3 compatibility,5
merge upstream,7
add missing colon,1
Adding toggle button functionality to easily pause dags in the dag view.,2
Merge pull request #265 from jeremyclover/pauseAdding toggle button functionality to easily pause dags in the dag view.,2
Removing js flicker on page load,4
Adding header/tooltip for pause toggle column,1
adding convenience function for uploading from string,1
simplify container logic,2
Simplify push/pull mechanism,5
example and docs,2
Merge pull request #266 from airbnb/adding_s3_load_from_stringadding convenience function for uploading from string,1
Bugfix on main dash around toggle when there are broken dags,2
adding extra to connection model init,5
v1.4.0,5
Merge pull request #271 from nave91/add_extra_to_connection_initadding extra to connection model init,5
remove typecheck,4
no need for session handling,5
return None instead of raising XComException,2
visible_on -> execution_date,5
"task, dag -> task_id, dag_id",2
better key for auto-returned XComs,1
delete any matching XComs before inserting new ones,1
re-add expunge/commit because @providesession calls expunge,1
split out get_one/get_many,1
update example,5
remove import,2
Adding links section ro README.md,2
README.md touchup,0
Error message when task is gone on task details page,0
Merge pull request #274 from airbnb/task_missingError message when task is gone on task details page,0
"Port of the kt_renewer from hue to airflow. It requires new settings in airflow.cfg in [security]. kinit_path to specify the location of kinit. keytab to specify the location of the keytab file, principal for the kerberos principal to use, reinit_frequency the interval for ticket renewal, ccache the location of the ccache file.",2
Add backfill option to not pickle the DAG,2
Merge pull request #276 from airbnb/dont_pickleAdd backfill option to not pickle the DAG,2
update return value,5
specify column length,5
add defaults for security,1
remove `is not None`,4
"update default key for xcom_pull, and docs/example",2
Displaying message notifying users that Connections are stored in clear if cryptography package isn't available,1
Merge pull request #260 from jlowin/XComAdd XCom (cross-communication) functionality,1
Merge remote-tracking branch 'airbnb/master' into Py3_debugging,0
Merge remote-tracking branch 'airbnb/master',7
make sure to retrieve an integer for reinit_frequency,5
Force depends_on_past = True when wait_for_downstream is used,1
Merge pull request #283 from mistercrunch/force_dopForce depends_on_past = True when wait_for_downstream is used,1
Tying the flower_port configuration param to the CLI,2
Merge pull request #284 from mistercrunch/flower_portTying the flower_port configuration param to the CLI,2
Making the dependency engine more flexible,1
Testing/debugging,3
Doc entry,1
Clarifying docs entry for trigger_rule,1
Improving the HiveToDruidOperator,1
Merge pull request #287 from airbnb/druidImproving the HiveToDruidOperator,1
Merge remote-tracking branch 'airbnb/master',7
load env var configuration first,5
document configuration,5
Merge pull request #288 from jlowin/env_var_configSet configuration with env vars,5
Small chance to concepts docs`airflow.connection.Connection` should be `airflow.models.Connection`,2
Merge pull request #291 from jlowin/patch-2Small chance to concepts docs,2
Merge pull request #279 from airbnb/crypto_msgWarning about Connections being stored in clear,2
Merge pull request #245 from jlowin/Py3_debuggingPy3 debugging,0
first pass as encrypting passwords,4
include fernet key when creating config file for the first time,2
make some changes to connection model,4
make upgrades for metadata database easier across the board,5
make crypto setup option,1
A few tweaks around alembic/migrations,5
follow symlinks in dag_folder and plugins_folder,2
Forgot to git add the new migration,1
Merge pull request #293 from jlowin/follow_symlinksFollow symlinks in DAG and Plugin folders,2
"CLI unit tests, bugfix",0
"Merge pull request #294 from airbnb/cli_testsCLI unit tests, bugfix",0
"fix a UnicodeDecodeError when render airflow/ti_code.html, logging write utf-8 by default",2
Merge pull request #298 from CooperLuan/masteruse unicode when render template,1
Pinning flask-admin to 1.2.0,5
Adding a link to docker-airflow,2
DbApiHook.run allows for a list of sql statements,1
Merge pull request #302 from airbnb/multi_runDbApiHook.run allows for a list of sql statements,1
Clarify SubDagOperator exception,2
Merge pull request #304 from jlowin/patch-1Clarify SubDagOperator exception,2
Making sure a fernet_key exists in airflow.cfg,5
Merge pull request #292 from airbnb/encrypt_passwordsEncrypt passwords + alembic!,4
v1.5.0,5
Merge pull request #261 from airbnb/dep_ruleMaking the dependency engine more flexible,1
Adding fields to task_instance,1
"Revert ""Making the dependency engine more flexible""",1
"Merge pull request #305 from airbnb/revert-261-dep_ruleRevert ""Making the dependency engine more flexible""",1
Improving is_encrypted migration to only add column if not exists,1
Making CheckOperator agnostic on the name of the get_first param,2
Merge pull request #311 from airbnb/fix_checkMaking CheckOperator agnostic on the name of the get_first param,2
drop alembic version_table in resetdb,5
update current_schema to reflect true current schema,5
delete unnecessary migration,4
update guards for adding encrypted,1
make generate_fernet_key function,1
add warning with instructions for creating a key,1
add note about creating a key,1
Py3 fixes for encryption,0
"if plaintext, don't write the bytes value as a string",5
simpler way to protect against bytes,2
ignore error.log,2
add version for compatibility,1
Merge pull request #312 from jlowin/fix_alembicFix alembic,0
Merge pull request #313 from jlowin/cryptography_warningMore instructions/warnings about using cryptography,1
Merge pull request #314 from jlowin/patch-1Py3 fixes for encryption,0
implement rich comparison operators,1
make sure bytes get decoded to strings,1
handle case where attribute doesn't exist,0
remove type restriction for ordering,4
fix missing comma,0
fix hash functions to handle non-hashable attributes,0
unit tests for comparisons,3
add timeout/fallback to tree traversal,1
expand_all -> force_expand for clarity,1
Adding Jampp as passionate Airflow user and lover,1
Merge pull request #321 from jampphq/readme_user_updateAdding Jampp as passionate Airflow user and lover,1
fixing s3 sensor missing '/',0
Merge pull request #323 from airbnb/fixing_s3_sensor_loggingfixing s3 sensor missing '/',2
Migration script,5
Merge pull request #325 from airbnb/ti_plusplusLogging operator name and queued_dttm into task_instance,1
#326 default_login.py does not work out of the box,1
Merge pull request #315 from jlowin/rich_comps(Py3) implement comparison operators,1
Adding unit test,3
Setting supports_autocommit to False for sqlitehook,1
Merge pull request #327 from kapil-malik/master#326 default_login.py does not work out of the box,1
Merge pull request #328 from airbnb/fix_sqliteFix sqlite,0
Allowing DbApi operators to accept list of statements,1
Encryption tweaks,5
Clarifying a few operators docstrings,2
Instructions on how to setup unit tests,3
Adding a note about using psycopg2 for Postgres,1
Preventing plugins_manager from loading the same plugin twice,2
pluggable executor needs to be instantiated,2
Stop recursion after 5000 nodes,5
Merge remote-tracking branch 'airbnb/master' into py3-encryption-2Conflicts:airflow/models.py,5
Merge pull request #331 from ikalinin/master[Fix] pluggable executor needs to be instantiated,5
stuff,5
in the base hook use an environment variable first as the conn_id before going to db,5
clean up the old cruft,4
remove the unneeded migration,4
Merge branch 'master' into env_connections,7
Adding a link to the Agari Blog Post about Airflow,2
Merge pull request #333 from r39132/masterAdding a link to the Agari Blog Post about Airflow,2
Py3 compatibility: make bytes explicit,1
left env_variable in the connection model,5
forgot to include the call to self,2
Fixing configuration item for FLOWER_PORT not making it through,1
fixed the base_hook in regards to env variable,1
added some documentation,2
Adding a GenericTransfer operator,1
Merge pull request #338 from airbnb/generic_transferAdding a GenericTransfer operator,1
merge master,7
Hooking up the port to the MySqlHook,1
Port should be an int,5
Merge pull request #339 from airbnb/mysql_portHooking up the port from the connection to the MySqlHook,1
Adding for element to navigate the tree view,1
Improvments to HiveToDruid,5
Merge pull request #344 from airbnb/navigate_treeAdding form element to navigate the tree view,1
Merge pull request #319 from jlowin/py3-encryption-2(Py3) bytes/string fix for encryption,0
Add a flag to clear without confirmation,5
#317 Multi-tenant web UI,5
Merge pull request #352 from kapil-malik/master#317 Multi-tenant web UI,7
Druid conf tweaks,5
Merge pull request #351 from airbnb/clear_noconfirmAdd a flag to clear without confirmation,5
Merge pull request #335 from jlowin/py3-bashoperatorPy3 compatibility: BashOperator sentinel,1
Fixing the unit tests,3
Merge pull request #354 from airbnb/env_connectionsEnv connections,7
Making the dependency engine more flexible,1
Merge pull request #355 from airbnb/dep_rule2Making the dependency engine more flexible - second try,1
Merge pull request #320 from jlowin/fast_tree_viewAdd timeout/fallback algorithm for building tree view,1
Bugfix for issues #356,0
Commenting out pyhs2 as readthedocs won't compile it,2
Trying ipython (not [all]) against readthedocs,2
Doc entry,1
Clarifying docs entry for trigger_rule,1
use future for urlparse,1
Bugfix for Invalid default value for timestamp,0
Updating our GitHub organization name since we claimed it,5
Merge pull request #358 from jlowin/py3-urlparsePy3: urlparse doesn't exist,7
Merge pull request #359 from jampp/readme_user_updateUpdating our GitHub organisation name since we claimed and gain it!,5
v1.5.1,5
support (nested) interpolation of env vars,1
expand airflow.cfg and defaults,5
str(env_var),5
use getboolean in jobs.py,1
Merge pull request #362 from jlowin/patch-2use getboolean in jobs.py,1
Merge pull request #361 from jlowin/expand-varssupport (nested) interpolation of env vars,1
Raise an error if a pool doesn't exist,0
add explicit encode/decode for Fernet Key,1
Merge pull request #365 from jlowin/fernet_key_str(Py3) Explicit bytes transformation for Fernet,7
Create oracle_hook.py,1
Update __init__.pyAdd oracle_hook integration,1
Update oracle_hook.pyadd dependencies,1
Update utils.pyadd support for Data Profiling with Oracle SQL,5
Update models.pyadd Oracle SQL support through the OracleHook,1
"Update oracle_hook.pyadd comments, remove unnecessary code",4
Merge pull request #364 from jlowin/patch-3Raise an error if a pool doesn't exist,0
[PythonOperator] pass a dict of templates to get templatified,1
Merge pull request #369 from airbnb/template_dict[PythonOperator] pass a dict of templates to get templatified,1
Update setup.pyAdd cx_Oracle requirements to extras_require for the OracleHook,1
"Update oracle_hook.pyRework code, add additional dependencies and additional docsRemove unnecessary code",4
Merge pull request #368 from yoziru-desu/airflow-oracle-hookOracleHook: Add Oracle SQL Support,1
Adding list and dict support to the UI template renderer,1
Improving the tests,3
Making the scheduler more resilient,1
Using gunicorn instead of Tornada as the production wsgi server,1
add attachment support in EmailOperator,1
Added charset and cursor functionality to mysql hook,1
Merge pull request #374 from airbnb/fix_list_templaterenderAdding list and dict support to the UI template renderer,1
Merge pull request #376 from airbnb/schedulerMaking the scheduler more resilient,1
Minor test adjustment,3
Merge pull request #381 from Arkoniak/mysql_hookAdded charset and cursor functionality to mysql hook,1
Merge pull request #378 from airbnb/gunicornUsing gunicorn instead of Tornado as the production wsgi server,1
Increasing timeout for gunicorn,1
reset param files default to None instead of [] in EmailOperator,1
Merge pull request #379 from CooperLuan/masteradd attachments support in EmailOperator,1
Fixed utf encoding of source code,0
changed round_time to allow for relative delta,1
encoding already fixed in master,0
Added comments explaining round_time,5
In case of equal distance return higher value to improve consistency with previous version,1
"Rebasing, adding docs and docetests",3
Merge pull request #386 from airbnb/fpaEnable schedule_interval to support dateutil.relativedelta.relativedelta (monthly DAGs!),2
Waiting for subprocess,5
"Fix grammatical error in README.mdFixed ""they becomes more"" to ""they become more""",2
Merge pull request #390 from johnw424/patch-1Fix grammatical error in README.md,2
Pinning flask-login as they are making breaking changes,4
"Safer, but still loosely pinned dependencies in setup.py",1
add extra fields to form columns,1
Warning if start_date isn't datetime,5
Merge pull request #396 from wooga/af-1.5.1add extra fields to form columns,1
Re-pinning flask-admin to 1.2.0,5
Unit tests improvements,1
Handling SKIPPED status in backfills,5
Merge pull request #397 from airbnb/fix_skipped_bfHandling SKIPPED status in backfills,0
Putting the type check warning in the right spot,2
Docs clarifications,2
revised dry run proposal,1
Don't warn if start_date is None,5
Adding note on setting postgres schema in install docs,2
Docs improvments to the tutorial,2
Merge pull request #403 from jlowin/patch-4Don't warn if start_date is None,5
Pythonesquifying,5
Warning on specifying a schedule_interval at the task level,2
use startswith instead,1
use exceptions intead of returning tuple,1
add FTPHook,1
Bump Pandas requirement to <1.0,1
Merge pull request #407 from jlowin/patch-5Bump Pandas requirement to <1.0,1
use exceptions intead of returning tuple,1
"Merge remote-tracking branch 'origin/feature_dry_run' into feature_dry_run# Please enter a commit message to explain why this merge is necessary,# especially if it merges an updated upstream into a topic branch.## Lines starting with '#' will be ignored, and an empty message aborts# the commit.",5
Sharing hack to restart the scheduler every N runs,1
move FTPHook to contrib folder,1
remove FTPhook from hooks,1
Adding Chartboost to list of companies using Airflow,1
Merge pull request #409 from dclubb/masterAdding Chartboost to list of companies using Airflow,1
add chain function,1
Doc improvments,2
Merge pull request #410 from jlowin/chainadd chain function,1
fix bugs in test_hql,3
remove verbosity in hiveOp dry_run,1
BaseOperator dry_run,1
add dry run for backfill CLI,1
remove formatting from xcom docstring,2
Testing the pickling,3
Merge pull request #412 from airbnb/test_picklesTesting the pickling,3
Merge pull request #411 from jlowin/patch-7remove formatting from xcom docstring,2
#393 Scheduler does not pickle DAGs for non-local executors,2
added vertica hook,1
Add MesosExecutor for airflow,1
issue fix for vertica,0
added VerticaOperator,1
added vertica_operator include path,1
changed vertica_operator color,1
bug fix for verticaoperator,1
changed vertica to hive color,4
added vertica to hive path,1
bug fix for verticaoperator,1
bug fix for vertica_to_hvie,0
Add BlueApron and gh handles to user list,1
use sasl/kerberos for snakebite if configured as such,5
Merge remote-tracking branch 'upstream/master' into kerberos,7
Merge pull request #415 from blueapron/update-readmeUpdate README with new BlueApron user,1
Add tests for dry run,1
fix util round_time bug,0
fix for vertica_to_hive type mapping,0
remove debug log,2
remove mysql include,4
Merge pull request #419 from jason-z-hang/zz_fix_round_timeFix utils.round_time() bug,0
Merge pull request #418 from griffinqiu/vertica_hookAdd Vertica Database support for Airflow,1
Merge pull request #400 from patrickleotardif/feature_dry_runRevised dry run proposal,1
Merge pull request #414 from kapil-malik/mesosAdd MesosExecutor for airflow,1
Improving json encoding,5
Fixing import failure when mesos lib is missing,0
Merge pull request #421 from airbnb/fix_nomesosFixing import failure when mesos lib is missing,0
Merge pull request #413 from kapil-malik/master#393 Scheduler does not pickle DAGs for non-local executors,2
Add check for kerberos,1
put FTPHook in contrib module,1
add contrib placeholder for operators,1
Merge branch 'airbnb/master' into ftp,7
Merge pull request #420 from airbnb/better_jsonImproving json encoding,5
Merge pull request #405 from jlowin/ftpAdd FTPHook,1
Python3 compatibility for MesosExecutor and documentation changes,4
Convert numpy types to their native python typesI don't know exactly why but for some reason the json encoder in python3 can't encode some of the numpy datetypes. Now I simply converted the numpy data types (http://docs.scipy.org/doc/numpy/user/basics.types.html) to the respective python types.,2
fixed typo,2
Merge pull request #426 from kapil-malik/masterPython3 compatibility for MesosExecutor and documentation changes,4
more informative error message,0
Merge pull request #429 from jlowin/import_errmore informative error message,0
Documenting contrib,2
More doc fixes,0
Doc clarifications around start_date,5
Merge remote-tracking branch 'upstream/master' into kerberos,7
Align code with PEP8* Replaces tabs with spaces* Removes unnecessary whitespace* Fixes formatting errors,0
import BaseHook,1
Retrieve principal from extra connection settings and make beeline work with kerberos/sasl,1
fix typo,2
remove cmd_extra awkward addition,1
Make sure KRB5CCNAME gets set before anything else,1
add documentation on kerberos authentication,2
Merge pull request #432 from jlowin/patch-7import BaseHook in airflow.hooks,1
Py3 fixes,0
Merge pull request #441 from jlowin/Py3Py3 fixes,0
Merge pull request #430 from airbnb/documenting_contribDocumenting contrib,2
option to push xcom from bashoperator,1
add default line (in case there's no stdout),1
Merge pull request #443 from jlowin/return_bashAllow XCom push from BashOperator,1
Add proxy impersonation to beeline support in the hive cli hook,1
Add description of how to use proxy_user for the hive cli and fix some formatting,0
pass bind parameters to execute,2
update hooks to use parameters,2
Fix typo,2
Merge pull request #446 from jlowin/patch-9Fix typo in jobs.py,2
Merge pull request #445 from jlowin/sql_paramsUse SQL params in DBApiHook,5
Move kerberos to its own sub module,4
Implement run_as functionality based on the login of the connection or the owner of the DAG,2
add buttons for Mark Success Future+Past,1
change action endpoint for success future and past,4
pep8 fixes,0
Fixing SKIPPED from propagating when it shouldn't,0
more detailed SLA miss email,5
Guard against no rootsCloses #453,2
Merge pull request #454 from jlowin/patch-11Guard against no roots,7
Merge pull request #449 from airbnb/fix_joinFixing SKIPPED from propagating when it shouldn't,5
Changing the default base date of the tree view,5
Merge pull request #427 from LilithWittmann/numpy_json_encoderConvert numpy types to their native python types,5
Fix issue https://github.com/airbnb/airflow/issues/459 - mysql error 2014,0
Properly treating new DAGs in Tree View,2
Merge pull request #1 from mtustin-handy/mtustin-handy-mysql-commit-patchFix issue https://github.com/airbnb/airflow/issues/459 - mysql error 2014,0
Merge pull request #455 from kerzhner/tree-viewChanging the default base date of the Tree View,5
Merge pull request #460 from mtustin-handy/mtustin-handy-mysql-commit-patchFix issue #459 - mysql error 2014,0
Merge pull request #2 from airbnb/masterUpdate from origin,5
print traceback for internal server error,0
Merge pull request #462 from airbnb/traceback_for_500print traceback for internal server error,0
Use airflow.security.utils instead of socket,1
Allow MesosExecutor to re-register with MesosThis PR adds the ability to re-register the framework of theMesosExecutor with Mesos. That way tasks on Mesos keep running whilethe Scheduler is being restarted.,1
added apply_defaults to SlackAPI operatorscleaned up docstring typesrenamed params attribute to api_call_params to avoid conflicts/confusion with task_instance paramsmoved the building of the api_call_params dict to it's own function which is run by the execute function.execute method will pass on failure. Notification should not cause a DAG to fail,0
consolidate base_date rounding,5
don't modify non-string env_vars,5
Use short hand for security.utils,1
don't round base_date if passed directly,4
use basestring (py3 compatible),1
use safe_dag_id (without periods),2
reinstated params as parameter to SlackAPIOperatorallow failure,0
move celery/statsd to optional requirements; alphabetize reqs,1
Merge pull request #467 from jlowin/fix_env_varFix issue with expand_env_var,0
Merge pull request #464 from jlowin/fix_tree_roundingconsolidate base_date rounding in tree view,5
Merge pull request #469 from jlowin/patch-12Move celery to optional requirements,1
Make sure to use defaults for security settings,1
"Revert ""Fix issue with expand_env_var""",0
"Merge pull request #472 from jlowin/revert-467-fix_env_varRevert ""Fix issue with expand_env_var""",0
Merge pull request #463 from syvineckruyk/slackapioperator_improvementssmall improvements to slack operator. fixed templating.,0
Printing hostname on 404 page,5
add MAX PERIODS cap to mark success,1
Adding plugins_folder to sys.path,5
bugfix for better sla messages,1
Merge pull request #3 from airbnb/masterRefresh from origin,7
Update configuration.py,5
Control donot_pickle default from conf,5
Merge pull request #477 from mtustin-handy/mtustin-handy-donot-pickleMake donot_pickle a configuration option,5
Make default behaviour of backfill command depend on donot_pickle configoption,5
ADding a param to the docs,2
Merge pull request #478 from mtustin-handy/mtustin-handy-donot-pickleMake default behaviour of backfill command depend on donot_pickle config,5
Merge pull request #448 from patrickleotardif/mark-success-ui-enhanceWeb UI Mark Success Future and Past toggles,7
Extra logging for celery queue,2
Namespacing plugins properly,2
Typo,2
"add large if block, modify sla misses",1
remove logging lines used for testing,3
Merge pull request #450 from patrickleotardif/sla_better_msgMore detailed SLA miss email,1
"Make sure that KRB5_KTNAME is also set to the keytab for airflow* A keytab is required to verify the authenticity of the KDC and is* required to prevent spoofing in case of user verification.* In general this points to the system's key tab, ie. in /etc/krb5.keytab* but this is normally only readable by root, hence it needs to point* to the keytab in use by airflow",1
Use [kerberos] in cfg instead of [security],5
Merge remote-tracking branch 'upstream/master' into kerberos,7
only mark needed tasks as success,5
Merge pull request #485 from patrickleotardif/mark_success_bugfix[bugfix] only mark needed tasks as success,0
Docs tweaks,2
bugfix for mark success,0
Merge pull request #486 from patrickleotardif/success-bugfix[bug] mark success fails to change existing failed tasks,0
Added default_var to Variable,1
Adding hive subpackages to installation docs,2
Merge pull request #438 from bolkedebruin/kerberosKerberos,7
Changed order of arguments and more informative error message,0
Formatting the queued_dttm column in TI view,5
Fix how we recurse through collection-like template fields.,0
Adding more options to TaskInstance view,1
Merge pull request #488 from Arkoniak/ext_variablesAdded default_var to Variable,1
_parse_s3_url -> (staticmethod) parse_s3_url,5
import warnings,2
use path.join,1
ship log to S3 (and don't serve),2
load log from s3,2
add note to airflow.cfg,5
try to clean up tmp files,2
check if key exists in S3,5
Adding mysql_postoperator to HiveToMySqlTransfer,1
add lingochamp,1
Merge pull request #496 from Jparks2532/mysql_postoperatorAdding mysql_postoperator to HiveToMySqlTransfer,1
Merge pull request #497 from lingochamp/lingochampadd lingochamp,1
"Allow user principals (without host part) as well as service principals* This allows teh configuration for the principal of the keytab to look like ""principal/_HOST@MYREALM.COM""* where _HOST is replaced by the fqdn of the current host",5
Added ability to filter node beyond mouse over by clicking on the state legend,1
check if file exists,2
Fix typo in CONTRIBUTING.md,2
Merge pull request #502 from karthikgollapudi/patch-1Fix typo in CONTRIBUTING.md,2
Merge pull request #499 from bolkedebruin/masterAllow user principals (without host part) as well as service principals,1
Merge pull request #491 from kkourtchikov/fix_template_recursionFix how we recurse through collection-like template fields.,0
Removed redundant 'supports_autocommit' field in DbApiHook,5
Merge pull request #504 from rdavison/masterRemoved redundant 'supports_autocommit' field in DbApiHook,5
Fix typo,2
Changed cursor from resize to pointer for legenditems,4
Merge pull request #506 from avram/patch-1Fix typo,2
Merge pull request #500 from DinoCow/persistent-state-focus-on-clickAdded ability to filter node beyond mouse over by clicking on the state legend,1
Removing the duplicated get_conn method in DbApiHook,5
Adding bulk load option for HiveToMySqlTransfer,1
Fixing bug with temp file,2
Adding postoperator code,1
importing temp_file,2
Fixing variable scope,0
applying local_infile flag,5
Pulling local_infile from json,5
Change function call in render templates,1
Fix `collect_dags` docstring typo.,2
Merge pull request #512 from ajw0100/fix-docstring-typoFix `collect_dags` docstring typo.,2
fix documentation typo,2
Applying maxs suggestions,5
removing logging import,2
Merge pull request #513 from avram/patch-2fix documentation typo,2
improving documentation,2
Adding link to chef recipe repo on README,2
Merge pull request #507 from Jparks2532/bulk_load_mysqlBulk load mysql,7
Add BaseSensorOperator to __init__,5
adding delimiter to core,1
fixing bug,0
fixing tab,0
removing size,4
removing carriage returns,4
fixing args,0
Merge pull request #515 from Jparks2532/to_csv_delimiterTo csv delimiter,7
Merge pull request #514 from patrickleotardif/patch-3Add BaseSensorOperator to __init__,5
Show more DAGs on main page,2
Fix typos in code.rst,2
Updating TODO,2
Merge pull request #519 from avram/patch-3Fix typos in code.rst,2
Update models.py,5
Merge pull request #510 from patrickleotardif/patch-2Change function call in render templates,1
Increasing max recursion to allow deepcopy of large DAGs,2
Documentation text did not match the sample code,2
Made the text and samples in tutorial.rst in line with each other,5
add function to check for table existence,1
Merge pull request #521 from JordyMoos/doc-tutorial-fixesDocumentation text did not match the sample code,2
Merge pull request #468 from jlowin/safe_dag_namesUse safe dag_ids for d3 compatibility,2
Merge pull request #525 from airbnb/hive_metastore_thrift_table_existsadd function to check for table existence,1
Updating TODO,2
Adding DbApiHook to docs,2
Adding missing param to hive_to_druid docs,2
"like MesosExecutor, import CeleryExecutor only when it's available",2
Only fetching dag states for active DAGs,2
"Initial support for kerberos logins, needs cleanup",4
Basic work on kerberos logins,2
allow empty hostname to return standard fqdn,1
Fix some minor issues,0
Update kerberos_login with new cfg locations and move some logic,2
Add initial travis-ci,5
Use travis containers instead,1
Port ci tests from snakebite in order to test against hadoop,3
allow execute rights,1
Add pip install to make sure to generate output fast enough for travis,1
Allow higher version of flask-login. Why is this capped?,2
make requirements work,1
Allow higher version of sqlalchemy,1
More verbose tox logging to make sure no timeouts happen at travis,1
Remove pip install as this is handled by tox,0
Use python setup instead of dependencies to make sure build can run without timeout,1
add travis wait to workaround timeouts,1
Add timeout parameter,2
testing,3
Testing,3
correct paths,5
use requirements.txt,5
Add db configs,5
fix statement,0
Add minikdc to be downloaded and made available,5
Add kdc download and some test,3
Add enviromnment variables important for secured minicluster,5
Add caching,1
Place on one line to ensure only relevant builds,5
Make setup_kdc executable,1
Try without travis_wait,1
Add wheel and cache,1
Fix path to wheelhouse,1
remove --use-wheel as it doesn't exist,1
invoke tox directly,5
add verbose to pip install,1
Install requirements from commands,1
Show current wheelhouse,1
Check if wheelhouse is used,1
test,3
set PIP_XXX for wheel,1
test env variables,3
multiple lines,5
install wheel first,2
skipsdist,5
Update script for misisng wheels (does not check versions),5
Use $HOME,1
testing,3
use {homedir},1
testing,3
use correct separator,1
make case insensitive,1
fix,0
fix typo,2
different way of matching,5
"use find links of pip wheel build, remove extra cache not required anymore",1
fix typo,2
Add suport for python 3.5,1
Remove incompatible builds from the matrix,4
Removed unneeded script,4
Adding pyhs2 in requirements.txt,5
Adding extra connection expected in tests,3
Trying different strategies to get initdb to run,1
Tweaks,5
Removing doctests for the time being,3
Fixing conn_ids,0
Allowing empty string password for mysql_hook,1
Adding a custom airflow.cfg from the repo,5
Fixing sqlite test,3
Moving to hadoop tests to the side for now,3
Moving to from mysql-python to pymysql,4
Test echo sql,3
Reverting,4
Move to mysqlclient instead of pymysql for python3,4
Make sure the cfg can be copied in any travis environment,5
Fix path,0
Fix path (again),0
"Switch to python 3.4, 3.5 was a bit too ambitious for now",5
Fix python version,0
Fixing chart unit test,3
Fixed some web dag views test,3
Making hive/presto tests optional,3
"Removing pyhs2 from reqs, failing pickling test",3
Py3 compatibility in tests,3
Typo,2
Preventing py3 regression in key modules,5
Rolling back some __future__ imports,2
Trying decode instead of encode,1
utf-8 instead of ascii,2
Adding coveralls file,2
Merge pull request #516 from airbnb/travisciIntegrating Airflow with Travis-CI,7
Adding badges to README,1
Fixing pypi badge,0
Adding coveralls dep to tox,1
Adding coveralls call,1
Trying to get coveralls to run,1
Forcing flask admin 1.2.0,1
More coveralls tries,5
More coverall tries,5
Moving setenv,1
Merge pull request #538 from airbnb/coverallsGetting coveralls to work.,1
Merge pull request #431 from wndhydrnt/mesos_framework_reregisterAllow MesosExecutor to re-register with Mesos,1
Merge pull request #532 from jochem/celery_optional_in_executorsImport CeleryExecutor only when available,2
Better hive cli default for hive_to_druid,1
Adding logging to h2d,2
Automating the killing of zombie task instances,2
Adding index to speed zombie lokkup,1
Adding state back to TI constructor,1
Fixing httphook,1
Trying https,1
Merge pull request #528 from airbnb/kill_zombiesAutomating the killing of zombie task instances,7
"Bugfix for new pandas version, unit test improvment",3
Merge pull request #535 from airbnb/speedup_dag_statesOnly fetching dag states for active DAGs,2
Moving the fernet msg some place else,4
added wetransfer to airflow users,1
Merge pull request #543 from jochem/masteradded wetransfer to airflow users,1
v1.5.2,5
"Py3.4: Pass ConfigParser's extra kwargs throughIn Python 3.4 ConfigParser has it's own string interpolation which recursivelycalls get, but with extra kwargs, raw and fallback.Accept these and pass them straight on to ConfigParser",5
Merge pull request #545 from lentinj/masterPy3.4: Pass ConfigParser's extra kwargs through,5
clamp flask-login to 0.2.11 see also https://github.com/GovLab/noi2/issues/7,0
Add Easy Taxi to list of companies using Airflow,1
Add Easy Taxi to list of companies using Airflow,1
Add Easy Taxi to list of companies using Airflow,1
Merge pull request #547 from easytaxibr/docsAdd Easy Taxi to list of companies using Airflow,1
add ShortCircuitOperator,1
add example for ShortCircuitOperator,1
First running version with app factory,1
Merge pull request #546 from bolkedebruin/masterclamp flask-login to 0.2.11,2
Merge pull request #548 from jlowin/shortcircuitAdd ShortCircuitOperator,1
Fix extra tests,3
Fixing tests,3
Rework the flask app to return an app. First iteration,1
Make python 3 compatible,1
Make py3 compatible,1
More tests,3
More tests,3
Merge pull request #550 from airbnb/more_tests2More unittests,3
Better doctest integration,3
Adding tests for CheckOperators,1
Making login=root for mysql_default,2
Merge pull request #551 from airbnb/more_tests2More tests2,3
Cleaning up default connections,4
Merge pull request #552 from airbnb/more_tests2Cleaning up default connections,3
Merge remote-tracking branch 'upstream/master' into rewire_www,7
Cleanup imports,2
Make FILTER_BY_OWNER present again,1
Testing emails (dryrun),1
More CLI tests,3
Fixing the wiring,0
Tons of tests improvments,3
"Fix cursor execution with None parameters with cx_Oracle (dbapi_hook)cx_Oracle doesn't like None parameters, so I changed all cursor execution lines to check if parameters are given or notIf not, the parameters are left out completely from the cursor execution",2
add s3_log_folder config option,5
run s3 log in parallel to log server,2
log documentation,2
Fix typos in HiveOperator,1
Merge pull request #556 from yoziru-desu/masterFix cursor execution with None parameters in cx_Oracle (dbapi_hook),5
Merge pull request #557 from thoralf-gutierrez/Fix_typos_in_HiveOperatorFix typos in HiveOperator,1
wrap S3 access in try/except,1
set old_log if file doesn't exist,2
improve log messages,2
S3Hook: better handling when key isn't replaced,1
pass concurrency argument to workers,1
"Fix typos, correct principal host substitution and update documentation",2
Host can be None sometimes,5
"Merge pull request #560 from bolkedebruin/masterFix typos, correct principal host substitution and update documentation",2
Merge pull request #559 from jlowin/worker_concurrencypass concurrency argument to workers,1
Adapting CONTRIBUTING.md,5
Merge pull request #493 from jlowin/patch-4Expose parse_s3_url() directly,7
raise error instead of returning,0
Merge pull request #558 from jlowin/patch-7S3Hook: better handling when key isn't replaced,1
Merge pull request #554 from airbnb/more_tests2Testing email and more cli subcommands,3
Making the end of celery_executor async by default,1
Merge pull request #561 from airbnb/fix_schedulerMaking the end of celery_executor async by default,0
Merge pull request #494 from jlowin/s3_logSupport worker logs on S3,2
correct state mentioned in logs,2
Merge pull request #564 from thoralf-gutierrez/Correct_logged_state_in_BranchPythonOperatorCorrect logged state in BranchPythonOperator,1
Speed up downloads of hadoop distros,5
Use travis instead of tox to set cache dir,1
Typo fix,0
Cache minikdc requirements,1
Set cache for ivy,1
Make cp recursive,1
Merge pull request #566 from bolkedebruin/masterSpeed up requirements downloading for tests,3
Merge remote-tracking branch 'upstream/master' into rewire_wwwalso includes PR-#494 conflicts,5
Fix test setup,1
Adding a sql metastore partition sensor,1
Merge pull request #568 from airbnb/metastore_sensorAdding a sql metastore partition sensor,1
Disable minikdc for now as it is not active and ivy is messing up the caching,5
Make sure to include the s3 improvements,1
Merge pull request #553 from bolkedebruin/rewire_wwwRewire www-app to have a function to return an app,1
Fix caching by using TRAVIS_CACHE and moving it from .cache to .travis_cache,4
ExternalTaskSensor should log the execution_date it is poking for,5
Merge pull request #581 from mlimotte/masterExternalTaskSensor should log the execution_date it is poking for,5
Lowering parallelism on Travis cfg,5
Merge pull request #579 from bolkedebruin/masterFix travis caching,0
Lowering travis parallelism to 2,5
Fix blueprint confusion when creating the app multiple times through gunicorn,1
Merge pull request #583 from airbnb/fix_webFix blueprint/links issue when creating using multithreaded gunicorn,1
Trying to see whether the sequential executor works better,1
Merge pull request #584 from airbnb/sequentialTrying to see whether the sequential executor works better,1
HivePartitionSensor doc fix,0
delete xcoms,4
tweak Log model,2
Merge pull request #587 from jlowin/xcom_deleteConvenience function for deleting XComs,4
Remove extra parameter from xcom,2
Merge pull request #588 from jlowin/patch-4Remove extra parameter from xcom,2
add None task instance case to log class init,5
action logging func decorator,2
separate actions into dif endpoints and add logging decorator,2
update unit tests by removing /action endpoint,4
action logging anonymous user case,1
nits for new action logging,2
Bugfix when using -sd with fullpath,1
Revert previous commit,4
"Remove deprecated except foo, bar syntax from kerberos_login.",2
Remove duplicate definition of pool_link.,2
Replace incorrect raising of the constant NotImplemented with raising of the Exception NotImplementedError.,0
Merge pull request #594 from CloverHealth/py3_and_bugfixesBugfixes and Python3 compatibility,0
A few improvements to backfill,1
Allowing to specify pool in CLI,1
Merge pull request #597 from airbnb/improve_bfA few improvements to backfill,1
Adding pool to task_instance list view,1
Adding queued status to graph view legend,1
alembic migration for adding extra col to Log,2
add extra col to Log model,2
action logging adds request JSON and uses func name,1
add more action logging and use new format with no var,1
fix action logging,2
Merge pull request #590 from patrickleotardif/new_log_uiLog actions in Web UI (v2),2
Implement ldap authentication* This update configuration to use the module config instead of a local scope version* Removes the hack from default_login,2
Load authentication backend when requested,5
Remove hack for User,1
In tests reset settings. Force this also on initial tests (why does this remain?),3
Also for WebUiTests make sure authentication is set to falseDo test for authenticate when importing,2
Set authenticate to falseAdd ldap_auth first stepTry,1
Implement ldap authentication with tests,3
Add ldap travis tests,3
Make py3 compat,1
"Better 500 error handler, now with node name",0
"Merge pull request #603 from airbnb/better_500Better 500 error handler, now with node name",0
Adding a new HDFS hook using a Python 3 compatible package,1
Rename HDFSCLIHook to WebHDFSHook and add to setup.py,1
Add WebHdfsSensor,1
Fix names + typos and load_file args,2
Adding test for sensor operator based on WebHDFSHook,1
Update documentation with LDAP info. Move authenticaton from installation to security.,4
filter on uid instead of True,2
Add docs,2
Add default connection in airflow initdb,5
Add timeout to test,3
Fix typo in config,5
Adding stub of Kerberos support,1
Merge pull request #604 from airbnb/hdfscli_hookHdfscli hook,1
Merge remote-tracking branch 'upstream/master' into conf_multiple,5
Add convenience functions for configuration namespace,5
use from airflow import configuration everywhere,5
Make sure to use .conf when setting items for now.,1
Pass kwargs correctly,4
add_section is only part of configuration.conf,5
Merge pull request #602 from bolkedebruin/conf_multipleImplement ldap authentication including tests,3
Quick bugfix on login is None,2
Adding method has_option to ocnfiguration,1
"more flexible UT execution- we now can specify nose parameter on the command line of ./run_unit_test.sh.- if unspecified, the script falls back to previous behaviour, i.e. run all UTs",1
improved contrib doc regarding UT execution- doc now provides examples of how to selectively execute UT's,1
"Make Core UT no longer dependent on mysql- Core.test_check_operators was previously relying a mysql_default, which assumes a mysql instance running on the host executing the tests- This UT is now relying on a simply sqlite tmp schema with a table created just for the test .",3
add dag_run table,2
adapt scheduler to use DagRun table for triggering,2
Squashed some commitsadd CLI to insert new DagRunsfix typos in commentsadd missing importadd state to DagRunAdding cron support for schedule_intervalAllow null on chartsRefactoring like a mad manadd dag_run table,2
"SquashedFixing jobsLining up db revisionsAdding CRUD in the adminSuccess, backend running, next is UI changesUpdating the docs to reflect the new behaviorGot the UI to show externaly triggered runs, root object for DAG RunUI improvments, mostly functionalDAG concurrency limitCommit resets dag runsMore unit testsAdapting the UIFixing brutal amount of merge conflictsPolish around UI and eventsAdding DB migration scriptFixed the chartsAdding schedule info in the dag pageAdding cron presetsFixing up the testsAdding @once as a schedule_interval optionA layer of polish and bug fixes",0
Lining up alembic revisions,5
Fixing tests,3
Last minute tweaks,5
Limiting/parameterizing the max number of active runs per DAG,2
Merge pull request #540 from airbnb/external_triggersExternal triggers / cron support /,1
Cosmetic <nobr> on icon list in DAGs view,2
Tree view bugfix None in hidden form field,0
Bugfix around saving a chart,2
Fixing the fix,0
[tree view] Making the DagRun circles look better,1
Making sure period is closed before scheduling,1
Add csrf,1
"Use crsf token in forms, fix missing divUse lxml for parsing the form to obtain csrf for testing",3
Add csrf and fix tests,3
Optimization that gets a list of task instances to skip as a batch from the db,5
Merge pull request #611 from bolkedebruin/csrfActivate csrf token and validations,5
Merge pull request #612 from airbnb/scheduler_plus_plusScheduler tweaks,7
Documenting DAG param max_active_runs,1
fix missplaced comment in run_unit_tests.sh,3
Adding username to headers view,1
Getting current user through flask.ext path,1
Fixing current_user logging,2
Improved headers view,1
Disable auto-sorting on query view,5
Merge pull request #614 from patrickleotardif/patch-4Disable auto-sorting on query view,7
Add Sense360 to list of users,1
Merge pull request #615 from KamilMroczek/add_sense360_to_usersAdd Sense360 to list of users,1
Cosmetic fix for DAG header with ROOT filter,2
Applying preventative abs only to timedelta since relativedelta doesn't support it,1
adding the workerclass (-k) option when starting gunicorn for the webserver,1
fixing typo in DEFAULT_CONFIG for configuring the number of worker processes for gunicorn,1
add worker_class to the defaults and DEFAULT_CONFIG,5
adding the async subpackage that is required to run with gunicorn with an async worker class,1
updating subpackage documentation with celery and async,2
"Bugfix, running queued jobs outside of pools",1
"Improved scheduler logging, bugfix",0
Yet more logging improvements,1
Merge branch 'master' of github.com:airbnb/airflow into gunicorn_workers,1
Fixing the auth mechanism to be able to use the default boto infra,5
"Scheduler tweaks, logging improvement",1
SLA management edge case bugfix,0
limit the choices for the worker_class configuration,5
renaming threads to workers both in the cli and configuration,5
Merge branch 'master' of github.com:airbnb/airflow into gunicorn_workers,1
Merge pull request #618 from datamindedbe/gunicorn_workersSupport sync and async gunicorn workers,1
Merge pull request #608 from svendx4f/doc_and_UTsUT: documentation clarification and improvements,1
Merge pull request #617 from airbnb/default_to_boto_fix_s3_hookFixing the auth mechanism to be able to use the default boto infra,5
support multipart uploads,1
docstring,2
add filechunkio to optional requirements,1
Add min size to docs and fix raise,0
fix markdown,0
fixed typo in documentation + added .idea to gitignore,1
"remove .idea, add unittests.db to .gitignore",5
fix migration script to support sqlite workaround,1
fix query,0
Merge pull request #622 from RealImpactAnalytics/fix_migration_scriptFixes for SQLite support,1
Merge pull request #620 from RealImpactAnalytics/fix_documentationFix markdown in contributing.md,2
Handle 'None' Type on 'task_instance' for Log models,2
Adding no cover and a test for parse_s3_url,3
Merge pull request #623 from Yongyiw/masterHandle 'None' Type on 'task_instance' for Log models,2
Merge pull request #619 from airbnb/add_multipart_uploads_to_s3_hookAdd multipart uploads to s3 hook,1
Adding a warning when the max number of active DAG runs has been reached,1
Merge pull request #624 from airbnb/max_runs_flagAdding a warning when the max number of active DAG runs has been reached,1
v1.6.0,5
Port of kerberos_login to new backend structure. Some cosmetic fixes to ldap_auth,0
Merge remote-tracking branch 'upstream/master',7
Removing old structure,4
tests/core.py: Added Chartkick regexp test to WebUiTests.test_dag_views for duration and landing_times,3
airflow/www/views.py: json dump render data for duration and landing_times,5
Scheduler bugfix and docs tweaks,2
Extract request args in action logging,2
Change Log model to support more specific kwargs,1
remove truncation,1
Merge pull request #630 from patrickleotardif/patch-5Extract request args in action logging,2
Merge pull request #629 from storpipfugl/chartkick_json_guardChartkick json guard,5
Merge pull request #627 from bolkedebruin/masterPort kerberos authentication to new authentication structure,1
Removing done items from TODO.md,2
Adding a TriggerDagRunOperator,2
Increase version for alembic due tohttps://bitbucket.org/zzzeek/alembic/issues/333/batch-fails-on-tables-that-have-indexes,0
Merge pull request #634 from bolkedebruin/masterIncrease version for alembic due to bug in alembic < 0.8.3,0
Reintroducing import *,2
Merge pull request #635 from airbnb/dr_opAdding a TriggerDagRunOperator,2
v1.6.1,5
docs/scheduler.rst: Fix table formatting,0
Merge pull request #639 from jdanbrown/patch-1docs/scheduler.rst: Fix table formatting,0
Allow auto-commit option for Mysql Operator,1
"now running travis on all DB backends- added Travis config to also run integration tests on sqlite- added Travis config to also run integration test on postgre, and commented them out as they seem to fail- added a bit more logs in bash scripts executed by Travis",2
"fixes datetime issue when persisting logs- the current SQL INSERT statement contains a mix of datetime and string when inserting logs in DB, which seems to work fine on Mysql but fails on sqlite",0
Adding MetastorePartitionSensor to docs,2
Merge pull request #640 from SimpleHQ/allow_auto_commit_in_mysql_operator[MySqlOperator] Fix issue https://github.com/airbnb/airflow/issues/459 - mysql error 2014,0
Merge pull request #628 from RealImpactAnalytics/ria/run_travis_on_all_backends_/3Now executing Travis build also on sqlite,1
No need to expand if not bool(env_var),5
Fixing a format in the wrong place raise the wrong error,0
Merge pull request #645 from airbnb/fix_pool_raiseFixing a format in the wrong place raise the wrong error,0
Merge pull request #643 from airbnb/fix_expand_env_var_returning_none_as_stringNo need to expand if not bool(env_var),0
Fixing non NOSASL use of beeline,1
Take two on auth for beeline,5
[bugfix] @once never gets its first schedule,1
Hide password in connection's form dom,4
Fix postgres test by adhering to flask specs to return none in case userid not valid,1
convert to int before checking db,5
Check on 'None' and None,5
Merge pull request #646 from airbnb/hide_passHide password in connection's form dom,4
Add URI encoding to Web UI actions,1
Fix retry number accounting when queuing a retry,1
Merge pull request #650 from patrickleotardif/patch-6Add URI encoding to Web UI actions,1
Merge pull request #651 from airbnb/fix_retryaccounting_queueFix retry number accounting when queuing a retry,1
Add extra logging,2
Fix conn_type parsing from uris.,0
We need the conn_id if we are getting the connection via uri.,1
"Add support for MySQL SSL connections. Uses ssl dictionary in extras to take cert, ca, and key pem locations.",1
Merge pull request #655 from criccomini/masterAdd support for MySQL SSL connections. Uses ssl dictionary in extras …,1
Merge pull request #654 from CloverHealth/fix_uri_conn_typeFix conn_type parsing from uris.,0
Adding None reference to schedule_interval docs,2
Gix success endpoint for @once DAGs,2
Merge pull request #661 from patrickleotardif/fix_successFix success endpoint for @once DAGs,2
Adhere to flask_login specs and return the user_id with get_id,1
Limit build matrix for now to CDH only,5
Add templating support for data field in SimpleHttpOperator,1
Merge pull request #648 from bolkedebruin/fix_postgresFix postgres regression,0
Fixing the druid hook after the API changed slightly,4
Adding datetil to macros and documenting macros references,2
Monkey patching apply_defaults decorator for doc generation,2
Merge pull request #667 from airbnb/macro_dateutilAdding datetil to macros and documenting macros references,2
Merge pull request #663 from sergioherg/data_template_httpoperatorAdd templating support for data field in SimpleHttpOperator,1
Merge pull request #662 from airbnb/fix_docsFixing the function headers for decorated functions in the docs,2
Merge pull request #665 from airbnb/fix_druid_hookFixing the druid hook after the API changed slightly,4
Add tests around schedule_dag. Make run_unit_tests.sh idempotent. Fix abug with @once dags,2
Add Handy to list of users,1
Adding actions to set collections of dag runs to multiple status,1
Address review comments. Add -y option to resetdb.,5
Merge branch 'master' of https://github.com/airbnb/airflow into refactor-scheduler-1,4
Merge pull request #672 from airbnb/set_dagrun_stateAdding actions to set collections of dag runs to specific status,1
Add systemd unit files,2
Remove hack by only importing when configured,5
Create failing test for duplicate run_id marked externally triggered,1
Try import instead of having it required,1
log sections/keys that cannot be found,2
Fix the bug where an externally triggered dag run with the same run_idas a future scheduled run will prevent any scheduling progress due to anexception loop.,1
Merge pull request #636 from airbnb/hiveop_fixFixing non NOSASL use of beeline,1
Merge pull request #676 from mtustin-handy/refactor-scheduler-2Refactor scheduler 2,4
Adding test_mode boolean to context,3
Making sure test_mode always exists,3
Merge pull request #675 from bolkedebruin/better_loggingBetter logging,2
Adding entry in docs,2
Merge pull request #678 from airbnb/test_modeTest mode,3
Adding a dag details page,2
Adding unit test,3
Adding template file,2
Merge pull request #679 from airbnb/dag_detailsAdding a dag details page,2
Add contents and description,1
Clarification around depends_on_past and BranchOperator,1
Branching related additions to docs,2
notify owner when others perform critical DAG actions on Web UI,2
Update utils.py,5
use jinja2 templating for non owner email notif;,1
fix merge conflict for jinja templating,5
Only send email if task.email exists,5
Merge pull request #686 from patrickleotardif/non-owner-notifNotify owner when others perform critical DAG actions on Web UI,2
Better logging for the scheduler,2
adjustments to TODO,2
adding an extra setup command to clean builds,4
Merge pull request #688 from airbnb/arthurw_add_extra_clean_commandadding an extra setup command to clean builds,4
Bugfix - scheduler skipping pools,0
Always restart scheduler if it stops after 5 runs,1
Merge remote-tracking branch 'upstream/master' into systemd,5
Port of ssh wrapper from luigi,5
Setup ssh on travis to be able to test the ssh hook / operator,1
set default connection for ssh,1
Add test for ssh command,3
Small fixes,0
Add openssh server,1
Cleaner logging,2
Make sure to add the command,1
Better logging and testing,3
Add tunnel test,3
Add contextmanager,1
Clean up,4
Improve documentation,2
Added ssh_hook,1
Move block to class to make sure it shows up in the documentation,2
include the part about how to change the metadata database,5
Improvments to documentation,2
Removing forced resetdb from ./run_unit_tests,3
Merge pull request #699 from airbnb/docsImprovments to documentation,2
added a password column to the user table,1
created a password_auth mechanism,4
"update requirements, tests and all that jazz",3
update docs,2
better about closing session and teardown in tests,3
ldap auth mechanism wasnt cleaning up after itself,4
Fix documentation,2
python 3,5
Add documentation for systemd and some small additions for optional packages,1
Small fix,0
more python3. postgres was having a hard time,5
Merge pull request #700 from neovintage/password-authPassword auth,4
Add BigQuery hook.,1
Making sure subdagoperator bubbles up error from subdag,2
Cosmetics on tree view time axis,5
Merge pull request #703 from airbnb/fix_subdag_bubbleupMaking sure subdagoperator bubbles up error from subdag,2
Use pandas.io.gbq for BigQuery integration.,1
Make scheduler runs configurable add example environment file,2
Adding utility macros around ts,1
Merge pull request #705 from airbnb/tsAdding utility macros around ts,1
[bugfix] removing rogue dagbag parsing in module head,2
Merge pull request #707 from airbnb/bugfix_dagbagimport[bugfix] removing rogue dagbag parsing in module head,2
Merge remote-tracking branch 'upstream/master' into systemd,5
Adding task instance state change to view.,4
Merge branch 'master' into change-ti-status,4
Removing debug print.,0
added at least to show possible exception in searching for DAGs,2
Merge pull request #715 from sray-handy/change-ti-statusBulk edit Task Instance state in web UI,2
removed pass,4
Session factory,5
fix bytestring in xcom with python3,3
Fixing unicode handling for bashop,0
"fixed Variable json deserialization - in case value is missing, previous behaviour was trying to deserialize the default value from json - added a convenience setter:  Variable.set(key, value) - cf https://github.com/airbnb/airflow/issues/701 - 2 new UTs",1
fix merge issue,0
Skip tasks triggered by `all_success` if any upstream task is skippedhttps://github.com/airbnb/airflow/issues/718,0
fix failed UT (merge issue again),0
Merge pull request #724 from RealImpactAnalytics/svv_variablesfixed Variable json deserialization,5
"add Fernet key to test config- congiguration.py now generates the test config and real airflow config with the same method- fix warning logs in local UT execution, related to missing FERNET-KEY in unittest.cfg- fix warning logs in Travis UT exeuction, related to missing FERNET-KEY in airflow_travis.cfg- added one to validate config generation",5
fix condition in run_unit_tests.sh + fix indent + remove duplicated loaded config,5
Merge remote-tracking branch 'upstream/master' into ssh_ops,7
Merge pull request #728 from RealImpactAnalytics/svv_fernetFernet parameter missing in config + improving config creation,1
Merge pull request #674 from bolkedebruin/systemdAdd systemd unit files,2
Merge pull request #725 from lamdrew/PR-fix-718Skip tasks triggered by `all_success` if any upstream task is skipped,0
fix DagBag.get_dag() for non existing dag_id- calling .get_dag('non existing dag_id') was previously leading to a crash related to a call to None.fileloc- the rest of the existing logic is already returning None for other case of Dag not found => fixing the bug by aligning the logic on that,2
import tests from models.py,3
remove leading zeros for python3 compatibility,4
Merge pull request #697 from bolkedebruin/ssh_opsSsh Hook,1
Merge pull request #736 from RealImpactAnalytics/fix_dagbag_getdagFix DagBag.get_dag() for non existing dag_id,2
Fix AttributeError when starting schedulerThis PR fixes the exception`AttributeError: 'SchedulerJob' object has no attribute 'num_runs'`that is thrown when starting the scheduler without setting `-n`.,1
[docs] missing param in metastore sensor,2
"improved logs readability and config- passing --logging-level=DEBUG  at the command line is no longer ignored- classes can now implement WithLogger to get access to convenience logging function (e.g. self.log_info(""blah"")), producing log statement with a logger whose name is the name of the classe.g.:2015-12-07 17:22:43,084 - BackfillJob - INFO - [backfill progress] waiting: 0 | succeeded: 1 | kicked_off: 1 | failed: 0 | wont_run: 02015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - Calling the <class 'airflow.executors.sequential_executor.SequentialExecutor'> sync method2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 0 running task instances2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 0 in queue2015-12-07 17:22:43,084 - SequentialExecutor - DEBUG - 32 open slots2015-12-07 17:22:43,085 - BackfillJob - INFO - All done. Exiting.- configured this WithLogger super class in a couple of key classes to improve log readability",2
add 2 UT for log_to_stdout(),2
added one more UT for WIthLogger + a few more tested cases +  minor code cleanup,4
adapts name and behaviour of WithLogger- logger is now accessed as a property- logger is now memoized- WithLogger is renamed to LoggingMixin + moved to utils.py- mentioned the --logging-level=DEBUG option in CONTRIB.md,2
add UT for round_time()(in order to make coverage checks ok ^__^),3
adding one UT for log_to_stdout,2
fix bytestring in xcom (bis) and decode every output line,3
conflict one bytestring fix,0
Don't require unittests.cfg to avoid overwriting airflow.cfg,5
Merge pull request #723 from thibault-ketterer/masterfix bytestring in xcom with python3,3
"Make --num_runs not compulsoryThis was failing to set a num_runs variable at all. This broke other code that relied on it, effectively making num_runs compulsory",1
Merge pull request #740 from RealImpactAnalytics/svv_logging_v2improved logs readability and config,5
improve error message and order + added unit tests,3
Merge pull request #737 from RealImpactAnalytics/unit_tests_for_duplicate_dependencies_and_cyclesUnit tests for duplicate dependencies and cyclic dependencies,3
Merge pull request #742 from mtustin-handy/patch-2Don't require unittests.cfg to avoid overwriting airflow.cfg,5
Fix base_url in http hook to always start with http://,1
Merge pull request #747 from sergiohgz/http-hook-startwith-httpFix base_url in http hook to always start with http://,1
Merge branch 'master' into bigquery-hook,1
Adding dagrun timeout parameter to DAG,2
Adding a bit of unit test coverage,3
debugging PR,0
Omit contrib from coverage report,3
Merge pull request #751 from airbnb/coverOmit contrib from coverage report,3
Switch BigQuery hook to use BaseHook instead of DbApiHook,5
Try to fix a bunch of logging stuff.Includes debugging print statements,0
Merge pull request #750 from airbnb/dagrun_timeout_realAdding dagrun timeout parameter to DAG,2
Set log level from settings,1
"Give loggers a hierarchical name, allowing attaching intermediatehandlers",0
Merge pull request #752 from mtustin-handy/fix_loggingFix logging,2
PrestoToMySqlTransfer,5
Killing all zombie task instances,2
Merge pull request #735 from wndhydrnt/fix_attribute_error_schedulerFix AttributeError when starting scheduler,0
Merge pull request #712 from airbnb/kill_more_zombiesKill more zombies,7
Moving test to the right place,3
"Merge pull request #716 from ziky90/try-catch-fixadded log, to at least to show possible exception in searching for DAGs",2
Add link to airflow blog post from handy,2
Merge pull request #755 from mtustin-handy/patch-3Add link to airflow blog post from handy,2
SLA = Service Level Agreements,5
Move BigQuery hook to contrib.,1
Add BigQueryHook to code.rst,1
Merge branch 'master' into bigquery-hook,1
Added title attribute for logs,2
Bugfix and better logging of edge case,2
Mark tasks triggered by `all_success` as `upstream_failed` if any upstream task is `upstream_failed`,0
Merge pull request #765 from lamdrew/PR-fix-718Mark tasks triggered by `all_success` as `upstream_failed` ...,0
This seems to address logging #hack,2
"set default smtp_user, smtp_password so they are not needed in userconfig",5
"Merge pull request #773 from abridgett/feature/make_smtp_auth_optionalset default smtp_user, smtp_password so they are not needed in user config",5
Reducing sqla connection pool,5
Replace markdown links with rst links,2
Merge pull request #774 from mrorii/rst-linksReplace markdown links with rst links,2
[bashoperator] making env param a templated field,2
add docstring for attachments,2
[core] Closing db connection during execution,5
Merge pull request #776 from airbnb/bash_env[bashoperator] making env param a templated field,2
remove duplicated definition,5
add info on creating initial user in password_auth backend,4
Merge branch 'master' into password_auth_docs,4
Merge pull request #787 from neovintage/password_auth_docsPassword auth docs,2
Merge pull request #786 from abridgett/feature/remove_duplicate_macroremove duplicated definition,5
Disposing of the sqla engine during task execution,5
Merge pull request #782 from airbnb/close_conn[core] Closing db connection during execution,5
Merge pull request #781 from abridgett/feature/add_attachments_support_to_slack_operatoradd attachments support to slack operator,1
doc: note on skipping a branch,2
Add BigQuery hook.,1
Use pandas.io.gbq for BigQuery integration.,1
Switch BigQuery hook to use BaseHook instead of DbApiHook,5
Move BigQuery hook to contrib.,1
Add BigQueryHook to code.rst,1
"slack_operator was using ""params"" which prevented user-defined paramsmacro from working",1
correct call to set_dependency,1
Merge pull request #794 from abridgett/fix/correct_set_dependency_callcorrect call to set_dependency,1
"Merge pull request #793 from abridgett/fix/slack_operator_uses_paramsslack_operator was using ""params"" which prevented user-defined params macro from working",1
add error handling for slack api,0
log then throw the exception,2
Remove search scope from LDAP query to make it work with ActiveDirectory.,1
Add support for super users and data profilers into LDAP.,2
Ignore config exceptions if LDAP superuser/data profiler are undefined.,2
Implemented GHE authentication,5
allow slack attachments to be templated,1
Add copyright,1
add FreshBooks to list of user,1
Merge pull request #803 from DinoCow/FreshBooks-As-Useradd FreshBooks to list of users,1
Bellhops uses airflow,1
Merge pull request #804 from adamhaney/patch-1Bellhops uses airflow,1
full_filepath handing fix,0
Adding Qubole Operator,1
Merge remote-tracking branch 'upstream/master',7
check trigger_rule syntax,5
new test: bad trigger_rule,3
add Holimetrix in user list,1
using xcom to interchange qubole command ids,4
some minor fixes,0
github name not twitter :),5
Merge pull request #807 from thibault-ketterer/holimetrixadd Holimetrix in user list,1
triggerrule classmethod for listings and validating options,5
Stop using force=True for pooled tasks. This prevents re-running ofalready successful tasks.,1
Merge pull request #810 from aminghadersohi/masterDon't force run pool tasks.,1
Addressing issues around try_number being off,1
Trying to pin flask-admin lib to fix build,0
Merge pull request #806 from thibault-ketterer/feature/verify_trigger_ruleFeature/verify trigger rule,7
Fixing datetime lib references,5
add missing states to graph and tree legends,1
Add ING to list of users,1
"harmonise tooltip popups in graph,tree views",5
add retry to graph and tree legends,1
Merge pull request #819 from bolkedebruin/masterAdd ING to list of users,1
Merge pull request #820 from abridgett/feature/ui_improvementsFeature/ui improvements,1
Merge pull request #814 from airbnb/try_numberAddressing issues around try_number being off,1
Refactoring states inferred from trigger rules,5
Merge pull request #790 from thibault-ketterer/masterdoc: note on skipping a branch,2
Merge pull request #821 from airbnb/infered_trRefactoring states inferred from trigger rules,5
Merge pull request #702 from criccomini/bigquery-hookAdd BigQuery hook.,1
More conservative connection pooling,5
v1.6.2,5
[hotfix] gunicorn version 19.4.? has issues serving static files,2
Fixing bad ONE_FAILED in recent PR,0
Merge pull request #823 from airbnb/fix_infer_trFixing bad ONE_FAILED in recent PR,0
Upgrading from flask-admin 1.2 to 1.4,5
Adding missing file,2
Moving operator logic to its hook,1
Fix ISSUE-812 to not allow the scheduler to start if the backend is sqlite and the executor is not SequentialExecutor,1
Set default executor to Sequential executor (as per out of the box experience),1
Merge pull request #824 from airbnb/upgrade_flask_adminUpgrade flask admin,7
[hotfix] making modal window accessible,1
[hotfix] More js flask-admin related fixes,0
Merge pull request #822 from airbnb/airbnb_prodMerging prod hotfixes into master,0
color alternate rows so it's easier to use,1
print help when using python3 rather than error,0
add missing help text (currently it reprints an earlier text),1
Fix invalid syntax in SSHHook,1
Add Sidecar Interactive to list of companies using Airflow,1
Merge pull request #828 from robottokauf3/fix_ssh_hookFix invalid syntax in SSHHook,1
order dag run drop down in graph view,4
Merge pull request #831 from abridgett/feature/order_graph_dag_runsorder dag run drop down in graph view,4
use more subdued colors,1
Merge pull request #802 from mtp401/github_enterprise_authImplemented GitHub Enterprise Authentication,7
Merge pull request #825 from abridgett/feature/gantt_color_alternate_rowscolor alternate rows so it's easier to use,1
Merge pull request #830 from robottokauf3/sidecar_uses_airflowAdd Sidecar Interactive to list of companies using Airflow,1
Updating TODO.md,2
add missing spaces,1
Merge pull request #826 from abridgett/feature/cli_fixesFeature/cli fixes,0
creating run_id if not given while executing trigger_dag,2
Adding visibility as to which dag is pickleable,2
Merge pull request #839 from airbnb/is_picklableAdding visibility as to which dag is pickleable,2
Only pickle_info for non subdags,2
Fixing SLA handling related bug,0
Add Gitter badge,1
Adding link to Gitter channel,2
using execution_date to set run_id,1
Merge pull request #841 from gitter-badger/gitter-badgeAdd a Gitter chat badge to README.md,2
Merge pull request #840 from airbnb/sla_fixFixing SLA handling related bug,0
Merge pull request #817 from bolkedebruin/ISSUE-812Fix ISSUE-812,0
Merge pull request #763 from JordyMoos/logs-title-attributeAdded title attribute for logs,2
Keeping reqs.txt version free,5
Update models.py to increase password field lengthIncrease length to allow for RSA keys and such.,1
Merge pull request #850 from rahul342/patch-1Update models.py to increase password field length,4
add a parameter for number of shard in batch ingestion,2
Deleting expired dags that have been removed,4
add missing commaJ,1
Merge pull request #851 from dayzzz/druid_shardsadd a parameter for number of shard in batch ingestion,2
Merge pull request #848 from airbnb/del_dagsDelete dags from dagbag,2
Fixing a bug where some dags can't be retrived from DagBag.get_dag,2
Merge pull request #853 from airbnb/hotfix_getdagFixing a bug where some dags can't be retrived from DagBag.get_dag,2
Cosmetics - More density in the legend area,5
[hotfix] subdag not showing up,2
Merge pull request #854 from airbnb/airbnb_prod[hotfix] subdag not showing up,2
"Base date and run number for duration/landingThis adds the possibility to specify the base date and the number of runs displayed, for the Task duration tab as well as the Landing Times.",1
Merge branch 'master' into bigquery-operator,1
Add BigQueryHook,1
Add comments,1
Add BigQueryOperator to code.rst,1
Merge pull request #779 from RealImpactAnalytics/BIG-2210_Base_date_in_task_duration_viewAdding base date and run number form to Task Duration and Landing Times views,1
Merge pull request #861 from criccomini/bigquery-operatorBigquery operator,1
handle default option for extra_options argument in HttpHook.run method,1
Add BigQuery to Google Cloud Storage operator.,1
More documentation for BigQuery to GCS operator.,1
Add BigQuery to GCS operator to code.rst docs.,2
Add Google Cloud Storage hook and download operator.,1
Update write disposition to work properly for BigQuery hook.,1
Add code.rst docs for GCS hook and operator.,1
Merge pull request #865 from criccomini/bigquery-operatorBigquery and Google Cloud Storage operators,1
[hotfix] fixing subdag not refreshing properly,2
Merge pull request #866 from airbnb/subdag_hotfix[hotfix] fixing subdag not refreshing properly,2
Merge pull request #867 from airbnb/airbnb_prodAirbnb prod,7
Encrypt logs,2
SLA Miss Alert Callbacks : Allow DAGs to specify a callback function that can be executed during SLA misses. One use-case for this is to allow 3rd party notification on SLA misses such as PagerDuty and VictorOps,1
logging from workers fix,0
Merge pull request #835 from msumit/I832Issue 832: creating run_id if not given while executing trigger_dag,2
SLA Alert Callback : Supporting the ability to do optional SLA alert call backs and emailing,1
Merge remote-tracking branch 'upstream/master',7
full_filepath handing for dags outside dags folder,2
merging with master,5
hoping it'll resolve conflicts,5
state wasn't being saved,5
Merge branch 'master' of https://github.com/airbnb/airflow,7
[readme] add Max's november conf ETL tips & tricks,5
[readme] rename link,2
Merge pull request #879 from thibault-ketterer/readme_updateReadme update,5
Merge pull request #878 from abridgett/feature/fix_session_statestate wasn't being saved,3
"The code was taken from: https://github.com/wndhydrnt/airflow/tree/docker_operatorCredit to: @wndhydrntThis branch, came to solve the CI problems.",0
"The code was taken from: https://github.com/wndhydrnt/airflow/tree/docker_operatorCredit to: @wndhydrntThis branch, came to solve the CI problems.",0
"The code was taken from: https://github.com/wndhydrnt/airflow/tree/docker_operatorCredit to: @wndhydrntThis branch, came to solve the CI problems.",0
support templates,1
docker_operator- Support multiple commands for entrypoint.It might be useful for sealed containers,1
support xcom,1
docker operator - example,1
docker operator - adding code example,1
docker operator - example,1
Fixing tests,3
Fixing issue where try_number isn't incremented,1
Merge pull request #884 from airbnb/retry_incFixing issue where try_number isn't incremented,1
typos and xcom changes,4
merging with master,5
Merge pull request #876 from iivvaall/airbnb_prodAirbnb prod,7
Poison pill for undeads,5
Pythoneskifying,5
Merge pull request #808 from msumit/masterAdding support for QDS (Qubole Date Services),5
Merge pull request #888 from airbnb/airbnb_prodAirbnb prod,7
Poison pill for undeads,5
Merge pull request #887 from airbnb/undead_suicideKilling tasks that aren't in a running state,1
Merge pull request #889 from airbnb/airbnb_prodMerging airbnb_prod hotfixes into master,0
Fix ISSUE-798 by using sqlalchemy engine.url,1
"Merge pull request #893 from DinoCow/remove-password-from-printFix ISSUE 798: password printed to STDOUT when running initdb, resetdb, upgradedb",5
[hotfix] fixing infinite retries in prod,5
Merge pull request #894 from airbnb/retry_2[hotfix] fixing infinite retries in prod,5
Cosmetics - More density in the legend area,5
[hotfix] subdag not showing up,2
[hotfix] fixing subdag not refreshing properly,2
Merge pull request #896 from airbnb/airbnb_prod_newCherry picked commits out of recent rollback,1
"Making force a task instance member, so it becomes available for operators in runtime.",1
"Merge pull request #898 from iddoav/making_force_ti_memberMaking force a task instance member, so it becomes available for",1
"Make Google Cloud Storage download operator use a filename, not a file descriptor.",2
Fixing the tutorial. Removing an unnecessary import of MySqlOperator and adding the import of timedelta.,2
Merge pull request #797 from criccomini/add-super-user-and-profiler-to-ldapAdd super user and profiler to ldap,2
Merge pull request #903 from cesararevalo/fix_tutorialFixing the tutorial. Removing an unnecessary import of MySqlOperator …,1
"Add ""search_scope"" as a configuration variable for LDAP (#796)This is the correct solution to #796 -- instead of completely droppingthe variable all together.Added a bit of ""pretty"" failure for this error as well--includingspecifying what is happening in the webserver log, and how it can befixed.",0
"Fix CI Test -- Accidentally quoted ""LEVEL"" instead of it beingldap3.LEVEL",3
"Resolve issue with not correctly loading config data for search_scopesearch_scope defaults to LEVEL, unless specified as SUBTREE inAIRFLOW_CONF",5
"Support for all search_scope options, as per the ldap3 spec.Updated documentation to reflect all options + link to docs.",2
"Add ""search_scope"" as a configuration variable for LDAP (#796)This is the correct solution to #796 -- instead of completely droppingthe variable all together.Added a bit of ""pretty"" failure for this error as well--includingspecifying what is happening in the webserver log, and how it can befixed.search_scope defaults to LEVEL, unless specified as SUBTREE inAIRFLOW_CONF",5
Bugfix. untangle mixed meaning of self.dag_idI rename the TriggerDagRunOperator attribute dag_id to be trigger_dag_id.   The original attribute name confused the dag to drigger with the dag to which this operator belongs.  The result was that the TaskInstances were stored in the DB with the incorrect dag_id.,2
Update docstring for changeRename dag_id param to trigger_dag_id.,2
Update unittest for TriggerDagRun after repair.,2
Add SimilarWeb to the who uses airflow section,1
Merge pull request #905 from similarweb/add_similarweb_to_readmeAdd SimilarWeb to the who uses airflow section,1
Merge pull request #904 from t1m0thy/patch-1Bugix for TriggerDagRunOperator,2
Update some datetime column default args for consistent treatment across all models,5
Merge pull request #906 from t1m0thy/masterUpdate some datetime column default args for consistent treatment,5
"Call `Session.remove` after each run, to survive DB restartWithout this, we get the following exception for subsequent runs after aDB restart, indefinitely:    sqlalchemy.exc.InvalidRequestError: This Session's transaction has    been rolled back due to a previous exception during flush. To begin    a new transaction with this Session, first issue Session.rollback().    Original exception was: (psycopg2.OperationalError) terminating    connection due to administrator command",5
"Merge pull request #907 from KMK-ONLINE/dispose-sessionCall `Session.remove` after each run, to survive DB restart",5
Added a link to puppet-airflow module,2
Now with a link to puppet forge,2
Fixes typo,2
Merge pull request #909 from rosner/patch-1Fixes typo,2
Merge pull request #910 from similarweb/add_similarweb_to_readmeAdd similarweb airflow puppet module to the readme,1
[bugfix] Missing a plus sign,0
[bugfix] try numbers,1
[bugfix] retry msg,1
"Merge pull request #813 from NeilHanlon/add_searchscope_configAdd ""search_scope"" as a configuration variable for LDAP",5
Adding try_number to Task Instance CRUD list view,1
Add Clover Health to Airflow users,1
Merge pull request #912 from vansivallab/patch-1Add Clover Health to Airflow users,1
fix try_number merge conflict,5
"Merge pull request #901 from criccomini/bigquery-operatorMake Google Cloud Storage download operator use a filename, not a fil…",2
Allowing for relative path and dot notation for -sd,1
Merge pull request #917 from airbnb/fix_dotAllowing for relative path and dot notation for -sd,0
Reverting production issues from 876 and undead,0
Add support for three-legged OAuth for Google connections. Useful for developer-mode.,1
Merge pull request #875 from r39132/masterSLA Miss Alert Callbacks : Allow DAGs to specify a callback function for SLA miss handling,1
Add docs to Google cloud base hook.,1
Add link to SmartNews in README,1
Merge pull request #922 from takus/smartnewsAdd link to SmartNews in README,1
added check for unicode type before decoding to fix a decoding bug,0
added check for unicode type before decoding to fix a decoding bug,0
Normalize plugin paths that include both slashes and dots,2
[fix] disregarding adhoc tasks when closing dag runs,1
Merge pull request #918 from airbnb/revert_876Reverting production issues from 876 and undead,0
Merge pull request #927 from airbnb/fix_adhoc_dr[fix] disregarding adhoc tasks when closing dag runs,1
Merge pull request #868 from CloverHealth/encrypt_logsEncrypt logs,2
fix funcname name error: active_tasks,0
Merge pull request #925 from kmevissen/masterSmall change to fix a crash when reading log files from remote workers,1
Merge pull request #882 from asnir/docker_operatorDocker operator,1
Merge pull request #928 from flying5/masterfix function name error: active_tasks,0
get passphrase during run time,1
add fallback support for all configs,5
Support for encrypting the connection extra field,1
Merge pull request #934 from r39132/encrypt_conn_extraSupport for encrypting the connection extra field,1
Commenting out content of example_docker_operator.py,2
Decreasing NUM_EXAMPLE_DAGS = 7 after commenting the docker ex,2
Merge pull request #926 from blueapron/fix/plugins-manager-initNormalize plugin paths that include both slashes and dots,5
Fixing tests after commenting out docker example,2
Define elements that can be fetched using the command.Document precedence rule.Tweak test to check other elements,3
Generalizing the statsd call,5
Mimic the statsd headers,5
[hotfix] dag missing from dagbag,2
Merge pull request #743 from praveev/run-time-acquisitionGet connection string containing username and password during runtime,1
Merge pull request #937 from airbnb/statsdStatsd abstraction,7
Merge pull request #919 from criccomini/google-cloud-three-legged-oauth-supportAdd support for three-legged OAuth for Google connections. Useful for…,1
Fix and refactor the httphook,1
Merge pull request #935 from airbnb/fix_httphookFix and refactor the httphook,1
Merge pull request #862 from JoergRittinger/masterhandle default option for extra_options argument in HttpHook.run method,1
Merge pull request #938 from airbnb/airbnb_prod[hotfix] dag missing from dagbag,2
Merge pull request #801 from abridgett/feature/allow_slack_attachments_to_be_templatedallow slack attachments to be templated,1
Merge pull request #224 from airbnb/presto_mysqlPrestoToMySqlTransfer,7
Show rendered templates from the CLI,5
Merge pull request #939 from airbnb/templatesShow rendered templates from the CLI,7
Refresh bordering coloring in graph view without refreshing entire page POC,5
Fix process_subdir bug,0
Merge pull request #941 from Attsun1031/fix-process-subdir-bugFix process_subdir bug,0
implement SSHExecuteOperator,1
Fix running custom pool task with mark-success is not marked success,1
Merge pull request #816 from KeenS/ssh_operatorImplement SSH Execute Operator,1
Merge pull request #943 from Attsun1031/fix-pooled-task-with-marksuccess-bugFix running custom pool task with mark-success is not marked success,1
Fixed some minor typos,2
Fixed Azkaban link,2
Slight editing of the installation documentation,2
Fixed typo in plugins.rst,2
Fix statuses in dag list for dags with dag_ids prefixed by a number,0
Add documentation to gcs_download_operator,1
Add dag_state to cli- Allow pausing a dag from the cli,2
Encrypt Variables if Fernet key provided,1
Add missing migration needed to support Variable value encryption,1
"Use the connection login username as hdfs proxy user for HDFSHook and WebHDFSHook, optionally allow to override it in the constructor.Add simple init test for WebHDFS.",3
Merge pull request #947 from r39132/masterEncrypt Variables if Fernet key provided,1
Merge pull request #945 from criccomini/masterAdd documentation to gcs_download_operator,1
"Fix encryption alert msg for env var, configuration->conf refactor",4
Add pause and unpause subcommands to cli,1
Allow for domain-wide delegation in google cloud apps.,1
"change name of sub to delegate_to, fix and add documentation.",2
"Support new configuration dags_are_paused_at_creation that, when True, will cause a new DAG to be created and loaded in a paused state. The default value is False, in line with current expected behavior",1
"Merge pull request #956 from r39132/pause_dag_at_creationSupport new configuration dags_are_paused_at_creation that, when True…",2
Merge pull request #946 from wil5for/dag_state_for_cliAdd dag_state to cli,2
Fix bug in reporting of attempt number when queuing tasks in a pool,0
Fixing the messaging around number of attempts in the case of retries against pool-queued tasks,0
Merge pull request #958 from r39132/masterFix bug in reporting of attempt number when queuing tasks in a pool,0
Merge pull request #949 from mtagle/allow_domain_delegationAllow for domain-wide delegation in google cloud apps,1
Fix parsing file that contains multi byte char,2
add unit test for refresh endpoint,3
change button to a refresh icon and moved it onto svg,4
fix WebUiTests for SequentialExecutor,3
teardown tests by clearing dags,2
tooltip on tasks now show up after button refresh.,5
Only schedule DagRuns between start and end dates,5
Merge pull request #960 from Attsun1031/fix-721-unicode-decode-errorFix parsing file that contains multi byte char,2
Allow a Dag to have no end_date specified,5
"Merge pull request #948 from airbnb/fernetFix encryption alert msg for env var, configuration->conf refactor",4
Adding mock lib to devel extras_require,1
Merge pull request #963 from nicktrav/nickt/fix-scheduling-between-datesOnly schedule DagRuns between start and end dates,5
Getting Gitter badge to line up in README,1
Merge pull request #965 from airbnb/add_mockAdding mock lib to devel extras_require,1
"Merge pull request #942 from xadhoom/hdfs_proxy_userSet effective user of (web)hdfs hooks using connection config, optionally overridden by constructor",5
"When either data_profiler_filter or superuser_filter aren't defined, don't crash, and set user to have data_profiler or superuser access.",1
Merge pull request #944 from wil5for/dag_names_w_numbers_ui_fixFix statuses in dag list for dags with dag_ids prefixed by a number,0
More verbose logging for are_dependencies_met when called from run,1
Merge pull request #976 from airbnb/verbose_depsMore verbose logging for are_dependencies_met when called from run,1
Make BigQuery hook support PEP 249.,1
Fixing conflicting params in default_args,2
Add a BigQueryCheckOperatorAdd types to BigQueryCursor.Add pydoc notesAdd value and interval checks for BigQuery,1
Add docs for BigQuery hook,1
Re-add Pandas support to BigQuery.,1
Add docs to BigQuery check operators,1
Support user-defined macros and params in dry-run backfills with task regex,1
Merge pull request #980 from r39132/operator_params_and_macros_in_dry_runSupport user-defined macros and params in dry-run backfills with task…,1
"Add tests to:1. make sure that if no superuser or dataprofiler filter is set, all logged in users get superuser or dataprofiler privileges.2. make sure that, if filters are set, users get acess when the filters allow it.change ldif loading so that it always loads in a reasonable order.",4
Merge pull request #981 from criccomini/add-bq-pep-supportAdd BigQuery PEP 249 support,1
Clean test database out in between unit test runs,1
Added destination_dataset_table to template_fieds of bigquery_operator,1
Removing log statements used for debugging.,0
Add tests for params handling in Dag construction,2
[airflow][presto] Gracefully handle 503 errors and avoid eval(),0
Merge pull request #983 from 0xR/masterAdded destination_dataset_table to template_fieds of bigquery_operator,1
[documentation] document that max_active_runs can prevent a DAG from running,1
Merge pull request #986 from airbnb/ddavydov/document_max_active_runs[documentation] document that max_active_runs can prevent a DAG from running,1
Merge pull request #984 from nicktrav/nickt/dag-unit-testsAdd tests for params handling in Dag construction,2
Add logout button to Airflow,2
Merge pull request #978 from airbnb/fix_paramsFixing conflicting params in default_args,2
error handling in dag refresh and spinner during refresh,2
Rename xcom_push -> xcom_push_flag to avoid collision with parent class BaseOperatorhttps://github.com/airbnb/airflow/issues/991,0
Merge branch 'master' of github.com:seregasheypak/airflow,7
Added yesterday_ds_nodash and tomorrow_ds_nodash,1
Updated documentation,2
Merge pull request #992 from seregasheypak/masterRename BashOperator.xcom_push member to avoid collision with parent class,1
Merge pull request #993 from 0xR/add-yesterday_ds_nodashAdded yesterday_ds_nodash and tommorow_ds_nodash,1
Use glyph icon for logout button.,2
"there could be dot in the key string, which is illegal in the hive table name",2
Fix LDAP error messages when login fails.,0
"Merge pull request #994 from dayzzz/update_namingthere could be dot in the key string, which is illegal in the hive ta…",5
"Merge pull request #930 from mtagle/no_ldap_filter_fixWhen either data_profiler_filter or superuser_filter aren't defined,…",1
Merge pull request #987 from criccomini/add-logout-buttonAdd logout button to Airflow,2
Merge pull request #940 from DinoCow/Refresh-Dag-StatusRefresh border coloring in graph view without having to refresh the entire page,2
Added the destionation filename to template_fields GoogleCloudStorageDownloadOperator,1
Slight rewording in the CONTRIBUTING.md file,2
Merge pull request #998 from joshmarlow/docs-tweakPedantic documentation tweaks,2
Fixing a few doc building warning,2
Update to the README such as adding Hootsuite,1
Merge pull request #1003 from r39132/masterUpdate to the README such as adding Hootsuite,1
Fix forgetting to expose yesterday_ds_nodash and tomorrow_ds_nodash,1
Merge pull request #1006 from 0xR/fix-nodash-not-exposedFix forgetting to expose yesterday_ds_nodash and tomorrow_ds_nodash,1
Merge pull request #995 from criccomini/better-error-on-login-failureFix LDAP error messages when login fails.,0
Add FTPSHook class.,1
[airflow][presto] Keep lines shorter than 80 chars,5
drop the tmp table after ingestion,4
Merge pull request #1011 from dayzzz/drop_temp_tabledrop the tmp table after ingestion,4
Fixed issue 1012: pool not used with celery executor,1
Merge pull request #1013 from rdavison/masterFixed issue 1012: pool not used with celery executor,1
Merge pull request #985 from airbnb/shahaf-presto-error[airflow][presto] Gracefully handle 503 errors and avoid eval(),0
add SSL support for SMTP,1
Add direct dependencies for Google cloud contribsSwitch freeze version to use setup.py like other requirements do.,1
Merge pull request #1019 from criccomini/fix-google-requirementsAdd direct dependencies for Google cloud contribs,5
Merge pull request #964 from nicktrav/nickt/unit-testsClean test database out in between unit test runs,1
Add BigQuery copy operator.,1
Merge pull request #1020 from criccomini/add-bq-copy-operatorAdd BigQuery copy operator.,1
Merge pull request #1018 from bbrumi/support_smtp_ssladd SSL support for SMTP,1
Add notes on connection password encryption,4
Merge pull request #1025 from d-lee/masterAdd notes on connection password encryption,4
Minor documentation tweaks to the FAQ under the fernet key section,2
Merge pull request #1026 from r39132/masterMinor documentation tweaks to the FAQ under the fernet key section,2
Add FTPSHook in _hooks register.,1
Only set headers and delimiters for CSV exports in Google BigQuery hook,1
Merge pull request #1039 from criccomini/set-headers-only-for-csvOnly set headers and delimiters for CSV exports in Google BigQuery hook,1
set celery_executor to use queue name as exchange,4
Adding an example to illustrate the TriggerDagRunOperator,2
Updating a test and fixing a bug,0
Fixing prints for Python3,0
Removing tabs,4
Merge pull request #1043 from r39132/masterAdding an example to illustrate the TriggerDagRunOperator,2
Updating the Readme with a link to the TriggerDagRunOperator post,2
Merge pull request #1044 from r39132/masterUpdating the Readme with a link to the TriggerDagRunOperator post,2
Documenting the cluster policy feature,2
Clarified installation docs around worker reqs,1
Merge pull request #1046 from airbnb/policyDocumenting the cluster policy feature,2
Improving the TriggerDagRunOperator example,2
Quickfix for VerticaHook with no password,4
Allow the use of the autoconfig client and allow the use of the effective user,1
Merge remote-tracking branch 'upstream/master' into hdfs_hook,1
Fixing 'signal only works in main thread' error with timeouts,0
add lendup,1
add lendup,1
Add Kogan.com to the list of users,1
Merge pull request #1053 from geeknam/readme_usersAdd Kogan.com to the list of users,1
Adding LendUp to company list,1
CLI's trigger_dag now accepts --conf as json,5
improve chart performance,2
revert local master commit,4
Merge remote-tracking branch 'upstream/master' into chart,2
Merge pull request #1054 from airbnb/trigger_dag_confCLI's trigger_dag now accepts --conf as json,5
Merge pull request #1055 from lumengxi/chartImprove [Hide/Show all series] perforamce,2
Merge pull request #1008 from geeknam/feature/ftps_hookAdd FTPSHook class.,1
Running unit tests with local executor,3
Merge pull request #1051 from airbnb/test_with_localexRunning unit tests with local executor,3
Merge pull request #996 from 0xR/GoogleCloudStorageDownloadOperator-template-dst-filenameAdded the dst filename to template_fields GoogleCloudStorageDownloadOperator,1
Add output encoding option to BashOperator,1
Merge pull request #1056 from Attsun1031/fix-bash-op-unicode-decode-errAdd output encoding option to BashOperator,1
Use json boolean and fix redundancy,0
Merge pull request #1049 from bolkedebruin/hdfs_hookAllow the use of the autoconfig client,5
Documenting task details doc_ feature,2
Merge pull request #1063 from airbnb/task_docDocumenting task details doc_ feature,2
Add Thumbtack to organizations using Airflow,1
dag pausing should pause queued tasks as well,1
Clarifying if logic,2
Merge pull request #1064 from natekupp/patch-2Add Thumbtack to organizations using Airflow,1
"Add two methods to BigQueryBaseCursor:get_method, which allows access to the schema of a given BigQuery table.get_tabledata, which allows access to all the data in a given table.",5
Merge pull request #1067 from mtagle/bq_schemaAdd two methods to BigQueryBaseCursor:,1
"Add a new hook for google datastoreAlso included, adding a new connection type for google datastore,and slightly correcting the gc base hook to reflect that SignedJwtAssertionCredentials initializer accepts a string or an iterable of strings for scope(s).",5
change local dataset id var name from datasetId to dataset_id,5
Merge pull request #1065 from airbnb/pause_queuedag pausing should pause queued tasks as well,1
Merge pull request #1068 from mtagle/datastore_hookAdd a new hook for google datastore,5
Proper sqlalchemy syntax for desc,5
fixing small issue with qbol operator and hook,1
Merge pull request #1073 from msumit/qbol_op_fixFixing small issue with qbol operator and hook,1
Pig hook and operator stub,1
SID Oracle DB connection support,1
Cleanup Contributing.mdHyperlinked Alembic reference. Also cleaned up whitespace and wrapped lines at ~80 characters since it's a Python project.,4
Add MySQL to Google cloud storage operator,1
Add Google cloud storage to BigQuery operatorRemeber to do docsAdd a max_key to gcs2gqAdd allow_large_results for BigQuery. Log max ID.Add documentationFix logging line,2
Replace deprecated flask.ext.* with flask_*https://github.com/mitsuhiko/flask/issues/1135,0
"Merge pull request #1080 from criccomini/mysql-to-gcsAdd MySQL->GCS, GCS->BQ operators",1
adding template support in qbol operator,1
Default to 0 if no rows loaded in GCS to BQ operator.,1
Merge pull request #1086 from criccomini/fix-empty-loadsDefault to 0 if no rows loaded in GCS to BQ operator.,1
Merge pull request #1082 from jeffwidman/masterCleanup Contributing.md,4
Merge pull request #1084 from jeffwidman/fix-flask-extReplace deprecated flask.ext.* with flask_*,0
Fix MySQL to Google cloud storage scoping.,0
Make approx_max_file_size_bytes Python3 compatible. Was getting invalid syntax.,1
SID Oracle DB connection support (indent fix),0
re-initiating hook in execute,1
Merge pull request #1075 from airbnb/pig_operatorPig hook and operator stub,1
Merge pull request #1088 from criccomini/fix-empty-loadsFix MySQL to Google cloud storage scoping.,5
Add MySQL to BQ support for TINYINT,1
Merge pull request #1093 from criccomini/fix-empty-loadsAdd MySQL to BQ support for TINYINT,1
Adding to inits,5
Merge pull request #1094 from airbnb/pig_operatorAdding to inits,5
Merge pull request #1079 from biln/masterSID Oracle DB connection support,1
Merge pull request #1070 from airbnb/sqla_descProper sqlalchemy syntax for desc,7
Make SqlAlchemy pool_recycle and pool_size configurable,5
Merge pull request #1096 from amread/pool-recycle-configMake SqlAlchemy pool_recycle and pool_size configurable,5
Adding @mention in Airbnb and links to airbnb open source projects,2
Merge pull request #1099 from airbnb/adding_at_mention_and_linkAdding @mention in Airbnb and links to airbnb open source projects,2
"modify datastore hook so that authorization is maintained for the lifetime of the hook, rather than re-authorizing for each request.",1
Add SubDagOperator example,2
Add SubDAG concept documentation,2
Merge pull request #1101 from mtagle/modify_datastore_hookmodify datastore hook so that authorization is maintained for the lif…,1
Merge pull request #1076 from nicktrav/nickt/subdag-docsSubDAG docs and examples,2
pep8 change,4
Add INT24 (MEDIUMINT) support to MySQL to Google cloud storage operator,1
Reverting to SequentialExecutor for unitests,3
new company + link to pitfallsallegro added as a company using airflow + link to common pitfalls,2
Merge pull request #1104 from criccomini/fix-empty-loadsAdd INT24 (MEDIUMINT) support to MySQL to Google cloud storage operator,1
Merge pull request #1108 from kretes/patch-1new company + link to pitfalls,2
Add date support to MySQL to GCS operator,1
Always allow nulls for MySQLdb to GCS fields,5
Merge pull request #1090 from msumit/qbol_templateAdding template support in qbol operator,1
Adding landscape.io code health badge,1
This patch adds license checking for Airflow. For now it will store a numberin Travis' cache to make sure current builds do not fail but newly addedfiles should have a license header included.,2
move oauth2client<2 and httplib2 out of requirements,1
add gcp extras,1
add google-api-python-client to extras,1
add GCSHook,1
remove google-api-python-client from requirements,1
rename gcp -> gcloud,5
readme to explain two GCP packages,5
Merge branch 'gcp_api' into gcp,7
Merge pull request #1118 from jlowin/gcp_apiDon't force installation of GCP API client dependencies,1
support P12 credentials; compatibility with existing GCS connections,1
add compatibility methods for GoogleCloudStorageHook,1
store connection rather than call every time,5
"support for scopes, also pass arguments at initialization",5
signature compatibility with GoogleCloudStorageHook,1
create base_hook for future GCP hooks,1
add GCP conn type,1
Add WePay and committer list to README.md,2
Support creating GCP connections from the Airflow UI,1
remove breakpoint!,4
ISSUE-1123 Use impyla instead of pyhs2,1
Use GSSAPI instead of KERBEROS and provide backwards compatibility,1
Use kerberos_service_name = 'hive' as standard instead of 'impala'.,1
Add warning for deprecated setting,1
FAQ entry about start_date,5
Add GSSAPI SASL to HiveMetaStoreHook.Will probably only work with python 2.7 until thrift 1.0 is released,1
Showing active dag runs as in (3/16) in tooltip,1
Merge pull request #1116 from criccomini/update-readmeAdd WePay and committer list to README.md,2
Merge pull request #1115 from bolkedebruin/license_checkThis patch adds license checking for Airflow.,1
Adding ssh connection type to webform,1
Merge pull request #792 from abridgett/feature/add_error_handling_to_slack_operatoradd error handling for slack api,0
Merge pull request #1131 from hyperborea/masterAdding ssh connection type to webform,1
Add upstart scripts,1
Merge pull request #1045 from d-lee/upstart_scriptsAdd startup scripts for upstart based systems,5
Parameterizing DagBag import timeouts,2
"Added start_date initialization for DagRun creation within schedule_log(self, dag_id)",2
Merge pull request #1134 from airbnb/dagbag_timeoutParameterizing DagBag import timeouts,2
typo,2
Merge pull request #1119 from jlowin/gcpAdd gcloud-based GCSHook,1
"Add custom email backends.Allow users to configure a custom email backend using the`EMAIL_BACKEND` configuration variable, which accepts a dotted importpath. The backend defaults to the existing `send_email` helper, which isrenamed to `send_email_smtp`. This can be used to send email withoutusing SMTP, e.g. when sending mail via API.Note: this patch uses `importlib` instead of the deprecated `imp` modulewhich is used elsewhere throughout `airflow`.[Resolves #1103]",0
Add tests for default email backend.,3
Merge pull request #1132 from jmcarp/custom-email-backendAdd custom email backends.,1
Add support for BigQuery User Defined Functions in BigQuery operatorSee https://cloud.google.com/bigquery/user-defined-functions for details on UDFs,1
Merge pull request #1138 from LeBlanc/masterAdd support for BigQuery User Defined Functions in BigQuery operator,1
Cranking up slackclient dep to 1.0,5
Make sure only to update counts when building master,5
Pass TRAVIS_PULL_REQUEST to the scripts,4
Merge pull request #1140 from bolkedebruin/license_checkLicense check,7
"Merge pull request #1135 from RvN76/masterAdded start_date initialization for DagRun creation within schedule_dag(self, dag_id)",2
Merge pull request #1110 from criccomini/support-dates-in-mysql-2-gcsAdd date support to MySQL to GCS operator,1
"allow bq base cursor methods run_extract, run_copy, run_load to alltake in source or destination table strings that include projects.For backwards compatibility reasons, the project is not required.This allows for decoupling of the execution of these methods fromprojects that have the information they access.",5
Merge pull request #1139 from mtagle/bq_proj_inputAllow specificiation of project in BigQuery Hook methods,1
small fixes to previous bq project inclusion pr.,0
Merge pull request #1143 from mtagle/bq_proj_fixessmall fixes to previous bq project inclusion pr.,0
More explicit zindex on modals,5
Merge pull request #1144 from airbnb/zindexMore explicit zindex on modals,7
Merge pull request #1129 from airbnb/active_runsShowing active dag runs as in (3/16) in tooltip,1
Merge pull request #1127 from airbnb/doc_start_dateFAQ entry about start_date,5
statuses column on /admin shows only active or most recent dag_runshttps://github.com/airbnb/airflow/issues/974,2
"label as ""recent statuses"", add tooltip",1
Merge pull request #975 from jtschoonhoven/issue-974statuses column on /admin shows only active or most recent dag_runs,2
Enhance CLI Test command to accept a JSON-formatted dictionary of params that can be added to a task's params dict.The CLI-provided params will overwrite params of the same name defined in the task definition if a key conflict occurs. This change will allow us to provide parameters to a DAG at runtime that are specific to a 'test' command run.,1
Merge pull request #1147 from r39132/masterEnhance CLI Test command to accept a JSON-formatted dictionary of par…,5
Merge branch 'airbnb/master',7
"Only use multipart upload in S3Hook if file is large enoughPrior to this change, uploads would fail if the target file was larger than the specified multipart_bytes value. This is problematic when you're handling files that can vary in size. It makes sense to handle this inside the hook rather than forcing the user to manage this. Now S3Hook.load_file will automatically detect whether or not the file is large enough to qualify for multibyte uploads and use the appropriate method. Also added a default of 5GB for multipart_bytes. This value makes sense because S3 will automatically fail when uploading files larger than this unless multipart uploads are used.",1
Merge pull request #1158 from adambom/tweak-multipart-uploadsOnly use multipart upload in S3Hook if file is large enough,2
refactor remote log read/write and add GCS supportsquashed:update configurationmore descriptive commentsplit remote log uploads into helper functions for S3 and GCSread logs from s3read logs from GCSkeep old_log as stringchange name to log_basebetter loggingoverwrite in GCSuse current configuration varobjects could be none; don't check if they exist with methodallow s3 encryption from hookfix capitalization typoreplace string search with indexingadd param docsrefactor remote log read/write into utility classes,2
change gcs_hook to self.hook,1
make sure paths don't conflict bc of trailing /,5
more detail in error message.,0
Adding support for ssl parameters.  (picking up from jthomas123),2
Adding fernet key to use it as part of stdout commands,1
Fix typo when returning VerticaHook,1
version cap for gcp_api,5
Merge pull request #1137 from jlowin/rls3Remote Log Storage (take 2),2
fix bigquery hook,1
Treat SKIPPED and SUCCESS the same way when evaluating depends_on_past=True,5
Merge pull request #1167 from xiaoliangsc/fix-bigquery-hookfix bigquery hook bugs,0
Merge pull request #1163 from asamasoma/fix_vertica_hook_package_pathFix typo when returning VerticaHook,1
Merge pull request #1161 from jlowin/path_conflictsMinor fix: path conflict with trailing /,5
Merge pull request #1164 from Nextdoor/postgres-ssl-paramsAdd ssl extra arguments to postgres hook  (picking up from jthomas123),1
This patch allows for testing of hive operators and hooks. Sasl is used (NoSasl in connection string is not possible). Tests have been adjusted.,3
Merge branch 'impyla' into minicluster,5
Add tests for Hiveserver2 and fix some issues from impyla,0
"Allow users to set hdfs_namenode_principal in HDFSHook configsnakebite library just added the support to specify hdfs_namenode_principalfor Kerberos auth method, and this PR allows users to pass in this config from HDFSHookAlso bump the version of snakebite",1
Test HivemetaStore if python 2,3
More impyla fixes,0
Ignore metastore,5
Make sure to write binary as string can be unicode,1
Support decimal types in MySQL to GCS,1
Merge pull request #1172 from criccomini/support-decimals-in-gcs-loadsSupport decimal types in MySQL to GCS,5
Merge remote-tracking branch 'upstream/master',7
Replace tab with spacesRemove unused import,2
Merge pull request #1169 from r39132/masterTreat SKIPPED and SUCCESS the same way when evaluating depends_on_pas…,7
Use unicodecsv to make it py3 compatible,1
Make sure to be py3 compatible,1
Convert to bytes for py3 compat,5
More py3 fixes,0
Also keep py2 compatible,5
Remove decode for logging,2
Make it work on py3,1
removing requirements.txt as it is uni-dimensional,5
new badge for showing staleness of reqs,1
Provide data for ci tests,3
Use correct table name,1
Use correct connection id,1
Add license and ignore for sql and csv,1
Merge pull request #1112 from garthcn/hdfs-principalAllow users to set hdfs_namenode_principal in HDFSHook config,5
badge for pypi version,5
Adding a .landscape.yml file,2
Some linting,5
Pointing to a reqs file,2
Adding a reqs.txt for landscape.io,5
Throwing in a few license to pass the build,4
Update README.md,2
Merge pull request #1176 from jgao54/masterUpdate README.md from GIF to Static Images,2
Merge pull request #1175 from airbnb/pep8Let's merge this to get builds working again! Enforcing PEP8,1
Merge remote-tracking branch 'upstream/master' into minicluster,5
Make testing on hive conditional,3
Merge pull request #1170 from bolkedebruin/miniclusterAllow for (unsecured) Hive unit tests and integrate impyla,3
[hotfix] fixing landscape requirement detection,1
[hotfix] typo that made it in master,2
fixing landscape's config,5
Adding more licenses to pass checks,4
Merge pull request #1177 from airbnb/fix_landscapeFixing landscape's config,5
Fix WebHdfsSensor,0
Linting,5
Merge pull request #1178 from airbnb/pep8lintLinting,7
"Set KillMode to 'control-group' for worker.serviceSystemd KillMode was set to ""process"" so only the main process waskill and not his childrens. By removing this line, the systemd default value""control-group"" is used and all childrens are correctly stopped.",1
"Set killMode to 'control-group' for webservice.serviceThe gunicorn workers where not properly killed on `systemctl stop`command. By removing the KillMode setting the default parameter""control-group"" is used and all childrens are properly killed",1
Merge pull request #1182 from xavierp/fix-systemctl-scriptsFix systemd scripts,5
Merge pull request #1173 from ahobson/webhdfsFix WebHdfsSensor,0
docs: fixes a spelling mistake in default config,5
Merge pull request #1183 from obulpathi/masterdocs: fixes a spelling mistake in default config,5
Modifying README to link to the wiki committer list,2
Merge pull request #1185 from r39132/masterModifying README to link to the wiki committer list,2
Changes to Contributing to reflect more closely the current state of development.,4
Add extras to installation.rst,1
Merge pull request #1184 from airbnb/changes_to_contributingChanges to Contributing to reflect more closely the current state of dev on Airflow,4
replace main_session with @provide_session,1
clear xcom data when task instance starts,5
Merge pull request #1180 from jgao54/791clear xcom when task starts,7
resolve conflict,5
Merge pull request #1188 from jlowin/remove-main_sessionreplace main_session with @provide_session,1
remove session reference,4
clean up references to old session,4
Merge pull request #1193 from jlowin/remove-main_sessionRemove one more session reference,4
Fixing the docs,2
Updating the Bug Reporting protocol in the Contributing.md file,2
Refactoring the CLI to be data-driven,5
Linting & debugging,0
Merge pull request #1198 from r39132/masterUpdating the Bug Reporting protocol in the Contributing.md file,2
Adding an ISSUE_TEMPLATE to ensure that issues are adequately defined,0
Merge pull request #1200 from r39132/masterAdding an ISSUE_TEMPLATE to ensure that issues are adequately defined,0
Fixing ISSUE_TEMPLATE name to include .md suffix,0
Merge pull request #1201 from r39132/masterFixing ISSUE_TEMPLATE name to include .md suffix,0
Merge pull request #1145 from airbnb/dag_cliper-DAG specifc CLI & refactoring,4
Documentation badge,2
Fix typo preventing from launching webserver,2
Merge pull request #1202 from xavierp/hotfix-cli-webserverFix typo preventing launching webserver from CLI,2
Merge pull request #1162 from himank/AddFernetKeyAdding fernet key to use it as part of stdout commands,1
"added Glassdoor to ""who uses airflow""",1
"Merge pull request #1204 from Glassdoor/masteradded Glassdoor to ""who uses airflow""",1
"Allow disabling periodic committing when inserting rows with DbApiHook```>>> i = 1>>> commit_every = 0>>> bool(i % commit_every == 0)  # previouslyTraceback (most recent call last):  File ""<stdin>"", line 1, in <module>ZeroDivisionError: integer division or modulo by zero>>> bool(commit_every and i % commit_every == 0)  # with this changeFalse```",4
Update link to Common Pitfalls wiki page in README,2
[hotfix] fixing the Scheduler CLI to make dag_id optional,2
Merge pull request #1210 from airbnb/fix_cli[hotfix] fixing the Scheduler CLI to make dag_id optional,2
Fix broken links in documentation,2
Fix typos in models.py,2
Merge pull request #1211 from Syeoryn/masterFix broken links in documentation and typos,2
Tasks references upstream and downstream tasks using strings instead of references,1
"Add two methods to bigquery hook's base cursor: run_table_upsert, which adds a table or updates an existing table; and run_grant_dataset_view_access, which grants view access to a given dataset for a given table.",5
Merge pull request #1205 from mtagle/insert_tableAdd two methods to bigquery hook's base cursor…,1
Add an example on pool usage in the documentation,2
Linting,5
Fix HttpOpSensorTest to use fake resquest session,1
"Make sure Executors properly trap errorsSequentialExecutor and LocalExecutor execute `airflow run` commandswith `subprocess.Popen().wait()` and try to catch errors with`try/except`. However, `subprocess.Popen()` doesn't raise errors;you have to check the `returncode`. As a result, these Executorsalways report that their commands are successful. This is normally finebecause task status gets precedence over executor status, so as longas the task reports on itself correctly the issue is avoided. Butif an error is raised BEFORE a task runs -- meaning the task is notyet monitoring its own status -- then the executor will incorrectlyreport success. Airflow will actually notice something went wrong,but because the task doesn't say it failed, it gets rescheduled,leading to an infinite loop.To resolve this, replace the Executor's `Popen().wait()` with`check_call()`, which is a blocking method that raises an errorif the returncode != 0. This way, errors are properly recognized.Also, prevent infinite loops by limiting the number of times atask is allowed to be rescheduled due to executor failure to 3.(Note: this doesn't affect the number of times a task can berescheduled due to its own failure).Last, check for an odd situation where the executor reports failurebut the task reports running.Closes #1199See #1220 for a test case",3
Add unit tests for trapping Executor errors,0
Merge pull request #1208 from underyx/patch-2Update link to Common Pitfalls wiki page in README,2
Update Airflow docs for remote logging,2
Fixes #1223,0
Merge pull request #1213 from airbnb/task_ref_strTasks refs to upstream and downstream tasks using strings instead obj refs,1
Set the service_name in coverals.yml,5
[hotfix] removing repo_token from .coveralls.ymlAfter readinghttps://github.com/lemurheavy/coveralls-public/issues/632,0
Check name of SubDag class instead of class itself`airflow.operators.SubDagOperator` and`airflow.operators.subdag_operator.SubDagOperator` are NOT thesame. Airflow needs to check against both classes to determineif a task is in fact a SubDagOperator. This is because of Airflow'simport machinery. It is *probably* ok to check both classes with`isinstance()` but the behavior is surprising and to cover our baseswe check for __class__.__name__ and a `subdag` attr.closes #1168,2
"Add the missing ""Date"" header to the warning e-mailsThe warning emails sent by airflow don't have theDate header set. This makes them appear in ""spam like""places in e-mail clients sorting the incomming e-mailusing the ""sent date"" e-mail information. Withoutthis header the ""sent date"" appears as ""Unknown"".",5
"Add the missing ""Date"" header to the warning e-mailsThe warning emails sent by airflow don't have theDate header set. This makes them appear in ""spam like""places in e-mail clients sorting the incomming e-mailusing the ""sent date"" e-mail information. Withoutthis header the ""sent date"" appears as ""Unknown"".",5
Merge pull request #1224 from mtoma/fix-issue-1223Fixes #1223. Thanks,0
"[hotfix] make email.Utils > email.utils for py3I'm not sure how this passed the build in the PR, but it certainly failson master",0
Add wiki link to README.md,2
Merge pull request #1221 from jgao54/mock-request-in-testFix HttpOpSensorTest to use fake request session,1
"Merge pull request #1220 from jlowin/fix_executor_failedFix case where Executors fail to report failure, creating infinite loopThanks for the great commit message",5
Merge pull request #1217 from ledsusop/masterAdd an example on pool usage in the documentation,2
Merge branch 'master' into hivemeta_sasl,7
"Add function to get configuration as dict, plus unit testsThere are many ways to set configuration options in Airflowbut no way to actually see all of them (the web UI only showsairflow.cfg). This takes the current configuration objectand writes it to a dict.The ""source"" of an option can be displayed (for example,'airflow.cfg', 'default', 'env var', etc.).Sensitive (confidential) configuration options can be includedUnless specified, they are censored as '< hidden >'.",5
Merge pull request #1206 from jlowin/extend-configAdd as_dict() function to print configuration,5
Properly measure number of task retry attemptsA task’s try_number is unbounded (incremented by 1 on every run) soit needs to be adjusted both for logging and for seeing if a taskhas eclipsed the retry cap. Rerunning a task (either because itfailed or with the `force` option) not only leads to nonsensicalerror messages (“Attempt 2 of 1”) but also would never kick off aretry attempt (because try_number > retries). The solution is to modthe `try_number` with `retries` to keep everything sensible.Fixed: use the correct attempt number when loggingFixed: log when tasks are queued (log message was being created butnot logged)Fixed: situation where tasks being run after the first time wouldnot be put up for retry,1
"Refactoring utils into smaller submodulesutils.py had become a little too complex. Other projects likeDjango or IPython have a more structured and, I would argue,clearer way to organize utils. I try to reproduce this here.Ideally we want a utils folder with submodules that are groupedthematically. I rebased off of master and fixed references acrossthe repository. I also introduced a PendingDeprecationWarning forcalling apply_defaults from `airflow.utils` directly and redirectpeople to the right place. I also moved exceptions to a top levelfile.",2
Fix required gcloud version,1
remove extra import of logging lib,2
Merge pull request #1236 from jlowin/gcloud-versionQuick fix: change required gcloud version,1
"remove unused logging,errno, MiniHiveCluster imports",2
Merge pull request #1219 from airbnb/arthurw_utils_refactor_take2Refactoring utils to be more sane,4
Merge pull request #1237 from notifytovishal/masterremove unused imports,2
"Use LocalExecutor on Travis if possibleTravis was only using the SequentialExecutor, which is suboptimal as theSequentialExecutor is not geared for production. This change enablesthe LocalExecutor if possible, ie. when not using sqllite.",1
Add changelog for 1.7.0,4
Merge pull request #1240 from bolkedebruin/travis_localexecutorUse LocalExecutor on Travis if possible,1
Merge pull request #1241 from bolkedebruin/masterAdd changelog for 1.7.0,4
Merge pull request #1230 from jlowin/attempt-messageFix retry number handling,1
Use refactored utils module in unit test imports,2
Merge pull request #1244 from jlowin/fix-utils-importUse refactored utils module in unit test imports,2
Set Postgres autocommit as supported only if server version is < 7.4The server-side autocommit setting was removed here http://www.postgresql.org/docs/7.4/static/release-7-4.htmlResolves: #690,2
Merge pull request #1242 from underyx/patch-3Set Postgres autocommit as supported only if server version is < 7.4,1
Extract dbapi cell serialization into its own method,5
Fix airflow.utils deprecation warning code being Python 3 incompatibleSee https://docs.python.org/3.0/whatsnew/3.0.html#operators-and-special-methods,2
Update docs with separate configuration section,5
Add documentation links to README,2
use num_shards instead of partitions to be consistent with batch ingestion,1
Merge pull request #1245 from underyx/patch-4Fix airflow.utils deprecation warning code being Python 3 incompatible,2
update link to Lyft's website,2
Merge pull request #1249 from SaurabhBajaj/patch-1update link to Lyft's website,2
Merge pull request #1246 from jlowin/docsDocumentation updates,5
"Make the provide_session decorator more robustModify the provide_session decorator in order to betteravoid providing a session to a function that already has one.Now, a decorated function can safely accept 'session' as apositional argument; before, only keyword arguments weresafe.Resolves #1234",0
Merge pull request #1248 from hongbozeng/exclude_ts_dimexclude timestamp column from the dimensions,7
Merge pull request #1247 from ty707/ty707/provide_session_improve_arg_checkImproving the check for an existing session in the provide_session decorator,1
Merge pull request #1128 from bolkedebruin/hivemeta_saslAdd GSSAPI SASL to HiveMetaStoreHook.,1
Use psycopg2's API for serializing postgres cell values,1
Merge pull request #1243 from underyx/feature/use-psycopg2-adaptUse psycopg2's API for serializing postgres cell values,1
"Fixed scheduling for @once intervalModified the conditional branching in schedule_dag becauseone logical branch was never reached, and another branchwas reached by the wrong condition.Resolves #1260",0
Make webserver worker timeout configurable,5
Merge pull request #1257 from airbnb/ddavydov/configurable_gunicorn_timeout[webserver] Make webserver worker timeout configurable,5
Merge pull request #1261 from ty707/fix_once_schedulingFixed scheduling for @once interval,0
"Use urlparse for remote GCS logs, and add unit testsIt’s conceivable that the bucket starts with g or s, in which case thislstrip would remove characters from the bucket name. Instead, useurlparse to properly parse the string.Also clean up the equivalent function in the S3 and GCS Hooks, using`strip` to clean up leading/trailing slashes.",4
"Validate that subdag tasks have pool slots available, and testIf a SubDagOperator has a pool that we know has only 1 slot, and it isshared with any of the subdag tasks, then we can raise an error becausethe subdag tasks will always be blocked by the subdag operator.",1
Add missing session.commit() at end of initdbThe chart is added to the session but not committed.,1
"Resurface S3Log class eaten by rebase/push -fDuring my refactoring of utils, I rebased several times,it seems however that the S3Log class was not ported to the newutils structure. This PR fixes this.",0
Set dags_are_paused_at_creation's default value to True,2
Merge pull request #1264 from underyx/patch-5Set dags_are_paused_at_creation's default value to True,2
Merge pull request #1228 from jlowin/remote-logging-fixDon't lstrip remote storage URL because it could strip the bucket name,1
Merge pull request #1268 from airbnb/fixing_remote_s3log_rebase_issueResurface S3Log class eaten by rebase/push -f,2
Add pypi meta data and sync version number,5
Merge pull request #1270 from bolkedebruin/pypi_metaAdd pypi meta data and sync version number,5
Merge pull request #1226 from jlowin/subdag_poolValidate subdag pools and add subdag unit tests,3
Merge pull request #1041 from caseybrown89/masterset celery_executor to use queue name as exchange,4
Add support for calling_format from boto to S3_Hook,1
Merge pull request #1231 from msumit/boto_calling_formatAdd boto calling_format support,1
Merge pull request #1196 from jlowin/subdag_classHandle both SubDagOperator classes,2
Beware of negative pool slots.Sometimes the scheduler over-allocates tasks in a pool. When that happens thenumber of open slot counts will go negative. The `not open_slots` code onlyworks if the scheduler observes the pool going to zero. If it has gone negativethe previous logic will schedule an unlimited number of pool tasks.,2
Rename user table to users to avoid conflict with postgres* Resolves issue 1083,0
Merge pull request #1216 from abridgett/abridgett/1215_slackclient_returns_decoded_dict[#1215] slackclient v1.0.0 returns a decoded dict,1
Use Popen with CeleryExecutorUse the same calling format as the other Executors,1
"Introduce ignore_depends_on_past parametersThe current machinery for running BackfillJobs overrides tasks’start_dates to deal with depends_on_past. This is fragile and,critically, doesn’t always carry through all of the nested Jobs. Wereplace it with an explicit instruction to ignore_depends_on_past whenconsidering whether a task can be queued.Also, this will be used later to evaluate whether a set of tasks isdeadlocked.",1
"Improve BackfillJob handling of queued/deadlocked tasks1. Introduce a concept of a deadlocked backfill, meaning no tasks canrun. The easiest way to create this is with depends_on_past.Previously, backfill would sit forever. Now it identifies the deadlockand exist, possibly informing the user that a cause of the deadlockwith depends_on_past2. Previously, BackfillJob would run a task once to put it in a queue,but then ignore the queued task on every subsequent loop, resulting init never being run. Now it considers queued tasks and runs them.3. “UP_FOR_RETRY” tasks were not handled properly by the executor (itraised the “the airflow run command failed at reporting an error”message). Now they are.",0
"Make SchedulerJob not run EVERY queued taskSchedulerJob loads EVERY queued task and tries to run it, which createsconflicts with any other Job trying to do the same (BackfillJob fromCLI or subdag, or potentially [one day] other schedulers). This createsa new method, process_events, which polls the Scheduler’s own executorfor queued tasks and adds them to a set. The scheduler then onlyconsiders that set when prioritizing queued tasks.",1
"Fix logic for determining DagRun statesPreviously, DagRuns failed if any task failed and succeeded if alltasks succeeded or were skipped. However, because of trigger behaviors,that’s not right — a task can fail and another task can start up withan “on failed” trigger.This changes the logic to consider three termination cases:1. Failure. If any of the root tasks fail, the dagrun fails. This isbecause there is no possibility of any “on failure” trigger coming offa root task.2. Success. If ALL of the root nodes succeed or skip, the dagrunsucceeds. This means there can be upstream failures as long as failuretriggers are respected.3. Deadlock — A dag run is deadlocked when no action is possible.This is determined by the presence of unfinished tasks without metdependencies. However, care must be taken when depends_on_past=Truebecause individual dag runs could *look* like they are deadlockedwhen they are actually just waiting for earlier runs to finish.To solve this problem, we evaluate deadlocks in two ways. First,across all dagruns simultaneously (to account for situations withdepends_on_past=True). Second, in each individual dagrun (but onlyif there are no depends_on_past relationship).",2
Fix miscellaneous bugs and clean up codeFix minor issues including:  - clean up State  - fix bug with nonstandard DAGS_FOLDER locations  - remove restriction on dags not being outside DAGS_FOLDER      because DagBags are allowed to load dags from anywhere  - miscellaneous Landscape fixes  - use logger instead of print for DAG.clear(),2
Add unit tests,3
Rewrite BackfillJob logic for clarity,2
Add note about airflow components to template,1
Fix SSHExecuteOperator crash when using a custom ssh port* Fix SSHExecuteOperator crash when using a ssh connection with a custom ssh portThe ssh port if not converted to a string makes the SSHExecuteOperator crash.This patch converts the port number to string and fixes problem.,0
Merge pull request #1271 from jlowin/subdag_pool_issue_1225Many fixes for handling tasks in jobs and executors,0
Merge pull request #1278 from bolkedebruin/ISSUE-1083Rename user table to users to avoid conflict with postgres,5
CHORE - Remove Trailing Spaces,4
Fix reading strings from confThe default remote_logging values of None and False were being read asstrings rather than Python objects and therefore misinterpreted,2
Merge pull request #1284 from jlowin/fix-remote-base-NoneFix reading strings from conf for remote logging,2
Increase timeout time for unit testTravis runs are occasionally failing this test. Increasing the timeout should help.,1
added Gentner Lab to list of users,1
Merge pull request #1287 from gentnerlab/gentnerlabAdd Gentner Lab to list of users,1
Fix module path of send_email_smtp in configuration,5
"Change inconsistent example DAG ownersChange two example DAG owners from 'me' to 'airflow'in order to1. Be consistent with other example DAGs2. Avoid confusing users, who may misinterpret ""me""",1
Merge pull request #1296 from ty707/fix-example-ownersChange inconsistent example DAG owners,2
Raise deep scheduler exceptions to force a process restart.,1
"Deprecate *args and **kwargs in BaseOperatorBaseOperator silently accepts any arguments. This deprecates thebehavior with a warning that says it will be forbidden in Airflow 2.0.This PR also turns on DeprecationWarnings by default, which in turnrevealed that inspect.getargspec is deprecated. Here it is replaced by`inspect.signature` (Python 3) or `funcsigs.signature` (Python 2).Lastly, this brought to attention that example_http_operator waspassing an illegal argument.",4
Missing comma in setup.py,1
"Set DAG_FOLDER for unit testsWhen tests are running, the default DAG_FOLDER becomes`airflow/tests/dags`. This makes it much easier to execute DAGs in unittests in a standardized manner.Also exports DAGS_FOLDER as an env var for Travis",2
Show only Airflow's deprecation warningsPrevious filter was too lenient and showed deprecation warnings fromALL modules.,2
Merge pull request #1304 from jlowin/warningsShow only Airflow's deprecation warnings,2
"Fix handling of deadlocked jobs- Raise an error when a backfill deadlocksDeadlocked backfills didn’t raise AirflowExceptions, soSubDagOperators didn’t recognize that their subdagswere failing.- Fix bug with marking DagRuns as failed- Let SchedulerJob mark DagRuns as deadlocked when thereare no TIs available; other deadlock metrics depend on TIs- Adds unit tests.",3
"Fix infinite retries with pools, with testAddresses the issue raised in #1299",0
Merge pull request #1290 from jlowin/subdag-backfill-statusMake sure backfill deadlocks raise errors,0
"Don't schedule runs before the DAG's start_datePreviously the Scheduler would start scheduling immediatelyafter ANY execution date, irrespective of the DAG'sstart_date.",5
Merge pull request #1291 from jlowin/scheduler_start_dateDon't schedule runs before the DAG's start_date,5
Doc: explain the usage of Jinja templating for templated params,2
Merge pull request #1307 from AntoineAugusti/doc-concepts-jinja-templatingDoc: explain the usage of Jinja templating for templated params,2
Update docstring for executor trap unit test,3
Allow Operators to specify SKIPPED status internally* Added ability to skip DAG elements based on raised Exception* Added nose-parameterized to test dependencies* Fix for broken mysql test - provided by jlowin,1
"Add consistent and thorough signal handling and loggingAirflow spawns childs in the form of a webserver, scheduler, and executors.If the parent gets terminated (SIGTERM) it needs to properly propagate thesignals to the childs otherwise these will get orphaned and end up aszombie processes. This patch resolves that issue.In addition Airflow does not store the PID of its services so they can bemanaged by traditional unix systems services like rc.d / upstart / systemdand the likes. This patch adds the ""--pid"" flag. By default it stores thePID in ~/airflow/airflow-<service>.pidLastly, the patch adds support for different log file locations: log,stdout, and stderr (respectively: --log-file, --stdout, --stderr). Bydefault these are stored in ~/airflow/airflow-<service>.log/out/err.* Resolves ISSUE-852",0
Merge pull request #855 from bolkedebruin/ISSUE-852Use proper signal handling and cascade signals to children (Fix #852),0
"Fixing a broken example dag, example_skip_dag.py",2
"Merge pull request #1316 from r39132/masterFixing a broken example dag, example_skip_dag.py",2
Make sure skipped jobs are actually skippedThe new AirflowSkipException means we have a new state to account for:Executor SUCCESS + Task SKIPPED,1
Include all example dags in backfill unit test,3
Merge pull request #1317 from jlowin/run-example-unit-testsInclude all example dags in backfill unit test,3
Add HipchatOperator,1
Fix typo in comment in prioritize_queued method,2
Fix for missing edit actions due to flask-admin upgrade,2
Fix celery flower port allocation,0
Use session instead of outdated main_session for are_dependencies_met,5
Fix usage of asciiart,0
Add Lucid to list of users,1
Adding a PR Template,1
Merge pull request #1332 from jbrownlucid/patch-3Add Lucid to list of users,1
Merge pull request #1335 from r39132/masterAdding a PR Template,1
Reduce logger verbosity,2
Github ISSUE_TEMPLATE & PR_TEMPLATE cleanup,4
Merge pull request #1337 from r39132/masterGithub ISSUE_TEMPLATE & PR_TEMPLATE cleanup,4
Merge pull request #1283 from clickthisnick/chore-remove-trailing-spacesCHORE - Remove Trailing Spaces,4
Merge pull request #1323 from jgao54/fix-commentFix typo in comment in prioritize_queued method,2
"Add twitter feed example dagAn example dag ""example_twitter_dag.py"" was added to exhibit a reallife use case scenario. In this example we collect, process, andanalyze twitter data from yesterday and store them in Hiveincrementally. A readme file, ""example_twitter_README.md""was added to explain the concepts and technologies behind this DAG.",2
Fix s3 logging issueUse getboolean() instead of get_bool()Closes issue #1326,0
Merge pull request #1336 from jlowin/reduce-verbosityReduce logger verbosity,2
"Update plugins.rst for clarity on the example (#1309)The plugins tutorial was lacking in the following ways:1. I wasn't sure where my template should live2. I wasn't aware that both the TestView and Blueprint were necessaryIn lieu of a code refactor, here's my suggestion on how to make the documentation more helpful from the perspective of someone who doesn't have experience with Flask Blueprints and Flask Admin, which can prevent the deep-dive into the code and supporting libs that I just did!",1
Extract non_pooled_task_slot_count into a configuration param,2
Fixing misnamed PULL_REQUEST_TEMPLATE,0
Merge pull request #1343 from r39132/masterExtract non_pooled_task_slot_count into a configuration param,2
Merge pull request #1345 from r39132/masterFixing misnamed PULL_REQUEST_TEMPLATE,0
Ensure attr is in scope for error messageDuring the recursion work below 'attr' dropped out of scope:https://github.com/airbnb/airflow/commit/376cdd4556f4b46ec50acc5cfc324a3fad6c51f3This commit ensures attr is in scope once more as it's very helpfulin tracing problems.,0
Fix GCS logging for gcp_api.,2
Merge pull request #1355 from criccomini/fix-gcp-api-gcs-loggingFix GCS logging for gcp_api.,2
Don't return error when writing files to Google cloud storage.,2
Merge pull request #1359 from criccomini/fix-gcp-api-gcs-loggingDon't return error when writing files to Google cloud storage.,2
"Add DAG inference, deferral, and context manager- Operators can be created without DAGs, but the DAG can be added atany time thereafter (by assigning to the ‘dag’ attribute). Once a DAGis assigned, it can not be removed or reassigned.- Operators can infer DAGs from other operators. Setting a relationshipwill also set the DAG, if possible. Operators from different DAGs andoperators with no DAGs can not be chained.- DAGs can be used as context managers. When “inside” a DAG contextmanager, the default DAG for all new Operators is that DAG (unless theyspecify a different one)- Unit tests- Add default owner for Operators- Support composing operators with >> and <<Three special cases:  op1 >> op2 is equivalent to op.set_downstream(op2)  op1 << op2 is equivalent to op1.set_upstream(op2)  dag >> op1 (in any order or direction) means op1.dag = dagThese can be chained:  dag >> op1 >> op2 << op3- Update concepts documentation",2
"Remove executor error unit testThis unit test was designed to trap unusual errors when setting up anExecutor and therefore relied on being able to create just such anerror. The previous version relied on the fact that dag_ids are fragilebut moving to deferred dag assignment fixed that fragility and “broke”the unit test. The only other solution I’ve found so far is to takeadvantage of the fact that the `pool` attribute is accessed exactlytwice when running a task and putting a 1/0 payload in an overloaded`pool` property. But that’s too fragile to make a unit test because noone will be able to figure out why accessing `pool` elsewhere inairflow makes this unit test fail. For the time being, I’m removing theunit test.",3
Add PyOpenSSL to Google cloud gcp_api.,1
Merge pull request #1363 from criccomini/fix-gcp-api-gcs-loggingAdd PyOpenSSL to Google cloud gcp_api.,5
Merge pull request #1351 from abridgett/feature/correct_render_errorEnsure attr is in scope for error message,0
"Merge pull request #1318 from jlowin/infer_dagSyntactic Sugar! Dag inference, operator composition, and a big docs update",5
added oracle operator with existing oracle hookadded apache header,1
Document the parameters of `DbApiHook`,5
Merge pull request #1207 from underyx/patch-1Allow disabling periodic committing when inserting rows with DbApiHook,5
Add a missing word to docs,2
"Prevent DAGs from being reloaded on every scheduler iterationThe get_dag method has an if-statement to determine if the DAG should be refreshed. Every DAG was getting refreshed on every iteration. This was happening because orm_dag.last_expired is None for all DAGs, which resulted in a check of dag.last_loaded < datetime(2100, 1, 1), which it always is.",5
User subquery in views to find running DAGs A non-indexed column dag_run.state was used for obtaining running DAGs in views.,2
Stop creating hook on instantiating of S3 operatorThe operator was initializing hooks on instantiating instead of on execute.Resolves: #1357,0
"Add support for zipped dagsCurrently dags are being read directly from the filesystem. Anyhierarchy (python namespaces, modules) need to be reflected onthe filesystem. This makes it hard to manage dags and theirdepedencies.This patch adds support for dags in zip files. It will addthe zip to sys.path and then it will read the zip file andtry to import any files as modules that are in the root ofthe zip.Please note that any module contained within the zip willoverwrite existing modules in the same namespace.",2
Change DAG.tasks from a list to a dict This prevents iterating over a potentially very large list of tasks.,2
Added the ability to view XCom variables in webserver,1
Merge pull request #1328 from withnale/www_xcom_viewAdded the ability to view XCom variables in webserver,1
Properly handle BigQuery booleans in BigQuery hook.,1
Merge pull request #1390 from criccomini/fix-boolean-bqProperly handle BigQuery booleans in BigQuery hook.,1
Support list/get/set variables in the CLI,1
Merge pull request #1387 from jgao54/cli-variable-supportSupport list/get/set variables in the CLI,1
"Partial fix to make sure next_run_date cannot be NoneIt seems possible that a TaskInstance has not yet been inserted for the first run.As a result, even though last_scheduled_run is None, then latest_run is also None.If latest_run is None, then the logic above picks the min start date for all of thetasks in the dag. This should never be None -- this is where there might be a bug.This patch works around the issue by making sure a next_run_date is always set toeither the dag start date if the TaskInstance has no calculated next_run_date yet.",5
"Add multiprocessing support to the schedulerAs the amount of dags grows and the ability to create dags programmaticallyis more often used, more and more time is spend in the scheduler which lowersthroughput.This patch adds the ability to use multiple threads to the scheduler. Theamount of threads can be specified by ""max_threads"" in the scheduler sectionof the configuration. The amount of threads will, however, not exceed theamount of cores.In case of using sqlite the max_threads will be set to 1 as sqlite does notsupport multiple db connections.",5
Merge pull request #1376 from bolkedebruin/multiprocessing_schedulerUse multiprocessing for the scheduler,1
Merge pull request #1366 from angelgao/masteradded oracle operator with existing oracle hook,1
Fixed a bug in the scheduler: num_runs used where runs intended,1
Merge pull request #1396 from r39132/masterFixed a blocker bug in the scheduler: num_runs used where runs intended,1
Add missing args to `airflow clear`Running `airflow clear dag_id` from the CLI fails because the`only_failed` and `only_running` args weren’t supplied by the factory.,1
Implement a Cloudant hook,1
Merge pull request #1401 from mtp401/cloudant-hookCloudant Hook,1
Merge pull request #1400 from jlowin/missing-args-clearAdd missing args to `airflow clear` - confirmed this change works locally.,1
"Revert from using ""--foreground"" to ""--daemon""This reinstalls old behavior or running on the foreground by default.",1
Use os.execvp instead of subprocess.Popen for the webserversubprocess.Popen forks before doing execv. This makes it difficultfor some manager daemons (like supervisord) to send kill signals.This patch uses os.execve directly. os.execve takes over the currentprocess and thus responds correctly to signals* Resolves residue in ISSUE-852,0
Reinstate imports for github enterprise authPylint based refactoring removed a necessary import. * Resolves #1303,0
Merge pull request #1420 from bolkedebruin/foreground_defaultMake foreground default and use os.execvp in specific cases,1
correct missed arg.foreground to arg.daemon in cli,5
Merge pull request #1426 from jgao54/bug-fixcorrect missed arg.foreground to arg.daemon in cli,0
Clean up issue template (#1419),0
Log dagbag metrics dupplicate messages in queue into Statsd (#1406),2
Log the number of errors when importing DAGs,2
Merge pull request #1429 from plypaul/plypaul_log_import_errorsLog the number of errors when importing DAGs,2
Add columns to toggle extra detail in the connection list view.,1
Merge pull request #1422 from stranbird/conn-ui-revampShow `extra` data on the connection list view,5
"Revert ""Show `extra` data on the connection list view"" (#1438)",5
Replace Github wiki links with Apache cwiki links,2
Merge pull request #1447 from r39132/masterReplace Github wiki links with Apache cwiki links,2
enable UI feature to recursively set success=True for all operators within SubDagOperator,2
AIRFLOW-15: Remove gcloud,4
Update dags.html,2
Merge pull request #1448 from criccomini/remove-gcloudAIRFLOW-15: Remove gcloud,4
Merge pull request #1449 from sharifyounes/masterUpdate dags.html,2
Add Zendesk as a company which is using Airflow,1
Merge pull request #1450 from jwswj/jsmale/add_zendeskAdd Zendesk as a company using Airflow,1
AIRFLOW-16: Update Google cloud hooks to use new Google cloud platform UI.,1
Merge pull request #1452 from criccomini/improve-gcp-hooksAIRFLOW-16: Update Google cloud hooks to use new Google cloud platfor…,1
Fixed MsSql autocommit bug,0
Update PR template with instructions about JIRA,5
Gracefully fail unit tests when docker-py isn't installed,2
Merge pull request #1456 from airbnb/jlowin-patch-2[AIRFLOW-34] Update PR template with instructions about JIRA,5
Merge pull request #1458 from biellls/master[AIRFLOW-27] Fixed MsSql autocommit bug,0
Merge pull request #1430 from whummer/success_recursive[AIRFLOW-35] Enable UI feature to recursively set success for nested DAG operators,1
Merge pull request #1457 from jlowin/docker-import[AIRFLOW-38] Gracefully fail unit tests when docker-py isn't installed,2
[AIRFLOW-42] Adding logging.debug DagBag loading stats (#1460)* Adding logging.debug DagBag loading stats* Linting* Fix py3* Tweaks,0
Docs tweaks while generating the docs,2
Add bulk_insert_rows() for more performant inserts.,1
AIRFLOW-21 upgrade GCP client lib,5
Merge pull request #1453 from alexvanboxel/feature/AIRFLOW-21-upgrade-gcp-libAIRFLOW-21 upgrade GCP client lib,7
Merge pull request #1464 from geeknam/feature/cx_oracle_bulk_insert[AIRFLOW-50] Add bulk_insert_rows() to OracleHook for more performant inserts.,1
Don't insert dag_runs beyond the min task end_date,5
Merge pull request #1466 from r39132/master[AIRFLOW-39] Don't insert dag_runs beyond the min task end_date,5
"[AIRFLOW-53] Adding DagBag stats report to CLI's list_dags (#1468)Adding DagBag stats report to CLI's list_dagsRemoving logging call in favor of CLI, on-demend based approachAddressing Dan's feedback",5
"Fix corner case with joining processes/queues (#1473)If a process places items in a queue and the process is joined before the queue is emptied, it can lead to a deadlock under some circumstances. Closes AIRFLOW-61.See for example: https://docs.python.org/3/library/multiprocessing.html#all-start-methods (""Joining processes that use queues"")http://stackoverflow.com/questions/31665328/python-3-multiprocessing-queue-deadlock-when-calling-join-before-the-queue-is-emhttp://stackoverflow.com/questions/31708646/process-join-and-queue-dont-work-with-large-numbershttp://stackoverflow.com/questions/19071529/python-multiprocessing-125-list-never-finishes",5
Add bulk_dump abstract method to DbApiHook (#1471),5
"[AIRFLOW-52] Fix bottlenecks when working with many tasksDag hash function tried (and failed) to hash the list of tasks, then fell back on repr-ing the list, which took forever. Instead, hash tuple(task_dict.keys()). In addition this replaces two slow list comprehensions with much faster hash lookups (using the new task_dict).",1
Use getfqdn to make sure urls are fully qualifiedgethostname only resolves host part while often fully qualified domain names are required.* Resolves #1437,0
AIRFLOW-77: Enable UI toggle whether to apply 'clear' operation recursively to sub-DAGs or not,2
[AIRFLOW-75] Fix bug in S3 config file parsing,2
[AIRFLOW-80] Move example_twitter dag to contrib/example_dags as it requires hive,1
"Handle queued tasks from multiple jobs/executorsWhen Scheduler is run with `—num-runs`, there can be multipleSchedulers and Executors all trying to run tasks. For queued tasks,Scheduler was previously only trying to run tasks that it itself hadqueued — but that doesn’t work if the Scheduler is restarting. This PRreverts that behavior and adds two types of “best effort” executions —before running a TI, executors check if it is already running, andbefore ending executors call sync() one last time",5
Add logic to lock DB and avoid race conditionThe scheduler can encounter a queued task twice before thetask actually starts to run -- this locks the task and avoidsthat condition.,1
AIRFLOW-52 Warn about overwriting tasks in a DAG,2
Merge pull request #1378 from jlowin/queued-tasks,7
AIRFLOW-92 Avoid unneeded upstream_failed session closes apache/incubator-airflow#1485,0
ssl gunicorn support,1
"Revert ""ssl gunicorn support""This reverts commit e332f63620a5f85e38c4a1a5ac9c9a4a5bfc6035.",4
Move presto.execute inside try catch to handle errorThis commit fixes an issue where malformed SQL would raise aDatabaseError outside of the try catch block in the hook. Thisshould now raise a PrestoException as expected.,1
Fix : Don't treat premature tasks as could_not_run tasks,1
Merge branch '1490',7
Change default DAG view from tree view to graph view,2
[AIRFLOW-112] Change default DAG view from tree view to graph view,2
[AIRFLOW-112] no-op README change to close this jira's PR,4
Merge branch '1498',7
[AIRFLOW-117] fix links in README.md,2
use targetPartitionSize as the default partition spec,1
Merge branch '1502',7
Use incubating instead of incubator in title,1
change TARGET_PARTITION_SIZE to DEFAULT_TARGET_PARTITION_SIZE,1
Merge branch '1503',7
"[AIRFLOW-109] Fix try catch handling in PrestoHookThis addresses the issue with executing the SQL statement outside ofthe try block. In the case of a syntax error in the statement, theunderlying library raises a Databases error which was meant to behandled (i.e., json parsed) by the catch.",5
[AIRFLOW-119] Fix Template not found error and default tags for Qubole operator,1
[AIRFLOW-121] Documenting dag doc_md feature,2
Merge branch '1493',7
Merge branch '1478',7
AIRFLOW-119: List support for tags in QuboleOperator,1
"AIRFLOW-124 Implement create_dagrunThis adds the create_dagrun function to DAG and the staticmethodDagRun.find. create_dagrun will create a dagrun including its tasks.By having taskinstances created at dagrun instantiation time,deadlocks that were tested for will not take place anymore. Testshave been adjusted accordingly.In addition, integrity has been improved by a bugfix to add_taskof the BaseOperator to make sure to always assign a Dag if it ispresent to a task.DagRun.find is a convenience function that returns the DagRunsfor a given dag. It makes sure to have a single place how tofind dagruns.",2
[AIRFLOW-127] Makes filter_by_owner aware of multi-owner DAG,2
Adding Nerdwallet to the list of Currently officially using Airflow:,1
Merge branch 'dag_run',2
[AIRFLOW-86] Wrap dict.items() in list for Py3 compatibilityAuthor: jlowin <jlowin@users.noreply.github.com>Closes #1483 from jlowin/AIRFLOW-86.,1
"[AIRFLOW-125] Add file to GCS operatorAdds an operator to upload a file to Google Cloud Storage. Used as follows:```pyfrom airflow.contrib.operators.file_to_gcs import FileToGoogleCloudStorageOperatorgcs = FileToGoogleCloudStorageOperator(        bucket='a-bucket-i-have-access-to-on-gcs',        dag=dag,        task_id='upload_stuff',        google_cloud_storage_conn_id='an-airflow-bigquery-connection',        src=os.path.join(os.path.dirname(__file__), 'csv/some_file.csv'),        dst='project/some_file.csv')```",2
Merge branch '1507',7
Merge branch '1508',7
Add a version view to display airflow version info,5
Increasing License Coverage,3
[AIRFLOW-143] setup_env.sh doesn't leverage cache for downloading minicluster,5
Add Kiwi.com as a user to README,1
[AIRFLOW-52] 1.7.1 version bump and changelog,4
Merge branch '1522',7
Make enhancements to VersionView,1
"[AIRFLOW-148] Use BQ connection project by default in view accessUpdates the internals of the BigQuery view access tools to make the source anddestination projects optional, where both values will default to the projectspecified in the connection if not explicitly provided.This is technically a breaking change but I've searched the public codebase andfound no direct calls to this (I'll be updating some internal operators that useit separately)",1
Updated HiveServer2Hook.to_csv() to add fetch_size,1
[AIRFLOW-150] setup.py classifiers dict should be list,1
Merge branch '1520',7
Merge branch '1505',7
Merge branch '1524',7
Merge branch '1527',7
[AIRFLOW-134] Add PR merge script,7
Merge pull request #1515 from jlowin/PR-MERGE,7
[AIRFLOW-153] fixed minor typos in help message,2
Merge pull request #1533 from markreddy/AIRFLOW-153-fix-typos,2
Pointing setup.py to then new repo,1
1.7.1.1,5
Bump version to unblock pypi release,5
docfix: Fix a couple of minor typos.,2
Merge branch '1526',7
[AIRFLOW-170] Add missing @apply_defaults,1
Merge pull request #1542 from Firehed/apply_defaults,7
[AIRFLOW-157] Make PR tool Py3-compat; add JIRA command- Adds Python3 compatibility (filter objects can't be indexed)- Adds JIRA command to close issues without merging a PR- Adds general usability fixes and starts cleaning up code,4
"[AIRFLOW-175] Run git-reset before checkout in PR toolIf the user made any changes, git checkout will fail because thechanges would be overwritten. Running git reset blows the changes away.",4
Merge pull request #1534 from jlowin/pr-tool-2,7
AIRFLOW-45: Support Hidden Airflow Variables,1
[AIRFLOW-176] Improve PR Tool JIRA workflow- Fix crash when non-integer IDs are passed- Improve workflow by always asking user if they  want to resolve another issue before exiting,0
Merge pull request #1530 from mattuuh7/hidden-fields,7
[AIRFLOW-178] Fix bug so that zip file is detected in DAG folder,2
AIRFLOW-167: Add dag_state option in cli,2
Merge pull request #1544 from jlowin/pr-tool-3,7
[AIRFLOW-176] remove unused formatting key,1
Merge pull request #1545 from jgao54/zip-bug-fix,0
Merge pull request #1546 from jlowin/pr-tool-4,7
Merge pull request #1541 from msumit/AIRFLOW-167,7
"AIRFLOW-181 Fix failing unpacking of hadoop by redownloadingcurl compares timestamps, but if the file is corrupt this canresult in hadoop tars that are never updated. This adds a retrywithout using the cache.",1
"[AIRFLOW-183] Fetch log from remote when worker returns 4xx/5xx responseDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-183This is mainly to make the behavior consistent when some log files havebeen deleted from the log folder. Without the change, the remote s3/gcsfallback will only trigger if the task ran on the local worker.Author: Yap Sok Ann <sokann@gmail.com>Closes #1551 from sayap/remote-log-remote-worker.",2
"[AIRFLOW-179] DbApiHook string serialization fails when string contains non-ASCII charactersDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-179In addition to correctly serializing non-ASCII characters the literal transformation also corrects an issue with escaping single quotes (').Note it was my intention to add another unit test to `test_hive_to_mysql` in `tests/core.py` however on inspection the indentations of the various methods seemed wrong, methods are nested and it's not apparent what class they refer to. Additionally it seems a number of the test cases aren't related to the corresponding class.For testing purposes I simply ran a pipeline which previously failed with the following exception,    [2016-05-26 22:03:39,256] {models.py:1286} ERROR - 'ascii' codec can't decode byte 0xc3 in position 230: ordinal not in range(128)    Traceback (most recent call last):      File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1245, in run    result = task_copy.execute(context=context)      File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/hive_to_mysql.py"", line 88, in execute    mysql.insert_rows(table=self.mysql_table, rows=results)      File ""/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py"", line 176, in insert_rows    l.append(self._serialize_cell(cell))      File ""/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py"", line 196, in _serialize_cell    return ""'"" + str(cell).replace(""'"", ""''"") + ""'""      File ""/usr/local/lib/python2.7/dist-packages/future/types/newstr.py"", line 102, in __new__    return super(newstr, cls).__new__(cls, value)    UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 230: ordinal not in range(128)and verified with the presence of the fix that the task succeeded and the resulting output was correct. Note currently from grokking the code base it seems that only `MySqlHook` objects call the the `insert_rows` method.Author: John Bodley <john.bodley@airbnb.com>Closes #1550 from johnbodley/dbapi_hook_serialization.",5
"Revert ""[AIRFLOW-179] DbApiHook string serialization fails when string contains non-ASCII characters""This reverts commit 87b4b8fa19cb660317198d74f6d51fdde0a7e067.Reverting as the method used in the dbapi hook is actually packagespecific to MySQLdb and would break the sqlite and mssql hooks.",1
AIRFLOW-168 Correct evaluation of @once scheduleIf the schedule @once was used with a start_date two dagrunswould be created as next_run_date would be none and comparedagainst dag.start_date. This patch fixes that by returningimmediately if a dagrun has already occured with an @onceschedule.,2
Merge branch 'AIRFLOW-168',7
AIRFLOW-190 Add codecov and remove download count,4
Do not add comments to pull requests,1
"[AIRFLOW-25] Configuration for Celery always requiredDear Airflow Maintainers,Please accept this PR that addresses the following issues:- AIRFLOW-25For now, if airflow.cfg has no [celery] section, all subcommands fail.This patch adds the default values for Celery-related propertiesas well as existing 'default_queue' and 'flower_port',so as to make all subcommands work and suppress ""not found in config""warnings even if [celery] section is omitted.Author: Kengo Seki <sekikn@apache.org>Closes #1558 from sekikn/AIRFLOW-25.",5
"[AIRFLOW-23] Support for Google Cloud DataProcDear Airflow Maintainers,Please accept this PR that addresses the following issues:- AIRFLOW-23Spark, Hadoop, PySpart, SparkSQL, Pig an Hive support of DataProc clusterAuthor: Alex Van Boxel <alex@vanboxel.be>Closes #1532 from alexvanboxel/AIRFLOW-23.",5
[AIRFLOW-195] : Add toggle support to subdag clearing in the CLI,2
Merge branch '1557',7
"[AIRFLOW-196] Fix bug that exception is not handled in HttpSensorDear Airflow Maintainers,Please accept this PR that addresses the following issues:- [*AIRFLOW-196*](https://issues.apache.org/jira/browse/AIRFLOW-196)If exception happens in poke function in HttpSensor, it is notwell handled that make the sensor finish successfully, which isincorrect obviously.Author: Junwei Wang <i.junwei.wang@gmail.com>Closes #1561 from junwei-wang/master.",5
AIRFLOW-202: Fixes stray print line,0
"[AIRFLOW-201] Fix for HiveMetastoreHook + kerberosDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-201Author: Alexey Ustyantsev <a.ustyantsev@corp.mail.ru>Closes #1563 from hudbrog/airflow_201.",5
Merge pull request #1564 from hudbrog/airflow_202,5
[AIRFLOW-185] Handle empty versions list,0
"Optimize and refactor process_dagThis patch addresses the following issues:get_active_runs was a getter that was also updatingto the database. This patch refactors get_active_runsinto two different functions that are part of DagRun.update_state update state of the dagrun based on thetaskinstances of the dagrun. verify_integrity checksand updates the dag run based on if the dag containsnew or missing tasks.Deadlock detection has been refactored to ensure thatdatabase does not get hit twice, in some circumstancesthis can reduce the time spent by 50%.process_dag has been refactored to use the functionsof DagRun reducing complexity and reducing pressure on thedatabase. In addition locking is now properly workingunder the assumption that the heartrate is longerthan the time process_dag spends.Two new TaskInstance states have been introduced. ""REMOVED""and ""SCHEDULED"". REMOVED will be set when taskinstancesare encountered that do no exist anymore in the DAG.This happens when a DAG is changed (ie. a new version).The ""REMOVED"" state exists for lineage purposes.""SCHEDULED"" is used when a Task that did not have a statebefore is sent to the executor. It is used by both thescheduler and backfills. This state almost removes therace condition that exists if using multiple schedulers:due to the fact UP_FOR_RETRY is being managed by theTaskInstance (I think that is the wrong place) is stillexists for that state.",0
Merge branch 'process_dag',2
"[AIRFLOW-155] Documentation of Qubole OperatorDear Airflow Maintainers,Please accept this PR that addresses the following issues:- *https://issues.apache.org/jira/browse/AIRFLOW-155*Thanks,SumitAuthor: Sumit Maheshwari <sumitm@qubole.com>Closes #1560 from msumit/AIRFLOW-155.",0
[AIRFLOW-187] Improve PR tool UX- Add command to setup_git_remotes- Add guard for running PR tool in a branch with uncommitted changes- Add option to edit squash commit message- Message styling for clarity- Improved error messages for PR Tool problems,0
Merge pull request #1565 from jlowin/pr-tool-5,7
[AIRFLOW-207] Improve JIRA auth workflow,1
[AIRFLOW-209] Add scheduler tests and improve lineage handlingThis patch adds schedule_dag and process_dag unittests. It alsofixes some minor bugs that were caught by these tests. Somesmall changes for readability.,4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-187] Improve PR Tool UXImproved styling for PR tool prompts,1
Merge pull request #1567 from jlowin/pr-tool-7,7
[AIRFLOW-114] Sort plugins dropdownCloses #1499 from varantz/sorting_plugins_in_dropdown.,4
[AIRFLOW-64] Add note about relative DAGS_FOLDERCloses #1474 from itajaja/patch-1.,2
"[AIRFLOW-211] Fix JIRA ""resolve"" vs ""close"" behaviorCloses #1571 from jlowin/pr-tool-8.",0
[AIRFLOW-206] Always load local log files if they exist,2
[AIRFLOW-206] Add commit to close PRCloses #1566GitHub requires the above string to properly mark the PR as closed,1
[AIRFLOW-214] Fix occasion of detached taskinstanceFor some reason occasionely taskinstanced could becomedetached from the database session. Now it uses a fresh sessionto ensure the taskinstances stay attached.,1
"[AIRFLOW-131] Make XCom.clear more selectiveXCOMs for a task were getting cleared on every run, no matter what.Selectively clear only when the task is actually going to be run.Closes #1570 from johnnason/AIRFLOW-131",1
[AIRFLOW-9] Improving docs to meet Apache's standards,2
Merge remote-tracking branch 'apache/master',7
"[AIRFLOW-68] Align start_date with the schedule_intervalThis particular issue arises because of an alignment issue betweenstart_date and schedule_interval. This can only happen with cron-basedschedule_intervals that describe absolute points in time (like “1am”) asopposed to time deltas (like “every hour”)In the past (and in the docs) we have simply said that users must makesure the two params agree. But this is counter intuitive. As in thesecases, start_date is sort of like telling the scheduler to“start paying attention” as opposed to “this is my first execution date”.This patch changes the behavior of the scheduler. The next run date ofthe dag will be treated as ""start_date + interval"" unless the start_dateis on the (previous) interval in which case the start_date will be thenext run date.",5
"[AIRFLOW-213] Add ""Closes #X"" phrase to commit messages",1
Merge pull request #1573 from jlowin/test-merge-pr,7
Merge pull request #1569 from mistercrunch/docs,2
"[AIRFLOW-218] Added option to enable webserver gunicorn access/err logsCloses #1577 from aoen/ddavydov/better_http_response_loggingAdded an option to enable gunicorn access/error logs.The default config will now log webserver errors/accesses to stderr.Also made the 404 page hostname text default page color instead ofwhite, since white text is pretty hard to see against a whitebackground.",0
[AIRFLOW-223] Make parametrable the IP on which Flower binds to,2
[AIRFLOW-142] setup_env.sh doesn't download hive tarball if hdp is specified as distroCloses #1518 from sekikn/AIRFLOW-142,1
Merge branch 'align_startdate',5
"[AIRFLOW-230] [HiveServer2Hook] adding multi statements supportChanging the library from pyhive to impyla broke the behavior where multiple statements, including statements that don't return results were previously supported and aren't anymore. impyla raises an exception if any of the statements doesn't return result.We have tasks that run multiple statements including DDL and want to run them atomically.Closes #1583 from mistercrunch/hooks_hive_presto[AIRFLOW-230] [HiveServer2Hook] adding multi statements support",1
[AIRFLOW-238] Make compatible with flask-admin 1.4.1The new flask-admin==1.4.1 release on 2016-06-13 breaks the Airflowrelease currently in Pypi (1.7.1.2). This fixes the edge case triggeredby this new release.* Closes #1588 on github,1
[AIRFLOW-171] Add upgrade notes on email and S3 to 1.7.1.2Closes #1587 from rfroetscher/upgrading_readme,1
[AIRFLOW-216] Add Sqoop Hook and OperatorThis patch adds a Sqoop hook and operator that implements Sqoop import.The hook is a wrapper around the sqoop 1 binary.* Closes #1576 from jwi078/sqoop_operator,1
"[AIRFLOW-231] Do not eval user input in PrestoHookRunning `eval` represent a security threat as the interpreter can behijacked by the service returning the string getting ""evaled"", in thiscase Presto. It turns out the code I'm changing here was written a longtime ago and misguided, casting a python object to a string and thenevaling it as a useless round trip.Closes #1584 from mistercrunch/security",1
"[AIRFLOW-222] Show duration of task instances in uiAt the moment mousing over a task instance in the Airflow tree viewonly shows the duration for completed task, and shows ""Duration: null""for running tasks.Instead the UI should show the current task instance's durationfor running tasks. This patch addresses this.Closes #1589 from sekikn/AIRFLOW-222",1
[AIRFLOW-241] Add testing done section to PR template,3
Merge pull request #1592 from aoen/ddavydov/add_testing_done_section_to_pr_template,3
"[AIRFLOW-225] Better units for task duration graphRight now the job duration window defaults to hours, which for short lived tasksresults in numbers out to five decimals. This patch adjusts the scale of the Y-axisin accordance with the maximum value of the durations to be shown.",1
[AIRFLOW-239] Fix tests indentationCloses #1591 from zodiac/deindent-tests,3
Merge branch '1582',7
[AIRFLOW-224] Collect orphaned tasks and reschedule themTasks can get orphaned if the scheduler is killed in the middleof processing the tasks or if the MQ queue is cleared withouta worker having picked these up. Now tasks do not get setto a scheduled state anymore if they have not been sent to theexecutor yet. Next to that a garbage collector scans the executorfor tasks not being present and reschedules those if needed.,1
[AIRFLOW-173] Initial implementation of FileSensorImplement a simple FileSensor to detect the creation of files and folderin a given filesystem (a la HDFSSensor)Closes #1543 from trixpan/AIRFLOW-173,5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-31] Use standard imports for hooks/operators,1
Merge pull request #1595 from sekikn/AIRFLOW-225,7
"Add Python 3 compatibility fixIn Python 3, errors don’t have a `message` attribute",0
Add Postmates to Airflow users listCloses #1599 from Syeoryn/masterAdd Postmates to Airflow users list,1
Merge pull request #1272 from jlowin/standard-imports,2
[AIRFLOW-256] Fix test_scheduler_reschedule heartratetest_scheduler_reschedule runs two schedulerjob quitefast after one another this sometimes is faster thanthe heartrate allows and thus the tasks will not getrescheduled and the test will fail. Fixed by settingheartrate to 0.,1
Merge remote-tracking branch 'apache/master',7
[AIRFLOW-234] make task that aren't `running` self-terminateCloses #1585 from mistercrunch/undeads,1
"[AIRFLOW-6] Remove dependency on HighchartsHighcharts' license is not compatible with the Apache 2.0license. This patch removes Highcharts in favor of d3,however some charts are not supported anymore.* This brings Maxime Beauchemin's work to master",1
Merge branch 'highcharts_to_d3',2
[AIRFLOW-31] Add zope dependencyCloses #1608 from jlowin/standard-imports-2.Also closes AIRFLOW-257.,2
[AIRFLOW-262] Simplify commands in MANIFEST.in,5
"[AIRFLOW-180] Fix timeout behavior for sensorsIn the previous state of the code, datetime.now was compared tostarted_at and seconds was pulled out. It turns out that the secondsattribute of a timedelta has a maximum of 86400 and the rolls up to 1 day.The unintended consequence is that timeout larger than 86400 areignored, with sensors running forever.To fix this we use the total_seconds method to get at the realtimedelta in seconds.",1
Merge pull request #1547 from artwr/artwr_fix_sensor_timeout,0
"[AIRFLOW-252] Raise Sqlite exceptions when deleting tasks instance in WebUIIf users who use SQLite as backend try to delete a task via browser, it fails with an exception.Though this is a bug on Flask-Admin's side basically, this patch provides a workaround for it.",1
Merge pull request #1612 from ajayyadava/262,7
[AIRFLOW-263] Remove temp backtick fileCloses #1613 from artwr/artwr_remove_backtick_file[AIRFLOW-263] Remove temp backtick fileThis removes the file erroneously introduced by the highchartsrefactor.Solves AIRFLOW-263,4
"[AIRFLOW-248] Add Apache license header to all files- Added Apache license header for files with extension (.service, .in, .mako, .properties, .ini, .sh, .ldif, .coveragerc, .cfg, .yml, .conf, .sql, .css, .js, .html, .xml.- Added/Replaced shebang on all .sh files with portable version - #!/usr/bin/env bash.- Skipped third party css and js files. Skipped all minified js files as well.Closes #1598 from ajayyadava/248",2
Merge pull request #1609 from sekikn/AIRFLOW-252,7
[AIRFLOW-162] Allow variable to be accessible into templatesCloses #1540 from alexvanboxel/AIRFLOW-162AIRFLOW-162 Allow variable to be accessible into templates,1
[AIRFLOW-244] Modify hive operator to inject analysis dataTesting Done:Test dags were run as backfills on an Airbnb Airflow dev box.This PR exposes task/dag id/run data through the HiveOperator foringestion by performance analysis tools like Dr. Elephant.Closes #1607 from paulbramsen/paulbramsen/modify_hive_operator_to_inject_analysis_data,5
[AIRFLOW-275] Update contributing guidelinesThe current guidelines do not reflect the move to Apache. Thisupdates the links and update the PR template.,5
[AIRFLOW-273] Create an svg version of the airflow logo.Closes #1619 from gwax/svg_logo,2
"[AIRFLOW-274] Add XCom functionality to GoogleCloudStorageDownloadOperatorUpdated GoogleCloudStorageDownloadOperator so that it has the option tostore the downloaded file's contents in an XCom instead of saving todisk. It now also takes filename as an optional argument instead of arequired argument, so it is not necessary to save to disk if you do notneed to.Closes #1618 from illop/risk_analytics",1
[AIRFLOW-280] clean up tmp druid table no matter if an ingestion job succeeds or notCloses #1624 from hongbozeng/cleanup_druidclean up tmp druid table no matter if the ingestion job success or not,4
"[AIRFLOW-278] Support utf-8 ecoding for SQLSupport utf-8 encoding for SQL queries, needed for Python 2users who have unicode strings inside the queriesCloses #1622 from biln/master",1
[AIRFLOW-283] Make store_to_xcom_key a templated field in GoogleCloudStorageDownloadOperatorCloses #1628 from illop/gcs_download_operator,1
"[AIRFLOW-200] Make hook/operator imports lazy, and print proper exceptions",2
"[AIRFLOW-277] Multiple deletions does not work in Task Instances view if using SQLite backendAIRFLOW-252 fixed the trash button in ""Task Instances"" view on Web UI,but ""With selected"" > ""Delete"" menu does still not working for the same reason.This patch fixes this problem by overriding Flask-Admin's method if the backend is SQLite.",0
Merge branch 'contributing',7
"[AIRFLOW-40] Add LDAP group filtering feature.It is now possible to filter over LDAP group (in the webinterface) when using the LDAP authentication backend.Note that this feature requires the ""memberOf""overlay to be configured on the LDAP server.Closes #1479 from dsjl/AIRFLOW-40",5
[AIRFLOW-285] Use Airflow 2.0 style imports for all remaining hooks/operators,1
[AIRFLOW-281] Add port to mssql_hookCloses #1626 from lukem-ow/master,1
Merge pull request #1586 from criccomini/AIRFLOW-200,5
"[ARFLOW-255] Check dagrun timeout when comparing active runsTimedout DagRuns and max active runs reached would be set to failed.Therefor, concurrency limits could be reached prematurely.Closes #1604 from reconditesea/klin-dagrun-timeout",2
[AIRFLOW-189] Highlighting of Parent/Child nodes in GraphsCloses #1554 from withnale/graph_highlight,5
"[AIRFLOW-246] Improve dag_stats endpoint queryFor now, accessing /dag_stats can take a relatively long time(e.g. over 20 seconds with less than a million rows on some environment).This patch replaces multiple LEFT OUTER JOINs with INNER JOINs and UNION ALLand improves that process by making it 3-5x faster.Closes #1610 from sekikn/AIRFLOW-246",1
Add an Apache Incubator Disclaimer and mocking modulesCloses #1634 from mistercrunch/mock_docsAdding an Apache Incubator Disclaimer and mocking modules,2
[AIRFLOW-243] Create NamedHivePartitionSensorCloses #1593 from zodiac/create-NamedHivePartitionSensor,1
[AIRFLOW-286] Improve FTPHook to implement context manager interfaceCloses #1632 from skudriashev/airflow-286,1
[AIRFLOW-296] template_ext is being treated as a string rather than a tuple in qubole operatorCloses #1638 from msumit/AIRFLOW-296,1
[AIRFLOW-269] Add some unit tests for PostgreSQLCloses #1616 from sekikn/AIRFLOW-269,3
[AIRFLOW-291] Add index for state in TI tableCloses #1635 from aoen/ddavydov/add_index_to_task_instance_state,1
[AIRFLOW-282] Remove PR Tool logic that depends on version formattingCloses #1625 from jlowin/pr-tool,2
"[AIRFLOW-100] Add execution_date_fn to ExternalTaskSensorCurrently, ExternalTaskSensor only supports querying execution_datesthat are either the same as the ExternalTaskSensor's execution_dateor a fixed interval from that date (using `execution_delta`). Thisadds the ability to provide a fn (`execution_date_fn`) that acceptsthe current execution_date and can return any desired date forquerying. This is much more flexible. For example, it couldsupply the last date of the previous month.",5
"[AIRFLOW-301] Fix broken unit testThis unit tests always fails on the last day of the month, since ittries to access a nonexistent day (like June 31st).",0
Add license to migration file,2
"[AIRFLOW-247] Add EMR hook, operators and sensors. Add AWS base hookCloses #1630 from rfroetscher/emr",1
Merge pull request #1641 from jlowin/external-task,7
"[AIRFLOW-187] Move ""Close XXX"" message to end of squash commit",4
[AIRFLOW-187] Fix typo in argument name,2
[AIRFLOW-187] Improve prompt styling,1
"[AIRFLOW-302] Improve default squash commit messagePreviously, we always used the PR title as the squash commit subject.Now, if the squash contains only one commit, then we use the commitmessage for the squash commit message. If the squash contains more thanone commit, we default to the old behavior (using the PR title). Westill ask if the user wants to include the PR body, but we only ask ifthey want to include the individual commits if there was more than one.",1
"[AIRFLOW-228] Handle empty version list in PR toolIf the filter matched no version names, it would return an empty listand the [0] index would fail.",0
"[AIRFLOW-260] Handle case when no version is foundIf no version is found, the “.name” attribute can’t be accessed,causing a crash.",1
"[AIRFLOW-260] More graceful exit when issues can't be closedPreviously, the “fail” function was called, which exited the entireprogram. By returning from this function, we allow the JIRA loop toresume and users can continue closing other issues.",0
"[AIRFLOW-284] HiveServer2Hook fix for cursor scope for get_resultsDear Airflow Maintainers,Please accept this PR that addresses the following issues:- *(replace with a link to AIRFLOW-X)*Detail:The function `get_results` implemented in the `HiveServer2Hook` does not execute multiple commands passed to it in a list, in the singular cursor scope. This has caused SQL statements that depend on the execution of `add jar` and `set` commands to fail as they are being executed in different cursor scopes which are not persistent.The code has been updated to have the cursor object persistent.Testing Done:Reminders for contributors:* Your PR's title must reference an issue on [Airflow's JIRA](https://issues.apache.org/jira/browse/AIRFLOW/). For example, a PR called ""[AIRFLOW-1] My Amazing PR"" would close JIRA issue #1. Please open a new issue if required!* Please squash your commits when possible and follow the [7 rules of good Git commits](http://chris.beams.io/posts/git-commit/#seven-rules).mistercrunchCloses #1629 from sherwian/hivehook_fx",1
[AIRFLOW-298] fix incubator diclaimer in docsCloses #1640 from mistercrunch/disclaimer_tweaks[AIRFLOW-298] fix incubator diclaimer in docs,2
"[AIRFLOW-314] Fix BigQuery cursor run_table_upsert methodCloses #1652 from mtagle/fix_bq_table_upsertBy default, bigquery will only return 50 tables when you ask for a listof all the tables in a datatset. If you are trying to upsert a tablethat exists, but you have more than 50 tables, the run_table_upsertmethod may conclude that the table doesn't exist, and try to insert it,and bigquery will error saying that the table does exist.This fix checks if the response has pagination data, and looks at allthe pages, rather than just the first one, to see if the table exists.",5
[AIRFLOW-308] Add link to refresh DAG within DAG view headerCloses #1646 from Firehed/refresh_on_dag,2
[AIRFLOW-24] DataFlow Java OperatorCloses #1648 from alexvanboxel/AIRFLOW-24,1
[AIRFLOW-311] Fix wrong path in CONTRIBUTING.mdCloses #1650 from skudriashev/airflow-311,0
[AIRFLOW-313] Fix code style for sqoop_hook.pyCloses #1651 from skudriashev/airflow-313,1
[AIRFLOW-307] Rename __neq__ to __ne__ python magic method.Closes #1649 from oza/AIRFLOW-307,5
[AIRFLOW-309] Add requirements of develop dependencies to docsCloses #1647 from oza/AIRFLOW-309,2
[AIRFLOW-307] There is no __neq__ python magic method.Closes #1645 from TheSAS/patch-1,5
[AIRFLOW-31][AIRFLOW-200] Add note to updating.mdAIRFLOW-31 and AIRFLOW-200 deprecated the old important mechanism and should be noted in UPDATING.mdCloses #1643 from jlowin/patch-1,5
[AIRFLOW-297] support exponential backoff option for retry delayCloses #1639 from jgao54/support-retry-backoff,1
"[AIRFLOW-264] Adding workload management for HiveDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-264CC: Original PR by Jparks2532https://github.com/apache/incubator-airflow/pull/1384Add workload management to the hive hook and operator.Edited operator_helper to avoid KeyError on retrieving conf values.Refactored hive_cli command preparation in a separate privatemethod.Added a small helper to flatten one level of an iterator to a list.Closes #1614 from artwr/artwr_fixing_hive_queue_PR",0
[AIRFLOW-316] Always check DB state for Backfill Job executionCloses #1654 from aoen/ddavydov/dont_skip_db_state_check_for_subdagAlways check DB state and not just the local state for backfill jobs fordetermining which task instances have not yet completed execution.This is to avoid potential race conditions with e.g. two backfill jobsrunning the same task instance.,1
Update README.md,2
"[AIRFLOW-321] Fix a wrong code example about tests/dagstests/dags/README.md has a code example such as""dag = dagbag.get(dag_id)"", but DagBag doesn't have a method called get.That should be fixed as get_dag.",2
Merge pull request #1642 from jlowin/pr-tool-1,7
Merge pull request #1659 from sekikn/AIRFLOW-321,7
Merge pull request #1657 from Shekharv/master,7
[AIRFLOW-327] Add rename method to the FTPHookCloses #1660 from skudriashev/airflow-327,1
[AIRFLOW-306] Add Spark-sql Hook and OperatorThis patch adds a hook and operator that allows execution of Spark-sql queries. The hook is a wrapper around the Spark-sql binary.Closes #1644 from danielvdende/spark_sql_operator,1
[AIRFLOW-315] Fix blank lines code style warningsCloses #1653 from skudriashev/airflow-315,2
Add blue-yonder to Airflow usersCloses #1661 from ctrebing/extend_list_of_companies_blue_yonder,1
[AIRFLOW-334] Fix using undefined variable,1
[AIRFLOW-337] Add __repr__ to VariableAccessor and VariableJsonAccessorThe VariableJsonAccessor and VariableAccessor were missing the __repr__function that leads to a VariableError when printing out the contextbeing passed to for example a PythonOperator.,1
Merge remote-tracking branch 'apache/master',7
[AIRFLOW-335] Fix simple style errors/warningsCloses #1665 from skudriashev/airflow-335,2
Merge branch '1664',7
Merge branch '1631',7
[AIRFLOW-341][operators] Add resource requirement attributes to operatorsThis PR adds optional resource requirements for tasks for use withresource managers such as Yarn and Mesos.Considerations:- I chose to force users to encapsulate resources in a resources objecte.g. Resources(cpu=1) instead of just cpu=1 in their dag attributes.This creates the pain of having to import Resources for almost everyDAG. I think this is kind of important for scoping/namespacing which weshould start doing.- Once resources are used by executors we need to add documentation forthese new resources (and examples)Testing Done:- New/existing unit testsplypaul artwr mistercrunch jlowin bolkedebruin criccominiCloses #1669 from aoen/ddavydov/ddavydov/augment_tasks_with_resources,5
AIRFLOW-339: Ability to pass a flower conf fileCloses #1671 from msumit/AIRFLOW-339,2
[AIRFLOW-340] Remove unused dependency on BabelCloses #1668 from kdeldycke/remove-unused-babel-deps,4
[AIRFLOW-349] Add metric for number of zombies killedCloses #1673 from aoen/ddavydov/metric_for_number_of_zombies_killed,1
[AIRFLOW-348] Fix code style warningsCloses #1672 from skudriashev/airflow-348,2
AIRFLOW-261 Add bcc and cc fields to EmailOperatorCloses #1670 from ajayyadava/261,1
[AIRFLOW-359] Pin flask-login to 0.2.11Closes #1679 from zodiac/xuanji_pin_ci_flask_login,2
[AIRFLOW-362] Import __future__ divisionFix integer division fencepost error on S3 chunked transfersCloses #1683 from clearclaw/master,0
"[AIRFLOW-356][AIRFLOW-355][AIRFLOW-354] Replace nobr, enable DAG only exists locally message, change edit DAG iconAddresses the following issues:- [https://issues.apache.org/jira/browse/AIRFLOW-356](https://issues.apache.org/jira/browse/AIRFLOW-356)- [https://issues.apache.org/jira/browse/AIRFLOW-355](https://issues.apache.org/jira/browse/AIRFLOW-355)- [https://issues.apache.org/jira/browse/AIRFLOW-354](https://issues.apache.org/jira/browse/AIRFLOW-354)- Replace `<nobr>` with `flexbox`- ""This DAG seems to be existing only locally"" now shows up- Change edit DAG icon from info to edit- Rename `dttm` variable to `file_last_changed_on_disk`- Rename `dags` variable to `webserver_dags`- Adds a comment clarifying what `self.file_last_changed` is- Clarifies what the `dag.last_expired` column represents- Refactors some previously very nested logic in `views.py` and adds comments- Properly indents `dags.html` and adds comments to it- Edit DAG icon changed- Home page now sort of responsive, no longer fixed width- User will occasionally see ""This DAG seems to be existing only locally"" message- Verify that edit dag button is now an edit icon and click on it- Resized home page, check that last column does not wrap![image](https://cloud.githubusercontent.com/assets/130362/17126889/2e7adb12-52b6-11e6-9a18-b31e424e4be8.png)Clean up html, replace nobr with flexboxRefactor HomeViewRename variables and update commentsCloses #1678 from zodiac/xuanji_refactor",4
"need to import login_user if we're going to use itlogin is broken in GHE for new users:```[2016-07-26 22:11:43,077] {github_enterprise_auth.py:199} ERROR -Traceback (most recent call last):  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/airflow/contrib/auth/backends/github_enterprise_auth.py"", line 188, in oauth_callback    'Null response from GHE, denying access.'airflow.contrib.auth.backends.github_enterprise_auth.AuthenticationError: Null response from GHE, denying access.[2016-07-26 22:12:12,313] {app.py:1423} ERROR - Exception on / [GET]Traceback (most recent call last):  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1817, in wsgi_app    response = self.full_dispatch_request()  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1477, in full_dispatch_request    rv = self.handle_user_exception(e)  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1381, in handle_user_exception    reraise(exc_type, exc_value, tb)  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/_compat.py"", line 33, in reraise    raise value  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1475, in full_dispatch_request    rv = self.dispatch_request()  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/flask/app.py"", line 1461, in dispatch_request    return self.view_functions[rule.endpoint](**req.view_args)  File ""/opt/virtualenvs/airflow/lib/python3.5/site-packages/airflow/contrib/auth/backends/github_enterprise_auth.py"", line 215, in oauth_callback    login_user(GHEUser(user))NameError: name 'login_user' is not defined```",2
Merge pull request #1686 from mylons/master,7
[AIRFLOW-331] modify the LDAP authentication config lines in  'Security' sample codesCloses #1674 from impangt/master,5
[AIRFLOW-379] Enhance Variables page functionality: import/export variablesAdd export option under 'with selected' menu to export selected variables to json.Add upload file option at top of page to import variables from json file.The decision was made to avoid flask admin's default export functionality because itdoes not handle the possibility of serialized jsons as variable values well.The import variables field should be made to look nicer.,2
Link to wiki in READMECloses #1695 from criccomini/readme-links,5
[AIRFLOW-373] Enhance CLI variables functionalityAdd export/import to/from json file option for CLI variable command.Add delete variable option for CLI variable command.,4
[AIRFLOW-381] Manual UI Dag Run creation: require dag_id field,2
Merge branch '1697',7
closes apache/mahout#ZZ *Won't fix*,0
closes apache/incubator-airflow#1691 *Fixed*,0
Merge branch '1696',7
merges apache/incubator-airflow#1696 *fixed*,0
merges apache/incubator-airflow#1691 *fixed*,0
"[AIRFLOW-160] Parse DAG files through child processesInstead of parsing the DAG definition files in the same process as thescheduler, this change parses the files in a child process. This helpsto isolate the scheduler from bad user code.Closes #1636 from plypaul/plypaul_schedule_by_file_rebase_master",2
[AIRFLOW-383] Cleanup example qubole operator dagCloses #1698 from yogesh2021/AIRFLOW-383,2
limit scope to user email only AIRFLOW-386,1
Merge pull request #1700 from mylons/limit_github_enterprise_scope,7
[AIRFLOW-375] Pylint fixesCloses #1690 from zodiac/xuanji_fix_pylint,0
[AIRFLOW-322] Fix typo in FAQ sectionCloses #1693 from ajayyadava/322,2
[AIRFLOW-388] Add a new chart for Task_Tries for each DAG,2
Merge branch '1701',7
[AIRFLOW-395] Remove trailing commas from resources in configCloses #1706 from aoen/ddavydov/fix_resources_typo,2
"[AIRFLOW-397] Documentation: Fix typo ""instatiating"" to ""instantiating""",2
Merge pull request #1705 from jalessio/master,7
[AIRFLOW-395] Fix colon/equal signs typo for resources in default configCloses #1708 from aoen/ddavydov/fix_colon_typo,2
[AIRFLOW-400] models.py/DAG.set_dag_runs_state() does not correctly set stateCloses #1710 from normster/set_dag_runs_state,2
"[AIRFLOW-399] - Remove dags/testdruid.pyRemove the current example in dags/testdruid.py. It requires the installation of an extraneous library (Hive) and executes a query against table that no one has. In its place, I am creating a simple DAG that only depends on DummyOperator and standard Python packages.Dear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-399Testing Done:Manual.aoen criccomini artwr jlowin bolkedebruinCloses #1709 from r39132/master",5
"[AIRFLOW-276] Gunicorn rolling restart- Tell gunicorn to prepend `[ready]` to worker process name once worker is ready (to serve requests) - in particular this happens after DAGs folder is parsed- Airflow cli runs gunicorn as a child process instead of `excecvp`-ing over itself- Airflow cli monitors gunicorn worker processes and restarts them by sending TTIN/TTOU signals to the gunicorn master process- Fix bug where `conf.get('webserver', 'workers')` and `conf.get('webserver', 'webserver_worker_timeout')` were ignored- Alternatively, https://github.com/apache/incubator-airflow/pull/1684/files does the same thing but the worker-restart script is provided separately for the user to run- Start airflow, observe that workers are restarted- Add new dags to dags folder and check that they show up- Run `siege` against airflow while server is restarting and confirm that all requests succeed- Run with configuration set to `batch_size = 0`, `batch_size = 1` and `batch_size = 4`Closes #1685 from zodiac/xuanji_gunicorn_rolling_restart_2",1
"[AIRFLOW-404] Retry download if unpacking fails for hiveTravis cache can have a faulty files. This results in buildsthat fail as they are dependent on certain components beingavailable, ie. hive. This addresses the issue for hive byredownloading if unpacking fails.",0
Merge remote-tracking branch 'apache/master',7
"[AIRFLOW-394] Add an option to the Task Duration graph to show cumulative timesThe current task duration chart only reports the task duration times for successful task attempts. If a task is retried multiple times, we don't get a sense of the cumulative duration for that task. With this feature, exposed as a checkbox UI option on the existing Task Duration chart, we can now display the total (a.k.a cumulative) task time for a task instance.Dear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-394Testing Done:- Added a unit test to CoreTests to check if task failures are correctly logged to TaskFail tableThe current task duration chart only reports the task duration times for successful task attempts. If a task is retried multiple times, we don't get a sense of the cumulative duration for that task. With this feature, exposed as a checkbox UI option on the existing Task Duration chart, we can now display the total (a.k.a cumulative) task time for a task instance.Closes #1712 from normster/cumulative",2
"[AIRFLOW-402] Remove NamedHivePartitionSensor static check, add docsAddresses the following issues:[https://issues.apache.org/jira/browse/AIRFLOW-402](https://issues.apache.org/jira/browse/AIRFLOW-402)Closes #1711 from zodiac/fix_named_hive_partition_sensor",0
"[AIRFLOW-78] airflow clear leaves dag_runsFix a bug in the scheduler where dag runs cleared via CLI would be picked up without checking max_active_dag_runs first, resulting in too many simultaneous dag runs.Dear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-78Testing Done:- Expanded the jobs.test_scheduler_verify_max_active_runs test to test if scheduler respects max_active_dag_runsFix a bug in the scheduler where dag runs cleared via CLI would be picked up without checking max_active_dag_runs first, resulting in too many simultaneous dag runs.Closes #1716 from normster/clear_dagrun",2
"Fix format string bugDear Airflow Maintainers,Please accept this PR that addresses the following issues:- [https://issues.apache.org/jira/browse/AIRFLOW-408](https://issues.apache.org/jira/browse/AIRFLOW-408)plypaulCloses #1719 from zodiac/xuanji/fix_format_bug",0
[AIRFLOW-413] Fix unset path bug when backfilling via picklePlease accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-413This fixes a bug when a pickled DAG is used in a backfill.Testing Done:- Existing unit tests + running a backfill command that used to fail  before on this errormistercrunch artwr plypaulCloses #1723 from aoen/ddavydov/fix_undefined_path_for_backfilling,0
"[AIRFLOW-412] Fix lxml dependencyDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-412Testing Done:-NoneCloses #1722 from normster/lxml",5
"[AIRFLOW-406] Sphinx/rst fixesDear Airflow Maintainers,- Fix some syntax errors in our sphinx/rst docstrings, which appears on pythonhosted.org- Move `xcom_push` documentation from constructor to class docstring- Rewrite some copy in `HivePartitionSensor` and `NamedHivePartitionSensor` docstrings- Fix `AirflowImporter` docstring that seems to have been automatically search-and-replacedCloses #1717 from zodiac/xuanji/fix_documentation",2
[AIRFLOW-414] Improve error message for missing FERNET_KEYCloses #1724 from r39132/more_descriptive_error,0
[AIRFLOW-407] Add different colors for some sensorsThese colors were added (leftmost is the default sensor color):https://cloud.githubusercontent.com/assets/130362/17538403/e57502e4-5e59-11e6-9a65-80134d1e77af.png- Create a DAG with all these operators and take a screenshotCloses #1718 from zodiac/xuanji/add_colours_to_sensors,1
"[AIRFLOW-410] Add 2 Q/A to the FAQ in the docsAlso changed the markup of Questions as sections to be directly linkable.I made sure the `rst` rendered nicely here:<img width=""690"" alt=""screen shot 2016-08-10 at 9 53 27 am"" src=""https://cloud.githubusercontent.com/assets/487433/17562690/c2dd3da0-5ee0-11e6-841b-9569eac4bf9a.png"">r39132  aoen plypaul[AIRFLOW-410] Adding 2 Q/A to the FAQ in the docsTyposCloses #1720 from mistercrunch/docs_faqs",2
"[AIRFLOW-369] Allow setting default DAG orientationDear Airflow Maintainers,Please accept this PR that addresses the following issues:[AIRFLOW-369](https://issues.apache.org/jira/browse/AIRFLOW-369)No tests infrastructure for webserver exists - verified manually.Thanks.Closes #1713 from spektom/dag_orientation",2
"[AIRFLOW-416] Use ordinals in README's company listDear Airflow Maintainers,Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-416Closes #1726 from r39132/doc",2
"[AIRFLOW-415] Make dag_id not found error clearerWhen a dag fails to parse and a user tries to run it e.g. via the command line airflow will give an error ""dag_id not found"" which is not clear since it sounds like the dag is completely missing rather than it just failing to parse. The message should be made clearer to address this possibility.The longer term fix is to not display exceptions with tracebacks to users like this but catch and give an error message.Please accept this PR that addresses the following issues:- https://issues.apache.org/jira/browse/AIRFLOW-415artwr mistercrunchCloses #1725 from aoen/ddavydov/clearer_dag_could_not_be_found_error",2
[AIRFLOW-69] Use dag runs in backfill jobsBackfill jobs create taskinstances without any associatedDagRuns. This creates consistency errors. This patch addressesthis issue and also makes the scheduler backfill aware.The scheduler makes sure to schedule new dag runs after thelast dag run including backfills. It will not pick upany tasks that are part of a backfill as those are consideredto be managed by the backfill process. This can be (and shouldbe) changed when backfill are running in a scheduled fashion.It doesn't deal with the remaining issue that backfills can bescheduled on top of existing dag runs and that due to thisTaskInstances can point to multiple DagRuns.Closes #1667 from bolkedebruin/backfill_dagrun,2
"[AIRFLOW-328][AIRFLOW-371] Remove redundant default configuration & fix unit test configurationAIRFLOW-328https://issues.apache.org/jira/browse/AIRFLOW-328Previously, Airflow had both a default template for airflow.cfg AND adictionary of default values. Frequently, these get out of sync (anoption in one has a different value than in the other, or isn’t presentin the other). This commit removes the default dict and uses theairflow.cfg template to provide defaults. The ConfigParser first readsthe template, loading all the options it contains, and then reads theuser’s actual airflow.cfg to overwrite the default values with any newones.AIRFLOW-371https://issues.apache.org/jira/browse/AIRFLOW-371Calling test_mode() didn't actually change Airflow's configuration! This actually wasn't an issue in unit tests because the unit test run script was hardcoded to point at the unittest.cfg file, but it needed to be fixed.[AIRFLOW-328] Remove redundant default configurationPreviously, Airflow had both a default templatefor airflow.cfg AND a dictionary of defaultvalues. Frequently, these get out of sync (anoption in one has a different value than in theother, or isn’t present in the other). This commitremoves the default dict and uses the airflow.cfgtemplate to provide defaults. The ConfigParserfirst reads the template, loading all the optionsit contains, and then reads the user’s actualairflow.cfg to overwrite the default values withany new ones.[AIRFLOW-371] Make test_mode() functionalPreviously, calling test_mode() didn’t actuallydo anything.This PR renames it to load_test_config() (toavoid confusion, ht @r39132).In addition, manually entering test_mode afterAirflow launches might be too late — someoptions have already been loaded (DAGS_FOLDER,etc.). This makes it so settingtests/unit_test_mode OR the equivalent env var(AIRFLOW__TESTS__UNIT_TEST_MODE) will load thetest config immediately, prior to loading therest of Airflow.Closes #1677 from jlowin/Simplify-config",5
[AIRFLOW-425] Add white fill for null state tasks in tree view.Closes #1727 from gwax/tree_null_task_fill,1
"[AIRFLOW-423][AIRFLOW-424][AIRFLOW-426] UX updates for PR ToolSome big UX improvements to the PR tool:AIRFLOW-423: commit messages are automatically wrapped at 50 characters. Paragraph breaks and tab indents are preserved. The first line of the commit is NOT forced to be 72 characters because handling this on subsequent lines is challenging. This is also deployed throughout the PR tool to make long status messages prettier.https://issues.apache.org/jira/browse/AIRFLOW-423AIRFLOW-424: this commit fixes a number of whitespace and formatting issues. It’s only contribution to functionality is that the JIRA resolution process is a little clearer.https://issues.apache.org/jira/browse/AIRFLOW-424AIRFLOW-426: I think this one is pretty cool. If the PR author mentions a JIRA issue anywhere in their PR (including on GitHub or buried in a commit), then the PR tool will automatically add it to the PR title. It even catches misformatted ones. So if the 20th commit mentions “AIRFLOW   - -426” somewhere in its body, you’ll still get “[AIRFLOW-426]” in the squash commit subject.https://issues.apache.org/jira/browse/AIRFLOW-426AIRFLOW-427: You can now run the PR tool from anywhere (previously you had to be `cd`'d to the Airflow git repo or set an env var)https://issues.apache.org/jira/browse/AIRFLOW-427[AIRFLOW-423] Add reflow function to PR toolThe reflow function takes a string and wrapsit to a specified width. It preservesparagraph breaks and 4+ space indentation.[AIRFLOW-424] Make JIRA process more clearSmall improvements to UX aroundclosing JIRA issues.Also a number of whitespace editsto clean up the PR tool code.[AIRFLOW-426] Automatically add JIRA references in PR ToolWhen squashing commits, the PR tool will automatically add a referenceto any JIRA issue found in the PR body or commits.[AIRFLOW-427] Automatically set AIRFLOW_GIT_LOCATIONPreviously, you had to run this file from the airflow gitdirectory unless you set an AIRFLOW_GIT env var.Closes #1728 from jlowin/pr-tool",1
[AIRFLOW-360] Fix style warnings in models.pyCloses #1681 from skudriashev/airflow-360,2
"[AIRFLOW-429] Add ZapierAdd Zapier. Add a Link. Correct Alphabetization.Correct MarkdownDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-429 Add Zapier](https://issues.apache.org/jira/browse/AIRFLOW-429)Testing Done:Not ApplicableChanges made: * Add Zapier.  Line 122. * Add a Link. The Github account for the Stripecontact was not included inconsistent with therest of the file.  Line 115. * Correct Alphabetization.  WePay was out oforder.  Line 117. * Correct Markdown. Spaces for Clairvoyant andClover Health were inconsistent with the rest ofthe document and markdown standards.  Lines 89 and90.Closes #1733 from drknexus/master",2
"[AIRFLOW-440] add iHeartRadio as Airflow user.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:[AIRFLOW-440 Add iHeartRadio](https://issues.apache.org/jira/browse/AIRFLOW-440)Testing Done:Not ApplicableChanges made:Add iHeartRadio. Line 100.Closes #1737 from yiwang/yi",1
"[AIRFLOW-439] Add Scaleway as Airflow user.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:* [AIRFLOW-439 Add Scaleway as an Airflow user](https://issues.apache.org/jira/browse/AIRFLOW-439).Testing:* Done: Not Applicable.Changes made:* Add Scaleway to `README.rst`. Line 111.Closes #1736 from kdeldycke/add-scaleway",1
"[AIRFLOW-329] Update Dag Overview Page with Better Status ColumnsRenamed 'Recent Statuses' to 'Recent Tasks'.Created 'DAG Stats' column. Created a cache tabledag_stats to hold data for 'DAG Stats' column withdirty bit for each row. Upon dagrun creation,state change, or deletion via web UI, appropriaterows will have dirty set to true and then dirtyrows will be refreshed with up to date data. Uponexecution of `airflow upgradedb` command, ifdag_stat is empty then it will be populated fromdag_run data.API endpoints in views.py have also been changed.'/dag_stats' has been renamed to '/task_stats',and endpoint reused for the 'DAG Stats' column.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-329Testing Done:- Added a unit test to core tests, tests thatdag_stats table is appropriately updated afterdagrun creation and state change-Added sanity check for '/dag_stats' and'/task_stats' API endpointsRenamed 'Recent Statuses' to 'Recent Tasks'.Created 'DAG Stats' column. Created a cache tabledag_stats to hold data for 'DAG Stats' column withdirty bit for each row. Upon dagrun creation,state change, or deletion via web UI, appropriaterows will have dirty set to true and then dirtyrows will be refreshed with up to date data. Uponexecution of `airflow webserver` command, all rowsin dag_stat will be dirtied and cleaned.API endpoints in views.py have also been changed.'/dag_stats' has been renamed to '/task_stats',and endpoint reused for the 'DAG Stats' column.Closes #1730 from normster/dagstat",2
"[AIRFLOW-446] Add Zenly as an airflow userSigned-off-by: Corentin Kerisit <c@42.am>Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-446 - Add Zenly as an Airflow user](https://issues.apache.org/jira/browse/AIRFLOW-446)Testing Done:- Non ApplicableSigned-off-by: Corentin Kerisit <c42.am>Closes #1749 from cerisier/new_user",1
"[AIRFLOW-431] Add CLI for CRUD operations on poolsDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-431Testing Done:- Added unit testsCloses #1735 from r39132/master",3
"[AIRFLOW-448] Adding Apigee as an official user of AirflowDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-448]https://issues.apache.org/jira/browse/AIRFLOW-448Closes #1750 from btallman/master",0
"[AIRFLOW-446][AIRFLOW-445] Adds missing dataproc submit optionsAdds support equivalent support for --files  and--py-files cli options.Signed-off-by: Corentin Kerisit <c@42.am>Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-445 - Missing dataproc operator submitoptions](https://issues.apache.org/jira/browse/AIRFLOW-445)Changes Done:- Adds support equivalent support for --files  and--py-files cli options.Testing Done:- Non existent for that operator yet, testedmanually by submitting a job with extra files andpy-filesSigned-off-by: Corentin Kerisit <c42.am>Closes #1748 fromcerisier/dataproc_additional_options",5
"[AIRFLOW-444] Add Google authentication backendAdd Google authentication backend.Add Google authentication information to securitydocs.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-444Testing Done:- Tested Google authentication backend locallywith no issuesThis is mostly an adaptation of the GHEauthentication backend.Closes #1747 from ananya77041/google_auth_backend",0
"[AIRFLOW-449] Add Whistle Labs as an Airflow userAdding Whistle Labs as a user of Airflow.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-449Testing Done:- N/ACloses #1751 from ananya77041/master",3
"[AIRFLOW-443] Make module names unique when importingModule names are normalized to ""unusual_prefix_<filename>"" in caseof non-ZIP imports. If there are two modules with the same filenamein a different directory they can overwrite each other. This isexhibited by the looking at the code from the web interface.This is fixed by using a hash of the filepath in the normalization.The hash is required to stay compatible with Python 2.",1
"[AIRFLOW-447] Store source URIs in Python 3 compatible listIn Python 2 map would generate a list, in Python 3this is no longer true.Downstream a job with source_uris set to a Python3 map object cannot be json-serializedwhen attempting to store the job in the database.This fix keeps the source_uris in a json-serializable list in both Python 2 and 3.Closes #1754 fromwaltherg/fix/python3_map_incompatibility",0
"[AIRFLOW-457] Adding Gusto as Airflow UserDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-457Testing Done:- N/ACloses #1757 from frankhsu/patch-2",3
[AIRFLOW-353] Fix dag run status update failureWhen multiple tasks gets removed update statusfails as the list is updated in place.Closes #1675 from yiqingj/master,5
Merge branch 'AIRFLOW-443',7
"[AIRFLOW-361] Add default failure handler for the Qubole OperatorDefault failure and retry handler in Qubole Operator will make surethat commands at Qubole and tasks in Airflow are in sync, and thereare no unwatched commands running at Qubole side.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:- *https://issues.apache.org/jira/browse/AIRFLOW-361*Thanks,SumitCloses #1682 from msumit/AIRFLOW-361",0
"[AIRFLOW-149] Task Dependency Engine + Why Isn't My Task Running ViewHere is the original PR with Max's LGTM:https://github.com/aoen/incubator-airflow/pull/1Since then I have made some fixes but this PR is essentially the same.It could definitely use more eyes as there are likely still issues.**Goals**- Simplify, consolidate, and make consistent the logic of whether or not  a task should be run- Provide a view/better logging that gives insight into why a task  instance is not currently running (no more viewing the scheduler logs  to find out why a task instance isn't running for the majority of  cases):![image](https://cloud.githubusercontent.com/assets/1592778/17637621/aa669f5e-6099-11e6-81c2-d988d2073aac.png)**Notable Functional Changes**- Webserver view + task_failing_deps CLI command to explain why a given  task instance isn't being run by the scheduler- Running a backfill in the command line and running a task in the UI  will now display detailed error messages based on which dependencies  were not met for a task instead of appearing to succeed but actually  failing silently- Maximum task concurrency and pools are not respected by backfills- Backfill now has the equivalent of the old force flag to run even for  successful tasks  This will break one use case:  Using pools to restrict some resource on airflow executors themselves  (rather than an external resource like a DB), e.g. some task uses 60%  of cpu on a worker so we restrict that task's pool size to 1 to  prevent two of the tasks from running on the same host. When  backfilling a task of this type, now the backfill will wait on the  pool to have slots open up before running the task even though we  don't need to do this if backfilling on a different host outside of  the pool. I think breaking this use case is OK since the use case is a  hack due to not having a proper resource isolation solution (e.g.  mesos should be used in this case instead).- To make things less confusing for users, there is now a ""ignore all  dependencies"" option for running tasks, ""ignore dependencies"" has been  renamed to ""ignore task dependencies"", and ""force"" has been renamed to  ""ignore task instance state"". The new ""Ignore all dependencies"" flag  will ignore the following:  - task instance's pool being full  - execution date for a task instance being in the future  - a task instance being in the retry waiting period  - the task instance's task ending prior to the task instance's    execution date  - task instance is already queued  - task instance has already completed  - task instance is in the shutdown state  - WILL NOT IGNORE task instance is already running- SLA miss emails will now include all tasks that did not finish for a  particular DAG run, even if  the tasks didn't run because depends_on_past was not met for a task- Tasks with pools won't get queued automatically the first time they  reach a worker; if they are ready to run they will be run immediately- Running a task via the UI or via the command line (backfill/run  commands) will now log why a task could not get run if one if it's  dependencies isn't met. For tasks kicked off via the web UI this  means that tasks don't silently fail to get queued despite a  successful message in the UI.- Queuing a task into a pool that doesn't exist will now get stopped in  the scheduler instead of a worker**Follow Up Items**- Update the docs to reference the new explainer views/CLI commandCloses #1729 from aoen/ddavydov/blockedTIExplainerRebasedMaster",1
"[AIRFLOW-463] Link Airflow icon to landing pageDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- *https://issues.apache.org/jira/browse/AIRFLOW-463*As of now the Airflow image icon on top leftdoesn't leads users to anywhere. It should takeusers to initial landing page, which is generallyhappened on most of the other sites.Closes #1764 from msumit/AIRFLOW-463",5
[AIRFLOW-466] Add Vente-Exclusive.com as an official Airflow userPlease accept this PR that addresses the followingissues:(https://issues.apache.org/jira/browse/AIRFLOW-466/).Trivial doc addCloses #1766 from alexvanboxel/feature/vex-as-user,1
"[AIRFLOW-472] Add liligo as an Airflow userDear Airflow Maintainers,Could you pls add liligo to the Airflow users listin the Readme?Thanks in advance!Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-472Closes #1769 from tromika/liligo",0
Removing highchart reference from NOTICE.txt,5
Adding DISCLAIMER.txt file to the repo,2
Bump version number to v1.7.2,5
Bumping to v1.7.2.dev0,5
Dropping .txt etension on repo's root files,2
Merge pull request #1772 from mistercrunch/v1_7_2,1
"[AIRFLOW-475] make the segment granularity in Druid hook configurableThe Druid hook now has hardcoded`segmentGranularity` - ""DAY"", we need itconfigurable for different use cases.mistercrunch aoen plypaulCloses #1771 fromhongbozeng/hongbo/segment_granularity",1
[AIRFLOW-476] Add link to Apache Incubation pageCloses #1774 from alexvanboxel/docs/incub-status,2
"[AIRFLOW-481] Add MarkovianDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-481Closes #1777 from waltherg/patch-2",0
"[AIRFLOW-483] Change print to logging statementDear Airflow Maintainers,Please accept this PR that addresses the followingissues:https://issues.apache.org/jira/browse/AIRFLOW-483Testing Done:This fix prevented the stdout from being spammedby the file content.Closes #1780 fromskogsbaeck/fix/gcs_download_operator",0
"[AIRFLOW-467] Allow defining of project_id in BigQueryHookIntroduced a toplevel table splitter that alsoallows for defining a project_id as a suffix seperated bythe legacy colon or the new dot. If the project is notdefined, the default project_id will be used.As it's so common of all the BigQuery classes(Hook, Cursor, ...) the same splitter is used over allthose classes.The documentation is adapted to allow defining thesuffix seperated as with the legacy colon as wellas with the new SQL dotted notation.The change is 100% backwards compatible. Unittests are added for all scenario's, including negativeand compatibility.Closes #1781 from alexvanboxel/feature/bq_tablename",1
[AIRFLOW-477][AIRFLOW-478] Restructure security section for clarityCloses #1775 from alexvanboxel/docs/security,2
[AIRFLOW-159] Add cloud integration section + GCP documentationCloses #1773 from alexvanboxel/feature/gcp-docs,2
[AIRFLOW-468] Update Panda requirement to 0.17.1BigQuery Hook requires at least Panda 0.17.1Closes #1767 from alexvanboxel/feature/panda-upgrade,1
[AIRFLOW-488] Fix test_simple failMake unittest test_simple pass on all platformsincluding MacOs.Closes #1782 from forevernull/master,4
"Revert ""[AIRFLOW-78] airflow clear leaves dag_runs""This reverts commit 197c9050ef3a142c18aa97819da48ee8cadbf8d8.Regressions were observed and tasks were not scheduled in case ofmax_dag_runs reached.",2
"[AIRFLOW-494] Add per-operator success/failure metricsAdds metrics for success/failure rates of each operator, that waywhen we e.g. do a new release we will have somesignal if there is a regression in an operator. Itwill also be useful if e.g. a user wants toupgrade their infrastructure and make sure thatall of the operators still work as expected.Testing Done:- Local staging and make sure that severaloperators successes/failures were accuratelyreflectedCloses #1785 from aoen/ddavydov/add_per_operator_success_fail_metrics",0
[AIRFLOW-469] Add MFG Labs as Airflow userHello.We'd like to be added as official Airflow users.RegardsCloses #1768 from dud225/patch-1,1
Do not include testing and directories in coverage reporting,3
Do not use migrations in coverage,3
[AIRFLOW-505] Support unicode characters in authors' namesCloses #1792 from jlowin/pr-tool-ascii,1
[AIRFLOW-498] Remove hard-coded gcp project idCloses #1786 from julianvmodesto/bug/AIRFLOW-498--fix-gcp-dataflow-hook,5
[AIRFLOW-509][AIRFLOW-1] Create operator to delete tables in BigQueryWe have a use case to delete BigQuery tables and views. This patchadds a delete operator that allows us to do so.Closes #1798 from illop/BigQueryDeleteOperator,4
"[AIRFLOW-512] Fix 'bellow' typo in docs & commentsDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-512Testing Done:- N/A, but ran core tests: `./run_unit_tests.shtests.core:CoreTest -s`Closes #1800 from dgingrich/master",3
"[AIRFLOW-191] Fix connection leak with PostgreSQL backendThis issue happens because job falls asleep duringheartbeat withoutclosing a session, which holds a connection. Thisturns databaseconnection into IDLE state, but doesn't releasesit for other clients,so when connection poll get exhausted, they getblocked for ~heartbeattimeframe causing global performance degradation.Closes #1790 from kxepal/AIRFLOW-191-postgresql-connection-leak",5
[AIRFLOW-91] Add SSL config option for the webserverSSL can now be enabled by providing certificateand key in the usualways (config file or CLI options). Providing thecert and key willautomatically enable SSL. The web server port willnot automaticallychange.The Security page in the docs now includes an SSLsection with basicsetup information.Closes #1760 from caseyching/master,5
[AIRFLOW-519] Add 99 as an Airflow userCloses #1795 from fbenevides/patch-1,1
[AIRFLOW-521] Add IFTTT as Airflow userWe at IFTTT currently use Airflow to monitor andschedule all our Data Pipelines and ETLs.Closes #1805 from apurvajoshi/patch-1,5
"[AIRFLOW-523] Add AltX as Airflow userDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-523](https://issues.apache.org/jira/browse/AIRFLOW-523)Testing Done:- Tests passing in Travis CICloses #1807 from PedroMDuarte/add-altx-airflow-user",1
"[AIRFLOW-198] Implement latest_only_operatorDear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-198Testing Done:- Local testing of dag operation withLatestOnlyOperator- Unit test addedCloses #1752 from gwax/latest_only",3
closes apache/incubator-airflow#1562 *fixed by another pr*,0
closes apache/incubator-airflow#1810 *no movement from submitter*,4
[AIRFLOW-531] Add T2 Systems as Airflow UserCloses #1812 from r39132/master,1
[AIRFLOW-535][AIRFLOW-1] Add OfferUp as an Airflow user.[]Closes #1814 from jghoman/AIRFLOW-535,1
[AIRFLOW-480] Support binary file download from GCSAllow for binary file download from Google Cloud StorageCloses #1793 from al-xv/feature/allow_binary_input_gcs_hook,1
"[AIRFLOW-525] Update template_fields in Qubole OpThere were couple of more fields in QuboleOperator which requiressupport of Jinja templating, so added these missedout fields as wellto template_fields. Also added a missing doc(about notify) and anexample of using macros.Closes #1808 from msumit/AIRFLOW-525",1
[AIRFLOW-537] Add WiseBanyan as Airflow user[]Closes #1815 fromkevinjmullen/AIRFLOW-537-WiseBanyan,1
"[AIRFLOW-530] Update docs to reflect connection environment var has to be in uppercaseDear Airflow Maintainers,Please accept this PR that addresses the followingissues:https://issues.apache.org/jira/browse/AIRFLOW-530Right now, the documentation does not clearlystate that connection names are converted touppercase form when searched in the environment(https://github.com/apache/incubator-airflow/blob/master/airflow/hooks/base_hook.py#L60-L60).This is confusing as the best practice in Airflowseems to be to define connections in lower caseform.Closes #1811 from danielzohar/connection_env_var",5
[AIRFLOW-542] Add tooltip to DAGs links iconsCloses #1817 from mmmaia/master,2
"[AIRFLOW-333][AIRFLOW-258] Fix non-module plugin components* Distinguish between module and non-module plugincomponents* Fix handling of non-module plugin components  * admin views, flask blueprints, and menu linksneed to not be    wrapped in modules* Fix improper use of zope.deprecation.deprecated  * zope.deprecation.deprecated does NOT supportclasses as    first parameter  * deprecating classes must be handled by callingthe deprecate    function on the class name* Added tests for plugin loading* Updated plugin documentation to match testplugin* Updated executors to always load plugins* More loggingCloses #1738 from gwax/plugin_module_fixes",0
"[AIRFLOW-544] Add Pause/Resume toggle buttonAdd Pause/Resume toggle button to DAG detailspage, so one does notneed to go back and forth to view the details anddo the action.Closes #1818 from msumit/AIRFLOW-544",2
[AIRFLOW-545] Add Bloc as Airflow user[]Closes #1819 from dpaola2/patch-1https://issues.apache.org/jira/browse/AIRFLOW-545,0
[AIRFLOW-378] Add string casting to params of spark-sql operatorFor parameters num_executors and executor_coresadd casts to strings to prevent issues when theseparameters are passed as integers (as comments specify).Also fix minor typo that breaks the use of num-executors param.Closes #1694 fromdanielvdende/spark_sql_operator_bugfixes,0
Add Auth0 to companies using AirflowWe are using airflow and loving it :),1
[AIRFLOW-539] Updated BQ hook and BQ operator to support Standard SQL.Closes #1820 from illop/master,1
Merge branch '1822',7
"[AIRFLOW-548] Load DAGs immediately & continuallyA recent commit has changed the scheduler behaviorthat it nowalways stops after a specified period of time. Theoperation scripts(systemd etc) are not updated for this behaviorand many users actuallyprefer to run the scheduler continously.Secondly the default behavior was changed to notpickup new DAGsimmediately, this has lead to confusion withusers.Closes #1823 from bolkedebruin/fix_duration",0
[AIRFLOW-358][AIRFLOW-430] Add `connections` cliThis PR adds a `connections` command to Airflow'sCLI. The new`connections` command hopes to make it easier toautomate Airflow'sdeployment to different environments. Users won'thave to directlyinteract with the database or input connectionsmanually on the UI.Closes #1802 from PedroMDuarte/connections-cli,5
closes apache/incubator-airflow#1734 *no movement from submitter*,4
"[AIRFLOW-556] Add UI PR guidelinesFor all PRs with UI changes, it is a requirementthat screenshots be included in the PR. UpdatePULL_REQUEST_TEMPLATE with this PR guideline forUI changes.Closes #1829 from r39132/master",4
closes apache/incubator-airflow#1789 *not needed since original code changed*,4
[AIRFLOW-500] Use id for github allowed teamsThe team string is not unique across an organizationand therefore we should use the long id instead.Closes #1788 from mylons/master,1
[AIRFLOW-550] Make ssl config check empty string safeThis config check on the ssl certificate makes itsafe for empty string. The empty string is providedby default configuration settings and could causethe webserver not starting up.Closes #1824 from alexvanboxel/bugfix/airflow-550,0
[AIRFLOW-554] Add Jinja support to Spark-sqlAllow SQL passed to Spark-SQL operator to be JinjatemplatedCloses #1828 fromdanielvdende/spark_sql_operator_jinja,1
[AIRFLOW-553] Fix load path for filters.jsCloses #1827 from msumit/AIRFLOW-553,0
"[AIRFLOW-518] Require DataProfilingMixin for Variables CRUDMany of us use the ""Variable"" model CRUD(create/update/delete) as a k/vstore to power frameworks that read these valuesto dynamically generatepipelines.With the basic ""LoginMixin"" role (lowest level ofaccess to Airflow)having access to the Variable CRUD, people couldeasily alter a Variableto run arbitrary code on the platform, dependingon how variables areuse in that environment.It's a safer bet to elevate CRUD on Variable toDataProfilingMixin, andmake sure that the lowest level of access cannotinterfere with theseVariables.Closes #1804 from mistercrunch/elevate_variables",1
[AIRFLOW-560] Get URI & SQLA engine from ConnectionCloses #1256 from gwax/sqlalchemy_conn,1
closes apache/incubator-airflow#1590 *no movement from submitter*,4
closes apache/incubator-airflow#1770 *PR abandonned by submitter*,5
[AIRFLOW-567] Add Easy Taxi as an Airflow userCloses #1835 fromWesleyBatista/docs/update_readme_links,5
closes apache/incubator-airflow#1702 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1580 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1531 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1787 *PR abandonned by submitter*,5
[AIRFLOW-577] Output BigQuery job for improved debuggingCloses #1838 from waltherg/fix/bq_error_message,0
[AIRFLOW-575] Clarify tutorial and FAQ about `schedule_interval` always inheriting from DAG object- Update the tutorial with a comment helping to explain the use of default_args andinclude all the possible parameters in line- Clarify in the FAQ the possibility of an unexpected default `schedule_interval`in caseairflow users mistakenly try to overwrite the default `schedule_interval` in a DAG's`default_args` parameter,2
Merge pull request #1402 from lauralorenz/schedule_interval_default_args_docs,2
closes incubator-airflow/airflow#1841 *Closing dummy PR*,5
closes  apache/incubator-airflow#1841 *Closing dummy PR*,5
[AIRFLOW-579] Mention BlaBlaCar as Airflow user[]Closes #1840 from wmorin/feat-mention-blablacar-as-user,1
closes apache/incubator-airflow#837 *PR abandonned by submitter*,5
"[AIRFLOW-96] s3_conn_id using environment variableDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- [AIRFLOW-96](https://issues.apache.org/jira/browse/AIRFLOW-96) : allow parameter ""s3_conn_id"" ofS3KeySensor and S3PrefixSensor to be defined usingan environment variable.Actually, S3KeySensor and S3PrefixSensor use theS3hook, which extends BaseHook. BaseHook hasget_connection, which looks a connection up :- in environment variables first- and then in the databaseCloses #1517 from dm-tran/fix-jira-airflow-96",0
closes apache/incubator-airflow#1703 *Obsolute PR*,5
[AIRFLOW-583] Fix decode error in gcs_to_bqCloses #1845 fromJalepeno112/bug/schema_object_decode_error,0
"[AIRFLOW-385] Add symlink to latest scheduler log directoryCreate a symbolic link to the directory contaningthe latest scheduler logs, and update the linkwhen the target changes.Update the test_scheduler_job test case to verifythat the symbolic link is created.Implementation:- Create a symbolic link to the directorycontaining the latest scheduler logs, and updatethe link when the target changes.Testing Done:- Extend test_scheduler_job test case to verifythat the correct symbolic link is created.Closes #1842 from vijaysbhat/latest-log-symlink",5
closes apache/incubator-airflow#652 *no movement from submitter*,4
closes apache/incubator-airflow#1488 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1392 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1344 *PR abandonned by submitter*,5
closes apache/incubator-airflow#1353 *PR abandonned by submitter*,5
closes apache/incubator-airflow#957 *PR abandonned by submitter*,5
"[AIRFLOW-319]AIRFLOW-319] xcom push response in HTTP OperatorAdds optional parameter to push response of theHTTP Operator to XComDocumentation suggests that any operator's executemethod that returnsvalue should push it into XCom:http://airflow.incubator.apache.org/concepts.html#xcoms> In addition, if a task returns a value (eitherfrom its Operator’sexecute() method, or from a PythonOperator’spython_callablefunction), then an XCom containing that value isautomatically pushed.Closes #1658 from jzelenkov/AIRFLOW-319_xcom_push_http_response",5
"[AIRFLOW-227] Show running config in config viewRight now the config page (when set to display)just loads the airflow.cfg and lists that.So any configuration values that are set viaan environment variable are not shown.This patch makes all configuration valueswhich are loaded via any means shown,only if expose_config is set to true.Closes #1597 from sekikn/AIRFLOW-227",1
[AIRFLOW-589] Add templatable job_name[]The jobname is the name that will appear in theDataProc web console.It's helpfull to have a one-to-one mapping betweenthe airflow task andthe job running on the cluster. Adding a templatedparameter will allowyou to customize how airflow will construct thejobname.The default is to add the {{task_id}} +{{ds_nodash}} + random hash.Closes #1847 from alexvanboxel/feature/airflow-589-dataproc-templated-job-name,5
[AIRFLOW-587] Fix incorrect scope for Google Auth[]The requested scope is incorrect when calling theGooge API. Thissplits the scope defines as a single script intoan array.Closes #1850 from alexvanboxel/bugfix/airflow-587-google-auth,0
[AIRFLOW-592] example_xcom import Errorfix exmple_xcom import error:AttributeError: 'module' object has no attribute'python_operator'Closes #1853 from forevernull/master,1
[AIRFLOW-588] Add Google Cloud Storage Object sensor[]The Cloud Storage sensor will check for theexistence if an object in a bucket. It will wait till the object existsbefore continuing.Closes #1849 from alexvanboxel/feature/airflow-588-gcs-sensor,1
closes apache/incubator-airflow#1432 *PR abandoned by submitter*,5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
closes apache/incubator-airflow#1339 *No movement from submitter*,4
closes apache/incubator-airflow#1434 *No movement from submitter*,4
closes apache/incubator-airflow#1572 *No movement from submitter*,4
closes apache/incubator-airflow#1758 *No movement from submitter*,4
"[AIRFLOW-599] Adding spotify to Airflow usersDear Airflow Maintainers,Please accept this PR that addresses the followingissues:- Adds Spotify to list of Airflow usersCloses #1855 from znichols/spotify_use",1
closes apache/incubator-airflow#746 *no movement from submitter*,4
closes apache/incubator-airflow#772 *no movement from submitter*,4
closes apache/incubator-airflow#746 *no movement from submitter*,4
closes apache/incubator-airflow#908 *no movement from submitter*,4
closes apache/incubator-airflow#989 *no movement from submitter*,4
closes apache/incubator-airflow#1276 *no movement from submitter*,4
closes apache/incubator-airflow#1274 *no movement from submitter*,4
closes apache/incubator-airflow#1301 *obsolete*,5
closes apache/incubator-airflow#1379 *obsolete*,5
closes apache/incubator-airflow#1384 *obsolete*,5
[AIRFLOW-453] Add XCom Admin PageCloses #1756 from msumit/AIRFLOW-453,1
"[AIRFLOW-586] test_dag_v1 fails from 0 to 3 a.m.dags/test_dag.py tries to set START_DATE to 3hours before usingdatetime.replace, but it doesn't support minusvalue as argument.So we have to use timedelta instead of simplenumeric subtraction.Closes #1852 from sekikn/AIRFLOW-586",1
"[AIRFLOW-597] Check if content is None, not false-equivalentCloses #1856 from gwax/non_boolean_templates",5
[AIRFLOW-580] Prevent landscape warning on .formatPrevent landscape warning for use of .format in loggingCloses #1843 from robin-miller-ow/DisableLoggingStringFormattingStyleWarningInLandscapeIO,2
closes apache/incubator-airflow#1491 *dupe of #1497*,5
[AIRFLOW-600] Added BandwidthX as a user of AirflowCloses #1857 from dineshdsharma/master,1
"[AIRFLOW-585] Fix race condition in backfill execution loopA subtle race condition in the backfill executionloop givesrise to occasional deadlocks, causing Travis CIbuilds torandomly fail. The root cause is unsynchronizedaccess toindividual task instance states for a DAG run inthe executioninner loop.The fix involves atomically reading the state ofall taskinstances for a DAG run once at the beginning ofeveryiteration of the inner loop.Closes #1846 from vijaysbhat/travis-ci-debugging",0
[AIRFLOW-568] Fix double task_stats count if a DagRun is activeCloses #1836 from dgies/master,2
[AIRFLOW-582] Fixes TI.get_dagrun filter (removes start_date)Closes #1844 from btallman/ti_get_dagrun,2
"[AIRFLOW-370] Create AirflowConfigException in exceptions.pyAirflowConfigException should be created inthe exceptions utility file, not inconfiguration.py.All exceptions should be created in`exceptions.py`.https://issues.apache.org/jira/browse/AIRFLOW-370Closes #1689 from jlowin/refactor-exception",4
"[AIRFLOW-604] Revert .first() to .one().one() enforces the integrity of airflow as weexpecta tuple to be returned here. If not the databaseisinconsistent and airflow should error out.Dear Airflow Maintainers,.one() enforces the integrity of airflow as weexpecta tuple to be returned here. If not the databaseisinconsistent and airflow should error out.partially reverts: https://github.com/apache/incubator-airflow/pull/1730Closes #1858 from bolkedebruin/AIRFLOW-604",4
closes apache/incubator-airflow#1391 *closed for inactivity*,5
closes apache/incubator-airflow#1497 *closed for inactivity*,5
closes apache/incubator-airflow#1439 *fixed by other PR:#1760*,0
closes apache/incubator-airflow#915 *closed for inactivity*,5
closes apache/incubator-airflow#1692 *closed for inactivity*,5
[AIRFLOW-590] Set module parameter in OracleHookCloses #1848 from gtoonstra/oracle_action_module,1
[AIRFLOW-552] upgrade funcsigs to 1.0.2Closes #1826 from jedipi/improvements/upgrade-funcsigs,1
"[AIRFLOW-551] pin flask to >=0.11, <0.12Closes #1825 from jedipi/improvement/upgrade-flask",1
[AIRFLOW-411] Add Python3 support to hipchat_operatorCloses #1721 from d-lee/hipchat_operator_python3,1
[AIRFLOW-612] Move resources/articles links to wikiCloses #1863 from mistercrunch/docs_links,2
[AIRFLOW-613][AIRFLOW-1] Add Astronomer as Airflow userCloses #1864 from schnie/master,1
[AIRFLOW-614][AIRFLOW-1] Add Madrone as Airflow userCloses #1866 from mbreining/master,5
closes apache/incubator-airflow#1797 *closed for inactivity*,5
[AIRFLOW-609] Add application_name to PostgresHookCloses #1861 fromgtoonstra/postgres_application_name,1
[AIRFLOW-606] Add requirements.txt for airflow-prCloses #1859 from zodiac/dev_pr_requirements,1
[AIRFLOW-615] Set graph glyphicon firstCloses #1867 from NeckBeardPrince/glyphicon_resort,1
[AIRFLOW-566] Add timeout while fetching logsCloses #1834 from msumit/AIRFLOW-566,2
closes apache/incubator-airflow#1538 *Not acceptable*,2
closes apache/incubator-airflow#1603 *PR abandonned by submitter*,5
[AIRFLOW-179] Fix DbApiHook with non-ASCII charsString serialization fails when string contains non-ASCII charactersCloses #1553 fromjohnbodley/dbapi_hook_serialization-remedy,5
[AIRFLOW-616][AIRFLOW-617] Minor fixes to PR tool UX[AIRFLOW-616] Store credentials for life of script[AIRFLOW-617] Fix redundant prompt text in PR toolCloses #1868 from jlowin/pr-tool-fixes,0
"[AIRFLOW-422] Add JSON endpoint for task infoExpose task information in an API endpoint forusage in other tools.This API is located in the experimental endpoints.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:https://issues.apache.org/jira/browse/AIRFLOW-422Testing Done:- Added a simple unit test.Expose task information in a web endpoint forusage in other tools.Closes #1740 from saguziel/master",5
"[AIRFLOW-618] Cast DateTimes to avoid sqllite errorsFor some reason, sqllite does not always matchdatetimes correctly.This causes unit test failures, in particular thetrigger_dagrun test.Casting the datetime explicitly appears to solvethe issue.Closes #1871 from jlowin/failing-sqlite-test",3
"[AIRFLOW-619] Fix exception in Gannt chartIf a TaskInstance has no end_date, usedatetime.now()Closes #1874 from dgies/master",5
[AIRFLOW-611] source_format in BigQueryBaseCursorCheck source_format in BigQueryBaseCursorThe edits to `bigquery_hook.py` are made to`BigQueryBaseCursor`.Closes #1873 from Jalepeno112/bug/AIRFLOW-611,0
"[AIRFLOW-623] LDAP attributes not always a listSometimes the search attributes that come back arenot a list always, but a string, so we need tocheck for an exact match as well as if it's a listCloses #1876 from wyndhblb/master",5
[AIRFLOW-507] Use Travis' ubuntu trusty for CITravis' ubuntu trusty provides a more up to dateenvironment for CI. It allows for better testingby integration more services like kerberos andcelery. Also it comes closer to actual productionenvironments (e.g. MySQL 5.6).,3
"[AIRFLOW-504] Store fractional seconds in MySQL tablesBoth utcnow() and now() return fractional seconds. Theseare sometimes used in primary_keys (eg. in task_instance).If MySQL is not configured to store these fractional secondsa primary key might fail (eg. at session.merge) resulting ina duplicate entry being added or worse.Postgres does store fractional seconds if left unconfigured,sqlite needs to be examined.",5
[AIRFLOW-570] Pass root to date form on ganttCloses #1837 from forklady42/gantt-root,5
[AIRFLOW-561] Add RedshiftToS3Transfer operatorAdd a redshift operator to unload tables from redshift into a single CSV in s3 with headersCloses #1831 from jackar/redshift-to-s3-csv,1
[AIRFLOW-591] Add datadog hook & sensorCloses #1851 from gtoonstra/contrib_datadog,5
closes apache/incubator-airflow#1763 *submitter will reopen when he has more time*,5
closes apache/incubator-airflow#1753 *example DAG not functional*,1
[AIRFLOW-626][AIRFLOW-1] HTML Content does not show up when sending email with attachmentCloses #1880 from illop/send_email_mimetype,5
"Revert ""[AIRFLOW-626] HTML Content does not show up when sending email with attachment""This reverts commit 55af3e04f8aa2062715370c8feec10308938715e.Master is currently broken as shown on https://travis-ci.org/apache/incubator-airflow/jobs/175858834======================================================================FAIL: test_custom_backend (tests.EmailTest)----------------------------------------------------------------------Traceback (most recent call last):  File ""/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/mock/mock.py"", line 1305, in patched    return func(*args, **keywargs)  File ""/home/travis/build/apache/incubator-airflow/tests/core.py"", line 1927, in test_custom_backend    send_email_test.assert_called_with('to', 'subject', 'content', files=None, dryrun=False, cc=None, bcc=None)  File ""/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with    six.raise_from(AssertionError(_error_message(cause)), cause)  File ""/home/travis/build/apache/incubator-airflow/.tox/py27-cdh-airflow_backend_sqlite/lib/python2.7/site-packages/six.py"", line 718, in raise_from    raise valueAssertionError: Expected call: mock('to', 'subject', 'content', bcc=None, cc=None, dryrun=False, files=None)Actual call: mock('to', 'subject', 'content', bcc=None, cc=None, dryrun=False, files=None, mime_subtype=u'mixed')",2
"[AIRFLOW-464] Add setdefault method to VariableIn order to assist with environment migrations, weadded a setdefault method tothe Variable object. This allows default variablesto be created (and thenedited) with less chance for typos/copy+pastebugs.Also changed Variable.get ValueError exception toa KeyErrorVariable.setdefault(key, default,deserialize_json=[True|False]) returns either thevalue stored in Variable(key) or setsVariable(key) = default and returns default.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:- _https://issues.apache.org/jira/browse/AIRFLOW-464_This was changed from adding a create_if_none flagto Variable.get based on feedback on this PR.Testing Done:- Added a test to test/core.py to cover thisfunctionalityCloses #1765 from btallman/CreateIfNone_feature",1
[AIRFLOW-629] stop pinning lxmlCloses #1882 from jedipi/improvement/stop-pinning-lxml,5
[AIRFLOW-533] Set autocommit via set_autocommit Delegate setting autocommit in insert_rows to set_autocommitCloses #1813 from thyming/fix-insert-rows-autocommit,0
[AIRFLOW-631] Add HelloFresh to Airflow UsersAdded HelloFresh to companies using AirflowCloses #1883 from tammymendt/add-company,1
[AIRFLOW-632] Add Stackspace to airflow users listCloses #1884 from dbkegley/master,5
[AIRFLOW-626][AIRFLOW-1] HTML Content does not show up when sending email with attachmentCloses #1885 from illop/send_email_mimetype,5
"[AIRFLOW-633] Show TI attributes in TI viewShow task instance properties in addition to taskproperties in the task instance view. The primaryreason for this is so that we can see things likethe host a task ran on without opening the logfile (which is huge in the case of long-running/persistent jobs, especially e.g due to theconstant airflow heartbeat). Pagination of the logwould help too but this change is still necessaryand a quicker win.I think ideally we want to only show an explicitsubset of fields, but for now making behaviorconsistent with how task (not task instance)attributes are displayed. Going to merge to opensource once this is merged.Testing Done:- Airbnb production webserver<img width=""1200"" alt=""screen shot 2016-11-15 at11 34 30 am"" src=""https://cloud.githubusercontent.com/assets/1592778/20324808/dae2861a-ab36-11e6-8bfa-3942ed529856.png"">Closes #1886 fromaoen/ddavydov/add_ti_info_to_ti_view",5
closes apache/incubator-airflow#1761 *closed for inactivity*,5
closes apache/incubator-airflow#1407 *closed for inactivity*,5
"[AIRFLOW-130] Fix ssh operator macosxCopy existing shell environment instead ofoverwritingTesting Done:- Unit tests previously did not pass on Mac OS X,now pass.Fix typos in commentsFix failing test when setting env for SSHHook onMac OS XAdd logging and use os.environ.copy() to set envin popen fixes test.Move import to top of fileCloses #1778 from jbhsieh/fix_ssh_operator_macosx",0
[AIRFLOW-343] Fix schema plumbing in HiveServer2HookAllow HiveServer2Hook to be used on other databases than default.Testing Done:- Added new unit test coverage in HiveOperator tocover these changes.  (This is the establishedtesting norm for HiveHook).Closes #1743 from gr8routdoors/airflow_343,1
close apache/incubator-airflow#1340 *superseded by other PRs*,5
close apache/incubator-airflow#1579 *closed for inactivity*,5
close apache/incubator-airflow#1529 *closed for inactivity*,5
closes apache/incubator-airflow#1410 *PR abandonned by submitter*,5
[AIRFLOW-137] Fix max_active_runs on clearing tasks,1
Merge pull request #1870 from gtoonstra/maxactiveruns_fix,0
[AIRFLOW-634] Add lumoslabs to readmeCloses #1887 from rfroetscher/add_lumoslabs,1
[AIRFLOW-635] Encryption option for S3 hookS3 gives the option of storing objects encryptedon the server side. Thischange exposes the boto S3 encrypt option in theload_file method of theS3 hook. It also updates missing documentation forthe load_string method.Closes #1888 from kerzhner/master,2
closes apache/incubator-airflow#1744 *No movement on PR*,4
"[AIRFLOW-565] Fixes DockerOperator on Python3.xThe issue is that `self.cli.pull()` returns`bytes()`, and not a string. Then,when we try to pass that to `json.loads()`, itraises an exception.The fix is to convert the bytes to a string bydecoding it as ""utf-8"". We'rehardcoding the encoding because, by the JSONschema, a JSON should encoded inUTF-8, UTF-16 or UTF-32. Considering we're onlypulling images from Dockerservers, we can be relatively safe that they'llbehave correctly.Closes #1832 from vitorbaptista/bug/fixes-AIRFLOW-565",0
[AIRFLOW-514] hive hook loads data from pandas DataFrame into hive and infers typesCloses #1801 from danfrankj/master,5
closes apache/incubator-airflow#1816 *No movement on PR*,4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-628] Adding SalesforceHook to contrib/hooksAlso added a salesforce option to setup.pyCloses #1881 from Jalepeno112/feature/salesforce-hook,1
"[AIRFLOW-347] Show empty DAG runs in tree viewUse DAG runs to construct the list of dates todisplay in the tree view. The old logic usedexecution dates from task instances to constructthe list of dates, which meant that empty DAG runswould not show up.Closes #1892 fromvijaysbhat/tree_view_empty_dag_runs",2
[AIRFLOW-375] Fix pylint errorsCloses #1893 from zodiac/landscape_errors,0
closes apache/incubator-airflow#1637 *Closed for inactivity*,5
closes apache/incubator-airflow#1425 *Closed for inactivity*,5
[AIRFLOW-639]AIRFLOW-639] Alphasort package namesCloses #1895 from zodiac/alphasort_requirements,1
[AIRFLOW-640] Install and enable nose-ignore-docstringCloses #1896 from zodiac/nose-ignore-docstring,2
[AIRFLOW-638] Add schema_update_options to GCP opsCloses #1891 fromJalepeno112/feature/gcs_to_bq_schemaUpdateOptions,5
[AIRFLOW-643] Improve date handling for sf_hookCloses #1898 from Jalepeno112/feature/salesforce-hook-coerce,1
"[AIRFLOW-510] Filter Paused Dags, show Last Run & Trigger DagModify the HomeView to filter out paused dags ifwanted (and a config value to setthe default), display the last run datetime oneach dag, and allow externally triggeringthe dag from an iconCloses #1833 frombtallman/HomeView_Improvements_feature",1
[AIRFLOW-650] Adding Celect to user listCloses #1901 from superdosh/master,1
closes apache/incubator-airflow#1900 *Not a bug*,0
[AIRFLOW-651] Hotfix setup.pyCloses #1902 from r39132/hotfix,0
[AIRFLOW-345] Add contrib ECSOperatorCloses #1894 from poulainv/ecs_operator,1
"[AIRFLOW-652] Remove obsolete endpointviews.py exposes an endpoint /sandbox, but it'sunused. This commit removes that for code cleanup.Closes #1904 from sekikn/AIRFLOW-652",4
"[AIRFLOW-653] Add some missing endpoint testsWebUiTests lack tests for some endpoints.This commit adds tests for /admin/airflow/headers,/admin/airflow/noaccess and/admin/airflow/pickle_info.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-653Testing Done:- All unit tests passed.WebUiTests lack tests for some endpoints.This commit adds tests for /admin/airflow/headers,/admin/airflow/noaccess and/admin/airflow/pickle_info.Closes #1903 from sekikn/AIRFLOW-653",5
[AIRFLOW-489] Add API FrameworkThis implements a framework for API calls to Airflow. Currentlyall access is done by cli or web ui. Especially in the contextof the cli this raises security concerns which can be alleviatedwith a secured API call over the wire.Secondly integration with other systems is a bit harder if you haveto call a cli. For public facing endpoints JSON is used.As an example the trigger_dag functionality is now made into aAPI call.Backwards compat is retained by switching to a LocalClient.,1
Merge branch 'api_v3',7
"[AIRFLOW-41] Fix pool oversubscriptionScheduler would send tasks to the queue for ""open minus running""instances. If the task eventually gets picked up and sees(race condition, because multiple tasks could compete for slot)that a slot is free, it would run the task. If the slot was not free,the task would be set back to QUEUED (or SCHEDULED), anyway, returnedto the scheduler for another run. In specific cases, there'd be acouple of task instances that suffer from the non-atomic read andbe run anyway.Closes #1872 from gtoonstra/feature/AIRFLOW-41",1
[AIRFLOW-658] Improve schema_update_options in GCPDefault argument for `schema_update_options` needsto be immutable.Changed to an empty tuple instead of an emptylist.Closes #1909 from Jalepeno112/bug/AIRFLOW-658,0
[AIRFLOW-656] Add dag/task/date index to xcom tableCloses #1907 from criccomini/AIRFLOW-656,5
"[AIRFLOW-662] Change seasons to months in project descriptionThings like ""winter 2016"" are ambiguous even inthe northern hemisphere, and mean somethingtotally different in the southern hemisphere(July/August/September). Changing the season-baseddates to months. References:*http://incubator.apache.org/projects/airflow.html* http://nerds.airbnb.com/airflow/* https://github.com/apache/incubator-airflow/graphs/contributorsCloses #1913 from benhoyt/patch-1",5
[AIRFLOW-647] Restore dag.get_active_runsSimply added a getter back to dag that returns thelist of active dag run execution dates for the dagfrom the DB.Closes #1899 frombtallman/RestoreActiveRuns_feature,1
Use jdk selector to set required jdk,1
[AIRFLOW-682] Bump MAX_PERIODS to make mark_success work for large DAGsIt is not possible to mark success on some largeDAGs due to the MAX_PERIODS being set to 1000. Weshould temporarily bump it up until work can bedone to scale the mark success endpoint muchhigher.Testing Done:- Has been running in Airbnb prod for many monthsnowCloses #1928 from aoen/ddavydov/bump_max_periods,1
"[AIRFLOW-674] Ability to add descriptions for DAGsPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-674Descriptions are rendered in two places right now,as a tooltip on the DAGs page for each DAG, andafter the DAG name in the various individual DAGviews (see screenshots below).Testing Done:- Spun up local webserver, tried emptydescription, normal description, very longdescription.Screenshots:https://cloud.githubusercontent.com/assets/1592778/20906424/dbf2885e-bafc-11e6-8eeb-78302e87d25a.pnghttps://cloud.githubusercontent.com/assets/1592778/20906420/d651ccfc-bafc-11e6-9893-ef677be50bf0.pngCloses #1920 from aoen/ddavydov/dag_desc",2
"[AIRFLOW-677] Kill task if it fails to heartbeathttps://issues.apache.org/jira/browse/AIRFLOW-677Testing Done:- We've been running this in production at Airbnbfor a bit, although off a different merge base[AIRFLOW-677] Kill task if it fails to heartbeatIf there's a connection error while heartbeating,it should retry. Also,if it hasn't been able to heartbeat for a while,it should kill thechild processes so that we don't have 2 of thesame task running.",1
[AIRFLOW-678] Prevent scheduler from double triggering TIsAt the moment there is no lock/synchronizationaround the loop where the scheduler puts tasks inthe SCHEDULED state. This means that if somehowthe task starts running or gets SCHEDULEDsomewhere else somehow (e.g. manually running atask via the webserver) the task can have it'sstate changed from RUNNING/QUEUED to SCHEDULEDwhich can cause a single task instance to be runtwice at the same time.Testing Done:- Tested this branch on the Airbnb Airflow stagingcluster- Airbnb has been running very similar logic inour production for many months (not 1-1 since weare still running off of the last release branch)- In the future we ideally need an integrationtest to catch double triggers but this is nottrivial to do properlyCloses #1924 fromaoen/ddavydov/fix_scheduler_race_condition,0
[AIRFLOW-680] Disable connection pool for commandsThis is a continuation of Max's PR here:https://github.com/apache/incubator-airflow/pull/1021We have seen a very substantial DB cpu usagedecrease from this PR (~10x).This PR was originally created by plypaul I amjust cherrypicking onto master for him.Testing Done:- Has been running in Airbnb production for quitesome time (off of a different merge base though)Closes #1925 from aoen/ddavydov/reduce_db_sessions,5
[AIRFLOW-667] Handle BigQuery 503 errorCloses #1938 from krmettu/master,0
[AIRFLOW-704][AIRFLOW-1] Fix invalid syntax in BQ hookCloses #1947 from criccomini/AIRFLOW-704,5
"[AIRFLOW-679] Stop concurrent task instances from runningCheck that PID remains unchanged, and throwexception otherwise.Testing Done:- Ran a task, set PID to be different, andensuring it failedIf there's a connection error while heartbeating,it should retry. Also,if it hasn't been able to heartbeat for a while,it should kill thechild processes so that we don't have 2 of thesame task running.Closes #1939 from saguziel/consistency",1
Add Digital First Media to companies using,1
Merge pull request #1949 from duffn/patch-1,7
[AIRFLOW-701] Add Lemann Foundation as an Airflow userCloses #1944 from fernandosjp/patch-2,1
"[AIRFLOW-703][AIRFLOW-1] Stop Xcom being cleared too earlyXComs should only be cleared when it is certainthat the task will run. Previously, XComs were clearedbefore it was determined if tasks were runnable, queable,or just being marked success. Now XComs are clearedimmediately before the task actually starts.Closes #1951 from blrnw3/fix/xcom_bug_AIRFLOW-703",0
[AIRFLOW-710] Add OneFineStay as official userCloses #1952 from slangwald/patch-1,1
[AIRFLOW-712] Fix AIRFLOW-667 to use proper HTTP error propertiesCloses #1955 from criccomini/AIRFLOW-712,5
[AIRFLOW-649] Support non-sched DAGs in LatestOnlyOpCloses #1956 from r39132/master,3
[AIRFLOW-700] Update to reference to web authentication documentationCloses #1943 from alanmcruickshank/config_update,5
[AIRFLOW-709] Use same engine for migrations and reflectionUse Same Engine for Migrations and ReflectionSolves query blocking in MS-SQL server whenrunning initdb.Closes #1953 from gritlogic/AIRFLOW-709,2
[AIRFLOW-691] Add SSH KeepAlive option to SSH_hookThis patch adds the option to set tcp keepaliveand to configurethe server alive interval for the ssh connection.Closes #1937 from danielvdende/add_ssh_keepalive,1
[AIRFLOW-686] Match auth backend config sectionThis commit makes that refers the sameconfiguration when usingldapgroup owner mode with ldap auth.Closes #1930 from keybod/AIRFLOW-686,1
[AIRFLOW-685] Add test for MySqlHook.bulk_load()Closes #1929 from sekikn/AIRFLOW-685,1
[AIRFLOW-641] Improve pull request instructionsCloses #1897 fromJalepeno112/improvement/AIRFLOW-641,1
[AIRFLOW-657] Add AutoCommit Parameter for MSSQLCloses #1908 from robin-miller-ow/MsSqlAutoCommit,2
[AIRFLOW-403] Bash operator's kill method leaves underlying processes runningCurrently only the main process is being killed due to the fact that theprocess group is not being terminated.Closes #1714 from spektom/bash_operator_kill,1
[AIRFLOW-720] Add Shopkick to Airflow usersCloses #1962 from ecesena/patch-1,1
[AIRFLOW-721] Descendant process can disappear before terminationThere is a race condition in helpers.py's kill_descendant_processesthat checks for running processes and then tries to terminate them.This is not done atomically allowing for a small window where a PIDcan disappear before termination.,1
[AIRFLOW-721] Descendant process can disappear before terminationThere is a race condition in helpers.py'skill_descendant_processesthat checks for running processes and then triesto terminate them.This is not done atomically allowing for a smallwindow where a PIDcan disappear before termination.Closes #1963 from bolkedebruin/AIRFLOW-721,1
Log needs to be part of try/catch block,1
Merge branch 'AIRFLOW-721',7
[AIRFLOW-724] Adding City of San Diego to Airflow usersCloses #1965 from MrMaksimize/sandiego_use,1
[AIRFLOW-718] Allow the query URI for DataProc PigThe query URI parameter was missing for theDataProc pig operator. Withthe addition of the parameter you can now storethe pig script in CloudStorage.Closes #1960 from alexvanboxel/feature/dataproc-pig-uri,5
[AIRFLOW-716] Allow AVRO BigQuery load-job without schemaNow allow a load job without specifying the schemafields or object.This allows for loading files with embedded schemalike AVRO files.Also made optional values None instead of False tomake it a bit morePythoneske without breaking compatibility.Closes #1958 fromalexvanboxel/feature/bq_load_avro,4
"[AIRFLOW-715] A more efficient HDFS Sensor:A more efficient HDFS Sensor:HDFS Sensor is now capable to trigger true basedon a file size, a directory status(empty or not) a regex to match files in adirectory and also to discard copying files.With the base HDFS Sensor, it was not possible towatch a directory for files with aunknown name.HDFS Sensors is now extended with (contrib):  - HdfsSensorRegex : for matching files wih a regex(re)  - HdfsSensorFolder : for matching with directoryHDFS Sensor has now to built in filters :  - filter_for_filesize : to filter list result bythe filesize  - filter_for_ignored_ext : to discard or notcopying filesUnittests added with a new FakeSnakebite clientand a FakeHdfsHookA more efficient HDFS Sensor:HDFS Sensor is now capable to trigger true basedon a file size, a directory status(empty or not) a regex to match files in adirectory and also to discard copying files.With the base HDFS Sensor, it was not possible towatch a directory for files with aunknown name.HDFS Sensors is now extended with (contrib):  - HdfsSensorRegex : for matching files wih a regex(re)  - HdfsSensorFolder : for matching with directoryHDFS Sensor has now to built in filters :  - filter_for_filesize : to filter list result bythe filesize  - filter_for_ignored_ext : to discard or notcopying filesUnittests added with a new FakeSnakebite clientand a FakeHdfsHookA more efficient HDFS Sensor:HDFS Sensor is now capable to trigger true basedon a file size, a directory status(empty or not) a regex to match files in adirectory and also to discard copying files.With the base HDFS Sensor, it was not possible towatch a directory for files with aunknown name.HDFS Sensors is now extended with (contrib):  - HdfsSensorRegex : for matching files wih a regex(re)  - HdfsSensorFolder : for matching with directoryHDFS Sensor has now to built in filters :  - filter_for_filesize : to filter list result bythe filesize  - filter_for_ignored_ext : to discard or notcopying filesUnittests added with a new FakeSnakebite clientand a FakeHdfsHookCloses #1957 from vfoucault/feature/AIRFLOW-715",1
[AIRFLOW-726] Add Vnomics to Airflow usersCloses #1968 from lpalum/master,1
"[AIRFLOW-695] Retries do not execute because dagrun is in FAILED stateThe scheduler checks the tasks instances without taking into accountif the executor already reported back. In this case the executorreports back several iterations later, but the task is queued nevertheless.Due to the fact tasks will not enter the queue when the task is consideredrunning, the task state will be ""queued” indefinitely and in limbobetween the scheduler and the executor.",5
"[AIRFLOW-727] try_number is not increasedA dag that has retries enabled will retry indefinitelyas try_number gets reset to 0 in LocalTaskJob astask_instance is not fully populated, but neverthelesssaved to the databases.This was caused by a commit inhttps://github.com/apache/incubator-airflow/pull/1939",1
"[AIRFLOW-673] Add operational metrics test for SchedulerJobExtend SchedulerJob to instrument the executionperformance of task instances contained in eachDAG.We want to know if any DAG is starved of resources,and this will be reflected in the stats printedout at the end of the test run.Extend SchedulerJob to instrument the executionperformance of task instances contained in eachDAG. We want to know if any DAG is starved ofresources, and this will be reflected in the statsprinted out at the end of the test run.this test is for instrumentingthe operational impact ofhttps://github.com/apache/incubator-airflow/pull/1906Closes #1919 from vijaysbhat/scheduler_perf_tool",3
"[AIRFLOW-695] Retries do not execute because dagrun is in FAILED stateThe scheduler checks the tasks instances without taking into accountif the executor already reported back. In this case the executorreports back several iterations later, but the task is queued nevertheless.Due to the fact tasks will not enter the queue when the task is consideredrunning, the task state will be ""queued” indefinitely and in limbobetween the scheduler and the executor.",5
Merge remote-tracking branch 'apache/master',7
Merge branch 'master' into temp_fix,0
[AIRFLOW-717] Add Cloud Storage updated sensorAdd a Cloud Storage sensor that triggers when aobject is createdor updated after a specific date. Allow setting acallback thatdefines the update requirements. The default isexecution_date+ trigger_interval.Closes #1959 fromalexvanboxel/feature/gcp_sensor_updated,5
Bump version to 1.8.0alpha2,5
[AIRFLOW-730] Add Handshake to Airflow usersCloses #1972 from mhickman/master,1
[AIRFLOW-702] Fix LDAP Regex BugCloses #1945 from robin-miller-ow/feature/LDAPAuthRegexFix,0
[AIRFLOW-734] Fix SMTP auth regression when not using user/passCloses #1974 from criccomini/AIRFLOW-734,5
Merge branch 'master' into v1-8-test,3
[AIRFLOW-665] Fix email attachmentsContent-Disposition must be set separately onthe MIMEApplication. Passing it to theconstructor just puts it in the Content-Type header.Closes #1916 from dgies/master,4
"[AIRFLOW-663] Improve time units for task performance chartsThe task duration and landing time charts displaytime interval values in hours. This is not theappropriate unit for tasks that execute on smallertime scales (~minutes, ~seconds), and the chartis unreadable in those cases.This patch converts the time values to theappropriate units and updates the y axis label toshow the unit of analysis.Closes #1914 from vijaysbhat/chart_time_units",2
[AIRFLOW-740] Pin jinja2 to < 2.9.0Jinja2 2.9.1 seems to have a conflict with flask-admin.,5
Merge branch 'master' into v1-8-test,3
Bump version to 1.8.0a3,5
[AIRFLOW-731] Fix period bug for NamedHivePartitionSensorFix a bug in partition name parsing for theoperator.Closes #1973 from artwr/artwr-bugfix_for_named_partition_sensor_and_periods,0
[AIRFLOW-741] Log to debug instead of info for app.pyCloses #1977 from bolkedebruin/AIRFLOW-741,5
"[AIRFLOW-728] Add Google BigQuery table sensorDesign a sensor that checks whether a certaintable is present in bigquery. The sensor willaccept the google cloud project id, bigquerydataset id and bigquery table id to check asparameters.Internally, it will use the bigquery hook to checkfor the existence of the table.Therefore a 'table_exists' method will be added tothe existing Bigquery hook.Closes #1970 from bodschut/feature/bq_sensor",1
"[AIRFLOW-729] Add Google Cloud Dataproc cluster creation operatorThe operator checks if there is already a clusterrunning with the provided name in the providedproject.If so, the operator finishes successfully.Otherwise, the operator issues a rest API call toinitiatethe cluster creation and waits until the creationis successful before exiting.Closes #1971 frombodschut/feature/dataproc_operator",5
"[AIRFLOW-738] Commit deleted xcom items before insertA delete insert sequence within one transaction can leadto a deadlocked transaction with Mariadb / MySQL.The deletes, in case they affected no rows, all get a shared lock(mode IX) on the end-of-table gap. Once the insert is executed,the shared lock is still held by all threads,and the insert intention waits for the release of this shared lock.The solution is to not do the following in parallel:1. Delete the rows you want to insert, when the rows aren't there.2. Insert the rowsIn this case the risk of not executing the delete and insertis relatively low, as it was the users intention to run thetask. In case it fails in between the two transactionsthe task can be tried.",0
Merge branch 'AIRFLOW-738',7
Update upgrade documentation for Google CloudCloses #1979 from alexvanboxel/pr/doc_gcloud,2
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-489] Allow specifying execution date in trigger_dag APICloses #1946 from robin-miller-ow/release/New_API_Functionality,1
Merge branch 'master' into v1-8-test,3
Bump version to 1.8.0a4,5
Merge branch 'master' into v1-8-test,3
Bump version to 1.8.0a5,5
"[AIRFLOW-558] Add Support for dag.catchup=(True|False) OptionAdded a dag.catchup option and modified thescheduler to look at the value when schedulingDagRuns(by moving dag.start_date up todag.previous_schedule),and added a config option catchup_by_default(defaults to True) that allows users to set thisto False for alldags modifying the existing DAGsIn addition, we added a test to jobs.py(test_dag_catchup_option)Closes #1830 frombtallman/NoBackfill_clean_feature",4
[AIRFLOW-752] Add Mercadoni to list of Airflow usersCloses #1991 from demorenoc/master,1
[AIRFLOW-750] Added CHOP DGD to Airflow user listCloses #1990 from genomics-geek/master,1
[AIRFLOW-753] Add PayPal as a airflow userCloses #1985 from jhsenjaliya/add-paypal-as-user,1
"[AIRFLOW-747] Fix retry_delay not honouredDag runs were marked deadlocked although a task wasstill up for retry and in its retry_delay period. Next tothat _execute_task_instances was picking up tasks inUP_FOR_RETRY state directly from the database, whiletasks that pass the dependency check will be set to scheduled.",1
Merge branch 'AIRFLOW-747',7
"[AIRFLOW-737] Fix HDFS Sensor directory.Due to a bad ordering in the fake snakebiteclient, one test was wrongly True.Closes #1989 fromvfoucault/fixbug/hdfssensor_folder",0
[AIRFLOW-692] Open XCom page to super-admins onlyCloses #1940 from msumit/AIRFLOW-692,2
Merge branch 'master' into v1-8-test,3
[AIRFLOW-757] Set child_process_log_directory default more sensibleThe default of child_process_log_directory was pointing to/tmp/airflow/logs/scheduler. This could take people by surprise asit is a non standard location and deviates from Airflow's otherlog folders.,2
[AIRFLOW-759] Use previous dag_run to verify depend_on_pastThe start_date and the schedule interval can be misaligned. Thisis automatically corrected in the scheduler. The dependency checkerhowever did not do this.,5
[AIRFLOW-760] Update systemd config,5
[AIRFLOW-762] Add Google DataProc delete operatorPair the recently added Create operator with aDelete operator forGoogle Cloud DataProc clusters.Closes #1997 from alexvanboxel/pr/dataproc,5
Merge branch 'master' into v1-8-test,3
Merge remote-tracking branch 'apache/master' into v1-8-test,3
Bump version to 1.8.0b1,5
"[AIRFLOW-683] Add jira hook, operator and sensorCloses #1950 from jhsenjaliya/AIRFLOW-683",1
Merge branch 'master' into v1-8-test,3
Merge branch 'AIRFLOW-757',7
Merge branch 'AIRFLOW-760',7
Merge branch 'master' into v1-8-test,3
Add incubating specifier to version,1
"[AIRFLOW-219][AIRFLOW-398] Cgroups + impersonationSubmitting on behalf of plypaulPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-219-https://issues.apache.org/jira/browse/AIRFLOW-398Testing Done:- Running on Airbnb prod (though on a differentmergebase) for many monthsCredits:Impersonation Work: georgeke did most of the workbut plypaul did quite a bit of work too.Cgroups: plypaul did most of the work, I just didsome touch up/bug fixes (see commit history,cgroups + impersonation commit is actually plypaul's not mine)Closes #1934 from aoen/ddavydov/cgroups_and_impersonation_after_rebase",0
[AIRFLOW-773] Fix flaky datetime addition in api testPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-773There is an API test inwww/api/experimental/test_endpoints.py that addsone to a datetime incorrectly (e.g. hours = 24 + 1is incorrect). This causes travis CI to alwaysfail for one hour of the day.Closes #2004 fromaoen/ddavydov/fix_flaky_datetime_tet,5
[AIRFLOW-771] Make S3 logs append instead of clobberCloses #2003 fromaoen/ddavydov/make_airflow_logs_append_only,2
"[AIRFLOW-739] Set pickle_info log to debugpickle_info tries to pickle. If it catches anexceptionit is assumed that the DAG is not pickable andcontinues.Therefore, it should log to debug instead and notprovidea full stacktrace.Closes #1975 from bolkedebruin/AIRFLOW-739",1
[AIRFLOW-778] Fix completey broken MetastorePartitionSensorMetastorePartitionSensor always throws anexception on initialization due to72cc8b3006576153aa30d27643807b4ae5dfb593 . This PRreverts the breaking part of this commit.Looks like the tests for this are only run if anexplicit flag is set which is how this got pastCI.Closes #2005 fromaoen/ddavydov/fix_metastore_partition_sensor,0
"[AIRFLOW-779] Task should fail with specific message when deletedTesting Done:- Killed a task while it was running using thetask instances UI, verified behavior is the sameas before, and logging workedCloses #2006 from saguziel/aguziel-terminate-nonexistent",1
"Revert ""[AIRFLOW-779] Task should fail with specific message when deleted""This reverts commit 9221587514e2a0155cdced2d3ae50129b0793a10.",4
[AIRFLOW-624] Fix setup.py to not import airflow.version as version,2
[AIRFLOW-784] Pin funcsigs to 1.0.0,5
[AIRFLOW-785] Don't import CgroupTaskRunner at global scopecgroups is not a required dependency,1
[AIRFLOW-777] Fix expression to check if a DagRun is in running state,1
[AIRFLOW-776] Add missing cgroups devel dependencyCloses #2009 from aminghadersohi/master,1
Merge pull request #2010 from gsakkis/fixes,0
"[AIRFLOW-139] Let psycopg2 handle autocommit for PostgresHookThe server-side autocommit setting was removed and reimplementedin client applications and languages. Server-side autocommit wascausing too many problems with languages and applications thatwanted to control their own autocommit behavior,so autocommit was removed from the server and added to individual client APIs as appropriateCloses #1821 from danielzohar/AIRFLOW-139_vacuum_operator",1
"[AIRFLOW-798] Check return_code before forcing terminationLocalTaskJob could still log an error en self destruct,although the underlying process already exited.",0
"[AIRFLOW-803] Revert join with dag_runs in _execute_task_instancesTaskInstances will be set to 'scheduled' if they meet the criteria to run, also the ones up for retry. No task_instance will be send to the executorin another state than 'scheduled'.",1
Merge branch 'fix_localtaskjob',0
"[AIRFLOW-807] Improve scheduler performance for large DAGsMySQL's query optimizer selects the wrong index, thishas a significant impact on the performance of thescheduler.Closes #2021 from criccomini/AIRFLOW-807",5
[AIRFLOW-810] Correct down_revision dag_id/state index creationDue to revert the revision were not correct anymore and an uncleanbuild environment would still consider it for alembic migrations.,4
[AIRFLOW-783] Fix py3 incompatibility in BaseTaskRunner,1
[AIRFLOW-782] Add support for DataFlowPythonOperator.DataFlowPythonOperator allows users to definie GCPdataflow task wherethe pipeline job is specified in Python. Thecorresponding unit testsare also included. To run the tests:nosetests tests.contrib.hooks.gcp_dataflow_hook:DataFlowHookTest andnosetests tests.contrib.operators.dataflow_operator:DataFlowPythonOperatorTest.Closes #2025 from fenglu-g/master,5
"[AIRFLOW-780] Fix dag import errors no longer workingThe import errors were no longer working after themultiprocessor update(since they are cleared after each DAG directoryis parsed). This changefixes them, and adds tests to prevent futureregressions.Also fix a couple of linter errors.Note that there are a few inefficiencies (e.g.sometimes we delete then add import errors in thesame place instead of just doing an update), butthis is equivalent to the old behavior.Testing Done:- Added missing unit tests for dag imports. Notethat some of them strangely fail for python 3 andit became too time consuming to debug since Idon't have a copy of the travis environment, Ieven ran with the same version of python locallyand couldn't reproduce. I have skipped those 3tests in python 3 for now.Closes #2018 from aoen/fix_parse_errors_not_displa",0
[AIRFLOW-812] Fix the scheduler termination bug.When checking max-runs for dag_files the schedulerwould stop immediately if there were no files.Closes #2027 from fenglu-g/master,2
[AIRFLOW-806] UI should properly ignore DAG doc when it is NoneCheck dag.doc_md before we try to convert it toMarkdown.Closes #2020 from dhuang/AIRFLOW-806,1
[AIRFLOW-813] Fix unterminated scheduler unit testsCloses #2028 from fenglu-g/master,3
[AIRFLOW-813] Fix unterminated unit tests in SchedulerJobTestCloses #2030 from fenglu-g/master,3
[AIRFLOW-813] Fix unterminated unit tests in SchedulerJobTestCloses #2032 from fenglu-g/master,3
[AIRFLOW-815] Add prev/next execution dates to template variablesThis patch adds the previous/next execution datesto the default variables available in a template.Closes #2033 from danielvdende/add-execution-dates,5
[AIRFLOW-822] Close db before exceptionThe basehook contains functionality to retrieveconnections from thedatabase. If a connection does not exist it willthrow an exception.This exception will be thrown before theconnection to the databaseis closed. Therefore the session to the db mightstay open and linger.Closes #2038 from Fokko/airflow-822,5
[AIRFLOW-817] Check for None value of execution_date in endpointexecution_date can be present in json whileresolving to None.Closes #2034 from bolkedebruin/AIRFLOW-817,5
[AIRFLOW-821] Fix py3 compatibilityiteritems() does not exist in py3.Closes #2039 from bolkedebruin/AIRFLOW-821,0
[AIRFLOW-816] Use static nvd3 and d3Closes #2035 from bolkedebruin/AIRFLOW-816,1
[AIRFLOW-789] Update UPDATING.mdCloses #2011 from bolkedebruin/AIRFLOW-789,5
Bump version to 1.9.0dev0,5
[AIRFLOW-781] Allow DataFlowOperators to accept jobs stored in GCSCloses #2037 from fenglu-g/master,5
"[AIRFLOW-365] Set dag.fileloc explicitly and use for Code viewCode view for subdag has not been working. I donot think we are ablecleanly figure out where the code for the factorymethod lives when weprocess the dags, so we need to save the locationwhen the subdag iscreated.Previously for a subdag, its `fileloc` attributewould be set to thelocation of the parent dag. I think it isappropriate to instead setit to the actual child dag location instead. We donot lose anyinformation this way (we still have the link tothe parent dag thathas its location) and now we can always read thisattribute for thecode view. This should not affect the use of thisfield for refreshingdags, because we always refresh the parent for asubdag.Closes #2043 from dhuang/AIRFLOW-365",2
"[AIRFLOW-694] Fix config behaviour for empty envvarCurrently, environment variable with empty valuedoes not overwrite theconfiguration value corresponding to it.Closes #2044 from sekikn/AIRFLOW-694",5
[AIRFLOW-794] Access DAGS_FOLDER and SQL_ALCHEMY_CONN exclusively from settingsCloses #2013 from gsakkis/settings,1
[AIRFLOW-831] Restore import to fix broken testsThe global `models` object is used in the code andwas inadvertentlyremoved. This PR restores itCloses #2050 from jlowin/fix-broken-tests,3
CHANGELOG for 1.8Closes #2000 from alexvanboxel/pr/changelog,4
"[AIRFLOW-844] Fix cgroups directory creationTesting Done:- Tested locally, we should add cgroup tests atsome point thoughCloses #2057 from aoen/ddavydov/fix_cgroups",0
[AIRFLOW-793] Enable compressed loading in S3ToHiveTransferTesting Done:- Added new unit tests for the S3ToHiveTransfermoduleCloses #2012 from krishnabhupatiraju/S3ToHiveTransfer_compress_loading,3
[AIRFLOW-814] Fix Presto*CheckOperator.__init__Use keyword args when initializing aPresto*CheckOperator.Closes #2029 from patrickmckenna/fix-presto-check-operators,0
"[AIRFLOW-830][AIRFLOW-829][AIRFLOW-88] Reduce Travis log verbosity[AIRFLOW-829][AIRFLOW-88] Reduce verbosity ofTravis testsRemove the -s flag for Travis unit tests tosuppress outputfrom successful tests.[AIRFLOW-830] Reduce plugins manager verbosityThe plugin manager prints all status to INFO,which is unnecessary andoverly verbose.Closes #2049 from jlowin/reduce-logs",2
Add pool upgrade issue description,0
[AIRFLOW-854] Add OKI as Airflow userOKI == Open Knowledge InternationalCloses #2061 from vitorbaptista/patch-1,1
"[AIRFLOW-856] Make sure execution date is set for local clientIn the local api client the execution date washardi coded to None.Secondly, when no execution date was specified theexecution datewas set to datetime.now(). Datetime.now() includesthe fractional secondsthat are supported in the database, but they arenot supported ina.o. the current logging setup. Now we cut offfractional seconds forthe execution date.Closes #2064 from bolkedebruin/AIRFLOW-856",5
Add known issue of 'num_runs',1
[AIRFLOW-857] Use library assert statements instead of conditionalsTesting Done:- TravisCloses #2062 from saguziel/aguziel-fix-asserts,3
[AIRFLOW-853] use utf8 encoding for stdout line decodeCloses #2060 from ming-wu/master,1
[AIRFLOW-863] Example DAGs should have recent start datesAvoid unnecessary backfills by having start datesofjust a few days ago. Adds a utility functionairflow.utils.dates.days_ago().Closes #2068 from jlowin/example-start-date,5
[AIRFLOW-858] Configurable database name for DB operatorsCloses #2063 from s7anley/configurable-schema,5
[AIRFLOW-862] Add DaskExecutorAdds a DaskExecutor for running Airflow tasksin Dask clusters.Closes #2067 from jlowin/dask-executor,1
[AIRFLOW-832] Let debug server run without SSLCloses #2051 from gsakkis/fix-debug-server,0
[AIRFLOW-834] change raise StopIteration into returnThis silences PendingDeprecationWarnings in Python3.5.See https://www.python.org/dev/peps/pep-0479/#id34for an example whyit isn't a good idea to raise StopIterationinstead of just returning.Closes #2073 from imbaczek/bug834,0
"[AIRFLOW-842] do not query the DB with an empty IN clauseThis is done to silence warnings coming fromsqlachemy, e.g.:sqlalchemy/sql/default_comparator.py:161:SAWarning: The IN-predicate on""dag_run.dag_id"" was invoked with an emptysequence. This results in acontradiction, which nonetheless can be expensiveto evaluate. Consideralternative strategies for improved performance.Closes #2072 from imbaczek/bug842",0
[AIRFLOW-826] Add Zendesk hookCloses #2066 from shreyasjoshis/add-zendesk-hook,1
"[AIRFLOW-877] Remove .sql template extension from GCS download operatorPrior to this patch, if you use a templated file with a .sql extension,and it's templated, you'd receive an exception because Airflow would tryand load the file (that hasn't yet been downloaded) to template it. Thispatch fixes that.Closes #2083 fromsarfarazsoomro/sas/fix_gcs_download_op",0
[AIRFLOW-869] Refactor mark success functionalityThis refactors the mark success functionality in amore generic function that can set multiple statesandproperly drills down on SubDags.Closes #2085 from bolkedebruin/AIRFLOW-869,2
[AIRFLOW-861] make pickle_info endpoint be login_requiredTesting Done:- Unittests passCloses #2077 from saguziel/aguziel-fix-login-required,2
Closes apache/incubator-airflow#2065 *Do not take CI time away from us*,5
[AIRFLOW-882] Remove unnecessary dag>>op assignment in docsCloses #2088 from dhuang/AIRFLOW-882,2
"[AIRFLOW-871] change logging.warn() into warning()This silences deprecation warnings, e.g.airflow/airflow/utils/dag_processing.py:578:DeprecationWarning: The'warn' method is deprecated, use 'warning' insteadCloses #2082 from imbaczek/bug871",0
[AIRFLOW-886] Pass result to post_execute() hookThe post_execute() hook should receivethe Operator result in addition to theexecution context.,1
[AIRFLOW-887] Support future v0.16,1
[AIRFLOW-862] Fix Unit Tests for DaskExecutorUnit tests were inadvertently disabled forDaskExecutorCloses #2076 from jlowin/fix-dask-tests,3
Merge pull request #2091 from jlowin/post-execute-hook,1
[AIRFLOW-836] Use POST and CSRF for state changing endpointsCloses #2054 from saguziel/aguziel-use-post,1
[AIRFLOW-885] Add change.org to the users listCloses #2089 fromvijaykramesh/change/add_change_to_users_list,4
"[AIRFLOW-881] Check if SubDagOperator is in DAG context managerWhen initializing a SubDagOperator, the `dag`param should not berequired if it is within a DAG context manager. Sowe check if thatis the case and use that as the parent DAG iffound (and `dag` paramis not specified).Closes #2087 from dhuang/AIRFLOW-881",2
[AIRFLOW-866] Add FTPSensorCloses #2070 from s7anley/ftp-sensor,1
[AIRFLOW-875] Add template to HttpSensor paramsCloses #2080 from jlowin/httpsensor,2
[AIRFLOW-809][AIRFLOW-1] Use __eq__ ColumnOperator When Testing BooleansThe .is_ ColumnOperator causes the SqlAlchemy'sMSSQL dialect to produceIS 0 when given a value of False rather than avalue of None. The __eq__ColumnOperator does this same test with the addedbenefit that it willmodify the resulting expression from and == to aIS NULL when the targetis None.This change replaces all is_ ColumnOperators thatare doing booleancomparisons and leaves all is_ ColumnOperatorsthat are checking forNone values.Closes #2022 from gritlogic/AIRFLOW-809,2
[AIRFLOW-889] Fix minor error in the docstrings for BaseOperatortask --> classCloses #2084 from ketanbhatt/patch-1,1
"[AIRFLOW-880] Make webserver serve logs in a sane way for remote logsThere are two major problems with remote logs inAirflow right now:1. Lack of Complete Logs: Remote logs should bethe default instead of the log that is only loadedif the local log is not present, because theremote log will have the logs for all of the triesof a task, whereas the local log is onlyguaranteed to have the most recent one2. Lack of Consistency: The logs returned willalways be the same from all the webservers (rightnow different logs can be returned if a webserverhas a log vs doesn't, and there can be differentlogs between webservers that have the log).Right now log functionality is not consistent whenit comes to remote logs.This PR addresses these issues by ALWAYS readingfrom remote logs and then also reading logs fromworker hosts if the task is already running (toget in-flight logs). The one issue with this PR isthat if a task is running on a worker it alreadyran on, then you will get duplicate logs for allof the previous runs of the task that alreadycompleted (delimited by something like ""***Getting remote logs"" ""*** Getting logs on localworker""). This can be fixed later (either bystreaming logs to the log server or by creating aproper abstraction for multiple task instanceruns), and is still better than the currentbehavior (duplicate info is better than omittingprevious task instance logs from the webserverlog).Testing Done:Tested on staging cluster:- Task instance doesn't exist- Task instance is running and has previous remotelog- Task instance is running for first time- Task instance has completed and has remote logCloses #2086 from aoen/ddavydov/fix_s3_logging",2
[AIRFLOW-893][AIRFLOW-510] Fix crashing webservers when a dagrun has no start dateCloses #2094 from aoen/ddavydov/fix_webservers_when_bad_startdate_dag,5
[AIRFLOW-895] Address Apache release incompliancies* Fixes missing licenses in NOTICE* Corrects license header* Removes HighCharts left overs.Closes #2098 from bolkedebruin/AIRFLOW-895,2
[AIRFLOW-899] Tasks in SCHEDULED state should be white in the UI instead of blackCloses #2100 fromaoen/ddavydov/fix_black_squares_in_ui,0
"[AIRFLOW-896] Remove unicode to 8-bit conversion in BigQueryOperatorFor some reason, we have a str() call on a string when loggingan error. This causes a unicode error if the BigQuery query stringhas non-ascii characters in it. The fix seems to be to just removethe str() call.Closes #2097 from mremes/master",4
[AIRFLOW-897] Prevent dagruns from failing with unfinished tasksCloses #2099 fromaoen/ddavydov/fix_premature_dagrun_failures,2
"[AIRFLOW-906] Update Code icon from lightning bolt to fileLightning bolts are not a visual metaphor for codeor files. Since Glyphicon doesn't have a code icon(<>, for instance), we should use its file icon.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:AIRFLOW-906Testing Done:None.Before/After screenshots in AIRFLOW-906 (https://issues.apache.org/jira/browse/AIRFLOW-906)Update Code icon from lightning bolt to fileLightning bolts are not a visual metaphor for codeor files. Since Glyphicon doesn't have a code icon(<>, for instance), we should use its file icon.Merge pull request #1 from djarratt/djarratt-patch-1Update Code icon from lightning bolt to fileAIRFLOW-906 change glyphicon flash to fileMerge pull request #2 from djarratt/djarratt-patch-2AIRFLOW-906 change glyphicon flash to fileCloses #2104 from djarratt/master",2
[AIRFLOW-911] Add coloring and timing to testsCloses #2106 from bolkedebruin/profile_tests,3
[AIRFLOW-916] Remove deprecated readfp functionConfigParser.readfp() is deprecated in favor ofread_file()Closes #2108 from jlowin/parser-deprecation,2
[AIRFLOW-725] Use keyring to store credentials for JIRANow allows to store the credentials in the keyringof the OS. Retains backwards compatibility.Closes #1966 from bolkedebruin/DEV_KEYRING,1
[AIRFLOW-802][AIRFLOW-1] Add spark-submit operator/hookAdd a operator for spark-submit to kick off ApacheSpark jobs byusing Airflow. This allows the user to maintainthe configurationof the master and yarn queue within Airflow byusing connections.Add default connection_id to the initdb routine toset sparkto yarn by default. Add unit tests to verify thebehaviour ofthe spark-submit operator and hook.Closes #2042 from Fokko/airflow-802,1
[AIRFLOW-919] Running tasks with no start date shouldn't break a DAGs UIPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-919I also made the airflow PR template a little bitless verbose (requires less edits when creating aPR).Testing Done:- Ran a webserver with this case and made surethat the DAG page loadedCloses #2110 fromaoen/ddavydov/fix_running_task_with_no_start_date,5
[AIRFLOW-925] Revert airflow.hooks change that cherry-pick pickedPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-925Testing Done:- Fixes bug in prodCloses #2112 from saguziel/aguziel-hivemetastorehook-import-apache,2
"Revert ""[AIRFLOW-916] Remove deprecated readfp function""This reverts commit ef6dd1b29b794c5e0fd4f2bc8422a386395950f5 whichbroke webservers.",4
[AIRFLOW-933] use ast.literal_eval rather eval because ast.literal_eval does not executeinput.This PR addresses the following issues:- *(https://issues.apache.org/jira/browse/AIRFLOW-933)*This PR is trying to solve a secure issue. Thetest was done by setting up a local web server andreproduce the issue described in JIRA link above.Closes #2117 from amaliujia/master,2
"[AIRFLOW-937] Improve performance of task_statsPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-937Testing Done:- Shouldn't change functionality significantly,should pass existing tests (if they exist)This leads to slightly different results, but itreduced the time of this endpoint from 90s to 9son our data, and the existing logic for task_idswas already incorrect (task_ids may not bedistinct across dags)Closes #2121 from saguziel/task-stats-fix",0
"[AIRFLOW-938] Use test for True in task_stats queriesFix a bug with the task_stats query on postgres which doesn't support== 1.https://issues.apache.org/jira/browse/AIRFLOW-938I've seen the other PR but I'll try to see if thismethod works because I believe `__eq__(True)` isjust `== True`, and it is how it is down here http://docs.sqlalchemy.org/en/latest/core/sqlelement.html#sqlalchemy.sql.expression.and_ (underscore ispart of link)Closes #2123 from saguziel/aguziel-fix-task-stats-2",0
[AIRFLOW-719] Prevent DAGs from ending prematurelyDAGs using ALL_SUCCESS and ONE_SUCCESS triggerrules were endingprematurely when upstream tasks were skipped.Changes mean that theALL_SUCCESS and ONE_SUCCESS triggers ruleencompasses both SUCCESS andSKIPPED tasks.Closes #2125 from dhuang/AIRFLOW-719,4
[AIRFLOW-939] add .swp to gitginoreCloses #2124 from saguziel/noswp,1
[AIRFLOW-942] Add mytaxi to Airflow usersCloses #2111 from terezaif/patch-1,1
[AIRFLOW-943] Update Digital First Media in users list- Fix incorrect link- Add new usersThere is no JIRA ticket for this change nor anytests involved.Closes #2115 from duffn/patch-2,3
[AIRFLOW-941] Use defined parameters for psycopg2This works aroundhttps://github.com/psycopg/psycopg2/issues/517 .Closes #2126 from bolkedebruin/AIRFLOW-941,0
[AIRFLOW-954] Fix configparser ImportErrorFixes support for Python 2.7 sincehttps://github.com/apache/incubator-airflow/pull/2091 was merged,7
[AIRFLOW-956] Get docs working on readthedocs.org,2
Merge pull request #2133 from dhuang/AIRFLOW-956,7
Merge pull request #2130 from seancron/airflow-954,7
"[AIRFLOW-931] Do not set QUEUED in TaskInstancesThe contract of TaskInstances stipulates that endstates for Taskscan only be UP_FOR_RETRY, SUCCESS, FAILED,UPSTREAM_FAILED orSKIPPED. If concurrency was reached task instanceswere set toQUEUED by the task instance themselves. This wouldprevent thescheduler to pick them up again.We set the state to NONE now, to ensure integrity.Closes #2127 from bolkedebruin/AIRFLOW-931",1
AIRFLOW-960 Add .editorconfig file,2
AIRFLOW-959 Cleanup and reorganize .gitignore,4
Merge pull request #2136 from gwax/update-gitignore,5
Merge pull request #2137 from gwax/editorconfig,5
[AIRFLOW-958] Improve tooltip readabilityCloses #2134 from jesusfcr/master,1
[AIRFLOW-967] Wrap strings in native for py2 ldap compatibilityldap3 has issues with newstr being passed. Thiswraps any callthat goes over the wire to the ldap server innative() to ensurethe native string type is used.Closes #2141 from bolkedebruin/AIRFLOW-967,1
Add Apache 2 License,1
"[AIRFLOW-910] Use parallel task execution for backfillsThe refactor to use dag runs in backfills caused aregressionin task execution performance as dag runs wereexecutedsequentially. Next to that, the backfills were nondeterministicdue to the random execution of tasks, causing roottasksbeing added to the non ready list too soon.This updates the backfill logic as follows:* Parallelize execution of tasks* Use a leave first execution model* Replace state updates from the executor by taskbased onlyCloses #2107 from bolkedebruin/AIRFLOW-910",5
[AIRFLOW-961] run onkill when SIGTERMedCloses #2138 from saguziel/aguziel-sigterm,1
AIRFLOW-932][AIRFLOW-932][AIRFLOW-921][AIRFLOW-910] Do not mark tasks removed when backfilling[In a backfill one can specify a specific task toexecute. Wecreate a subset of the orginal tasks in a subdagfrom the original dag.The subdag has the same name as the original dag.This breaksthe integrity check of a dag_run as tasks aresuddenly not inscope any more.Closes #2122 from bolkedebruin/AIRFLOW-921,2
"[AIRFLOW-900] Fixes bugs in LocalTaskJob for double run protectionRight now, a second task instance being triggeredwill causeboth itself and the original task to run becausethe hostnameand pid fields are updated regardless if the taskis already running.Also, pid field is not refreshed from db properly.Also, we shouldcheck against parent's pid.Will be followed up by working tests.Closes #2102 from saguziel/aguziel-fix-trigger-2",0
Fix tests for topological sort,2
[AIRFLOW-900] Double trigger should not kill original task instanceThis update the tests of an earlier AIRFLOW-900.Closes #2146 from bolkedebruin/AIRFLOW-900,3
"[AIRFLOW-770] Refactor BaseHook so env vars are always readThe WebHDFS and HDFS hooks ignore connections setin the environmentvariables because they use`BaseHook.get_connections()` directly,which fetches a list of connections from DB. Imoved that method'slogic to `_get_connections_from_db()` and made anew`get_connections()` that first checks environmentvariables beforefalling back on connections in DB. Also becauseconnection extrascannot be specified when using environmentvariables, I added an argto HDFSHook for using Snakebite'sAutoConfigClient, which can beinitialized without any connection info.Closes #2056 from dhuang/AIRFLOW-770",5
[AIRFLOW-917] Fix formatting of error messageVariables were interpolated into error messagein the wrong order.Closes #2109 from vijaykramesh/vijay/incorrect_format_of_slots_available,0
"[AIRFLOW-974] Fix mkdirs race conditionmkdirs congtained a race condition for when if thedirectory iscreated between the os.path.exists and theos.makedirs calls,the os.makedirs will fail with an OSError.This reworks the function to be non-recursive aswell, aspermission errors were due to umasks beingapplied.Closes #2147 from bolkedebruin/AIRFLOW-974",0
Update changelog for 1.8.0,4
[AIRFLOW-933] Replace eval with literal_eval to prevent RCEReplace eval with a literal eval to help prevent arbitrary codeexecution on the webserver host.,5
Merge pull request #2150 from artwr/artwr-fix_another_use_of_eval,0
[AIRFLOW-979] Add GovTech GDSCloses #2149 from chrissng/add-govtech-gds,1
[AIRFLOW-903] New configuration setting for the default dag viewAdded a new configuration setting for the defaultview a dag should display when clicked on theindex page.Make sure we do lower for jinja url_for functionCloses #2103 from jakromm/master,1
"[AIRFLOW-989] Do not mark dag run successful if unfinished tasksDag runs could be marked successful if all roottasks were successful,even if some tasks did not run yet, ie. in case ofclearing. Nowwe consider unfinished_tasks, before markingsuccessful.Closes #2154 from bolkedebruin/AIRFLOW-989",5
[AIRFLOW-995][AIRFLOW-1] Update GitHub PR TemplateCloses #2160 from jlowin/PR-template,5
[AIRFLOW-994] Add MiNODES to the official airflow user listAdd MiNODES to the official airflow user listCloses #2159 from dice89/master,1
"[AIRFLOW-997] Update setup.cfg to point to ApacheThe setup.cfg should point to the Apache PMC as ""author"" and thedev mailing list as contact email.",5
Merge pull request #2162 from artwr/artwr-update_setup_cfg,5
[AIRFLOW-984] Enable subclassing of SubDagOperatorNote this maintains an awkward name checkfor backwards compatibility reasons.Closes #2152 from patrickmckenna/recognize-subdag-subclasses,2
"[AIRFLOW-969] Catch bad python_callable argumentChecks for callable when Operator iscreated, not when it is run.* added initial PythonOperator unit test, testingrun* python_callable must be callable; added unittestCloses #2142 from abloomston/python-callable",3
[AIRFLOW-963] Fix non-rendered code examplesPlease accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-963Testing Done:- ran sphinx-build locally and confirmed correctlyrenderedCloses #2139 from sekikn/AIRFLOW-963,5
"[AIRFLOW-990] Fix Py27 unicode logging in DockerOperatorIf a Docker container's logs has unicode values(e.g. ""echo 😁""), theDockerOperator will try to log it using`logging.info(line)`, which raises a`UnicodeDecodeError`. To solve this, we need todecode the string as UTF-8before sending it to `logging.info()`.Closes #2155 from vitorbaptista/bug/fixes-AIRFLOW-990",0
"[AIRFLOW-705][AIRFLOW-706] Fix run_command bugs`run_command` currently uses `str.split` for shellcommands, but thisbreaks commands like `echo ""foo bar""`.  Instead,use `shlex.split`,which understands how to handle quotes.In python3, subprocess functions returnstdout/stderr as `bytes`, not`str`.  To fix py2/py3 compatibility,`run_command` now decodes theoutput so that it always returns a `str`.Dear Airflow Maintainers,Please accept this PR that addresses the followingissues:-https://issues.apache.org/jira/browse/AIRFLOW-705-https://issues.apache.org/jira/browse/AIRFLOW-706Testing Done:- Changed `sql_alchemy_conn` to`sql_alchemy_conn_cmd` in test configuration toexercise `run_command` in testsCloses #2053 from thesquelched/AIRFLOW-706",3
[AIRFLOW-681] homepage doc link should pointing to apache repo not airbnb repoCloses #2164 from bowenli86/AIRFLOW-681,2
"[AIRFLOW-995] Remove reference to actual Airflow issueRemove example reference to AIRFLOW-[one] becauseit confuses merge tools. In addition, simplifythe checkboxes because Github displays how manyof them have been checked off.Closes #2163 from jlowin/pr-template-2",1
[AIRFLOW-1010] Add convenience script for signing releasesCloses #2169 from bolkedebruin/AIRFLOW-1010,1
"[AIRFLOW-1005] Improve Airflow startup timeAirflow’s startup time can be reduced by 50% bydeferring imports of Cryptography (and relatedly,not generating Fernet keys unless we have to) andAlembic.",2
[AIRFLOW-1006] Move config templates to separate filesConfig templates are easier to work with as standalonefiles instead of giant strings inside configuration.py,5
"[AIRFLOW-1009] Remove SQLOperator from Concepts pageRemove SQLOperator from Concepts page. Update witha sample list of database backend-specificoperators, lile, MySqlOperator, SqliteOperator,PostgresOperator, MsSqlOperator, OracleOperator,JdbcOperatorCloses #2168 from Tagar/AIRFLOW-1009",5
Merge pull request #2166 from jlowin/speedup,7
"[AIRFLOW-999] Add support for Redis databaseThis PR includes a redis_hook and a redis_key_sensor to enablechecking for key existence in redis. It also updates thedocumentation and add the relevant unit tests.- [x] Opened a PR on Github- [x] My PR addresses the following Airflow JIRAissues:    -https://issues.apache.org/jira/browse/AIRFLOW-999- [x] The PR title references the JIRA issues. Forexample, ""[AIRFLOW-1] My Airflow PR""- [x] My PR adds unit tests- [ ] __OR__ my PR does not need testing for thisextremely good reason:- [x] Here are some details about my PR:- [ ] Here are screenshots of any UI changes, ifappropriate:- [x] Each commit subject references a JIRA issue.For example, ""[AIRFLOW-1] Add new feature""- [x] Multiple commits addressing the same JIRAissue have been squashed- [x] My commits follow the guidelines from ""[Howto write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":  1. Subject is separated from body by a blank line  2. Subject is limited to 50 characters  3. Subject does not end with a period  4. Subject uses the imperative mood (""add"", not""adding"")  5. Body wraps at 72 characters  6. Body explains ""what"" and ""why"", not ""how""Closes #2165 from msempere/AIRFLOW-999/support-for-redis-database",5
"[AIRFLOW-1006] Add config_templates to MANIFESTWithout this line, the config templates are notincluded when Airflowis installedCloses #2173 from jlowin/speedup-2",5
[AIRFLOW-1017] get_task_instance shouldn't throw exception when no TIget_task_instance should return None instead ofthrowing exception in the case where dagrun does not have the taskinstance.Closes #2178 from aoen/ddavydov--one_instead_of_first_for_dagrun,2
[AIRFLOW-1040] Fix some small typos in comments and docstringsCloses #2174 from mschmo/fix-some-typos-models,2
[AIRFLOW-1047] Sanitize strings passed to MarkupWe add the Apache-licensed bleach library and useit to sanitize htmlpassed to Markup (which is supposed to be alreadyescaped). This avoidssome XSS issues with unsanitized user input beingdisplayed.Closes #2193 from saguziel/aguziel-xss,1
"[AIRFLOW-1045] Make log level configurable via airflow.cfgFor now, changing log level needs to modifysettings.py directly.It's inconvenient. This PR makes it configurablevia airflow.cfg.Closes #2191 from sekikn/loglevel",2
[AIRFLOW-1034] Make it possible to connect to S3 in sigv4 regionsCloses #2181 from buyology/fix-s3-in-sigv4-regions,2
[AIRFLOW-985] Extend the sqoop operator and hookThe sqoop operator was a bit outdated and neededsome reworkincluding tests. Many lines have changed becausethe code neededsome restructuring for better testing. Removed thehive_home andjob_tracker because they are not used in any wayinside of thesqoop class. Moved the num-mappers argument to theconstructorbecause it is used for both importing andexporting. Addedsupport for parquet. Added the ability to set thedriver and directmode and ability to pass jvm parameters to sqoop.Closes #2177 from Fokko/airflow-985-extend-sqoop-operator-hook,1
[AIRFLOW-840] Make ticket renewer python3 compatibleThe return from the subprocess is in bytes whenthe universalnewlines is set to False (default). This will failin Py3 andworks fine in Py2. And with a working unit test.Closes #2158 from abij/AIRFLOW-840,3
[AIRFLOW-1043] Fix doc strings of operatorsCloses #2188 from gtoonstra/feature/AIRFLOW-1043,1
"Revert ""[AIRFLOW-719] Prevent DAGs from ending prematurely""This reverts commit 1fdcf2480555f06cce3fc9bba97fbf3d64f074d3.This reinstates the previous logic (< 1.8.0) that ALL_SUCCESS requiresall tasks to be successful instead of also counting SKIPPEDtasks as part of the successful tasks.",1
"[AIRFLOW-719] Fix race condition in ShortCircuit, Branch and LatestOnlyBoth the ShortCircuitOperator, Branchoperator and LatestOnlyOperator were arbitrarily changing the states of TaskInstances without lockingthem in the database. As the scheduler checks the state of dag runsasynchronously the dag run state could be set to failed while theoperators are updating the downstream tasks.A better fix would to use the dag run iteself in the context of theOperator.",1
"[AIRFLOW-1007] Use Jinja sandbox for chart_data endpointRight now, users can put in arbitrary strings intothe chart_dataendpoint, and execute arbitrary code using thechart_data endpoint. Byusing literal_eval andImmutableSandboxedEnvironment, we can reduce RCE.Right now, users can put in arbitrary strings intothe chart_dataendpoint, and execute arbitrary code using thechart_data endpoint. Byusing literal_eval andImmutableSandboxedEnvironment, we can preventRCE.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1007### Description- [x] I changed Jinja to use theImmutableSandboxedEnvironment, and usedliteral_eval, to limit the amount of RCE.### Tests- [x] My PR adds the following unit tests:SecurityTest chart_data tests### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""to: aoen plypaul artwr  bolkedebruinCloses #2184 from saguziel/aguziel-jinja-2",1
[AIRFLOW-1054] Fix broken import in test_dagCloses #2201 fromr39132/fix_broken_import_on_test_dag,3
"[AIRFLOW-1038] Specify celery serialization options explicitlySpecify the CELERY_TASK_SERIALIZER and CELERY_RESULT_SERIALIZER aspickle explicitly, and CELERY_EVENT_SERIALIZER as json.",5
Merge pull request #2185 from saguziel/aguziel-celery-fix,0
"[AIRFLOW-1011] Fix bug in BackfillJob._execute() for SubDAGsBackfillJob._execute() checks that the next rundate is less thanor equal to the end date before creating a DAG runand taskinstances. For SubDAGs, the next run date is notrelevant,i.e. schedule_interval can be anything other thanNoneor '@once' and should be ignored. However, currentcode calculatesthe next run date for a SubDAG and the conditioncheck mentionedabove always fails for SubDAG triggered manually.This change adds a simple check to determine ifthis is a SubDAGand, if so, sets next run date to DAG run's startdate.Closes #2179 from joeschmid/AIRFLOW-1011-fix-bug-backfill-execute-for-subdags",2
"[AIRFLOW-1062] Fix DagRun#find to return correct resultDagRun#find returns wrong result ifexternal_trigger=False is specified,because adding filter is skipped on thatcondition. This PR fixes it.Closes #2210 from sekikn/AIRFLOW-1062",0
"[AIRFLOW-1004][AIRFLOW-276] Fix `airflow webserver -D` to run in backgroundAIRFLOW-276 introduced a monitor process forgunicornto find new files in the dag folder, but it alsochanged`airflow webserver -D`'s behavior to run inforeground.This PR fixes that by running the monitor as adaemon process.Closes #2208 from sekikn/AIRFLOW-1004",1
[AIRFLOW-1051] Add a test for resetdb to CliTestsCliTests lacks a test for resetdb command for now.It should be added.Closes #2198 from sekikn/AIRFLOW-1051,1
[AIRFLOW-1030][AIRFLOW-1] Fix hook import for HttpSensorCloses #2180 frompdambrauskas/fix/http_hook_import,2
Merge branch 'AIRFLOW-719' into AIRFLOW-719-3,7
Merge pull request #2195 from bolkedebruin/AIRFLOW-719,7
"[AIRFLOW-1064] Change default sort to job_id for TaskInstanceModelViewThe TaskInstanceModelView default sort column ison an unindexed column.We shouldn't need an index on start_date, andjob_id is just as logicalof a default sort.Closes #2215 from saguziel/aguziel-fix-ti-page",0
"[AIRFLOW-1067] use example.com in examplesWe use airflow@airflow.com in examples. However,https://airflow.comis owned by a company named Airflow (selling fans,etc). We should useairflow@example.com instead. That domain iscreated for this purpose.Closes #2217 from mengxr/AIRFLOW-1067",1
[AIRFLOW-947] Improve exceptions for unavailable Presto clusterThis improves error logging when the Presto cluster is unavailableand the underlying error is a 503 http response. This introspectsthe error to prevent trying to access the 'message' attribute whennot present.,1
Merge pull request #2128 from artwr/artwr-improve_presto_hook_error_when_cluster_is_unavailable,0
[AIRFLOW-1065] Add functionality for Azure Blob Storage over wasb://This PR implements a hook to interface with Azurestorage over wasb://via azure-storage; adds sensors to check for blobsor prefixes; andadds an operator to transfer a local file to theBlob Storage.Design is similar to that of the S3Hook inairflow.operators.S3_hook.Closes #2216 from hgrif/AIRFLOW-1065,1
[AIRFLOW-1001] Fix landing times if there is no following schedule@once does not have a following schedule. This wasnot checked forand therefore the landing times page could bork.Closes #2213 from bolkedebruin/AIRFLOW-1001,0
"[AIRFLOW-111] Include queued tasks in scheduler concurrency checkThe concurrency argument in dags appears to not beobeyed because thescheduler does not check the concurrency properlywhen checking tasks.The tasks do not run, but this leads to a lot ofscheduler churn.Closes #2214 from saguziel/aguziel-fix-concurrency",0
[AIRFLOW-970] Load latest_runs on homepage asyncThe latest_runs column on the homepage loadssynchronously with an n+1query. Homepage loads will be significantly fasterif this happensasynchronously and as a batch.Closes #2144 from saguziel/aguziel-latest-run-async,3
"[AIRFLOW-1016] Allow HTTP HEAD request method on HTTPSensorThis PR provides the HEAD http method on top of GET. This is usefulfor getting responses without a body, and provides a lighter weightresponse.Closes #2175 from msempere/AIRFLOW-1016/allow-http-head-request-method-on-httpsensor",1
[AIRFLOW-1033][AIFRLOW-1033] Fix ti_deps for no schedule dagsDAGs that did not have a schedule (None or @once)make the dependencychecker raise an exception as the previousschedule will not exist.Also activates all ti_deps tests.Closes #2220 from bolkedebruin/AIRFLOW-1033,3
[AIRFLOW-1075] Security docs cleanupCloses #2222 from dhuang/AIRFLOW-1075,4
"[AIRFLOW-1028] Databricks Operator for AirflowAdd DatabricksSubmitRun OperatorIn this PR, we contribute a DatabricksSubmitRun operator and aDatabricks hook. This operator enables easy integration of Airflowwith Databricks. In addition to the operator, we have created adatabricks_default connection, an example_dag using thisDatabricksSubmitRunOperator, and matching documentation.Closes #2202 from andrewmchen/databricks-operator-squashed",5
[AIRFLOW-1050] Do not count up_for_retry as not readyup_for_retry tasks were incorrectly countedtowards not_readytherefore marking a dag run deadlocked instead ofretrying.Closes #2225 from bolkedebruin/AIRFLOW-1050,1
"[AIRFLOW-1085] Enhance the SparkSubmitOperator- Allow the Spark home to be set on per connectionbasis to obviate  the need for the spark-submit to be on the PATH,and allows different  versions of Spark to be easily used.- Enable the use of the --driver-memory parameteron the spark-submit  by making it parameter on the operator- Enable the use of the --class parameter on thespark-submit by making  it a parameter on the operatorCloses #2211 from camshrun/sparkSubmitImprovements",1
[AIRFLOW-1078] Fix latest_runs endpoint for old flask versionsOld versions of flask (<0.11) dont support jsonifyon arrays due anECMAScript 4 vulnerability in older browsers. Thisshould work on oldflask versions as well.Closes #2224 from saguziel/aguziel-fix-homepage,0
[AIRFLOW-1081] Improve performance of duration chartThis commit reduces the number of queries toimprove perf.Closes #2226 from saguziel/aguziel-duration-chart-fix,2
[AIRFLOW-1035] Use binary exponential backoffCloses #2196 from IvanVergiliev/exponential-backoff,1
[AIRFLOW-1090] Add HBOCloses #2230 from yiwang/AIRFLOW-1090,1
"[AIRFLOW-1095] Make ldap_auth memberOf come from configurationIf the key ldap/group_member_attr is set in theairflow.cfg, this value is used to lookup groupsfor the user.Closes #2232 from vfoucault/fixbug/ldap_auth",0
"[AIRFLOW-1074] Don't count queued tasks for concurrency limitsThere may be orphaned tasks queued but not in arunning dag run thatwill not cleared. We should not count these asthey will interfere.I hate to do this, but I changed my mind oncounting queued tasks.1. Queued tasks that are actually queued generallyget set to running pretty quickly.2. Because of the worker-side check, we won'tactually pass concurrency.I don't think the queued thing is a big dealbecause of this, I'm more worried about orphanedtasks that are in QUEUED state but not in arunning dag_run (so they wont get reset)interfering with concurrency.There may be orphaned tasks queued but not in arunning dag run thatwill not cleared. We should not count these asthey will interfere.Closes #2221 from saguziel/aguziel-concurrency-2",1
[AIRFLOW-1109] Use kill signal to kill processes and log resultsThe kill_process_tree function comments state thatit uses SIGKILL whenit uses SIGTERM. We should update this to becorrect as well as logresults.Closes #2241 from saguziel/aguziel-kill-processes,2
[AIRFLOW-1106] Add Groupalia/Letsbonus to the ReadMeCloses #2239 from jesusfcr/docs,2
[AIRFLOW-1112] Log which pool when pool is full in schedulerCloses #2242 from saguziel/aguziel-logging-pool-scheduler,2
[AIRFLOW-1094] Run unit tests under contrib in TravisRename all unit tests under tests/contrib to startwith test_* and fixbroken unit tests so that they run for the Python2 and 3 builds.Closes #2234 from hgrif/AIRFLOW-1094,1
[AIRFLOW-1000] Rebrand distribution to Apache AirflowPer Apache requirements Airflow should be brandedApache Airflow.It is impossible to provide a forward compatibleautomatic updatepath and users will be required to manuallyupgrade.Closes #2172 from bolkedebruin/AIRFLOW-1000,1
[AIRFLOW-1107] Add support for ftps non-default portCloses #2240 from jesusfcr/ftps,1
[AIRFLOW-1091] Add script that can compare jira target against mergesWhen working towards a release it is convenient tobe able to comparewhat has been merged into the current branch andwhat has not.Closes #2231 from bolkedebruin/AIRFLOW-1091,7
[AIRFLOW-1120] Update version view to include Apache prefixCloses #2244 from criccomini/AIRFLOW-1120,5
[AIRFLOW-1124] Do not set all tasks to scheduled in backfillBackfill is supposed to fill in the blanks and notto rescheduleall tasks. This fixes a regression from 1.8.0.Closes #2247 from bolkedebruin/AIRFLOW-1124,0
"[AIRFLOW-1121][AIRFLOW-1004] Fix `airflow webserver --pid` to write out pid fileAfter AIRFLOW-1004, --pid option is no longerhonored andthe pid file is not being written out. This PRfixes it.Closes #2249 from sekikn/AIRFLOW-1121",0
[AIRFLOW-1118] Add evo.company to Airflow usersCloses #2243 from Orhideous/master,1
[AIRFLOW-1127] Move license notices to LICENSECloses #2250 from bolkedebruin/AIRFLOW-1127,4
[AIRFLOW-1136] Capture invalid arguments for SqoopInvalid arguments are not captured for theSqoopHook and SqoopOperator:- SqoopHook should raise an exception if thefile_type is invalid- SqoopOperator should raise an exception if thecmd_type is invalidCloses #2252 from hgrif/AIRFLOW-1136,1
[AIRFLOW-1138] Add missing licenses to files in scripts directoryCloses #2253 from criccomini/AIRFLOW-1138,5
[AIRFLOW-1122] Increase stroke width in UIA stroke width of 2px is to narrow to determinethe color for people with color vision deficiency.A 3px stroke is much more accessible.Closes #2246 from michaelosthege/master,1
[AIRFLOW-1125] Document encrypted connectionsClarify documentation regarding fernet_key and howtoenable encryption if it was not enabled duringinstall.Closes #2251 from boristyukin/airflow-1125,0
[AIRFLOW-1089] Add Spark application argumentsAllows arguments to be passed to the Sparkapplication beingsubmitted. For example:- spark-submit --class foo.Bar foobar.jar arg1arg2- spark-submit app.py arg1 arg2Closes #2229 from camshrun/sparkSubmitAppArgs,1
[AIRFLOW-1119] Fix unload query so headers are on first row[]Closes #2245 from th11/airflow-1119-fix,0
"[AIRFLOW-492] Make sure stat updates cannot fail a taskPreviously a failed commit into the db for thestatisticscould also fail a task. Secondly, the ui coulddisplayout of date statistics.This patch reworks DagStat so that failure toupdate thestatistics does not propagate. Next to that, itmake sure the ui always displays the lateststatistics.Closes #2254 from bolkedebruin/AIRFLOW-492",3
[AIRFLOW-1142] Do not reset orphaned state for backfillsThe scheduler could interfere with backfills whenit resets the stateof tasks that were considered orphaned. This patchprevents the schedulerfrom doing so and adds a guard in the backfill.Closes #2260 from bolkedebruin/AIRFLOW-1142,1
[AIRFLOW-1155] Add Tails.com to communityCloses #2261 fromalanmcruickshank/tails_community_docs,2
"[AIRFLOW-1036] Randomize exponential backoffThis prevents the thundering herd problem. Using acombination ofdag_run, task_id, and execution_date makes thisrandom with respect totask instances, while still being deterministicacross machines. Theretry delay is within a range that doubles insize.Closes #2262 from saguziel/aguziel-random-exponential-backoff",1
[AIRFLOW 1149][AIRFLOW-1149] Allow for custom filters in Jinja2 templatesCloses #2258 fromNielsZeilemaker/jinja_custom_filters,1
[AIRFLOW-1160] Update Spark parameters for MesosCloses #2265 from cameres/master,2
"[AIRFLOW-1140] DatabricksSubmitRunOperator should template the ""json"" field.Add ""json"" in the templated_fields list for theDatabricksSubmitRunOperator.Closes #2255 from andrewmchen/DatabricksOperator-templated",5
[AIRFLOW-945][AIRFLOW-941] Remove psycopg2 connection workaroundCloses #2272 from dlackty/AIRFLOW-945,1
[AIRFLOW-1173] Add Robinhood to who uses AirflowCloses #2271 from vineet-rh/patch-2,1
[AIRFLOW-1167] Support microseconds in FTPHook modification timeCloses #2268 from NielsZeilemaker/fix-ftp-hook,0
[AIRFLOW-1179] Fix Pandas 0.2x breaking Google BigQuery changeCloses #2279 from NielsZeilemaker/AIRFLOW-1179,1
[AIRFLOW-1181] Add delete and list functionality to gcs_hookCloses #2281 from mattuuh7/gcs-delete-list,4
"[AIRFLOW-XXX] Updating CHANGELOG, README, and UPDATING after 1.8.1 release",5
[AIRFLOW-1185] Fix PyPi URL in templatesCloses #2283 from maksim77/master,0
[AIRFLOW-1187][AIRFLOW-1185] Fix PyPi package names in documentsCloses #2285 from sekikn/AIRFLOW-1187,2
[AIRFLOW-1188] Add max_bad_records param to GoogleCloudStorageToBigQueryOperatorCloses #2286 from ckpklos/master,1
[AIRFLOW-1168] Add closing() to all connections and cursorsThis will prevent any left-open connectionswhenever an exception occursCloses #2269 from NielsZeilemaker/AIRFLOW-1168,1
[AIRFLOW-1193] Add Checkr to company using AirflowCloses #2276 from tongboh/patch-1,1
"[AIRFLOW-1141] remove crawl_for_tasksThis method is defined in `models.DAG`, but it isnot used by anywhere.The method doesn't implement anything (throwing`NotImplementedError`).I suspect that this is an artifact from Airbnbdays.Closes #2275 from jeeyoungk/AIRFLOW-1141",0
"[AIRFLOW-1150] Fix scripts execution in sparksql hook[]When using the the SparkSqlOperator and submittinga file (ending with`.sql` or `.hql`), a whitespace need to beappended, otherwise a Jinjaerror will be raised.However the trailing whitespace confused the hookas those files willnot end with `.sql` and `.hql`, but with `.sql `and `.hql `. This PRfixes this.In the test, I've added the `get_after` functionto easily check if thepath is really stripped or not by the `-f` option.Closes #2259 from gglanzani/master",1
[AIRFLOW-1175] Add Pronto Tools to Airflow user listCloses #2277 fromzkan/add_pronto_tools_to_airflow_user_list,1
[AIRFLOW-823] Allow specifying execution date in task_info APICloses #2045 from robin-miller-ow/release/API_TaskInstanceInfo,5
[AIRFLOW-1182] SparkSubmitOperator template field,1
[AIRFLOW-1184] SparkSubmitHook does not split args,1
[AIRFLOW-1189] Fix get a DataFrame using BigQueryHook failingCloses #2287 from mremes/master,0
Merge pull request #2289 from vfoucault/feature/AIRFLOW-1182_1184,7
"[AIRFLOW-860][AIRFLOW-935] Fix plugin executor import cycle and executor selectionWhen a plugin is made with a custom Operator andexecutor, an import cycle occurs when the executoris chosen in airflow.cfg because theexecutors/__init__.py starts loading plugins tooearly.changed DEFAULT_EXECUTOR use to a function callwhich returns the default executor. This lazyapproach fixes the import cycle.revision eb5982d (included in 1.8) breaks pluginexecutors altogether. It makes a new module forevery plugin, so import statements need to beadapted, but the executor selection is leftunchanged, so it ends up assigning the pluginmodule as an executor.fixed executor selection to work with the newplugin modules system introduced in 1.8. inAirflow.cfg a executor can now be specified as{plugin_name}.{executor_name}Fixes: -https://issues.apache.org/jira/browse/AIRFLOW-860 -https://issues.apache.org/jira/browse/AIRFLOW-935Closes #2120 from stverhae/master",0
[AIRFLOW-1041] Do not shadow xcom_push method[]Closes #2274 from ludovicc/AIRFLOW-1041,5
"[AIRFLOW-1170] DbApiHook insert_rows inserts parameters separatelyInstead of creating a sql statement with allvalues, we send the valuesseparately to prevent sql injectionCloses #2270 from NielsZeilemaker/AIRFLOW-1170",1
"[AIRFLOW-993] Update date inference logicDAGs should set task start_date and end_date whenpossible, making surethey agree with the DAG’s own dates.Closes #2157 from jlowin/run-bug",0
"[AIRFLOW-1180] Fix flask-wtf version for test_csrf_rejectionFor now, SecurityTests.test_csrf_rejection failsbecause flask-wtf version specified in setup.py istoo old.This PR fixes it.Closes #2280 from sekikn/AIRFLOW-1180",0
"[AIRFLOW-1145] Fix closest_date_partition function with before set to TrueIf we're looking for the closest date before, we should take the latest date in the list of date before.Closes #2257 from julien-gm/fix_closest-date-partition",5
[AIRFLOW-1203] Pin Google API client version to fix OAuth issueCloses #2296 from criccomini/AIRFLOW-1203,5
Add Quora and Tictail to companies using Airflow in READMEWith the informal approval of my points of contactof course.Closes #2297 from mistercrunch/tic_quora,1
"[AIRFLOW-1186] Sort dag.get_task_instances by execution_datetask.get_task_instances is sorted byexecution_date, so we sortdag.get_task_instances by execution_date so itdoesn't breakduration chartCloses #2284 from OpringaoDoTurno/fix-duration",0
"[AIRFLOW-1201] Update deprecated 'nose-parameterized'The 'parameterized' package should be used now,Closes #2298 from skudriashev/airflow-1201",1
[AIRFLOW-1213] Add hcatalog parameters to sqoopAdd parameters to specify a hive table instead ofa hdfs directory.This will also source the schema from the hivemetastore.Closes #2305 from Fokko/AIRFLOW-1213-add-hcatalog-parameters,2
[AIRFLOW-1207] Enable utils.helpers unit testsCloses #2300 from skudriashev/airflow-1207,3
[AIRFLOW-1200] Forbid creation of a variable with an empty keyCloses #2299 from skudriashev/airflow-1200,1
[AIRFLOW-1199] Fix create modalCloses #2293 from skudriashev/airflow-1199,1
[AIRFLOW-1210] Enable DbApiHook unit testsCloses #2302 from skudriashev/airflow-1210,3
[AIRFLOW-1221] Fix templating bug with DatabricksSubmitRunOperatorIn our implementation ofDatabricksSubmitRunOperator we mistakenlyswitched the order of the tuples returned byenumerate. The bugis fixed in this commit and an additional unittest is added toverify that this bug is fixed.Closes #2308 from andrewmchen/fix-templating-bug,0
[AIRFLOW-1226] Remove empty column on the Jobs viewCloses #2309 from skudriashev/airflow-1226,4
[AIRFLOW-1227] Remove empty column on the Logs viewCloses #2310 from skudriashev/airflow-1227,2
[AIRFLOW-1233] Cover utils.json with unit testsCloses #2316 from skudriashev/airflow-1233,3
[AIRFLOW-1232] Remove deprecated readfp warningCloses #2315 from skudriashev/airflow-1232,2
[AIRFLOW-1231] Use flask_wtf.CSRFProtectUse `flask_wtf.CSRFProtect` instead of`flask_wtf.CsrfProtect`to remove deprecation warning.Closes #2313 from skudriashev/airflow-1231,2
"[AIRFLOW-645] Support HTTPS connections in HttpHookConsider the connection schema when building theURL in HttpHook. Thisallows making HTTPS requests. If a schema is notspecified in theconnection, default to HTTP. If the connectionhost contains a schema,use it instead to maintain backwardscompatibility.Closes #2311 from johnzeringue/AIRFLOW-645",1
"[AIRFLOW-1217] Enable Sqoop loggingThe output of the subprocess was not redirected toairflow, now thesqoop output gets written to Airflow and capturedin the logs.Extended the tests of the sqoop popen operation.Closes #2307 from Fokko/AIRFLOW-1217-enable-sqoop-logging",2
[AIRFLOW-1234] Cover utils.operator_helpers with UTsCloses #2317 from skudriashev/airflow-1234,1
"[AIRFLOW-1191] : SparkSubmitHook custom cmdAdd the capability to set the spark-submit binary to call.The default behaviour set the spark-submit command to'spark-submit', or to set it via a Spark env var.the spark binary can now be set in the spark connection.Test coverage extended for the new settings.",1
[AIRFLOW-1197] : SparkSubmitHook on_kill errorThe on_kill method was buggy. Some corrections as beenmade with the killcmd and globally the method.Test coverage update to reflect changes.,4
[AIRFLOW-1248] Fix wrong conf name for worker timeoutThe parameter `web_server_worker_timeout` iswronglyreferenced by airflow/bin/cli.py. This PR fixesit.Closes #2328 from sekikn/AIRFLOW-1248,0
[AIRFLOW-1245] Fix random failure in test_trigger_dag_for_dateCloses #2325 from skudriashev/airflow-1245,5
[AIRFLOW-1243] DAGs table has no default entries to showCloses #2323 from skudriashev/airflow-1243,2
[AIRFLOW-1237] Fix IN-predicate sqlalchemy warningCloses #2320 from skudriashev/airflow-1237,2
[AIRFLOW-908] Print hostname at the start of cli runCloses #2329 from AllisonWang/master,1
[AIRFLOW-1251] Add eRevalue to Airflow usersCloses #2331 from hamedhsn/master,1
[AIRFLOW-1256] Add United Airlines to readmeCloses #2332 from r39132/master,1
Merge pull request #2292 from vfoucault/airflow_1191_1197,7
"[AIRFLOW-654] Add SSL Config Option for CeleryExecutor w/ RabbitMQ- Add BROKER_USE_SSL config to give option to send AMQP messages over SSL- Can be set using usual airflow options (e.g. airflow.cfg, env vars, etc.)Closes #2333 from forsberg/ssl_amqp",5
[AIRFLOW-1274][HTTPSENSOR] Rename parameter params to dataRenamed the parameter `params` to `request_params` in `HttpSensor` as itwas conflicting with `BaseOperator` `params` and causing failed DAGsparsing.Closes #2342 from aliceabe/master,2
[AIRFLOW-1244] Forbid creation of a pool with empty nameCloses #2324 from skudriashev/airflow-1244,1
[AIRFLOW-1266] Increase width of gantt y axisIncrease the width of the gantt view y axis toaccommodate largertask names.Closes #2345 from aoen/ddavydov--increase_width_of_gantt_y_axis,1
[AIRFLOW-1263] Dynamic height for chartsDynamic heights for webserver charts so thatlonger tasknames fitCloses #2344 from aoen/ddavydov--dynamic_chart_heights,2
[AIRFLOW-1276] Forbid event creation with end_data earlier than start_date,5
[AIRFLOW-1277] Forbid KE creation with empty fieldsKE = KnownEvent,1
Merge pull request #2349 from skudriashev/airflow-1277,7
[AIRFLOW-1281] Sort variables by key field by defaultCloses #2347 from skudriashev/airflow-1281,5
[AIRFLOW-1192] Some enhancements to qubole_operator1. Upgrade qds_sdk version to latest2. Add support to run Zeppelin Notebooks3. Move out initialization of QuboleHook frominit()Closes #2322 from msumit/AIRFLOW-1192,5
[AIRFLOW-1208] Speed-up cli testsCloses #2301 from skudriashev/airflow-1208,3
[AIRFLOW-1166] Speed up _change_state_for_tis_without_dagrun_change_state_for_tis_without_dagrun was locking asignificantamount of tasks uncessarily. This could end up ina deadlockin the database due to the time the lock stood.Closes #2267 from bolkedebruin/fix_deadlock,0
[AIRFLOW-1282] Fix known event column sortingCloses #2350 from skudriashev/airflow-1282,0
[AIRFLOW-1242] Allowing project_id to have a colon in it.Closes #2335 from zoyahav/master,1
[AIRFLOW-1290] set docs author to 'Apache Airflow',2
Merge pull request #2352 from mistercrunch/remove_max_author,4
[AIRFLOW-1265] Fix exception while loading celery configurationsCloses #2340 from orezahc/celery-bug-fix,0
"[AIRFLOW-1024] Ignore celery executor errors (#49)Code defensively around the interactions withcelery so thatwe just log errors instead of crashing thescheduler.It might makes sense to make the try catches onelevel higher(to catch errors from all executors), but thisneeds some investigation.Closes #2355 from aoen/ddavydov--handle_celery_executor_errors_gracefully",0
[AIRFLOW-1289] Removes restriction on number of scheduler threadsThis removes the restriction that the number ofthreads can be at mostthe number of CPU cores. There's no reason to havethis restriction.Closes #2353 from saguziel/aguziel-increase-cores,1
[AIRFLOW-1301] Add New Relic to list of companiesCloses #2359 from marcweil/patch-1,1
[AIRFLOW-1291] Update NOTICE and LICENSE files to match ASF requirementsJIRA:https://issues.apache.org/jira/browse/AIRFLOW-1291* Update NOTICE with proper year range for ASFcopyright* Break down LICENSE intolicenses/LICENSE-[project].txt* add license header to jqClock.min.js[AIRFLOW-1291] Update NOTICE and LICENSE files tomatch ASF requirements* Update NOTICE with proper year range for ASFcopyright* Break down LICENSE intolicenses/LICENSE-[project].txt* add license header to jqClock.min.jsfix license checkCloses #2354 frommistercrunch/copyright_license_touchups,0
[AIRFLOW-1299] Support imageVersion in Google Dataproc clusterCloses #2358 from yu-iskw/dataproc-image-version,5
"[AIRFLOW-1294] Backfills can loose tasks to executeIn backfills we can loose tasks to execute due toa tasksetting its own state to NONE if concurrencylimits are reached,this makes them fall outside of the scope thebackfill ismanaging hence they will not be executed.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [X] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1294### Description- [X] Here are some details about my PR, includingscreenshots of any UI changes:In backfills we can loose tasks to execute due toa tasksetting its own state to NONE if concurrencylimits are reached,this makes them fall outside of the scope thebackfill ismanaging hence they will not be executed.### Tests- [X] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Should be covered by current tests, will adjust ifrequired.### Commits- [X] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""mistercrunch aoen saguziel This is a simplifiedfix that should be easier to digest in 1.8.2. Itdoes not address all underlying issues as inhttps://github.com/apache/incubator-airflow/pull/2356 , but those can be addressedseparately and in smaller bits.Closes #2360 from bolkedebruin/fix_race_backfill_2",0
"[AIRFLOW-936] Add clear/mark success for DAG in the UIThis PR adds a modal popup when clicking circleDAG icon in Airflow tree view UI. It adds thefunctionalities to clear/mark success of theentire DAG run. This behavior is equivalent toindividually clear/mark each task instance in theDAG run. The original logic of editing DAG runpage is moved to the button ""Edit DAG Run"".Closes #2339 from AllisonWang/master",1
[AIRFLOW-1172] Support nth weekday of the month cron expressionCloses #2321 from sekikn/AIRFLOW-1172,1
[AIRFLOW-1308] Disable nanny usage for DaskNanny is deprecated and results in build errors.Closes #2366 from bolkedebruin/fix_dask,0
Pin Hive and Hadoop to a specific version and create writable warehouse dir,1
Merge branch 'fix_travis',0
Re-enable caching for hadoop components,0
[AIRFLOW-1286] Use universal newline when opening log filesCloses #2363 from ronfung/Airflow-1286,2
[AIRFLOW-1317] Fix minor issues in API reference,0
Merge pull request #2374 from sekikn/AIRFLOW-1317,7
Updating README.md to add Pandora Media Inc. as a userCloses #2371 from Acehaidrey/master,1
"[AIRFLOW-1296] Propagate SKIPPED to all downstream tasksThe ShortCircuitOperator and LatestOnlyOperatordid not markall downstream tasks as skipped, but only directdownstreamtasks.Closes #2365 from bolkedebruin/AIRFLOW-719-3",3
[AIRFLOW-1275] Put 'airflow pool' into APICloses #2346 from skudriashev/airflow-1275,5
[AIRFLOW-1339] Add Drivy to the list of users,1
Merge pull request #2389 from AntoineAugusti/drivy,7
[AIRFLOW-1335] Use MemoryHandler for buffered loggingCloses #2386 from saguziel/aguziel-buffer-logger-apache,2
[AIRFLOW-1320] Update LetsBonus users in READMECloses #2376 from OpringaoDoTurno/AIRFLOW-1320,1
[AIRFLOW-1337] Allow log format customization via airflow.cfgCloses #2392 from ronfung/customize-logging-thru-config,5
[AIRFLOW-1333] Enable copy function for Google Cloud Storage HookCloses #2385 from yk5/gcs_hook_copy,1
[AIRFLOW-1338] Fix incompatible GCP dataflow hookCloses #2388 from fenglu-g/master,1
[AIRFLOW-1344] Fix text encoding bug when reading logs for Python 3.5Closes #2394 from anselmwang/master,2
"[AIRFLOW-801] Remove outdated docstring on BaseOperatorThe docstring of the BaseOperator still makesreference toit inheriting from SQL Alchemy's Base class,which it no longer does. So that should beremoved.Closes #2373 from sekikn/AIRFLOW-801",4
[AIRFLOW-1338][AIRFLOW-782] Add GCP dataflow hook runner change to UPDATING.mdCloses #2326 from yk5/df-python,5
[AIRFLOW-1337] Make log_format key names lowercaseCloses #2395 from ronfung/master,2
"[AIRFLOW-1321] Fix hidden field key to ignore caseWebserver has a feature to hide sensitive variable fields,which key contain specific words. But its matching iscase-sensitive, so ""google_api_key"" is hidden but""GOOGLE_API_KEY"" is not. This behaviour is not intuitive,so this PR fixes it to be case-insensitive.",0
"[AIRFLOW-1273]AIRFLOW-1273] Add Google Cloud ML version and model operatorsIncludes Google Cloud ML hooks for version andmodel operations,and their unit tests.https://issues.apache.org/jira/browse/AIRFLOW-1273Closes #2379 from N3da/master",3
[AIRFLOW-1273] Add Google Cloud ML version and model operatorshttps://issues.apache.org/jira/browse/AIRFLOW-1273Closes #2379 from N3da/master,0
[AIRFLOW-1343] Add Airflow default label to the dataproc operatorCloses #2396 from XiangbingJi/master,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1334] Check if tasks are backfill on scheduler in a joinCloses #2384 from saguziel/aguziel-use-join-apache,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1350] Add query_uri param to Hive/SparkSQL DataProc operatorCloses #2402 from lukeFalsina/master,1
"[AIRFLOW-1352][AIRFLOW-1335] Revert MemoryHandler change ()[]Revert ""[AIRFLOW-1335] Use MemoryHandler forbuffered logging""This reverts commit0d23d30d6812fe1eb3cfb52d2992131cbf028062.Closes #2403 from saguziel/aguziel-revert-mem-logger",4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1272] Google Cloud ML Batch Prediction OperatorCloses #2390 fromjiwang576/GCP_CML_batch_prediction,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1265] Fix celery executor parsing CELERY_SSL_ACTIVEChanged retrieval of celery/celery_ssl_active touse configuration.getboolean()Add correct except block and log warning ifcelery/celery_ssl_active key is left undefinedCloses #2341 from holygits/master,2
"[AIRFLOW-1367] Pass Content-IDTo reference inline images in an email, we need to be able to add<img src=""cid:{}""/> to the HTML. However currently the Content-ID (cid)is not passed, so we need to add itCloses #2410 from aliceabe/master",1
Merge pull request #2400 from sekikn/AIRFLOW-1321,7
"[AIRFLOW-1343] Fix dataproc label formatDataproc label must conform to the following regex:[a-z]([-a-z0-9]*[a-z0-9])?. Current ""airflow_version""label violates this format. This commit fixes the formatand updates the unittest to prevent future violations.Closes #2413 from fenglu-g/master",3
[AIRFLOW-300] Add Google Pubsub hook and operatorOnly publishing and topic creation are included.Topic consumption was explicitly not included inthis feature request.Closes #2036 from wwlian/airflow-300,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1271] Add Google CloudML Training OperatorCloses #2408 from leomzhong/cloudml_training,5
[AIRFLOW-1300] Enable table creation with TBLPROPERTIESEnable TBLPROPERTIES parameter in load_df andload_file methods ofHiveCliHook and TransferHive operatorsCloses #2364 fromkrishnabhupatiraju/tblproperties_hiveclihook,1
"[AIRFLOW-1366] Add max_tries to task instanceRight now Airflow deletes the task instance whenuser clear it. We have no way of keeping track ofhow many times a task instance gets run either viauser or itself. So instead of deleting the taskinstance record, we should keep the task instanceand make try_number monotonically increasing forevery task instance attempt. max_tries isintroduced as an upper bound for retrying tasks bytask itself.This new column will be used to update logicbehind clear_task_instances.db migration is tested locally.Closes #2409 from AllisonWang/allison--max-tries",3
Adding imgix to list of companies using AirflowCloses #2429 from dclubb/add-imigix,1
[AIRFLOW-1387] Add unicode string prefixPython's format function has a feature of encodingthe suppliedvariable with the same encoding as a string wheresubstitution willtake place. In case the string is not originallyspecified as unicodedefault encoding will be used. This will yield anerror ifsys.getdefaultencoding() is not 'utf-8' because itwill try to encodepreviously utf8 decoded string (with unicodechars) as non unicode.Solution based on SO 5082452.Closes #2426 from artiom33/logger_unicode_fix1,2
[AIRFLOW-1388] Add Cloud ML Engine operators to integration docCloses #2425 from leomzhong/integration_doc,2
[AIRFLOW-1382] Add working dir option to DockerOperatorAllow the user to specify the working directory tobe used in thecreated container. Equivalent to docker run/create-w.Closes #2419 frombenjaminsims/specify_working_directory,1
[AIRFLOW-1357] Fix scheduler zip file supportZipped DAGs are supported on the models but nottaken into accountby the scheduler since 1.8. This fixes the issue.Closes #2406 from ultrabug/jira_1357,0
[AIRFLOW-1384] Add ARGO/CaDC as a Airflow userCloses #2434 from abhijeetdhumal/master,1
[AIRFLOW-1326][[AIRFLOW-1326][AIRFLOW-1184] Don't split argument array -- it's already an array.[Closes #2382 from ashb/spark-submit-operator-preserve-spaces,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1402] Cleanup SafeConfigParser DeprecationWarningCloses #2435 from ronfung/work,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1394] Add quote_character param to GCS hook and operatorCloses #2428 from dclubb/master,1
"[AIRFLOW-1401] Standardize cloud ml operator argumentsStandardize on project_id, to be consistent withother cloud operators,better-supporting default arguments.This is one of multiple commits that will berequired to resolveAIRFLOW-1401.Closes #2439 from peterjdolan/cloudml_project_id",0
Add Credit Karma to company listCloses #2436 from greg-finley-ck/ck,2
Add Mercari to the company listCloses #2424 from yu-iskw/add-mercari,1
[AIRFLOW-1247] Fix ignore all dependencies argument ignoredFix typo in ignore_all_dependencies argument to fix it.Closes #2441 from aoen/ddavydov--fix_ignore_all_deps_cli,0
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1359] Add Google CloudML utils for model evaluationCloses #2407 from yk5/evaluate,1
[AIRFLOW-1255] Fix SparkSubmitHook output deadlockRefactor the SparkSubmitHook output processing toavoid a deadlock.The prior implementation deadlocks if the stderrpipe buffer fills:1. Airflow tries to drain stdout before startingon stderr;2. Spark gets suspended if the stderr pipe fills;3. Both processes are now waiting for somethingthat will never happen.Closes #2438 from asnare/fix/spark-submit,0
"[AIRFLOW-1059] Reset orphaned tasks in batch for schedulerThe current implementation resets state for tasks1 dagrun at a time. Weshould be able to do this in larger batches, whichwill improvescheduler startup time.Closes #2205 from saguziel/aguziel-reset-state",1
"[AIRFLOW-1345] Dont expire TIs on each scheduler loopTIs get expired on commit, which causes any accessto their propertiesto cause a new query to the DB to be issued,causing an n+1 query issue,even when the TI is not scheduled. This changemakes all queriesbatches, which will make scheduling substantiallyfaster.Closes #2397 from saguziel/aguziel-commit-last",1
"[AIRFLOW-1393][[AIRFLOW-1393] Enable Py3 tests in contrib/spark_submit_hook[The unit tests in`tests/contrib/hooks/test_spark_submit_hook.py`were skiped if run in Python3 because some testcases loop foreverdue to a mismatch/misunderstanding about bytes vsstring that didn'tmatter under Py2 (i.e. the mocked data forsubprocess.Popen wasreturning a String, but the actual Popen callwould return bytes.)The fix is to use bytes and `six.ByteIO` so thatthe tests work on Py2and Py3. Alsowe had to patch `subprocess.Popen` inthe right place sothe mocks are picked up.Closes #2427 from ashb/enable-spark_submit_hook_tests-py3",3
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
"[AIRFLOW-756][AIRFLOW-751] Replace ssh hook, operator & sftp operator with paramiko basedCloses #1999 from jhsenjaliya/AIRFLOW-756",2
Fix new SSH documentation,2
"[AIRFLOW-1385] Create abstraction for Airflow task loggingThis PR adds abilities to provide customizedimplementations of airflow task logging. Itcreates an abstraction for setting up, cleaning upand get task instance logs.This change is primarily a refactor of logginglogic. It is tested locally with custom loggingimplementations.Closes #2422 from AllisonWang/allison--log-handler",2
"Revert ""[AIRFLOW-1385] Create abstraction for Airflow task logging""This reverts commit e6ef06c53fd4449db6e665cce5cad9418dde232f whichwas committed accidentally.",5
"[Airflow 1332] Split logs based on try numberThis PR splits logs based on try number and addtabs to display different task instance tries.**Note this PR is a temporary change forseparating task attempts. The code in this PR willbe refactored in the future. Please refer to #2422for Airflow logging abstractions redesign.**Testing:1. Added unit tests.2. Tested on localhost.3. Tested on production environment with S3 remotestorage, MySQL database, Redis, one Airflowscheduler and two airflow workers.Closes #2383 from AllisonWang/allison--add-task-attempt",1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1437] Modify BigQueryTableDeleteOperatorBigQueryTableDeleteOperator should define deletion_dataset_tableas a template field.Closes #2459 from yu-iskw/bq-delete,4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1439] Add max billing tier for the BQ Hook and OperatorCloses #2437 from aviDms/master,1
[AIRFLOW-1438] Change batch size per query in schedulerThis should help if query size is limited. It alsoreduces how longlocks are held.Closes #2462 from saguziel/aguziel-paginate-query,4
[AIRFLOW-1442] Remove extra space from ignore_all_deps generated commandFix extra whitespace in the ignore_all_deps argwhich was causing commands to fail.Closes #2468 from aoen/ddavydov--fix_ignore_all_deps_extra_space,0
[AIRFLOW-1399] Fix cli reading logfile in memoryDon't read logfile in memory if remote base isn'tspecifiedCloses #2433 fromNielsZeilemaker/no_remote_base_no_read,1
"[AIRFLOW-1398] Allow ExternalTaskSensor to wait on multiple runs of a taskCurrently using the execution_date_fn parameter ofthe ExternalTaskSensorsensors only allows to wait for the completion ofone given run of thetask the ExternalTaskSensor is sensing.However, this prevents users to have setups wheredags don't have the sameschedule frequency but still depend on oneanother. For example, let's sayyou have a dag scheduled hourly that transformslog data and is owned bythe team in charge of logging. In the currentsetup you cannot have otherhigher level teams, that want to use thistransformed data, createdags processing transformed log data in dailybatches, while making surethe logged transformed data was properly created.Note that simply waiting for the data to bepresent (using e.g. theHivePartitionSensor if the data is in hive) mightnot be satisfactorybecause the data being present doesn't mean it isready to be used.This commit makes it possible to do exactly thatby being able to havean ExternalTaskSensor wait for multiple runs ofthe task it is sensing tohave finished. Now higher level teams can setupdags with anExternalTaskSensor sensing the end task of the dagthat transforms thelog data and waiting for the successful completionof 24 of its hourly runs.Closes #2431 from rlk-ama/pr/multiple-dates-external-task-sensor",5
"[AIRFLOW-1448] Revert ""Fix cli reading logfile in memory""This reverts commit2de4b7cfb12f5a36eeaf5e78d3ee0fb12d67f3b2 which wasbreaking CI due to a logical merge conflict.Closes #2475 from aoen/ddavydov--revert_bad_pr",4
Add 'steps' into template_fields in EmrAddStepsRendering templates which are in steps is especially useful if youwant to pass execution time as one of the paramaters of a step inan EMR cluster. All fields in template_fields will get rendered.,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1459] Fixed broken integration .rst formattingCloses #2481 from yk5/docs,2
"[AIRFLOW-1349] Refactor BackfillJob _executeBackfillJob._execute is doing multiple things - it is pretty hard tofollow and maintain.Changes included are just a re-org of the code, no logic has beenchanged.Refactor includes:- Break BackfillJob._execute into functions- Add a Status object to track BackfillJobinternal status while  executing the job.Closes #2463 from edgarRd/erod-backfill-refactor",4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1389] Support createDisposition in BigQueryOperatorCloses #2470 from yu-iskw/bq-operator,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
"[AIRFLOW-1359] Use default_args in Cloud ML evalThis change makes the create_evaluate_ops utilitymethod make use of the default_args parameters ofthe DAG when possible. This simplifies the usageof the create_evaluate_ops method, and improvesthe usefulness of a variety of default_args.To further the usefulness of default_args forCloud ML Operators, this change also introducesversion_name to the CloudMLVersionOperator,allowing model_name and version_name to bespecified across an entire pipeline.This change also resolves a small TODO by makingthe DataFlowPythonOperator's `options` and`dataflow_default_options` variables templatized.Closes #2445 frompeterjdolan/eval_ops_arguments_from_default_args",5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
"[AIRFLOW-1445] Changing HivePartitionSensor UI color to lighter shadeMy PR is simply to improve the readability of thetext using the HivePartitionSensor. The screenshots below show the before and after. The darkershade (nearly black) is the before, and the purplecolor is the after.Closes #2476 from Acehaidrey/master",2
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1474] Add dag_id regex feature for `airflow clear` commandCloses #2486 from jgao54/airflow-clear,2
[AIRFLOW-1397][AIRFLOW-1] No Last Run column data displyed in Airflow UI 1.8.1Closes #2430 from preete-dixit-ck/master,5
[AIRFLOW-1478] Chart owner column should be sortableCloses #2493 from skudriashev/airflow-1478,2
"[AIRFLOW-1349] Fix backfill to respect limitsBefore, if a backfill job was triggered that wouldinclude a dag runalready in a RUNNING state, the dag run within thebackfill would beincluded in the count agains the max_active_runslimit. Also, if abackfill job generated multiple dag runs it couldpotentiallyviolate max_active_runs limits by executing alldag runs.Now the limit is checked per dag run to becreated, and the backfill jobwill only run the dag runs within the backfill jobthat could beincluded within the limits.Also, if the max_active_runs limit has alreadybeen reached, theBackfillJob will wait and loop trying to createthe required dag runs assoon as a dag run slot within the limit isavailable until all dag runsare completed.These changes provide a more consistent behavioraccording to themax_active_runs limits definition and allows theuser to run backfilljobs with existing RUNNING state when alreadyconsidered within thelimits.Closes #2454 from edgarRd/erod-fix-backfill-max",0
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1489] Fix typo in BigQueryCheckOperatorCloses #2501 from mrkm4ntr/airflow-1489,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1487] Added links to all companies officially using AirflowCloses #2458 from tanmaythakur/patch-1,1
[AIRFLOW-1486] Unexpected S3 writing log errorRemoved unexpected S3 writing log error and added tests for s3 logging.Closes #2499 from skudriashev/airflow-1486,2
[AIRFLOW-1443] Update Airflow configuration documentationThis PR updates Airflow configurationdocumentations to include a recent change to splittask logs by try number #2383.Closes #2467 from AllisonWang/allison--update-doc,5
[AIRFLOW-1492] Add gauge for task successes/failuresCloses #2504 from saguziel/aguziel-add-task-stat,1
"[AIRFLOW-940] Handle error on variable decryptInvalid variables could break the variable view byunhandledInvalidToken exception from Fernet.This commit converts the fernet error into anAirflowException, giventhat fernet is loaded dynamically. Also, theexception is handled inthe VariableView by showing the token ""INVALID"" inthe UI render.Closes #2510 from edgarRd/erod-error-handling-var-decrypt",0
[AIRFLOW-1385] Make Airflow task logging configurableThis PR adds configurable task logging to Airflow.Please refer to #2422 for previous discussions.This is the first step of making entire Airflowlogging configurable ([AIRFLOW-1454](https://issues.apache.org/jira/browse/AIRFLOW-1454)).Closes #2464 from AllisonWang/allison--log-abstraction,2
"[AIRFLOW-1452] workaround lock on methodWorkaround lock on method ""has_table"" in casemssql is usedas storage engine.Closes #2514 from patsak/master",1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1507] Template parameters in file_to_gcs operatorCloses #2516 from aravinduv/feature,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1280] Fix Gantt chart heightCloses #2502 from skudriashev/airflow-1280,2
"[AIRFLOW-1239] Fix unicode error for logs in base_task_runnerThe details here are that there exists a PR forthis JIRA already (https://github.com/apache/incubator-airflow/pull/2318). The issue is that in python 2.7 notall literals are automatically unicode like theyare in python 3. That's what's the root cause, andthat can simply be fixed by just explicitlystating all literals should be treated as unicode,which is an import from the `__future__` module.https://stackoverflow.com/questions/3235386/python-using-format-on-a-unicode-escaped-string also explains thissame solution, which I found helpful.Closes #2496 from Acehaidrey/master",1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1504] Log dataproc cluster nameCloses #2517 fromTrevorEdwards/dataproc_log_clustername,5
[AIRFLOW-1505] Document when Jinja substitution occursCloses #2523 from TrevorEdwards/airflow-1505,2
"[AIRFLOW-855] Replace PickleType with LargeBinary in XComPickleType in Xcom allows remote code execution.In order to deprecateit without changing mysql table schema, changePickleType to LargeBinary because they both maps to blob type in mysql. Add""enable_pickling"" tofunction signature to control using ether pickletype or JSON. ""enable_pickling"" should also be added to core section ofairflow.cfgPicked up where https://github.com/apache/incubator-airflow/pull/2132 left off. Took thisPR, fixed merge conflicts, addeddocumentation/tests, fixed broken tests/operators,and fixed the python3 issues.Closes #2518 from aoen/disable-pickle-type",0
"[AIRFLOW-1495] Add TaskInstance index on job_idColumn job_id is unindexed in TaskInstance, it wasused asdefault sort column in TaskInstanceView.This commit adds the required migration to add theindex ontask_instance.job_id on future db upgrades.Closes #2520 from edgarRd/erod-ti-jobid-index",5
"[AIRFLOW-1483] Making page size consistent in listViews showing model listings had large page sizeswhich made pageloading really slow client-side, mostly due to DOMprocessing andJS plugin rendering.Also, the page size was inconsistent across somelistings.This commit introduces a configurable page size,and by defaultit'll use a page_size = 100. Also, the same pagesize is applied toall the model views controlled by flask_admin tobe consistent.Closes #2497 from edgarRd/erod-ui-page-size-conf",5
"[AIRFLOW-1495] Fix migration on index on job_idThere was a merge conflict on the migration hashfor down revisionat the time that two commits including migrationswere merged.This commit restores the chain of revisions forthe migrations,pointing to the last one. The job_id indexmigration was regeneratedfrom the top migration.Closes #2524 from edgarRd/erod-ti-jobid-index-fix",0
[AIRFLOW-1420][AIRFLOW-1473] Fix deadlock checkUpdate the deadlock check to prevent falsepositives on upstreamfailure or skip conditions.Closes #2506 from gwax/fix_dead_dagruns,2
"[AIRFLOW-1516] Fix error handling getting fernetThere were unhandled cases for exceptions whenimporting fernet inmodels.py. This seems to be a remanent of aprevious refactor,replacing logic that would depend on thedefinition of a global variablefor Fernet if it was imported correctly.Generally catching all exceptions from get_fernetfunction, given thatother functions are already handling it that wayand the onlyerror handling case here is to not use encryption.Closes #2527 from edgarRd/erod-fernet-error-handling",0
"[AIRFLOW-1324] Generalize Druid operator and hookMake the druid operator and hook more specific.This allows us tohave a more flexible configuration, for exampleingest parquet.Also get rid of the PyDruid extension since it ismore focussed onquerying druid, rather than ingesting data. Justrequests issufficient to submit an indexing job. Add a testto the hive_to_druidoperator to make sure it behaves as we expect.Furthermore cleanedup the docstring a bitCloses #2378 from Fokko/AIRFLOW-1324-make-more-general-druid-hook-and-operator",1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1521] Fix emplate rendering for BigqueryTableDeleteOperatorThe list of template_fields contains only 1 entry and wasinterpreted by python as a list of character. That wasbreaking the render_template function (see AIRFLOW-1521ticket)Closes #2534 from moe-nadal-ck/AIRFLOW-1521/fix_table_delete_operator_template_fields_list,4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
add Grand Rounds to companies listCloses #2533 from richddr/add-grand-rounds-to-company-list,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1529] Add logic supporting quoted newlines in Google BigQuery load jobsCloses #2545 from wileeam/bq-allow-quoted-nl,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1544] Add DataFox to companies listCloses #2544 from sudowork/datafox-companies,5
[AIRFLOW-1545] Add Nextdoor to companies listAdd Nextdoor to company listAdd Nextdoor to companies listCloses #2448 from SivaPandeti/master,1
[AIRFLOW-1546] add Zymergen 80to org list in READMECloses #2512 from mistercrunch/add_zymergen,7
"[AIRFLOW-1384] Add to README.md CaDC/ARGOadded to  currently **officially** using Airflow:[California Data Collaborative](http://californiadatacollaborative.org) powered by [ARGOLabs](http://www.argolabs.org)Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!- [x] My PR addresses the following [Airflow JIRA]**https://issues.apache.org/jira/browse/AIRFLOW-1384**- The California Data Collaborative is a uniquecoalition of forward thinking municipal watermanagers in California who along with ARGO, astartup non-profit that builds, operates, andmaintains data infrastructures, are pioneering newstandards of collaborating around andadministering water data for millionsCalifornians.ARGO has deployed a hosted version of Airflow onAWS and it is used to orchestrate data pipelinesto parse water use data from participatingutilities to power analytics. Furthermore, ARGOalso uses Airflow to power a data infrastructurefor citywide street maintenance viahttps://github.com/ARGO-SQUID- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Change to README.md does not require unit testing.- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Update README.mdadded to  currently **officially** using Airflowsection of README.md[California Data Collaborative](https://github.com/California-Data-Collaborative) powered by [ARGOLabs](http://www.argolabs.org)Added CaDC/ARGO Labs to README.mdPlease consider adding [ArgoLabs](www.argolabs.org) to the Airflow userssection.**Context**- The California Data Collaborative is a uniquecoalition of forward thinking municipal watermanagers in California who along with ARGO, astartup non-profit that builds, operates, andmaintains data infrastructures, are pioneering newstandards of collaborating around andadministering water data for millionsCalifornians.- ARGO has deployed a hosted version of Airflow onAWS and it is used to orchestrate data pipelinesto parse water use data from participatingutilities to power analytics. Furthermore, ARGOalso uses Airflow to power a data infrastructurefor citywide street maintenance viahttps://github.com/ARGO-SQUIDCloses #2421 from vr00n/patch-3",5
closes apache/incubator-airflow#1444 *Won't fix*,0
closes apache/incubator-airflow#1415 *Won't fix*,0
closes apache/incubator-airflow#1382 *Won't fix*,0
closes apache/incubator-airflow#1186 *Won't fix*,0
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1535] Add service account/scopes in dataprocCloses #2546 from fenglu-g/master,5
[AIRFLOW-1541] Add channel to template fields of slack_operatorCloses #2549 from Acehaidrey/AIRFLOW-1541,1
"[AIRFLOW-108] Add CreditCards.com to companies listDear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [/] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-108### Description- [/] Here are some details about my PR, includingscreenshots of any UI changes:Adding an entry to the companies list in README.mdfile.### Tests- [/] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason: Documentation change only.### Commits- [/] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2554 from r39132/master",1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1556][Airflow 1556] Add support for SQL parameters in BigQueryBaseCursorCloses #2557 from rajivpb/sql-parameters,2
"[AIRFLOW-1562] Spark-sql logging contains deadlockLogging in SparkSqlOperator does not work asintended. Spark-sqlinternally redirects all logs to stdout (includingstderr),which causes the current two iterator logging toget stuck withthe stderr pipe. This situation can lead to adeadlockbecause the std-err can grow too big and it willstart to blockuntil it will be consumed, which will only happenwhen the processends, so the process stalls.Closes #2563 from Fokko/AIRFLOW-1562-Spark-sql-loggin-contains-deadlock",2
[AIRFLOW-1564] Use Jinja2 to render logging filenameStill backwards compatible with python formatCloses #2565 from NielsZeilemaker/AIRFLOW-1564,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1568] Add datastore export/import operatorsCloses #2568 from jgao54/ds-import-export,2
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1567][Airflow-1567] Renamed cloudml hook and operator to mlengineCloses #2567 from yk5/cmle,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1493][AIRFLOW-XXXX][WIP] fixed dumb thingCloses #2505 from saguziel/aguziel-fix-double-trigger,0
[AIRFLOW-1568] Fix typo in BigQueryHookCloses #2575 from jgao54/ds-import-export,2
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
Add ContaAzul as an Airflow userCloses #2566 from sabino/patch-1,1
[AIRFLOW-1572] add carbonite to company listCloses #2571 from ajbosco/add_carbonite,1
[AIRFLOW-1574] add 'to' attribute to templated vars of email operatorThe to field may sometimes want to be to betemplate-able when you have a DAG that is usingXCOM to find the user to send the information to(i.e. we have a form that a user submits and basedon the ldap user we send this specific user theinformation). It's a rather easy fix to add the'to' user to the template-able options.Closes #2577 from Acehaidrey/AIRFLOW-1574,1
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1567] Updated docs for Google ML Engine operators/hooksCloses #2573 from yk5/master,1
"[AIRFLOW-1580] Error in string formatingThe string formatting should be done on thestring, and not on theexception that is being raised.Closes #2583 from Fokko/AIRFLOW-1580-error-in-checkout-operator",0
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1577] Add token support to DatabricksHookCloses #2579 from andrewmchen/token,5
[AIRFLOW-1579] Adds support for jagged rows in Bigquery hook for BQ load jobsCloses #2582 from DannyLee12/master,1
[AIRFLOW-1586] Add mapping for date type to mysql_to_gcs operatorCloses #2589 from mikeghen/bug/airflow-1586,0
[AIRFLOW-1584] Remove insecure /headers endpointCloses #2588 from aoen/ddavydov--remove_headers_endpoint,4
[AIRFLOW-1573] Remove `thrift < 0.10.0` requirementCloses #2574 from dan-disqus/Thrift,1
[AIRFLOW-XXX] 1.8.2 release notesCloses #2562 from mistercrunch/release_182,1
[AIRFLOW-950] Missing AWS integrations on documentation::integrationsCloses #2552 from Swalloow/master,2
[AIRFLOW-1522] Increase text size for var field in variables for MySQLCloses #2535 from saguziel/aguziel-increase-text,1
[AIRFLOW-XXX] Save username and password in airflow-pr,4
[AIRFLOW-1476] add INSTALL instruction for source releasesCloses #2492 from mistercrunch/install,1
Merge branch 'pr_nicer',7
"[AIRFLOW-1582] Improve logging within AirflowClean the way of logging within Airflow. Removethe old logging.py andmove to the airflow.utils.log.* interface. Removesetting the loggingoutside of the settings/configuration code. Moveaway from the stringformat to logging_function(msg, *args).Closes #2592 from Fokko/AIRFLOW-1582-Improve-logging-structure",2
"[AIRFLOW-1594] Don't install test packages into python root.[]By default `find_packages()` will find _any_ validpython package,including things under tests. We don't want toinstall the testspackages into the python path, so exclude those.Closes #2597 from ashb/AIRFLOW-1594-dont-install-tests",3
[AIRFLOW-1597] Add GameWisp as Airflow userCloses #2599 from TJBIII/master,1
[AIRFLOW-1593] expose load_string in WasbHookCloses #2596 from NielsZeilemaker/AIRFLOW-1593,1
[AIRFLOW-1602] LoggingMixin in DAG classWithin the DAG class we want to use theLoggingMixer for moretransparent logger instead of creating a newanonymous loggerCloses #2602 from Fokko/AIRFLOW-1602-use-loggingmixin-in-dag-class,2
"[AIRFLOW-1606][Airflow-1606][AIRFLOW-1605][AIRFLOW-160] DAG.sync_to_db is now a normal methodPreviously it was a static method that took asit's first argument aDAG, which really meant it wasn't truly a staticmethod.To avoid reversing the parameter order I havegiven sensible defaultsfrom the one and only use in the rest of the codebase.Also remove documented ""sync_to_db"" parameter onDagBag that no longerexists -- this doc string refers to a parameterthat was removed in[AIRFLOW-160].Closes #2605 from ashb/AIRFLOW-1606-db-sync_to_db-not-static",5
[AIRFLOW-1606] Use non static DAG.sync_to_dbFinalizes refactor where one line was missed.Closes #2606 from bolkedebruin/AIRFLOW-1606,4
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1608] Handle pending job state in GCP Dataflow hookCloses #2607 from TJBIII/gcp_dataflow_hook,5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-XXX] Bumping Airflow 1.10.0dev0+incubating version,5
"[AIRFLOW-1601] Add configurable task cleanup timeWhen task processes are SIGTERMed, they have bydefault 5 seconds tocleanup before a SIGKILL arrives. This allows thisvalue to beconfigurable.Closes #2601 from saguziel/aguziel-configure-task-killer",5
[AIRFLOW-1609] Fix gitignore to ignore all venvsCloses #2608 from saguziel/aguziel-amend-gitignore,0
"[AIRFLOW-1603] add PAYMILL to companies listDear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1603- [x] Here are some details about my PR, includingscreenshots of any UI changes:  - added my company to README.md- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2603 from matthiashuschle/pr2",1
[AIRFLOW-1613] Make MySqlToGoogleCloudStorageOperator compaitible with python3Closes #2609 from jgao54/make-MySqlToGoogleCloudStorageOperator-python3-compaitible,1
"[AIRFLOW-1309] Allow hive_to_druid to take tblpropertiesDear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [ ] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1309### Description- [ ] Here are some details about my PR, includingscreenshots of any UI changes: Add optionaltblproperties for the druid hook### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason: Will add### Commits- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2368 from saguziel/aguziel-update-hive-to-druid",5
"[AIRFLOW-1519] Add server side paging in DAGs listAirflow's main page previously did paging client-side via ajQuery plugin (DataTable) which was very slow atloading all DAGs.The browser would load all DAGs in the table.The result was performance degradation when havinga number ofDAGs in the range of 1K.This commit implements server-side paging usingthe webserver pagesize setting, sending to the browser only theelements for thespecific page.Closes #2531 from edgarRd/erod-ui-dags-paging",2
"[AIRFLOW-1614] Replace inspect.stack() with sys._getframe()inspect.stack() is really expensive, and slowsdown processing of dagshaving large numbers (100s, 1000s) of subdags.Closes #2610 from gbenison/gcbenison2",2
[AIRFLOW-1600] Fix exception handling in get_fernetAlso adds LoggingMixin to Connection so it can useself.logger.Closes #2600 from GeorgeSirois/fix-fernet-no-cryptography,0
"[AIRFLOW-1177] Fix Variable.setdefault w/existing JSONPreviously due to a logic error if you attempt touse`Variable.setdefault()` with`deserialize_json=True` and the valuealready existed it would die with:    ...        my = Variable.setdefault('regions', ['uk'],deserialize_json=True)      File ""/usr/local/lib/python3.5/site-packages/airflow/models.py"", line 3623, insetdefault        return json.loads(obj.val)    AttributeError: 'str' object has no attribute'val'The problem was that the `Variable.get()` call wasreturning the value,not a variable object.Closes #2540 from ashb/variable-setdefault-json",5
[AIRFLOW-XXX] Remove non working service badgesLandscape and requirements.io are malfunctioning for some timenow.,1
[AIRFLOW-XXX] Remove landscape.io config,5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
[AIRFLOW-1619] Add poll_sleep parameter to GCP dataflow operatorCloses #2612 from TJBIII/gcp_dataflow_poll_sleep,5
[AIRFLOW-1497] Reset hidden fields when changing connection typeCloses #2507 from mrkm4ntr/airflow-1497,4
"[AIRFLOW-1617] Fix XSS vulnerability in Variable endpointIn case a Variable form was accessed by a get request andthe form did not exist as a template, the input wasreturned as is to the user.Closes #2611 from bolkedebruin/xss_fix",0
[AIRFLOW-1512] Add PythonVirtualenvOperatorCloses #2446 from saguziel/aguziel-virtualenv,1
[AIRFLOW-1604] Rename logger to logIn all the popular languages the variable name logis the de factostandard for the logging. Rename LoggingMixin.pyto logging_mixin.pyto comply with the Python standard.When using the .logger a deprecation warning willbe emitted.Closes #2604 from Fokko/AIRFLOW-1604-logger-to-log,2
[AIRFLOW-1031] Replace hard-code to DagRun.ID_PREFIXCloses #2613 from morefreeze/patch-1,2
[AIRFLOW-1591] Avoid attribute error when rendering logging filenameCloses #2578 from mrkm4ntr/airflow-1564,2
[AIRFLOW-1621] Add tests for server side pagingAdding tests to check logic included inAIRFLOW-1519.Closes #2614 from edgarRd/erod-ui-dags-paging-tests,3
[AIRFLOW-1247] Fix ignore_all_dependencies argument ignoredCloses #2327 from mremes/patch-1,0
[AIRFLOW-1356] Add `--celery_hostname` to `airflow worker`Closes #2405 from d2207197/airflow-1356,1
[AIRFLOW-289] Make airflow timezone independentAirflow mixes datetime.now()/today() and utcnow().This can leadto issues in case the OS is not in UTC.Closes #2618 from bolkedebruin/use_utcnow,1
Bring consistency to oxford comma usagehttps://i.imgur.com/fycHx.jpg ;)Closes #2616 from KeeonTabrizi/patch-1,5
[AIRFLOW-1368] Automatically remove Docker container on exitCloses #2411 from nathanielvarona/docker-operator,2
[AIRFLOW-1629] Make extra a textarea in edit connections formCloses #2623 from dalupus/airflow-1629,2
"[AIRFLOW-1627] Only query pool in SubDAG init when necessaryWhen checking for pool conflicts in a SubDAG, ensure that a task inthe SubDAG is actually in the same pool as the SubDagOperator itselfto avoid querying the database unnecessarily.Closes #2620 from dhuang/AIRFLOW-1627",5
"[AIRFLOW-1331] add SparkSubmitOperator optionspark-submit has --packages option to useadditional java packages.but current version of SparkSubmitOperatorcouldn't handle it.I added ""packages"" option to SparkSubmitOperatorto resolve it.I added same option for TestSparkSubmitOperator,too.Closes #2622 from chie8842/AIRFLOW-1331",3
[AIRFLOW-1628] Fix docstring of sqlsensorCloses #2621 from mrkm4ntr/airflow-1628,2
"Revert ""[AIRFLOW-1368] Automatically remove Docker container on exit""This reverts commit 46c86a5cd2b69f4f1853280b442e8810b178e6c7.",4
[AIRFLOW-1637] Fix Travis CI build status linkIn the readme it is linkinghttps://travis-ci.org/apache/incubator-airflow.svginstead ofhttps://travis-ci.org/apache/incubator-airflow.svg?branch=masterSo this is showing the build status for the latestbuild insteadof the master branchCloses #2627 from dalupus/airflow-1637,3
"[AIRFLOW-1639] Fix Fernet error handlingWhen the encrypted string cannot be decryted usingFernet for somereason, an error will be thrown. In Python 3.6.2the .message attris not available. By casting the ValueError to astring, the messagewill be extracted from the Error.Closes #2629 from Fokko/AIRFLOW-1639-fix-error-handling",0
[AIRFLOW-1527] Refactor celery configThe celery config is currently part of the celery executor definition.This is really inflexible for users wanting to change it. In additionCelery 4 is moving to lowercase.Closes #2542 from bolkedebruin/upgrade_celery,4
"[AIRFLOW-1636] Add AWS and EMR connection typeaws and emr connection types are not specified in models, so when youupdate them the type gets cleared.  Since default ones are includedootb, they should be added to the model as valid options.Closes #2626 from dalupus/airflow-1636",1
[AIRFLOW-1626] Add Azri Solutions to Airflow usersCloses #2619 from userimack/mahendra_dev,1
[AIRFLOW-1643] Add healthjump to officially using listCloses #2556 from miscbits/master,1
[AIRFLOW-1576] Added region param to Dataproc{*}OperatorsCloses #2625 from cjqian/1576,5
[Airflow-1640][AIRFLOW-1640] Add qubole default connectionCloses #2630 from rupesh92/AIRFLOW-1640,1
"[AIRFLOW-1587] Fix CeleryExecutor import error`CeleryExcutor` can not import directly from`airflow.executors`, we should import from`airlfow.executors.celery_executor` instead.Closes #2590 fromlxneng/bugfix/fix_pkg_importerror_in_run_task_view",2
[AIRFLOW-1647] Fix Spark-sql hookThe logging in the spark-sql hook was not workingand causing anexception because the _process_log is notavailable. Write tothe log handler directly because the spark-sqlhook cant run inyarn-cluster mode. Write tests which verify thepopen call of thehook.Closes #2637 from Fokko/AIRFLOW-1647-Fix-spark-sql-hook,0
[AIRFLOW-1650] Fix custom celery config loadingCelery config loading was broken as it was just passinga string. This fixes it by loading it as a module with anattribute. Inspired by Django's module loading.,0
Merge branch 'celery_loadconfig',5
[AIRFLOW-1635] Allow creating GCP connection without requiring a JSON fileCloses #2640 from barrywhart/airflow-1635-gcp-json-data-master,5
[AIRFLOW-1659] Fix invalid obj attribute bug in file_task_handler.pyCloses #2645 from jgao54/fix-file_task_handler-bug,2
[AIRFLOW-1664] write file as binary instead of strCloses #2649 from jgao54/write-binary,2
[AIRFLOW-1660] Change webpage width to full-widthCloses #2646 from lxneng/feature/full_width_page,4
[AIRFLOW-1654] Show tooltips for link icons in DAGs viewCloses #2642 from mrkm4ntr/airflow-1654,2
[AIRFLOW-1560] Add AWS DynamoDB hook and operator for inserting batch itemsCloses #2587 fromsid88in/feature/dynamodb_hook_and_operator,5
"[AIRFLOW-891] Make webserver clock include dateCurrently the webserver clock has only hour andminute.We change the clock format to ""yyyy-MM-ddHH:mm:ss"",because date information is helpful for user andthis format is universal.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-891### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Currently the webserver clock has only hour andminute.We change the clock format to ""yyyy-MM-ddHH:mm:ss"",because date information is helpful for user andthis format is universal.I updated jqClock.min.js with the latest versionfor now(https://github.com/JohnRDOrazio/jQuery-Clock-Plugin/tree/v2.3.0).<img width=""1152"" alt=""screen shot 2017-06-26 at 138 16"" src=""https://user-images.githubusercontent.com/898388/27517983-9e676122-5a10-11e7-90e1-db1e16e931d8.png"">### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Added no test. Instead, I ran webserver manuallyand confirmed the clock was displayed expectedly.Also, I confirmed all unit tests passed.### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2399 from sekikn/AIRFLOW-891",1
[AIRFLOW-1669][AIRFLOW-1368] Fix Docker importWe want to fix the Docker import. At the revert ofAIRFLOW-1368something went wrong.Closes #2653 from Fokko/AIRFLOW-1669-fix-docker-import,2
"[AIRFLOW-1658] Kill Druid task on timeoutIf the total execution time of a Druid taskexceeds the max timeoutdefined, the Airflow task fails, but the Druidtask may still keeprunning. This can cause undesired behaviour ifAirflow retries thetask. This patch calls the shutdown endpoint onthe Druid task tokill any still running Druid task.This commit also adds tests to ensure that allmocked requests inthe Druid hook are actually called.Closes #2644 fromdanielvdende/kill_druid_task_on_timeout_exceeded",1
[AIRFLOW-1668] Expose keepalives_idle for Postgres connectionsControls the number of seconds of inactivity afterwhich TCPshould send a keepalive message to the server.A value of zero uses the system default.Important for Redshift which requires a settinglower than 300.Closes #2650 from bolkedebruin/AIRFLOW-1688,1
[AIRFLOW-1611] Customize loggingChange the configuration of the logging to makeuse of the pythonlogging and make the configuration easyconfigurable. Some of thesettings which are now not needed anymore sincethey can easilybe implemented in the config file.Closes #2631 from Fokko/AIRFLOW-1611-customize-logging-in-airflow,2
"[AIRFLOW-988] Fix repeating SLA miss callbacksWhen a callback is passed to `sla_miss_callback` but an email addressis not specified, the callback will be continuously called. This is dueto the logic used when pulling the slas in `SchedulerJob.manage_slas`.By filtering on `notification_sent` only we will still handle the caseswhere email is used, but it will prevent the continuous callbacks.Closes #2415 from cjonesy/master",1
[AIRFLOW-1671] Add @apply_defaults back to gcs download operatorCloses #2655 from jgao54/bug-fix,0
[AIRFLOW-1590] fix unused module and variableCloses #2652 from ProstoMaxim/master,1
[AIRFLOW-1323] Made Dataproc operator parameter names consistentCloses #2636 from cjqian/1323,2
[AIRFLOW-1678] Fix erroneously repeated word in function docstringsCloses #2660 from thundergolfer/thundergolfer/small-docstring-fix,2
[AIRFLOW-1676] Make GCSTaskHandler write to GCS on closeCloses #2659 from criccomini/AIRFLOW-1676,5
[AIRFLOW-1634] Adds task_concurrency featureThis adds a feature to limit the concurrency ofindividual tasks. Thedefault will be to not change existing behavior.Closes #2624 from saguziel/aguziel-task-concurrency,4
[AIRFLOW-1682] Make S3TaskHandler write to S3 on closeApplies fix for GCSTaskHandler made as part of AIRFLOW-1676 to fixthe same bug in S3TaskHandler. Also rename s3_log_read function tos3_read for consistency with other functions in airflow.utils.logCloses #2664 from ahvigil/AIRFLOW-1682,2
[AIRFLOW-1690] Add detail to gcs error messagesProvide the system error detail in the logmessage.Closes #2670 from wrp/master,2
[AIRFLOW-1691] Add better Google cloud logging documentationCloses #2671 from criccomini/fix-log-docs,5
[AIRFLOW-1697] Mode to disable charts endpoint,2
[AIRFLOW-1613] Handle binary field in MySqlToGoogleCloudStorageOperatorCloses #2680 from jgao54/write-binary,1
[AIRFLOW-1696] Fix dataproc version label errorCloses #2676 from TrevorEdwards/airflow-1696,0
"[AIRFLOW-1681] Add batch clear in task instance viewAllow users to batch clear selected taskinstance(s) in task instance view. Only state(s)of selected task instance(s) will be cleared--noupstream nor downstream task instance will beaffected.DAG(s) involved will be set to ""RUNNING"" state,same as existing ""clear"" operation.Keeping both ""Delete"" and ""Clear"" operations formore smooth user habit transition--informing DAGstate change in pop-up (check screenshots).Closes #2681 from yrqls21/add-batch-clear-in-task-instance-view",1
[AIRFLOW-1714] Fix misspelling: s/seperate/separate/Closes #2688 from wrp/separate,0
"Revert ""[AIRFLOW-1613] Handle binary field in MySqlToGoogleCloudStorageOperator""Reverting due to improper handling of binary description_flag.This reverts commit d578b292e96d5fdd87b5168508005cd73edc4f96.",4
"[AIRFLOW-1683] Cancel BigQuery job on timeout.This change causes the BigQuery job to be canceled when thetask that started it is killed, for example on execution timeout,reducing wasted resources.Closes #2665 from janczak10/master",1
[AIRFLOW-1724] Add Fundera to Who uses Airflow?Closes #2694 from andyxhadji/fundera,1
"[AIRFLOW-1631] Fix local executor unbound parallelismBefore, if unlimited parallelism was used passing`0` for theparallelism value, the local executor would stallexecution since noworker was being created, violating theBaseExecutor contract on theparallelism option.Now, if unbound parallelism is used, processeswill be created on demandfor each task submitted for execution.Closes #2658 from edgarRd/erod-localexecutor-fix",0
"[AIRFLOW-1631] Fix timing issue in unit testLocalWorker instances wait 1 sec for each unit ofwork they perform, sogetting the response of a processor takes at least1 sec after the unitof work.Increasing timeout inLocalTaskJobTest.test_mark_success_no_kill anddecreasing the poking time on local executorchecking for results in theunlimited implementation.Closes #2699 from edgarRd/erod-fix-timing-failure",0
[AIRFLOW-1727] Add unit tests for DataProcHookCloses #2697 from cjqian/1727,5
[AIRFLOW-1718] Set num_retries on Dataproc job request executionCloses #2696 from cjqian/1718,5
[AIRFLOW-1723] Support sendgrid in email backendCloses #2695 from fenglu-g/master,1
[AIRFLOW-1722] Fix typo in scheduler autorestart output filenameExtra r in /tmp/airflow_scheduler_errors.txt hasbeen removed.Closes #2693 from mschmo/AIRFLOW-1722,4
[AIRFLOW-1692] Change test_views filename to support WindowsCloses #2673 from NielsZeilemaker/AIRFLOW-1692,1
[AIRFLOW-1694] Stop using itertools.izipItertools.zip does not exist in Python 3.Closes #2674 from yati-sagade/fix-py3-zip,0
"[AIRFLOW-1698] Remove SCHEDULER_RUNS env var in systemdIn the very early days, the Airflow schedulerneeded to be restartedevery so often to take new DAG_FOLDERS mutationsinto account properly. This is no longerrequired.Closes #2677 from mistercrunch/scheduler_runs",1
[AIRFLOW-1330] Add conn_type argument to CLI when adding connectionCloses #2525 from mrkm4ntr/airflow-1330,1
[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHookExecutes SQL using psycopg2 copy_expert methodNecessary to execute COPY command without access to a superuserCloses #2698 from andyxhadji/AIRFLOW-1726,1
"[AIRFLOW-1728] Add networkUri, subnet, tags to Dataproc operatorCloses #2706 from jfantom/master",1
"[AIRFLOW-1741] Correctly hide second chart on task duration pageA timing/load order bug meant that this "".trigger""call was happeningtoo early, resulting in a trigger being sentbefore the hook callbackhad been registered, meaning we didn't hide thesecond chart correctlyon page load.Closes #2711 from ashb/AIRFLOW-1741-task-duration-charts-load",2
"[AIRFLOW-1745] Restore default signal dispositionRestore defaults for SIGPIPE, SIGXFZ, and SIGXFSZPython 2.7 subprocess resets signal dispositionfor thesesignals to ignore, which can cause problems.  Forexample,a simple BashOperator executing 'yes | head' mayneverterminate.  For details, see discussion at:https://bugs.python.org/issue1652https://stackoverflow.com/questions/22077881/yes-reporting-error-with-subprocess-communicateetc.Closes #2714 from wrp/sigpipe",0
[AIRFLOW-1743] Verify ldap filters correctlyThe superuser and data profiler filter where setby defaultin the config template and could not be unset.Closes #2712 from bolkedebruin/AIRFLOW-1743,1
"[AIRFLOW-1432] Charts label for Y axis not visibleThe NVD3 charts did _have_ labels on the y-axissaying the unit (hours,minutes etc) but they weren't _visible_. nvd3.jscorrectly places labelson the xAxis, but it doesn't correctly space themon the vertical axis.Closes #2710 from ashb/AIRFLOW-1432-chart-y-axes",2
[AIRFLOW-1719] Fix small typoCloses #2689 from j450h1/master,2
[AIRFLOW-XXX] Fix DateTime in Tree ViewCloses #2687 from stas-em/www-tree-view-fix-displaytime,0
[AIRFLOW-1716] Fix multiple __init__ def in SimpleDagCloses #2692 from MortalViews/master,2
"Revert ""[AIRFLOW-1716] Fix multiple __init__ def in SimpleDag""This reverts commit e05254f8731ac55e169cdc581a38b0d3fe06267d.",4
"[AIRFLOW-1520] Boto3 S3Hook, S3LogCloses #2532 from NielsZeilemaker/AIRFLOW-1520",1
[AIRFLOW-926] Fix JDBC HookJayDeBeApi made a backwards incompatible changeThis updates the JDBC Hook's implementationand changes the required JayDeBeApi to >= 1.1.1Closes #2651 from r-richmond/AIRFLOW-926,1
[AIRFLOW-1677] Fix typo in example_qubole_operatorCloses #2661 from rupesh92/AIRFLOW-1677,1
[AIRFLOW-1657] Handle failing qubole operatorCloses #2643 from rupesh92/AIRFLOW-1657,1
[AIRFLOW-1736] Add HotelQuickly to Who Uses AirflowCloses #2705 from zinuzoid/AIRFLOW-1736,1
[AIRFLOW-1732] Improve dataflow hook loggingCloses #2702 from TrevorEdwards/1732,2
[AIRFLOW-1744] Make sure max_tries can be settask.retries can be False. Which is not acceptableforand integer field.Closes #2713 from bolkedebruin/AIRFLOW-1744,1
"[AIRFLOW-1641] Handle executor events in the schedulerWhile in Backfills we do handle the executorstate,we do not in the Scheduler. In case there is anunspecifiederror (e.g. a timeout, airflow command failure)taskscan get stuck.Closes #2715 from bolkedebruin/AIRFLOW-1641",1
"[AIRFLOW-1731] Set pythonpath for loggingBefore initializing the logging framework, we wantto set the pythonpath so the logging config can be found.Closes #2721 from Fokko/AIRFLOW-1731-import-pythonpath",2
[AIRFLOW-1761] Fix type in scheduler.rstCloses #2707 from mhue/patch-1,0
[AIRFLOW-1734][Airflow 1734] Sqoop hook/operator enhancementsCloses #2703 from Acehaidrey/sqoop_contrib_fixes,0
[AIRFLOW-1757] Add missing options to SparkSubmitOperatoradd 'exclude-packages' and 'repositories' asoptionsto SparkSubmitOperator as they were missingCloses #2725 from kretes/AIRFLOW-1757-spark-new-options,1
[AIRFLOW-1723] Make sendgrid a pluginCloses #2727 from fenglu-g/master,1
[AIRFLOW-1711] Use ldap3 dict for group membershipCertain schemas for group membership return astringinstead of a list. Instead of using a check we nowuse the entries API from ldap3.Closes #2731 from bolkedebruin/AIRFLOW-1711,1
[AIRFLOW-1706] Fix query error for MSSQL backendMSSQL doesn't support key word 'is' as synonym for'='Closes #2733 frompatsak/fix/illegal_query_for_mssql,0
"[AIRFLOW-1695] Add RedshiftHook using boto3Adds RedshiftHook class, allowing for managementof AWS Redshiftclusters and snapshots using boto3 library. Alsoadds new test file andunit tests for class methods.Closes #2717 from andyxhadji/1695",3
"[AIRFLOW-1018] Make processor use logging frameworkUntil now, the dga processor had its own loggingimplementation,making it hard to adjust for certain use caseslike workingin a container.This patch moves everything to the standardlogging framework.Closes #2728 from bolkedebruin/AIRFLOW-1018",1
[AIRFLOW-1315] Add Qubole File & Partition SensorsCloses #2401 from msumit/AIRFLOW-1315,2
"[AIRFLOW-1763] Fix S3TaskHandler unit testsFix breaking S3TaskHandler unit tests, and createa package so thattests are identified by CI.Closes #2732 from andyxhadji/AIRFLOW-1763",3
[AIRFLOW-1769] Add support for templates in VirtualenvOperatorCloses #2741 from saguziel/aguziel-virtualenv-templates,1
[AIRFLOW-1771] Rename heartbeat to avoid confusionCloses #2743 from saguziel/aguziel-heartbeat,5
"[AIRFLOW-1764] The web interface should not use the experimental APIThe web interface should not use the experimentalapi as theauthentication options differ between the two.Additionally, rather thanhaving an API call to get the last run data we caneasily include it inthe generated HMTL response. One less round-trip,less endpoints, andless time before the page has fully rendered.This is based original off @NielsZeilemaker's PRfor the same Jiraissue (#2734)Closes #2738 from ashb/no-exp-api-from-web-interface",0
[AIRFLOW-1765] Make experimental API securable without needing Kerberos.Previously the experimental API was either wide-open only (allow anyrequest) or secured behind Kerberos. This adds athird option ofdeny-all.Closes #2737 from ashb/exp-api-securable,1
[AIRFLOW-1776] Capture stdout and stderr for loggingThe new logging framework was not properlycapturing stdout/stderroutput. Redirection the the correct loggingfacility is required.Closes #2745 from bolkedebruin/redirect_std,1
"[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator outputSSHOperator does now write stdout to log, justlike SSHExecutorOperatordid in the pastCloses #2686 from OpringaoDoTurno/bring-ssh-logs-back",2
"[AIRFLOW-1675] Fix docstrings for API docsSome docstrings were missing spaces, causing themto render strangelyin documentation. This corrects the issue byadding in the spaces.Closes #2667 from cjonesy/master",1
[AIRFLOW-1571] Add AWS Lambda HookCloses #2718 from sid88in/feature/aws_lambda_hook2,1
[AIRFLOW-XXX] Correct typos in the faq docs pageCloses #2735 from aaronmyatt/correct-typos-in-faq-page,2
[AIRFLOW-XXX] Give a clue what the 'ds' variable isIt's not explained anywhere else in thetutorial...Closes #2679 from tgs/patch-1,5
"[AIRFLOW-71] Add support for private Docker imagesPulling images from private Docker registries requires authentication,so additional parameters are added in order to perform the login step.",2
"[AIRFLOW-1669] Fix Docker and pin Moto to 1.1.19https://github.com/spulec/moto/pull/1048 introduced `docker` as adependency in Moto, causing a conflict as Airflow uses `docker-py`. Asboth packages don't work together, Moto is pinned to the versionprior to that change.",4
"[AIRFLOW-1779] Add keepalive packets to ssh hookMake use of paramiko's set_keepalive method tosend keepalive packets everykeepalive_interval seconds.  This will preventlong running queries with no terminaloutput from being termanated as idle, for exampleby an intermediate NAT.Set on by default with a 30 second interval.Closes #2749 from RJKeevil/add-sshhook-keepalive",1
"[AIRFLOW-387] Close SQLAlchemy sessions properlyThis commit adopts the `provide_session` helper inalmost the entirecodebase. This ensures session are handled andclosed consistently.In particular, this ensures we don't forget toclose and thus leakdatabase connections.As an additional change, the `provide_session`helper has been extendedto also rollback and close created connectionsunder error conditions.As an additional helper, this commit alsointroduces a contextmanagerthat provides the same functionality as the`provide_session`decorator. This is helpful in cases where thescope of a session shouldbe smaller than the entire method where it isbeing used.Closes #2739 from StephanErb/session_close",1
Merge pull request #2628 from moertel/registry-login,2
[AIRFLOW-1780] Fix long output lines with unicode from hanging parentFix long task output lines with unicode fromhanging parent process. Tasks that create outputthat gets piped into a file in the parent airflowprocess would hang if they had long lines withunicode characters.Closes #2758 from aoen/ddavydov--fix_unicode_output_string,0
[AIRFLOW-1787] Fix task instance batch clear and set state bugsFixes Batch clear in Task Instances view is not workingfor task instances in RUNNING state and all batchoperations in Task instances view cannot work whenmanually triggered task instances are selectedbecause they have a different execution dateformat.Closes #2759 from yrqls21/fix-ti-batch-clear-n-set-state-bugs,0
[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warningLogging functionality for SSHOperator was added in[AIRFLOW-1712] but itonly logged stdout.This commit also logs stderr to log.warningCloses #2761 from OpringaoDoTurno/stderr_in_ssh,2
[AIRFLOW-1792] Missing intervals DruidOperatorThe DruidOperator allows you to template theintervals field which isimportant when you are doing backfills with Druid.This field wasmissing in the constructor and Airflow threw awarningCloses #2764 from Fokko/patch-1,2
"[AIRFLOW-646] Add docutils to setup_requirespython-daemon declares its docutils dependency in a setup_requiresclause, and 'python setup.py install' fails since it missesthat dependency.Closes #2765 from wrp/docutils",2
"[AIRFLOW-1797] S3Hook.load_string didn't work on Python3With the switch to Boto3 we now need the contentto be bytes, not astring. On Python2 there is no difference, but forPython3 this matters.And since there were no real tests covering theS3Hook I've added somebasic ones.Closes #2771 from ashb/AIRFLOW-1797",1
"[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3HookThe change from boto2 to boto3 in S3Hook causedthis to break (thereturn type of `hook.get_key()` changed. There's abetter methoddesigned for that we should use anyway.This wasn't caught by the tests as the mocksweren't updated. Ratherthan mocking the return of the hook I have changedit to use ""moto""(already in use elsewhere in the tests) to mock atthe S3 layer, notour hook.Closes #2773 from ashb/AIRFLOW-1756-s3-logging-boto3-fix",2
[AIRFLOW-1102] Upgrade Gunicorn >=19.4.0Closes #2775 from briancharous/upgrade-gunicorn,5
[AIRFLOW-1799] Fix logging line which raises errorsCloses #2777 from ryancbuckley/log-syntax-fix,2
[AIRFLOW-1794] Remove uses of Exception.message for Python 3Closes #2766 from dhuang/AIRFLOW-1794,1
[AIRFLOW-1563] Catch OSError while symlinking the latest log directoryCloses #2564 from NielsZeilemaker/AIRFLOW-1563,1
[AIRFLOW-1801][AIRFLOW-288] Url encode execution datesExecution dates can contain special charactersthatneed to be url encoded. In case of timezoneinformationthis information is lost if not url encoded.Closes #2779 from bolkedebruin/AIRFLOW-1801,5
"[AIRFLOW-1813] Bug SSH Operator empty bufferThe SSH Operator will throw an empty ""SSH operatorerror"" when runningcommands that do not immediately log something tothe terminal. This isdue to a call to stdout.channel.recv when thechannel currently has a0-size buffer, either because the command has notyet logged anything,or never will (e.g. sleep 5)Make code PEP8 compliantCloses #2785 from RJKeevil/fix-ssh-operator-no-terminal-output",0
"[AIRFLOW-1817] use boto3 for s3 dependencySince S3Hook is reimplemented based on the AwsHookusing boto3, its package dependencies need to beupdated as well.Closes #2790 from m1racoli/fix-setup-s3",0
"[AIRFLOW-1613] make mysql_to_gcs_operator py3 compatibleUses `__future__.unicode_literals` and replaces calling `json.dumps`with `json.dump` followed by `tmp_file_handle.write` to write json linesto the ndjson file. When using python3, `json.dump` will return aunicode string instead of a byte string, therefore we encode the unicodestring to `utf-8` which is compatible with bigquery (see:https://cloud.google.com/bigquery/docs/loading-data#loading_encoded_data).",5
[AIRFLOW-868] Add postgres_to_gcs operator and unittestsAdds a postgres_to_gcs operator to contrib so that a user can copy adump from postgres to google cloud storage. Tests write to localNamedTemporayFiles so we correctly test serializing encoded ndjson inboth python3 and python2.7.,5
[AIRFLOW-1816] Add region param to Dataproc operatorsCloses #2788 from DanSedov/master,1
[AIRFLOW-1805] Allow Slack token to be passed through connectionAllow users to pass in Slack token throughconnection which can provide better security. Thisenables user to expose token only to workersinstead to both workers and schedulers.Closes #2789 fromyrqls21/add_conn_supp_in_slack_op,1
[AIRFLOW-1819] Fix slack operator unittest bugFix failing slack operator unittest and add testcoverage.Closes #2791 from yrqls21/kevin-yang-fix-unit-test,3
[AIRFLOW-1811] Fix render Druid operatorSet the correct fields to enable the visualisationof the renderingof the Druid indexing spec. Add some tests to makesure that thetemplating is working :-)Closes #2783 from Fokko/AIRFLOW-1811-fix-druid-operator,0
"[AIRFLOW-1795] Correctly call S3Hook after migration to boto3In the migration of S3Hook to boto3 the connectionID parameter changedto `aws_conn_id`. This fixes the uses of`s3_conn_id` in the code baseand adds a note to UPDATING.md about the change.In correcting the tests for S3ToHiveTransfer Inoticed thatS3Hook.get_key was returning a dictionary, ratherthen the S3.Object asmentioned in it's doc string. The important thingthat was missing wasability to get the key name from the return a callto get_wildcard_key.Closes #2795 fromashb/AIRFLOW-1795-s3hook_boto3_fixes",0
[AIRFLOW-1831] Add driver-classpath spark submitAdd the ability to set the driver-classpath forthe spark_submitoperator and hook.Closes #2800 from danielvdende/add-spark-driver-classpath,1
[AIRFLOW-1830] Support multiple domains in Google authentication backendCloses #2797 from wileeam/multiple-domains-google-auth,1
"[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migrationThere were some more bugs as a result of the bototo boto3 migrationthat weren't covered by existing tests. Now theyare fixed, and covered.Hopefully I got everything this time.Closes #2805 from ashb/AIRFLOW-1839-s3-hook_loadsa-tests",3
[AIRFLOW-1841] change False to None in operator and hookDocumentation stated it's String type but in codeit was union of String and Bool.Changed to to pure string by substituting False toNone since in operator and hookcode checks only for presence of value invariable.Make it more predictable by using simpler Stringtype.Closes #2807 from litdeviant/gcs-operator-hook,1
"[AIRFLOW-1842] Add gcs to gcs copy operator with renaming if requiredCopies an object from a Google Cloud Storagebucket to another GoogleCloud Storage bucket, with renaming if required.Closes #2808 from litdeviant/gcs_to_gcs",1
"[AIRFLOW-1229] Add link to Run Id, incl execution_dateAdd two UI improvement. 1: the links from ""DAGruns"" to DAG graph viewinclude the execution_date. So you land on theexpected DAG, instead ofthe last DAG run. 2: A new link is added for thecolumn ""Run Id"" with thesame behaviour.Closes #2801 from abij/AIRFLOW-1229",1
[AIRFLOW-1845] Modal background now covers long or tall pagesIf the page was scrolled before the dialog wasdisplayed then the greybackground would not cover the whole pagecorrectly.Closes #2813 from ashb/AIRFLOW-1845-modal-background-on-long-pages,2
[AIRFLOW-1842] Fixed Super class name for the gcs to gcs copy operatorFixed incorrect Super class name in gcs to gcscopy operator from `GoogleCloudStorageOperatorToGoogleCloudStorageOperator` to`GoogleCloudStorageToGoogleCloudStorageOperator`.Closes #2812 from kaxil/patch-2,1
[AIRFLOW-1838] Properly log collect_dags exceptionAs a user it would be nice to properly log theexceptionsthrown in the collect_dags function to debug thefaulty dagsCloses #2803 from Fokko/AIRFLOW-1838-Properly-log-collect-dags-exception,2
[AIRFLOW-1810] Remove unused mysql import in migrations.Closes #2782 from MortalViews/master,2
[AIRFLOW-1820] Remove timestamp from metric nameCloses #2792 from wrp/datetime,5
[AIRFLOW-XXX] Update README.mdCloses #2780 from runongirlrunon/patch-1,1
[AIRFLOW-1790] Add support for AWS Batch operatorCloses #2762 from hprudent/aws-batch,1
[AIRFLOW-XXX] Add dask lock files to excludes,2
[AIRFLOW-1802] Convert database fields to timezone aware,5
[AIRFLOW-1804] Add time zone configuration optionsTime zone defaults to UTC as is the default now in orderto maintain backwards compatibility.,5
[AIRFLOW-1808] Convert all utcnow() to time zone awaredatetime.utcnow() does not set time zone information.,5
[AIRFLOW-1807] Force use of time zone aware db fieldsThis change will check if all date times being stored areindeed timezone aware.,5
[AIRFLOW-1806] Use naive datetime for cron schedulingConverting to naive time is required in order to make sureto run at exact times for crons.E.g. if you specify to run at 8:00pm every day you do notwant suddenly to run at 7:00pm due to DST.,1
[AIRFLOW-1809] Update tests to use timezone aware objects,1
[AIRFLOW-1806] Use naive datetime when using cron,1
[AIRFLOW-1827] Fix api endpoint date parsing,5
[AIRFLOW-1826] Update views to use timezone aware objects,1
[AIRFLOW-1803] Time zone documentation,2
Merge pull request #2786 from x/postgres_to_bigquery_operator,1
[AIRFLOW-1843] Add Google Cloud Storage Sensor with prefixSensor for checking if there any files in bucketat certain prefixCloses #2809 from litdeviant/gcs_prefix_sensor,0
Merge pull request #2781 from bolkedebruin/AIRFLOW-1802,7
[AIRFLOW-1848][Airflow-1848] Fix DataFlowPythonOperator py_file extension doc commentCloses #2816 from cjqian/1848,2
[AIRFLOW-1559] Make database pooling optionalIn situations where a database is heavily loaded with connections itcan be beneficial for operators to (temporarily) reduce the connectionfootprint of Airflow on the database. This is particularly importantwhen Airflow or self-made extensions do not dispose the connectionpool when terminating.Disabling the connection pool comes with a slowdown but that may beacceptable in many deployment scenarios.,2
"[AIRFLOW-1559] Close file handles in subprocessesAll file descriptors except 0, 1 and 2 will be closed before thechild process is executed. This is the default on Python 3.2 andabove. This patch ensures consistent behaviour for older Pythonversions.Resources will be released once the main thread disposesthem, independent of the longevity of its subprocesses.Background information:* https://www.python.org/dev/peps/pep-0446/* https://bugs.python.org/issue7213",0
"[AIRFLOW-1559] Dispose SQLAlchemy engines on exitWhen a forked process or the entire interpreter terminates, we haveto close all pooled database connections. The database can run outof connections otherwise. At a minimum, it will print errors in itslog file.By using an atexit handler we ensure that connections are closedfor each interpreter and Gunicorn worker termination. Only usagesof multiprocessing.Process require special handling as thoseterminate via os._exit() which does not run finalizers.This commit is based on a contribution by @dhuanghttps://github.com/apache/incubator-airflow/pull/2767",1
[AIRFLOW-1665] Reconnect on database errorsThis change enables the scheduler to recover from temporary databaseerrors and downtimes. The same holds true for the webserver if runwithout its regular worker refresh.The reconnect logic is based on a truncated exponential binary backoffto ensure reconnect attempts don't overload the database.Included changes:* Switch to recommended pessimistic disconnect handling for engines  http://docs.sqlalchemy.org/en/rel_1_1/core/pooling.html#disconnect-handling-pessimistic* Remove legacy pool-based disconnect handling.* Ensure event handlers are registered for each newly created engine.  Engines are re-initialized in child processes so this is crucial for  correctness.This commit is based on a contribution by @vkloginhttps://github.com/apache/incubator-airflow/pull/2744,2
[AIRFLOW-1850] Copy cmd before maskingThe cmd is first copied before the password ismasked. This ensuresthat the orignal cmd isn't changed. Replacing thepassword with amasked value replaces the password in the originalcommand since itis passed by reference.Closes #2817 from Fokko/AIRFLOW-1850-copy-cmd-before-replacing-password,4
[AIRFLOW-1785] Enable Python 3 testsEnable tests under Python 3 to make sure thattests run under Python3.Closes #2755 from Fokko/AIRFLOW-1785-Enable-Python3-tests,3
[AIRFLOW-1870] Enable flake8 testsFlake8 tests now run for diffsCloses #2829 from bolkedebruin/use_flake8,1
[AIRFLOW-1855][AIRFLOW-1866] Add GCS Copy Operator to copy multiple filesCloses #2819 from kaxil/master,2
"[AIRFLOW-1872] Set context for all handlers including parentsPreviously setting the context was not propagatedto the parentloggers. Unfortnately, in case of a non explicitlydefined loggerthe returned logger is shallow, ie. it does nothave handlersdefined. So to set the context it is required towalk the tree.Closes #2831 from bolkedebruin/fix_logging",2
Merge pull request #2822 from StephanErb/db_robustness,5
[AIRFLOW-1883] Get File Size for objects in Google Cloud StorageCloses #2840 from kaxil/Get_File_Size,2
[AIRFLOW-XXX] Added DataReply to the list of Airflow UsersCloses #2841 from kaxil/patch-1,1
[AIRFLOW-1881] Make operator log in task logPreviously operators logged underairflow.operators orairflow.contrib.operators. This unifies them underairflow.task.operators allowing the task log topickthem up and not have 'double' logging.Closes #2838 from bolkedebruin/AIRFLOW-1881,2
[AIRFLOW-966] Make celery broker_transport_options configurableRequired for changing visibility timeout and otheroptions requiredfor Redis/SQS.Closes #2842 from bolkedebruin/AIRFLOW-966,1
"[AIRFLOW-342] Do not use amqp, rpc as result backendamqp and rpc (and redis most likely) cannot storeresults for taskslong enough.Closes #2830 from bolkedebruin/AIRFLOW-342",1
add hostnfly as users of airflowCloses #2845 from alexisrosuel/master,1
[AIRFLOW-1554] Fix wrong DagFileProcessor termination method callCloses #2821 frompdambrauskas/fix/wrong_termination_call,0
[AIRFLOW-1876] Write subtask id to task log headerCloses #2835 from wrp/subtask-id,2
[AIRFLOW-1869] Write more error messages into gcs and file logsCloses #2826 from wrp/gcs-log,2
"[AIRFLOW-1879] Handle ti log entirely within tiPreviously logging was setup outside aTaskInstance,this puts everything inside. Also propery closesthe logging.Closes #2837 from bolkedebruin/AIRFLOW-1879",2
Update README.mdAdded Playbuzz to list of companies using Airflow.Closes #2828 from clintonboys/patch-5,1
[AIRFLOW-1891] Fix non-ascii typo in default configuration templateCloses #2851 from wileeam/non-utf8-typo-default-cfg,5
"[AIRFLOW-1873] Set TI.try_number to right value depending TI stateRather than having try_number+1 in various places,try_numberwill now automatically contain the right value forwhen the TIwill next be run, and handle the case wheretry_number isaccessed when the task is currently running.This showed up as a bug where the logs fromrunning operators wouldshow up in the next log file (2.log for the firsttry)Closes #2832 from ashb/AIRFLOW-1873-task-operator-log-try-number",2
[AIRFLOW-1887] Renamed endpoint url variables3_endpoint_url is a legacy name from when AwsHookwas only used toconnect to S3. The endpoint_url is more generaland what is effectivelyused elsewhere for this piece of information.Closes #2848 from villasv/AIRFLOW-1887,5
"[AIRFLOW-1888] Add AWS Redshift Cluster SensorAdd AWS Redshift Cluster Sensor to contrib, alongwith correspondingunit tests. Additionally, updated Redshift Hookcluster_status method tobetter handle cluster_not_found exception, addedunit tests, andcorrected linting errors.Closes #2849 from andyxhadji/AIRFLOW-1888",0
"[AIRFLOW-1869] Do not emit spurious warning on missing logsIf the log does not already exist then we are notdiscarding it, andthis message is more confusing than helpful.Closes #2856 from wrp/suppress-log",2
[AIRFLOW-XXX] Fix typo in commentCloses #2850 from MortalViews/fix-typo,2
"[AIRFLOW-1884][AIRFLOW-1059] Reset orphaned task state for external dagrunsOn scheduler startup, orphaned task instances havetheir state cleared and are rescheduled to avoidhaving tasks that are stuck in a QUEUED stateforever. Previously, this check ignored backfilledand externally triggered dagruns, meaning thatbackfilled and externally triggered dagruns couldhave orphaned tasks that are stuck forever. Thischangeset removes the special case logic forexternally triggered dagruns, ensuring thatexternally triggered dagruns are crash safe. Thissame fix cannot be applied to backfilled dagruns,so for now backfilled dagruns are not crash safe.Closes #2843 from grantnicholas/AIRFLOW-1884",2
[AIRFLOW-1896] FIX bleach <> html5lib incompatibilityRunning airflow with bleach 2.0.0 can cause:`ImportError: No module named base`https://github.com/mozilla/bleach/issues/267This was resolved in https://github.com/mozilla/bleach/releases/tag/v2.1.2Closes #2858 from kamilchm/patch-1,0
[AIRFLOW-1897][AIRFLOW-1873] Task Logs for running instance not visible in WebUIDue to the change in AIRFLOW-1873 we inadvertentlychanged the behavioursuch that task logs for a try wouldn't show up inthe UI until after thetask run had completed.Closes #2859 from ashb/AIRFLOW-1897-view-logs-for-running-instance,2
[AIRFLOW-1878] Fix stderr/stdout redirection for taskslogging.StreamHandler keeps a reference to theinitial streamit has been assigned. This prevents redirectionafter initalizationto a logging facility.Closes #2836 from bolkedebruin/AIRFLOW-1878,2
[AIRFLOW-1840] Make celery configuration congruent with Celery 4Explicitly set the celery backend from the configand align the configwith the celery config as this might be confusing.Closes #2806 from Fokko/AIRFLOW-1840-Fix-celery-config,5
[AIRFLOW-1829] Support for schema updates in query jobsCloses #2796 from wileeam/bq-operator-query-schema-update-support,5
[AIRFLOW-1892] Modify BQ hook to extract data filtered by columnCloses #2855 from kaxil/patch-2,5
"[AIRFLOW-1893][AIRFLOW-1901] Propagate PYTHONPATH when using impersonationWhen using impersonation via `run_as_user`, thePYTHONPATH environmentvariable is not propagated hence there may beissues when depending onspecific custom packages used in DAGs.This PR propagates only the PYTHONPATH in theprocess creating thesub-process with impersonation, if any.Tested in staging environment; impersonation testsin airflow are not very portable and fixing themwould take additional work, leaving as TODO andtracking with jira ticket: https://issues.apache.org/jira/browse/AIRFLOW-1901.Closes #2860 from edgarRd/erod-pythonpath_run_as_user",1
[AIRFLOW-1909] Add away to list of usersCloses #2868 from trunsky/patch-1,1
Add Global Fashion Group as an Airflow userCloses #2815 from duynguyenhoang/Add-GFG-Using-Airflow,1
[AIRFLOW-1907] Pass max_ingestion_time to Druid hookFrom the Druid operator we want to pass themax_ingestion_time to thehook since some jobs might take considerably moretime than the othersBy default we dont want to set a max ingestiontime.Closes #2866 from Fokko/AIRFLOW-1907-pass-max-ingestion-time,4
"[AIRFLOW-1908] Fix celery broker options config loadOptions were set to visibility timeout instead ofbroker_optionsdirectly. Furthermore, options should be int,float, bool or stringnot all string.Closes #2867 from bolkedebruin/AIRFLOW-1908",1
[AIRFLOW-1854] Improve Spark Submit operator for standalone cluster modeCloses #2852 from milanvdmria/svend/submit2,1
"[AIRFLOW-1885] Fix IndexError in ready_prefix_on_cmdlineIf while trying to obtain a list of ready gunicornworkers, one of thembecomes a zombie, psutil.cmdline returns [] (seehere:https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007)Boom:    Traceback (most recent call last):      File ""/usr/local/bin/airflow"", line 28, in<module>        args.func(args)      File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, inwebserver        restart_workers(gunicorn_master_proc, num_workers)      File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, inrestart_workers        num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)      File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, inget_num_ready_workers_running        proc for proc in workers      File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in<listcomp>        if settings.GUNICORN_WORKER_READY_PREFIX inproc.cmdline()[0]    IndexError: list index out of rangeSo ensure a cmdline is actually returned beforedoing the cmdline prefixcheck in ready_prefix_on_cmdline.Also: * Treat psutil.NoSuchProcess error as non readyworker * Add in tests for get_num_ready_workers_runningCloses #2844 from j16r/bugfix/poll_zombie_process",0
[AIRFLOW-1911] Rename celeryd_concurrencyThere are still celeryd_concurrency occurrencesleft in the codethis needs to be renamed to worker_concurrency tomake the configwith Celery consistentCloses #2870 from Fokko/AIRFLOW-1911-update-airflow-config,5
[AIRFLOW-1912] airflow.processor should not propagate loggingThe root logger will write to stdout. Ifredirection is usedwhich is the case for processors and task runs(not runners)this can end up in an endless loop in casepropagation is True.Closes #2871 from bolkedebruin/AIRFLOW-1912,1
[AIRFLOW-1687] fix fernet error without encryptionCloses #2668 from TrevorEdwards/airflow-1687,0
[AIRFLOW-1525] Fix minor LICENSE and NOTICE issuesCloses #2884 from criccomini/AIRFLOW-1525,5
[AIRFLOW-1913] Add new GCP PubSub operatorsCloses #2872 from prodonjs/master,1
[AIRFLOW-XXX] Upgrade to python 3.5 and disable dask testsDask tests seem to create issues down the line.,0
Merge branch 'disable_dask',7
[AIRFLOW-XXX] Fix failing PubSub tests on Python3Correct failing tests due to not explicitly usingb'' string when b64encoding.Closes #2893 from prodonjs/master,1
"[AIRFLOW-1916] Don't upload logs to remote from `run --raw`In a previous change we removed theairflow.task.raw handler (whichprinted to stdout directly) and replaced it withone that wrote to thelog file itself. The problem comes that pythonautomatically calls`logging.shutdown()` itself on process clean exit.This ended upuploading the log file twice: once from the end of`airflow run --raw`,and then again from the explicit shutdown() callat the end of cli's`run()`Since logging is automatically shutdown thischange adds and explicitflag to control if the GC and S3 handlers shouldupload the file or not,and we tell them not to when running with `--raw`Closes #2880 from ashb/AIRFLOW-1916-dont-upload-logs-twice",2
[AIRFLOW-1938] Remove tag version check in setup.pyCloses #2889 from criccomini/AIRFLOW-1938,5
[AIRFLOW-XXX] Remove unused coveralls token,1
[AIRFLOW-XXX] Purge coveralls,5
"[AIRFLOW-1932] Add GCP Pub/Sub Pull and AckAdds the necessary hooks to support pulling andacknowleding Pub/Submessages. This is implemented by adding aPubSubPullSensor operatorthat will attempt to retrieve messages from aspecified subscriptionand will meet its criteria when a message ormessages is available.The configuration allows those messages to beacknowledged immediately.In addition, the messages are passed to downstreamworkers via thereturn value of operator's execute method.An end-to-end example is included showing topicand subscriptioncreation, parallel tasks to publish and pullmessages, and a downstreamchain to echo the contents of each message beforecleaning up.Closes #2885 from prodonjs/airflow-1932-pr",4
[AIRFLOW-1938] Clean up unused exceptionThere is no longer the possibility of aGitCommandError (since cc4404b5f75)Closes #2898 from wrp/setup,1
[AIRFLOW-1948] Include details for on_kill failureRemove the bare exception and propagate errorinformation to the log.Closes #2897 from wrp/onkill,2
[AIRFLOW-1846][AIRFLOW-1697] Hide Ad Hoc Query behind secure_mode configCloses #2895 from bitsofdave/AIRFLOW-1846,5
"[AIRFLOW-1942] Update Sphinx docs to remove deprecated import structureUpdate Sphinx docs to use correct importstructure. Fixes improperlymocked modules that resulted in hooks notdisplaying. Fixes executorsand operators section, which weren't displayinganything.Closes #2894 from andyxhadji/AIRFLOW-1942",1
[AIRFLOW-1920] Update CONTRIBUTING.md to reflect enforced linting rulesUpdates Contributing guidelines to reflectenforced linting rules & addsinstructions for running single file unit tests.Closes #2878 from andyxhadji/AIRFLOW-1920,3
[AIRFLOW-1915] Relax flask-wtf dependency specificationCloses #2876 from wrp/flask-wtf,5
"[AIRFLOW-1909] Update docs with supported versions of MySQL serverAlso, updates a the docs script to be compatiblewith bothPython 2 and Python 3.Updated to include that MySQL server's version hasto be 5.6.4+Closes #2869 from markgrover/airflow-1909",5
[AIRFLOW-1904] Correct DAG fileloc to the right filepathCloses #2863 from jgao54/fix-fileloc,2
[AIRFLOW-1821] Enhance default logging config by removing extra loggersCloses #2793 from jgao54/logging-enhancement,2
"[AIRFLOW-1937] Speed up scheduling by committing in batchNewly scheduled task instances (state = None, upfor retry)were committed per task instance instead of all atonce.This isn't required as tasks cannot be picked upby anotherprocess in the mean time. Committing in batchsignificantlyspeeds up task scheduling for dags that have a lotof tasks.Closes #2888 from bolkedebruin/AIRFLOW-1937",2
Add Creditas to Airflow usersCloses #2901 from dcassiano/add-creditas-to-company-list,2
"[AIRFLOW-1928] Fix @once with catchup=FalseWhile @once with catchup=False don't make sense,the scheduler could schedule these kind of dagsdue to a logic error.Closes #2883 from bolkedebruin/AIRFLOW-1928",0
[AIRFLOW-1517] Kubernetes Operator,1
[AIRFLOW-1939] add astronomer contributorsCloses #2890 from andscoop/master,1
[AIRFLOW-1935] Add BalanceHero to readmeCloses #2905 from r39132/master,1
closes incubator-airflow/#1408,5
closes apache/incubator-airflow#1408 *Already Merged,7
[AIRFLOW-1517] Remove authorship of secrets and init container,5
[AIRFLOW-1517] Restore authorship of secrets and init container,5
Closes apache/incubator-airflow#2440 *Already Merged*,7
[AIRFLOW-1957] Add contributor to BalanceHero in ReadmeCloses #2907 from r39132/master,1
[AIRFLOW-1955] Do not reference unassigned variableCloses #2904 from wrp/old_log,2
"Revert ""[AIRFLOW-1955] Do not reference unassigned variable""This reverts commit 9565a9879280d83c6c3987d3a6f8b8933168cedf.",4
[AIRFLOW-XXX] Pin sqlalchemy dependency,5
Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/incubator-airflow,7
"Revert ""Revert ""[AIRFLOW-1955] Do not reference unassigned variable""""This reverts commit fdd7f43fe2aa64bc9001db4f2555c59d568e249a.",5
"[AIRFLOW-1470] Implement BashSensor operatorThis sensor succeeds once a bash command/scriptreturns 0, and keeps poking otherwise. Theimplementation is very similar to BashOperator.Closes #2489 from diogoalexandrefranco/master",1
"Added Quizlet to ""Who uses Airflow?"" listCloses #2910 from dustinstansbury/patch-1",1
Added Ubisoft to Airflow usersCloses #2911 from Walkoss/master,1
[AIRFLOW-XXX] Changelog for 1.9.0,4
[AIRFLOW-1964] Add Upsight to list of Airflow usersCloses #2912 from dhuang/upsight,1
"[AIRFLOW-1967] Update Celery to 4.0.2Update Celery to 4.0.2 for fixing errorTypeError: '<=' not supported between instances of'NoneType' and 'int'Hi all,I'd like to update Celery to version 4.0.2. Whileupdating my Docker container to version 1.9, Icaught this error:```worker_1     | [2018-01-03 10:34:29,934:CRITICAL/MainProcess] Unrecoverable error:TypeError(""'<=' not supported between instances of'NoneType' and 'int'"",)worker_1     | Traceback (most recent call last):worker_1     |   File ""/usr/local/lib/python3.6/site-packages/celery/worker/worker.py"", line 203,in startworker_1     |     self.blueprint.start(self)worker_1     |   File ""/usr/local/lib/python3.6/site-packages/celery/bootsteps.py"", line 115, instartworker_1     |     self.on_start()worker_1     |   File ""/usr/local/lib/python3.6/site-packages/celery/apps/worker.py"", line 143,in on_startworker_1     |     self.emit_banner()worker_1     |   File ""/usr/local/lib/python3.6/site-packages/celery/apps/worker.py"", line 159,in emit_bannerworker_1     |string(self.colored.reset(self.extra_info() or'')),worker_1     |   File ""/usr/local/lib/python3.6/site-packages/celery/apps/worker.py"", line 188,in extra_infoworker_1     |     if self.loglevel <=logging.INFO:worker_1     | TypeError: '<=' not supportedbetween instances of 'NoneType' and 'int'```This is because I've been running Python 2 in mylocal environments, and the Docker image is Python3:https://github.com/puckel/docker-airflow/pull/143/filesThis is the issue in Celery:https://github.com/celery/celery/blob/0dde9df9d8dd5dbbb97ef75a81757bc2d9a4b33e/Changelog#L145Make sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    - https://issues.apache.org/jira/browse/AIRFLOW-XXX### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #2914 from Fokko/AIRFLOW-1967-update-celery",5
[AIRFLOW-1953] Add labels to dataflow operatorsCloses #2913 from fenglu-g/master,1
[AIRFLOW-1946][AIRFLOW-1855] Create a BigQuery Get Data OperatorCloses #2896 from kaxil/patch-4,1
[AIRFLOW-1963] Add config for HiveOperator mapred_queueAdding configuration setting for specifying adefault mapred_queue forhive jobs using the HiveOperator.Closes #2915 from edgarRd/erod-hive-mapred-queue-config,5
[AIRFLOW-1954] Add DataFlowTemplateOperatorCloses #2909 from dsdinter/dataflow_template,5
added Newzoo to airflow organizationsCloses #2917 fromwalternewzoo/airfloworganizations-newzoo,1
[AIRFLOW-1969] Always use HTTPS URIs for Google OAuth2Closes #2900 from intellectronica/google-auth-force-scheme,1
"[AIRFLOW-1971] Propagate hive config on impersonationCurrently, if hive specific settings are definedin the configurationfile, they are not being propagated when usingimpersonation.We need to propagate this configuration down tothe impersonatedprocess.Closes #2920 from edgarRd/erod-propagate-hive-conf",5
[AIRFLOW-1982] Fix Executor event log formattingCloses #2925 fromNielsZeilemaker/fix_email_templating,0
[AIRFLOW-1976] Fix for missing log/logger attribute FileProcessHandlerCloses #2922 fromNielsZeilemaker/fix_log_file_process_handler,2
[AIRFLOW-1958] Add **kwargs to send_emailCloses #2908 from ms32035/email_kwargs,1
[AIRFLOW-1480] Render template attributes for ExternalTaskSensor fieldsCloses #2926 from pdambrauskas/feature/external_task_sensor_templates,5
[AIRFLOW-1975] Make TriggerDagRunOperator callback optionalCloses #2921 from bcb/make-trigger-dag-run-callback-optional,2
[AIRFLOW-1688] Support load.time_partitioning in bigquery_hookCloses #2820 from albertocalderari/master,1
"[AIRFLOW-1930] Convert func.now() to timezone.utcnow()func.now() defaults to the timezone of thedatabase,we assume that everything is in UTC which mightnot bethe case if func.now() is used.Closes #2882 from bolkedebruin/AIRFLOW-1930",1
"[AIRFLOW-1949] Fix var upload, str() produces ""b'...'"" which is not jsonCloses #2899 from j16r/bugfix/AIRFLOW-1949_config_upload",5
[AIRFLOW-790] Clean up TaskInstances without DagRunsCloses #2886 from gwax/AIRFLOW-790_guard_against_orphans,2
[AIRFLOW-1988] Change BG color of None state TIsOn Task Instances page 'None' state TIs are not-visible due to whitebackground. Changing background color for betterUX.Closes #2931 from msumit/AIRFLOW-1988,1
[AIRFLOW-1517] Add minikube for kubernetes integration testsAdd better support for minikube integration tests; By default minikube integration tests will run with kubernetes 1.7 and kubernetes 1.8,1
"[AIRFLOW-1517] Remove authorship of resourcesCollaboration authors got destroyed when splitting up a PR, this commit removes code which will be readded in the next commit to restore authorship",1
[AIRFLOW-1517] Created more accurate failures for kube cluster issues,0
[AIRFLOW-1517] fixed license issues,0
"[AIRFLOW-1517] Restore authorship of resourcesCollaboration authors got destroyed when splitting up a PR, this commit adds back in the code which was be removed in the previous commit to restore authorship",4
"Revert ""[AIRFLOW-1517] Add minikube for kubernetes integration tests""This reverts commit 0197931609685a98181387014f7c8f3b5cd5f9a8.",4
[AIRFLOW-1517] Add minikube for kubernetes integration testsAdd better support for minikube integration tests; By default minikube integration tests will run with kubernetes 1.7 and kubernetes 1.8,1
"[AIRFLOW-1517] Remove authorship of resourcesCollaboration authors got destroyed when splitting up a PR, this commit removes code which will be readded in the next commit to restore authorship",1
"[AIRFLOW-1517] Restore authorship of resourcesCollaboration authors got destroyed when splitting up a PR, this commit adds back in the code which was be removed in the previous commit to restore authorship",4
[AIRFLOW-1517] started documentation of k8s operator,1
[AIRFLOW-1517] addressed PR comments,1
[AIRFLOW-1517] Kubernetes operator PR fixesFix python flake8 linting issues and AIRFLOW license issues,0
[AIRFLOW-1436][AIRFLOW-1475] EmrJobFlowSensor considers Cancelled step as SuccessfulCloses #2937 from Swalloow/master,5
[AIRFLOW-1994] Change background color of Scheduled state Task InstancesOn Task Instances page 'Scheduled' state TIs arenot-visible due to whitebackground. Changing background color to tan forbetter UX.Closes #2933 from wolfier/AIRFLOW-1994,1
"[AIRFLOW-1770] Allow HiveOperator to take in a fileClarify and upgrade HiveOperator. Includedescription of hql parameter being able totake in a relative path from the dag fileof a hive script, templated or not. Addability to template hiveconf variables. Adddefault value to the map reduce job name aswell as add updated hiveconf var for queue.Closes #2752 from wolfier/AIRFLOW-1770",5
[AIRFLOW-1995][Airflow 1995] add on_kill method to SqoopOperatorCloses #2936 from Acehaidrey/AIRFLOW-1995,1
[AIRFLOW-1996] Update DataflowHook waitfordone for Streaming type job[]AIRFLOW-1996 Update DataflowHook waitfordone forStreaming type jobfix flake8Closes #2938 from ivanwirawan/AIRFLOW-1996,0
[AIRFLOW-1997] Fix GCP operator doc stringsCloses #2939 from kaxil/docstring_fix,2
Merge pull request #2853 from dimberman/Airflow_1517_kubenetes_operator,1
[AIRFLOW-2004] Import flash from flask not flask.loginCloses #2943 from bolkedebruin/AIRFLOW-2004,2
[AIRFLOW-2002] Do not swallow exception on logging importCloses #2945 from bolkedebruin/AIRFLOW-2002,2
"[AIRFLOW-2003] Use flask-caching instead of flask-cacheFlask-cache has been unmaintained for over threeyears,flask-caching is the community supported version.Closes #2944 from bolkedebruin/AIRFLOW-2003",1
[AIRFLOW-2000] Support non-main dataflow job classCloses #2942 from fenglu-g/master,5
"[AIRFLOW-1984] Fix to AWS Batch operatorCorrect key is ""container"" rather than ""attempts"":https://docs.aws.amazon.com/batch/latest/APIReference/API_DescribeJobs.htmlCloses #2927 from richardpenman/master",3
Closes apache/incubator-airflow#2919 *Not merging*,5
Closes apache/incubator-airflow#2873 *Not merging due to inactivity*,5
[AIRFLOW-2008] Use callable for python column defaultsWe were using timezone.utcnow() instead of acallable. Thisresulted in inserts with all the same values.Closes #2949 from bolkedebruin/AIRFLOW-2008,1
[AIRFLOW-192] Add weight_rule param to BaseOperatorImproved task generation performance significantlyby using sets oftask_ids and dag_ids instead of lists whencalculating total priorityweight.Closes #2941 from wongwill86/performance-latest,3
[AIRFLOW-511][Airflow 511] add success/failure callbacks on dag levelCloses #2934 from Acehaidrey/AIRFLOW-511,2
[AIRFLOW-1755] Allow mount below rootThis enables Airflow and Celery Flower to livebelow root. Draws on the work of Geatan Semet(@Stibbons).This closes #2723 and closes #2818Closes #2952 from bolkedebruin/AIRFLOW-1755,1
"[AIRFLOW-1950] Optionally pass xcom_pull task_idsChanges the `task_ids` parameter of xcom_pull fromrequired to optional.This parameter has always allowed None to bepassed, but since it's arequired parameter, it must be specified as such.With this change, we're no longer forced to passit.Closes #2902 from bcb/make-xcom-pull-task-ids-optional",1
[AIRFLOW-1889] Split sensors into separate filesMoving the sensors to seperate files increasesreadability of thecode. Also this reduces the code in the bigcore.py file.Closes #2875 from Fokko/AIRFLOW-1889-move-sensors-to-separate-package,4
"[AIRFLOW-2017][Airflow 2017] adding query output to PostgresOperatorMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow 2017](https://issues.apache.org/jira/browse/AIRFLOW-2017/) issues and references them in the PR title.For example, ""[AIRFLOW-2017] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2017### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Currently we're not getting the output logs of thepostgres operator that you would get otherwise ifyou ran a psql command. It's because the postgresconn has an attribute called [notices](http://initd.org/psycopg/docs/connection.html#connection.notices) which contains this information.We need to just print the results of this to getthat output in the airflow logs, which makes iteasy to debug amongst other things.I've included some images for before and afterpictures.**BEFORE**<img width=""1146"" alt=""screen shot 2018-01-19 at 446 59 pm"" src=""https://user-images.githubusercontent.com/10408007/35178405-6f6a1da8-fd3d-11e7-8f50-0dbd567d8ab4.png"">**AFTER**<img width=""1147"" alt=""screen shot 2018-01-19 at 446 25 pm"" src=""https://user-images.githubusercontent.com/10408007/35178406-74ea4ae6-fd3d-11e7-9551-631eac6bfe7b.png"">### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:There isn't anything to test, there is nothingchanging to the current implementation besides anaddition of logging.### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #2959 from Acehaidrey/AIRFLOW-2017",4
[AIRFLOW-2019] Update DataflowHook for updating Streaming type jobCloses #2965 from ivanwirawan/AIRFLOW-2019,5
[AIRFLOW-XXX] Typo node to nodesCloses #2950 from maxcountryman/patch-1,1
[AIRFLOW-XXX] Fixed a typoCloses #2954 from DanielWFrancis/patch-1,2
"[AIRFLOW-1267][AIRFLOW-1874] Add dialect parameter to BigQueryHookAllows a default BigQuery dialect to be specifiedat the hook level, which is threaded through totheunderlying cursors.This allows standard SQL dialect to be used,while maintaining compatibility with the`DbApiHook` interface.Addresses AIRFLOW-1267 and AIRFLOW-1874Closes #2964 from ji-han/master",1
[AIRFLOW-2025] Reduced Logging verbosityCloses #2967 frombritishbadger/reduce_logging_verbosity,2
[AIRFLOW-2016] Add support for Dataproc Workflow TemplatesCloses #2958 from DanSedov/master,1
Closes apache/incubator-airflow#730 *No movement from submitter*,4
Closes apache/incubator-airflow#1839 *No movement from submitter*,4
[AIRFLOW-2028] Add JobTeaser to official users listCloses #2966 from stefani75/master,1
Kick mirroring,5
[AIRFLOW-2029] Fix AttributeError in BigQueryPandasConnectorCloses #2971 from fenglu-g/master,0
[AIRFLOW-2031] Add missing gcp_conn_id in the example in DataFlow docstrings- Added `gcp_conn_id` parameter in the examplesprovided in docstrings for DataFlow operators.Closes #2973 from kaxil/patch-3,1
[AIRFLOW-2006] Add local log catching to kubernetes operatorCloses #2947 from dimberman/AIRFLOW-2006-kubernetes-log-aggregation,2
[AIRFLOW-2033] Add Google Cloud Storage List OperatorAdded an operator to get object names in a GCSbucket filtered by prefix and delimiter withexample.Closes #2974 from kaxil/gcs_list_op,0
[AIRFLOW-1943] Add External BigQuery Table featureAdd ability to create a BigQuery External Table.- Add new method create_external_table() inBigQueryHook()- Add parameters to existingGoogleCloudStorageToBigQueryOperator()Closes #2948 from kaxil/external_table,1
[AIRFLOW-2030] Fix KeyError:`i` in DbApiHook for insertCloses #2972 from untwal/dbahook/fix_key_error,5
"[AIRFLOW-1895] Fix primary key integrity for mysqlsla_miss and task_instances cannot have NULLexecution_dates. The timezone migration scripts forgot to set this properly. Inaddition to make sureMySQL does not set ""ON UPDATE CURRENT_TIMESTAMP""or MariaDB ""DEFAULT0000-00-00 00:00:00"" we now check ifexplicit_defaults_for_timestamp is turnedon and otherwise fail an database upgrade.Closes #2969, #2857Closes #2979 from bolkedebruin/AIRFLOW-1895",5
"[AIRFLOW-2015] Add flag for interactive runsWe capture the standard output and error streamsso that they're handledby the configured logger. However, sometimes, whendeveloping dags orAirflow code itself, it is useful to put pdbbreakpoints in codetriggered using an `airflow run`. Such a flowwould of course requirenot redirecting the output and error streams tothe logger.This patch enables that by adding a flag to the`airflow run`subcommand. Note that this does not require`--local`.Closes #2957 from yati-sagade/ysagade/airflow-2015",1
[AIRFLOW-1453] Add 'steps' into template_fields in EmrAddStepsCloses #2981 from Swalloow/emr-add-steps,1
[AIRFLOW-XXX] Add Pernod-ricard as a airflow userCloses #2983 from romain-nio/AddPernodRicardAsAirflowUser,1
[AIRFLOW-2023] Add debug logging around number of queued filesAdd debug logging around number of queued files toprocess in thescheduler. This makes it easy to see when thereare bottlenecks due to parallelism and how long ittakes for all files to be processed.Closes #2968 from aoen/ddavydov--add_more_scheduler_metrics,1
[AIRFLOW-2043] Add Intercom to list of companiesCloses #2985 from fox/add-intercom,1
[AIRFLOW-2050] Fix Travis permission problemThe travis build has issues with reading the FlaskAdmin wheel.Explicitly delete the wheel for now.Closes #2993 from Fokko/fd-fix-permissions-travis,0
[AIRFLOW-2037] Add methods to get Hash values of a GCS object- Added `get_md5hash` and `get_crc32c` in`gcs_hook` to aid in Data integrity validations.Closes #2977 from kaxil/hashing_gcs_hook,1
[AIRFLOW-2044] Add SparkSubmitOperator to documentationAdded community contributed SparkSubmitOperator toAPI documentationCloses #2987 from Debdutto/docs/added-spark-submit-operator,2
[AIRFLOW-XXX] Add Plaid to Airflow usersCloses #2995 from AustinBGibbons/master,1
[AIRFLOW-2057] Add Overstock to list of companiesCloses #3001 from mhousley/add-overstock-to-list,1
[AIRFLOW-2053] Fix quote character bug in BQ hookModified the condition to check if thequote_character is set. This will allow to set`quote_character` as empty string when the datadoesn't contain quoted sections.Closes #2996 from kaxil/bq_hook_quote_fix,0
[AIRFLOW-2039] BigQueryOperator supports priority propertyCloses #2980 from yu-iskw/modify-bqoperator,1
[AIRFLOW-2055] Elaborate on slightly ambiguous documentationCloses #2999 from AetherUnbound/bugfix/var-doc-reference,2
[AIRFLOW-1793] Use docker_url instead of invalid base_urlCloses #2998 from kodieg/patch-3,2
[AIRFLOW-XXX] Fix typo in docsCloses #3004 from cbockman/patch-1,2
"[AIRFLOW-2063] Add missing docs for GCP- Add missing operator in `code.rst` and`integration.rst`- Fix documentation in DataProc operator- Minor doc fix in GCS operators- Fixed codeblocks & links in docstrings forBigQuery, DataProc, DataFlow, MLEngine, GCS hooks& operatorsCloses #3003 from kaxil/doc_update",5
[AIRFLOW-2046] Fix kerberos error to work with python 3.xBecause Popen returns bytes and not str in python3we need to join itusing bytes.  This simple fix ensures that we joinwith a byte.  Python2.7 is unaffected by this.Closes #2988 from tobes/kerberos-python3,0
[AIRFLOW-2048] Fix task instance failure string formattingCloses #2990 from AetherUnbound/bugfix/job-error-collection,0
"[AIRFLOW-1968][AIRFLOW-1520] Add role_arn and aws_account_id/aws_iam_role support back to aws hookIn PR2532 (AIRFLOW-1520), the AWS credential codewas refactored into a general AWS hook.  When that change was made, the existingassume role code was removed, leaving only ID/Secret credentials as anoption.  Our dags rely on role assumption to access external S3 buckets, sothis code re-adds role assumption via STS.Additionally, in order to make this a bit easier,I changed _get_credentials to return a functioning boto3 session which is usedby the public methods to initialize clients/resources/whatever.  Thisseemed a better route than adding another returnval in an already long list.Closes #2918 from CannibalVox/aws_hook_support_sts",1
[AIRFLOW-2040] Escape special chars in task instance logs URLCloses #2982 from cinhil/AIRFLOW-2040,2
"[AIRFLOW-2038] Add missing kubernetes dependency for devWhen doing initdb, it fails on the kubernetesdependency from theexamplesCloses #2978 from Fokko/fd-fix-dependencies",0
[AIRFLOW-1760] Password auth for experimental APIModified the Password authentication to supportHTTP Basic authCloses #2730 from NielsZeilemaker/AIRFLOW-1760,1
[AIRFLOW-1927] Convert naive datetimes for TaskInstancesTaskInstances are sometimes instantiated outsidecoreAirflow with naive datetimes. In case this happenswenow default to using the time zone of the DAG ifthatis available or the default system time zone.Closes #2946 from bolkedebruin/AIRFLOW-1927,5
"[AIRFLOW-2074] Fix log var name in GHE authA previous log refactor changed the logger namefrom `_log` to `log`,but didn't update callers.Closes #3011 from cmlad/fix-ghe-auth-logging",2
[AIRFLOW-2069] Allow Bytes to be uploaded to S3Closes #3009 from NielsZeilemaker/s3_load_bytes,1
[AIRFLOW-XXX] Fix typo in concepts doc (dag_md)Closes #2994 from eschwartz/patch-1,2
"[AIRFLOW-2018][AIRFLOW-2] Make Sensors backward compatibleTo keep compatibility until Airflow 2.0, we wantto keep the sensorsapi compatible.Closes #2961 from Fokko/fd-sensor-backward-compatible",1
"[AIRFLOW-1985] Impersonation fixes for using `run_as_user`Changes to propagate config insubdags, and make sure tasks running have theconfig.Making sure all tasks have the properconfiguration by copying it into atemporary file.BashOperators should have the context of theAIRFLOW_HOME and PYTHONPATHas other tasks have.Closes #2991 from edgarRd/erod-impersonation-fix",0
[AIRFLOW-XXX] Add TM to list of companiesCloses #3014 from marksteve/add-tm,1
[AIRFLOW-2077] Fetch all pages of list_objects_v2 responseCloses #3012 from NielsZeilemaker/s3_paged,1
[AIRFLOW-2080] Use a log-out icon instead of a power buttonSome users think the existing log out icon willrestart Airflow itself. Using the log out iconshould avoid confusion.Closes #3010 from pingbat/patch-1,5
[AIRFLOW-2078] Improve task_stats and dag_stats performanceCloses #3013 from ori-moisis/master,2
"[AIRFLOW-2073] Make FileSensor fail when the file doesn't existPreviously it never returned False because os.walknever failed.It also wasn't clear the behaviour of this sensorshould have when givena directory. I think the most useful behaviourwould be to wait for anyfiles to exist, rather than returning as soon asthe directory itselfexists.Closes #3017 from ashb/AIRFLOW-2073-file-sensor-never-failed",2
[AIRFLOW-XXX] add Karmic to list of companies,1
"[AIRFLOW-2066] Add operator to create empty BQ table- Add operator that creates a new, empty table inthe specified BigQuery dataset, optionally withschema.Closes #3006 from kaxil/bq_empty_table_op",5
"[AIRFLOW-2083] Docs: Use ""its"" instead of ""it's"" where appropriateCloses #3020 from wrp/spelling",1
Merge pull request #3019 from hyw/master,7
"[AIRFLOW-713] Jinjafy {EmrCreateJobFlow,EmrAddSteps}Operator attributesTo dynamically templat the fields of the Emr Operators, we needto pass the fields to jinjaCloses #3016 from Swalloow/emr-jinjafied",4
[AIRFLOW-1157] Fix missing pools crashing the schedulerThrow a warning when a pool associated with a TaskInstancedoesn't exist instead of crashing the scheduler.Use the default value of 0 slots for non-existentpools.Closes #3002 from iansuvak/1157_nonexistent_pool,1
[AIRFLOW-2090] Fix typo in DataStore HookSpelling mistake in the word `simultaneously` inDataStore docsCloses #3024 from kaxil/patch-1,2
"[AIRFLOW-2091] Fix incorrect docstring parameter in BigQuery Hook- Instead of `seq_of_parameters`, the docstringcontains `parameters`. Fixed thisCloses #3025 from kaxil/fix-parameter",2
[AIRFLOW-2088] Fix duplicate keys in MySQL to GCS Helper function- Remove the duplicate key in`MySqlToGoogleCloudStorageOperator` in the`type_map` helper function that maps from MySQLfields to BigQuery fields.Closes #3022 from kaxil/duplicate-keys-fix,0
[AIRFLOW-XXX] Add SocialCops to Airflow usersCloses #3018 from vinayak-mehta/update_readme,5
[AIRFLOW-2092] Fixed incorrect parameter in docstring for FTPHookFixed the datatype of parameter`local_full_path_or_buffer` in `retrieve_file`method for FTPHookCloses #3026 from kaxil/patch-2,1
"[AIRFLOW-2094] Jinjafied project_id, region & zone in DataProc{*} Operators- Minor docstring change- Jinjafied project_id, region & zone inDataProc{*} Operators to allow using AirflowvariablesCloses #3027 from kaxil/patch-3",1
"[AIRFLOW-1002] Add ability to clean all dependencies of removed DAGAfter removing a dag file, there is no way toclean databaseexcept for removing corresponding records directlyfor now.This PR enables user to do this via command line.Closes #2199 from sekikn/AIRFLOW-1002",1
"[AIRFLOW-2085] Add SparkJdbc operatorAdd the Spark JDBC hook/operator pair. This pairextendsthe existing SparkSubmitOperator. Also includesthespark_jdbc_script, which is the actual Spark jobthat is run.Closes #3021 from danielvdende/AIRFLOW-2085-add-spark-jdbc-operator",5
[AIRFLOW-2095] Add operator to create External BigQuery Table- Added operator to create External BigQuery Table- Added documentation- Added tests- Fixed documentation for GCSCloses #3028 from kaxil/bq-external-tb-op,2
[AIRFLOW-1983] Parse environment parameter as templateCloses #2928 frompatsak/f/environment_paramater_as_template,2
[AIRFLOW-1319] Fix misleading SparkSubmitOperator and SparkSubmitHook docstring- Reword docstring to reduce ambigiuity in theusage of `--files` option.- See JIRA issuehttps://issues.apache.org/jira/browse/AIRFLOW-1319for more info.Closes #2377 from chrissng/airflow-1319,5
[AIRFLOW-800] Initialize valid Google BigQuery Connection- Modified `GoogleCloudBaseHook` to change thename of parameter `conn_id` to keep it consistentwith other Hooks.- Changed the connection to `GoogleCloudBaseHook`instead of `BigQueryHook` which was causing aninvalid `conn_type` creation during `airflowinitdb`Closes #3031 from kaxil/AIRFLOW-800,5
[AIRFLOW-1404] Add 'flatten_results' & 'maximum_bytes_billed' to BQ Operator- Updated BQ hook `run_query` method to add'flatten_results' & 'maximum_bytes_billed'parameters- Added the same in BQ OperatorCloses #3030 from kaxil/AIRFLOW-1404,1
[AIRFLOW-2100] Fix Broken Documentation Links- Replaced `Pythonhosted` documentation links with`Apache Docs`Closes #3033 from kaxil/AIRFLOW-2100,2
[AIRFLOW-XXX] Add PMC to list of companies using AirflowCloses #3034 from andrewm4894/patch-1,1
"Revert ""[AIRFLOW-2050] Fix Travis permission problem""This reverts commit 48202ad5bd763c5f500baf6f0ea208f25cf4134b.",4
[AIRFLOW-2116] Set CI Cloudant version to <2.0The python-cloudant release 2.8 is broken andcauses our CI to fail.In the setup.py we install cloudant version <2.0and in our CI pipelinewe install the latest version.Closes #3051 from Fokko/fd-fix-cloudant,0
"[AIRFLOW-2112] Fix svg width for Recent Tasks on UI.As we expect to have 8 elements with width between25px and 30px, 30 pxwere used.Closes #3047 from diraol/fix2112-svg-width",0
"[AIRFLOW-2113] Address missing DagRun callbacksGiven that the handle_callback method belongs tothe DAG object, we are able to get the list oftask directly with get_task and reduce thecommunication with the database, making airflowmore lightweight.Closes #3038 from wolfier/master",1
[AIRFLOW-2089] Add on kill for SparkSubmit in Standalone Clusteradds a kill command in case of running a Standalone Cluster for SparkCloses #3023 from milanvdm/milanvdm/improve-spark-on-kill,1
[AIRFLOW-1882] Add ignoreUnknownValues option to gcs_to_bq operator- Added `ignore_unknown_values` to`run_load` method in `BigQuery Hook`- Added `ignore_unknown_values` to`GoogleCloudStorageToBigQueryOperator`Closes #3042 from kaxil/AIRFLOW-1882,1
[AIRFLOW-XXX] Add contributor from Easy companyCloses #3052 from diraol/add-contributor,1
[AIRFLOW-2115] Fix doc links to PythonHostedReplaced `http://pythonhosted.org/airflow/` linksto `https://airflow.incubator.apache.org/` in:- Airflow Web UI- `default_airflow.cfg` file- Tutorial- `CONTRIBUTING.md`Closes #3050 from kaxil/AIRFLOW-2115,2
[AIRFLOW-2108] Fix log indentation in BashOperator- Changed from `strip` to `rstrip` to retainleading whitespacesCloses #3045 from kaxil/AIRFLOW-2108,4
[AIRFLOW-1618] Add feature to create GCS bucket- Added `create_bucket` method to `gcs_hook` andcreated corresponding operator`GoogleCloudStorageCreateBucket`- Added tests- Added documentationCloses #3044 from kaxil/AIRFLOW-1618,2
[AIRFLOW-2126] Add Bluecore to active usersCloses #3057 from JLDLaughlin/add_bluecore,1
[AIRFLOW-1852] Allow hostname to be overridable.This allows hostnames to be overridable tofacilitate service discoveryrequirements in common production deployments.Closes #3036 from thekashifmalik/hostnames,1
[AIRFLOW-2131] Remove confusing AirflowImport docs,2
"[AIRFLOW-2133] Remove references to GitHub issues in CONTRIBUTINGSince GitHub issues are now disabled for theproject, this guide shouldn't reference them…Closes #3060 from reidab/patch-1",0
[AIRFLOW-2134] Add Alan to the list of companies that use AirflowCloses #3041 from Charles-Go/patch-1,1
Merge pull request #3062 from reidab/AIRFLOW-2131,7
[AIRFLOW-XXX] Add timeout units (seconds)Timeout seconds according to paramiko SSHClientdocs: http://docs.paramiko.org/en/2.4/api/client.html#paramiko.client.SSHClient.connectCloses #3069 from miracodezu/master,2
"[AIRFLOW-2130] Add missing Operators to API Reference docs* Fix autodoc import path for BaseSensorOperator* Add autodoc imports for many core & contrib Operators that were missing* Rename ""Operator API"" section to ""Core Operators"" for better contrast with the following ""Community-contributed Operators"" section* Add subheadings to API Reference#Operators. Since all the Sensors were already alphabetized separately from the rest of the operators, this formalizes that distinction and moves all the Transfer operators to their own section as well.* Alphabetize Operator class names* Improve formatting in top-level Operators sectionThis also fixes the earlier and more narrowly scoped [AIRFLOW-951]",0
Merge pull request #3061 from reidab/AIRFLOW-2130,7
[AIRFLOW-XXX] Add Tile to the list of usersTile uses airflow to schedule and manages its ETL and Data Science jobs in production. We have been using airflow for almost a year now with thecommunity help.Closes #3072 from ranjanmanish/patch-1,1
[AIRFLOW-2122] Handle boolean values in sshHookThis allows passing boolean values in theextra field of ssh connectionCloses #3070 from sreenathkamath/AIRFLOW-2122,4
[AIRFLOW-1615] SSHHook: use port specified by ConnectionCloses #3056 from CaptainYANG/master,1
[AIRFLOW-2142] Include message on mkdir failureCloses #3068 from wrp/errstr,0
[AIRFLOW-2125] Using binary package psycopg2-binaryCloses #3055 from bern4rdelli/AIRFLOW-2125,1
[AIRFLOW-2139] Remove unncecessary boilerplate to get DataFrame using pandas_gbqCloses #3066 from mremes/improvement/cleanup-bigquery-hook,4
[AIRFLOW-2087] Scheduler Report shows incorrect Total task numberCloses #3074 from feng-tao/airflow-2087,5
[AIRFLOW-2146] Resolve issues with BQ using DbApiHook methods- Resolves issues with using methods like`get_records()` from `BigQueryHook` which isextended from `DbApiHook`.- Reverting one changed file fromhttps://github.com/apache/incubator-airflow/pull/3031 to use BigQuery Hook againinstead of `GoogleCloudBaseHook`- Fix `conn_type` of `bigquery_default` connectionCloses #3073 from kaxil/AIRFLOW-2146,0
"[AIRFLOW-2127] Keep loggers during DB migrationsPython's logging.config.fileConfig function will,by default,disable all existing loggers when it is called.The fileConfigfunction is used with default arguments by AirflowduringAlembic migrations and disables all loggers exceptthose fromalembic and sqlalchemy (this includes disablingAirflow's own loggers).This change sets the disable_existing_loggers flagof fileConfig toFalse so that it _does not_ disable any existingloggers, allowing themto continue working as normal.See more in https://issues.apache.org/jira/browse/AIRFLOW-2127.Closes #3059 from jiffyclub/AIRFLOW-2127/fileconfig-keep-loggers",5
[AIRFLOW-1035][AIRFLOW-1053] import unicode_literals to parse Unicode in HQLCloses #3053 from naoyak/AIRFLOW-1053,2
[AIRFLOW-2102] Add custom_args to Sendgrid personalizationsCloses #3035 from ms32035/sendgrid_personalization,1
"[AIRFLOW-2034] Fix mixup between %s and {} when using str.formatConvention is to use .format for string formating oustide logging, else use lazy formatSee comment in related issuehttps://github.com/apache/incubator-airflow/pull/2823/filesIdentified problematic case using following command line.git/COMMIT_EDITMSG:`grep -r '%s'./* | grep '\.format('`Closes #2976 from knil-sama/fix-mixup-format-str",0
[AIRFLOW-1551] Add operator to trigger Jenkins jobCloses #2553 from moe-nadal-ck/AIRFLOW-1551/AddJenkinsOperator,1
[AIRFLOW-2152] Add Multiply to list of companies using AirflowCloses #3080 from nrhvyc/master,1
[AIRFLOW-2097] tz referenced before assignmentUnboundLocalError: local variable 'tz' referenced before assignmentCloses #3076 from feng-tao/airflow-2097,0
[AIRFLOW-2151] Allow getting the session from AwsHookAdd new tests for get_credentials and get_sessionmethods of AwsHook Implement get_credentials andget_session methods in AwsHookCloses #3079 from inytar/aws-session,1
[AIRFLOW-2149] Add link to apache Beam documentation to create self executing JarCloses #3077 from lcaggio/master,1
[AIRFLOW-2161] Add Vevo to list of companies using AirflowCloses #3083 from csetiawan/add_vevo_to_companies,1
[AIRFLOW-2160] Fix bad rowid deserializationUse Flask-Admin's iterdecode to correctly decodetask instance rowidswhen they contain a dotCloses #3081 from cmlad/fix-bad-rowid-deserialization,0
[AIRFLOW-2132] Add step to initialize databaseCloses #3064 from kylehayes/patch-2,5
[AIRFLOW-2159] Fix a few typos in salesforce_hook,1
"[AIRFLOW-2059] taskinstance query is awful, un-indexed, and does not scaleCloses #3086 from feng-tao/airflow-2059",2
[AIRFLOW-2147] Plugin manager: added 'sensors' attributeAirflowPlugin required both BaseOperator and BaseSensorOperatorto be included in its `operators` attribute. Add a `sensors`attribute and updated import logic so that anything added tothe new attribute can be imported from `airflow.sensors.{plugin_name}`The integration/`make_module` calls in `airflow.plugins_manager`for operators is also updated to maintain the ability to importsensors from `operators` to avoid breaking existing plugins- Update unit tests and documentation to reflect this- Added exclusion for flake8 module level import not at top of fileCloses #3075 from arcward/AIRFLOW-2147,2
"[AIRFLOW-2065] Fix race-conditions when creating loggersIf two or more workers start at the same time,they will executethe same operations to create output directoriesfor storing the logfiles. It can lead to race-conditions when, forexample,  worker Acreate the directory right after the non-existancecheck done byworker B; worker B will also try to create thedirectory while it doesalready exist.Closes #3040 from blinkseb/fix-airflow-2065",2
[AIRFLOW-2163] Add HBC Digital to users of airflow,1
"docs: ""Implement Features"" section text changes",4
"[AIRFLOW-2166] Restore BQ run_query dialect paramRestores the use_legacy_sql parameter in the run_query method ofBigQueryBaseCursor.This method was removed by commitd5d2c01f37f345458d9eeb8cdfbb0e77b55eb7ea, which introduced abackward-incompatible change for direct calls to the cursormethods.Closes #3087 from ji-han/AIRFLOW-2166",4
Merge pull request #3089 from feluelle/master,7
[AIRFLOW-2171] Store delegated credentialsadd logic to google cloud base hook to make it able to createdelegated credentialsCloses #3090 from morgendave/AIRFLOW-2171-create-delegated-credentials,1
[AIRFLOW-2174] Fix typos and wrongly rendered documentsFix typos and wrongly rendered documents in thefollowing pages:- tutorial.html#default-arguments- configuration.html#connections- code.html#airflow.operators.hive_stats_operator.HiveStatsCollectionOperator- code.html#airflow.operators.redshift_to_s3_operator.RedshiftToS3Transfer- code.html#airflow.contrib.operators.dataflow_operator.DataFlowJavaOperator- code.html#airflow.contrib.operators.dataflow_operator.DataflowTemplateOperator- code.html#airflow.contrib.operators.dataproc_operator.DataprocWorkflowTemplateInstantiateOperator- code.html#airflow.contrib.operators.mlengine_operator.MLEngineModelOperator- code.html#airflow.contrib.operators.mlengine_operator.MLEngineVersionOperator- code.html#airflow.models.DAG.following_schedule- code.html#airflow.models.DAG.previous_scheduleCloses #3093 from sekikn/AIRFLOW-2174,2
[AIRFLOW-2129] Presto hook calls _parse_exception_message but defines _get_pretty_exception_messageCloses #3094 from feng-tao/airflow-2129,1
Merge pull request #3084 from tmccartan/master,7
[AIRFLOW-2123] Install CI dependencies from setup.pyInstall the dependencies from setup.py so we keep all the dependenciesin one single placeCloses #3054 from Fokko/fd-fix-ci-2,0
[AIRFLOW-2177] Add mock test for GCS Download op- Added mock test`GoogleCloudStorageDownloadOperatorTest` for`GoogleCloudStorageDownloadOperator`- Minor change in example for`GoogleCloudStorageCopyOperator`Closes #3097 from kaxil/gcs-tests,3
[AIRFLOW-2176] Change the way logging is carried out in BQ Get Data Operator- Changed the way logging is implemented in`BigQueryGetDataOperator`.  Changed `logging.info`to `self.log.info`Closes #3096 from kaxil/AIRFLOW-2176,5
[AIRFLOW-XXX] Add DocuTAP to list of usersCloses #3098 from cloneluke/add_docutap,2
[AIRFLOW-2168] Remote logging for Azure Blob Storageadd wasb_task_handler.py to enable remote loggingon Azure Blob Storage.add file and read capabilities to wasb_hook tosupport wasb_task_handlerCloses #3095 from marcusrehm/master,0
[AIRFLOW-2173] Don't check task IDs for concurrency reached checkCloses #3099 from aoen/ddavydov--ddavydov--remove_unecessary_in_clause_in_dep_check,4
[AIRFLOW-2175] Check that filepath is not NoneThis handles the case where the fileloc no longer exists in the database,5
Merge pull request #3104 from samschlegel/AIRFLOW-2175,7
[AIRFLOW-2187] Fix Broken Travis CI due to AIRFLOW-2123- Added the packages to ignore for Python 3,1
Merge pull request #3108 from kaxil/Airflow-2187,7
[AIRFLOW-2181] Convert password_auth and test_password_endpoints from DOS to UNIXCloses #3102 from dan-sf/AIRFLOW-2181,3
[AIRFLOW-2186] Change the way logging is carried out in few ops- Changed the way logging is implemented in`PostgresToGoogleCloudStorageOperator` and`HiveToDynamoDBTransferOperator`.  Changed`logging.info` to `self.log.info`Closes #3106 from kaxil/AIRFLOW-2186,5
"[AIRFLOW-2150] Use lighter call in HiveMetastoreHook().max_partition()Call self.metastore.get_partition_names() instead ofself.metastore.get_partitions(), which is extremely expensive forlarge tables, in HiveMetastoreHook().max_partition().Closes #3082 fromyrqls21/kevin_yang_fix_hive_max_partition",0
"[AIRFLOW-2197] Silence hostname_callable config error messageSince the hostname_callable key is not defined in the config, we endup with a lot of warnings. Add the key to the config and simplify thecode.",5
[AIRFLOW-2106] SalesForce hook sandbox optionCannot pass sandbox argument to sales_force hook preventing sandbox connection.Closes #3111 from feng-tao/airflow-2106,5
Add Bodastage and BTS-CE to current user listCloses #3110 from erssebaggala/master,1
[AIRFLOW-2191] Change scheduler heartbeat logs from info to debugCloses #3109 from johnarnold/johnar/log1,2
"[AIRFLOW-2199] Fix invalid reference to loggerThis should be log instead of logger. Somewhere itwent wrong withthe refactoring of the code.This patch had conflicts when merged, resolved byCommitter: Ash Berlin-Taylor<ash_github@firemirror.com>Closes #3114 from Fokko/fd-fix-logger",2
Merge pull request #3113 from Fokko/fd-fix-fqdn-errors,0
"[AIRFLOW-1588] Cast Variable value to stringVariables are considered a ""Key-Value"" pair, and usually the values areconsidered as strings (like when encrypting/decrypting with fernet key).Thus, we need to cast it to string before storing it (when loaded from aJSON).Closes #3037 from diraol/fix1588-json-parsing",5
"[AIRFLOW-2184][AIRFLOW-2138] Google Cloud Storage allow wildcards- closes #2184- Add support for wildcards to be provided insource object argumentThis allows the user of the Operator to provide awildcard in the format accepted by thedocumentation. This message is echoed in thedocstring for ease of use, and also because it isonly three sentences and adding a link is notrequired.- Add an argument move_object (bool) to theoperator that, when true runs a mv command asopposed to a cp command. That is to say, it movesan object instead of copying the object. This isespecially useful when this operator is used tomove objects in the same bucket, perhaps fromfolder to folder.- Add dotmodus and dannylee12 to companies usingairflowWe use airflow in almost all of our projects.-Unit tests written for the 3 use cases of theadded operator.Remove newlineSplit too long line over 2 lines.Closes #3067 from DannyLee12/master",1
[AIRFLOW-2169] Add schema to MySqlToGoogleCloudStorageOperator,1
Merge pull request #3091 from whynick1/master,7
[AIRFLOW-442] Add SFTPHookCloses #2487 from sdiazb/sftp_hook,1
"[AIRFLOW-102] Fix test_complex_template always succeedsExceptions from on_success_callback are stompedon, meaningtest_complex_template would never fail.Rather than follow the ""success = True"" pattern(which results in thelovely-to-track-down error message of`AssertionError: False is nottrue` I instead changed all these to have theverify method be execute-- meaning exceptions are propagated unchanged.Thanks to @sekikn inhttps://github.com/apache/incubator-airflow/pull/2375 for the info totrack down what was going on.Closes #2375Closes #3100 fromashb/template_tests_always_succeed",3
"[AIRFLOW-2204] Fix webserver debug modeThe command `airflow webserver -d` crashes because it triesto call the method run from the cached_app returned value,i.e. a DispatcherMiddleware instance. To run the flask appin debug mode (without gunicorn) it has to be createddirectly via create_app, that returns a Flask instance.Ref: https://issues.apache.org/jira/browse/AIRFLOW-2204",0
[AIRFLOW-XXX] Add Xero to list of usersCloses #3120 from yan9yu/master,1
[AIRFLOW-2140] Add Kubernetes scheduler to SparkSubmitOperatorCloses #3112 from RJKeevil/spark-k8s,1
[AIRFLOW-2206] Remove unsupported args from JdbcOperator docJdbcOperator's docstring has unsupported arguments anda wrongly named argument. This PR fixes them.,0
"[AIRFLOW-2207] Fix flaky test that uses app.cached_app()tests.www.test_views:TestMountPoint.test_mount changes base_urlthen calls airflow.www.app.cached_app().But if another test calls app.cached_app() first without changingbase_url, succeeding test_mount fails on Travis.So test_mount should clear cached app for itself in its setup methodso as to remount base_url forcefully.",1
Merge pull request #3123 from sekikn/AIRFLOW-2207,7
Merge pull request #3122 from sekikn/AIRFLOW-2206,7
Merge pull request #3118 from bbonagura9/master,7
[AIRFLOW-2205] Remove unsupported args from JdbcHook docJdbcHook's docstring has unsupported argumentsand unimplemented feature description.This PR fixes them and adds JdbcHook to the API reference.,5
Merge pull request #3121 from sekikn/AIRFLOW-2205,7
[AIRFLOW-2203] Store task ids as sets not listsMassively improve performance by using sets to represent a task'supstream and downstream task ids.,1
[AIRFLOW-2203] Cache static rules (trigger/weight)No need to recalculate them everytime just to see if they are valid,5
[AIRFLOW-2203] Speed up Operator ResourcesSet default values of Resources loaded from the configuration to prevent4x Config lookups for every task created.,1
"[AIRFLOW-2203] Cache signature in apply_defaultsCache inspect.signature for the wrapper closure to avoid calling it atevery decorated invocation. This is separate sig_cache created perdecoration, i.e. each function decorated using apply_defaults will havea different sig_cache.",1
[AIRFLOW-2203] Remove Useless Commands.self.tasks is a temp list gen from self.task_dict. no reason to appendto it,1
[AIRFLOW-2203] Defer cycle detectionMoved from adding_task to when dag is being bagged.This changes import dag runtime from polynomial to somewhat linear.Closes #3116 from wongwill86:dag_import_speed,2
[AIRFLOW-2183] Refactor DruidHook to enable sqlRefactor DruidHook to be able to issue druid sql query to druid brokerCloses #3105 from feng-tao/airflow-2183,0
"[AIRFLOW-2185] Use state instead of query paramBoth the Google OAuth2 and GHE authenticationplugins include the`next_url` as a query parameter in redirect_uri.This breaks at leastGoogle OAuth2, unless you include the queryparameter in theauthorized redirection URI. This isn't the mostflexible solution, as youwould have to do the same for every potential nextURL, and seems togo against the OAuth2 spec.Instead the next_url should be sent via the stateparameter which MUSTbe maintained by all spec compliant OAuth2implementations, and is notused when comparing redirection URIs.Closes #3103 from samschlegel/AIRFLOW-2185",1
[AIRFLOW-2215] Update celery task to preserve environment variables and improve logging on exceptionCloses #3126 from johnarnold/celery_env,2
[AIRFLOW-XXX] Update tutorial documentationCloses #3128 from Raniazy/master,2
[AIRFLOW-2220] Remove duplicate numeric list entry in security.rstThis duplicate entry was causing rst formattingissues in the securitysection of the documentation.Closes #3133 from dan-sf/AIRFLOW-2220,2
"[Airflow-2202] Add filter support in HiveMetastoreHook().max_partition()Adding back support for filter in max_partition()which could be used by some valid use cases. Itwill work for tables with multiple partitions,which is the behavior before (tho the doc statedit only works for single partitioned table). Thischange also kept the behavior when trying to getmax partition on a sub-partitioned table withoutsupplying filter--it will return the max partitionvalue of the partition key even it is not unique.Some extra checks are added to provide moremeaningful exception messages.Closes #3117 from yrqls21/kevin_yang_add_filter",1
[AIRFLOW-2225] Update document to include DruidDbApiHookCloses #3140 from feng-tao/airflow-2225,5
[AIRFLOW-2211] Rename hdfs_sensors.py to hdfs_sensor.py for consistencyCloses #3127 from feng-tao/airflow-2211,5
[AIRFLOW-2226] Rename google_cloud_storage_default to google_cloud_defaultThe Google cloud operators uses bothgoogle_cloud_storage_default andgoogle_cloud_default as a default conn_id. This isconfusing and thegoogle_cloud_storage_default conn_id isntinitialized by default in db.pyTherefore we rename thegoogle_cloud_storage_default togoogle_cloud_default for simplicity andconvenienceCloses #3141 from Fokko/airflow-2226,5
[AIRFLOW-2212] Fix ungenerated sensor API referenceSome community-contributed sensors are missingfrom API reference.This PR fixes docs/code.rst to refer to collectsensor classes.Closes #3125 from sekikn/AIRFLOW-2212,2
"[AIRFLOW-2124] Upload Python file to a bucket for DataprocIf the Python Dataproc file is on local storage,we want to uploadthis to google cloud storage before submitting itto the dataprocclusterCloses #3130 from Fokko/airflow-stash-files-on-gcs",2
[AIRFLOW-XXX] Fix chronological order for companies using Airflow- Merging https://github.com/apache/incubator-airflow/pull/3067 disturbed the chronologicalorder that was followed by the list of companiesusing Airflow. This PR is just fixing it.Closes #3135 from kaxil/patch-1,0
[airflow-2235] Fix wrong docstrings in two operators,1
"[AIRFLOW-1460] Allow restoration of REMOVED TI'sWhen a task instance exists in the database butits corresponding taskno longer exists in the DAG, the scheduler marksthe task instance asREMOVED. Once removed, task instances stayedremoved forever, even ifthe task were to be added back to the DAG.This change allows for the restoration of REMOVEDtask instances. If atask instance is in state REMOVED but thecorresponding task is presentin the DAG, restore the task instance by settingits state to NONE.A new unit test simulates the removal andrestoration of a task from aDAG and verifies that the task instance isrestored:`./run_unit_tests.sh tests.models:DagRunTest`JIRA:https://issues.apache.org/jira/browse/AIRFLOW-1460Closes #3137 from astahlman/airflow-1460-restore-tis",3
"[AIRFLOW-1235] Fix webserver's odd behaviourIn some cases, the gunicorn master shuts downbut the webserver monitor process doesn't.This PR add timeout functionality to shutdownall related processes in such cases.Dear Airflow maintainers,Please accept this PR. I understand that it willnot be reviewed until I have checked off all thesteps below!### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""[AIRFLOW-XXX] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-1235### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:In some cases, the gunicorn master shuts downbut the webserver monitor process doesn't.This PR add timeout functionality to shutdownall related processes in such cases.### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:tests.core:CliTests.test_cli_webserver_shutdown_when_gunicorn_master_is_killed### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""Closes #2330 from sekikn/AIRFLOW-1235",1
Merge pull request #3147 from feng-tao/airflow-2235,7
[AIRFLOW-1433][AIRFLOW-85] New Airflow Webserver UI with RBAC supportCloses #3015 from jgao54/rbac,1
[AIRFLOW-2248] Fix wrong param name in RedshiftToS3Transfer docThe parameter 'unload_options' is wrongly referred to as 'options'in the RedshiftToS3Transfer doc. This PR fixes it.,0
Merge pull request #3156 from sekikn/AIRFLOW-2248,7
[AIRFLOW-2060] Update pendulum version to 1.4.4This fixes a task clearing issue with deep copyCloses #3154 from cinhil/AIRFLOW-2060,0
[AIRFLOW-1206] TyposCloses #2294 from benrudolph/patch-1,2
"[AIRFLOW-2228] Enhancements in ValueCheckOperatorAllow ValueCheckOperator to accept a tolerance of1.Modify pass_value to be a template field,so that its value can be determined at runtime.Add tolerance value in airflow exception.This gives an idea about the allowed range forresultant records.Closes #3149 from sakshi2894/AIRFLOW-2228",1
[AIRFLOW-XXX] Add Qplum to Airflow usersCloses #3157 from manti/patch-1,1
[AIRFLOW-2249] Add side-loading support for Zendesk HookAdd side_loading parameter to ZendeskHook and pep8Write additional test for Zendesk side-loading andflake8Closes #3153 from theodoresiu/zendesk_sideloading,3
"[AIRFLOW-2247] Fix RedshiftToS3Transfer not to fail with ValueErrorRedshiftToS3Transfer callsS3Hook.get_credentials() and triesto take the return value as a tuple with twoelements but fails.This is because the return value isReadOnlyCredentials, whichshould be handled as a single object or a tuplewith tree elements.This PR fixes the above problem.Closes #3158 from sekikn/AIRFLOW-2247",0
[AIRFLOW-2244] bugfix: remove legacy LongText code from models.pyCloses #3151 from johnarnold/settings_guard,1
[AIRFLOW-2251] Add Thinknear as an Airflow userCloses #3155 from d3cay1/add-thinknear,1
[AIRFLOW-1430] Solve GPL dependencyOne of the dependencies was pulling ina GPL library by default. With the newrelease of python-nvd3 this is now solved.Closes #3160 from bolkedebruin/legal,1
[AIRFLOW-1430] Include INSTALL instructions to avoid GPL,2
[AIRFLOW-2258] Allow import of Parquet-format files into BigQueryCloses #3164 from stevesoundcloud/stevesoundcloud/allow-import-of-parquet-format-files-into-bigquery-AIRFLOW-2258,2
[AIRFLOW-2261] Check config/env for remote base log folderCloses #3166 from brandonwillard/add-remote-base-env-var,1
[AIRFLOW-2260][AIRFLOW-2260] SSHOperator add command template .sh filesCloses #3169 from nrhvyc/AIRFLOW-2260,2
[AIRFLOW-2264] Improve create_user cli help messageCloses #3168 from feng-tao/airflow-2264,1
[AIRFLOW-1729] improve dagBag timeCloses #3171 from q2w/master,2
"[AIRFLOW-2217] Add Slack webhook operatorAdd the Slack webhook hook/operator pair. Thisallows postingmessages to Slack in an easy, light-weight manner.Closes #3129 from danielvdende/AIRFLOW-2217-add-slack-webhook-operator",1
[AIRFLOW-2233] Update updating.md to include the info of hdfs_sensors renamingCloses #3145 from feng-tao/airflow-2233,5
"[AIRFLOW-2259] Dataflow Hook Index out of rangeBecause the api doesn't return the job, because itisn't known yetor because it cannot be found, it could be thatthe log lines arentavailable yet. Also use the locations based apicall, which returnsthe jobs immediately. This call seems to work forus-central, butdoesn't return any jobs for other regions.Closes #3165 from Fokko/AIRFLOW-2259",1
[AIRFLOW-2269] Add Custom Ink as an Airflow userCloses #3175 from mpeteuil/mpeteuil/add-custom-ink,1
[AIRFLOW-2274] Fix Dataflow testsCloses #3181 from Fokko/fd-fix-dataflow-test,5
[AIRFLOW-2253] Add Airflow CLI instrumentationCloses #3159 from jinhyukchang/cli-instrumentation,4
[AIRFLOW-2215] Pass environment to subproces.Popen in base_task_runnerCloses #3183 from johnarnold/env2,1
[AIRFLOW-2169] Fix type 'bytes' is not JSON serializable in python3,5
[AIRFLOW-2178] Add handling on SLA miss errorsCloses #3173 from d3cay1/airflow2178-master,0
[AIRFLOW-2200] Add snowflake operator with tests,3
[AIRFLOW-2282] Fix grammar in UPDATING.mdAlso remove trailing whitespace.,4
Merge pull request #3177 from whynick1/master,7
Merge pull request #3189 from tedmiston/updating-doc-grammar-fix,5
Merge pull request #3150 from devinXL8/AIRFLOW-2200,7
[AIRFLOW-2273] Add Discord webhook operator/hookAdd Discord webhook operator and hook to allow posting of messagesto a Discord channel via Discord incoming webhooks.,1
Merge pull request #3178 from TJBIII/discord_webhook_operator,1
Closes apache/incubator-airflow#2953 *No movement from submitter*,4
[AIRFLOW-2286] Add tokopedia to the readme,1
*Kick mirroring*,5
*Kick mirroring again*,5
[AIRFLOW-2287] Add license header to docs/Makefile,2
[AIRFLOW-XXX] Remove outdated migrations.sql,5
[AIRFLOW-XXX] Update PR template,5
[AIRFLOW-2292] Fix docstring for S3Hook.get_wildcard_keyFix a misleading description and wrong parameternamesin S3Hook.get_wildcard_key's docstring.Closes #3196 from sekikn/AIRFLOW-2292,2
[AIRFLOW-2298] Add Kalibrr to who uses airflowCloses #3194 from charlesverdad/patch-3,1
[AIRFLOW-2296] Add Cinimex DataLab to ReadmeCloses #3198 from r39132/add_cinemex_to_readme,1
Closes apache/incubator-airflow#3192 *Already Merged*,7
[AIRFLOW-2287] Update license noticesCloses #3195 from bolkedebruin/AIRFLOW-2287,5
[AIRFLOW-2284] GCS to S3 operatorCloses #3190 from NielsZeilemaker/gcp_to_s3,1
[AIRFLOW-2256] SparkOperator: Add Client Standalone mode and retry mechanismCloses #3163 from milanvdm/milanvdm/improve-spark-operator,1
[AIRFLOW-2027] Only trigger sleep in scheduler after all files have parsedCloses #2986 from aoen/ddavydov--open_source_disable_unecessary_sleep_in_scheduler_loop,2
[AIRFLOW-2281] Add support for Sendgrid categoriesCloses #3188 from ms32035/sendgrid_categories,1
[AIRFLOW-2305][AIRFLOW-2027] Fix CI failure caused by []Closes #3205 from sekikn/AIRFLOW-2305,1
[AIRFLOW-2306] Add Bonnier Broadcasting to list of current usersCloses #3206 from wileeam/add-bbr-to-readme,1
[AIRFLOW-2209] restore flask_login importsignore unused import with flake8Closes #3124 from berislavlopac/AIRFLOW-2209,2
[AIRFLOW-2303] Lists the keys inside an S3 bucketLists the keys matching a prefix and a delimiter inside an S3 bucketCloses #3203 from wileeam/s3-list-operator,1
[AIRFLOW-1340] Add S3 to Redshift transfer operatorCurrently RedshiftToS3Transfer (UNLOAD) exists butthe opposite doesn't.This PR adds COPY operation asS3ToRedshiftTransfer.Closes #3161 from sekikn/AIRFLOW-1340,1
[AIRFLOW-1633] docker_operator needs a way to set shm_sizeCloses #3199 from feng-tao/airflow_1633,1
[AIRFLOW-2304] Update quickstart doc to mention scheduler partCloses #3207 from feng-tao/airflow-2304,2
"[AIRFLOW-2162] When impersonating another user, pass env variables to sudo",4
Merge pull request #3184 from johnarnold/johnar/env3,7
"[AIRFLOW-1623] Trigger on_kill method in operatorson_kill methods were not triggered, due toprocessesnot being properly terminated. This was due to thefactthe runners use a shell which is then replaced bythechild pid, which is unknown to Airflow.Closes #3204 from bolkedebruin/AIRFLOW-1623",1
"[AIRFLOW-2312] Docs Typo Correction: Corresponding# JIRA[x] My PR addresses the following Airflow JIRAissues and references them in the PR title.*https://issues.apache.org/jira/browse/AIRFLOW-2312# Description[x] Here are some details about my PRMinor typo fix in the docs. `correpsonding` to`corresponding`# Tests[x] My PR adds the following unit tests OR doesnot need testing for this extremely good reason:I assume I don't need testing for docs# Commits[x] My commits all reference JIRA issues in theirsubject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""How to write a good git commit message"":1. Subject is separated from body by a blank line2. Subject is limited to 50 characters3. Subject does not end with a period4. Subject uses the imperative mood (""add"", not""adding"")5. Body wraps at 72 characters6. Body explains ""what"" and ""why"", not ""how""Closes #3211 from tamsanh/patch-1",1
"[AIRFLOW-2302] Add missing operators and hooksMany operators and hooks are not listed in theconfig, and thereforenot picked up by the docks generator.Closes #3201 from Fokko/airflow-2302-add-missing-docs",2
[AIRFLOW-1774] Allow consistent templating of arguments in MLEngineBatchPredictionOperatorFix a minor typo and a wrong non-defaultassignmentFix one more typoAdapt tests to new error messages and fix anothertypoFix exception type in utils operator test classImprove cleansing of non-valid training andprediciton job namesCloses #2746 from wileeam/ml-engine-prediction-job-normalization,5
[AIRFLOW-2291] Add optional params to ML EngineThis commit adds three extra optional parametersto the`MLEngineTrainingOperator` as well as a new unittest for the`MLEngineVersionOperator`Closes #3202 from dlebech/ml-engine-python-version,1
[AIRFLOW-1509][AIRFLOW-442] SFTP SensorCloses #3213 from sdiazb/sftp_sensor,5
[AIRFLOW-3212][AIRFLOW-2314] Remove only leading slash in GCS pathCloses #3212 from wileeam/fix-gcs-hook-helper-function,0
[AIRFLOW-2293] Fix S3FileTransformOperator to work with boto3S3FileTransformOperator doen't work for now sinceit uses a functionwhich is no longer supported by boto3. This PRreplaces it with avalid function and adds an unit test for thisoperator.Closes #3200 from sekikn/AIRFLOW-2293,1
[AIRFLOW-2301] Sync files of an S3 key with a GCS pathCloses #3216 from wileeam/s3-to-gcs-operator,1
[AIRFLOW-1325] Add ElasticSearch log handler and readerCloses #3214 fromyrqls21/kevin_yang_add_es_task_handler,0
[AIRFLOW-952] fix save empty extra field in UICloses #3222 from luckytaxi/AIRFLOW-952-fix-empty-extra-field,0
[AIRFLOW-XXX] Add Zego as an Apache Airflow userCloses #3224 from ruimffl/add-zego-as-user,1
[AIRFLOW-2287] Fix incorrect ASF headersCloses #3219 from bolkedebruin/fix_header,0
[AIRFLOW-610] Respect _cmd option in config before defaultsThe command versions of config parameters wereoverriden by thedefault config. E.g sql_alchemy_conn got thedefault value evenwhen sql_alchemy_conn_cmd was specified.Closes #3029 from cjgu/airflow-610,5
"[AIRFLOW-2254] Put header as first row in unloadCurrently, data is ordered by first column indescending orderHeader row comes as first only if the first columnis integerThis fix puts header as first row regardless offirst column data typeCloses #3180 from sathyaprakashg/AIRFLOW-2254",5
"[AIRFLOW-2299] Add S3 Select functionarity to S3FileTransformOperatorCurrently, S3FileTransformOperator downloads thewhole file from S3before transforming and uploading it. Addingextraction feature usingS3 Select to this operator improves its efficiencyand usablitily.Closes #3227 from sekikn/AIRFLOW-2299",1
[AIRFLOW-2184] Add druid_checker_operatorCloses #3228 from feng-tao/airflow-2184,1
"[AIRFLOW-2335] fix issue with jdk8 download for ciMake sure you have checked _all_ steps below.- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2335    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.- [x] Here are some details about my PR, includingscreenshots of any UI changes:There is an issue with travis pulling jdk8 that ispreventing CI jobs from running. This blocksfurther development of the project.Reference: https://github.com/travis-ci/travis-ci/issues/9512#issuecomment-382235301- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:This PR can't be unit tested since it is justconfiguration. However, the fact that unit testsrun successfully should show that it works.- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""- [ ] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.- [ ] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3236 from dimberman/AIRFLOW-2335_travis_issue",0
[AIRFLOW-2309] Fix duration calculation on TaskFailCloses #3208 from johnarnold/duration,0
[AIRFLOW-2240][DASK] Added TLS/SSL support for the dask-distributed scheduler.As of 0.17.0 dask distributed has support forTLS/SSL.[dask] Added TLS/SSL support for the dask-distributed scheduler.As of 0.17.0 dask distributed has support forTLS/SSL.Add a test for tls under dask distributedCloses #2683 from mariusvniekerk/dask-ssl,3
[AIRFLOW-2330] Do not append destination prefix if not givenCloses #3233 from berislavlopac/AIRFLOW-2330,0
[AIRFLOW-2346] Add Investorise as official user of AirflowCloses #3238 from svenvarkel/master,1
"[AIRFLOW-2347] Add Banco de Formaturas to ReadmeMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2347    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Add a company to the README### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason: N/A -- documentation update only### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3242 fromr39132/Add_banco_Formaturas_to_readme",1
closes apache/incubator-airflow#3225 *Closed for inactivity*,5
[AIRFLOW-2345] pip is not used in this setup.pyCloses #3241 from sinemetu1/patch-1,1
[AIRFLOW-2302] Fix documentationCloses #3226 from sekikn/AIRFLOW-2302,2
closes apache/incubator-airflow#3187 *Closed for inactivity*,5
[AIRFLOW-2350] Fix grammar in UPDATING.mdCloses #3248 from r39132/patch-1,5
[AIRFLOW-2328] Fix empty GCS blob in S3ToGoogleCloudStorageOperatorCloses #3231 from wileeam/fix-empty-blob-in-s3-to-gcs-operator,0
[AIRFLOW-2326][AIRFLOW-2222] remove contrib.gcs_copy_operatorCloses #3232 from berislavlopac/AIRFLOW-2326,1
[AIRFLOW=1314] Basic Kubernetes Mode,5
[AIRFLOW-1314] Add support for volume mounts & Secrets in KubernetesExecutor,1
[AIRFLOW-1314] Git Mode to pull in DAGs for Kubernetes Executor,2
[AIRFLOW-1314] Create integration testing environment,3
"[AIRFLOW-1314] Use VolumeClaim for transporting DAGs- fix issue where watcher process randomly dies- fixed alembic head, was pointing to two tips",0
"[AIRFLOW-1314] Improve k8s supportAdd kubernetes config section in airflow.cfg and Inject GCP secrets upon executor start. (#17)Update Airflow to Pass configuration to k8s containers, add some Py3 … (#9)* Update Airflow to Pass configuration to k8s containers, add some Py3 compat., create git-sync pod* Undo changes to display-source config setter for to_dict* WIP Secrets and Configmaps* Improve secrets support for multiple secrets. Add support for registry secrets. Add support for RBAC service accounts.* Swap order of variables, overlooked very basic issue* Secret env var names must be upper* Update logging* Revert spothero test code in setup.py* WIP Fix tests* Worker should be using local executor* Consolidate worker setup and address code review comments* reconfigure airflow script to use new secrets method",1
[AIRFLOW-1314] Add executor_config and tests* Added in executor_config to the task_instance table and the base_operator table* Fix test; bump up number of examples* Fix up comments from PR* Exclude the kubernetes example dag from a test* Fix dict -> KubernetesExecutorConfig* fixed up executor_config comment and type hint,5
[AIRFLOW-1314] Small cleanup to address PR comments (#24)* Small cleanup to address PR comments* Remove use of enum* Change back to 3.4,4
[AIRFLOW-1314] Rebasing against master,2
[AIRFLOW-1999] Add per-task GCP service account support,1
[AIRFLOW-1314] Improve error handlingHandle too old resource versions and throw exceptions on errors- K8s API errors will now throw Airflow exceptions- Add scheduler uuid to worker pod labels to match the two,1
[AIRFLOW-1314] Polish some of the Kubernetes docs/config,5
[AIRFLOW-1314] Cleanup the configCloses #2414 from bloomberg:airflow-kubernetes-executor,5
"[AIRFLOW-2300] Add S3 Select functionarity to S3ToHiveTransferTo improve efficiency and usability, this PR addsS3 Select functionarity to S3ToHiveTransfer.It also contains some minor fixes for documentsand comments.Closes #3243 from sekikn/AIRFLOW-2300",2
"[AIRFLOW-2344] Fix `connections -l` to work with pipe/redirect`airflow connections -l` uses 'tabulate' packagewithfancy_grid format, which outputs box drawingcharacters.It can occur UnicodeEncodeError with pipe orredirect,since the default encoding for Python 2.x isascii.This PR fixes it and contains some flask8 relatedfixes.Closes #3244 from sekikn/AIRFLOW-2344",0
"[AIRFLOW-2270] Handle removed tasks in backfillFix issue with backfill jobs of dags, where tasksin theremoved state are not run but still considered tobe pending,causing an indefinite loop.Closes #3176 from ji-han/AIRFLOW-2270_dag_backfill_removed_tasks",4
[AIRFLOW-1433] Set default rbac to initdb05e1861e24de42f9a2c649cd93041c5c744504e1 breaksthe api forany program directly importing airflow.utils.This setsa reasonable default.Closes #3240 from wrp/initdb,5
"[AIRFLOW-2351] Check for valid default_args start_dateA bug existed when default_args did containstart_datebut it was set to None, failing to instantiate theDAG.Closes #3256 from bolkedebruin/AIRFLOW-2351",2
[AIRFLOW-766] Skip conn.commit() when in Auto-commit,5
[AIRFLOW-2357] Add persistent volume for the logsThe logs are kept inside of the worker pod. Byattaching a persistentdisk we keep the logs and make them available forthe webserver.- Remove the requirements.txt since we dont wantto maintain another  dependency file- Fix some small casing stuff- Removed some unused code- Add missing shebang lines- Started on some docs- Fixed the loggingCloses #3252 from Fokko/airflow-2357-pd-for-logs,2
[AIRFLOW-2364] Warn when setting autocommit on a connection which does not support it,1
[AIRFLOW-775] Fix autocommit settings with Jdbc hook,1
"[AIRFLOW-1153] Allow HiveOperators to take hiveconfsHiveOperator can only replace variables via jinjaand the replacementsare global to the dag through the context anduser_defined_macros.It would be much more flexible to open uphive_conf to the HiveOperatorlevel so hive scripts can be recycled at the tasklevel, leveragingHiveHook already existing hive_conf param and_prepare_hiveconffunction.Closes #3136 from wolfier/AIRFLOW-1153",5
[AIRFLOW-2208][Airflow-22208] Link to same DagRun graph from TaskInstance viewAllow graph view to accept blank execution_dateand pass itthrough when it's available.Closes #3132 from iansuvak/persistent_graph,4
"[AIRFLOW-2234] Enable insert_rows for PrestoHookPrestoHook.insert_rows() raisesNotImplementedError for now.But Presto 0.126+ allows specifying column namesin INSERT queries,so we can leverage DbApiHook.insert_rows() almostas is.This PR enables this function.Closes #3146 from sekikn/AIRFLOW-2234",1
Merge pull request #3257 from artwr/awiedmer-fix-issue-with-jdbc-autocommit,5
[AIRFLOW-1652] Push DatabricksRunSubmitOperator metadata into XCOM[AIRFLOW-1652] Push DatabricksRunSubmitOperatormetadata into XCOMPush run_id and run_page_url into xcom socallbacks and othertasks can reference this informationaddress commentsCloses #2641 from andrewmchen/databricks-xcom,5
Bump version,5
Add incubating,1
"[AIRFLOW-2068] MesosExecutor allows optional Docker imageIn its current form, MesosExecutor schedules taskson mesos slaves whichjust contain airflow commands assuming that themesos slaves alreadyhave airflow installed and configured on them.This assumption goesagainst the Mesos philosophy of having aheterogeneous cluster.Since Mesos provides an option to pull a Dockerimage before actuallyrunning the actual task/command so thisimprovement changes themesos_executor.py to specify an optional dockerimage containingairflow which can be pulled on slaves beforerunning the actualairflow command. This also opens the door for anoptimization ofresources in a future PR, by allowing thespecification of CPU andmemory needed for each airflow task.Closes #3008 from agrajm/AIRFLOW-2068",1
[AIRFLOW-2365] Fix autocommit attribute checkCloses #3258 from artwr/awiedmer-fix-dbapi-test-issue,5
[AIRFLOW-2369] Fix gcs testsThe version was hardcoded and would break if youupdate the versionof Apache AirflowCloses #3260 from Fokko/airflow-2369-fix-tests,3
[AIRFLOW-74] SubdagOperators can consume all celeryd worker processesCloses #3251 from feng-tao/airflow-74,1
"[AIRFLOW-2041] Correct Syntax in python examplesI parsed it with the ol' eyeball compiler. Someonecould flake8 it better, perhaps.Changes: - correct `def` syntax on line 50 - use literal dict on line 67Closes #2479 from 0atman/patch-1",1
[AIRFLOW-2336] Use hmsclient in hive_hookThe package hmsclient is Python2/3 compatible andoffer a handy contextmanager to handle opening and closing connections.Closes #3239 from gglanzani/AIRFLOW-2336,0
[AIRFLOW-XXX] Remove wheelhouse files from travis not owned by travis,2
[AIRFLOW-2042] Fix browser menu appearing over the autocomplete menuCloses #2984 from fox/fix-autocomplete,0
[AIRFLOW-1781] Make search case-insensitive in LDAP groupCloses #2750 from patsak/f/ldap_search,1
[AIRFLOW-1835] Update docs: Variable file is jsonSearching through all the documentation I couldn'tfind anywherethat explained what file format it expected foruploading settings.Closes #2802 from bovard/variable_files_are_json,5
closes apache/incubator-airflow#3032 *Closed for inactivity*,5
[AIRFLOW-2331] Support init action timeout on dataproc cluster createCloses #3235 from piffall/master,1
[AIRFLOW-2377] Improve Sendgrid sender supportCloses #3266 from ms32035/sendgrid_sender,1
[AIRFLOW-2380] Add support for environment variables in Spark submit operator.Closes #3268 from piffall/master,1
[AIRFLOW-2382] Fix wrong description for delimiterFix misleading descriptions for the 'delimiter'parameter in S3ListOperator andS3ToGoogleCloudStorageOperator's docstring.Closes #3270 from sekikn/AIRFLOW-2382,2
[AIRFLOW-2378] Add Groupon to list of current usersCloses #3267 from stevencasey/add_groupon,1
"[AIRFLOW-2381] Fix the flaky ApiPasswordTests testThis test is in conflict with different testsrunning in parallelBy calling a simple overview page, the behaviourof checking thepassword is still checked, but isn't dependent ona specific dagbeing present in the databaseCloses #3269 from Fokko/AIRFLOW-2381",5
closes apache/incubator-airflow#1933 *Closed for inactivity*,5
closes apache/incubator-airflow#2586 *Closed for inactivity*,5
closes apache/incubator-airflow#2827 *Closed for inactivity*,5
[AIRFLOW-2391] Fix to Flask 0.12.2Flask 0.12.3 has issues with Airflow and needs tobe fixed.Therefore lock the version to 0.12.2.Closes #3277 from Fokko/fd-fix-master-ci,0
[AIRFLOW-2348] Strip path prefix from the destination_object when source_object contains a wildcard[]Closes #3247 from csoulios/master,0
[AIRFLOW-2370] Implement --use_random_password in create_userCloses #3262 from wrp/passwords,4
[AIRFLOW-2266][AIRFLOW-2343] Remove google-cloud-dataflow dependencyThis is caused due to the fact that the latestrelease (2.4) for apache-beam[gcp] is notavailable for Python 3.x. Also as we are usingGoogle's discovery based API for all google cloudrelated commands we don't require to importgoogle-cloud-dataflow packageCloses #3273 from kaxil/patch-3,5
closes apache/incubator-airflow#3276 *Messed up PR - hundreds of old commits.*,5
[AIRFLOW-1575] Add AWS Kinesis Firehose Hook for inserting batch recordsCloses #3275 from sid88in/feature/kinesis_hookv2,1
closes apache/incubator-airflow#2744 *Closed for inactivity.*,5
closes apache/incubator-airflow#2555 *Fixed by another PR.*,0
closes apache/incubator-airflow#2555 *Fixed by another PR.*,0
closes apache/incubator-airflow#3209 *PR in heavy need of squashing and cleanup.*,4
[AIRFLOW-1313] Add vertica_to_mysql operatorCloses #2370 from juise/master,1
[AIRFLOW-1960] Add support for secrets in kubernetes operatorCloses #3271 from ese/secrets-kubernetes-operator,1
closes apache/incubator-airflow#2930 *Fix belongs in SQLAlchemy.*,0
[AIRFLOW-1933] Fix some typosCloses #2474 from Philippus/patch-1,2
closes apache/incubator-airflow#2539 *Closed for inactivity*,5
closes apache/incubator-airflow#2047 *Closed for inactivity*,5
closes apache/incubator-airflow#2970 *Closed for inactivity*,5
[AIRFLOW-2390] Resolve FlaskWTFDeprecationWarning- Replace soon to be deprecated `flask_wtf.Form`class with `from flask_wtf import FlaskForm`Closes #3278 from kaxil/AIRFLOW-2390,2
[AIRFLOW-2389] Create a pinot db api hookCloses #3274 from feng-tao/pinot_db_hook,5
"Revert ""[AIRFLOW-2391] Fix to Flask 0.12.2""This reverts commit 3368f4258c2dcfbcdbaf631fa887a742f12720b8.",5
[AIRFLOW-2363] Fix return type bug in TaskHandlerCloses #3259 fromyrqls21/kevin_yang_fix_s3_logging,2
[AIRFLOW-2398] Add BounceX to list of current airflow usersCloses #3282 from JoshFerge/master,1
[AIRFLOW-1313] Fix license headerCloses #3281 from juise/master,0
[AIRFLOW-2403] Fix License HeadersCloses #3285 from r39132/fix_license_headers,0
closes apache/incubator-airflow#2814 *Messed up PR - hundreds of old commits.*,5
[AIRFLOW-2401] Document the use of variables in Jinja templateCloses #2847 from moe-nadal-ck/patch-1,1
"[AIRFLOW-1853] Show only the desired number of runs in tree viewPreviously, the ""Number of runs"" option was notbeing respected for DAGs that were externallytriggered. Now, only the set number of runs isshown regardless of DAG trigger type.Also adjust www_rbacCloses #3288 fromAetherUnbound/feature/AIRFLOW-1853",1
[AIRFLOW-XXX] Add Twine Labs as an Airflow userCloses #3287 from ivorpeles/twine_airflow,1
[AIRFLOW-2400] Add Ability to set Environment Variables for K8s[AIRFLOW-2400] Env Variables for K8sAllow environment variables to be set for theKubernetesPodOperator.Fix typoFix documentation variable typeCloses #3284 from jkao/add-env-vars-to-k8s-operator,1
closes apache/incubator-airflow#2337 *No longer a bug*,0
[AIRFLOW-2404] Add additional documentation for unqueued taskCloses #3286 from AetherUnbound/feature/task-not-queued-doc,2
[AIRFLOW-2406] Add Apache2 License Shield to ReadmeCloses #3290 fromr39132/add_apache2_license_shield_to_readme,1
[AIRFLOW-2394] default cmds and arguments in kubernetes operatorCommands aand arguments to docker image in kubernetes operatorCloses #3289 from ese/k8soperator,1
[AIRFLOW-2410][AIRFLOW-75] Set the timezone in the RBAC Web UISqlAlchemy does not know how to handle thetimestamp since it isnttimezone awareCloses #3303 from Fokko/AIRFLOW-2410,0
[AIRFLOW-2409] Supply password as a parameterSupply the password as a parameter on the cliCloses #3304 from Fokko/supply-password,4
[AIRFLOW-XXX] Fix wrong table header in scheduler.rstCloses #3306 from sekikn/table_header,0
[AIRFLOW-XXX] Add Reddit to Airflow usersCloses #3309 from seato/reddit_airflow,1
"[AIRFLOW-2411] add dataproc_jars to templated_fieldsThis commit makes it possible to use jinjatemplates when passingJAR file URIs to the DataProc operators thatrequire JAR files,specifically the DataProc Hive, Pig, SparkSql,Spark, Hadoop andPySpark operators.Closes #3305 from mchalek/template-dataproc-jars",5
[AIRFLOW-2313] Add TTL parameters for DataprocThree additional optional parameters toDataprocClusterCreateOperatorwhich configure Cloud Dataproc Cluster ScheduledDeletion features.Closes #3217 from ebartkus/dataproc-ttl,5
closes apache/incubator-airflow#2962 *Closed for inactivity.*,5
closes apache/incubator-airflow#2675 *Closed for inactivity.*,5
"[AIRFLOW-1812] Update logging exampleThe logging has changed, therefore we should alsoupdate theupdating.md guideCloses #2784 from Fokko/AIRFLOW-1812-update-logging-example",5
closes apache/incubator-airflow#2478 *Closed for inactivity.*,5
[AIRFLOW-1899] Fix Kubernetes tests[AIRFLOW-1899] Add full deployment- Made home directory configurable- Documentation fix- Add licenses[AIRFLOW-1899] Tests for the Kubernetes ExecutorAdd an integration test for the Kubernetesexecutor. Done byspinning up different versions of kubernetes andrun a DAGby invoking the REST APICloses #3301 from Fokko/fix-kubernetes-executor,0
closes apache/incubator-airflow#2923 *Closed in favor of newer PR*,1
[AIRFLOW-XXX] Update README.md with Craig@WorkCloses #3311 from allanjsx/patch-1,1
"[AIRFLOW-1914] Add other charset support to email utilsThe built-in email utils does not supportmultibyte string content, for example,Japanese or emojis. The fix is to addmime_charset parameter to allow for othervalues such as `utf-8`.Closes #3308 from wolfier/AIRFLOW-1914",1
[AIRFLOW-2417] Wait for pod is not running to end taskCloses #3312 from ese/kubernetes-operator,1
[AIRFLOW-2418] Bump Flask-WTFFlask-appbuilder needs at least 0.14.2Closes #3313 from Fokko/AIRFLOW-2418,5
[AIRFLOW-2426] Add Google Cloud Storage Hook tests- Added mock tests for methods in`GoogleCloudStorageHook`.Closes #3322 from kaxil/AIRFLOW-2426,1
[AIRFLOW-2222] Implement GoogleCloudStorageHook.rewriteCloses #3264 from berislavlopac/AIRFLOW-2222,1
closes apache/incubator-airflow#3237 *Closed for inactivity.*,5
"[AIRFLOW-1952] Add the navigation bar color parameterENH: Add the navigation bar color parameterWhen operating multiple Airflow's (eg. Production,Staging, etc.),we cannot distinguish which Airflow is.This feature enables us to distinguish whichAirflow you seeby the color of navigation bar.Merge branch 'master' into add-navigation-bar-color-parameterCloses #2903 from Licht-T/add-navigation-bar-color-parameter",2
closes apache/incubator-airflow#2772 *Closed for inactivity.*,5
closes apache/incubator-airflow#2769 *Closed for inactivity.*,5
closes apache/incubator-airflow#2770 *Closed for inactivity.*,5
closes apache/incubator-airflow#2768 *Closed for inactivity.*,5
[AIRFLOW-2407] Resolve Python undefined namesCloses #3307 from cclauss/AIRFLOW-2407,0
[AIRFLOW-2431] Add the navigation bar color parameter for RBAC UICloses #3326 from jgao54/navbar-color,2
"Revert ""[AIRFLOW-2335] fix issue with jdk8 download for ci""This reverts commit 0f8507ae351787e086d1d1038f6f0ba52e6d9aaa.The JDK artifact has been fixed and the hotfix is no longer needed",0
[AIRFLOW-2412] Fix HiveCliHook.load_file to address HIVE-10541HiveCliHook.load_file doesn't actually executeLOAD DATA statement via beeline bundled withHive under 2.0 due to HIVE-10541.This PR provides a workaround for this problem.Closes #3327 from sekikn/AIRFLOW-2412,0
[AIRFLOW-2427] Add tests to named hive sensorCloses #3323 from gglanzani/AIRFLOW-2427,3
[AIRFLOW-1978] Add WinRM windows operator and hookCloses #3316 from cloneluke/winrm_connector2,1
"[AIRFLOW-XXX] Add Quantopian to list of Airflow usersWe've been using it in production for a few monthsnow, and we love it! :)Closes #3331 from Eronarn/patch-1",1
[AIRFLOW-2437] Add PubNub to list of current airflow usersCloses #3332 from jzucker2/add-pubnub,1
"[AIRFLOW-2086][AIRFLOW-2393] Customize default dagrun number in tree viewCloses #3279 from feng-tao/reduce-tree-viewThis introduces a new configuration variable to set the defaultnumber of dag runs displayed in the tree view. For large DAGs, thiscould cause timeouts in the webserver.",1
closes apache/incubator-airflow#3325 *Need to be reopened and follow contributor guidelines*,5
[AIRFLOW-2445] Allow templating in kubernetes operatorCloses #3338 from ese/k8s-templating,1
[AIRFLOW-2447] Fix TestHiveMetastoreHook to run all casesTestHiveMetastoreHook has a method which namedoesn't start with test_. This PR renames itto be executed.Closes #3341 from sekikn/AIRFLOW-2447,3
[AIRFLOW-2444] Remove unused option(include_adhoc) in cli backfill commandCloses #3337 from feng-tao/remove-unused-flag,4
[AIRFLOW-2436] Remove cli_logger in initdbCloses #3330 from jinhyukchang/master,4
[AIRFLOW-2358][AIRFLOW-201804] Make the Kubernetes example optionalIf you havent installed Kubernetes the initdboperation will failCloses #3315 from Fokko/AIRFLOW-2358,0
[AIRFLOW-2441] Fix bugs in HiveCliHook.load_dfThis PR fixes HiveCliHook.load_df to:1. encode delimiter with the specified encoding   before passing it to pandas.DataFrame.to_csv   so as not to fail2. flush output file by pandas.DataFrame.to_csv   before executing LOAD DATA statement3. remove header and row index from output file   by pandas.DataFrame.to_csv so as to read it   as expected via HiveCloses #3334 from sekikn/AIRFLOW-2441,5
[AIRFLOW-2424] Add dagrun status endpoint and increased k8s test coverage[AIRFLOW-2424] Add dagrun status endpoint andincrease k8s test coverage[AIRFLOW-2424] Added minikube fixes by @kimoonkim[AIRFLOW-2424] modify endpoint to remove 'status'Closes #3320 from dimberman/add-kubernetes-test,3
[AIRFLOW-2449] Fix operators.py to run all test casesMySqlTest and PostgresTest intests/operators/operators.pyhave some unexecuted tests due to wrong namingconvention.This PR fixes it.Closes #3344 from sekikn/AIRFLOW-2449,0
"[AIRFLOW-2446] Add S3ToRedshiftTransfer into the ""Integration"" docThis PR adds an undocumented AWS-related operatorinto the ""Integration"" section and fixes someobsolete description.Closes #3340 from sekikn/AIRFLOW-2446",0
add BelugaDB to Airflow usersCloses #3339 from lucianoviola/add-belugadb-to-company-list,5
"[AIRFLOW-2016] assign template_fields for Dataproc Workflow Templatesub-classes, not base classCloses #3346 from mchalek/mchalek-fix-inline-workflow-template-operator",0
[AIRFLOW-2442][AIRFLOW-2] Airflow run command leaves database connections openCloses #3336 from afernandez/afernandez_db,5
[AIRFLOW-2333] Add Segment Hook and TrackEventOperatorAdd support for Segment with an accompanying hookand anoperator for sending track eventsCloses #3335 from jzucker2/add-segment-support,1
[AIRFLOW-2450] update supported k8s versions to 1.9 and 1.10Closes #3345 from dimberman/upgrade-k8s,1
[AIRFLOW-2454][Airflow 2454] Support imagePullPolicy for k8sCloses #3348 from jkao/k8s-operator-image-pull-policy,1
[AIRFLOW-2457] Update FAB version requirementCloses #3349 from jgao54/update-fab-version,5
[AIRFLOW-XXX] Add spotahome in user listCloses #3353 from ese/spotahome,1
[AIRFLOW-2169] Encode binary data with base64 before importing to BigQueryCloses #3221 from whynick1/master,2
[AIRFLOW-2396] Add support for resources in kubernetes operator[AIRFLOW-2396] Add support for resources inkubernetes operator[AIRFLOW-2396] Add support for resources inkubernetes operatorCloses #3352 from ese/resources,1
[AIRFLOW-2453] Add default nil value for kubernetes/git_subpathCloses #3350 from ese/gitsubpath,1
[AIRFLOW-2430] Extend query batching to additional slow queriesCloses #3324 from gsilk/batch-inserts,1
[AIRFLOW-2425] Add lineage supportAdd lineage support by having inlets and ouletsthatare made available to dependent upstream ordownstreamtasks.If configured to do so can send lineage data to abackend. Apache Atlas is supported out of the box.Closes #3321 from bolkedebruin/lineage_exp,1
[AIRFLOW-2376] Fix no hive section errorCloses #3343 from feng-tao/airflow-2376,0
[AIRFLOW-2461] Add support for cluster scaling on dataproc operatorCloses #3357 from piffall/master,1
[AIRFLOW-2451] Remove extra slash ('/') char when using wildcard in gcs_to_gcs operatorCloses #3355 from berislavlopac/AIRFLOW-2451,1
[AIRFLOW-2435] Add launch_type to ECSOperator to allow FARGATECloses #3329 from ThomasVdBerge/ecs-fargate,5
[AIRFLOW-XXX] Updated contributors listCloses #3358 from kaxil/patch-3,5
"[AIRFLOW-2110][AIRFLOW-2122] Enhance Http Hook- Use a header in passed in the ""extra"" argument and  add tenacity retry- Fix the tests with proper mockingCloses #3071 from albertocalderari/master",3
[AIRFLOW-2460] Users can now use volume mounts and volumesWhen launching pods using k8s operatorCloses #3356 from dimberman/k8s-mounts,1
[AIRFLOW-1929] Modifying TriggerDagRunOperator to accept execution_dateCloses #3246 from sreenathkamath/AIRFLOW-1929,5
[AIRFLOW-2465] Fix wrong module names in the docCloses #3359 from sekikn/AIRFLOW-2465,2
[AIRFLOW-2213] Add Quoble check operatorCloses #3300 from sakshi2894/AIRFLOW-2213,1
Closes apache/incubator-airflow#2997 *Obsolete PR*,5
Closes apache/incubator-airflow#2484 *Obsolete PR*,5
Add AdBOOST in user listCloses #3362 from matejkloska/patch-1,1
closes apache/incubator-airflow#2472 *Obsolete PR.*,5
closes apache/incubator-airflow#2318 *Obsolete PR.*,5
closes apache/incubator-airflow#2291 *Obsolete PR.*,5
"[AIRFLOW-2420] Azure Data Lake HookAdd AzureDataLakeHook as a first step to enableAirflow connect toAzure Data Lake.The hook has a simple interface to upload anddownload files with allparameters available in Azure Data Lake sdk andalso a check_for_fileto query if a file exists in data lake.[AIRFLOW-2420] Add functionality for Azure DataLakeMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW-2420) issues and references them in the PR title.    -https://issues.apache.org/jira/browse/AIRFLOW-2420### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:       This PR creates Azure Data Lake hook(adl_hook.AdlHook) and all the setup required tocreate a new Azure Data Lake connection.### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:       Adds tests to airflow.hooks.adl_hook.py intests.hooks.test_adl_hook.py### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3333 from marcusrehm/master",4
"[AIRFLOW-2452] Document field_dict must be OrderedDictHiveCliHook.load_file has a parameter calledfield_dict, which defines name-type pairsfor columns, must be OrderedDict so as tokeep columns' order, but it's undocumented.This PR adds an note about that, and fixesHiveCliHook.load_df function which callsload_file internally.Closes #3347 from sekikn/AIRFLOW-2452",2
"[AIRFLOW-XXX] Fix order of companiesMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    - https://issues.apache.org/jira/browse/AIRFLOW-XXX    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Fix the alphabetic order of companies in theREADME### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason: N/A -- documentation only### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3363 from r39132/fix_order_of_company_list",0
[AIRFLOW-2467][AIRFLOW-2] Update import direct warn message to use the module nameCloses #3361 from dan-sf/AIRFLOW-2467,1
[AIRFLOW-48] Parse connection uri querystringCloses #3292 from inytar/extras-in-uri,5
[AIRFLOW-2474] Only import snakebite if using py2Closes #3365 from jgao54/snakebite-import,2
[AIRFLOW-2477] Improve time units for task duration and landing times charts for RBAC UI,2
Deleting specific namesDeleting specific names because I no longer workat Xoom.Closes #3372 from gepser/master,1
Merge pull request #3368 from feng-tao/airflow-2477,7
[ARIFLOW-2458] Add cassandra-to-gcs operatorCloses #3354 from jgao54/cassandra-to-gcs,1
[AIRFLOW-2484] Remove duplicate key in MySQL to GCS Op- Remove duplicate `FIELD_TYPE.INT24: 'INTEGER'`key from MySQL to GCS OperatorCloses #3376 from kaxil/AIRFLOW-2484,1
[AIRFLOW-2491] Resolve flask version conflictCloses #3381 from kaxil/travis-ci-fix-flask-conflict,5
[AIRFLOW-2429] Make Airflow flake8 compliantCloses #3342 from feng-tao/airflow-2429,1
[AIRFLOW-2486] Remove unnecessary slash after port`self.base_url` includes an unnecessary slash when`conn.port` is specified.This often leads to unintended redirects that areespecially problematic when a request body isneeded.Closes #3379 from jason-udacity/AIRFLOW-2486,0
[AIRFLOW-2485] Fix Incorrect logging for Qubole Sensor- Replace incorrect `this.log` keyword for loggingof Qubole Sensor to `self.log`I am sure https://github.com/apache/incubator-airflow/pull/3297#issuecomment-385988083 wassuppose to mean `self.log.info` instead of`this.log.info`.Closes #3378 from kaxil/AIRFLOW-2485,5
[AIRFLOW-2479] Improve doc FAQ sectionCloses #3373 from feng-tao/airflow-2478,2
"[AIRFLOW-2481] Fix flaky Kubernetes testThe test does not have enough time to finish.Therefore simplifythe dag a bit to let the test execute morequickly, and give it abit more time.Closes #3371 from Fokko/AIRFLOW-2481-flaky-kub",3
[AIRFLOW-2482] Add test for rewrite method in GCS Hook- Added mocking test for rewrite method for GCShookCloses #3374 from kaxil/gcs-hook-test-rewrite,3
"[AIRFLOW-2397] Support affinity policies for Kubernetes executor/operatorKubernetesPodOperator now accept a dict typeparameter called ""affinity"", which represents agroup of affinity scheduling rules (nodeAffinity,podAffinity, podAntiAffinity).API reference: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#affinity-v1-coreCloses #3369 from imroc/AIRFLOW-2397",5
[AIRFLOW-2487] Enhance druid ingestion hookCloses #3380 from feng-tao/aiflow-2487,1
"[AIRFLOW-2448] Enhance HiveCliHook.load_df to work with datetimeHiveCliHook.load_df can not handle DataFramewhich contains datetime for now.This PR enhances it to work with datetime,fixes some bug introduced by AIRFLOW-2441,and addresses some flake8 issues.Closes #3364 from sekikn/AIRFLOW-2448",0
[AIRFLOW-2489] Update FlaskAppBuilder to 1.11.1This will bring Airflow back up to date and willallow us to runFlask 0.12.4Closes #3382 from Fokko/AIRFLOW-2489-update-dependencies,5
"[AIRFLOW-2493] Mark template_fields of all Operators in the API document as ""templated""Make all the ""template_fields"" (jinjia template)of all Operators marked as ""templated"" in the APIdocument.Closes #3386 from imroc/AIRFLOW-2493",2
"[AIRFLOW-2429] Fix api, bin, config_templates folders flake8 errorCloses #3391 from feng-tao/flake8_p2",0
[AIRFLOW-2495] Update celery to 4.1.1This also updates the kombu dependency to >=4.2.0.This allows broker_url with sqlalchemy urls towork.Closes #3388 from rodrigc/AIRFLOW-2495,1
Added user in readmeCloses #3387 from harishbisht/master,1
"[AIRFLOW-2471] Fix HiveCliHook.load_df to use unused parametersThis PR fixes HiveCliHook.load_df to passload_file the parameter called create andrecreate, which are currently ignored, aspart of kwargs.Closes #3390 from sekikn/AIRFLOW-2471",1
[AIRFLOW-2429] Fix contrib folder's flake8 errorsFix contrib/ folder's flake8 errorsCloses #3394 from kaxil/AIRFLOW-2429,0
"[AIRFLOW-2501] Refer to devel instructions in docs contrib guideWithout the devel extra, the docs do not build.The build fails due tomissing the mock package.Closes #3395 from tswast/airflow-2501-docs-contributing",2
[AIRFLOW-2503] Fix broken links in CONTRIBUTING.md- Fix broken links in `CONTRIBUTING.md`Closes #3397 from kaxil/AIRFLOW-2503,2
[AIRFLOW-2502] Change Single triple quotes to double for docstrings- Changed single triple quotes to double quotecharacters to be consistent with the docstringconvention in PEP 257Closes #3396 from kaxil/AIRFLOW-2502,2
"[AIRFLOW-2429] Fix dag, example_dags, executors flake8 errorCloses #3398 from feng-tao/flake8_p3",0
[AIRFLOW-2429] Add BaseExecutor backCloses #3401 from feng-tao/quick_fix_airflow_2429,0
"[AIRFLOW-2509] Separate config docs into how-to guidesAlso moves how-to style instructions for loggingfrom ""integration"" pageto a ""Writing Logs"" how-to.Closes #3400 from tswast/howto",2
"[AIRFLOW-2498] Fix Unexpected argument in SFTP Sensor- The SFTP sensor is using SFTP hook and passing`sftp_conn_id` to `sftp_conn_id` parameter whichdoesn't exist. The solution would be to remove theparameter name, hence defaulting to firstparameter which in this case would be`ftp_conn_id`Closes #3392 from kaxil/AIRFLOW-2498",2
[AIRFLOW-2419] Use default view for subdag operatorCloses #3314 from milanvdm/milanvdm/subdag_view,2
[AIRFLOW-2472] Implement MySqlHook.bulk_dumpImplement MySqlHook.bulk_dump since the oppositeoperation bulk_load is already implemented.This PR also addresses some flake8 warnings.Closes #3385 from sekikn/AIRFLOW-2472,2
closes apache/incubator-airflow#3403 *Obsolete PR.*,5
[AIRFLOW-2473] Fix wrong skip condition for TransferTestsThis PR fixes wrong @skipUnlessImported whichdecoratesTransferTests and does minor refactoring.Closes #3411 from sekikn/AIRFLOW-2473,4
"[AIRFLOW-2415] Make airflow DAG templating render numbersCurrently, if you have an operator with a templatefields argument, that is a dictionary, e.g.:template_fields = ([dict_args])And you populate that dictionary with a field thatan integer in a DAG, e.g.:...dict_args = {'ds': '{{ ds }}', num_times: 5}...Then ariflow will give you the following error:{base_task_runner.py:95} INFO - Subtask:airflow.exceptions.AirflowException: Type '<type'int'>' used for parameter 'dict_args[num_times]'is not supported for templatingThis fix aims to resolves that issue byimmediately resolving numbers without attemptingto template themCloses #3410 fromArgentFalcon/support_numeric_template_fields",1
[AIRFLOW-1057][AIRFLOW-1380][AIRFLOW-2362][2362] AIRFLOW Update DockerOperator to new APIupdate import to docker's new API version >=2.0.0changed dependency for docker package; now dockerrather than docker-pyupdated test cases to align to new docker classCloses #3407 from Noremac201/fixer,0
[AIRFLOW-2107] add time_partitioning to run_query on BigQueryBaseCursorCloses #3043 from marengaz/query_time_part,1
"[AIRFLOW-2520] CLI - make backfill less verboseUsed backfill recently and it would log a shit tonof logging messagestelling me all the tasks that were not ready torun at every tick.These messages are not useful and should be mutedby default.I understand that this may be helpful in thecontext of `airflow run`in the context where dependencies aren't met, sodecided to managea flag instead of simply going `logging.debug` onit.Closes #3414 frommistercrunch/backfill_less_verbose",1
[AIRFLOW-1472] Fix SLA misses triggering on skipped tasks.Closes #3370 from milliburn/airflow-1472-master,0
[AIRFLOW-2518] Fix broken ToC links in integration.rstCloses #3412 from sekikn/AIRFLOW-2518,2
[AIRFLOW-1730] Unpickle value of XCom queried from DB,5
[AIRFLOW-2510] Introduce new macros: prev_ds and next_dsCloses #3418 from milton0825/introduce-next_ds-prev_ds,1
"[AIRFLOW-2523] Add how-to for managing GCP connectionsI'd like to have how-to guides for all connectiontypes, or at least thedifferent categories of connection types. I foundit difficult to figureout how to manage a GCP connection, this commitadd a how-to guide forthis.Also, since creating and editing connectionsreally aren't all thatdifferent, the PR renames the ""creatingconnections"" how-to to ""managingconnections"".Closes #3419 from tswast/howto",1
[AIRFLOW-2515] Add dependency on thrift_sasl to hive extraThis PR adds a dependency on thrift_sasl to hiveextraso that HiveServer2Hook.get_conn() works.Closes #3408 from sekikn/AIRFLOW-2515,1
Merge pull request #2701 from mrkm4ntr/airflow-1730,7
"[AIRFLOW-2525] Fix PostgresHook.copy_expert to work with ""COPY FROM""For now PostgresHook.copy_expert supports""COPY TO"" but not ""COPY FROM"", because itopens a file with write mode and doesn'tcommit operations. This PR fixes it byopening a file with read and write modeand committing operations at last.",2
Merge pull request #3421 from sekikn/AIRFLOW-2525,7
[Airflow-XXX] add Prime to company listCloses #3424 from davideberdin/master,1
"[AIRFLOW-2429] Fix hook, macros folder flake8 errorCloses #3420 from feng-tao/flake8_p4",0
[AIRFLOW-2521] backfill - make variable name and logging messages more acurate[AIRFLOW-2521] backfill - make variable name andlogging messages more accurateThe term kicked_off in logging and the variablestarted are used torefer to `running` task instances. Let's clarifythe variable names andmessages here.Fixing unit testsCloses #3416 from mistercrunch/kicked_off_running,1
[AIRFLOW-1499] Eliminate duplicate and unneeded codeCloses #2509 from bananarepublic/airflow-1499,5
[AIRFLOW-2530] KubernetesOperator supports multiple clustersCloses #3425 from mrkm4ntr/airflow-2530,1
[AIRFLOW-2536] docs about how to deal with airflow initdb failureAdd docs to faq.rst to talk about how to deal withException: Global variableexplicit_defaults_for_timestamp needs to be on (1)for mysqlCloses #3429 from milton0825/fix-docs,2
[AIRFLOW-XXX] Add M4U to user listCloses #3426 frommsantino/AIRFLOW-2437-add_m4u_to_users_list,1
[AIRFLOW-2402] Fix RBAC task logCloses #3319 from yrqls21/kevin_yang_fix_rbac_view,0
"[AIRFLOW-2519] Fix CeleryExecutor with SQLAlchemyWhen using a CeleryExecutor with SQLAlchemyspecified in broker_url, such as:broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflowdo not pass invalid options to the sqlalchemybackend. - In default_airflow.cfg, comment outvisibility_timeout from   [celery_broker_transport_options].  The user canspecify the   correct values in this section for the celerybroker transport   that they choose.  visibility_timeout is onlyvalid   for Redis and SQS celery brokers. - Move ssl options from[celery_broker_transport_options] where   they were wrongly placed, into the [celery]section where they   belong.Closes #3417 from rodrigc/AIRFLOW-2519",0
[AIRFLOW-2466] consider task_id in _change_state_for_tis_without_dagrunCloses #3360 from gwax/AF2466,4
[AIRFLOW-2532] Support logs_volume_subpath for KubernetesExecutorThe kubernetes section in the configuration filesupportslogs_volume_subpath for KubernetesExecutor.Closes #3430 from imroc/AIRFLOW-2532,2
"[AIRFLOW-2517] backfill support passing key values through CLI### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2517    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:In backfill, we can provide key-value pairsthrough CLI and those pairs can be accessedthrough macros. This is just like the way`trigger_dag -c` works [1].Let's walk through an example.In the airflow CLI we specify a key-value pair.```airflow backfill hello_world -s 2018-02-01 -e2018-02-08 -c '{""text"": ""some text""}'```In the DAG file, I have a `BashOperator` thatcontains a template command and I want{{ dag_run.conf.text }} resolves to the text Ipassed in CLI.```pythontemplated_command = """"""    echo ""ds = {{ ds }}""    echo ""prev_ds = {{macros.datetime.strftime(prev_execution_date,""%Y-%m-%d"") }}""    echo ""next_ds = {{macros.datetime.strftime(next_execution_date,""%Y-%m-%d"") }}""    echo ""text_through_conf = {{ dag_run.conf.text }}""""""""bash_operator = BashOperator(    task_id='bash_task',    bash_command=templated_command,    dag=dag    )```Rendered Bash command in Airflow UI.<img width=""1246"" alt=""screen shot 2018-05-22 at 433 59 pm"" src=""https://user-images.githubusercontent.com/6065051/40395666-04c41574-5dde-11e8-9ec2-c0312b7203e6.png"">[1]https://airflow.apache.org/cli.html#trigger_dag### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3406 from milton0825/backfill-support-conf",5
[AIRFLOW-2529] Improve graph view performance and usabilityLimit number of dag runs shown in drop down. Add base dateand number of runs widgets known from other views whichallows kind of paging through all dag runs.,1
closes apache/incubator-airflow#3310 *Fixed in another PR.*,0
"[AIRFLOW-2538] Update faq doc on how to reduce airflow scheduler latencyMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2538    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Update the faq doc on how to reduce airflowscheduler latency. This comes from our internalproduction setting which also aligns with Maxime'semail(https://lists.apache.org/thread.html/%3CCAHEEp7WFAivyMJZ0N+0Zd1T3nvfyCJRudL3XSRLM4utSigR3dQmail.gmail.com%3E).### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [ ] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [ ] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [ ] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3434 from feng-tao/update_faq",5
[AIRFLOW-2547] Describe how to run tests using DockerFor Airflow it can be a bit tricky to test thecode. Add a descriptionon how to build a clean enviroment from scratchCloses #3445 from Fokko/fd-local-devel-environment,4
[AIRFLOW-XXX] Add Yieldr to who is using airflowCloses #3448 from ggeorgiadis/master,1
"[AIRFLOW-2544][AIRFLOW-1967] Guard against next major release of Celery, FlowerCloses #3439 from tedmiston/celery-flower-guard",2
"[AIRFLOW-2526] dag_run.conf can override paramsMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2526    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:params can be overridden by the dictionary passedthrough `airflow backfill -c````templated_command = """"""    echo ""text = {{ params.text }}""""""""bash_operator = BashOperator(    task_id='bash_task',    bash_command=templated_command,    dag=dag,    params= {        ""text"" : ""normal processing""    })```In daily processing it prints:```normal processing```In backfill processing `airflow trigger_dag -c""{""text"": ""override success""}""`, it prints```override success```### Tests- [ ] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3422 from milton0825/params-overridden-through-cli",2
[AIRFLOW-2537] Add reset-dagrun option to backfill commandCloses #3444 from feng-tao/add_reset_dagrun_for_backfill,2
[AIRFLOW-2551] Encode binary data with base64 standard rather than base64 url,5
Merge pull request #3441 from seelmann/AIRFLOW-2529-graph-view-dag-runs,2
Merge pull request #3449 from whynick1/master,7
[AIRFLOW-2504] Log username correctly and add extra to search columnsCloses #3438 from youngyjd/log-username-old-ui,2
closes apache/incubator-airflow#3328 *Closed for inactivity.*,5
[AIRFLOW-1863][AIRFLOW-2529] Add dag run selection widgets to gantt viewAdd same widgets to filter and select dag runknown from graph viewto the gantt chart view. Extract common code tohandle requestparameters and DB query.Closes #3450 from seelmann/AIRFLOW-1863-gantt-view,5
[AIRFLOW-2553] Add webserver.pid to .gitignoreCloses #3451 from r39132/update_gitignore,5
[AIRFLOW-2525] Fix a bug introduced by commit dabf1b9The previous commit on this issue (#3421)introduced a new bug on COPY FROM operation.This PR fixes it by opening a file with 'r+'mode instead of 'w+' to avoid truncating it.Closes #3423 from sekikn/AIRFLOW-2525-2,1
[AIRFLOW-2462] Change PasswordUser setter to correct syntaxCloses #3415 from Noremac201/setterFix,0
"[AIRFLOW-2500] Fix MySqlToHiveTransfer to transfer unsigned type properlyMySQL supports unsigned data types, but Hivedoesn't.So if MySqlToHiveTransfer maps MySQL's data typestoHive's corresponding ones directly (e.g. INT ->INT),unsigned values over signed type's upper boundtransferred from MySQL are interpreted as invalidby Hive, and users get NULL.To avoid it, this PR fixes MySqlToHiveTransferto map MySQL data types to Hive's wider ones(e.g. SMALLINT -> INT, INT -> BIGINT, etc.).Closes #3446 from sekikn/AIRFLOW-2500",5
"[AIRFLOW-2545] Eliminate DeprecationWarningDo not import BaseOperator as KubernetesOperator.This eliminates confusing DeprecationWarnings whensetting up a default airflow install whereadditionalkubernetes modules are not installed.Also, use LoggingMixin instead of logger module.Closes #3442 from rodrigc/AIRFLOW-2545",2
[AIRFLOW-2557] Fix pagination for s3Paged tests for s3 are taking over 120 seconds.Thereis functionality to set the page size. Thisreducesthe time spent on tests.Closes #3455 from bolkedebruin/AIRFLOW-2557,3
[AIRFLOW-2513] Change `bql` to `sql` for BigQuery Hooks & Ops- Change `bql` to `sql` for BigQuery Hooks &Operators for consistencyCloses #3454 from kaxil/consistent-bq-lang,1
[AIRFLOW-XXX] Fix doc typosCloses #3459 from ccayg-sainsburys/master,2
[AIRFLOW-2558] Clear task/dag is clearing all executionsCloses #3465 from feng-tao/airflow_2588_new,1
[AIRFLOW-83] add mongo hook and operatorCloses #3440 fromandscoop/AIRFLOW_83_add_mongo_hooks_and_operators,1
closes apache/incubator-airflow#3399 *Obsolete PR*,5
closes apache/incubator-airflow#1477 *Obsolete and Outdated PR*,5
closes apache/incubator-airflow#1699 *Closed for inactivity*,5
[AIRFLOW-2565] templatize cluster_labelMake cluster_label in QuboleOperator a templatedfield.Closes #3463 from milton0825/make-cluster-label-template-field,1
[AIRFLOW-2560] Adding support for internalIpOnly to DataprocClusterCreateOperatorCloses #3458 from piffall/master,5
[AIRFLOW-2573] Cast BigQuery TIMESTAMP field to float,5
[AIRFLOW-2561] Fix typo in EmailOperatorThere's a typo in the params in send_email.It should be mime_charset instead of mine_charset.Closes #3468 from wolfier/AIRFLOW-2561,1
[AIRFLOW-XXX] Typo fixCloses #3474 from prabeesh/patch-1,0
Merge pull request #3471 from whynick1/master,7
"[AIRFLOW-1021] Fix double login for new users with LDAPBy adding the new user to the session and commitbefore merging, the correct user is returned by`load_user(userid)` instead of `None` due to no`id` assoicated with that user.Closes #2778 from wolfier/AIRFLOW-1021",1
[AIRFLOW-2566] Change backfill to rerun failed tasksCloses #3464 from feng-tao/airflow-2566,0
"[AIRFLOW-437] Send TI context in kill zombiesFix to provide proper TI context while calling ti.handle_failure duringkill_zombies, as without the context handler_failure is of no use andits equivalent of marking those TIs as failed directly.This patch had conflicts when merged, resolved byCommitter: Ash Berlin-Taylor<ash_github@firemirror.com>Closes #1796 from msumit/AIRFLOW-437-2",0
[AIRFLOW-2575] Make gcs to gcs operator work with large filesUse `GoogleCloudStorageHook.rewrite` instead of`copy` so that itworks with files > 5TBCloses #3472 from torkjel/AIRFLOW-2575-gcs-to-gcs-operator-support-large-files,2
[AIRFLOW-2578] Add option to use proxies in JiraHookCloses #3480 from arashrai/master,1
closes apache/incubator-airflow#3409 *Closed for inactivity*,5
"[AIRFLOW-2581] RFLOW-2581] Fix DbApiHook autocommitDbApiHook.run actually do not commit when I set toautocommit=TrueFor example: hook.run(sql,autocommit=True)This commit fixed this problem.Closes #3482 from imroc/AIRFLOW-2581",0
[AIRFLOW-2533] Fix path to DAG's on kubernetes executor workersCloses #3470 frombnutt/brian/kubernetes_executor_fix_path_to_dags,2
"[AIRFLOW-59] Implement bulk_dump and bulk_load for the Postgres hookThis PR implements bulk_dump and bulk_load,which are inherited from DbApiHook andalready implemented for MySqlHook.Closes #3456 from sekikn/AIRFLOW-59",1
"[AIRFLOW-2591][AIRFLOW-2581] Set default value of autocommit to False in DbApiHook.run()In previous PR (AIRFLOW-2581), I changed thedefault value  ofautocommit to True in DbApiHook.run(), which maycause some performanceissues, so I changed it back to False.Closes #3487 from imroc/AIRFLOW-2591",4
[AIRFLOW-2587] Add TIMESTAMP type mapping to MySqlToHiveTransferThis PR fixes MySqlToHiveTransfer to mapMySQL TIMESTAMP to Hive TIMESTAMP(not STRING) so that users are notrequired to cast it explicitly.Closes #3486 from sekikn/AIRFLOW-2587,1
[AIRFLOW-1115] fix github oauth api URLCloses #3469 from renzofrigato/airflow_1115,0
[AIRFLOW-2590] Fix commit in DbApiHook.run() for no-autocommit DBCloses #3485 from yrqls21/kevin_yang_fix_dbapi,5
[AIRFLOW-2597] Restore original dbapi.run() behaviorCloses #3490 from yrqls21/kevin_yang_fix_ci,0
[AIRFLOW-2585] Fix several bugs in CassandraHook and CassandraToGCSOperatorCloses #3483 from jgao54/fix-cassandra-hook-bug,0
[AIRFLOW-2429] Fix operators folder flake8 errorCloses #3481 from feng-tao/flake8_p6,0
"[AIRFLOW-2512][AIRFLOW-2522] Use google-auth instead of oauth2client* Updates the GCP hooks to use the google-authlibrary and removes  dependencies on the deprecated oauth2clientpackage.* Removes inconsistent handling of the scopeparameter for different  auth methods.Note: using google-auth for credentials requires anewer version of thegoogle-api-python-client package, so this commitalso updates theminimum version for that.To avoid some annoying warnings about thediscovery cache not beingsupported, so disable the discovery cacheexplicitly as recommend here:https://stackoverflow.com/a/44518587/101923Tested by running:    noseteststests/contrib/operators/test_dataflow_operator.py\        tests/contrib/operators/test_gcs*.py \        tests/contrib/operators/test_mlengine_*.py \        tests/contrib/operators/test_pubsub_operator.py \        tests/contrib/hooks/test_gcp*.py \        tests/contrib/hooks/test_gcs_hook.py \        tests/contrib/hooks/test_bigquery_hook.pyand also tested by running some GCP-related DAGslocally, such as theDataproc DAG example athttps://cloud.google.com/composer/docs/quickstartCloses #3488 from tswast/google-auth",2
[AIRFLOW-2615] Remove not used app creation,1
[AIRFLOW-2550] Implements API endpoint to list DAG runsCloses #3499 from verdan/AIRFLOW-2550-list-dagruns,2
[AIRFLOW-2429] Fix security/task/sensors/ti_deps folders flake8 errorCloses #3501 from feng-tao/flake8_p7,0
[AIRFLOW-2617] add imagePullPolicy config for kubernetes executorCloses #3500 from Cplo/k8sexecutor,5
[AIRFLOW-1656] Tree view dags query changed[AIRFLOW-1656] Tree view query changesCloses #3427 from djo10/master,4
[AIRFLOW-2539][AIRFLOW-2359] Move remaing log config to configuration fileCloses #3435 fromNielsZeilemaker/env_logging_filename,2
[AIRFLOW-2605] Fix autocommit for MySqlHookCloses #3493 fromyrqls21/kevin_yang_fix_mysql_autocommit,0
[AIRFLOW-2586] Stop getting AIRFLOW_HOME value from config file in bash operatorCloses #3484 fromyrqls21/kevin_yang_fix_bash_operator,0
"[AIRFLOW-2534] Fix bug in HiveServer2HookThis commit also adds numerous tests forHiveServer2 and switchesImpyla for PyHive (0.6.0), making HiveServer2Python 2 compatible.Closes #3432 from gglanzani/AIRFLOW-2534",1
[AIRFLOW-2630] Fix classname in test_sql_sensor.pyCloses #3511 from sekikn/AIRFLOW-2630,3
"[AIRFLOW-2562] Add Google Kubernetes Engine OperatorsAdd Google Kubernetes Engine create_cluster,delete_cluster operatorsThis allows users to use airflow to create ordelete clusters in thegoogle cloud platformCloses #3477 from Noremac201/gke_create",1
"[AIRFLOW-2611] Fix wrong dag volume mount path for kubernetes executorThere are two way of syncing dags, pvc and git-sync, if set both(dags_volume_claim and git_subpath), I won't getthe mountPath what Iwant. I think the priority of pvc should higherthan git-sync, so Ithink when dags_volume_claim is been set, themountPath shuold not join the git_subpath.Closes #3497 from imroc/AIRFLOW-2611",1
[AIRFLOW-2634][AIRFLOW-2534] Remove dependency for impylaCloses #3514 from sekikn/AIRFLOW-2634,4
[AIRFLOW-2627] Add a sensor for CassandraCloses #3510 from sekikn/AIRFLOW-2627,1
"[AIRFLOW-2613] Fix Airflow searching .zip bugWhen Airflow was populating a DagBag from a .zipfile, if a singlefile in the root directory did not contain thestrings 'airflow' and'DAG' it would ignore the entire .zip file.Also added a small amount of logging to notbombard user with infoabout skipping their .py files.Closes #3505 from Noremac201/dag_name",2
"[AIRFLOW-2355] Airflow trigger tag parameters in subdagParameters passed through airflow trigger_dag -c'{""text"": ""blah""}' can be accessed through{{ dag_run.conf.text }} in subdag.Closes #3460 from milton0825/trigger-dag-subdag",2
[AIRFLOW-1786] Enforce correct behavior for soft-fail sensorsSoft-fail sensor failure causes skip of alldownstream tasks. It also enables ability to setup non-blocking and soft-fail sensors in the sameway as for regular sensors.Closes #3509 from artem-kirillov/AIRFLOW-1786,0
[AIRFLOW-2559] Azure Fileshare hookCloses #3457 from NielsZeilemaker/fileshare_hook,2
[AIRFLOW-2601] Allow user to specify k8s configCloses #3491 from Noremac201/config_kube,5
"[AIRFLOW-2606] Fix DB schema and SQLAlchemy model* Add test that verifies that database schema and SQLAlchemy model are in sync* Add exception for users.password that doesn't exist in model and tables created by other tests* Add migration script to merge the two heads* Add migration script to fix not-null constrains for MySQL that were lost by 0e2a74e0fc9f_add_time_zone_awareness* Add migration script to fix FK constraint for existing SQLite DBs* Enable ForeignKey support for SQLite, otherwise 2e82aab8ef20_rename_user_table won't change FK in chart and known_event tables",2
closes apache/incubator-airflow#3507 *Closed for inactivity.*,5
[AIRFLOW-XXX] Adding REA Group to readme,1
[AIRFLOW-2567] Extract result from the kubernetes pod as XcomCloses #3466 from mrkm4ntr/airflow-2567,4
[AIRFLOW-2542][AIRFLOW-1790] Rename AWS Batch Operator queue to job_queue- Improved the retries times to jobs below 60s- Renamed property queue to job_queue to preventAWS Batch and CeleryExecutor queue conflict- Added Breaking Chain note for the UPDATING.mdmaster- Fixed operator infinit loop- Added documentation warning about the Breakingchain- Fixed the commit parameter to keep it on Airflowguidelines- Fixed logging typo- rebased with masterChanges to be committed:modified:   ../../../UPDATING.mdmodified:   awsbatch_operator.pymodified:   ../../../tests/contrib/operators/test_awsbatch_operator.pyCloses #3436 from hprudent/master,3
"[AIRFLOW-2638] dbapi_hook: support REPLACE INTOSometimes, it's desirable to be able to useREPLACE INTO instead ofINSERT INTO for insert_rows method (if importingthe same data multipletimes).This adds an optional parameter to the insert_rowscolumn that flips thegenerated sql statement from ""INSERT INTO"" to""REPLACE INTO"".Closes #3517 from flokli/dbapi_hook-replace",5
[AIRFLOW-2607] Fix failing TestLocalClientCloses #3494 from verdan/fix-api-tests,3
[AIRFLOW-2608] Implements/Standardize custom exceptions for experimental APIsImplements/Standardize custom exceptions forexperimental APIsImplements/Standardize custom exceptions forexperimental APIsCloses #3496 from verdan/AIRFLOW-2608-api-exceptions-handling,5
"[AIRFLOW-2612][AIRFLOW-2534] Clean up Hive-related testsThere is several HiveServer2-related testswhich are not run by default intests/operators/hive_operator.py,but AIRFLOW-2534 implemented almost all of themin airflow/hooks/hive_hooks.py.This PR removes duplicated tests fromtests/operators/hive_operator.py andmoves an unimplemented test toairflow/hooks/hive_hooks.py with a bit of fix.Closes #3498 from sekikn/AIRFLOW-2612",0
"[AIRFLOW-2646] Fix setup.py not to install snakebite on Python3setup.py has a logic to avoid installingsnakebite on Python3, but it doesn't work.This is because the variable devel_all isnow a tuple of lists. This PR fixesthat variable to be a flat list andmakes the logic work as expected.Closes #3522 from sekikn/AIRFLOW-2646",1
[AIRFLOW-2592] Bump bleach dependencyBleach dependency is updated to 2.1.3 to addressCVE-2018-7753Closes #3524 from ctrebing/AIRFLOW-2592-bump-bleach-dependency,1
[AIRFLOW-1919] Add option to query for DAG runs given a DAG IDCloses #3515 from feng-tao/airflow-1919,2
"[AIRFLOW-2640] Add Cassandra table sensorJust like a partition sensor for Hive,this PR adds a sensor that waits fora table to be created in Cassandra cluster.Closes #3518 from sekikn/AIRFLOW-2640",1
"[AIRFLOW-2645][AIRFLOW-2617] Add worker_container_image_pull_policySet worker_container_image_pull_policy in default_airflow.cfgAs AIRFLOW-2617 added worker_container_image_pull_policy configto the section of kubernetes, but the airflow_default.cfgwas not updated, this PR add worker_container_image_pull_policyto default_airflow.cfg.Closes #3521 from imroc/AIRFLOW-2645",5
[AIRFLOW-2654] Fix incorret URL on refresh in Graph View of FAB UI- Fix incorrect URL requested on pressing the refresh button in the graph view.,0
Merge pull request #3516 from seelmann/AIRFLOW-2606-db-schema-model,5
[AIRFLOW-2602] Show failed attempts in Gantt view* Get failed attempts from `task_fail` table and show them in Gantt view.* Fix TaskFail class column definition to match actual database schema.  Change was required to retrieve all records for one task instance run  otherwise only one was returned by SQLAlchemy.,1
Merge pull request #3527 from kaxil/AIRFLOW-2654,7
Merge pull request #3492 from seelmann/AIRFLOW-2602-show-failed-attempts-in-gantt-view,0
"[AIRFLOW-2652] implement / enhance baseOperator deepcopyMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2652    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:When running ``airflow backfill`` onpythonOperator, it will do / trigger a deepcopy ofthe task_instance. If some objects can't bedeepcopy in certain python version(e.g Protobuf inpython 2.7) , an exception will be thrown. Weshould just do a shallow copy instead of deep copyfor the object.The pr here is to copy the ``_deepcopy__`` methodin BaseOperator, but skip doing deepcopy for`op_kwargs` and `python_callable`.### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:I can't think of a good way to test. We encounterthis in our production.### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.    - When adding new operators/hooks/sensors, theautoclass documentation generation needs to beadded.### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3528 from feng-tao/airflow-2652",4
[AIRFLOW-2663] Add instructions to install SSH dependenciesCloses #3536 from kaxil/patch-1,1
"[AIRFLOW-2661] fix config dags_volume_subpath and logs_volume_subpathMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2661    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes:Changes the use of `log_volume_subpath` and`dags_volume_subpath` which arenow passed into the construction of the workerpod's volumeMounts instead ofthe volume section (where subPath is not valid).### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Unit tests have been added but I'm not sure how toadd integration testsfor this without breaking the other minikube tests### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""### Documentation- [x] In case of new functionality, my PR addsdocumentation that describes how to use it.No new functionality added### Code Quality- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3537 from r4vi/AIRFLOW-2661",4
Merge pull request #3506 from yrqls21/kevin_yang_remove_extra_step,4
[AIRFLOW-2604] Add index to task_fail,0
[AIRFLOW-2624] Fix webserver login as anonymous,2
"Revert ""[AIRFLOW-2615] Remove not used app creation""This reverts commit 9525003ff929020b553c3a9ed7aff121d83a2bd6.",4
[AIRFLOW-2657] Add ability to delete dag from web UICloses #3531 from Noremac201/master,2
[AIRFLOW-XXX] Fix typo in http_operator.pyReplace sensor by operator in the comment:`:param http_conn_id: The connection to run thesensor against` by `:param http_conn_id: Theconnection to run the operator against`Closes #3544 from thibaultclem/patch-1,1
[AIRFLOW-2662][AIRFLOW-2397] Add k8s node_selectors and affinityAdd the ability to set the node selection and the affinityfor the k8s executorCloses #3535 from Cplo/affinity,5
[AIRFLOW-2669] Fix bug when setting logs_volume_subpathCloses #3543 from Cplo/subpath,2
[AIRFLOW-2648] Update mapred job name in HiveOperatorCloses #3534 fromyrqls21/keivn_yang_reorder_mapred,1
[AIRFLOW-1978] Add support for additional WinRM parametersImplemented all of the WinRM options from pywinrm.Implemented support for streaming stdout/stderr.Closes #3512 from jshvrsn/winrm,1
"[AIRFLOW-2671] Monitor gunicorn process, and exit if it exitsCloses #3545 fromNielsZeilemaker/monitor_webserver",1
closes apache/incubator-airflow#3389 *Obsolete PR*,5
[AIRFLOW-2678] Fix db schema unit test to remove checking fab models,4
[AIRFLOW-2681] Include last dag run of externally triggered DAGs in UI.Closes #3551 from dhatch/AIRFLOW-2681,2
Merge pull request #3508 from yrqls21/kevin_yang_fix,0
Merge pull request #3548 from feng-tao/airflow-2678,7
Merge pull request #3539 from seelmann/AIRFLOW-2604-index-task-fail,0
"[AIRFLOW-1840] Support back-compat on old celery configThe new names are in-line with Celery 4, but ifanyone upgrades Airflowwithout following the UPDATING.md instructions(which we probably assumemost people won't, not until something stopsworking) their workerswould suddenly just start failing. That's bad.This will issue a warning but carry on working asexpected. We canremove the deprecation settings (but leave thecode in config) afterthis release has been made.Closes #3549 from ashb/AIRFLOW-1840-back-compat",5
"[AIRFLOW-2668] Handle missing optional cryptography dependencycryptography is a recommended, but optionaldependency. It wasmistakenly made a hard dependency by a refactor.This restores thatbehaviour (though without tests, as it's hard totest that in aunittest)In testing this I found that running `airflowinitdb` would end upprinting the ""crypto is missing"" message 15 times,making it hard to seewhat was actually going on. So I have re-worked`get_fernet()` to onlycompute (and warn) once. This also makes theconsuming code easier as inthe case of the dep not being installed we stillhave a class thatpresents the same interface as Fernet.Closes #3550 from ashb/AIRFLOW-2668-optional-cryptography",1
[AIRFLOW-2650] Mark SchedulerJob as succeed when hitting Ctrl-cWithout this fix it turns out that the job wouldremain in the runningstate.This also sets things to failed in case of anyother exception.Closes #3525 from ashb/scheduler-job-status,0
[AIRFLOW-2359] Add set failed for DagRun and task in tree viewCloses #3255 fromyrqls21/kevin_yang_add_set_failed,0
[AIRFLOW-2692] Add job_name as templated parameter in AWS Batch OperatorCloses #3557 from craigforster/master,1
[AIRFLOW-2682] Add how-to guides for bash and python operatorsCloses #3552 from tswast/airflow-2682-bash-python-how-to,1
[AIRFLOW-2696] Setting UTF-8 as default mime_charset mailupdate UPDATING.mdCloses #3559 from lxneng/feature/utf8_mime_charset,1
[AIRFLOW-2702] Handle uncaught statsd configuration errorsCloses #3564 fromNoremac201/the_spy_who_errored_me,0
[AIRFLOW-2655] Fix inconsistency of default config of kubernetes workerCloses #3529 from mrkm4ntr/airflow-2655,1
[AIRFLOW-2713] Rename async variable in setup.py for Python 3.7.0 compatibilityCloses #3561 from Perados/rename-async-to-async_packages-in-setup,1
"[AIRFLOW-2708] unittest2 is reqired for devel, not just devel_ci",3
"[AIRFLOW-1163][AIRFLOW-XXX] Add support for x-forwarded-* headersWhen running Airflow behind a L7 proxy that sendsx-fowarded-* headers, the Flask app miscontructsredirect URIs.Closes #3580 from c4milo/master",1
"[AIRFLOW-2706] AWS Batch Operator should use top-level job state to determine statusRather than inspecting the state of job attempts,the operator should use the top-level job statusto determine the overall success or failure of thetask. This means the following cases are handledcorrectly:1. Any infrastructure failure that results in noattempts being performed is now detected.2. Any retry policy that AWS Batch will do is nowhonored -- the job isn't marked FAILED until all   attempts to retry have failed. Previously, thefirst failed *attempt* would make the task as   failed.Closes #3567 from craigforster/master",0
"[AIRFLOW-2622] add confirm option to SFTPOperator[]surfaces the confirm option in the SFTPOperatorprovided by theunderlying parmiko library, useful for when thereceiving servermoves the incoming file before the confirmationstep can be completedCloses #3542 from caddac/master",1
Merge pull request #3581 from ashb/AIRFLOW-2708-unittest2-devel,3
[AIRFLOW-2710] Clarify fernet key value in documentationCloses #3574 from padwasabimasala/AIRFLOW-2710,2
[AIRFLOW-XXX] Add KPN B.V. to company listCloses #3577 from biyanisuraj/master,1
[AIRFLOW-2718] Backfill reset_dagrun option allows user to specify certain tasksCloses #3579 from feng-tao/reset_specic_tasks_for_backfill,1
"[AIRFLOW-2735] Use equality, not identity, check for detecting AWS Batch failures[]Closes #3589 from craigforster/master",0
"[AIRFLOW-2463] Make task instance context available for hive queries[AIRFLOW-2463] Make task instance contextavailable for hive queriesupdate UPDATING.md, please squashCloses #3405 from yrqls21/kevin_yang_add_context",1
"[AIRFLOW-2614] Speed up trigger_dag API call when lots of DAGs in systemRather than loading all dags in the DagBag, find the path to thespecific DAG from the ORM and load only that one.Closes #3590 from mishikaSingh/master",2
[AIRFLOW-XXX] Use .sha512 for signatureAs requested by the IPMC,1
[AIRFLOW-XXX] Wrap DISCLAIMER text,5
[AIRFLOW-2739] Always read default configuration files as utf-8Closes #3593 from cjgu/airflow-2739,2
[AIRFLOW-2723] Update lxml dependancy to >= 4.0.0Closes #3583 from neil90/master,5
[AIRFLOW-1729][AIRFLOW-2797][AIRFLOW-2729] Ignore whole directories in .airflowignoreWe can ignore whole directories by removing themfrom the `dirs` arraythat `os.walk()` returns. Doing this means that wefewer disk ops ifsomeone has a set of modules in their dag folderthat they want toignore.Also fixes [AIRFLOW-2797] - we weren't honoring.airflowignore from aparent dir as of #3717 -- that (expected)behaviour is now back again.De-duplicate the walking code as well - we had twoversions that hadgotten out of sync as of #3171. So that doesn'thappen again we now onlyhave one version.Closes #3602 from ashb/ignore-whole-dirs-airflowignore,0
[AIRFLOW-2751] add job properties update in hive to druid operator.Closes #3600 from happyjulie/AIRFLOW-2751,1
[AIRFLOW-2752] Log using logging instead of stdoutLogging should be done using the Python log moduleinstead of writing directly to stdoutCloses #3604 from Fokko/patch-2,2
"[AIRFLOW-1729][AIRFLOW-XXX] Remove extra debug log at info levelI left an extra log call, at info level in #3602that was being used fordebugging.Closes #3603 from ashb/remove-extra-log",4
[AIRFLOW-2754] Fix invalid static path in case of using HDP.Closes #3607 from happyjulie/AIRFLOW-2754,1
[AIRFLOW-2748] add new command args for dbimport and dbexport command to qubole hookCloses #3597 from Joylal4896/master,1
"[AIRFLOW-2267] Airflow DAG level accessMake sure you have checked _all_ steps below.### JIRA- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/)issues and references them in the PR title. Forexample, ""\[AIRFLOW-XXX\] My Airflow PR""    -https://issues.apache.org/jira/browse/AIRFLOW-2267    - In case you are fixing a typo in thedocumentation you can prepend your commit with\[AIRFLOW-XXX\], code changes always need a JIRAissue.### Description- [x] Here are some details about my PR, includingscreenshots of any UI changes: Provide DAG level access for airflow.  The detaildesign could be found at https://docs.google.com/document/d/1qs26lE9kAuCY0Qa0ga-80EQ7d7m4s-590lhjtMBjmxw/edit#### Tests- [x] My PR adds the following unit tests __OR__does not need testing for this extremely goodreason:Unit tests are added.### Commits- [x] My commits all reference JIRA issues intheir subject lines, and I have squashed multiplecommits if they address the same issue. Inaddition, my commits follow the guidelines from""[How to write a good git commitmessage](http://chris.beams.io/posts/git-commit/)"":    1. Subject is separated from body by a blank line    2. Subject is limited to 50 characters    3. Subject does not end with a period    4. Subject uses the imperative mood (""add"", not""adding"")    5. Body wraps at 72 characters    6. Body explains ""what"" and ""why"", not ""how""- [x] Passes `git diff upstream/master -u --""*.py"" | flake8 --diff`Closes #3197 from feng-tao/airflow-2267",4
[AIRFLOW-XXX] Add 6play to company listCloses #3608 from julien-gm/patch-1,1
[AIRFLOW-XXX] Add Dailymotion to company listCloses #3601 from germaintanguy/master,1
"[AIRFLOW-2750] Add subcommands to delete and list usersCurrently, adding user is the onlyoperation that CLI has on RBAC.This PR adds functionality to deleteand list users via CLI.Closes #3610 from sekikn/AIRFLOW-2750",1
[AIRFLOW-2749] Add feature to delete BQ DatasetCloses #3598 from MENA1717/Add-bq-op,1
[AIRFLOW-2704] Add support for labels in the bigquery_operator[AIRFLOW-2704]Add support for labels in thebigquery_operatorAdds support for bigquery labels in the bigqueryoperator and hook.Make labels template fieldsCloses #3573 from mastoj/AIRFLOW-2704,1
[AIRFLOW-XXX] Add Publicis Pixelpark to READMECloses #3556 from feluelle/master,1
[AIRFLOW-2705] Move class-level moto decorator to method-levelMoto decorators at class-level make othertests that sends HTTP request fail.This PR moves them to method-levelso as to fix this problem.Closes #3565 from sekikn/AIRFLOW-2705,0
"[AIRFLOW-2758] Add a sensor for MongoDBThis PR adds a sensor for MongoDB,which waits for some document thatmatches the given query to beinserted to the specified collection.Closes #3611 from sekikn/AIRFLOW-2758",2
[AIRFLOW-2737] Restore original license headerCloses #3591 from seelmann/AIRFLOW-2737-restore-original-license-header,5
[AIRFLOW-2771] Add except type to broad S3Hook try catch clausesS3Hook will silently fail if given a conn_id thatdoes not exist. Thecalls to check_for_key done by an S3KeySensor willnever fail if thecredentials object is not configured correctly.This adds the expectedClientError exception type when performing a HEADoperation on anobject that doesn't exist to the try catchstatements so that otherexceptions are properly raised.Closes #3616 from mascah/AIRFLOW-2771-S3hook-except-type,1
[AIRFLOW-2596] Add Oracle to Azure Datalake Transfer OperatorCloses #3613 frommarcusrehm/oracle_to_azure_datalake_transfer,5
[AIRFLOW-2769] Increase num_retries polling value on Dataflow hookCloses #3617 from pwoods25443/2769-dataflow-num-retries,5
Fix Typo in Scheduler documentationCloses #3618 from amir656/patch-1,2
[AIRFLOW-2731] Raise psutil restriction to <6.0.0Closes #3585 from gwax/upgrade_psutil,5
[AIRFLOW-2778] Explicit import for dag_processing.list_py_file_pathsThe use of utils.dag_processing.list_py_file_paths causes a failure ifutils.dag_processing is not already loaded indirectly.,2
[AIRFLOW-XXX] Include license folder for binary,5
[AIRFLOW-2691] Manage JS dependencies via npmCloses #3572 from verdan/AIRFLOW-2691-npm-webpack,5
Merge pull request #3624 from KevYuen/AIRFLOW-2778-bad-dagbag-import,2
[AIRFLOW-XXX] Fix typos in docsCloses #3625 from feluelle/master,2
[AIRFLOW-2753] Add dataproc_job_id instance var holding actual DP jobIdCloses #3622 from jeffkpayne/master,5
[AIRFLOW-2773] Validates Dataflow Job NameCloses #3623 from kaxil/AIRFLOW-2773,5
[AIRFLOW-2734] Resolve setuptools normalized_version warningCloses #3588 from tedmiston/normalized-version,2
[AIRFLOW-2785] Add context manager entry points to mongoHookCloses #3628 from andscoop/Add-connection-close-to-mongo-hook,1
[AIRFLOW-2615] Limit DAGs parsing to once onlyCloses #3614 from verdan/double-dag-parsing,2
[AIRFLOW-2712] Pass annotations to KubernetesExecutorConfigCloses #3576 from tfpk/master,5
Merge pull request #2480 from martinzlocha/emrAddStepsTemplate,1
[AIRFLOW-1762] Implement key_file support in ssh_hook create_tunnelSwitched to using sshtunnel package instead ofpopen approachCloses #3473 from NielsZeilemaker/ssh_hook,1
[AIRFLOW-2782][Airflow 2782] Upgrades the Dagre D3 versionCloses #3634 from verdan/AIRFLOW-2782-upgrade-dagre-d3,2
[AIRFLOW-2792] change parameter in post requests.Closes #3633 from happyjulie/AIRFLOW-2792,2
[AIRFLOW-2782][Airflow 2782] Removes unused hard-coded dagreD3Closes #3635 from verdan/AIRFLOW-2782-dagred3-fix,2
"[AIRFLOW-2798] Remove needless code from models.pyIf `email_on_failure` or `email_on_failure` set toTRUE and email available, there will be email sentout on event of retry or failure.In the implementation, there is a argument`is_retry` passed to method `self.email_alert`.However, inside this method, this argument is notused at all. I believer the initial author of thismethod was planning to differentiate the email tobe sent out, but for whatever reason this was notimplemented.Given in the email to be sent out, there will be aline ""Try {try_number} out of {max_tries}<br>"", itwould be fine not to differentiate **retry email**and **failure email**.Closes #3640 from XD-DENG/patch-1",0
[AIRFLOW-XXX] Add Vidio and BBM to READMECloses #3631 from BagusThanatos/add-kmk-bbm,1
[AIRFLOW-2791] Add Hipages to list of current airflow usersCloses #3632 from arihantsurana/AIRFLOW-2791,1
"[AIRFLOW-2801] Skip test_mark_success_no_kill in PostgreSQL on CISee mailing list thread ""Flaky test case:test_mark_success_no_kill"".Closes #3642 fromtedmiston/test_mark_success_no_kill-postgresql",3
[AIRFLOW-2783][Airflow 2783] Implement eslint for JS code checkCloses #3641 from verdan/AIRFLOW-2783-eslint,5
[AIRFLOW-2765] Set default mime_charset to UTF-8Closes #3627 from jeffkpayne/AIRFLOW-2765,1
"[AIRFLOW-2777] speed up dag.sub_dag(...)previous version created the subdag by copyingover all the tasks, andthen filtering them down. it's a lot faster if weonly copy over thetasks we needCloses #3621 from abdul-stripe/faster-subdag",2
"[AIRFLOW-2776] Compress tree view JSONThe tree view generates JSON that can be massivefor bigger DAGs,up to 10s of MBs. The JSON is currentlyprettified, which bothtakes up more CPU time during serialization, andslows downeverything else that uses it. Considering the JSONis onlymeant to be used programmatically, this is an easywinCloses #3620 from abdul-stripe/smaller-tree-view-json",5
[AIRFLOW-2766][Airflow-2766] Respect shared datetime across tabsCloses #3615 from verdan/AIRFLOW-2766-shared-datetime,5
[AIRFLOW-XXX] Document dag_dir_list_interval in cfgDocument dag_dir_list_interval in default_airflow.cfghttps://github.com/apache/incubator-airflow/blob/master/UPDATING.md#dag_dir_list_intervalCloses #3645 from elgalu/patch-2,5
[AIRFLOW-2810] Fix typo in Xcom model timestampFix typo in Xcom model timestamp fieldNo new testing - the field is already representedin migrationsCloses #3652 from andywilco/fix_datetime_typo,5
[AIRFLOW-XXX] Add Strongmind to the ReadmeCloses #3655 from tomchapin/master,1
[AIRFLOW-2563] Fix PigCliHook Python 3 string/bytes useUnit tests added for PigCliHook as well to preventfuture issues.Closes #3594 from jakahn/master,0
"[AIRFLOW-2815] Use correct copyright period""onwards"" is not specific enough",1
[AIRFLOW-2816] Fix license text in docs/license.rst,2
[AIRFLOW-2779] Update licenses and remove copyright headers* ssh_hook is not a port anymore* auth backends should never had a copyright clause* minihivecluster isnt used,1
"[AIRFLOW-2807] Support STS Assume Role External IDCurrently the role assumption method works only ifthe granting accountdoes not specify an External ID. The external IDis used to solved theconfused deputy problem. When using the AWS hookto export data tomultiple customers, it's good security practice touse the external ID.There is no backwards compatibility break, the IDwill be `None` inexisting cases. Moto doesn't provide anyconvenient way to verify thevalue was passed in the credential response intests, so existingtest cases are kept.Documentation: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.htmlCloses #3647 from vvondra/support_sts_external_id",1
closes apache/incubator-airflow#3654 *Already merged*,7
[AIRFLOW-2814] Fix inconsistent default configvalue of min_file_process_interval in configtemplate is 0However it's supposed to be 180 according toairflow/jobs.py line 592Closes #3659 from XD-DENG/patch-3,5
[AIRFLOW-2716] Replace async and await py3.7 keywordsCloses #3578 from JacobHayes/py37-keywords,5
"[AIRFLOW-2809] Fix security issue regarding Flask SECRET_KEYIt's recommended by Falsk community to use randomSECRET_KEY for security reason.However, in Airflow there is a default value forsecret_key and most users will ignore to changeit.This may cause security concern.Closes #3651 from XD-DENG/patch-2",1
[AIRFLOW-2238] Flake8 fixes on dev/airflow-pr,0
[AIRFLOW-2238] Update PR tool to push directly to Github,5
"[AIRFLOW-2825]Fix S3ToHiveTransfer bug due to caseBecause upper/lower case was not consideredin the file extension check, S3ToHiveTransferoperator may mistakenly think a GZIP file withuppercase ext "".GZ"" is not a GZIP file andraise exception.",2
Merge pull request #3413 from ashb/pr-tool-git-config[AIRFLOW-2238] Switch PR tool to push to Github,5
[AIRFLOW-2800] Remove low-hanging linting errors,0
[AIRFLOW-2822] Fix HipChat Deprecation WarningFixes PendingDeprecationWarning on HipChatAPISendRoomNotificationOperatorUsing `HipChatAPISendRoomNotificationOperator` on Airflow master branch (2.0) gives:airflow/models.py:2390: PendingDeprecationWarning:Invalid arguments were passed to HipChatAPISendRoomNotificationOperator.Support for passing such arguments will be dropped in Airflow 2.0.Invalid arguments were:*args: ()**kwargs: {'color': 'green'}category=PendingDeprecationWarning,2
Merge pull request #3665 from XD-DENG/patch-6[AIRFLOW-2825] Fix S3ToHiveTransfer bug due to case,0
[AIRFLOW-2795] Oracle to Oracle Transfer Operator (#3639),1
[AIRFLOW-2670] Update SSH Operator's Hook to respect timeout (#3666),1
[AIRFLOW-1104] Update jobs.py so Airflow does not over schedule tasks (#3568)This change will prevent tasks from getting scheduled and queued overthe concurrency limits set for the dag,2
[AIRFLOW-XXX] Add Gradeup to the official list of companies (#3672),1
[AIRFLOW-XXX] Add Collectivehealth to company list (#3671)Add CollectiveHealth Inc. to the official list of companies in the README.md,2
[AIRFLOW-2832] Lint and resolve inconsistencies in Markdown files (#3670)Clean up the Markdown files and make the formatting consistent,1
[AIRFLOW-2835] Remove python-selinux (#3673)This package is not used and it sometimes breaks the CI because itis not available. Therefore it makes sense to just remove it :-),4
[AIRFLOW-2756] Fix bug in set DAG run state workflow (#3606),1
[AIRFLOW-2817] Force explicit choice on GPL dependency (#3660)By default one of Apache Airflow's dependencies pulls in a GPLlibrary. Airflow should not install (and upgrade) without an explicit choice.This is part of the Apache requirements as we cannot depend on Category Xsoftware.,1
"[AIRFLOW-2820] Add Web UI triggger in doc ""Scheduling & Triggers""In documentation page ""Scheduling & Triggers"",it only mentioned the CLI method tomanually trigger a DAG run.However, the manual trigger feature in Web UIshould be mentioned as well(it may be even more frequently used by users).",1
[AIRFLOW-2829] Brush up the CI script for minikubeFix scripts/ci/kubernetes/minikube/start_minikube.shas follows:- Make minikube version configurable via  environment variable- Remove unused variables for readability- Reorder some lines to remove warnings- Replace ineffective `return` with `exit`- Add -E to `sudo minikube` so that non-root  users can use this script locally,1
"[AIRFLOW-2099] Handle getsource() calls gracefullyThere are several scenarios where Task Instance view tries to renderPython callables where 'x' is not the correct artefact to target.This commit adds a helper fuction to test for known scenarios, andderives the source from the correc artefact or as a default returns 'Nosource available for <type>'. This means that even in unknown orunfixable edge cases, the Task Instance view still renders instead ofdisplaying an exception.Closes #3571 from night0wl/AIRFLOW-2099_task_view_type_check",0
[AIRFLOW-XXX] Add Liberty Global to company list (#3685),1
[AIRFLOW-2238] Use SSH protocol for pushing to Github (#3680)Since we have MFA enforced we won't be able to use passwords to push andwill have to use SSH keypairs.,1
[AIRFLOW-2658] Add GCP specific k8s pod operator (#3532)Executes a task in a Kubernetes pod in the specified Google KubernetesEngine cluster. This makes it easier to interact with GCP kubernetesengine service because it encapsulates acquiring credentials.,1
[AIRFLOW-XXX] Update changelog for 1.10(cherry picked from commit 862ad8b9c2eb9af206d369bc661b6a2decc22148)Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>,4
Revert [AIRFLOW-2814] - Change `min_file_process_interval` to 0 (#3669)- Change the time (in seconds) after which a new DAG should be picked up from the filesystem,5
[AIRFLOW-2805] Display multiple timezones on UI (#3687),5
[AIRFLOW-2796] Expand code coverage for utils/helpers.py (#3686),3
[AIRFLOW-2836] Minor improvement-contrib.sensors.FileSensor (#3674)- The default value of fs_conn_id was not proper.- Added a new test in which we try to ignore setting  fs_conn_id explicitly.- a minor change on how a path is concatenated,4
[AIRFLOW-XXX] Add Feng Tao to committers list (#3689),1
[AIRFLOW-2849] Add flake8 to setup.pyCloses #3694 from eyaltrabelsi/AIRFLOW-2849-add-flake8-to-setup.py-dev-req-to-run-quality-check-locally,1
[AIRFLOW-2796] Improve utils helpers code coverage (#3637),3
"[AIRFLOW-2848] Ensure dag_id in metadata ""job"" for LocalTaskJob (#3693)dag_id is missing for all entries in metadatatable ""job"" with job_type ""LocalTaskJob"".This is due to that dag_id was not specifiedwithin class LocalTaskJob.A test is added to check if essentialattributes of LocalTaskJob can be assignedwith proper values without intervention.",1
[AIRFLOW-2839] Refine Doc Concepts->Connections (#3678),2
[AIRFLOW-2850] Remove deprecated airflow.utils.apply_defaults (#3695),4
"[AIRFLOW-2821] Refine Doc ""Plugins"" (#3664)",2
[AIRFLOW-2853] Add official committers to READMECloses #3699 from andscoop/Add-core-commiters-to-readme,1
[AIRFLOW-2858] Make tox.ini indentation and whitespace consistent (#3705),5
[AIRFLOW-2845] Asserts in contrib package code are changed on raise ValueError and TypeError (#3690),0
[AIRFLOW-XXX] Add docs for running Travis CI on fork (#3706),1
[AIRFLOW-XXX] Add how to run the docs server to docs (#3704),2
[AIRFLOW-2860] DruidHook: time variable is not updated correctly when checking for timeout (#3707),5
[AIRFLOW-2811] Fix scheduler_ops_metrics.py to work (#3653)This PR fixes timezone problem inscheduler_ops_metrics.py and makesits timeout configurable.,5
"Revert ""[AIRFLOW-2860] DruidHook: time variable is not updated correctly when checking for timeout (#3707)""This reverts commit d12aacd552878308f9b1c3663414bb7c00c0632b.",4
[AIRFLOW-2755] Added `kubernetes.worker_dags_folder` configuration (#3612)It was previously hardcoded to `/tmp/dags`.This causes problems with python import of modules in the DAGs folder.,2
AIRFLOW-2787 Allow is_backfill to handle NULL DagRun.run_id (#3629),2
"[AIRFLOW-2864] Fix docstrings for SubDagOperator (#3712)The docstrings are currently in the `__init__` method, due to which they are not automatically shown in the Sphinx documentation.",2
[AIRFLOW-2857] Fix Read the Docs env (#3703)The Read the Docs build process was broken due to #3660. This PR fixes this.,0
[AIRFLOW-2860] Raise ValueError if timeout < 1 in druid hook,1
[AIRFLOW-2231] Fix relativedelta DAG schedule_interval (#3174)Fixes issues when specifying a DAG with a schedule_interval of type relativedelta.,2
[AIRFLOW-2863] Fix GKEClusterHook catching wrong exception (#3711),0
[AIRFLOW-2140] Don't require kubernetes for the SparkSubmit hook (#3700)This extra dep is a quasi-breaking change when upgrading - previouslythere were no deps outside of Airflow itself for this hook. Importingthe k8s libs breaks installs that aren't also using Kubernetes.This makes the dep optional for anyone who doesn't explicitly use thefunctionality,1
"[AIRFLOW-2851] Canonicalize ""as _..."" etc imports (#3696)",2
[AIRFLOW-XXX] Updating instructions about logging changes in 1.10 (#3715)We had a few other logging changes that weren't mentioned in here thatmeant previous logs were not viewable anymore.,2
[AIRFLOW-2869] Remove smart quote from default configCloses #3716 from wdhorton/remove-smart-quote-from-cfg,5
"[AIRFLOW-2867] Refactor Code to conform standards (#3714)- Dictionary creation should be written by dictionary literal- Python’s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.- Functions calling sets which can be replaced by set literal are now replaced by set literal- Replace list literals- Some of the static methods haven't been set static- Remove redundant parentheses",4
"[AIRFLOW-2855] Check Cron Expression Validity in DagBag.process_file() (#3698)A DAG can be imported as a .py script properly,but the Cron expression inside as ""schedule_interval"" may beinvalid, like ""0 100 * * *"".This commit helps check the validity of Cron expression in DAGfiles (.py) and packaged DAG files (.zip), and help showexception messages in web UI by add these exceptions intometadata ""import_error"".",2
"[AIRFLOW-2859] Implement own UtcDateTime (#3708)The different UtcDateTime implementations all have issues.Either they replace tzinfo directly without convertingor they do not convert to UTC at all.We also ensure all mysql connections are in UTCin order to keep sanity, as mysql will ignore thetimezone of a field when inserting/updating.",5
[AIRFLOW-2826] Add GoogleCloudKMSHook (#3677)Adds a hook enabling encryption and decryption through Google Cloud KMS.This should also contribute to AIRFLOW-2062.,0
[AIRFLOW-2874] Enables FAB's theme support (#3719),1
[AIRFLOW-2856] Pass in SLUGIFY_USES_TEXT_UNIDECODE=yes ENV to docker run (#3701),1
[AIRFLOW-2870] Use abstract TaskInstance for migration (#3720)If we use the full model for migration it can have columnsadded that are not available yet in the database. Usingan abstraction ensures only the columns that are requiredfor data migration are present.,5
[AIRFLOW-2861] Add index on log table (#3709),2
[AIRFLOW-XXX] Update changelog for 1.10,4
"[AIRFLOW-2878] Fix www_rbac display issueThe new RBAC UI has some issues about layout/UI display.The header (<h2>) is not shown (""hidden"" by the Nav Bar),or tables are not shown completely.This is addressed by a simple change ontemplates/appbuilder/baselayout.html",4
[AIRFLOW-XXX] Update BaseOperator documentation on priority_weight (#3727),2
[AIRFLOW-2763] Add check to validate worker connectivity to metadata Database,5
[AIRFLOW-2884] Fix Flask SECRET_KEY security issue in www_rbac (#3729)The same issue was fixed for /www previously inPR https://github.com/apache/incubator-airflow/pull/3651(JIRA ticket 2809),0
[AIRFLOW-1874] use_legacy_sql added to BigQueryCheck operators (#3717),1
[AIRFLOW-2786] Gracefully handle Variable import errors (#3648)Variables that are added through a file are notchecked as explicity as creating a Variable in theweb UI. This handles exceptions that could be causedby improper keys or values.,5
"Revert ""[AIRFLOW-2878] Fix www_rbac display issue"" (#3737)This reverts commit 1f57dafd70ac41da8295f20793f8913b7f5a5dff.",4
[AIRFLOW-2889] Fix typos detected by github.com/client9/misspell (#3732),2
"[AIRFLOW-2524] Add Amazon SageMaker Training (#3658)Add SageMaker Hook, Training Operator & SensorCo-authored-by: srrajeev-aws <srrajeev@amazon.com>",1
[AIRFLOW-2860] DruidHook: time check is wrong (#3745),0
"[AIRFLOW-2886] Secure Flask SECRET_KEY (#3738)The Flask SECRET_KEY should be as random as possible.On the other hand, we can nott genrate random value whenwe launch the webserver (the secret_key will beinconsistent across the workers).We can generate a random one in the configuration fileairflow.cfg, just like how we deal with FERNET_KEY.The SECRET_KEY is generated using os.urandom, asrecommended by Flask community.",1
"[AIRFLOW-XXX] Make pip install commands consistent (#3752)- Replace airflow PyPI package name with apache-airflow- Remove unnecessary quotes on pip install commands- Make install extras commands consistent with pip docs [1] (e.g.,alpha sort order without spaces or quotes)[1]: https://pip.pypa.io/en/stable/reference/pip_install/#examples",2
"[AIRFLOW-XXX] Clean up installation extra packages table (#3750)Sort the extra packages table, use official product names, improvecapitalization, and make table whitespace consistent.",1
[AIRFLOW-2888] Remove shell=True and bash from task launch (#3740)shell=True is a security risk. Bash is not required to launchtasks and will consume extra resources.,1
[AIRFLOW-2893] fix stuck dataflow job due to name mismatch (#3744),5
"[AIRFLOW-2903] Change default owner from ""Airflow"" to ""airflow""",4
"Revert ""[AIRFLOW-2903] Change default owner from ""Airflow"" to ""airflow""""This reverts commit 54ae12b59affd71d2de641826d7d62008f09bb4a.",4
Added Ville de Montréal as an official airflow user (#3759),1
"[AIRFLOW-2913] Check bucket_key/bucket_name combination in S3KeySensorWhen bucket_name is provided, and bucket_key is also provided as a fullS3:// url, the full_url obtained eventually will be wrong.  It will belike 's3://bucket_name/s3://bucket_name/object_key'.This should be avoided by adding checking and raise exception in suchcase.Closes #3762 from XD-DENG/patch-6",1
"[AIRFLOW-2917] Set AIRFLOW__CORE__SQL_ALCHEMY_CONN only when needed (#3766)Only when `airflow_configmap` is not provided and `AIRFLOW__CORE__SQL_ALCHEMY_CONN` not in secrets, it is set as an env var.",1
[AIRFLOW-2245] Add remote_host of SSH/SFTP operator as templated field (#3765)It allows remote_host to be passed to operator with XCOM.,1
[AIRFLOW-2524] Add Amazon SageMaker Tuning (#3751)Add SageMaker tuning Operator and sensorCo-authored-by: srrajeev-aws <srrajeev@amazon.com>,1
[AIRFLOW-XXX] Fixing the issue in Documentation (#3756),2
[AIRFLOW-2904] Remove unnecessary line in celery_executor (#3753)Clean an unnecessary line in celery_executor.py,4
[AIRFLOW-2907] Fix Sendgrid attachments bytes err (#3757)[AIRFLOW-2907] Fix Sendgrid Attachments Object of type 'bytes' is not JSON serializableAttempting to attach files via Sendgrid operator always gives:    Sendgrid - Attachments - ERROR - Object of type 'bytes' is not JSON serializable,5
[AIRFLOW-XXX] Specify email domain in documentation (#3771)This makes it less ambigious,1
[AIRFLOW-2895] Prevent scheduler from spamming heartbeats/logsReverts most of AIRFLOW-2027 until the issues withit can be fixed.Closes #3747 fromaoen/revert_min_file_parsing_time_commit,4
Added Modernizing Medicine to list of Who Uses Airflow? in readme (#3775),1
[AIRFLOW-2915] Add example DAG for GoogleCloudStorageToBigQueryOperator (#3763)* Don't add DAG if GCP operators aren't installed.* Remove extra blank lines for flake8.,4
[AIRFLOW-2908] Allow retries with KubernetesExecutor. (#3758),1
[AIRFLOW-2918] Fix Flake8 violations (#3772)- Unused imports- Wrong import order- Small identation fixes- Remove one letter variables- Fix noqa annotations,0
[AIRFLOW-2905] Fix get job API endpoint (#3755),1
[AIRFLOW-XXX] Fix minor typo (#3776),2
[AIRFLOW-XXX] Fix some operator names in the docs (#3778),2
"[AIRFLOW-2499] Dockerise CI pipeline (#3393)Airflow tests depend on many external services and other custom setup,which makes it hard for contributors to work on this codebase. CIbuilds have also been unreliable, and it is hard to reproduce thecauses. Having contributors trying to emulate the build environmentevery time makes it easier to get to an ""it works on my machine"" sortof situation.This implements a dockerised version of the current build pipeline.This setup has a few advantages:* TravisCI tests are reproducible locally* The same build setup can be used to create a local development environment",1
"[AIRFLOW-2921][AIRFLOW-2922] Fix bugs in CeleryExecutor (#3773)Bug-1:if a task state becomes either SUCCESS or FAILURE or REVOKED,it will be removed from self.tasks() and self.last_state().However, because line 108 is not indented properly,this task will be added back to self.last_state() again.Bug-2:When the state is updated, it's referring to the lateststate `task.state` rather than variable `state`.This may result in dead-lock if the state changed from`STARTED` to `SUCCESS` after the if-elif-else blockstarted.Test case is updated for fix to bug-1.",0
[AIRFLOW-XXX] Typo in the write-logs.rst (#3781)Fixed airflow_tas_runner -> airflow_task_runner,1
[AIRFLOW-2909] Deprecate airflow.operators.sensors module (#3760),1
[AIRFLOW-XXX] Add Flipp to list of Airflow users (#3777)* Add Flipp to list of Airflow users* Update README.md,2
[AIRFLOW-XXX] Replaces incorrect env var name in INSTALL (#3788)INSTALL mis-named AIRFLOW_GPL_UNIDECODE environment variable.,2
Add Arquivei to companies list (#3789),1
[AIRFLOW-XXX] Added G Adventures to Users (#3794),1
[AIRFLOW-2938] Handle improperly formatted extra field in connection gracefully (#3785),0
[AIRFLOW-XXX] Adding King.com to the list of companies. (#3800),1
add 8fit to list of companies,1
[AIRFLOW-XXX] Add THE ICONIC to the list of orgs using AirflowCloses #3807 from ksaagariconic/patch-2,1
"[AIRFLOW-2933] Enable Codecov on Docker-CI Build (#3780)- Add missing variables and use codecov instead of coveralls.  The issue why it wasn't working was because missing environment variables.  The codecov library heavily depends on the environment variables in  the CI to determine how to push the reports to codecov.- Remove the explicit passing of the variables in the `tox.ini`  since it is already done in the `docker-compose.yml`,  having to maintain this at two places makes it brittle.- Removed the empty Codecov yml since codecov was complaining that  it was unable to parse it",5
[AIRFLOW-2960] Pin boto3 to <1.8 (#3810)Boto 1.8 has been released a few days ago and they break our tests.,3
[AIRFLOW-2957] Remove obselete sensor references,4
"[AIRFLOW-2959] Refine HTTPSensor doc (#3809)HTTP Error code other than 404,or Connection Refused, would fail the sensoritself directly (no more poking).",0
"[AIRFLOW-2961] Refactor tests.BackfillJobTest.test_backfill_examples test (#3811)Simplify this test since it takes up 15% of all the time. This is becauseevery example dag, with some exclusions, are backfilled. This will put somepressure on the scheduler and everything. If the test just covers a coupleof dags should be sufficient254 seconds:[success] 15.03% tests.BackfillJobTest.test_backfill_examples: 254.9323s",3
[AIRFLOW-XXX] Remove residual line in Changelog (#3814),4
"[AIRFLOW-2930] Fix celery excecutor scheduler crash (#3784)Caused by an update in PR #3740.execute_command.apply_async(args=command, ...)-command is a list of short unicode strings and the above code pass multiplearguments to a function defined as taking only one argument.-command = [""airflow"", ""run"", ""dag323"",...]-args = command = [""airflow"", ""run"", ""dag323"", ...]-execute_command(""airflow"",""run"",""dag3s3"", ...) will be error and exit.",0
"[AIRFLOW-2916] Arg `verify` for AwsHook() & S3 sensors/operators (#3764)This is useful when1. users want to use a different CA cert bundle than the  one used by botocore.2. users want to have '--no-verify-ssl'. This is especially useful  when we're using on-premises S3 or other implementations of  object storage, like IBM's Cloud Object Storage.The default value here is `None`, which is also the defaultvalue in boto3, so that backward compatibility is ensured too.Reference:https://boto3.readthedocs.io/en/latest/reference/core/session.html",3
[AIRFLOW-2709] Improve error handling in Databricks hook (#3570)* Use float for default value* Use status code to determine whether an error is retryable* Fix wrong type in assertion* Fix style to prevent lines from exceeding 90 characters* Fix wrong way of checking exception type,0
[AIRFLOW-2854] kubernetes_pod_operator add more configuration items (#3697)* kubernetes_pod_operator add more configuration items* fix test_kubernetes_pod_operator test_faulty_service_account failure case* fix review comment issues* pod_operator add hostnetwork config* add doc example,2
[AIRFLOW-2994] Fix command status check in Qubole Check operator (#3790),1
[AIRFLOW-2928] Use uuid4 instead of uuid1 (#3779)for better randomness.,1
[AIRFLOW-2949] Add syntax highlight for single quote strings (#3795)* AIRFLOW-2949: Add syntax highlight for single quote strings* AIRFLOW-2949: Also updated new UI main.css,1
"[AIRFLOW-2948] Arg check & better doc - SSHOperator & SFTPOperator (#3793)There may be different combinations of arguments, andsome processings are being done 'silently', while usersmay not be fully aware of them.For example- User only needs to provide either `ssh_hook`  or `ssh_conn_id`, while this is not clear in doc- if both provided, `ssh_conn_id` will be ignored.- if `remote_host` is provided, it will replace  the `remote_host` which wasndefined in `ssh_hook`  or predefined in the connection of `ssh_conn_id`These should be documented clearly to ensure it'stransparent to the users. log.info() should also beused to remind users and provide clear logs.In addition, add instance check for ssh_hook to ensureit is of the correct type (SSHHook).Tests are updated for this PR.",5
[AIRFLOW-XXX] Fix Broken Link in CONTRIBUTING.md,2
[AIRFLOW-2980] ReadTheDocs - Fix Missing API Reference,0
[AIRFLOW-2984] Convert operator dates to UTC (#3822)Tasks can have start_dates or end_dates separatelyfrom the DAG. These need to be converted to UTC otherwisewe cannot use them for calculation the next executiondate.,5
[AIRFLOW-2779] Make GHE auth third party licensed (#3803)This reinstates the original license.,1
[AIRFLOW-XXX] Add Format to list of companies (#3824),1
[AIRFLOW-2900] Show code for packaged DAGs (#3749),2
[AIRFLOW-2983] Add prev_ds_nodash and next_ds_nodash macro (#3821),1
[AIRFLOW-2989] Add param to set bootDiskType in Dataproc Op (#3825)Add param to set bootDiskType for master andworker nodes in `DataprocClusterCreateOperator`,5
"[AIRFLOW-2974] Extended Databricks hook with clusters operation (#3817)Add hooks for:- cluster start, - restart,- terminate. Add unit tests for the added hooks. Add hooks for cluster start, restart and terminate. Add unit tests for the added hooks.Add cluster_id variable for performing cluster operation tests.",3
[AIRFLOW-XXX] Fix Docstrings for Operators (#3820),1
[AIRFLOW-2994] Fix flatten_results for BigQueryOperator (#3829),1
[AIRFLOW-2951] Update dag_run table end_date when state change (#3798)The existing airflow only change dag_run table end_date value whena user teminate a dag in web UI. The end_date will not be updatedif airflow detected a dag finished and updated its state.This commit add end_date update in DagRun's set_state function tomake up tho problem mentioned above.,0
"[AIRFLOW-2145] fix deadlock on clearing running TI (#3657)a `shutdown` task is not considered be `unfinished`, so a dag run candeadlock when all `unfinished` downstreams are all waiting on a taskthat's in the `shutdown` state. fix this by considering `shutdown` tobe `unfinished`, since it's not truly a terminal state",5
[AIRFLOW-2981] Fix TypeError in dataflow operators (#3831)- Fix TypeError in dataflow operators when using GCS jar or py_file,2
[AIRFLOW-XXX] Fix typo in docstring of gcs_to_bq (#3833),2
[AIRFLOW-2476] Allow tabulate up to 0.8.2 (#3835),1
[AIRFLOW-XXX] Fix typos in faq.rst (#3837),2
[AIRFLOW-2979] Make celery_result_backend conf Backwards compatible (#3832)(#2806) Renamed `celery_result_backend` to `result_backend` and broke backwards compatibility.,5
[AIRFLOW-2866] Fix missing CSRF token head when using RBAC UI (#3804),1
[AIRFLOW-491] Add feature to pass extra api configs to BQ Hook (#3733),1
[AIRFLOW-208] Add badge to show supported Python versions (#3839),1
[AIRFLOW-3007] Update backfill example in Scheduler docsThe scheduler docs at https://airflow.apache.org/scheduler.html#backfill-and-catchup use deprecated way of passing `schedule_interval`. `schedule_interval` should be pass to DAG as a separate parameter and not as a default arg.,2
[AIRFLOW-3005] Replace 'Airbnb Airflow' with 'Apache Airflow' (#3845),5
[AIRFLOW-3002] Fix variable & tests in GoogleCloudBucketHelper (#3843),5
[AIRFLOW-2991] Log path to driver output after Dataproc job (#3827),5
[AIRFLOW-XXX] Fix python3 and flake8 errors in dev/airflow-jiraThis is a script that checks if the Jira's marked as fixed in a releaseare actually merged in - getting this working is helpful to me inpreparing 1.10.1,1
[AIRFLOW-3006] Add note on using None for schedule_interval,1
"[AIRFLOW-3003] Pull the krb5 image instead of building (#3844)Pull the image instead of building it, this will speed up the CIprocess since we don't have to build it every time.",2
[AIRFLOW-2883] Add import and export for pool cli using JSON,5
[AIRFLOW-2847] Remove legacy imports support for plugins (#3692),1
"[AIRFLOW-1998] Implemented DatabricksRunNowOperator for jobs/run-now … (#3813)Add functionality to kick of a Databricks job right away.* Per feedback: fixed a documentation error,   reintegrated the execute and on_kill onto the objects.* Fixed a  documentation issue.",0
[AIRFLOW-3021] Add Censys to who uses Airflow list> Censys> Find and analyze every reachable server and device on the Internet> https://censys.io/closes AIRFLOW-3021 https://issues.apache.org/jira/browse/AIRFLOW-3021,0
[AIRFLOW-3018] Fix Minor issues in Documentation,2
Add Branch to Company List,1
[AIRFLOW-3023] Fix docstring datatypes,5
[AIRFLOW-3008] Move Kubernetes example DAGs to contrib,2
"[AIRFLOW-2997] Support cluster fields in bigquery (#3838)This adds a cluster_fields argument to the bigquery hook, GCS tobigquery operator and bigquery query operators. This field requests thatbigquery store the result of the query/load operation sorted accordingto the specified fields (the order of fields given is significant).",1
[AIRFLOW-XXX] Redirect FAQ `airflow[crypto]` to How-to Guides.,5
[AIRFLOW-XXX] Remove redundant space in Kerberos (#3866),4
[AIRFLOW-3028] Update Text & Images in Readme.md,2
[AIRFLOW-1917] Trim extra newline and trailing whitespace from log (#3862),2
"[AIRFLOW-2985] Operators for S3 object copying/deleting (#3823)1. Copying:Under the hood, it's `boto3.client.copy_object()`.It can only handle the situation in which theS3 connection used can access both source anddestination bucket/key.2. Deleting:2.1 Under the hood, it's `boto3.client.delete_objects()`.It supports either deleting one single object ormultiple objects.2.2 If users try to delete a non-existent object, therequest will still succeed, but there will be anentry 'Errors' in the response. There may also beother reasons which may cause similar 'Errors' (request itself would succeed without explicitexception). So an argument `silent_on_errors` is addedto let users decide if this sort of 'Errors' shouldfail the operator.The corresponding methods are added into S3Hook, andthese two operators are 'wrappers' of these methods.",1
[AIRFLOW-3030] Fix CLI docs (#3872),2
[AIRFLOW-XXX] Update kubernetes.rst docs (#3875)Update kubernetes.rst with correct KubernetesPodOperator inputsfor the volumes.,1
[AIRFLOW-XXX] Add Enigma to list of companies,1
"[AIRFLOW-2965] CLI tool to show the next execution datetimeCover different cases- schedule_interval is ""@once"" or None, then following_schedule  method would always return None- If dag is paused, print reminder- If latest_execution_date is not found, print warning saying  not applicable.",2
[AIRFLOW-XXX] Add Bombora Inc using Airflow,1
[AIRFLOW-2156] Parallelize Celery Executor task state fetching (#3830),5
[AIRFLOW-XXX] Move Dag level access control out of 1.10 section (#3882)It isn't in 1.10 (and wasn't in this section when the PR was created).,1
[AIRFLOW-3040] Enable ProBot to clean up stale Pull Requests (#3883),4
[AIRFLOW-3012] Fix Bug when passing emails for SLA,4
[AIRFLOW-2797] Create Google Dataproc cluster with custom image (#3871),5
[AIRFLOW-XXX] Updated README  to include CAVA,5
"[AIRFLOW-2988] Run specifically python2 for dataflow (#3826)Apache beam does not yet support python3, so it's best to run dataflowjobs with python2 specifically until python3 support is complete(BEAM-1251), in case if the user's 'python' in PATH is python3.",1
[AIRFLOW-3035] Allow custom 'job_error_states' in dataproc ops (#3884)Allow caller to pass in custom list of Dataproc job states into theDataProc*Operator classes that should result in the_DataProcJob.raise_error() method raising an Exception.,5
"[AIRFLOW-3034]: Readme updates : Add Slack & Twitter, remove Gitter",4
[AIRFLOW-3056] Add happn to Airflow user list,1
[AIRFLOW-3052] Add logo options to Airflow (#3892),2
[AIRFLOW-3060] DAG context manager fails to exit properly in certain circumstances,0
[AIRFLOW-2524] Add SageMaker Batch Inference (#3767)* Fix for comments* Fix sensor test* Update non_terminal_states and failed_states to static variables of SageMakerHookAdd SageMaker Transform Operator & SensorCo-authored-by: srrajeev-aws <srrajeev@amazon.com>,1
[AIRFLOW-2772] Fix Bug in BigQuery hook for Partitioned Table  (#3901),1
[AIRFLOW-XXX] Added Jeitto as one of happy Airflow users! (#3902)[AIRFLOW-XXX] Add Jeitto as one happy Airflow user!,1
"[AIRFLOW-3044] Dataflow operators accept templated job_name param (#3887)* Default value of new job_name param is templated task_id, to match theexisting behavior as much as possible.* Change expected value in test_mlengine_operator_utils.py to matchdefault for new job_name param.",2
"[AIRFLOW-2707] Validate task_log_reader on upgrade from <=1.9 (#3881)We changed the default logging config and config from 1.9 to 1.10, butanyone who upgrades and has an existing airflow.cfg won't know they needto change this value - instead they will get nothing displayed in the UI(ajax request fails) and see ""'NoneType' object has no attribute 'read'""in the error log.This validates that config section at start up, and seamlessly upgradesthe old previous value.",5
[AIRFLOW-3025] Enable specifying dns and dns_search options for DockerOperator (#3860)Enable specifying dns and dns_search options for DockerOperator,2
[AIRFLOW-1298] Clear UPSTREAM_FAILED using the clean cli (#3886)* [AIRFLOW-1298] Fix 'clear only_failed'* [AIRFLOW-1298] Fix 'clear only_failed',0
"[AIRFLOW-3059] Log how many rows are read from Postgres (#3905)To know how many data is being read from Postgres, it is nice to logthis to the Airflow log.Previously when there was no data, it would still create a single file.This is not something that we want, and therefore we've changed thisbehaviour.Refactored the tests to make use of Postgres itself since we have itrunning. This makes the tests more realistic, instead of mockingeverything.",3
[AIRFLOW-XXX] Fix typo in docs/timezone.rst (#3904),2
[AIRFLOW-3070] Refine web UI authentication-related docs (#3863),2
[AIRFLOW-3068] Remove deprecated imports,2
"[AIRFLOW-3036] Add relevant ECS options to ECS operator. (#3908)The ECS operator currently supports only a subset of available optionsfor running ECS tasks. This patch adds all ECS options that could berelevant to airflow; options that wouldn't make sense here, like`count`, were skipped.",1
[AIRFLOW-1195] Add feature to clear tasks in Parent Dag (#3907),2
"[AIRFLOW-3073] Add note-Profiling feature not supported in new webserver (#3909)Adhoc queries and Charts features are no longer supported in newFAB-based webserver and UI. But this is not mentioned at all in the doc""Data Profiling"" (https://airflow.incubator.apache.org/profiling.html)This commit adds a note to remind users for this.",1
[AIRFLOW-XXX] Fix SlackWebhookOperator docs (#3915)The docs refer to `conn_id` while the actual argument is `http_conn_id`.,2
[AIRFLOW-1441] Fix inconsistent tutorial code (#2466),0
[AIRFLOW-XXX] Add 90 Seconds to companies,1
[AIRFLOW-3096] Reduce DaysUntilStale for probot/stale,5
[AIRFLOW-3096] Further reduce DaysUntilStale for probo/stale,5
[AIRFLOW-3072] Assign permission get_logs_with_metadata to viewer role (#3913),5
[AIRFLOW-3090] Demote dag start/stop log messages to debug (#3920),0
[AIRFLOW-2407] Use feature detection for reload() (#3298)* [AIRFLOW-2407] Use feature detection for reload()[Use feature detection instead of version detection](https://docs.python.org/3/howto/pyporting.html#use-feature-detection-instead-of-version-detection) is a Python porting best practice that avoids a flake8 undefined name error...flake8 testing of https://github.com/apache/incubator-airflow on Python 3.6.3,3
[AIRFLOW-2747] Explicit re-schedule of sensors (#3596)* [AIRFLOW-2747] Explicit re-schedule of sensorsAdd `mode` property to sensors. If set to `reschedule` anAirflowRescheduleException is raised instead of sleeping which setsthe task back to state `NONE`. Reschedules are recorded in new`task_schedule` table and visualized in the Gantt view. New TIdependency checks if a sensor task is ready to be re-scheduled.* Reformat sqlalchemy imports* Make `_handle_reschedule` private* Remove print* Add comment* Add comment* Don't record reschule request in test mode,3
"[AIRFLOW-XXX] Fix a wrong sample bash command, a display issue & a few typos (#3924)",2
"[AIRFLOW-3090] Make No tasks to consider for execution debug (#3923)During normal operation, it is not necessary to see the message.  Thiscan only be useful when debugging an issue.",0
AIRFLOW-2952 Fix Kubernetes CI (#3922)The current dockerised CI pipeline doesn't run minikube and theKubernetes integration tests. This starts a Kubernetes cluster using minikube and runs k8s integration tests using docker-compose.,2
[AIRFLOW-2918] Fix Flake8 violations (#3931),0
"[AIRFLOW-3076] Remove preloading of MySQL testdata (#3911)One of the things for tests is being self contained. This means thatit should not depend on anything external, such as loading data.This PR will use the setUp and tearDown to load the data into MySQLand remove it afterwards. This removes the actual bash mysql commandsand will make it easier to dockerize the whole testsuite in the future",3
[AIRFLOW-2887] Added BigQueryCreateEmptyDatasetOperator and create_emty_dataset to bigquery_hook (#3876),1
[AIRFLOW-2918] Remove unused imports,2
[AIRFLOW-3099] Stop Missing Section Errors for optional sections (#3934),0
[AIRFLOW-3090] Specify path of key file in log message (#3921),2
"[AIRFLOW-3067] Display www_rbac Flask flash msg properly (#3903)The Flask flash messages are not displayed properly.When we don't give a category for a flash message, defautlvalue will be 'message'. In some cases, we specify 'error'category.Using Flask-AppBuilder, the flash message will be givena CSS class 'alert-[category]'. But We don't have'alert-message' or 'alert-error' in the current'bootstrap-theme.css' file.This makes the the flash messages in www_rbac UI come withno background color.This commit addresses this issue by adding 'alert-message'(using specs of existing CSS class 'alert-info') and'alert-error' (using specs of existing CSS class 'alert-danger')into 'bootstrap-theme.css'.",1
[AIRFLOW-3109] Bugfix to allow user/op roles to clear task intance via UI by default,1
add show statements to hql filtering.,1
"[AIRFLOW-3051] Change CLI to make users ops similar to connectionsThe ability to manipulate users from the  command line is a bit clunky.  Currently 'airflow create_user' and 'airflow delete_user' and 'airflow list_users'.  It seems that these ought to be made more like connections, so that it becomes 'airflow users list ...', 'airflow users delete ...' and 'airflow users create ...'",1
[AIRFLOW-3009] Import Hashable from collection.abc to fix Python 3.7 deprecation warning (#3849),2
[AIRFLOW-XXX] Add Tesla as an Apache Airflow user (#3947),1
"[AIRFLOW-3111] Fix instructions in UPDATING.md and remove comment (#3944)artifacts in default_airflow.cfg- fixed incorrect instructions in UPDATING.md regarding core.log_filename_template and elasticsearch.elasticsearch_log_id_template- removed comments referencing ""additional curly braces"" fromdefault_airflow.cfg since they're irrelevant to the rendered airflow.cfg",5
"[AIRFLOW-3117] Add instructions to allow GPL dependency (#3949)The installation instructions failed to mention how to proceed with the GPL dependency. For those who are not concerned by GPL, it is useful to know how to proceed with GPL dependency.",1
[AIRFLOW-XXX] Add Square to the companies lists,1
[AIRFLOW-XXX] Add Fathom Health to readme,1
[AIRFLOW-XXX] Pin Click to 6.7 to Fix CI (#3962),0
[AIRFLOW-XXX] Fix SlackWebhookOperator execute method comment (#3963),1
[AIRFLOW-3100][AIRFLOW-3101] Improve docker compose local testing (#3933),3
"[AIRFLOW-3127] Fix out-dated doc for Celery SSL (#3967)Now in `airflow.cfg`, for Celery-SSL, the item names are""ssl_active"", ""ssl_key"", ""ssl_cert"", and ""ssl_cacert"".(since PR https://github.com/apache/incubator-airflow/pull/2806/files)But in the documentationhttps://airflow.incubator.apache.org/security.html?highlight=celeryorhttps://github.com/apache/incubator-airflow/blob/master/docs/security.rst,it's ""CELERY_SSL_ACTIVE"", ""CELERY_SSL_KEY"", ""CELERY_SSL_CERT"", and""CELERY_SSL_CACERT"", which is out-dated and may confuse readers.",5
[AIRFLOW-XXX] Fix PythonVirtualenvOperator tests (#3968)The recent update to the CI image changed the defaultpython from python2 to python3. The PythonVirtualenvOperatortests expected python2 as default and fail due toserialisation errors.,0
[AIRFLOW-2952] Fix Kubernetes CI (#3957)- Update outdated cli command to create user- Remove `airflow/example_dags_kubernetes` as the dag already exists in `contrib/example_dags/`- Update the path to copy K8s dags,2
"[AIRFLOW-3104] Add .airflowignore info into doc (#3939).airflowignore is a nice feature, but it was not mentioned at all in the documentation.",2
[AIRFLOW-3130] Add CLI docs for users command,1
[AIRFLOW-XXX] Add Delete for CLI Example in UPDATING.md,5
[AIRFLOW-3123] Use a stack for DAG context management (#3956),2
[AIRFLOW-3125] Monitor Task Instances creation rates (#3966)Montor Task Instances creation rates by Operator type.These stats can provide some visibility on how much workload Airflow isgetting. They can be used for resource allocation in the long run (i.e.to determine when we should scale up workers) and debugging in scenarioslike the creation rate of certain type of Task Instances spikes.,1
[AIRFLOW-3129] Backfill mysql hook unit tests. (#3970),3
[AIRFLOW-3124] Fix RBAC webserver debug mode (#3958),0
[AIRFLOW-XXX] Add Compass to companies list (#3972)We're using Airflow at Compass now.,4
"[AIRFLOW-XXX] Speed up DagBagTest cases (#3974)I noticed that many of the tests of DagBags operate on a specific DAGonly, and don't need to load the example or test dags. By not loadingthe dags we don't need to this shaves about 10-20s of test time.",3
[AIRFLOW-2912] Add Deploy and Delete operators for GCF (#3969)Both Deploy and Delete operators interact with GoogleCloud Functions to manage functions. Both are idempotentand make use of GcfHook - hook that encapsulatescommunication with GCP over GCP API.,1
[AIRFLOW-1390] Update Alembic to 0.9 (#3935),5
[AIRFLOW-2238] Update PR tool to remove outdated info (#3978),5
"[AIRFLOW-XXX] Don't spam test logs with ""bad cron expression"" messages (#3973)We needed these test dags to check the behaviour of invalid cronexpressions, but by default we were loading them every time we create aDagBag (which many, many tests to).Instead we ignore these known-bad dags by default, and the test checkingthose (tests/models.py:DagBagTest.test_process_file_cron_validity_check)is already explicitly processing those DAGs directly, so it remainstested.",3
[AIRFLOW-XXX] Fix undocumented params in S3_hookSome function parameters were undocumented. Additional docstringswere added for clarity.,1
"[AIRFLOW-3079] Improve migration scripts to support MSSQL Server (#3964)There were two problems for MSSQL.  First, 'timestamp' data type in MSSQL Serveris essentially a row-id, and not a timezone enabled date/time stamp. Second, alembic creates invalid SQL when applying the 0/1 constraint to boolean values. MSSQL shouldenforce this constraint by simply asserting a boolean value.",3
[AIRFLOW-XXX] Add DoorDash to README.md (#3980)DoorDash uses Airflow https://softwareengineeringdaily.com/2018/09/28/doordash/,1
[AIRFLOW-3062] Add Qubole in integration docs (#3946),2
[AIRFLOW-3129] Improve test coverage of airflow.models. (#3982),3
"[AIRFLOW-2574] Cope with '%' in SQLA DSN when running migrations (#3787)Alembic uses a ConfigParser like Airflow does, and ""%% is a specialvalue in there, so we need to escape it. As per the Alembic docs:> Note that this value is passed to ConfigParser.set, which supports> variable interpolation using pyformat (e.g. `%(some_value)s`). A raw> percent sign not part of an interpolation symbol must therefore be> escaped, e.g. `%%`",1
[AIRFLOW-3137] Make ProxyFix middleware optional. (#3983)The ProxyFix middleware should only be used when airflow is runningbehind a trusted proxy. This patch adds a `USE_PROXY_FIX` flag thatdefaults to `False`.,0
[AIRFLOW-3004] Add config disabling scheduler cron (#3899),5
[AIRFLOW-3103][AIRFLOW-3147] Update flask-appbuilder (#3937),5
[AIRFLOW-XXX] Fixing the issue in Documentation (#3998)Fixing the operator name from DataFlowOperation  to DataFlowJavaOperator  in Documentation,2
[AIRFLOW-3088] Include slack-compatible emoji image,5
[AIRFLOW-3161] fix TaskInstance log link in RBAC UI,2
"[AIRFLOW-3148] Remove unnecessary arg ""parameters"" in RedshiftToS3Transfer (#3995)""Parameters"" are used to help render the SQL command.But in this operator, only ""schema"" and ""table"" are needed.There is no SQL command to render.By checking the code,we can also find argument""parameters"" is never really used.(Fix a minor issue in the docstring as well)",2
[AIRFLOW-3159] Update GCS logging docs for latest code (#3952),3
[AIRFLOW-XXX] Fix  airflow.models.DAG docstring mistakeCloses #4004 from Sambeth/sambeth,2
[AIRFLOW-XXX] Adding Home Depot as users of Apache airflow (#4013)* Adding Home Depot as users of Apache airflow,1
[AIRFLOW-XXX] Added ThoughtWorks as user of Airflow in README (#4012),1
[AIRFLOW-XXX] Added DataCamp to list of companies in README (#4009),5
[AIRFLOW-3165] Document interpolation of '%' and warn (#4007),2
[AIRFLOW-3099] Complete list of optional airflow.cfg sections (#4002),5
[AIRFLOW-3162] Fix HttpHook URL parse error when port is specified (#4001),0
[AIRFLOW-3055] add get_dataset and get_datasets_list to bigquery_hook (#3894)* [AIRFLOW-3055] add get_dataset and get_datasets_list to bigquery_hook,1
[AIRFLOW-3141] Add missing missing sensor tests. (#3991),3
[AIRFLOW-XXX] Fix wrong {{ next_ds }} description (#4017),0
[AIRFLOW-XXX] Fix Typo in SFTPOperator docstring (#4016),2
"[AIRFLOW-3139] include parameters into log.info in SQL operators, if any (#3986)For all SQL-operators based on DbApiHook, sql command itself is printedinto log.info. But if parameters are used for the sql command, theparameters would not be included in the printing. This makes the logless useful.This commit ensures that the parameters are also printed into thelog.info, if any.",5
[AIRFLOW-XXX] Include Danamica in list of companies using Airflow (#4019),1
[AIRFLOW-XXX] Update manage-connections.rst (#4020)Explain how to connect with MySQL,5
[AIRFLOW-XXX] Add CarLabs to companies list (#4021),1
[AIRFLOW-3175] Fix docstring format in airflow/jobs.py (#4025)These docstrings could not parsed properly in Sphinx syntax,2
"[AIRFLOW-3086] Add extras group for google auth to setup.py. (#3917)To clarify installation instructions for the google auth backend, add aninstall group to `setup.py` that installs dependencies google auth via`pip install apache-airflow[google_auth]`.",1
[AIRFLOW-XXX] Include Pagar.me in list of users of Airflow (#4026),1
"[AIRFLOW-3173] Add _cmd options for password config options (#4024)There were a few more ""password"" config options added over the last fewmonths that didn't have _cmd options. Any config option that is apassword should be able to be provided via a _cmd version.",1
"[AIRFLOW-3078] Basic operators for Google Compute Engine (#4022)Add GceInstanceStartOperator, GceInstanceStopOperator and GceSetMachineTypeOperator.Each operator includes:- core logic- input params validation- unit tests- presence in the example DAG- docstrings- How-to and Integration documentationAdditionally, in GceHook error checking if response is 200 OK was added:Some types of errors are only visible in the response's ""error"" fieldand the overall HTTP response is 200 OK.That is why apart from checking if status is ""done"" we also checkif ""error"" is empty, and if not an exception is raised with errormessage extracted from the ""error"" field of the response.In this commit we also separated out Body Field Validator toseparate module in tools - this way it can be reused betweenvarious GCP operators, it has proven to be usable in at leasttwo of them now.Co-authored-by: sprzedwojski <szymon.przedwojski@polidea.com>Co-authored-by: potiuk <jarek.potiuk@polidea.com>",1
[AIRFLOW-3168] More resillient database use in CI (#4014)Make sure mysql is available before calling it in CI,1
"[AIRFLOW-3177] Change scheduler_heartbeat from gauge to counter (#4027)This updates the scheduler_heartbeat metric from a gauge to a counter tobetter support the statsd_exporter for usage with Prometheus. A counterallows users to track the rate of the heartbeat, and integrates with theexporter better. A crashing or down scheduler will no longer emit themetric, but the statsd_exporter will continue to show a 1 for the metricvalue. This fixes that issue because a counter will continually change,and the lack of change indicates an issue with the scheduler.Add statsd change notice in UPDATING.md",5
[AIRFLOW-2956] Add kubernetes tolerations (#3806),1
"[AIRFLOW-3183] Fix bug in DagFileProcessorManager.max_runs_reached() (#4031)The condition is intended to ensure the functionwill return False if any file's run_count is still smallerthan max_run. But the operator used here is ""!="".Instead, it should be ""<"".This is because in DagFileProcessorManager,there is no statement helping limit the upperlimit of run_count. It's possible thatfiles' run_count will be bigger than max_run.In such case, max_runs_reached() methodmay fail its purpose.",0
"[AIRFLOW-3099] Don't ever warn about missing sections of config (#4028)Rather than looping through and setting each config variableindividually, and having to know which sections are optional and whicharen't, instead we can just call a single function on ConfigParser andit will read the config from the dict, and more importantly here, nevererror about missing sections - it will just create them as needed.",1
[AIRFLOW-1837] Respect task start_date when different from dag's (#4010)Currently task instances get created and scheduled based on the DAG'sstart date rather than their own.  This commit adds a check beforecreating a task instance to see that the start date is not afterthe execution date.,5
"[AIRFLOW-3089] Drop hard-coded url scheme in google auth redirect. (#3919)The google auth provider hard-codes the `_scheme` in the callback url to`https` so that airflow generates correct urls when run behind a proxythat terminates tls. But this means that google auth can't be used whenrunning without https--for example, during local development. Also,hard-coding `_scheme` isn't the correct solution to the problem ofrunning behind a proxy. Instead, the proxy should be configured to setthe `X-Forwarded-Proto` header to `https`; Flask interprets this headerand generates the appropriate callback url without hard-coding thescheme.",1
[AIRFLOW-XXX] Add Grab to companies list (#4041),1
"[AIRFLOW-3178] Handle percents signs in configs for airflow run (#4029)* [AIRFLOW-3178] Don't mask defaults() function from ConfigParserConfigParser (the base class for AirflowConfigParser) expects defaults()to be a function - so when we re-assign it to be a property some of themethods from ConfigParser no longer work.* [AIRFLOW-3178] Correctly escape percent signs when creating temp configOtherwise we have a problem when we come to use those values.* [AIRFLOW-3178] Use os.chmod instead of shelling outThere's no need to run another process for a built in Python function.This also removes a possible race condition that would make temporaryconfig file be readable by more than the airflow or run-as userThe exact behaviour would depend on the umask we run under, and theprimary group of our user, likely this would mean the file was readablyby members of the airflow group (which in most cases would be just theairflow user). To remove any such possibility we chmod the filebefore we write to it",2
[AIRFLOW-2216] Use profile for AWS hook if S3 config file provided in aws_default connection extra parameters (#4011)Use profile for AWS hook if S3 config file provided in aws_default connection extra parametersAdd test to validate profile set,1
[AIRFLOW-3001] Add index 'ti_dag_date' to taskinstance (#3885)To optimize query performance,5
"[AIRFLOW-2794] Add WasbDeleteBlobOperator (#3961)Deleting Azure blob is now supported. Either single blobs can bedeleted, or one can choose to supply a prefix, in which case onecan match multiple blobs to be deleted.",4
[AIRFLOW-3138] Use current data type for migrations (#3985)* Use timestamp instead of timestamp with timezone for migration.,1
[AIRFLOW-393] Add callback for FTP downloads (#2372),1
[AIRFLOW-3119] Enable debugging with Celery(#3950)This will enable --loglevel when launching a celery worker and inherit that LOGGING_LEVEL setting from airflow.cfg,5
[AIRFLOW-3112] Make SFTP hook to inherit SSH hook (#3945)This is to aline the arguments of SFTP hook with SSH hook,1
[AIRFLOW-3195] Log query and task_id in druid-hook (#4018)Log query and task_id in druid-hook,1
[AIRFLOW-3187] Update airflow.gif file with a slower version (#4033),2
[AIRFLOW-2789] Create single node DataProc cluster (#4015)Create single node cluster - infer from num_workers,1
[AIRFLOW-3190] Make flake8 compliant (#4035)Enforce Flake8 over the entire project,1
[AIRFLOW-3192] Remove deprecated post_execute logic (#4040),2
[AIRFLOW-3201] Add sid88in to Glassdoor (#4045),1
"[AIRFLOW-2932] GoogleCloudStorageHook - allow compression of file (#3893)- Add gzip functionality to GoogleCloudStorageHook.upload- Resolve docstring mistype, added additional information to  tell user that there is option to compress- Add test case for file_to_gcs",2
[AIRFLOW-3174] Refine Docstring for SQL Operators & Hooks (#4043)Add missing arguments,1
"[AIRFLOW-3197] EMRHook is missing new parameters of the AWS API (#4044)Allow passing any params to the CreateJobFlow API, so that we don't haveto stay up to date with AWS api changes.",4
[AIRFLOW-3190] Make flake8 compliantOne voilation that slipped in by PR that didn't rebase ontolatest master,3
[AIRFLOW-3202] add missing documentation for AWS hooks/operator (#4048)Add missing documentation for AWS hooks/operator,1
"[AIRFLOW-3064] Show logs/output from operators in `airflow test` command (#4051)The logging rejig we did a for 1.10 ened with us the output fromoperators/tasks when running `airflow test` not going anywhere (becausewe have call `ti.init_run_context()` the FileTaskHandler doens't have afilename, so logs don't go anywhere)",2
[AIRFLOW-XXX] Fix BashOperator Docstring (#4052),2
[AIRFLOW-XXX] Add missing docstring in GCS Hook & Operator (#4053),1
"Revert ""[AIRFLOW-XXX] Add Grab to companies list (#4041)""This reverts commit eaafff295f6d67fa35a3b379f9b7c4ecbe0cbf08.",4
[AIRFLOW-3190] Make flake8 compliantOne voilation that slipped in by PR that didn't rebase ontolatest master,3
"[AIRFLOW-3069] Log all output of the S3 file transform script (#3914)The output of the process spawned by S3FileTransformOperator is notproperly decoded, which makes reading logs rather difficult. Additonally,the stderr stream is only shown when process exit code is not equal to 0.- Send both output streams (stdout & stderr) to the logger.- Decode the output, so that new lines can be displayed correctly.- Include process exit code in the exception message, if the process fails.- Remove a potential deadlock, caused by `stderr=subprocess.PIPE`.- Don't load all output into memory.",1
[AIRFLOW-XXX] Fix GCS Operator docstrings (#4054),2
[AIRFLOW-3155] Add ability to filter by a last modified time in GCS Operator (#4008)* [AIRFLOW-3155] Add ability to filter by a last modified time in GoogleCloudStorageToGoogleCloudStorageOperator,1
[AIRFLOW-3141] Handle duration for missing dag. (#3984),2
[AIRFLOW-1945] Add Autoscale config for Celery workers (#3989),1
"[AIRFLOW-843] Exception availabe in context for on_failure_callback (#2135)Store exceptions encountered executing a task in the context dict,making it available for on_failure_callback handlers.",0
[AIRFLOW-3121] Define closed property on StreamLogWriter (#3955)[AIRFLOW-3121] Define closed property on StreamLogWriter,2
[AIRFLOW-XXX] Add `BigQueryGetDataOperator` to Integration Docs (#4063),2
[AIRFLOW-3222] Remove bql keyword from BigQueryOperator (#4060)Will be removed in Apache Airflow 2.0,4
[AIRFLOW-XXX] Removed spot in Airflow logo (#4065),2
"[AIRFLOW-3046] Report fail from ECS Operator when host terminates (#4039)Add check for host termination to ECS OperatorIf an ECS task fails to complete because the host it's running on is terminated, we need to raise an exception so it can be retried.",1
[AIRFLOW-3232] More readable GCF operator documentation (#4067),2
[AIRFLOW-XXX] BigQuery Hook - Minor Refactoring (#4066),4
[AIRFLOW-2993] s3_to_sftp and sftp_to_s3 operators (#3828)Add operators for transferring files between s3 and sftp.,2
[AIRFLOW-1867] Add sandbox mode and py3k bug  (#2824)- Fix sendgrid attachments bug in py3k- Add sandbox mode option- Minor style fixes- Add test,3
"[AIRFLOW-3203] Fix DockerOperator & some operator test (#4049)- For argument `image`, no need to explicitly  add ""latest"" if tag is omitted.  ""latest"" will be used by default if no  tag provided. This is handled by `docker` package itself.- Intermediate variable `cpu_shares` is not needed.- Fix wrong usage of `cpu_shares` and `cpu_shares`.  Based on  https://docker-py.readthedocs.io/en/stable/api.html#docker.api.container.ContainerApiMixin.create_host_config,  They should be an arguments of  self.cli.create_host_config()  rather than  APIClient.create_container().- Change name of the corresponding test script,  to ensure it can be discovered.- Fix the test itself.- Some other test scripts are not named properly,  which result in failure of test discovery.",3
"[AIRFLOW-3239] Fix test recovery further (#4074)Prepend ""test_"" for- tests/executors/dask_executor.py- tests/security/kerberos.py",3
[AIRFLOW-3238] Fix models.DAG to deactivate unknown DAGs on initdb (#4073)Unknown dags are now deactivated on initdb,5
[AIRFLOW-461]  Support autodetected schemas in BigQuery run_load (#3880),1
[AIRFLOW-461] Restore parameter position for BQ run_load method (#4077),1
"Revert ""[AIRFLOW-461] Restore parameter position for BQ run_load method (#4077)""This reverts commit cc360e5fd470794b58124fc5433cf541e1ea766b.",4
[AIRFLOW-XXX] Add Surfline to companies list (#4079),1
[AIRFLOW-XXX] Add Neoway to companies list (#4081),1
[AIRFLOW-520] Fix Version Info in Flask UI (#4072),5
[AIRFLOW-2744] Allow RBAC to accept plugins for views and links. (#4036)Airflow Users that wish to create plugins for the new www_rbac UIcan not add plugin views or links. This PR fixes that by lettinga user specify their plugins for www_rbac and maintains backwardscompatibility with the existing plugins system.,5
Add Betterment to companies list (#4088),1
[AIRFLOW-3235] Add list function in AzureDataLakeHook (#4070),5
[AIRFLOW-1508] Add SKIPPED to task states. (#4059),1
"[AIRFLOW-3257] Pin flake8 version to avoid checks changing over time (#4092)Flake8 3.6.0 was just released and it introduced some new checks thatdidn't exist before. As a result all of our CI pipelines are now failing.To avoid this happening in future we should pin the version of flake8to 3.5.0 (currently this is in tox.ini, and setup.py)",1
"[AIRFLOW-3241] Remove Invalid template ext in GCS Sensors (#4076)- Remove `template_ext = ('.sql',)`- Fix Docstrings (Incorrect connection name and indentation)",2
[AIRFLOW-XXX] Add Jetlore to companies (#4096),1
[AIRFLOW-3237] Refactor example DAGs (#4071),2
[Airflow-2760] Decouple DAG parsing loop from scheduler loop (#3873),2
[AIRFLOW-839] Fix status logging for docker operator (#4095),1
[AIRFLOW-1970] Let empty Fernet key or special `no encryption` phrase. (#4038)Once the user has installed Fernet package then the application enforces setting valid Fernet key.This change will alter this behavior into letting empty Fernet key or special `no encryption` phrase and interpreting those two cases as no encryption desirable.,4
[AIRFLOW-XXX] Add Strava to airflow users list (#4100),1
[AIRFLOW-XXX] add DigitalOcean to official Airflow users list (#4099),1
[AIRFLOW-XXX] Fix incorrect statement in contributing guide (#4104),0
"[AIRFLOW-3265] Support for ""unix_socket"" extra for MySQL hook (#4110)MySQL hook does not support ""unix_socket"" extra - which allowsto specify a different location of Linux socket than the default one.This is a blocker for tools like cloud-sql-proxy thatcreates sockets in an arbitrary place:https://mysqlclient.readthedocs.io/user_guide.html",2
"[AIRFLOW-3264] URL decoding when parsing URI for connection (#4109)Full URL decoding is performed now when parsing differentcomponents of URI for connection. This enables to configurepaths to sockets including (for example "":"") - so faronly '/' (%2f) was hard-coded in hostname. This change introducesfull decoding for all components of the URI.Note that this is potentially breaking change if someone uses% in some of their AIRFLOW_CONN_ defined connections.",1
"[AIRFLOW-3049] Add extra operations for Mongo hook (#3890)This commit adds update, replace and delete operations for the Mongohook.",1
[AIRFLOW-3112] Fix SFTPHook not validating hosts by default (#4085),5
[AIRFLOW-3260] Correct misleading BigQuery error (#4098),0
"[AIRFLOW-XXX] Minor fix of docs/scheduler.rst (#4115)DaskExecutor is not mentioned in docs/scheduler.rst,while it's listed as one of the main executors in `airflow/config_templates/default_airflow.cfg`.",5
"[AIRFLOW-2703] Catch transient DB exceptions from scheduler's heartbeat it does not crash (#3650)If there is any issue in DB connection then rest of the functions take care of those exceptions but in heartbeat of scheduler, there is no handling for this kind of situation.Airflow Scheduler should not crash if a ""transient"" DB exception occurs in the heartbeat of scheduler.",5
"[AIRFLOW-3136] Add retry_number to TaskInstance.key to avoid race condition (#3994) We were seeing an intermittent issue where executor reports task instance finished while task says it's in queue state, it was due to a race condition between scheduler which was clearing event_buffer in _process_executor_events method in jobs.py executor was about to put next_retry task's status as running which was failed in previous try. So, we thought to add retry_number as the member of TaskInstance key property.",5
"[AIRFLOW-3231] Basic operators for Google Cloud SQL (#4097)Add CloudSqlInstanceInsertOperator, CloudSqlInstancePatchOperator and CloudSqlInstanceDeleteOperator.Each operator includes:- core logic- input params validation- unit tests- presence in the example DAG- docstrings- How-to and Integration documentationAdditionally, small improvements to GcpBodyFieldValidator were made:- add simple list validation capability (type=""list"")- introduced parameter allow_empty, which can be set to Falseto test for non-emptiness of a string instead of specifyinga regexp.Co-authored-by: sprzedwojski <szymon.przedwojski@polidea.com>Co-authored-by: potiuk <jarek.potiuk@polidea.com>",3
[AIRFLOW-3236] Create AzureDataLakeStorageListOperator (#4094),5
"[AIRFLOW-2524] Update SageMaker hook and operators (#4091)This re-works the SageMaker functionality in Airflow to be more complete, and more useful for the kinds of operations that SageMaker supports.We removed some files and operators here, but these were only added after the last release so we don't need to worry about any sort of back-compat.",1
[AIRFLOW-3287] Moving database clean-up code into the CoreTest.tearDown() (#4122),3
[AIRFLOW-3262] Add param to log response when using SimpleHttpOperator (#4102)* [AIRFLOW-3262] Add param to log response when using SimpleHttpOperator,1
[AIRFLOW-3276] Cloud SQL: database create / patch / delete operators (#4124),1
"[AIRFLOW-3288] Add SNS integration (#4123)Provides a hook and an operator for publishing Amazon SNS messages.Useful for integrating various Amazon services (SQS, Lambda) andsending regular notifications (e-mail, SMS, ...).",1
"[AIRFLOW-3295] Fix potential security issue in DaskExecutor (#4128)When user decides to use TLS/SSL encryptionfor DaskExecutor communications,`Distributed.Security` object will be created.However, argument `require_encryption` is missedto be set to `True` (its default value is `False`).This may fail the TLS/SSL encryption setting-up.",1
[AIRFLOW-3294] Update connections form and integration docs (#4129),2
"[AIRFLOW-3277] Correctly observe DST transitions for cron (#4117)`following_schedule` converts to naive time by using thelocal time zone. In case of a DST transition, say 3AM -> 2AM(""summer time to winter time"") we incorrectly re-appliedthe timezone information which meant that a ""CEST -> CEST""could happen instead of a ""CEST -> CET"". This resultedin infinite loops.",5
"[AIRFLOW-3205] Support multipart uploads to GCS (#4084)* [AIRFLOW-3205] Support multipart uploads to GCSCloud Storage supports resumable/multipart uploads for large files,which can be used to avoid limitations on the size of a single HTTPrequest, or by adding a retry behaviour, increase the reliability oflarge transfers.* [AIRFLOW-3205] Use only the multipart keywordThis removes the chunksize keyword, using instead multipart=True for adefault chunk size or multipart=int to override the default.",1
[AIRFLOW-3239] Enable existing CI tests (#4131)1. Renamed files:- tests/configuration.py → tests/test_configuration.py- tests/impersonation.py → tests/test_impersonation.py- tests/utils.py → tests/test_utils.py- tests/operators/operators.py → tests/operators/test_operators.py- tests/operators/bash_operator.py → tests/operators/test_bash_operator.py- tests/jobs.py → tests/test_jobs.py2. Updated tests/__init__.py accordingly3. Fixed database-specific tests in tests/operators/test_operators.py4. Fixed issue in tests/operators/test_bash_operator.py,3
[AIRFLOW-3268] Better handling of extras field in MySQL connection (#4113),1
[AIRFLOW-3022] Add volume mount to KubernetesExecutorConfig (#3855)Added volumes and volume_mounts to the KubernetesExecutorConfig so`volumes` or `secrets` can be mount to worker pod.,1
"[AIRFLOW-2865] Call success_callback before updating task state (#4082)In cases where the success callback takes variabletime, it's possible for it to interrupted by the heartbeat process.This is because the heartbeat process looks for tasks that are nolonger in the ""running"" state but are still executing and reaps them.This commit reverses the order of callback invocation and stateupdating so that the ""SUCCESS"" state for the task isn't committedto the database until after the success callback has finished.",5
[AIRFLOW-3160] Load latest_dagruns asynchronously (#4005),3
[AIRFLOW-3218] add support for poking a whole DAG (#4058)Add support for poking a whole DAG,2
[AIRFLOW-2780] Add IMAP Hook to retrieve email attachments (#4119)[AIRFLOW-2780] Add IMAP Hook to retrieve email attachments- Add has_mail_attachments to check if there are mail attachments in the given mailbox with the given attachment name- Add retrieve_mail_attachments to download the attachments to a local directory- Add some test cases but more are coming- Add license header- Change retrieve_mail_attachments to download_mail_attachments- Add retrieve_mail_attachments that return a list of tuple containing the attachments found- Change IMAP4_SSL close() method to be called after retrieving the attachments and not before logging out- Change test_connect to not check for close method because no mail folder will be opened when only connecting- Add some test cases that are still in WIP- Fixes a bug causing multiple attachments in a single mail not being correctly added to the all mails attachments- Fixes a bug where MailPart is_attachment always returns None- Add logging when an attachment has been found that matches the name- Add more test cases with sample mail,3
[AIRFLOW-2192] Allow non-latin1 usernames with MySQL backend by adding a SQL_ENGINE_ENCODING param and default to UTF-8 (#4087)Compromised of:Since we have unicode_literals importred and the engine arguments must be strings in Python2 explicitly make 'utf-8' a string.replace bare exception with conf.AirflowConfigException for missing value.It's just got for strings apparently.Add utf-8 to default_airflow.cfg - question do I still need the try try/except block or can we depend on defaults (I note some have both).Get rid of try/except block and depend on default_airflow.cfgUse __str__ since calling str just gives us back a newstr as well.Test that a panda user can be saved.,1
"[AIRFLOW-3193] Pin docker requirement version (#4130)Since the change of AIRFLOW-3203, the dockerpy client has upgradedactually. Therefore, the requirements config should upgrade also toavoid potentially misleading.",5
[AIRFLOW-3132] Enable specifying auto_remove option for DockerOperator (#3977),2
[AIRFLOW-3262] Update SimpleHttpOpTests to check Example.com (#4135)Update SimpleHttpOpTests to check Example.com instead of Google.com so that the ip doesn't get banned.,1
[AIRFLOW-3190] Make flake8 compliant,1
[AIRFLOW-2715] Use region setting when launching Dataflow templates (#4125),5
"Revert ""[AIRFLOW-2715] Use region setting when launching Dataflow templates (#4125)""This reverts commit 79e4faebfdc4396a44a282aab0615f4cca3b69be.",4
[AIRFLOW-3301] Update DockerOperator CI test for PR #3977 (#4138),3
"[AIRFLOW-XXX] Fix Docstrings in Hooks, Sensors & Operators (#4137)",1
[AIRFLOW-XXX] Use mocking in SimpleHttpOperator tests (#4144)This changes the tests in [AIRFLOW-3262]/(#4135) to use requests_mockrather than making actual HTTP requests as we have had this test fail onTravis with connection refused.,1
[AIRFLOW-XXX] Fix flake8 errors from #4144,0
[AIRFLOW-XXX] Update chat channel details from gitter to slack (#4149),5
"Revert ""[AIRFLOW-3160] Load latest_dagruns asynchronously (#4005)"" (#4145)* Revert ""[AIRFLOW-3190] Make flake8 compliant""This reverts commit 1691c988bd398c7d8e2446991f5246223bdce37b.* Revert ""[AIRFLOW-3160] Load latest_dagruns asynchronously (#4005)""This reverts commit 0287cceed8137823743497b7e11f19ef35bacd9d.",4
[AIRFLOW-3302] Small CSS fixes (#4140)* Don't highlight logout button when viewing Log tab of a task* Align Airflow logo to the center of the login page,2
[AIRFLOW-3062] Add QDS dependency in readthedocs (#4148),2
[AIRFLOW-XXX] Pin Flask App Builder to Fix CI (#4150),0
"[AIRFLOW-3297] added more failure states to the EMR step sensor (#4152)Previously, the states 'PENDING_CANCELLED' and 'INTERRUPTED' resulted in theAirflow task being marked as successful. This PR fixed this by mentioning those statesas NON_TERMINAL_STATES and as FAILED_STATE, accordingly.",0
AIRFLOW-3259] Fix internal server error when displaying charts (#4114)This is caused by the fact that the function 'sort' is no longer a part of Dataframe in pandas and is still used in the code base. It has ever since been replaced by 'sort_values'. Replacing the function gets the chart display back to its normal behaviour.,2
"[AIRFLOW-2698] Simplify Kerberos code (#3563)Some functions were not used. On top of that, the`principal_from_username` function was getting the wrong config value(""security"" instead of ""kerberos"").Since the results were only used by `kerberos.checkPassword`, and thefunction can cope with needing a realm in the `username` when `realm` isprovided, we removed the `principal_from_username` function altogether.",1
[AIRFLOW-3190] Make flake8 compliant,1
[AIRFLOW-2903] Change default owner to `airflow` (#4151)This makes the default owner when one is not provided in the dag the same as in the examples and docs for consistency.,2
"[AIRFLOW-2799] Fix filtering UI objects by datetime (#4061)Our conversion to Timezone-aware date times in 1.10 missed this case -any filter on a date time would blow up with `naive datetime isdisallowed` - this makes our datetime filters timezone aware now (theyrespect the default_timezone, and they accept timezones in input eventhough the UI won't let you craft those.)I manually tested this by changing query parameters - a value of`2018-10-30+01%3A05%3A00%2B01:00` matched against an execution date of00:05:00+00:00",5
[AIRFLOW-XXX] Add GeneCards to companies list (#4159),1
"[AIRFLOW-2524] More AWS SageMaker operators, sensors for model, endpoint-config and endpoint (#4126)",5
Add HAVAN to companies list (#4168),1
[AIRFLOW-3164] Verify server certificate when connecting to LDAP (#4006),5
[AIRFLOW-3307] Upgrade rbac node deps via `npm audit fix`. (#4147)Also drop the unused istanbul package.,1
Add AirDNA as Airflow user (#4171),1
"[AIRFLOW-XXX] Speed up RBAC view tests (#4162)Not re-creating the FAB app ones per test functions took the run time ofthe TestAirflowBaseViews from 223s down to 53s on my laptop, _and_ madeit only print the deprecation warning (fixed in another PR already open)once instead of 10+ times.",0
[AIRFLOW-3315] Add ImapAttachmentSensor (#4161)- update license header in imap_hook and test_imap_hook,3
[AIRFLOW-XXX] Add missing docs for SNS classes (#4155),2
[AIRFLOW-XXX] Update Contributing Guide - Git Hooks (#4120)- changes pre-commit example to use methods- adds activating virtual env for python to run things like flake8 locally- changes pre-commit file to use set -e command to instantly exit if any non-zero error occurs- changes flake8 call to lint the repo instead of not only the changes files,2
"[AIRFLOW-3325] Fix UI Page DAGs-column 'Recent Tasks' display issue (#4173)A new task state ""Skipped"" was added while UI was not updated accordingly.This makes the last circle in column ""Recent Tasks"" not displayed completely.Fixed both www/ and www_raac/",0
"[AIRFLOW-3306] Disable flask-sqlalchemy modification tracking. (#4146)By default, flask-sqlalchemy tracks model changes for its event system, whichadds some overhead. Since I don't think we're using the flask-sqlalchemyevent system, we should be able to turn off modification tracking and improveperformance.",1
"[AIRFLOW-XXX] Airflow 1.10.1 release notes in UPDATING.mdSimply adding a header in the right place, as through dumb luck we hadthe changes in the right order.",4
[AIRFLOW-3330] Add Civey to the list of Airflow users (#4176),1
"[AIRFLOW-2779] Add project version to license (#4177)The Incubator PMC requested that we add version information to thelicense declarations to cope with the possibility of projects changinglicenses on us in the future.Where possible I have worked out the version we are actually using, butwhere that hasn't been possible and the project is still on the samelicense (they all were) I have taken the latest version as of right now[ci-skip]",3
[AIRFLOW-3220] Add Instance Group Manager Operators for GCE (#4167),1
[AIRFLOW-XXX] Correct schedule_interval in Scheduler docs (#4157),2
[AIRFLOW-3275] Add Google Cloud SQL Query operator (#4170),1
[AIRFLOW-XXX] Add Iflix to Airflow Users (#4180),1
[AIRFLOW-3245] fix list processing in resolve_template_files (#4086)* [AIRFLOW-3245] fix list processing in resolve_template_files* [AIRFLOW-3245] add tests* [AIRFLOW-3245] modify tests,3
[AIRFLOW-2779] Add license headers to doc files (#4178)This adds ASF license headers to all the .rst and .md files with theexception of the Pull Request template (as that is included verbatimwhen opening a Pull Request on Github which would be messy),2
[AIRFLOW-3323] Support HTTP basic authentication for Airflow Flower (#4166)The current `airflow flower` doesn't come with any authentication.This may make essential information exposed in an untrusted environment.This commit add support to HTTP basic authentication for Airflow FlowerRef:https://flower.readthedocs.io/en/latest/auth.html,3
[AIRFLOW-XXX] Remove duplicated line in Changelog (#4181),4
[AIRFLOW-XXX] Fix typo in plugin docs (#4183),2
[AIRFLOW-XXX] Revise template variables documentation (#4172)Updated documentation to elaborate on the (yesterday|tomorrow)_.*variables' relations to the execution date.,5
[AIRFLOW-1262] Allow configuration of email alert subject and body (#2338),5
[AIRFLOW-502] Add docs on BashOperator success/failure conditions (#4184),0
[AIRFLOW-3339] Correctly get DAG timezone when start_date in default_args (#4186),5
[AIRFLOW-XXX] Fix flake8 failure from #4184,0
"[AIRFLOW-3343] Update DockerOperator for Docker-py 3.0.0 API changes (#4187)The API of `wait()` changed to return a dict, not just a number so thisOperator wasn't actually working, but the tests were passing because thereturn was mocked in-correctly.I also removed `shm_size` from kwargs passed to BaseOperator to avoidthe deprecation warning about unknown args.",2
[AIRFLOW-3309] Add MongoDB connection (#4154),5
[AIRFLOW-3271] Fix issue with persistence of RBAC Permissions modified via UI (#4118),0
[AIRFLOW-3353] Pin redis verison (#4195),5
[AIRFLOW-3345] Add Google Cloud Storage (GCS) operators for ACL (#4192)Add 2 operators for adding ACL entries to GCS buckets and objects:- GoogleCloudStorageBucketCreateAclEntryOperator- GoogleCloudStorageObjectCreateAclEntryOperator,1
[AIRFLOW-3350] Explain how to use Bitshift Composition with lists (#4191),1
[AIRFLOW-251] Add option SQL_ALCHEMY_SCHEMA parameter to specify schema for metadata (#4199),5
[AIRFLOW-3266] Add AWS Athena Hook and Operator (#4111)Provides AWS Athena hook and operator to submit Athena(presto) queries on AWS.Authored-by: Phanindhra <phani8996@gmail.com>,1
[AIRFLOW-3251] KubernetesPodOperator now uses 'image_pull_secrets' argument when creating Pods (#4188),1
"[AIRFLOW-3355] Fix BigQueryCursor.execute to work with Python3 (#4198)BigQueryCursor.execute uses dict.iteritems internally,so it fails with Python3 if binding parameters areprovided. This PR fixes this problem.",0
[AIRFLOW-3352] Fix expose_config not honoured on RBAC UI (#4194),5
[AIRFLOW-3346] Add hook and operator for GCP transfer service (#4189),1
[AIRFLOW-XXX] Fix incorrect URL for Task Tries and Task Duration (#4202),0
[AIRFLOW-XXX] Add Etsy to companies list (#4204),1
[AIRFLOW-3353] Upgrade redis client (#4203),5
"Revert ""[AIRFLOW-3353] Upgrade redis client (#4203)"" (#4205)This reverts commit ae629872c5d3d616e189ff9d6d2c8948de66a077.",4
[AIRFLOW-3233] Fix deletion of DAGs in the UI (#4069),2
[AIRFLOW-3332] Add method to allow inserting rows into BQ table (#4179),1
[AIRFLOW-XXX] Remove spots in all Airflow logos (#4206),2
[AIRFLOW-XXX] Update readme for lyft (#4208),5
[AIRFLOW-2966] Catch ApiException in the Kubernetes Executor (#3960)Creating a pod that exceeds a namespace's resource quota throws anApiException. This change catches the exception and the task isre-queued inside the Executor instead of killing the scheduler.,4
[AIRFLOW-1252] API accept JSON when invoking a trigger dag (#2334),2
"Revert ""[AIRFLOW-2966] Catch ApiException in the Kubernetes Executor (#3960)""This reverts commit 03de9ee155b79f83c3d78c77af66c7d07572dc4b.",4
[AIRFLOW-3361] Log the task_id in the PendingDeprecationWarning from BaseOperator (#4030)Log the task_id in the PendingDeprecationWarning when passing invalid arguments.,4
[AIRFLOW-XXX] Don't publish md5 sigs as part of release (#4210)Apache recommend against publishing MD5 files now as they are relativelyeasy to collide and shouldn't be trusted anymore,2
"AIRFLOW-XXX Fix copy&paste mistake (#4212)In emr_create_job_flow_operator.py the :type clearly mismatches withthe :param name, suggesting a copy&paste mistake.",2
[AIRFLOW-3359] Add option to pass customer encryption keys to Dataproc (#4200),5
[AIRFLOW-2966] Catch ApiException in the Kubernetes Executor (#4209)Creating a pod that exceeds a namespace's resource quota throws anApiException. This change catches the exception and the task isre-queued inside the Executor instead of killing the scheduler.,4
[AIRFLOW-3308] Fix plugins import (#4153)Revert #3906 partially.,4
"[AIRFLOW-XXX] Better instructions for airflow flower (#4214)* Better instructions for airflow flowerIt is not clear in the documentation that you need to have flower installed to successful run airflow flower. If you don't have flower installed, running airflow flower will show the following error which is not of much help:airflow flower                                                                                       [2018-11-20 17:01:14,836] {__init__.py:51} INFO - Using executor SequentialExecutor                                                      Traceback (most recent call last):                                                                                                         File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/bin/airflow"", line 32, in <module>                         args.func(args)                                                                                                                        File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/site-packages/airflow/utils/cli.py"", line 74, in wrapper                                                                                                                              return f(*args, **kwargs)                                                                                                              File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/site-packages/airflow/bin/cli.py"", line 1221, in flower                                                                                                                               broka, address, port, api, flower_conf, url_prefix])                                                                                   File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py"", line 559, in execvp                  _execvpe(file, args)                                                                                                                   File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py"", line 604, in _execvpe                raise last_exc.with_traceback(tb)                                                                                                      File ""/mnt/secondary/workspace/f4/typo-backend/pipelines/model-pipeline/airflow/lib/python3.6/os.py"", line 594, in _execvpe                exec_func(fullname, *argrest)                                                                                                        FileNotFoundError: [Errno 2] No such file or directory* Update use-celery.rst",1
[AIRFLOW-3375] Support returning multiple tasks with BranchPythonOperator (#4215),1
[AIRFLOW-3380] Add metrics documentation (#4219),2
[AIRFLOW-XXX] Update NOTICE file per suggestion (#4220),2
[AIRFLOW-XXX] Update changelog for 1.10.1,4
[AIRFLOW-3371] BigQueryHook's Ability to Create View (#4213),1
[AIRFLOW-3213] Create ADLS to GCS operator (#4134),1
[AIRFLOW-3382] Fix incorrect docstring in DatastoreHook (#4222)Correct docstring in DatastoreHook,5
[AIRFLOW-3263] Ignore exception when 'run' kills already killed job (#4108)Sometimes when you run tasks from command line you get exit code = 1 dueto race condition (job runner tries to get process group from theprocess that has already been terminated in the meantime),1
[AIRFLOW-3348] update run statistics on dag refresh (#4197)* [AIRFLOW-3348] update run statistics on dag refresh,2
[AIRFLOW-1739] Resolve TestDbApiHook naming ambiguity (#2709),5
[AIRFLOW-3336] Add new TriggerRule for 0 upstream failures (#4182)Add new TriggerRule that triggers only if all upstream do not fail (success or skipped tasks are allowed),1
"[AIRFLOW-2715] Use region setting when launching Dataflow templates (#4139)To launch an instance of a Dataflow template in the configured region,the API service.projects().locations().teplates() instead ofservice.projects().templates() has to be used. Otherwise, all jobs willalways be started in us-central1.In case there is no region configured, the default region `us-central1`will get picked up.To make it even worse, the polling for the job status already honors theregion parameter and will search for the job in the wrong region in thecurrent implementation. Because the job's status is not found, thecorresponding Airflow task will hang.",1
[AIRFLOW-1196][AIRFLOW-2399] Add templated field in TriggerDagRunOperator (#4228)* [AIRFLOW-1196][AIRFLOW-2399] Make trigger_dag_id a templated field for TriggerDagRunOperator* Update dagrun_operator.py,2
[AIRFLOW-1561] Fix scheduler to pick up example DAGs without other DAGs (#2635),2
[AIRFLOW-3250] Fix for Redis Hook for not authorised connection calls (#4090)Password stay None value and not None (str) in case there is no password set through webadmin interfaces.This is fix for connections for Redis that not expect autorisation from clients.,0
[AIRFLOW-3365][AIRFLOW-3366] Allow celery_broker_transport_options to be set with environment variables (#4211)* [AIRFLOW-3365] Add visibility timeout by key* [AIRFLOW-3366] Make getsection scan env variables,1
[AIRFLOW-2642] fix wrong value git-sync initcontainer env GIT_SYNC_ROOT (#3519),5
[AIRFLOW-3378] KubernetesPodOperator does not delete on timeout failure (#4218)Signed-off-by: Victor Noel <victor.noel@brennus-analytics.com>,0
[AIRFLOW-XXX] Remove quotes from domains in Google Oauth (#4226)Related SO: https://stackoverflow.com/a/52528091/10638329,4
[AIRFLOW-3395] added the REST API endpoints to the doc (#4236),2
[AIRFLOW-3392] Add index on dag_id in sla_miss table (#4235)The select queries on sla_miss table produce a great % of DB traffic andthus made the DB CPU usage unnecessarily high. It would be a low hangingfruit to add an index and reduce the load.,1
"[AIRFLOW-XXX] Correct typos in UPDATING.md (#4242)Started with ""habe"", ""serever"" and ""certificiate"" needing to be:  ""have"", ""server"", and ""certificate"".Ran a check, ignoring British and US accepted spellings.Kept jargon. EG admin, aync, auth, backend, config, dag, s3, utils, etc.Took exception to: ""num of dag run"" meaning ""number of dag runs"",  ""upness"" is normally for quarks,  ""url"" being lower-case, and  sftp example having an excess file ending.Python documentation writes ""builtin"" hyphenated, cases ""PYTHONPATH"".Gave up on mixed use of ""dag"" and ""DAG"" as well as long line lengths.",2
[AIRFLOW-XXX] Add TEK to list of companies (#4240),1
"[AIRFLOW-XXX] Remove unnecessary ""# noqa"" in airflow/bin/cli.py (#4223)",4
[AIRFLOW-XXX] Replace airflow with apache-airflow (#4246),5
[AIRFLOW-3384] Allow higher versions of Sqlalchemy and Jinja2 (#4227)* [AIRFLOW-3384] Allow higher versions of SQLAlchemy and Jinja2,1
[AIRFLOW-3410] Add feature to allow Host Key Change for SSH Op (#4249),4
[AIRFLOW-XXX] Add VeeR VR to org list in README (#4250)* add VeeR VR to org list in README* 1. [VeeR VR](https://veer.tv) [[@pishilong](https://github.com/pishilong)],1
[AIRFLOW-2761] Parallelize enqueue in celery executor (#4234),5
[AIRFLOW-XXX] Add Poshmark to Companies (#4252)* Add Poshmark to Companies,1
"[AIRFLOW-3239] Fix/refine tests for api/common/experimental/ (#4255)Follow-up on [AIRFLOW-3239]Related PRs: #4074, #41311. Fix (test_)trigger_dag.py2. Fix (test_)mark_tasks.py  2-1. properly name the file  2-2. Correct the name of sample DAG  2-3. Correct the range of sample execution_dates       (earlier one conflict with the start_date of the sample DAG)  2-4. Skip for test running on MySQL       Seems something is wrong with       airflow.api.common.experimental.mark_tasks.set_state,       Corresponding test case works on Postgres & SQLite,       but fails when on MySQL (""(1062, ""Duplicate entry '110' for key 'PRIMARY'"")"").       A TODO note is added to remind us fix it for MySQL later.3. Remove unnecessary lines in test_pool.py",3
[AIRFLOW-3425] Fix setting default scope in hook (#4261),1
[AIRFLOW-3403] Add AWS Athena Sensor (#4244),1
[AIRFLOW-3396] Make sql param as required in BigQueryOperator (#4224),1
[AIRFLOW-3426] Bugfix / Correct Python Version Documentation Reference (#4259)- Change all Python 3.4 references to 3.5 as 3.4 to reflect the currently supported version within the CI test suite (2.7 and 3.5),3
[AIRFLOW-XXX] Add missing GCP operators to Docs (#4260),2
[AIRFLOW-3416] Fixes Python 3 compatibility with CloudSqlQueryOperator (#4254)Added several missing decodes on reading output from runningsubprocess (cloud_sql_proxy),1
"[AIRFLOW-3432] Add test for feature ""Delete DAG in UI"" (#4266)Related Commits:1. [AIRFLOW-2657](PR #3531)2. [AIRFLOW-3233](PR #4069)Added for both www/ and www_rbac",1
[AIRFLOW-3414] Fix reload_module in DagFileProcessorAgent (#4253),2
[AIRFLOW-XXX] Fix display of SageMaker operators/hook docs (#4263),2
[AIRFLOW-3367] Run celery integration test with redis broker. (#4207),3
[AIRFLOW-3431] Document how to report security vulnerabilities. (#4262)Wording based on Kafka's[ci-skip],2
[AIRFLOW-3434] Allows creating intermediate dirs in SFTPOperator (#4270),1
[AIRFLOW-XXX] Add Get Simpl to Companies (#4272),1
[AIRFLOW-XXX] Update kubernetes.rst (#4280)import modules to complete the example set.,1
[AIRFLOW-XXX] Add Kubernetes Dependency in Extra Packages Doc (#4281),2
[AIRFLOW-2440] Google Cloud SQL import/export operator (#4251),1
[AIRFLOW-XXX] GCP operators documentation clarifications (#4273),2
[AIRFLOW-3408] Remove outdated info from Systemd Instructions (#4269),5
[AIRFLOW-3322] Update QuboleHook to fetch args dynamically from qds_sdk (#4165),1
[AIRFLOW-XXX] Add Docstrings as Requirement in Contribution template (#4282)* [AIRFLOW-XXX] Add Docstrings as Requirement in Contribution template* Update PULL_REQUEST_TEMPLATE.md,5
"[AIRFLOW-3406] Implement an Azure CosmosDB operator (#4265)Add an operator and hook to manipulate and use AzureCosmosDB documents, including creation, deletion, andupdating documents and collections.Includes sensor to detect documents being added to acollection.",1
[AIRFLOW-2524] Add SageMaker doc to AWS integration section (#4278),2
[AIRFLOW-3484] Fix Over-logging in the k8s executor (#4296)There are two log lines in the k8sexecutor that can cause schedulers to crashdue to too many logs.,2
[AIRFLOW-3479] Keeps records in Log Table when DAG is deleted (#4287)Users will use either API or web UI to delete DAG (after DAG file isremoved):- Using API: provide one boolean parameter to let users             decide if they want to keep records in Log table             when they delete a DAG.             Default value it True (to keep records in Log table).- From UI: will keep records in the Log table when delete records for a           specific DAG ID (pop-up message is updated accordingly).,5
"[AIRFLOW-XXX] Fix Minor issues with Azure Cosmos Operator (#4289)- Fixed Documentation in integration.rst- Fixed Incorrect type in docstring of `AzureCosmosInsertDocumentOperator`- Added the Hook, Sensor and Operator in code.rst- Updated the name of example DAG and its filename to follow the convention",2
[AIRFLOW-3438] Fix default values in BigQuery Hook & BigQueryOperator (#4274),1
[AIRFLOW-987] pass kerberos cli args keytab and principal to kerberos.run() (#4238),1
[AIRFLOW-3397] Fix integrety error in rbac AirflowSecurityManager (#4305)This was caused by the variable `role` being shadowed in a loop statement.,1
[AIRFLOW-XXX] add ARMEDANGELS to the list of customers (#4310),1
[AIRFLOW-2770] kubernetes: add support for dag folder in the docker image (#3683),2
"[AIRFLOW-3502] Add celery config option for setting ""pool"" (#4308)",1
[AIRFLOW-3310] Google Cloud Spanner deploy / delete operators (#4286),1
[AIRFLOW-3411]  Add OpenFaaS hook (#4267),1
"[AIRFLOW-3505] replace 'dags_in_docker' with 'dags_in_image' (#4311)As kubernetes is moving away from docker to OCI, it will be more correct to use the'dags_in_image' name to be more container system agnostic",5
"Revert  [AIRFLOW-2770] [AIRFLOW-3505] (#4318)* Revert ""[AIRFLOW-3505] replace 'dags_in_docker' with 'dags_in_image' (#4311)""This reverts commit 457ad83e4eb02b7348e5ce00292ca9bd27032651.* Revert ""[AIRFLOW-2770] kubernetes: add support for dag folder in the docker image (#3683)""This reverts commit e9a09d408e4cd1bda1d6e8b7670f08beab37de8a.",4
[AIRFLOW-3452] removed an unused/dangerous display-none (#4295)* removed an unused display-none that is currently overriden but could resurface as a bug.* remove the other display none in /www,4
[AIRFLOW-3444] Explicitly set transfer operator description. (#4279),1
[AIRFLOW-3521] Fetch more than 50 items in `airflow-jira compare` script (#4300),5
[AIRFLOW-3518] Performance fixes for topological_sort of Tasks (#4322)For larger DAGs topological_sort was found to be very inefficient. Madesome small changes to the code to improve the data structures used in themethod.,1
"[AIRFLOW-1552] Airflow Filter_by_owner not working with password_auth (#4276)Local users were always a superuser, this adds a column to the DB (and defaults to false,which is going to cause a bit of an upgrade pain for people, but defaulting to not being anadmin is the only secure default.)",1
[AIRFLOW-3447] Add 2 options for ts_nodash Macro (#4323),1
[AIRFLOW-3500] Make task duration display user friendly (#4304),1
[AIRFLOW-XXX] Add Société générale to company list (#4330),1
"[AIRFLOW-2770] Read `dags_in_image` config value as a boolean (#4319)* Read `dags_in_image` config value as a booleanThis PR is a minor fix for #3683The dags_in_image config value is read as a string. However, the existing code expects this to be a boolean.For example, in worker_configuration.py there is the statement: if not self.kube_config.dags_in_image:Since the value is a non-empty string ('False') and not a boolean, this evaluates to true (since non-empty strings are truthy)and skips the logic to add the dags_volume_claim volume mount.This results in the CI tests failing because the dag volume is missing in the k8s pod definition.This PR reads the dags_in_image using the conf.getboolean to fix this error.Rebased on 457ad83e4eb02b7348e5ce00292ca9bd27032651, before the previousdags_in_image commit was reverted.* Revert ""Revert  [AIRFLOW-2770] [AIRFLOW-3505] (#4318)""This reverts commit 77c368fd228fe5edfdb3304ed4cb000a50667010.",5
[AIRFLOW-XXX] Add missing remote logging field (#4333),2
[AIRFLOW-XXX] Added LeMans Corporation as user of Airflow in README (#4334),1
[AIRFLOW-3506] use match_phrase to query log_id in elasticsearch (#4342),2
"[AIRFLOW-3246] Make hmsclient optional in airflow.hooks.hive_hooks (#4080)Delay the import right up until it is needed, like how we do with the thrift imports.",2
[AIRFLOW-3540] Respect environment config when looking up config file. (#4340),2
[AIRFLOW-3398] Google Cloud Spanner instance database query operator (#4314),1
[AIRFLOW-3546] Fix typos in jobs.py logs (#4346),2
[AIRFLOW-XXX] Fix incorrect parameter in SFTPOperator example (#4344),1
[AIRFLOW-XXX] Fix inconsistent comment in example_python_operator.py (#4337),1
[AIRFLOW-3458] Move models.Connection into separate file (#4335),2
[AIRFLOW-XXX] Add Next Trucking as airflow user (#4327),1
[AIRFLOW-3555] Remove lxml dependency (#4352),5
[AIRFLOW-3557] Fix various typos (#4357),2
[AIRFLOW-850] Add a PythonSensor (#4349),1
"[AIRFLOW-3558] Improve default tox flake8 excludes (#4361)Right now our gitignore skips a bunch of temporary Python directoriesbut our flake8 config will still test against them, leading tounnecessary error messages. This changes the excludesto skip the common directories that can cause false flake8 failures.",0
[AIRFLOW-XXX] Correct instructions for Travis CI on fork (#4358),2
[AIRFLOW-3556] Add cross join set dependency function (#4356),1
[AIRFLOW-1684] - Branching based on XCom variable (Docs) (#4365)Elaborate how to use branching with xcoms,1
"[Airflow-1413] Fix FTPSensor failing on error message with unexpected text. (#2450)* [AIRFLOW-1413] Fix FTPSensor file presence checkCurrently FTPSensor operates by checking text of errormessage returned from ftp lib. It only succeeds if themessage matches the expected text. Otherwise it failswith an exception. However the message is dependendon a system, locale and possibly other factors.This patch changes the operation to inspect error coderather than message text.It also adds option to ignore certain classes of errorssuch as Host Unavailable that are recoverable, thus theperformed action can and should be retried accordingto ftp spec.* [AIRFLOW-1413] Adjustments as per code review* [AIRFLOW-1413] fixing styleCo-Authored-By: mdziemianko <michal.dziemianko@gmail.com>",0
[AIRFLOW-2568] Azure Container Instances operator (#4121)Add an operator to create a Docker container in Azure ContainerInstances. Azure Container Instances hosts a container and abstractsaway the infrastructure around orchestration of a container service.Operator supports creating an ACI container and pull an image from AzureContainer Registry or public Docker registries.,2
[AIRFLOW-3459] Move DagPickle to separate file (#4374),2
Update UPDATING.md (#4348)[AIRFLOW-XXX] Add section to Updating.md regarding timezones,5
[AIRFLOW-XXX] Update tutorial.rst (#4336),5
[AIRFLOW-3561] Improve queries (#4368)* improve queries* Adding field to the database* Set length of field* remove dagbag use in xcom call* Fixing typo* Adding test* Remove default_view* fixing test* rename var* Fixing rbac dag_stats* Fixing rbac task_stats* Fixing rbac code* Fixing rbac xcom* Fixing template* Fixing default view call* Added timezone to DagModel* Fixing timezone,0
[AIRFLOW-3551] Improve BashOperator Test Coverage (#4367),3
[AIRFLOW-3327] Add support for location in BigQueryHook (#4324),1
[AIRFLOW-2939][AIRFLOW-3568] Fix TypeError in GCSToS3Op & S3ToGCSOp (#4371)Fix TypeError on GoogleCloudStorageToS3Operator & S3ToGoogleCloudStorageOperator,1
[AIRFLOW-3550] Standardize GKE hook (#4364),1
[AIRFLOW-3573] Remove DagStat table (#4378)* Remove DagStat usage* Remove tests* Remove dag_stat table from db* Removed dagstat class* Revert change* Fixing test,3
[AIRFLOW-3597] Add tests for JdbcOperator (#4402)- reformatting,5
[AIRFLOW-3281] Fix Kubernetes operator with git-sync (#3770)* Refactor Kubernetes operator with git-syncCurrently the implementation of git-sync is broken because:- git-sync clones the repository in /tmp and not in airflow-dags volume- git-sync add a link to point to the revision required but it is nottaken into account in AIRFLOW__CORE__DAGS_FOLDERDags/logs hostPath volume has been added (needed if airflow run inkubernetes in local environment)To avoid false positive in CI `load_examples` is set to `False`otherwise DAGs from `airflow/example_dags` are always loaded. In thisway is possible to test `import` in DAGsRemove `worker_dags_folder` config:`worker_dags_folder` is redundant and can lead to confusion.In WorkerConfiguration `self.kube_config.dags_folder` defines the path ofthe dags and can be set in the worker using airflow_configmapRefactor worker_configuration.pyUse a docker container to run setup.pyCompile web assetsFix codecov application path* Fix kube_config.dags_in_image,5
[AIRFLOW-3595] Add tests for Hive2SambaOperator (#4400)- adds missing doc parameter destination_filepath- adds missing file close for tmp file (through ContextManager Usage)- refactoring,4
[AIRFLOW-3581] Fix next_ds/prev_ds semantics for manual runs (#4385),1
[AIRFLOW-3599] Removed Dagbag from delete dag (#4406)* Removed Dagbag from delete dag* delete when fileloc does not exist,2
[AIRFLOW-3600] Remove dagbag from trigger (#4407)* Remove dagbag from trigger call* Adding fix to rbac* empty commit* Added create_dagrun to DagModel* Adding testing to /trigger calls* Make session a class var,1
[AIRFLOW-XXX] Reorder Airflow Users (#4410),1
[AIRFLOW-3609] Fix bug in volumes readWriteMany (#4417)When running integration tests on a k8s cluster vs. MinikubeI discovered that we were actually using an invalid permissionstructure for our persistent volume. This commit fixes that.,0
"[AIRFLOW-3576] Remove unnecessray arg 'root' for /delete in dag.html (#4380)'root' is not used anywhere in `delete` method in eitherwww/views.py or www_rbac/views.py.Having it in url_for(""airflow.delete"", dag_id=dag.dag_id, root=root)in dag.html is meaningless.",2
[AIRFLOW-3606] Fix Flake8 test & fix the Flake8 errors introduced since Flake8 test was broken (#4415)The flake8 test in the Travis CI was broken since https://github.com/apache/incubator-airflow/pull/4361(https://github.com/apache/incubator-airflow/commit/7a6acbf5b343e4a6895d1cc8af75ecc02b4fd0e8 )And some Flake8 errors (code style/quality issues. found in 10 files) were introduce since flake8 test was broken.,3
"[AIRFLOW-3587] Remove unnecessary condition checks in dag_stats & task_stats (#4395)These condition checks will always be pass no matterwhether ""all_dags"" is in filter_dag_ids.They're not necessary.",2
[Airflow-XXX] - fix missing type in docstring (#4403)Add missing type of delegate_to,1
"[AIRFLOW-360] Launch custom images to Airflow CI tests (#4416)To help move away from Minikube, we need to remove the dependency ona local docker registry and move towards a solution that can be usedin any kubernetes cluster. Custom image names allow users to usesystems like docker, artifactory and gcr",2
[AIRFLOW-XXX] Adds image code comment (#4413),1
[AIRFLOW-3580] Adds tests for HiveToMySqlTransfer (#4387)- adds tests for hive_to_mysql operator- refactoring,4
[AIRFLOW-1921] Add support for https and user auth (#2879),1
[AIRFLOW-XXX] Fix Flake8 error (#4422),0
[AIRFLOW-3578] Fix Type Error for BigQueryOperator (#4384)* Fix Type Error for BigQueryOperator and support the unicode object.* Add tests,3
[AIRFLOW-2548] Output plugin import errors to web UI (#3930),0
[AIRFLOW-XXX] Fix Flake8 error,0
"[AIRFLOW-3402] Support global k8s affinity and toleration configs (#4247)* Support setting global k8s affinity and toleration configuration in the airflow config file.* Copy annotations as dict, not list* Update airflow/contrib/kubernetes/pod.pyCo-Authored-By: kppullin <kevin.pullin@gmail.com>",5
[AIRFLOW-3613] Updated ReadMe to add Adobe as an airflow user (#4420)* [AIRFLOW-3613] Updated ReadMe to add Adobe as an airflow user* [AIRFLOW-3613] Corrected Adobe as an Airflow User entry in README file,2
[AIRFLOW-3560] Add DayOfWeek Sensor (#4363)* [AIRFLOW-3560] Add WeekEnd & DayOfWeek Sensors* Change to using Enum* Fix Docstring* Refactor into a Single Sensor,4
[AIRFLOW-XXX] Fix WeekDay Sensor Example (#4431),0
[AIRFLOW-XXX] Add a doc on how to add a new role in RBAC UI (#4426),1
[AIRFLOW-2641] Fix MySqlToHiveTransfer to handle MySQL DECIMAL correctly,0
[AIRFLOW-3583] Fix AirflowException import (#4389)Looks like the class path changed and broke wasb_hook,1
[AIRFLOW-3316] For gcs_to_bq: add missing init of schema_fields var (#4430),5
[AIRFLOW-3623] Support download logs by attempts from UI (#4425),2
[AIRFLOW-3446] Add Google Cloud BigTable operators (#4354),1
[AIRFLOW-3527] Update Cloud SQL Proxy to have shorter path for UNIX socket (#4350),5
[AIRFLOW-3340] Placeholder support in connections form (#4185),1
[AIRFLOW-3622] Add ability to pass hive_conf to HiveToMysqlTransfer (#4432),5
[AIRFLOW-3150] Make execution_date templated in TriggerDagRunOperator (#4359),2
[AIRFLOW-3480] Add GCP Spanner Database Operators (#4353),1
[AIRFLOW-3634] Fix GCP Spanner Test (#4440),3
[AIRFLOW-3612] Remove incubation/incubator mention (#4419),4
[AIRFLOW-XXX] Update committer list based on latest TLP discussion (#4427),3
[AIRFLOW-3612] Remove remaining incubator mention & Fix CI Behaviour (#4441),0
[AIRFLOW-3635] Fix incorrect logic in detele_dag (introduced in PR#4406) (#4445)Incorrect logic was introduced in PR #4406(https://github.com/apache/airflow/pull/4406)This was not found out because the Travis CI was not working as expected.,1
[AIRFLOW-3637] Fix test for HiveToMySqlTransfer Operator (#4447),1
[AIRFLOW-3636] Fix a test introduced in #4425 (#4446),3
[AIRFLOW-3624] Add masterType parameter to MLEngineTrainingOperator (#4428),5
[AIRFLOW-3531] Add gcs to gcs transfer operator. (#4331),1
[AIRFLOW-3531] Fix test for GCS to GCS Transfer Hook (#4452),1
[AIRFLOW-3610] Add region param for EMR jobflow creation (#4418),1
[AIRFLOW-2082] Resolve a bug in adding password_auth to api as auth method (#4343),4
[AIRFLOW-XXX] Add BaseTIS as Airflow User (#4442),1
[AIRFLOW-XXX] Fix a flake8 error to unblock CI (#4453),0
[AIRFLOW-3650] Skip running on mysql for the flaky test (#4457),3
[AIRFLOW-3519] Fix example http operator (#4455),1
[AIRFLOW-3515] Remove the run_duration option (#4320),1
[AIRFLOW-3478] Make sure that the session is closed (#4298),1
[AIRFLOW-3631] Update flake8 and fix lint. (#4436),0
[AIRFLOW-XXX] Add Bloomberg to list of Airflow users (#4462)* Add Bloomberg to list of Airflow users* Indicate that Daniel Imberman is Bloomberg's Airflow PoC,1
[AIRFLOW-3627] Refine performance of /task_stats (#4433),5
[AIRFLOW-3646] Rename plugins_manager.py to test_xx to trigger tests (#4464),3
[AIRFLOW-3582] Adds tests for HiveStatsCollectionOperator (#4398),1
[AIRFLOW-3630] Cleanup of GCP Cloud SQL Connection (#4451),4
[AIRFLOW-3596] Clean up undefined template variables. (#4401),4
[AIRFLOW-3661] Add Waze as an Airflow user (#4469),1
[AIRFLOW-3655] Escape links generated in model views (#4463),2
[AIRFLOW-3662] Add dependency for Enum (#4468),1
"[AIRFLOW-3504] Refine the functionality of ""/health"" endpoint (#4309)Extend the functionality of ""/health"" endpoint by:1. Checking if database backend can be connected;2. Checking the latest scheduler heartbeatThe response will be in format:  {    ""metadatabase"":{      ""status"":""healthy""    },    ""scheduler"":{      ""status"":""healthy"",      ""latest_scheduler_heartbeat"":""2018-12-26 17:15:11+00:00""    }  }This is done for both /www and /www_rbac.No authentication is required to access this endpoint (nosensitive information will be exposed through it).Tests & documentation are added accordingly.(Deleted an unnecessary line in airflow/www/views.py as well)",4
[AIRFLOW-3657] Fix zendesk integration (#4466),0
"[AIRFLOW-3605] Load plugins from entry_points (#4412)* [AIRFLOW-3605] Add entrypoint plugin docsThis documentation came from https://github.com/apache/incubator-airflow/pull/730 which had already started work on a PR for this functionality.* [AIRFLOW-3605] Extend plugin loading functionalityAdded business logic to import AirflowPlugin classes through entry_points.This means we don’t have to interact with the file system directly to install plugins, and can manage them via `pip`.",5
[AIRFLOW-XXX] Update README.md (#4482)We are using Airflow at Nine.,1
"[AIRFLOW-3670] Add stages to Travis build (#4477)Allow travis to fail the entire build quickly, if the flake8 build fails",0
[AIRFLOW-XXX] Docs: Fix paths to GCS transfer operator (#4479),1
"[AIRFLOW-3671] Remove arg `replace` of MongoToS3Operator from `kwargs` (#4480)If the operator get arguments from `kwargs`, it will fire DeprecationWarning",2
[AIRFLOW-3638] Add tests for PrestoToMySqlTransfer (#4449)- reformatting[AIRFLOW-3638] Add tests for PrestoToMySqlTransfer- add missing license header,1
[AIRFLOW-3207] Option to push result to xcom (#4056),5
[AIRFLOW-3589] Visualize reschedule state in all views (#4408)* [AIRFLOW-3589] Visualize reschedule state in all views* Add explicit `UP_FOR_RESCHEDULE` state* Add legend and CSS to views* [AIRFLOW-3589] Visualize reschedule state in all views* Use set or tuple instad of list* Use `with` statement for session handling,1
[AIRFLOW-3584] Use ORM DAGs for index view. (#4390)* [AIRFLOW-3584] Use ORM DAGs for index view.* Serialize schedule interval to json rather than pickle.,5
[AIRFLOW-3475] Move ImportError out of models.py (#4383),2
[AIRFLOW-3664] Fix interpreter errors in test_python_operator.py (#4472),3
[AIRFLOW-3468] Remove KnownEvent(Event)?The KnownEvent and KnownEventType isn't used by 99% of the companiesand therefore we would like to deprecate this for Airflow 2.0,1
[AIRFLOW-3594] Unify different License Header,5
"[AIRFLOW-3197] Remove invalid parameter KeepJobFlowAliveWhenNoSteps in example DAG (#4404)The parameter 'KeepJobFlowAliveWhenNoSteps' in  JOB_FLOW_OVERRIDES doesn't pass boto API parameter validation, as it should be a part of 'Instances' object.Signed-off-by: Anton Weiss <anton@otomato.link>",2
[AIRFLOW-3212] Add AwsGlueCatalogPartitionSensor (#4112)Adds AwsGlueCatalogPartitionSensor and AwsGlueCatalogHook withsupporting functions. Unit tests are included but rely on mocking sinceMoto does not yet fully support AWS Glue Catalog at this time.,2
"[AIRFLOW-3554] Include contrib folders in code coverage stats (#4351)contrib/ was excluded previously (in the AirBnB days) as there was a distinction between contrib and ""core"" even though it was in tree. That distinction doesn't hold any more, so we care about the coverage of the contrib folders too.",3
[AIRFLOW-3679] Added Google Cloud Base Hook to documentation (#4487),2
[AIRFLOW-3592] Fix logs when task is in rescheduled state (#4492),2
[AIRFLOW-3676] Add required permission to CloudSQL export/import example (#4489),2
[AIRFLOW-XXX] Fix/complete example code in plugins.rst (#4376)[ci skip]Based on- https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/baseviews.py#L76- tests/plugins/test_plugin.pyWe need to specify `default_view` for the plugin class  from `AppBuilderBaseView`.,3
[AIRFLOW-3439] Decode logs with  'utf-8' (#4474),2
[AIRFLOW-3675] Use googlapiclient for google apis (#4484)The deprecated apiclient package name is used in a number of places.This commit changes it to googleapiclient and modifies the rightpackages to be used instead.,1
[AIRFLOW-XXX] Adding Palo Alto Networks as a user (#4495),1
"[AIRFLOW-3680] Consistency update in tests for All GCP-related operators (#4493)This commit performs consistency change for tests for all GCP-related operators.The operators were evolving over time and the approach to implement thosehave changed over time. After some 30+ operators implemented, it's timeto introduce some consistency across all the operators. Those are:* Separating out System test cases from Unit test cases* Consistent names of variables that are used in system test cases* Updated documentation to be consistent across the operators* Updated examples to be better readable and runnable as System Tests* Added helper methods that allow to make setUp/tearDown for System Tests",3
[AIRFLOW-3673] Add official dockerfile (#4483),2
[AIRFLOW-3191] Fix not being able to specify execution_date when creating dagrun (#4037),2
[AIRFLOW-XXX] Fix Typo in README (#4501),2
[AIRFLOW-3687] Add missing @apply_defaults decorators (#4498),1
[AIRFLOW-3685] Move licence header check (#4497),4
[AIRFLOW-722] Add celery queue sensor (#4496),1
[AIRFLOW-3691] Update notice to 2019 (#4503),5
[AIRFLOW-3692] Remove ENV variables to avoid GPL (#4506),4
"[AIRFLOW-3689] Update pop-up message when deleting DAG in RBAC UI (#4505)This feature was added in https://github.com/apache/airflow/pull/4287,but the pop-up messages was only updated in airflow/www/templates/airflow/dag.html,while it should be updated for all dag.html & dags.html for both /www and /www_rbac.",2
[AIRFLOW-XXX] Fix misspelling of Vertica (#4509),0
[AIRFLOW-3690] Fix bug to set state of a task for manually-triggered DAGs (#4504),2
[AIRFLOW-3694] Add README.md to the Dockerfile (#4510),2
"[AIRFLOW-3693] Replace psycopg2-binary by psycopg2 (#4508)For Python packages, psycopg2 is preferred over psycopg2-binaryhttp://initd.org/psycopg/docs/install.html#binary-install-from-pypi",5
[AIRFLOW-3696] Add Version info to Airflow Documentation (#4512),2
"[AIRFLOW-3700] Change the lowest allowed version of ""requests"" (#4517)",1
"[AIRFLOW-3569] Add ""Trigger DAG"" button in DAG page (/www_rbac only) (#4373)To have the Manual Trigger DAG button in the DAG page as well,rather than only in the home page.",2
[AIRFLOW-3522] Add support for sending Slack attachments (#4332)* [AIRFLOW-3522] Add support for sending Slack attachmentsAdd attachments args to SlackWebhookHook andSlackWebhookOperator to support sending moredetailed Slack messages.* [AIRFLOW-3522] Update attachments docstrings,2
"[AIRFLOW-3699] Speed up Flake8 (#4515)Flake8 runs within the Docker container, which is not needed.Running this outside Docker will dramatically speed up theCI process.",2
[AIRFLOW-3303] Deprecate old UI in favor of FAB (#4339),5
[AIRFLOW-3655] Escape links generated in model views (#4522),2
"[AIRFLOW-3704] Support SSL Protection When Redis is Used as Broker for CeleryExecutor (#4521)From Celery 4.1 (current Airflow is using 4.1.1),""broker_use_ssl"" argument starts to support Redis(earlier this argument is only supported when amqp is used for broker)(REF: https://github.com/celery/celery/blob/4.1/docs/userguide/configuration.rst).",5
[AIRFLOW-3319] - KubernetsExecutor: Need in try_number in  labels if getting them later (#4163)* Need in labels if getting them later* has to be an int to match running keys- otherwise  running list will never empty* pr comments* bad merge* mend pep issue* add try_numer to make_pod test,3
[AIRFLOW-XXX] Adding users to CAVA & PXYData (#4527)adding patchus to 2 companies,1
[AIRFLOW-XXX] Include Los Angeles Times as a contributor in the Readme (#4526)[AIRFLOW-XXX] Include Los Angeles Times as a contributor in the Readme,5
[AIRFLOW-3681] All GCP operators have now optional GCP Project ID (#4500),1
"[AIRFLOW-3709] Validate `allowed_states` for ExternalTaskSensor (#4536)In ExternalTaskSensor, we can specify `allowed_states`.This commit adds validation for it, so that users will notspecify any invalid state.This change works no matter the sensor waits for a DAG ora specific task.",2
"[AIRFLOW-3108] Define get_autocommit method for MsSqlHook (#4525)The default implementation of DbApiHook merely checks for an attribute named `autocommit`.Since `pymssql` Connection object actually has a method with that name, it returns a bound method when fetching an attribute with that name, evaluating to a ""truthy"" value. It resulted in SQL statements not actually being committed.",5
[AIRFLOW-3516] Support to create k8 worker pods in batches (#4434),1
[AIRFLOW-3712] Remove settings.RBAC from GCP System Tests (#4540),3
[AIRFLOW-XXX] Remove `of to` typo. (#4542)[AIRFLOW-XXX] Remove `of to` typo.,2
[AIRFLOW-3559] Add missing options to DatadogHook. (#4362),5
[AIRFLOW-XXX] Fix a typo of config (#4544),5
[AIRFLOW-XXX] Correct Typo in sensor's exception (#4545),2
[AIRFLOW-3455] add region in snowflake connector (#4285)* [AIRFLOW-3455] add region in snowflake connector* add test cases,3
"[AIRFLOW-2843] Add flag in ExternalTaskSensor to check if external DAG/task exists (#4547)In ExternalTaskSensor, it may be good to providean option to cease waiting immediately if the externalDAG/task specified doesn't exist.To provide an argument ""check_existence"". Set to True to checkif the external DAG/task exists, and immediately cease waitingif the external DAG/task does not exist.The default value is set to False (no check or ceasingwill happen) so it will not affect any existing DAGs orcurrent user expectation.",1
[AIRFLOW-3713] Updated documentation for GCP optional project_id (#4541),2
[AIRFLOW-3724] Fix the broken refresh button on Graph View in RBAC UI (#4548),0
"[AIRFLOW-3591] Fix start date, end date, duration for rescheduled tasks (#4502)* Use first start date when running a rescheduled task, this also fixes  the duration. Use actual start date to record reschedule requests.* Simplify gantt view code now that start date and duration are correct  in `task_instance` table",5
[Airflow 3728] Remove double comma (#4552),4
[AIRFLOW-XXX] Add Firestone Inventing as a Big Data & AI company uses Apache Airflow into README.md (#4555),2
[AIRFLOW-3714] Correct DAG name in docs/start.rst (#4550),2
[AIRFLOW-3731] Constrain mysqlclient to <1.4 (#4558)To maintain Python2 compatibility,5
[AIRFLOW-XXX] Fix typo (#4564)Fix typos in various files,2
[AIRFLOW-3383] Rotate fernet keys. (#4225)Add the ability to change the encryption key of all encrypted variables andconnections,4
"[AIRFLOW-1191] Simplify override of spark submit command. (#4360)Adds a spark_binary param to the spark submit operator toallow folks to more easily configure the operator to use adifferent binary, as is needed for some distros of theHadoop ecosystem which ship multiple version of Spark.",5
[AIRFLOW-XXX] Fix doc string of DataprocClusterDeleteOperator (#4565),5
[AIRFLOW-3732] Fix issue when trying to edit connection in RBAC UI (#4559),2
[AIRFLOW-1262] Adds missing docs for email configuration (#4557),5
[AIRFLOW-2009] Fix dataflow hook connection-id (#4563),1
"[AIRFLOW-XXX] Fix typos (#4570)Correct some spelling, grammar, and punctuation",2
[AIRFLOW-3698] Add documentation for AWS Connection (#4514),2
"[AIRFLOW-3752] Add/remove user from role via CLI (#4572)* [AIRFLOW-3752] Add/remove user from role via the CLIUpdate the `users` subcommand to enable 2 new actions:- `--add-role`: Make the user a member of the given role- `--remove-role`: Remove the user's membership in the given roleFor installations that use an external identity provider (e.g., GoogleOAuth) the username is typically a long ID string. For the sake ofconvenience, we allow the CLI operator to reference the target uservia either their `username` or their `email` (but not both).* Update argparse specAccidentally left off this update to the argparse spec in the lastcommit.* Add unit tests* Fix lint failures",0
[AIRFLOW-3744] Abandon the use of obsolete aliases of methods (#4568),1
[AIRFLOW-XXX] Update Changelog for 1.10.2,4
[AIRFLOW-XXX] Update the UPDATING.md file for 1.10.2,2
[AIRFLOW-XXX] Add CHANGELOG & K8s to Documentation,2
"[AIRFLOW-3725] Add private_key to bigquery_hook get_pandas_df (#4549)[AIRFLOW-3725] Bigquery Hook authentication currently defaults to Google User-account    credentials, and the user is asked to authenticate with Pandas GBQ    manually. This diff allows users to specify a private_key in either    json or key_path form, in keeping with the Google Cloud Platform    connection type.",5
[AIRFLOW-XXX] Add GoDataDriven to the list of companies (#4573),5
[AIRFLOW-2508] Handle non string types in Operators templatized fields (#4292),1
[AIRFLOW-XXX] Pin version of Pip in tests to work around pypa/pip#6163 (#4576)There is a bug or a new feature that causes a number of our dependenciesto fail to install.,0
"[AIRFLOW-XXX] Only upgrade pip on Travis, not everythingThis was a mistake I introduced in #4576 that I only noticed after thatwas merged",7
[AIRFLOW-XXX] Fix spark submit hook KeyError (#4578)Fix string format KeyError in spark_submit_hook.py,1
[AIRFLOW-XXX] Adds SpotHero as a user of Airflow (#4581),1
[AIRFLOW-XXX] Reduction of the number of warnings in the documentation (#4585),2
[AIRFLOW-XXX] Automatically link Jira/GH on doc's changelog page (#4587),4
[AIRFLOW-3745] Fix viewer not able to view dag details (#4569),2
[AIRFLOW-XXX] Mock optional modules when building docs (#4586),2
[AIRFLOW-XXX] Removes Data Profiling docs as it is not supported in RBAC UI,1
[AIRFLOW-3764] Simplify chained comparisons in IF block (#4580),5
[AIRFLOW-3719] Handle StopIteration in CloudWatch logs retrieval (#4516),2
[AIRFLOW-3490] Add BigQueryHook's Ability to Patch Table/View (#4299),1
[AIRFLOW-3216] HiveServer2Hook need a password with LDAP authentication (#4057),4
[AIRFLOW-3602] Improve ImapHook handling of retrieving no attachments (#4475),1
"[AIRFLOW-2190] Fix TypeError when returning 404 (#4596)When processing HTTP response headers, gunicorn checks that the name of eachheader is a string. Here's the relevant gunicorn code:From gunicorn/http/wsgi.py, line 257    def process_headers(self, headers):        for name, value in headers:            if not isinstance(name, string_types):                raise TypeError('%r is not a string' % name)In Python3, `string_types` is set to the built-in `str`. For Python 2,it's set to `basestring`. Again, the relevant gunicorn code:From gunicorn/six.py, line 38:    if PY3:        string_types = str,        ...    else:        string_types = basestring,On Python2 the `b''` syntax returns a `str`, but in Python3 it returns `bytes`.`bytes` != `str`, so we get the following error when returning a 404 onPython3:    File ""/usr/local/lib/python3.6/site-packages/airflow/www/app.py"", line 166, in root_app    resp(b'404 Not Found', [(b'Content-Type', b'text/plain')])    File ""/usr/local/lib/python3.6/site-packages/gunicorn/http/wsgi.py"", line 261, in start_response    self.process_headers(headers)    File ""/usr/local/lib/python3.6/site-packages/gunicorn/http/wsgi.py"", line 268, in process_headers    raise TypeError('%r is not a string' % name)    TypeError: b'Content-Type' is not a stringDropping the `b` prefix in favor of the single-quote string syntax should workfor both Python2 and 3, as demonstrated below:    Python 3.7.2 (default, Jan 13 2019, 12:50:15)    [Clang 10.0.0 (clang-1000.11.45.5)] on darwin    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.    >>> isinstance('foo', str)    True    Python 2.7.15 (default, Jan 12 2019, 21:43:48)    [GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)] on darwin    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.    >>> isinstance('foo', basestring)    True",5
[AIRFLOW-3771] Minor refactor securityManager (#4594),4
[AIRFLOW-XXX] Remove images related profiling doc (#4599),2
"[AIRFLOW-3761] Decommission User & Chart models & Update doc accordingly (#4577)In master branch, we have already decommissioned the Flask-Admin UI.In model definitions, User and Chart are only applicable for the""old"" UI based on Flask-Admin.Hence we should decommission these two models as well.Related doc are updated in this commit as well.",5
[AIRFLOW-XXX] Remove almost all warnings from building docs (#4588),2
[AIRFLOW-XXX] Remove profiling link (#4602),2
"[AIRFLOW-3773] Fix /refresh_all endpoint (#4597)* [AIRFLOW-3773] Fix /refresh_all endpointCall `sync_perm_for_dag` for each DAG in the DagBag (`dag_id` is arequired argument).I looked for a test suite for the web UI, but it seems the existingtests have all been disabled since the switch to FAB. I've created a newclass for FAB tests and added a test to exercise this `/refresh_all`endpoint.* Move tests to www/test_views.pyI didn't realize that we already had test scaffolding in place fortesting the FAB-based UI.",3
[AIRFLOW-XXX] Add Tinder to the companies list (#4604),1
[AIRFLOW-XXX] Add Capital One to the companies list (#4606),1
[AIRFLOW-3762] Add list_jobs to CLI (#4579)* [AIRFLOW-3762] Add list_jobs to CLIAdd list_jobs to CLI* [AIRFLOW-3762] Add list_jobs to CLIImprove test_cli_list_jobs_with_args*  [AIRFLOW-3762] Add list_jobs to CLIDirectly parse args.limit to list_jobs query* [AIRFLOW-3762] Add list_jobs to CLIFormat list_jobs code,1
[AIRFLOW-3474] Move SlaMiss out of models.py (#4608),4
[AIRFLOW-865] Configure FTP connection mode (#4535),5
[AIRFLOW-3734] Fix hql not run when partition is None (#4561),1
[AIRFLOW-3552] Add ImapToS3TransferOperator (#4476)NOTE: This operator only transfers the latest attachment by name.,3
[AIRFLOW-XXX] Update timezone doc (#4592),2
"[AIRFLOW-3742] Respect the `fallback` arg in airflow.configuration.get (#4567)This argument is part of the API from our parent class, but we didn'tsupport it because of the various steps we perform in `get()` - thismakes it behave more like the parent class, and can simplify a fewinstances in our code (I've only included one that I found here)",1
"AIRFLOW-3590: Change log message of executor exit status (#4616)Try to make the log message clearer in the presence of rescheduled tasks -i.e that the task exited with 0/1, not the status of the task, without having eachexecutor having to know about reschedule or other states we might introduce.",2
[AIRFLOW-3789] Fix flake8 3.7 errors. (#4617),0
[AIRFLOW-3774] Register blueprints with app (#4598),5
"[AIRFLOW-3779] Don't install enum34 backport when not needed (#4620)https://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependenciesInstalling this in more recent versions causes a ""AttributeError: module'enum' has no attribute 'IntFlag'`"" in re.py",0
"[AIRFLOW-3787] Import/export users from JSON file (#4624)* [AIRFLOW-3787] Import/export users from JSON fileProvide a CLI command to import or export users from a JSON file. TheCLI arguments are modeled after the import/export commands for Variablesand Pools.Example Usage:    airflow users -i users.json    airflow users -e /tmp/exported-users.jsonThe import command will create any users that do not yet exist andupdate any users that already exist. It never deletes users.The format of the file produced by an export is compatible with theimport command, i.e., `import(export())` should succeed but have noside-effects.* Add input file format to help text",2
[AIRFLOW-3462] Move TaskReschedule out of models.py (#4618),4
[AIRFLOW-XXX] Add a doc about fab security (#4595),2
[AIRFLOW-2876] Update Tenacity to 4.12 (#3723)Tenacity 4.8 is not python 3.7 compatible because it containsreserved keywords in the code,1
[AIRFLOW-3471] Move XCom out of models.py (#4629),4
[AIRFLOW-3730] Standarization use of logs mechanisms (#4556),2
"[AIRFLOW-3782] Clarify docs around celery worker_autoscale in default_airflow.cfg (#4609)Celery supports `autoscale` by accepting values in format ""max_concurrency,min_concurrency"".But the default value in default_airflow.cfg is wrong, and the comment can be clearer.",0
[AIRFLOW-3461] Move TaskFail out of models.py (#4630),0
[AIRFLOW-XXX] The execution_date is Pendulum,5
[AIRFLOW-3463] Move Log out of models.py (#4639),2
"[AIRFLOW-XXX] Fixed note in plugins.rst (#4649)Changing it to rst notation, so it stands out in read the docs.",2
[AIRFLOW-XXX] Add missing class references to docs (#4644),2
[AIRFLOW-3814] Add exception details to warning log (#4651)* Add exception details to warning log* Fix log format,2
"[AIRFLOW-3476,3477] Move Kube classes out of models.py (#4443)",4
[AIRFLOW-3810] Remove duplicate autoclass directive (#4656),4
[AIRFLOW-XXX] Extract reverse proxy info to a separate file (#4657),2
"[AIRFLOW-2694] Declare permissions in DAG definition (#4642)* [AIRFLOW-2694] Declare permissions in DAG definitionThis PR adds support for declaratively assigning DAG-level permissionsto a role via the `DAG.__init__()` method.When the DAG definition is evaluated and the `access_control` argumentis supplied, we update the permissions on the ViewMenu associated withthis DAG according to the following rules:- If the role does not exist, we raise an exception.- If the role exists, we ensure that it has the specified set of  permissions on the DAG- If any other permissions exist for the DAG that are not specified in  `access_control`, we revoke them* Move RBAC constants to break circular dependency* Add license header* Sync DAG permissions via CLI and /refresh* endpointsMove the DAG-level permission syncing logic into`AirflowSecurityManager.sync_perm_for_dag`, and trigger this method fromthe CLI's `sync_perm` command and from the `/refresh` and `/refresh_all`web endpoints.* Default access_control to None",2
"[AIRFLOW-3813] Add CLI commands to manage roles (#4658)* [AIRFLOW-3813] Add CLI commands to manage rolesHere is the help text of the new command `airflow roles`:    usage: airflow roles [-h] [-c] [-l] [role [role ...]]    positional arguments:      role          The name of a role    optional arguments:      -h, --help    show this help message and exit      -c, --create  Create a new role      -l, --list    List rolesCreate is reentrant, i.e., it only adds a new role if it does not exist.* Update docs on role creation",1
[AIRFLOW-3817] - Corrected task ids returned by BranchPythonOperator to match the dummy operator ids (#4659),1
[AIRFLOW-3802] Updated documentation for HiveServer2Hook (#4647),1
"[AIRFLOW-3643] Add shebang to docs/start_doc_server.sh (#4650)Since this script uses bash syntax, shebang needs to be added.",1
"[AIRFLOW-3647] Add archives config option to SparkSubmitOperator (#4467)To enable to spark behavior of transporting and extracting an archiveon job launch,  making the _contents_ of the archive available to thedriver as well as the workers (not just the jar or archive as a zipfile) - this configuration attribute is necessary.This is required if you have no ability to modify the Python env onthe worker / driver nodes, but you wish to use versions, modules, orfeatures not installed.We transport a full Python 3.5 environment to our CDH cluster usingthis option and the alias ""#PYTHON"" paired an additional configurationto spark to use it:    --archives ""hdfs:///user/myuser/my_python_env.zip#PYTHON""    --conf ""spark.yarn.appMasterEnv.PYSPARK_PYTHON=./PYTHON/python35/bin/python3""",5
[AIRFLOW-XXX] Improve linking to classes (#4655),2
[AIRFLOW-XXX] Correct typo for `prev_ds` (#4664)`@weekly` probably shouldn't yield the date from 2 years + 1 week ago :),5
[AIRFLOW-3547] Fixed Jinja templating in SparkSubmitOperator (#4347)* [AIRFLOW-3547] Fixed Jinja templating in SparkSubmitOperatorThis is a minor change to allow Jinja templating in parameters where itmakes sense for SparkSubmitOperator.* [AIRFLOW-3547] Fixed Jinja templating in SparkSubmitOperatorThis is a minor change to allow Jinja templating in parameters where itmakes sense for SparkSubmitOperator.,1
[AIRFLOW-XXX] Add backreference in docs between operator and integration (#4671),1
[AIRFLOW-3707] Group subpackages/extras by cloud providers (#4524),1
[AIRFLOW-3828] Use context manager to manage session in cli.rotate_fernet_key (#4668),1
[AIRFLOW-3820] Add back the gunicorn config (#4661),5
[AIRFLOW-XXX] Move out the examples from integration.rst (#4672),4
[AIRFLOW-3464] Move SkipMixin out of models.py (#4386),4
AIRFLOW-[3823] Exclude branch's downstream tasks from the tasks to skip (#4666),5
"[AIRFLOW-3742] Fix handling of ""fallback"" for AirflowConfigParsxer.getint/boolean (#4674)We added (and used) fallback as an argument on `getboolean` but didn'tadd it to the method, or add tests covering those ""casting"" accessors,so they broke.This fixes those methods, and adds tests covering them",3
[AIRFLOW-3866] Run docker-compose pull silently in CI (#4688)To reduce the output in Travis,2
[AIRFLOW-XXX] Docs rendering improvement (#4684),1
[AIRFLOW-XXX] Add section on task lifecycle & correct casing in docs (#4681),2
[AIRFLOW-3749] Fix Edit Dag Run page when using RBAC (#4613),1
[AIRFLOW-3801] Fix DagBag collect dags invocation to prevent examples to be loaded (#4677),2
[AIRFLOW-3851] ExternalTasksensor should not check existence in subsequent poke (#4673),5
[AIRFLOW-XXX] added company to the Users list in the README (#4693),1
[AIRFLOW-3876] AttributeError: module 'distutils' has no attribute 'util'fix for AttributeError: module 'distutils' has no attribute 'util',0
[AIRFLOW-XXX] Upgrade FAB to 1.12.3 (#4694),5
[AIRFLOW-XXX] Add notes for imports and sensors (#4698)- Add note for the removal of deprecated import mechanism- Add note for changes to sensor imports,2
[AIRFLOW-XXX] Update docs with new BranchPythonOperator behaviour (#4682),1
[AIRFLOW-XXX] Fix headlines in UPDATING.md (#4697),5
[AIRFLOW-3875] Simplify SlackWebhookHook code and change docstring (#4696),2
[AIRFLOW-3874] Improve BigQueryHook.run_with_configuration's location support (#4695)* [AIRFLOW-3874] mprove BigQueryHook.run_with_configuration's location supportAssign location from the resp of insert() back to job.get()* [AIRFLOW-3874] Improve BigQueryHook.run_with_configuration's location supportRemove unused code,1
"[AIRFLOW-3869] Raise consistent exception in AirflowConfigParser.getboolean (#4692)AirflowConfigParser.getboolean raises an AirflowConfigExceptionwhen the value of a configuration cannot be converted to boolean.But, getint and getfloat raise a ValueError in the same case whenthe value cannot be casted to the desired type.They should raise the same exception - ValueError.Also, added tests for getboolean, getint and getfloat.",1
[AIRFLOW-3639] Fix request creation in Jenkins Operator (#4450)Change Jenkins Operator to work with native Jenkins library method to configure REST request headers correctly,5
[AIRFLOW-3780] Fix some incorrect when base_url is used (#4643)Some links are incorrect when base_url is set.,1
[AIRFLOW-3821] Add replicas logic to GCP SQL example DAG (#4662),2
"[AIRFLOW-3861] Use the create_session for the db session (#4683)Instead of opening and closing these session manually, it is easierto do this using the create_session and that should be the preferredway as well.",1
[AIRFLOW-3808] Add cluster_fields to BigQueryHook's create_empty_table (#4654),1
[AIRFLOW-XXX] Docs: Add note -  airflow pool is not honored by SubDagOperator (#4634),2
[AIRFLOW-3792] Fix validation in BQ for useLegacySQL & queryParameters (#4626),2
[AIRFLOW-3770] Validation of documentation on CI] (#4593),2
"[AIRFLOW-3884] Fixing doc checker, no warnings allowed anymore and fixed the current… (#4702)* Fixing doc checker, no warnings allowed anymore and fixed the current warnings* Fixing flake8 issues",0
[AIRFLOW-XXX] Pin pinodb dependency (#4704),5
[AIRFLOW-3857] spark_submit_hook cannot kill driver pod in kubernetes (#4678),1
[AIRFLOW-XXX] Add note about plugin docs (#4706),2
[AIRFLOW-3702] Add backfill option to run backwards (#4676),1
[AIRFLOW-3891] Disable SQL Alchemy tracking in the Webserver (#4707),5
[AIRFLOW-3598] Add tests for MsSqlToHiveTransfer (#4405)- refactoringApply suggestions from code review- changes tests to only be executed when pymssql has been imported successfullyCo-Authored-By: feluelle <feluelle@users.noreply.github.com>[AIRFLOW-3598] Remove unnecessary contextlib.closing and update tests,3
[AIRFLOW-XXX] Update README.md (#4709),2
[AIRFLOW-XXX] add Asana to companies list (#4711),1
"[AIRFLOW-3887] Downgrade dagre-d3 to 0.4.18 (#4713)dagre-d3 v0.6.3 has a bug that causes this Javascript error when loadingthe Graph View:    TypeError: previousPaths.merge is not a functionThe bug fix [1] has been merged to master, but hasn't been released tonpm yet. This change temporarily downgrades our version of dagre-d3until dagre-d3 v0.6.4 is released [2]I also fixed a bug I encountered in the `compile_assets.sh` where thescript would fail if the directory `airflow/www/static/dist/` exists butis empty.[1] https://github.com/dagrejs/dagre-d3/pull/350[2] https://github.com/dagrejs/dagre-d3/blob/5450627790ff42012ef50cef6b0e220199ae4fbe/package.json#L3",5
[AIRFLOW-3249] Make all take the same named `do_xcom_push` flag (#4345),1
"[AIRFLOW-161] New redirect route and extra links (#3533)With this change different operators would be able to customize thetask instance model view with extra links, those can be used toredirect users to systems which are out of Airflow.Add some context on format of whitelisted domainsAdd a new endpoint to views that gets redirect linksIn order to be able to display the link on UI, and not have the backend do the external routing, I had to setup the endpoint tobe a RESTful endpoint that the frontend could ping whenever. I also wanted to handle a handful of error cases, so it returns different errorsIf the URL is not whitelisted, if the URL is not provided, and there is even a way for operator to provide their own custom errorsAdd some documenation for how get_redirect_url and the redirect endpoint workUpdate the modal box to make a AJAX call for link resolutionsThis gives us multiple benefits: 1. It enables us to disable links when actionable 2. It enables us to give better feedback to the user on whitelisting and errors without navigating the page 3. You can see and interact with the link as it points to it's real destitnation, as opposed to the airflow backend doing the routing   This let's you see where the link will resolve in advanceUpdate graph.html and tree.html for www_rbac as wellAlso, fix some linting issuesCapitilize the blueprint name, test fixFix unit tests to handle bytes from requestsThe problem is the python3.5 and python3.6 return different data types from network connections (bytes versus unicode), and tests were breakingFix documentation for extra_linksCover case when link URL name has a spaceRename instances of redirect to extra_linksAlso, remove the whitelist, as we're no longer doing backend redirectsRename 'redirect_to' to 'link_name' to make things clearerAlso, fix a small bug in the views method that flashed on screen when the DAG was invalidFix a small bug where the link name itself would replace spaces with underscoresRename redirect_to to link_name in the quboleOperatorUse the correct endpoint for rbacFix flak8 issuesFix call to call_modal so that they all resolve properly.Essentially second merge passAlso, there seems to have been an issue with the order of arguments relative to the try_numbers. They were only used sometimes, and in graph.html, they seemed to be used in the wrong order relative to the function definition. So I reordered some argumentsFix rbac permissions to extra_links commandFix some UI elements that have changed since I originally made this PRAlso, undefined is subdag doesn't exist, not truereplace only replaces first instance, replace all instancesAlso, use the underscore tooltip instead of the original link name. I'm not sure how this was not caught sooner by me :/Also, add a comment about subdags to the tree documentation, I don't think it calculates subdag identities correctly.Fix flake8 issuesFix importFix docstring",2
[AIRFLOW-3898] Show example connection string when using Psycopg2 (#4718),1
"Revert ""[AIRFLOW-161] New redirect route and extra links (#3533)"" (#4720)This reverts commit 897e2620f13f9f9935516b34b08ce21a3e719a83.",4
[AIRFLOW-3900] Error on undefined template variables in unit tests. (#4719),3
"[AIRFLOW-3807] Fix Graph View Highlighting of Tasks (#4653)- fixes up_for_retry and up_for_reschedule tasks when hovering over retry/rescheduled task state- adds missing task states to stateFocusMap that will be used for highlighting tasks when clicking on task state- removed invalid attributes for some tags- reformatted accordingly to rules from .editorconfig[AIRFLOW-3807] Fix no_status tasks highlighting in graph view[AIRFLOW-3807] Change ""no status"" string to ""no_status""[AIRFLOW-3807] Fix syntax issue in js statement[AIRFLOW-3807] Correct tree view tasks' status labels- reformat tree.html file- remove invalid attributes from tree.html tags",4
[AIRFLOW-3799] Add compose method to GoogleCloudStorageHook (#4641),1
"[AIRFLOW-3885] ~20x speed-up of slowest unit test (#4726)The test `SchedulerJobTest.test_scheduler_start_date` is the slowest test,taking ~5 minutes on average:    >>> [success] 12.99% tests.test_jobs.SchedulerJobTest.test_scheduler_start_date: 295.1935s[success] 6.79% tests.test_jobs.SchedulerJobTest.test_scheduler_multiprocessing: 154.2304s[success] 6.72% tests.test_jobs.SchedulerJobTest.test_scheduler_task_start_date: 152.7215s[success] 4.34% tests.test_jobs.SchedulerJobTest.test_new_import_error_replaces_old: 98.7339s[success] 3.63% tests.test_jobs.SchedulerJobTest.test_remove_error_clears_import_error: 82.4062sAfter setting the subdirectory and eliminating (I think) redundant schedulerloops, the test time comes down to ~15 seconds.",3
[AIRFLOW-3733] Don't raise NameError in HQL hook to_csv when no rows returned (#4560),1
"[AIRFLOW-3885] ~2.5x speed-up for backfill tests (#4731)The BackfillJobTest suite now takes 57 seconds vs. the baseline of 147seconds on my laptop.A couple of optimizations:- Don't sleep() if we are running unit tests- Don't backfill more DagRuns than needed (reduced from 5 to 2, since we  only need 2 DagRuns to verify that we can run backwards)I've also made a few tests reentrant by clearing out the Pool, DagRun,and TaskInstance table between runs.",1
"[AIRFLOW-3885] ~10x speed-up of SchedulerJobTest suite (#4730)The SchedulerJobTest suite now takes ~90 seconds on my laptop (down from~900 seconds == 15 minutes) on Jenkins.There are a few optimizations here:1. Don't sleep() for 1 second every scheduling loop (in unit tests)2. Don't process the example DAGs3. Use `subdir` to process only the DAGs we need, for a couple of tests   that actually run the scheduler4. Only load the DagBag once instead of before each testI've also added a few tables to the list of tables that are cleaned upin between test runs to make the tests re-entrant.",3
[AIRFLOW-3911] Change Harvesting DAG parsing results to DEBUG log level (#4729),2
[AIRFLOW-3901] add role as optional config parameter for SnowflakeHook (#4721),1
[AIRFLOW-3907] Upgrade flask and set cookie security flags. (#4725),1
[AIRFLOW-3616][AIRFLOW-1215] Add aliases for schema with underscore (#4523),1
[AIRFLOW-3910] Raise exception explicitly in Connection.get_hook() (#4728)Passing exception silently here makes debugging/troubleshooting very hard,0
[AIRFLOW-3885] Fix race condition in scheduler test (#4737)We're hitting this race condition frequently now that we don't sleep()during unit tests. We don't actually need to assert that the task iscurrently running - it's fine if it has already run successfully.,1
[AIRFLOW-3923] Update flask-admin dependency to 1.5.3 to resolve security vulnerabilities from safety (#4739),0
"[AIRFLOW-3905] Allow using ""parameters"" in SqlSensor (#4723)* [AIRFLOW-3905] Allow 'parameters' in SqlSensor* Add check on conn_type & add testNot all SQL-related connections are supported by SqlSensor,due to limitation in Connection model and hook implementation.",1
[AIRFLOW-3865] Add API endpoint to get python code of dag by id (#4687),2
[AIRFLOW-3933] Fix various typos (#4747)Fix typos,2
[AIRFLOW-XXX] Add Slack Badge to Readme (#4750),1
[AIRFLOW-3683] Fix formatting of error message for invalid TriggerRule (#4490),0
"[AIRFLOW-3153] Send DAG processing stats to statsd (#4748)Add 2 stats under the `airflow.dag_processing` namespace. The metricnames follow the template: `dag_processing.<metric>.<dag_file>`, where`<dag_file>` is the name of a file in the dag_folder (without theextension) and `<metric>` is one of the following:- `last_runtime`: the number of seconds it took to process the DAG file  on the most recent iteration- `last_run.seconds_ago`: the number of seconds that have elapsed since  the DAG file was last processedI've verified the logging by running the scheduler on the examples DAGsand logging the value of the gauges with netcat:    $ nc -u -l -p 8125 | tr '|' '\n' | grep dag_processing      gairflow.dag_processing.last_runtime.example_docker_operator:2.002253      gairflow.dag_processing.last_run.seconds_ago.example_docker_operator:18.066831      gairflow.dag_processing.last_runtime.tutorial:2.001403      gairflow.dag_processing.last_run.seconds_ago.tutorial:36.114995      gairflow.dag_processing.last_runtime.docker_copy_data:2.003188      gairflow.dag_processing.last_run.seconds_ago.docker_copy_data:28.097275",5
[AIRFLOW-3925] Don't pull docker-images on pretest (#4740)Co-authored-by: Joshua Carp <jm.carp@gmail.com>,3
[AIRFLOW-3701] Add Google Cloud Vision Product Search operators (#4665),1
"[AIRFLOW-3741] Add extra config to Oracle hook (#4584)Add extra config to Oracle hook including encoding, mode, threaded etc",1
[AIRFLOW-3924] Fix try number in alert emails (#4741)Alert emails sent via email_alert() have the correct try numberin the body of the text.Add a test to ensure the first email sent says `Try 1`.,1
[AIRFLOW-3896] Add running command logging back to SSHOperator (#4716)* [AIRFLOW-3896] Add running command logging back to SSHOperatorAdd the SSH command logging back to SSHOperator* [AIRFLOW-3896] Add running command logging back to SSHOperatorChange self.logger to self.log,2
[AIRFLOW-3932] Optionally skip dag discovery heuristic. (#4746),2
"[AIRFLOW-3697] Vendorize nvd3 and slugifynvd3 has a dependency on python-slugify which pulls in aGPL dependency by default, which we don't want.This commit brings in nvd3 0.15.0 and slugify 2.0.1 WITH NO CHANGES -those will come in the next commit",4
[AIRFLOW-3697] Use vendorized slugify,1
[AIRFLOW-3697] Vendorize nvd3 and slugify (#4513)Merge branch 'AIRFLOW-3697'This PR was done as a merge commit so that we maintain the history ofvendorizing modules and local changes separately.,4
"[AIRFLOW-3940] Migrate Hive Metastore plugin to FAB (#4758)Migrate this plugin from Flask-Admin to Flask-AppBuilder. I tested theplugin by:1. Creating a dummy table named `foo` with a single column in a local Hive cluster2. Configuring the connections `metastore_default` (Thrift) and `metastore_mysql` (MySQL)3. Symlink `airflow/contrib/plugins/metastore_browser/` to `$AIRFLOW_HOME/plugins/`It'd be nice to automate this tests, but I decided not to invest theeffort given that there's some discussion of removing this pluginentirely.`",4
[AIRFLOW-3944] Remove code smells (#4762)- Removed mutable default arguments- Replace function calls by literals- Remove reusing same variable names- Make function name lowercase- Remove redundant parentheses- Remove unreachable code- Replaced unncessary list by set in dataproc_operator.py,5
[AIRFLOW-3926] Remove references to Flask-Admin (#4759)Remove all remaining references to Flask-Admin and remove it as adependency. This should be the final step in the deprecation ofFlask-Admin in favor of Flask-AppBuilder.,4
[AIRFLOW-3932] Update unit tests and documentation for safe mode flag. (#4760),2
[AIRFLOW-3945] Stop inserting row when permission views unchanged (#4764)- Stop inserting a row where only the id is not NULL in table  ab_permission_view_role when there are no permission views to update.- Add a test for above issue,0
[AIRFLOW-XXX] Add 4G Capital to list of Airflow users. (#4771),1
[AIRFLOW-3947] Flash msg for no DAG-level access error (#4767)* [AIRFLOW-3947] Flash msg for no DAG-level access errorIt will show and remind user when a user clicks on a DAG thathe/she doesn't have can_dag_read or can_dag_edit permissions.* Change the flash msg contents,4
[AIRFLOW-3929] Use anchor tags for modal links on dag detail pages. (#4742),2
[AIRFLOW-3950] Improve AirflowSecurityManager.update_admin_perm_view (#4774)- simplify the implementation- improve the performance,1
"[AIRFLOW-XXX] Clarify enforcement of dagrun_timeout (#4782)As recently brought up on the mailing list, the behavior of the`dagrun_timeout` setting for DAGs is not very intuitive (we've beenbitten by this before, as well). I've attempted to clarify thescheduler's logic in regard to this parameter in the docstring.",2
[AIRFLOW-XXX] Fix typos in AirflowSecurityManager.has_access (#4778)[ci skip],2
[AIRFLOW-XXX] Correct BranchPythonOperator docs (#4745),2
"[AIRFLOW-1814] : Temple PythonOperator {op_args,op_kwargs} fields (#4691)",1
"[AIRFLOW-3931] set network, subnetwork when launching dataflow template (#4744)",5
[AIRFLOW-XXX] Fix syntax docs errors (#4789)Co-authored-by: Jarek Potiuk <jarek.potiuk@polidea.com>,0
[AIRFLOW-XXX] Add history become ASF top level project (#4757),1
[AIRFLOW-3867] Rename GCP's subpackage (#4690),5
AIRFLOW-3543: Fix deletion of DAG with rescheduled tasks (#4646),2
[AIRFLOW-XXX] Update pull request template to include AIP info (#4765),5
[AIRFLOW-3795] provide_context param is now used (#4735)* provide_context param is now used* Fixed new PythonVirtualenvOperator test,3
[AIRFLOW-3881] Correct to_csv row number (#4699),5
[AIRFLOW-XXX] Add GitLab to list of organizations using Airflow (#4798),1
[AIRFLOW-XXX] Fix CI for broken lib (#4800),0
[AIRFLOW-2767] - Upgrade gunicorn to 19.5.0 to avoid moderate-severity CVE (#4795)Upgrade gunicorn to 19.5.0 to avoid moderate-severity CVE,5
[AIRFLOW-3906] Add npm compile to docker file (#4724),2
"[AIRFLOW-3870] Update log level and return value (#4355)After using this operator, it's very useful to see which file is getting downloaded. Changed the debug logging to info. Also, return the filepath into xcom so that downstream tasks can use it.",1
[AIRFLOW-3766] Add support for kubernetes annotations (#4589),1
[AIRFLOW-3962] Added graceful handling for creation of dag_run of a dag which doesn't have any task (#4781),2
"[AIRFLOW-3918] Add ssh private-key support to git-sync for KubernetesExecutor (#4777)Add configuration for git SSH auth and update git-sync version intemplate (mutually exclusive of user authentication).Update git-sync version to the latest, current version did notsupport SSH authentication environment variablesSecurity context was required to read the mounted SSH key forgit-sync SSH authenticationAdd example of configmap + Kubernetes secret snippet in config template",5
[AIRFLOW-2511] Fix improper failed session commit handling causing deadlocks (#4769),1
[AIRFLOW-XXX] Pin version of tornado pulled in by Celery. (#4815)https://github.com/tornadoweb/tornado/issues/2604,0
[AIRFLOW-3992] 1-setup-env.sh should be re-runable (#4817),1
[AIRFLOW-3983] Exclude node_modules from being linted by flake8 (#4809),5
[AIRFLOW-3975] Handle null inputs in attribute renderers. (#4799),0
"[AIRFLOW-3977] Add examples of trigger rules in doc (#4805)The current LatestOnlyOperator will skip all downstream tasks blindly.The doc shows a wrong behavior. It also shows an incorrect example aboutthe interaction between skipped tasks and trigger rules. I replace it withanother example using BranchingOperator in schedule level.This fix can resolve this ticket:https://issues.apache.org/jira/browse/AIRFLOW-3977Also,https://issues.apache.org/jira/browse/AIRFLOW-1784",0
[AIRFLOW-4001] Update docs about how to run tests (#4826)fix docs,2
[Airflow-XXX] Update GoDataDriven owner list in ReadMe (#4812),5
[AIRFLOW-XXX] Unpin cryptography (2.6.1 fixes issue in 2.6) (#4801),0
"[AIRFLOW-XXX] Solve lodash security warning (#4820)We don't use lodash at runtime, so this won't affect anything, but it's nice to remove the warning",2
[AIRFLOW-XXX] Add contents to cli (#4825),1
[AIRFLOW-XXX] Split connection guide to multiple files (#4824),2
[AIRFLOW-4000] Return response when no file (#4822),2
[AIRFLOW-3990] Compile regular expressions. (#4813),5
[AIRFLOW-3767] Correct bulk insert function (#4773)* [AIRFLOW-3767] Correct bulk insert functionFix Oracle hook bulk_insert bug whenparam target_fields is None or rowsis empty iterable* change without overwriting variables as Fokko said,4
"[AIRFLOW-3973] Commit after each alembic migration (#4797)If `Variable`s are used in DAGs, and Postgres is used for the internaldatabase, a fresh `$ airflow initdb` (or `$ airflow resetdb`) spams thelogs with error messages (but does not fail).This commit corrects this by running each migration in a separatetransaction.",1
[AIRFLOW-3793] Decommission configuration items for Flask-Admin web UI & related codes (#4637),5
[AIRFLOW-3353] Upgrade Redis client (#4834)Now that Celery/Kombu have updated and work with RedisPy 3.x (they infact force us to use 3.2) we should re-introduce this change.,4
[AIRFLOW-4006] Make better use of Set in AirflowSecurityManager (#4833)* [AIRFLOW-4006] Make better use of Set in AirflowSecurityManagerFor performance & simplicity* Use more neat method for Set union operation,1
[AIRFLOW-4011] Add Classmethod as an Airflow user (#4837),1
[AIRFLOW-3830] Remove DagBag from /dag_details (#4831)* Remove DagBag from /dag_details* Fixing method call* Adding sync to db calls,5
[AIRFLOW-3938] QuboleOperator Fixes and Support for SqlCommand (#4832),1
[AIRFLOW-3758] Fix circular import in WasbTaskHandler (#4601)WasbHook was causing a circular import error when configure_logging() was called.,5
"[AIRFLOW-4012]  - Upgrade tabulate to 0.8.3 (#4838)Pin upper version bound to 0.9 too, so we don't have to update this againuntil 0.9 is out (when we can check for compatability)",5
[AIRFLOW-4016] Clear runs for BackfillJobTest (#4839)* Clear runs for BackfillJobTest* Fixing import* Fixing flake8,0
[AIRFLOW-XXX] Add note about backwards incompatible changes (#4843),4
[AIRFLOW-4019] Fix AWS Athena Sensor object has no attribute 'mode' (#4844),0
[AIRFLOW-4020] Remove viewer DAG edit permissions (#4845),2
[AIRFLOW-4024] Improve local client tests (#4850)* Improve local client tests* remove self.session* Fixing import,2
[AIRFLOW-3895] GoogleCloudStorageHook/Op create_bucket takes optional resource params (#4717)This makes it possible to supply missing request-parameters whencreating a bucket in Google Cloud Storage,1
"[AIRFLOW-3258] K8S executor environment variables section. (#4627)* [AIRFLOW-3258] K8S executor, optional mount current env in worker pods* fix(AIRFLOW-3258): fix tests* fetch config in KubeConfig init* fix(AIRFLOW-3258): address pr commentsfeat(AIRFLOW-3258): move feature to AIRFLOW-4008fix(AIRFLOW-3258): fix tests* refactor(AIRFLOW-3258): changes for PR",4
"[AIRFLOW-3892] Create Redis pub sub sensor (#4712)* [AIRFLOW-3892] Create Redis pub sub sensor* [AIRFLOW-3892] - Updated based on review comments (#4712)Use redis_conn_id, removed logging from test, corrected unittest import, removed rtype from poke doc* [AIRFLOW-3892] Combined import based on review comments* [AIRFLOW-3892] Removed extra log based on review comments* [AIRFLOW-3892] - Added integration tests based on review comments* [AIRFLOW-3892] - Added example dag to kick off the build* [AIRFLOW-3892] - Removed example dag based on review comments",2
[AIRFLOW-XXX] Split guide for operators to multiple files (#4814),2
[AIRFLOW-4027] Make experimental tests more stateless (#4854),3
[AIRFLOW-2224] Add support CSV files in MySqlToGoogleCloudStorageOperator (#4738)MySqlToGoogleCloudStorageOperator supported export from MySQL innewline-delimited JSON format only. Added support for exportfrom MySQL in CSV format with the option of specifying a fielddelimiter.Thanks to Bernardo Najlis(@bnajlis) for the originalPR(#3139). I made some changes to the the original PR.,4
"[AIRFLOW-3761] Fix `DROP TABLE user` migration for upgrades. (#4840)If you are upgrading an old install you will likely have rows in theuser and known_event tables. We dropped the KnownEvent without amigration to remove it (on purpose) in [AIRFLOW-3468] (#4421) but thismeans if we have any rows in there we won't be able to drop the USERtable:> sqlalchemy.exc.InternalError: (psycopg2.InternalError) cannot drop table users because other objects depend on it> DETAIL:  constraint known_event_user_id_fkey on table known_event depends on table usersSo we need to drop that FK constraint first (if the table exists, whichit won't on a fresh install, as the create migration has been deleted.)",4
[AIRFLOW-3984] Add tests for WinRMHook (#4811)- fix docs- refactoring code[AIRFLOW-3984] Change docs to be clearer about None values,2
[AIRFLOW-2888] Add deprecation path for task_runner config change (#4851)This change allows users to seamlessly upgrade without a hard-to-debugerror when a task is actually run. This allows us to pull the change into 1.10.3,4
[AIRFLOW-3751] Option to allow malformed schemas for LDAP authentication (#4574),1
[AIRFLOW-XXX] Add Veikkaus to Airflow users (#4874),1
[AIRFLOW-3834] Remove dagbag from /log (#4841),2
[AIRFLOW-3841] Remove DagBag from /tree (#4863),2
[AIRFLOW-4047] Remove DagBag from /paused (#4879),1
[AIRFLOW-XXX] Improve airflow-jira script to make RelManager's life easier (#4857)These changes do a number of things:- Add colours to the output so  I can easily see which issues against a  release are still open- Order the Jira issues by update date (so oldest updated are at the  bottom) - the older issuer will likely cherry-pick better if done in  the bottom to top order- Add an `--unmerged` flag to only show un-merged issues in the output- Attempt to find the PR# and merge commit from master branch for a  given jira issue. This won't cope with the case where we have multiple  PRs targeting one issue.,0
"[AIRFLOW-4052] Allow filtering using ""event"" and ""owner"" in ""Log"" view (#4881)In the RBAC UI, users can check Logs. But they could only use ""dag_id"", ""task_id"", ""execution_date"", or ""extra"" to filter, while filtering using ""event"" and ""owner"" will be very useful (to allow users to check specific events that happened, or check what a specific user did).",1
[AIRFLOW-XXX] Avoid spamming the log from Airflow security manager (#4849),2
"[AIRFLOW-4053] Fix KubePodOperator Xcom on Kube 1.13.0 (#4883)Newer versions of Kube return ""failed"" events for the side car containerwhen the ^C causes the python process to exit with 1Kube 1.13 runs a different number of kube-dns pods (2 by default, 1.9and 1.10 ran only 1) so the setup scripts needed changing a little bit.To get a Kube 1.13 cluster I had to upgrade minikube, and it no longerworks on a dist without systemd installed (#systemdsucks) so I had toupdate the travis dist to xenial which is no bad thing!This version of minikube doesn't need the localkube bootstrapper setanymore, it handles driver=none much more gracefully, and some of thepermissions set up for context files/keys needed to be updated.",5
[AIRFLOW-4044] The documentation of `query_params` in `BigQueryOperator` is wrong.  (#4876)* Updated the documentation of `query_params` in BigQueryOperator.* Removed trailing whitespaces.,4
[AIRFLOW-4037] Log response in SimpleHttpOperator even if the response check fails,0
[AIRFLOW-XXX] Add Xiaodong Deng to committers list,1
[AIRFLOW-XXX] Add Hint at user defined macros (#4885),1
[AIRFLOW-4009] Fix docstring issue in GCSToBQOperator (#4836),1
[AIRFLOW-4054] Fix assertEqualIgnoreMultipleSpaces util & add tests (#4886),3
[AIRFLOW-4031] Allow for key pair auth in snowflake hook (#4875)Reference: https://docs.snowflake.net/manuals/user-guide/python-connector-example.html#using-key-pair-authentication,2
[AIRFLOW-XXXX] create user in quick start (#4860),1
[AIRFLOW-3980] Unify logger (#4804),2
[AIRFLOW-3830] Remove unnecessary sync to db (#4853),5
[AIRFLOW-4038] Restructure database queries on /home (#4872),5
[AIRFLOW-4046] Add validations for poke_interval & timeout for Sensor (#4878),5
[AIRFLOW-4033] record stats of task duration (#4858)update docsfix,2
[AIRFLOW-4063] Fix exception string in BigQueryHook (#4899),1
[AIRFLOW-4058] Name models test file to get automatically picked up (#4901)We had `from .models import *` inside test/__init__.py as a kludge around this testfile not being named according to expected Python conventions. Renaming the filemakes more test tools happier (and makes it easier to run a single test file withoutimporting half of the test tree which the current approach suffers from),3
[AIRFLOW-4063] Fix exception string in BigQueryHook [2/2] (#4902),1
[AIRFLOW-XXX] Improvements to formatted content in documentation (#4835)* [AIRFLOW-XXX] Improvements to documentation templates,2
"[AIRFLOW-3272] Add base grpc hook (#4101)* [AIRFLOW-3272] add base grpc hook* [AIRFLOW-3272] fix based on comments and add more docs* [AIRFLOW-3272] add extra fields to www_rabc view in connection model* [AIRFLOW-3272] change url for grpc, fix some bugs* [AIRFLOW-3272] Add mcck for grpc* [AIRFLOW-3272] add unit tests for grpc hook* [AIRFLOW-3272] add gRPC connection howto doc",2
[AIRFLOW-4073] add template_ext for AWS Athena operator (#4907),1
"[AIRFLOW-4076] Correct port type of beeline_default in init_db (#4908)airflow initdb will create default beeline connectionwith port ""10000"", but airflow.models.connectionvariable port is Integer type. It's better to setvalues in same type as int although it could autotransfer",1
[AIRFLOW-XXX] Add note on ASF licensing (#4909)[ci skip],1
"[AIRFLOW-3736] Allow int value in SqoopOperator.extra_import_options(#4906)* now the value of a dict inside extra_export_options can be an int* added modified unit tests test_import_cmd, test_export_cmd and test_popen to include testing for integer values passed in extra_export_options* added ""fetch-size"" containing integer value to extra_import_options to one SqoopOperator inside test_execute",3
[AIRFLOW-XXX] Drop deprecated sudo option; use default docker compose on Travis. (#4732),2
"[AIRFLOW-3997] Extend Variable.get so it can return None when var not found (#4819)This will not change existing regular functions in the `Variable` class. Ifvariable `foo` doesn't exist:```foo = Variable.get(""foo"")-> KeyError```For passing `default_var=None` to get, `None` is returned instead:```foo = Variable.get(""foo"", default_var=None)if foo is None:    handle_missing_foo()```",0
"[AIRFLOW-2190] Send correct HTTP status for base_url not found (#4910)The previous fix for this (#4596) fixed the type error, but didn't sendthe correct status - we were sending a HTTP response of:    HTTP/1.0 404 b'Not Found'which some browsers interpret as a 200!",0
[AIRFLOW-XXX] Add Korbit as an Airflow user (#4917)[ci skip],1
[AIRFLOW-3939] Add Google Cloud Translate operator (#4755),1
"[AIRFLOW-4083] Add tests for link generation utils (#4912)We were making use of the ""bleach"" module or jinja.escape function toclean parameters when it wasn't needed - we could simply call .format onthe Markup object and it will handle escaping for us. (format theobject, not format the string passed to the constructor)This removes the (direct?) dependency on bleach - one less thing todepend on is a good thing too.",4
[AIRFLOW-4002] Option to open debugger on errors in `airflow test`. (#4828),3
[AIRFLOW-4086] Fixed pipefail - set bash shell in the current dockerfile (#4915),2
[AIRFLOW-4087] remove sudo in basetaskrunner on_finish (#4916),5
[AIRFLOW-3862] Check types with mypy. (#4685),5
[AIRFLOW-3917] Specify alternate kube config file/context when running out of cluster (#4859),1
[AIRFLOW-4045] Fix hard-coded URLs in FAB-based UI (#4914),0
"[AIRFLOW-3768] Escape search parameter in pagination controls (#4911)The ""minidom"" we were using from lxml didn't cope with the &gt; etcentities (because it is an XML parser, not an HTML parser): rather thanspecial casing each one I have instead swapped out lxml-based parser forBeautifulSoup which 1) handles these, and 2) is pure-python so is easierto install :)",0
"[AIRFLOW-4100] Correctly JSON escape data for tree/graph views (#4921)It was possible to end up with invalid JS-in-script if you createdcertain structures. This fixes it.`|tojson|safe` is the method recommended by Flask of dealing with thissort of case: http://flask.pocoo.org/docs/1.0/templating/#standard-filtersThe json_ser function is not used anymore (Flask needs an encoder class)so it and the tests have been removed. The AirflowJsonEncoder behavesthe same on dates, and has extra behaviour too.",5
[AIRFLOW-4095] Add template_fields for S3CopyObjectOperator & S3DeleteObjectsOperator (#4920),4
"[AIRFLOW-XXX] Remove verbose test on nosetests (#4913)Something about the tests or how we run them changed and we ended up with a lot more lines appearing in the output, taking us over Travis' ""will display in the UI"" limit, making it harder to debug failures. This isn't a longterm fix, but improves things while we fix the tests for the better.",1
"[AIRFLOW-XXX] Remove old/non-test files that nose ignores (#4930)This file contains tests for a module that was deleted in[AIRFLOW-1582] but the code was deleted in a7a518902dc (in 2017)This file _also_ matches the name of a python module:tests.test_utils.db etc, so having a file with the same name as thepackage is bad/ambigious at best.",2
[AIRFLOW-4107] instrument executor (#4928),2
[AIRFLOW-4070] AirflowException -> log.warning for duplicate task dependencies (#4904)* [AIRFLOW-4070] log.warning for duplicate task dependenciesThis change logs a warning on duplicate task dependencies rather thanraising an AirflowException. This will allow automated taskdependencies to be generated while giving the user the option toexplicitly define task dependencies.* remove test_duplicate_dependencies,3
[AIRFLOW-4124] add get_table and get_table_location in aws_glue_hook and tests (#4942),3
"[AIRFLOW-4122] Remove chain function (#4940)* [AIRFLOW-4122] Remove chain functionBit operation like `>>` or `<<` are suggestedto set dependency, which visual and easier toexplain. and have multiple ways is confusion* change UPDATING.md as recommend[ci skip]",5
[AIRFLOW-4106] instrument staving tasks in pool (#4927),2
[AIRFLOW-4123] Add Exception handling for _change_state method in K8 Executor (#4941),4
"[AIRFLOW-3706] Fix tooltip max-width by correcting ordering of CSS files (#4947)The ""airflowDefaultTheme.css"" file is created by webpack out ofbootstrap-theme.css, which is a ""base"" file. But this was loaded _after_Airflow's main.css, meaning the `max-width` we set on `.tooltip-inner`was being replaced by the default from bootstrap.",1
[AIRFLOW-3615] Preserve case of UNIX socket paths in Connections (#4591),5
[AIRFLOW-4118] instrument DagRun duration (#4946),2
[AIRFLOW-3908] Add more Google Cloud Vision operators (#4791),1
"[AIRFLOW-4129] Escape HTML in generated tooltips  (#4950)Many of these properties aren't user generated, but it is better to besafe",1
"[AIRFLOW-XXX] Note removal, not deprecation of chain in UPDATING.md (#4953)",5
[AIRFLOW-XXX] Upgrade FAB to latest version (#4955),3
[AIRFLOW-XXX] Update README with contacts at zego (#4954),5
[AIRFLOW-4057] statsd should handle invalid characters (#4889),0
[AIRFLOW-4127] Correct AzureContainerInstanceHook._get_instance_view's return (#4945),1
"[AIRFLOW-3737] Kubernetes executor cannot handle long dag/task names (#4636)Kubernetes has a 63char limit on label values, so when running either asubdag or a dag with a name with a length longer than 63 chars, the podcreation process will fail with the following error:    [2019-01-21 08:07:05,337] {rest.py:219} DEBUG - response body:    {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""unable    to parse requirement: invalid label value:    \""very_long_dag_name.very_long_task_name\"": must be no more than 63    characters"",""reason"":""BadRequest"",""code"":400}Annotations however, allow for a relatively unrestricted length whenstoring them against pods, and as such would be better suited in thissituation.Generate deterministic random data for dag_id and task_idTo ensure testing consistency, when calling the '_gen_random_string()'function, we want to set the seed dependant on the string length anditeration within the `cases.extend` loop.This allows random values to be created (as required) but ensuringconsistency of these values during each and every time we run testsagainst the code.",3
[AIRFLOW-4144] add description of is_delete_operator_pod (#4943)[AIRFLOW-4144] add description of is_delete_operator_pod,4
[AIRFLOW-XXX] Add Daniel to committer list (#4961),1
[AIRFLOW-4112] Remove beeline_default in default connection (#4934)We use both `beeline_default` and `hive_cli_default`as our default when we run `airflow initdb`. But inairflow source folder we only use `hive_cli_default`as conn_id in hive related hook/operator and onlyuse `beeline_default` in airflow test folder fortest hive hook/operator. That why I think we shouldmerge then as one default connectionAnd in [Hive doc](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-DeprecationinfavorofBeelineCLI) couldknow that hive cli will be deprecation in favor ofBeeline. In this situation I think we should remove`beeline_default` and change `hive_cli_default` assame configure as `beeline_default`,5
[AIRFLOW-4131] Make template undefined behavior configurable. (#4951),5
[AIRFLOW-3541] Add Avro logical type conversion to bigquery hook (#4553),1
[AIRFLOW-3659] Create Google Cloud Transfer Service Operators (#4792)Co-authored-by: Antoni Smolinski <antoni.smolinski@polidea.com>,1
[AIRFLOW-XXX] Fix Flake8 issues,0
"[AIRFLOW-4145] Allow RBAC roles permissions , VM to be overridable (#4960)",1
[AIRFLOW-XXX] Add to the list of engineers working on Airflow at Bombora Inc (#4964)Add to the list of engineers working on Airflow at Bombora IncSigned-off-by: Alex Johnson <ajohnson@bombora.com>,1
[AIRFLOW-3982] Update DagRun state based on its own tasks (#4808),2
[AIRFLOW-1557] Backfill should respect pool (#4949)The number of backfill tasks should be limitedby either non_pooled_backfill_task_slot_countor the open slots of pool.,5
[AIRFLOW-4062] Improve docs on install extra package commands (#4897)Some command for installing extra packages are`pip install apache-airflow[devel]` we shouldclear install extra package command to`pip install 'apache-airflow[devel]'`[ci skip],2
"Revert ""[AIRFLOW-4062] Improve docs on install extra package commands (#4897)"" (#4965)This reverts commit d4655c506e0439e9cb7cc962d9ce9fbb8f7c6b3b as it causes doc test warnings/failures.",2
[AIRFLOW-3988] Airflow CLI - Sort commands alphabetically (#4962),5
[AIRFLOW-XXX] Fix race condition in CI test (#4968),3
[AIRFLOW-3987] Unify GCP's Connection IDs (#4818),5
"[AIRFLOW-3743] Unify different methods of working out AIRFLOW_HOME (#4705)There were a few ways of getting the AIRFLOW_HOME directory usedthroughout the code base, giving possibly conflicting answer if theyweren't kept in sync:- the AIRFLOW_HOME environment variable- core/airflow_home from the config- settings.AIRFLOW_HOME- configuration.AIRFLOW_HOMESince the home directory is used to compute the default path of theconfig file to load, specifying the home directory Again in the configfile didn't make any sense to me, and I have deprecated that.This commit makes everything in the code base use`settings.AIRFLOW_HOME` as the source of truth, and deprecates thecore/airflow_home config option.There was an import cycle form settings -> logging_config ->module_loading -> settings that needed to be broken on Python 2 - so Ihave moved all adjusting of sys.path in to the settings module(This issue caused me a problem where the RBAC UI wouldn't work as itdidn't find the right webserver_config.py)",5
[AIRFLOW-3423] Fix mongo hook to work with anonymous access (#4258)If no login/password was supplied it would generate an invalid DSN,4
"[AIRFLOW-4062] Improve docs on install extra package commands (#4966)Some command for installing extra packages like`pip install apache-airflow[devel]` cause errorin special situation/shell, We should clear themby add quotation like`pip install 'apache-airflow[devel]'`",1
[AIRFLOW-4093] AWSAthenaOperator-Throw exception if job failed/cancelled/reach max retries (#4919),0
[AIRFLOW-XXX] Add Beeswax as a company who uses airflow (#4976)[ci skip],1
"[AIRFLOW-4057] Fix bug in stat name validation (#4974)* [AIRFLOW-4057] Fix bug in stat name validationThe `@validate_stats` wrapper is wrapping an instance method, so it's firstargument is actually an instance of `SafeStatsdLogger`, not the stat_name.Here's the backtrace that shows up in the webserver logs without the fix:      File ""/Users/andrewstahlman/src/incubator-airflow/airflow/www/views.py"", line 86, in <module>dagbag = models.DagBag(os.devnull, include_examples=False)      File ""/Users/andrewstahlman/src/incubator-airflow/airflow/models/__init__.py"", line 312, in __init__safe_mode=safe_mode)      File ""/Users/andrewstahlman/src/incubator-airflow/airflow/models/__init__.py"", line 594, in collect_dags'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)      File ""/Users/andrewstahlman/src/incubator-airflow/airflow/stats.py"", line 85, in wrapperlog.warning('Invalid stat name: {stat}.'.format(stat=stat), err)    Message: 'Invalid stat name: <airflow.stats.SafeStatsdLogger object at 0x10bf194a8>.'    Arguments: (InvalidStatsNameException('The stat_name has to be a string'),)I've verified the fix by updating the unit tests to exercise the ""public""methods rather than testing the internal validation logic directly.* Wrap stat_name_handler in staticmethod",0
[AIRFLOW-XXX] Add Telia Company to Airflow users (#4978),1
[AIRFLOW-4154] Correct string formatting in jobs.py (#4972),5
[AIRFLOW-XXX] Add another engineer to Bombora Inc's list of engineers (#4977),1
[AIRFLOW-XXX] Update plugin macros documentation (#4971)Make readers aware of how to access a custom plugin macro in theirtemplates,1
[AIRFLOW-2227] Add delete method to Variable class (#4963),4
[AIRFLOW-XXX] Add missing docstring for 'autodetect' in GCS to BQ Operator (#4979),1
[AIRFLOW-4072] enable GKEPodOperator xcom (#4905)this provides consistent functionality with KubernetesPodOperator,1
[AIRFLOW-4160] Fix redirecting of 'Trigger Dag' Button in DAG Page (#4981),2
[AIRFLOW-3811] automatic generation of API Reference in docs (#4788)Co-authored-by: Jarek Potiuk <jarek.potiuk@polidea.com>,2
[AIRFLOW-4104] Add type annotations to common classes. (#4926),1
[AIRFLOW-3937] KubernetesPodOperator support for envFrom configMapRef and secretRef (#4772),5
[AIRFLOW-XXX] Mention Oracle in the Extra Packages documentation (#4987)Add the `oracle` subpackage to the list of available subpackages,1
[AIRFLOW-XXX] Fix flaky test - test_execution_unlimited_parallelism (#4988),3
[AIRFLOW-3419] Fix S3Hook.select_key on Python3 (#4970),1
[AIRFLOW-3417] ECSOperator: pass platformVersion only for FARGATE launch type (#4256),4
[AIRFLOW-XXX] Add to list of Xero engineers (#4995),1
[AIRFLOW-XXX] fix check docs failure on CI (#4998),0
[AIRFLOW-4120] Modify SchedulerJob.manage_slas to respect zero timedelta SLAs (#4939)Modify SchedulerJob.manage_slas to respect zero timedelta SLAs,5
[AIRFLOW-XXX] Remove note about autoclasses (#5001),4
"[AIRFLOW-4173] Improve SchedulerJob.process_file() (#4993)By avoid processing paused DAGs.The actions we avoid here is mainly the dagbag.get_dag() on paused DAGs.DagBag.get_dag() itself is relatively expensive, so this change bringsconsiderable performance improvement.",1
"[AIRFLOW-1526] Add dingding hook and operator (#4895)Dingding is popular team collaboration tools talkjust like Slack. This PR is add it to master, couldhelp us easy send message to Dingding.Add how to operator dingding section tell usershow to send different kind of dingding message orhow to use dingding as callback funtion in DAGand add example to show how to use.Set operator UI color as dingding icon color",1
[AIRFLOW-XXX] 1-setup-env.sh should only run in docker (#5003)[AIRFLOW-XXX] 1-setup-env.sh should only run in docker,2
[AIRFLOW-4177] Check types in tests (#4994),3
"[AIRFLOW-3623] Fix bugs in Download task logs (#5005)They didn't work in Python 3 (Bytes vs Strings), the graph_view wasn'tspecifying the try_number correctly, and the ""All"" logs had ""None"" asthe suffix instead of ""all"".",0
[AIRFLOW-3996] Add view source link to included fragments,2
[AIRFLOW-3458] Move connection tests (#4680),3
[AIRFLOW-4193] Remove code duplication in test_gcp_api_base_hook (#5011),3
[AIRFLOW-4172] Fix changes for driver class path option in Spark Subm… (#4992)* [AIRFLOW-4172] Fix changes for driver class path option in Spark Submit Operator* [AIRFLOW-4172] Fix changes for driver class path option in Spark Submit,4
[AIRFLOW-XXX] Fix typo in README (#5008),2
[AIRFLOW-4014] Change DatastoreHook and add tests (#4842)- update default used version for connecting to the Admin API from v1beta1 to v1- move the establishment of the connection to the function calls instead of the hook init- change get_conn signature to be able to pass an is_admin arg to set an admin connection- rename GoogleCloudBaseHook._authorize function to GoogleCloudBaseHook.authorize- rename the `partialKeys` argument of function `allocate_ids` to `partial_keys`.- add tests- update docs- refactor codeMove version attribute from get_conn to __init__- revert renaming of authorize function- improve docs- refactor code,4
[AIRFLOW-XXX] Fix typo in docs/conf.py,5
"[AIRFLOW-XXX] Update WeTransfer in ""Who uses Apache Airflow"" (#5014)",1
[AIRFLOW-3960] Adds Google Cloud Speech operators (#4780),1
[AIRFLOW-796] Add processor_poll_interval and num_runs to config (#5009),5
[AIRFLOW-4034] Remove unnecessary string formatting with **locals() (#4861)**locals() is used a lot for string formatting. This is consideredbad programming practice.,5
[AIRFLOW-4223] Fix mypy issue in gcp speech_to_text and text_to_speech hooks and operators (#5027)- adds mock.ANY to compat.py,1
[AIRFLOW-XXX] Adds Reverb to the list of official users (#5029),1
[AIRFLOW-4194] Set dag_run state to failed when user terminate backfill (#5016),1
[AIRFLOW-4008] add envFrom for Kubernetes Executor (#4952),1
"[AIRFLOW-XXX] Pin Sendgrid dep. (#5031)Sendgrid just released 6.0 with breaking changes, and I don't have thetime to define how to change our code or tests - as they haven'tpublished a migration guide :(",3
[AIRFLOW-4163] IntervalCheckOperator supports relative diff and not ignore 0 (#4983),1
[AIRFLOW-XXX] Pin psycopg2 due to breaking change (#5036),4
[AIRFLOW-3552] Fix encoding issue in ImapAttachmentToS3Operator (#5040)- change method to upload data to s3 from load_string to load_bytes,5
[AIRFLOW-XXX] Mention that statsd must be installed to gather metrics (#5038),2
[AIRFLOW-4149] add extra gRPC fields to connections forms (#4975),1
[AIRFLOW-4229]  Add missing sqoop connector (#5028)Add missing sqoop connector from HTTP drop-down,4
[AIRFLOW-4235] Add table-hover css class to DAGs table (#5033),2
[AIRFLOW-4246] Flask-Oauthlib needs downstream dependencies pinning due to breaking changes (#5045),4
[AIRFLOW-4247] Template Region on the DataprocOperators (#5046),5
"[AIRFLOW-4240] State-changing actions should be POST requests (#5044)To make the requests POSTs and to follow the redirect that the backendissue I turned the ""toggle"" buttons in to an actual form, which makesthere much less logic needed to build up the URL - the browser handlesit all for us. The only thing we have to do is set the ""action"" on theURL.For the ""link"" ones (delete,trigger,refresh) I wrote a short`postAsForm` which takes the URL and submits a form. A little bit messy,but it works.",1
[AIRFLOW-4248] Fix 'FileExistsError' makedirs race in file_processor_handler (#5047),2
[AIRFLOW-3274] Add run_as_user and fs_group options for Kubernetes (#4648),1
[AIRFLOW-3971] Add Google Cloud Natural Language operators (#4980),1
[AIRFLOW-4069] Add Opsgenie Alert Hook and Operator (#4903),1
revert [AIRFLOW-4122] Remove chain functionReverts 2 commits:- ee71a8bb102dcc3a591c5c175ab88a9043cffb0f- 430efc9afb23d7fe2f88e2bac2e3c45825218410,4
"[AIRFLOW-4220] Change CloudantHook to a new major version and add tests (#5023)- upgrade cloudant version from `>=0.5.9,<2.0` to `>=2.0`- remove the use of the `schema` attribute in the connection- remove `db` function since the database object can also be retrieved by calling `cloudant_session['database_name']`- update docs- refactor code",4
"[AIRFLOW-4232] Add `none_skipped` trigger rule (#5032)Downstream tasks should run as long as their parents are in`success`, `failed`, or `upstream_failed` states.",0
[AIRFLOW-XXX] CHANGELOG and UPDATING for 1.10.3,5
[AIRFLOW-4252] Remove the unused sessions (#5051)* [AIRFLOW-4252] Remove the unused sessionsSome housekeeping* [AIRFLOW-4252] Remove the provide_context,1
[AIRFLOW-XXX] how to setup test env with mysql (#4898),3
[AIRFLOW-XXX] Add newline for DingdingHook doc generation (#5058),2
[AIRFLOW-XXX] Add JULO to company list in readme (#5062),1
[AIRFLOW-XXX] Omit vendor packages from being covered by codecov (#5013),5
"[AIRFLOW-4262] Ensure SlackWebhookHook can take either http_conn_id or webhook_token, or both (#5066)* fix SlackWebhookHook* update docstring",2
[AIRFLOW-XXX] Ignore python files under node_modules in docs (#5063)Some node modules ship .py files which we don't want in our docs.Without this change if you attempt to build docs locally on a tree whichyou have already compiled the www assets it will fail.,0
[AIRFLOW-4261] Minor refactoring on jobs.py (#5065),4
[AIRFLOW-4265] Lineage backend did not work normally (#5067)* add debug log* change SendMessage to staticmethod* updated doc,2
[AIRFLOW-4260] Fix sphinx deprecation warnings (#5064)- replaced autodoc_default_flags by autodoc_default_options- replaced viewcode_import by viewcode_follow_imported_members,2
[AIRFLOW-4259] Move models out of models.py (#5056),4
[AIRFLOW-3677] Improve CheckOperator test coverage (#4756)Add tests for check_operator module- add missing tests cases for CheckOperator- add missing tests cases for ValueCheckOperator- refactor all three classes- replace **locals in str.format by explicit args,4
[AIRFLOW-4256] Remove noqa from migrations (#5055),4
[AIRFLOW-4255] Replace Discovery based api with client based for GCS (#5054),5
[AIRFLOW-4267] Fix TI duration in Graph View (#5071),0
[AIRFLOW-XXX] update SlackWebhookHook and SlackWebhookOperator docstring (#5074),2
[AIRFLOW-XXX] Fix Typo & formatting in Updating.md (#5073),5
"[AIRFLOW-XXX] Build a universal wheel with LICNESE files (#5052)These settings enable us to run `setup.py sdist bdist_wheel` and havethat wheel contian all the license files that we ship in the other distformats.A wheel is a ""binary"" package for python that is quicker to install thana tar.gz -- with one important difference: setup.py isn't ""run"" on theinstalling machine.",1
"[AIRFLOW-XXX] Correct changelog for 1.10.3PR 4476 had an incorrect subjectPR 4119 was a duplicate cherry pick (c4c5b23), but the original changewas in 1.10.1",4
[AIRFLOW-XXX] Change allowed version of Jinja2 to fix CVE-2019-10906 (#5075),0
"[AIRFLOW-XXX] Use Py3.7 on readthedocs (#5078)Previous builds were failing with ""Command killed due to excessivememory consumption"".This updated the RTD config, removes the un-needed extras (which we havemocked for a while now), switches to building on Py3.7. This seems tohelp the docs build again",2
[AIRFLOW-2421] HTTPHook verifies HTTPS certificats by default (#4855)Change the default value of verify from False to True,4
[AIRFLOW-3603] QuboleOperator: Remove SQLCommand from SparkCmd documentation (#4411)[AIRFLOW-3603] QuboleOperator: Remove SQLCommand from SparkCmd Doc,2
Added Arrive (Parkwhiz) to list of users (#5082),1
interia.pl use Airflow too (#5081),1
[AIRFLOW-4269] Minor acceleration of jobs._process_task_instances() (#5076)* [AIRFLOW-4269] Minor acceleration of jobs._process_task_instances()by breaking from unnecessary steps of a for-loop* [AIRFLOW-4269] Improve log.info a bit,5
[AIRFLOW-4251] Instrument DagRun schedule delay (#5050),2
[AIRFLOW-4197] Remove Python2 CI jobs (#5022),4
[AIRFLOW-4293] Fix downgrade in d4ecb8fbee3_add_schedule_interval_to_dag.py (#5086),2
"[AIRFLOW-161] New redirect route and extra links (#5059)Co-authored-by: Max Payton <mpayton@lyft.com>With this change different operators would be able to customize thetask instance model view with extra links, those can be used toredirect users to systems which are out of Airflow.",5
[AIRFLOW-4302] Remove if sys.version_info Python 2 code (#5092),5
[AIRFLOW-4115] Multi-staging Aiflow Docker image (#4936),2
"[AIRFLOW-4055] Add AWS SQS Sensor (#4887)* [AIRFLOW-4055] Add AWS SQS Sensor* [AIRFLOW-4055] Added more tests* [AIRFLOW-4055] Corrected debug log based on review* [AIRFLOW-4055] Corrected documentation based on review comments* [AIRFLOW-4055] Updated based on review comments (changed logging, consolidated exception, documentation)* [AIRFLOW-4055] Updated default conn id and moved hook to the poke method based on review comments* [AIRFLOW-4055] Corrected parenthesis based on review comments* [AIRFLOW-4055] Corrected try block, if conditions and xcom push based comments* [AIRFLOW-4055] Updated to use conn rather than hook* [AIRFLOW-4055] - Updated based on review comments - added moto tests, added boto docs ref, added wait time seconds param* [AIRFLOW-4055] - Updated documentation, changed default arguments based on review feedback",5
[AIRFLOW-4266] Add mypy to setup.py dependencies (#5069)- remove old comment regarding to cloudant upgrade,4
[AIRFLOW-3993] Add tests for salesforce hook (#4829)- refactor code- update docs- change sign_in to get_conn- add salesforce to devel_all packages- add note to UPDATING.mdCo-Authored-By: mik-laj <mik-laj@users.noreply.github.com>,1
[AIRFLOW-4255] Make GCS Hook Backwards compatible (#5089)* [AIRFLOW-4255] Make GCS Hook Backwards compatible* Update UPDATING.md* Add option to stop warnings* Update test_gcs_hook.py* Add tests,3
[AIRFLOW-3672] Add support for Mongo DB DNS Seedlist Connection Format (#4481)* [AIRFLOW-3672] Add support for Mongo DB DNS Seedlist Connection Formathttps://docs.mongodb.com/manual/reference/connection-string/index.html#dns-seedlist-connection-formathttp://api.mongodb.com/python/current/api/pymongo/mongo_client.html#pymongo.mongo_client.MongoClient* [AIRFLOW-3672] Add unit test for srv uri* [AIRFLOW-3672] Fix unit test for Mongo srv uri* [AIRFLOW-3672] Construct MongoDB URI when hook init,5
[AIRFLOW-4311] Remove sleep in localexecutor (#5096),4
[AIRFLOW-XXX] Adding SnapTravel to list of users (#5098),1
[AIRFLOW-4312] - Add template_fields & template_ext to BigQueryCheckO… (#5097),1
[AIRFLOW-XXX] Fix docstrings of SQSHook (#5099),1
[AIRFLOW-4169] Add Google Cloud Vision Detect Operators (#4986),1
[AIRFLOW-XXX] Fix docstrings for CassandraToGoogleCloudStorageOperator (#5103),1
[AIRFLOW-4321] Replace incorrect info of Max Size limit of GCS Object Size (#5106),5
"[AIRFLOW-XXX] Add rat excludes (#5068)If you execute the scripts/ci/6-check-license.sh script in a running airflow env it checks a few more files that don't need to be checked.This includes logs, unittests.cfg, _api (from docs) and sources (from www).",2
[AIRFLOW-4103] Allow uppercase letters in dataflow job names (#4925),5
[AIRFLOW-3934] Increase standard Dataproc PD size (#4749),5
[AIRFLOW-4323] Add 2 tests for WinRMOperator (#5108),1
[AIRFLOW-4322] Add test for VerticaOperator (#5107),1
[AIRFLOW-XXX] Update docstring for SchedulerJob (#5105),2
[AIRFLOW-4332] Upgrade sqlalchemy to remove security Vulnerability (#5113),4
[AIRFLOW-4211] Add tests for WebHDFSHook (#5015),1
[AIRFLOW-4326] Airflow AWS SQS Operator (#5110),1
[AIRFLOW-4299] Upgrade to Celery 4.3.0 to fix crashing workers (#5116),1
"[AIRFLOW-4294] Fix missing dag & task runs in UI dag_id contains a dot (#5111)We used to key on `safe_dag_id` but in that got changed in #4368 to use a more efficient query, which broke for non-sub-DAGs that contain a `.` in their id.Arguably this escaping is a front-end concern anyway, so handling the escaping in the front end makes sense anyway.",1
[AIRFLOW-4313] Remove the Mesos executor (#5115)* [AIRFLOW-4313] Remove the Mesos executor* Update UPDATING.md,5
[AIRFLOW-4308] Fix test-only bug of DST behaviour on python 3.6 (#5095),0
[AIRFLOW-4338] Change k8s pod_request_factory to use yaml safe_load (#5120)- fixes yaml.load deprecation warning,2
"[AIRFLOW-4342] Use @cached_property instead of re-implementing it each time (#5126)It's not many lines, but I just find this much clearer",5
[AIRFLOW-4337] Fix docker-compose deprecation warning in CI (#5119)Since docker-compose 1.21.0 the `–parallel` flag is deprecated.,2
[AIRFLOW-XXX] Add Jarek Potiuk to commiter list (#5132),1
[AIRFLOW-4334] Remove deprecated GCS features & Rename built-in params (#5087),2
"[AIRFLOW-4341] Remove `View.render()` already exists in fab.BaseView (#5125)This behaviour is exactly what the fab.BaseView.render_template functionalready does, and since that exists it makes more sense to use that thandefine our own functionThe two error handlers have been moved out of the class since eventhough they _looked_ like they were instance methods they weren't: the`self` argument was actually an error object.",0
[AIRFLOW-4320] Add tests for SegmentTrackEventOperator (#5104),1
[AIRFLOW-4319] Add tests for Bigquery related Operators (#5101),1
[AIRFLOW-4362] Fix test_execution_limited_parallelism (#5141),3
[AIRFLOW-4335] Add default num_retries to GCP connection (#5117)Add default num_retries to GCP connection,1
[AIRFLOW-4361] Fix flaky test_integration_run_dag_with_scheduler_failure (#5140),3
[AIRFLOW-4307] Backfill respects concurrency limit (#5128)Airflow backfill should respect DAG concurrency limit.,2
[AIRFLOW-4296] Remove py2 in ci process (#5090),4
[AIRFLOW-4268] Add MsSqlToGoogleCloudStorageOperator (#5077),1
[AIRFLOW-4380] Remove enum dependency from setup.py (#5146),1
[AIRFLOW-4377] Remove needless object conversion in DAG.owner() (#5144),2
[AIRFLOW-4200] Remove all __future__ imports (#5020),2
[AIRFLOW-4356] Add extra RuntimeEnvironment keys to DataFlowHook (#5149),5
[AIRFLOW-4199] Remove all sys.version_info[0] == 3 (#5019),5
[AIRFLOW-4379] Remove duplicate code & Add validation in gcs_to_gcs.py (#5145),5
[AIRFLOW-4386] Remove urlparse and replace it with urllib.parse (#5154)* [AIRFLOW-4386] Remove urlparse and replace it with urllib.parse* Replace remaining imports* Update test_utils.py,3
[AIRFLOW-4336] Stop showing entire GCS files bytes in log for gcs_download_operator (#5151),1
[AIRFLOW-XXX] Add Bas Harenslak to committer list (#5157),1
"[AIRFLOW-XXX] Speed up tests for PythonSensor (#5158)This _tried_ to set a timeout to 1 so that it only took 1 second to runthis test, but it ended up sleeping for 60s (the default poke_interval)instead, often making this our slowest individual test case!",3
[AIRFLOW-4208] Replace @abstractproperty by @abstractmethod (#5041),5
"[AIRFLOW-4204] Update super() calls (#5143)Replace super(_class, self) by super() for all filesexcept ones in _vendor.",2
[AIRFLOW-4402] Update super() calls to PY3 for nvd3 (#5168),5
[AIRFLOW-4401] Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167),1
[AIRFLOW-XXX] Move article about defining links (#5170),2
[AIRFLOW-4324] Fix DAG fuzzy search in UI,2
[AIRFLOW-XXX] Fix doc error (#5179),0
"[AIRFLOW-4403] search by `dag_id` or `owners` in UI (#5184)- filtering on both columns restored, not only on `dag_id`.This was the default bahavior, but broken in https://github.com/apache/airflow/commit/a62bc75fce1bb9a1700c55e8cbd24c7c9400fa80.",2
[AIRFLOW-4306] Global operator extra links (#5094)Provide a way to register global operator extralinks through airflow plugins.,2
[AIRFLOW-XXX] Add Bill DeRose to Jetlore in ReadMe (#5186),1
[AIRFLOW-4409] Prevent task duration break by null value (#5178),4
fixed some typos (#5180),2
"[AIRFLOW-4416] Revert ""Reliable SynchronizedQueue used instead of multiprocessing.Queue (#5167)"" (#5191)This reverts commit 5bf9704cc0c4f87a919193de3ed708e198a95f63.",4
"[AIRFLOW-XXX] Reduce log spam in tests (#5174)The change I introduced in #5158 caused this sensor test to keep pokingin a busy loop, resulting in about 4k lines to the logs like:> INFO  [airflow.task.operators] Poking callable: <function PythonSensorTests.test_python_sensor_false.<locals>.<lambda> at 0x7f354b1d0c80>The fix is to make sure that poke_interval is set to _something_ greaterthan zero.",1
"[AIRFLOW-4399] Avoid duplicated os.path.isfile() check in models.dagbag (#5165)The check removed in this commit always return True for sure,because the same check has alreaddy been done earlier in this functionbody",1
[AIRFLOW-4397] Add GCSUploadSessionCompleteSensor (#5166)* [AIRFLOW-4397] Add GCSUploadSessionCompleteSensorThis commit add a GoogleCloudStorageUploadSessionCompleteSensorto address the use case of accepting files from a third party vendorwho refuses to send a success indicator when providing data filesinto a bucket and waiting until an inactivity period has passed toindicate the end of an upload session.,4
[AIRFLOW-3720] Fix missmatch while comparing GCS and S3 files (#4766),2
[AIRFLOW-4168] Create Google Cloud Video Intelligence Operators (#4985),1
[AIRFLOW-XXX] Fix CVE-2019-11358 (#5197),0
[AIRFLOW-XXX] Update readme for Lyft (#5198),5
[AIRFLOW-4228] DatabricksRunNowOperator does not show up under airflow docs (#5171)* Add DatabricksRunNowOperator to integration.rst documentation,2
[AIRFLOW-4361] Fix flaky test_integration_run_dag_with_scheduler_failure (#5182),3
"[AIRFLOW-3143] Support Auto-Zone in DataprocClusterCreateOperator (#5169)Allows you to let GCP decide what zone to put your cluster on by setting zoneto None or a blank string, making the parameter optional. Per the API spec athttps://cloud.google.com/dataproc/docs/reference/rest/v1beta2/ClusterConfig#InstanceGroupConfig,this means that all machineTypeUris have to be in short form.",5
"[AIRFLOW-4394] Don't test behaviour of BackfillJob from CLI tests (#5160)It is slow, and we already have tests of that behaviour. All we need totest is that we call `dag.run()` correctly. Mocking ftw.",2
[AIRFLOW-3626] Fixed triggering DAGs contained within zip files (#4439),2
[AIRFLOW-4159] Add support for additional static pod labels for K8sExecutor (#5134),1
[AIRFLOW-XXX] Add Trocafone to user list (#5203),1
[AIRFLOW-4419] Refine concurrency check in scheduler (#5194),5
"[AIRFLOW-4358] Speed up test_jobs by not running tasks (#5162)We don't care about the behaviour of the tasks in most cases (as we testthose elsewhere) - we just care that the task is ""run"" and ends in astate, which we can all do more directly.On my laptop this takes the time for SchedulerJobTest from 133s to 35s,and reduces the BackfillJobTests from >5mins to mere seconds!",3
[AIRFLOW-4433] - Add missing type in DockerOperator doc string (#5205)Add missing type in DockerOperator docstring for the volumes params,2
[AIRFLOW-4201] Replace unicode strings by normal strings (#5026),5
[Airflow-4136] Fix overwrite of key_file by constructor (#5155)This fix the overwrite of the constructor which sets the key_file to None even if user specified otherwise.,1
[AIRFLOW-XXX] Add Joshua and Kevin as committer (#5207),1
"[AIRFLOW-4401] Use managers for Queue synchronization (#5200)It is a known problem https://bugs.python.org/issue23582 thatmultiprocessing.Queue empty() method is not reliable - sometimes it mightreturn True even if another process already put something in the queue.This resulted in some of the tasks not picked up when sync() methodswere called (in AirflowKubernetesScheduler, LocalExecutor,DagFileProcessor). This was less of a problem if the method was called in sync()- as the remaining jobs/files could be processed in next pass but it was a problemin tests and when graceful shutdown was executed (some tasks could be stillunprocessed while the shutdown occured).We switched to Managers() managed queues to handle that - the queue in this caseis run in a separate subprocess and each process using it uses a proxy to accessthis shared queue. Additionally all Queues() returned by managers are JoinableQueues so we should run task_done() after all processing and we can now performjoin() in termination/end code to wait until all tasks are actually processed,not only retrieved from the queue. That increases gracefulness of shutdown.All the cases impacted follow the same general pattern now:while True:   try:       res = queue.get_nowait()       try:          .... do some processing       finally:           queue.task_done()   except Empty:       breakIn all these cases overhead for inter-processing locking is negligiblecomparing to the action executed (Parsing DAG, executing job)so it appears it should be safe for concurrency as well.",2
[AIRFLOW-4401] Fixup to: Use managers for Queue synchronization (#5208),1
[AIRFLOW-4436] Don't build the same docker image twice in tests (#5209),3
[AIRFLOW-4300] Fix graph modal call when DAG has not yet run (#5185),1
Name default config_file param in KubernetesPodOperator docstring (#5153)Update airflow/contrib/operators/kubernetes_pod_operator.pyCo-Authored-By: leahecole <6719667+leahecole@users.noreply.github.com>Minor grammar tweak,1
AIRFLOW-4218 Support to Provide http args to K8executor while calling k8 python client lib apis (#5060),1
[AIRFLOW-XXX] Add Kamil as committer (#5216),1
[AIRFLOW-4419] Restore used_slots and queued_slots Pool methods (#5210)These are still used in the UI and are helpful for observability ofAriflow,1
[AIRFLOW-4446] Fix typos (#5217),2
[AIRFLOW-4348] Add GCP console link in BigQueryOperator (#5195),1
"[AIRFLOW-XXX] Update G Adventures personnel (#5219)@samuelmullin is no longer at G Adventures, but we miss him. Thanks for everything, Sam!",5
[AIRFLOW-4397] add integrations docs manually for gcs sensors (#5204),2
"[AIRFLOW-3449] Write local dag parsing logs when remote logging enabled. (#5175)The default ""processor"" handler is a FileProcessorHandler (compare toFileTaskHandler as the default ""task"" handler) so setting this as asubclass of FileTaskHandler ended up with an invalid path for theprocessor logs, meaning they never got written to a file when remotelogging is enabled.This was likely a mis-configuration introduced in #2793",5
[AIRFLOW-4434] Support Impala with the HiveServer2Hook (#5206)Fix support for impala in hive hook through a config optionthat turns off set variable commands. Breaking changes wereintroduced in [AIRFLOW-2463].,4
[AIRFLOW-XXX] Adding new contributor to G Adventures (#5222)G Adventures' Airflow-based project has a new contributor[ci skip],1
[AIRFLOW-4450] Fix request arguments in has_dag_access (#5220)has_dag_access needs to use request arguments from bothrequest.args and request.form.This is related to the changes made in AIRFLOW-4240/#5039.,4
[AIRFLOW-4447] Display task duration as human friendly format in UI (#5218),5
[AIRFLOW-4452] Webserver and Scheduler keep crashing because of slackclient update (#5225),5
"[AIRFLOW-4430] Fix ""Zoom into Sub DAG"" link (#5212)",2
[AIRFLOW-XXX] Link to correct class for timedelta in macros.rst (#5226)The `macros.timedelta` variable previously referred to`datetime.datetime` which was incorrect.,5
[AIRFLOW-XXX] update readme for Lyft (#5228),5
[AIRFLOW-4417] Add AWS IAM authenication for PostgresHook (#5223)Enhance the exisitng PostgresHook to allow for IAM authentication forRDS Postgres and Redshift.,1
[AIRFLOW-1501] Add GoogleCloudStorageDeleteOperator (#5230)Add an operator for deleting objects from GCS by either explicit list orall objects matching a prefix. Add test cases for same.,3
[AIRFLOW-4146] Fix CgroupTaskRunner errors (#5224)* fix cgrouptaskrunner* add comment* call super.on_finish() to delete temp cfg files* flake 8* update docstring* add unittests* add header* refactor cgrouptaskrunner test* refactor cgrouptaskrunner test* empty commit to trigger tesgt* nit* add cgroups to setup.py devel_all,1
[AIRFLOW-XXX] Speed up building of Cassanda module on Travis (#5233)Our new environment already sets this env var but we aren't using thisyet,1
[AIRFLOW-XXX] Update super (#5236),5
"[AIRFLOW-4460] Remove __future__ import in models (#5237)Remove _future_ should done inhttps://github.com/apache/airflow/pull/5020, but inthe same time our code base refactor models andmove some code out of _init.py. This PR to remove__future__ in models.",4
[AIRFLOW-4198] Remove all try/import compatibility imports (#5091),2
[AIRFLOW-4207] Metaclass class argument for Python 3 (#5024),5
[AIRFLOW-4197] Remove Python2 CI jobs (#5022) (#5021),4
[AIRFLOW-XXX] Fix mistakes in docs of Dataproc operators (#5192),1
"[AIRFLOW-XXX] Remove incorrect note about Scopes of GCP connection (#5242)The issue raised in https://issues.apache.org/jira/browse/AIRFLOW-2522 was resolved.Scope is not ignored when default credentials are used. This note should be deleted.Scope is read in:https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/gcp_api_base_hook.py#L88-L92When default credentails is used, then this code is used:https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/gcp_api_base_hook.py#L94-L97so scope is passed to the external library.",4
[AIRFLOW-XXX] add Braintree to companies list (#5245),1
braintree-add-2 (#5246),1
"[AIRFLOW-4381] Use get_direct_relative_ids get task relatives (#5147)models.baseoperator get relatives byattribute, but _downstream_task_idsand _upstream_task_ids is private memberThis patch will get relatives fromexists function get_direct_relative_ids",1
[AIRFLOW-4459] Fix wrong DAG count in /home page when DAG count is zero (#5235),2
[AIRFLOW-4376] Remove all past library usage (#5247),4
[AIRFLOW-4467] Add dataproc_jars to templated fields in Dataproc oper… (#5248)* [AIRFLOW-4467] Add dataproc_jars to templated fields in Dataproc operatorshttps://github.com/apache/airflow/pull/5192/ edit docs about:dataproc_pig_jars  in DataProcPigOperatordataproc_hive_jars in DataProcHiveOperatordataproc_spark_jars in DataProcSparkSqlOperatordataproc_hadoop_jars in DataProcHadoopOperatorThe edit mentioned that these fields are tempated but they are not.Raising PR to make them templated as doc suggest.* Fix flake8,0
[AIRFLOW-XXX] Add Telefonica Innovation Alpha to users list (#5252),1
"[AIRFLOW-4448] Don't bake ENV and _cmd into tmp config for non-sudo (#4050)If we are running tasks via sudo then AIRFLOW__ config env vars won't bevisible anymore (without them showing up in `ps`) and we likely mightnot have permission to run the _cmd's specified to find the passwords.But if we are running as the same user then there is no need to ""bake""those options in to the temporary config file -- if the operator decidedthey didn't want those values appearing in a config file on disk, thenlets do our best to respect that.",2
[AIRFLOW-4472] Use json.dumps/loads for templating lineage data (#5253)jinja2 cannot use dict/lists as templates hence convertingit to json solves this while keeping complexity down.,5
Add Skyscanner to companies list (#5258),1
[AIRFLOW-4468] add sql_alchemy_max_overflow parameter (#5249)* [AIRFLOW-4468] add sql_alchemy_max_overflow parameter,2
"[AIRFLOW-4482] Add execution_date to ""trigger DagRun"" API response (#5260)",2
[AIRFLOW-4471] Dataproc operator templated fields improvements (#5250),1
[AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods (#4551)* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* Resolved conflicts.* [AIRFLOW-2955] Fix kubernetes pod operator to set requests and limits on task pods.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* Resolved conflicts.* Resolve conflicts.* [AIRFLOW-2955] Remove bare except to follow flake8.* [AIRFLOW-2955] Remove unused library.* [AIRFLOW-2955] clear up commits.* Resolve nits form @galuszkak and @dimberman.,0
[AIRFLOW-XXX] Documents about task_concurrency and pool (#5262),2
"[AIRFLOW-4420] Backfill respects task_concurrency (#5221)Ensure that backfill respects task_concurrency.That is, the number of concurrent running tasksacross DAG runs should not exceed task_concurrency.",1
[AIRFLOW-3341] FAQ return DAG object example (#4605)* added example of a function returning a dag object,2
[AIRFLOW-4473] Add papermill operator (#5254)Add papermill operator to productize python notebooks.,1
[REVERT] Fix package-lock.json,5
"[AIRFLOW-4487] Move k8s executor from contrib folder to main project (#5261)* Move k8s executor from contrib folderConsidering that the k8s executor is now fully supported by corecommitters, we should move it from contrib to the primary executordirectory.",4
"[AIRFLOW-3888] HA for metastore connection (#4708)* HA for Metastore* [AIRFLOW-3888] HA for metastore connectionCreating a connection to a metasotor with two hosts for high avitablity (eg connection 1, connection 2) is not possible because the entire value entered is taken. For our needs, it is necessary to go through subsequent hosts and connect to the first working.This change allows you to check and then connect to a working metastor.* add function to base_hook* update webhdfs_hook* back to original version* back to original version* Update hive_hooks.pyThank you. I made a few changes because during the tests I detected several errors.I have a question, when I do marge to my pull it will be  still possible to land it in the airflow main branch?* [AIRFLOW-3888] HA for metastore connection flake8 code repair* [AIRFLOW-3888] HA for metastore connection Flake8 repair* [AIRFLOW-3888] HA for metastore connectionCode behavior improvements* [AIRFLOW-3888] HA for metastore connection Add test* [AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionAdd test[AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionAdd test[AIRFLOW-3888] HA for metastore connectiontest improvement[AIRFLOW-3888] HA for metastore connectiontest improvement* [AIRFLOW-3888] HA for metastore connectionImproving the typo in the variable name*  [AIRFLOW-3888] HA for metastore connectionMock return_value edit* [AIRFLOW-3888] HA for metastore connectionFlake8 repair* [AIRFLOW-3888] HA for metastore connectionTest repair* [AIRFLOW-3888] HA for metastore connectionFlake8 repair[AIRFLOW-3888] HA for metastore connectionTest repair",3
"[AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc (#4923)* [AIRFLOW-4092] Add gRPCOperator, unit test and added to auto doc* [AIRFLOW-4092] fix documentation errors* [AIRFLOW-4092] remove hook dispatcher and auth_type as we don't use it now",1
[AIRFLOW-4504] Remove join_args option in run_command() (#5272),1
AIRFLOW-4174 Fix run with backoff (#5213)* AIRFLOW-4174 Fix run with backoff* AIRFLOW-4174 Fix flake 8 issues,0
"[AIRFLOW-4455] dag_details broken for subdags in RBAC UI (#5234)* [AIRFLOW-4455] dag_details broken for subdags in RBAC UI* [AIRFLOW-4455] dag_details broken for subdags in RBAC UIAdd assert to test_sync_to_db to confirm subdag.fileloc == dag.filelocAdd new test test_dag_details_subdag to check dag_details endpoint for subdags.* [AIRFLOW-4479] ImapAttachmentToS3Operator s3_overwrite arg not wired up.Add replace=self.s3_overwrite to s3_hook.load_bytes* [AIRFLOW-4479] ImapAttachmentToS3Operator s3_overwrite arg not wired up.fix test* Revert ""Imap to s3 overwrite not wired up""",4
[AIRFLOW-4084] fix ElasticSearch log download (#5177),2
[AIRFLOW-4503] Support fully pig options (#5271)* Support fully pig options,1
[AIRFLOW-4457] Enhance Task logs by providing the task context (#5264)* [AIRFLOW-4457] Enhance Task logs by providing the task context,1
"[AIRFLOW-4511] Fixes Travis CI stalling at pip install, docker pull (#5278)",2
"[AIRFLOW-XXX] Add Fuller, Inc. to list of Airflow users (#5280)",1
[AIRFLOW-XXX] - Add missing docs for GoogleCloudStorageDeleteOperator (#5274)AIRFLOW-1501 introduced GoogleCloudStorageDeleteOperator.This PR adds the operator to the integration docs.,2
[AIRLOW-XXX] Add Secret Escapes to companies list (#5286),1
[AIRFLOW-4492] Change Dataproc Cluster operators to poll Operations (#5269),1
"Revert ""[AIRFLOW-4511] Fixes Travis CI stalling at pip install, docker pull (#5278)""This reverts commit 795f386f391b5b03ff1de0f846c14527d81334af.",4
[AIRLOW-XXX] Add Blacklane to companies list (#5291),1
[AIRFLOW-4215] Replace mock with unittest.mock (#5292),3
[AIRLOW-XXX] Add Outcome Health to companies list (#5296),1
[AIRFLOW-XXX] Fix missing comma (#5299),0
[AIRFLOW-4532] Optimise iteration in deepcopy (#5295),5
[AIRFLOW-4395] Remove pickle_info view (#5161),5
"[AIRFLOW-XXX] Fix example ""extras"" field in mysql connect doc (#5285)",2
[AIRFLOW-4318] Create Google Cloud Translate Speech Operator (#5102),1
[AIRFLOW-4295] Make `method` case insensitive in HTTPHook (#5173)Make the method: `run` in the HttpHook compare the attribute: 'method' in a case insensitive way.This resolves the issue where a Httphook created with parameter `method='get'` would not betreated as a GET-request in the run method and the attribute `params`would be omitted in the Http request.,2
"Revert ""[AIRFLOW-4295] Make `method` case insensitive in HTTPHook (#5173)""This reverts commit 58105308d8a5d13f6593e2a9fcfd08cf5dc067e8.",4
[AIRFLOW-4537] Remove the mkdir_p function in favour of native Python pathlib (#5301),1
[AIRFLOW-4535] Break jobs.py into multiple files (#5303),2
[AIRFLOW-4519] Optimise operator classname sorting in views (#5282),1
[AIRFLOW] Provide a link to external Elasticsearch logs in UI. (#5164),2
"[AIRFLOW-4491] Add a ""Jump to end"" button for logs (#5266)",2
[AIRFLOW-4393] Add exponential backoff retry (#5284),1
[AIRFLOW-4546] Upgrade google-cloud-bigtable. (#5307)Resolves conflicts among `gcp_api` dependency versions.,5
[AIRFLOW-4528] Cancel DataProc task on timeout (#5293)Make DataProc Operators cancel the underlying dataproc job on timeout.,5
[AIRFLOW-XXX] Fix wrong inline code highlighting in docs (#5309),2
[AIRFLOW-XXX] Fix DaskExecutor formatting in Scheduler doc (#5287),2
Remove mention of pytz compatibility from timezone documentation (#5316),2
[AIRFLOW-4560] Fix Tez queue parameter name in mapred_queue (#5315),2
"[ AIRFLOW-4554] Test for sudo command, add some other test docs (#5310)* AIRFLOW-4554 Unit tests use sudo conditionally",1
[AIRFLOW-4566] Document sla & sla_miss_callback task params (#5322),2
[AIRFLOW-4557] Add gcp_conn_id parameter to get_sqlproxy_runner() of CloudSqlDatabaseHook (#5314),5
[AIRFLOW-4147] Add Operator to publish event to Redis (#4967),1
[AIRFLOW-XXX] Clarify documentation related to autodetect parameter in GCS_to_BQ Op (#5294),2
[AIRFLOW-4295] Make `method` attribute case insensitive in HttpHook (#5313)Make the method: `run` in the HttpHook compare the attribute: 'method' in a case insensitive way.This resolves the issue where a Httphook created with parameter `method='get'` would not betreated as a GET-request in the run method and the attribute `params`would be omitted in the Http request.,2
[AIRFLOW-1464] Batch update task_instance state (#5323),5
[AIRFLOW-4570] Remove future library (#5324)Remove all usage of the future library because we're dropping Python 2 support.,1
"[AIRFLOW-4205] Replace type comments by native Python typing (#5327)We supported typing in Python 2 with mypy & type comments. Now thatwe're dropping Python 2 support, we can switch to native Python types.We support Python 3.5 which doesn't include variable type annotationsyet, so only function arguments and return values are typed.",1
[AIRFLOW-4521] Pause dag also pause its subdags (#5283),2
[AIRFLOW-4571] Add headers to templated field for SimpleHttpOperator (#5326)* [AIRFLOW-4571] Add headers field to templated field for SimpleHttpOperator,1
[AIRFLOW-4442] fix hive_tblproperties in HiveToDruidTransfer (#5211)* [AIRFLOW-4442] fix hive_tblproperties in HiveToDruidTransferHiveToDruidTransfer execute() func uses self.hive_tblproperties.items()if self.hive_tblproperties is None then this raise error.Modify it to empty dict if it's None.* fix dictCo-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-XXX] Add missing extras package to installation.rst (#5325)* [AIRFLOW-XXX] Add missing extras package to installation.rst* Fix error,0
[AIRFLOW-XXX] Add cleartax to companies list (#5331),1
"[AIRFLOW-4343] Show warning in UI if scheduler is not running (#5127)Now that the webserver is more stateless, if the scheduler is notrunning the list of dags won't populate, making it harder for newstarters to work out what is going on.New dep is BSD-2 which is Cat-A under ASF",1
[AIRFLOW-4565] instrument celery executor (#5321)* instrument celery executor* remove unused import* nit* add test to all executors* fix test,3
[AIRFLOW-XXX] Add information about user list (#5341),1
[AIRFLOW-4486] Support IAM Auth in MySqlHook (#5334)update hook and add new test,3
[AIRFLOW-4364] Add Pylint to CI (#5238),1
[AIRFLOW-XXX] add ConnectWise to list of users (#5348),1
[AIRFLOW-XXX] Fix S3FileTransformOp reference typo (#5354),2
[AIRFLOW-4720] Allow comments in .airflowignore files. (#5355),2
[AIRFLOW-4721] Remove all builtin imports (#5357),2
[AIRFLOW-4572] Rename prepare_classpath() to prepare_syspath() (#5328),5
[AIRFLOW-986] HiveCliHook ignores 'proxy_user' value in a connection's extra parameter (#5305)fix HiveCliHook ignores 'proxy_user' value in a connection's extra parameter,2
[AIRFLOW-4598] Task retries are not exhausted for K8s executor (#5347),5
[AIRFLOW-4573] Import airflow_local_settings after prepare_classpath (#5330)Moves the airflow_local_settings import code into a dedicated functionin settings.py and adds a call to it in initialize after prepare_syspath,5
[AIRFLOW-4585] Implement Kubernetes Pod Mutation Hook (#5359),1
"[AIRFLOW-3370] Add stdout output options to Elasticsearch task log handler (#5048)When using potentially larger offets than javascript can handle, they can get parsed incorrectly on the client, resulting in the offset query getting stuck on a certain number. This patch ensures that we return a string to the client to avoid being parsed. When we run the query, we ensure the offset is set as an integer.Add unnecesary prefix_ in config for elastic search section",5
[AIRFLOW-1381] Allow setting host temporary directory in DockerOperator (#5369)Allow user to specify temporary directory to use on the host machine;    default settings will cause an error on OS X due to the standard    temporary directory not being shared to Docker.Based on PR #2418 by benjamin@techcitylabs.com. Closes #2418 #4315,2
"[AIRFLOW-XXX] Add StoneCo to section ""Currently using Airflow"" (#5377)",1
[AIRFLOW-XXX] Add Aizhamal Nurmamat kyzy to contributors list (#5370),1
[AIRFLOW-4501] Register pendulum datetime converter for sqla+pymysql (#5190),5
[AIRFLOW-XXX] changing docutap to experity as it underwent merger (#5338)[ci skip],7
"[AIRFLOW-XXX] Add .github/SECURITY.md (#5329)This commit adds a .github/SECURITY.md file that defines thecontents of the ""Policy"" tab in the new ""Security"" section ofthe GitHub interface.Currently the Policy tab obtains its content from thedocs/security.rst file, which contains technical, non-policyrelated information. This commit retains the""Reporting Vulnerabilities"" section of docs/security.rst, whichis relevant, and strips the extraneous content.",2
[AIRFLOW-4738] Enforce exampleinclude for example DAGs (#5375),2
[AIRFLOW-3160] (Unrevert) Load latest_dagruns asynchronously (#5339),3
[AIRFLOW-XXX] Add missing word in concepts documentation (#5380),2
[AIRFLOW-4521] Don't load the whole DagBag in Pause dag actions (#5342),2
[AIRFLOW-XXX] Fix WS-2019-0032 (#5384),0
[AIRFLOW-XXX] fix typos in README.md (#5390)fix typos,2
[AIRFLOW-4725] Fix setup.py PEP440 & Sphinx-PyPI-upload dependency (#5363),1
[AIRFLOW-4473] Move Papermill guide (#5371),4
[AIRFLOW-4743] Add environment variables support to SSHOperator (#5385)* [AIRFLOW-4743] Add environment variables support to SSHOperator,1
[AIRFLOW-4669] Make airflow/dag Pylint compatible (#5362),2
[AIRFLOW-4364] Allow module names to begin with 0-9 and max 60 chars (#5391),1
[AIRFLOW-4752] Add missing * in build exclusion and generated config (#5392),5
[AIRFLOW-4670] Make airflow/example_dags Pylint compatible (#5361),2
[AIRFLOW-4753] fixes pylint for json log formatter (#5393),2
[AIRFLOW-4116] Dockerfile now supports CI image build on DockerHub (#4937),2
[AIRFLOW-4689] Make setup.py Pylint compatible (#5395),1
[AIRFLOW-4754] Fixed failure when no .git repo is found (#5396),0
[AIRFLOW-4755] Fixed default DOCKERHUB_USER (#5397),2
"[AIRFLOW-3729] Support ""DownwardAPI"" in env variables for KubernetesPodOperator (#4554)https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/#the-downward-api",5
[AIRFLOW-3729] Fix pylint issues from #4554That PR was opened before Pylint was added to the repo so we merged witha green check that then broke things,7
[AIRFLOW-4731] Fix GCS hook with google-storage-client 1.16 (#5368)google-storage-client 1.16 introduced a breaking change where thesignature of client.get_bucket changed from (bucket_name) to(bucket_or_name). Calls with named arguments to this method now fail.This commit makes all calls positional to work around this.,1
[AIRFLOW-4659] Fix pylint problems for api module (#5398),0
[AIRFLOW-4757] Selectively disable missing docstrings for tests (#5400),3
[AIRFLOW-4737] Increase and document celery queue name limit (#5383),2
"[AIRFLOW-4759] Batch queries in set_state API. (#5403)Setting a `dagrun` to success or failure calls set_state for each task in the dag, running multiple database queries for each one. We can reducethe number of queries, and improve performance for the associatedendpoints, by setting the states of all relevant tasks in the same query.",1
"[AIRFLOW-3057] add prev_*_date_success to template context (#5372)* Two new variables are added to template context: prev_execution_date_success and prev_start_date_success.* These return the exec / start dates for the same task in prior successful dag run, without regard to TI status.* Lazy evaluation is employed so that query to look up prev_ti is not executed unnecessarily.",1
[AIRFLOW-4463] Handle divide-by-zero errors in short retry intervals (#5243),1
"[AIRFLOW-4756] add ti.state to ti.start_date as criteria for gantt (#5399)* viewing gantt chart of running dagrun with a task that failed initially but was cleared would result in json encode error* when you clear a TI it only nulls out the state -- not the start_date. so these cleared TIs would still be added to `tis` and thus to gantt but would have no state, and json conversion does not like None type for state.* we should check state in addition to start_date to handle this case",0
[AIRFLOW-4760] Fix zip-packaged DAGs disappearing from DagBag when reloaded (#5404),2
[AIRFLOW-5409] Added name under Who uses Apache Airflow for tracking purpose. (#5409)* Update README.md* Corrected SequenceCorrected Sequence in who uses airflow section,1
[AIRFLOW-4750] Log identified zombie task instances (#5389),2
"[AIRFLOW-3211] Reattach to GCP Dataproc jobs upon Airflow restart  (#4083)* [AIRFLOW-3211] Reattach to GCP Dataproc jobs upon Airflow restartThis change allows Airflow to reattach to existing Dataproc jobs uponscheduler restart, preventing duplicate job submissions. Previously,if the Airflow scheduler restarts while it's running a job on GCPDataproc, it'll lose track of that job, mark the task as failed, andeventually retry. However, the jobs may still be running on Dataprocand maybe even finish successfully. So when Airflow retries and rerunsthe job, the same job will run twice. This can result in issues likedelayed workflows, increased costs, and duplicate data.* [AIRFLOW-3211] Fixed flake8 formatting* Update test with new GCP_PROJECT convention* More flake8 cleanups",4
AIRFLOW-4793 Add signature_name to mlengine operator (#5417)* AIRFLOW-4793 Add signature_name to mlengine operator* Add pydoc,2
[AIRFLOW-4795] Upgrade alembic to latest release. (#5411),3
"[AIRFLOW-4799] don't mutate self.env in BashOperator execute method (#5421)* in tests using bash operator repeatedly, env is populated with contents of environment.* on subsequent runs, render_templates will try to render contents of env.* this produces unpredictable behavior where missing template error may be thrown, or env paths may be replaced with ""template file"" contents",2
[AIRFLOW-4766] Add autoscaling option for DataprocClusterCreateOperator (#5425),5
[AIRFLOW-4800] fix GKEClusterHook ctor calls (#5424),1
[AIRFLOW-4798] obviate interdependencies for dagbag and TI tests (#5422),3
"[AIRFLOW-4777] Simplify python_requires in setup (#5405)From Airflow 2.0 we support Python 3.5+, but not yet Python 4.0",1
[AIRFLOW-4765] Fix DataProcPigOperator execute method (#5426),5
[AIRFLOW-4716] Instrument dag loading time duration (#5350),2
[AIRFLOW-4423] Improve date handling in mysql to gcs operator. (#5196)* Handle TIME columns* Ensure DATETIME and TIMESTAMP columns treated as UTC,5
[AIRFLOW-4781] Add the ability to specify ports in kubernetesOperator (#5410)* [AIRFLOW-4781] Added the ability to specify ports in kubernetesOperator* [AIRFLOW-4781] Added the ability to specify ports in kubernetesOperator* [AIRFLOW-4781] Added the ability to specify ports in kubernetesOperatoradded docstring* [AIRFLOW-4781] Added the ability to specify ports in kubernetesOperatoradd typehintsCo-Authored-By: Fokko Driesprong <fokko@driesprong.frl>* [AIRFLOW-4781] Added the ability to specify ports in kubernetesOperatorfixed docstrings and typehints,2
"AIRFLOW-4740 Accept string `end_date` in DAG default_args (#5381)A dag will accept, in its default_args, a start_date as simple as 2019-06-01. If it detects a string, it converts to a richer type. However, it did not accept a similar string forend_date instead an exception was thrown.That's a very confusing user experience. end_date should be as permissive as start_date",5
[AIRFLOW-4135] Add Google Cloud Build operator and hook (#5251),1
[AIRFLOW-4812] Add batch images annotation (#5433),1
"[AIRFLOW-4807] Make GCS operators, hooks, sensors Pylint compatible (#5434)",1
"[AIRFLOW-4418] Add ""failed only"" option to task modal (#5193)",0
[AIRFLOW-XXX] Updated Readme with Company name and user information (#5437),5
[AIRFLOW-4784] Make GCP operators Pylint compatible (#5432),1
[AIRFLOW-4817] Remove deprecated methods from tests (#5438),3
[AIRFLOW-4819] Fix singleton-comparision errors in pylint (#5440),0
[AIRFLOW-4820] Fix unnecessary-pass errors in pylint (#5441),0
[AIRFLOW-4818] Remove valid files from pylint_todo.txt (#5439),5
Added Bagelcode as an official Apache Airflow user (#5442),1
[AIRFLOW-4048] http_sensor provide Context to response_check (#4890),1
[AIRFLOW-XXX] Add Crealytics to the list of Airflow users (#5446),1
"[AIRFLOW-4591] Make default_pool a real pool (#5349)`non_pooled_task_slot_count` and `non_pooled_backfill_task_slot_count`are removed in favor of a real pool, e.g. `default_pool`.By default tasks are running in `default_pool`.`default_pool` is initialized with 128 slots and user can change thenumber of slots through UI/CLI. `default_pool` cannot be removed.",4
"AIRFLOW-4791 add ""schema"" keyword arg to SnowflakeOperator (#5415)* added role, schema and warehouseadded addition connection params - current hook does not allow for connecting with correct role ie security.  current  hook requires full qualification of database.schema.table_name in query - we should have the ability to have per schema/database connections in Airflow* added extra connection paramaters to operatorAdded role, warehouse and database parameters to the operator.  Currently unable to use operator without qualifying the database.schema.table_name in the sql statement.  Proper security is not applied if user id belongs to multiple roles, so roles is not defined when connecting.* AIRFLOW-4791 readability* AIRFLOW-4791 add to (and improve) docstring",2
[AIRFLOW-4827] Remove compatible test for python 2 (#5448),3
"[AIRFLOW-4739] Add ability to arbitrarily define kubernetes worker pod labels (#5376)Allow task definitions to specify labels on the worker pods thatexecute that task by specifying an extra field in executor_configlike so`executor_config={""KubernetesExecutor"": {""labels"": {""foo"":""bar""}}}`",5
[AIRFLOW-4826] Remove warning from `airflow resetdb` command (#5447)Change engine to connection because error is thrown from new alembic version.Connection is expected.,1
"[AIRFLOW-4233] Remove Template Extension from Bq to GCS Operator (#5456)The issue happens when we have destination url ending with .sql.It is related to the defined template extension template_ext = ('.sql',)The operator looks for jinja template, however, it's an output path, so thefile is not found when looking for any jinja template syntax.The BigQueryToCloudStorageOperator doesn't have sql parameter as it doesn'twork with queries at all. It takes only tables.",1
[AIRFLOW-3746] Fix DockerOperator missing container exit (#4583)switch to cli.attach to prevent missing container exit,2
[AIRFLOW-4759] Don't error when marking sucessful run as failed (#5435)The previous PR (#5403) that optimized the DB queries introduced an edgecase which would cause this to fail if no tasks were passed in.,4
[AIRFLOW-3703] Add dnsPolicy option for KubernetesPodOperator (#4520),1
[AIRFLOW-4479] - Include s3_overwrite kwarg in load_bytes method (#5312),5
[AIRFLOW-2141][AIRFLOW-3157][AIRFLOW-4170] Serialize non-str value by JSON when importing Variables (#4991),2
[AIRFLOW-3958] Support list tasks as upstream in chain (#4779),1
[AIRFLOW-4414] AWSAthenaOperator: Push QueryExecutionID to XCom (#5276)Currently it is not possible to make use of QueryExecutionID (theunique identifier of the query submitted to Athena) in the tasks thatfollow.,1
"[AIRFLOW-4829] More descriptive exceptions for EMR sensors (#5452)EMR sensors, including the EmrStepSensor and the EmrJobFlowSensordon't include failure reasons in exceptions though they alreadyhave that info in the boto3 response from the EMR api. This addsthat info to exceptions.Example exception:> AirflowException: EMR job failed for code: BOOTSTRAP_FAILURE with message Master instance (i-02c25cade3cbfc453) failed attempting to download bootstrap action 1 file from S3",2
Add Zynga to list of users (#5460),1
[AIRFLOW-4836] Fix pylint errors regarding file opening (#5463),2
[AIRFLOW-4839] Fix pylint errors regarding superfluous-parens (#5465),0
[AIRFLOW-4837] Fix pylint errors regarding ungrouped imports (#5464),2
[AIRFLOW-4841] Pin Sphinx AutoApi to 1.0.0 (#5468)Co-authored-by: kaxil <kaxilnaik@gmail.com>,5
[AIRFLOW-4559] JenkinsJobTriggerOperator bugfix (#5318)Previous version provides url to Request object implicitly. Itcreates weird interaction assigning it to 'method' parameterinstead of 'url' which raises'requests.exceptions.MissingSchema: Invalid URL 'None': No schemasupplied'. Fixed making both explicit.,1
[AIRFLOW-4831] conf.has_option no longer throws if section is missing. (#5455),5
[AIRFLOW-4838] Surface Athena errors in AWSAthenaOperator (#5467)When a Athena query results in a failure statethe Athena error message is available from theboto3 response. This commit surfaces that errormessage in the AWSAthenaOperator through theget_state_change_reason in AWSAthenaHook.,1
"[AIRFLOW-4298] Stop Scheduler repeatedly warning ""connection invalidated"" (#5470)This appeared a lot more often with recent versions of SQLAlchemy and isnot a sign of any problem in the _initial_ case, so don't log themessage as a warning the first time we retry.",1
[AIRFLOW-4516] K8s runAsUser and fsGroup cannot be strings (#5429)The securityContext sections  for runAsUser and fsGroup in Poddefinitions are always integers. This PR updates makes surethe configuration returns an integer for both runAsUser andfsGroup.,1
"[AIRFLOW-4456] Add sub-classable BaseBranchOperator (#5231)This adds an abstract BaseBranchOperator, which implementors can use tocreate concrete operators for branching workflows fully encapsulated ina class, rather than needing to pass an external callable (orself-reference) to BranchPythonOperator. Also adds tests for same.It also moves the logic of skipping non-selected branches intoairflow.models.SkipMixin:skip_all_except, and modifiesBranchPythonOperator and BaseBranchOperator to use this mixin instead ofimplementing the logic themselves.",2
[AIRFLOW-4782] Make GCP hooks Pylint compatible (#5431),1
[AIRFLOW-XXX] Remove smart quotes from default config (#5471)It causes problems on Python2 trying to read the file as ASCII,2
[AIRFLOW-4587] Replace self.conn with self.get_conn() in AWSAthenaHook (#5462),1
[AIRFLOW-4805] Add py_file as templated field in DataflowPythonOperator (#5451),5
[AIRFLOW-XXX] Update Mailing List link for removing Mesos Executor (#5476),4
[AIRFLOW-4844] Add optional is_paused_upon_creation argument to DAG (#5473)If this option is set (not None) it will be used in place of what ever the configsetting is. If it's not passed the config will be used.,1
[AIRFLOW-4422] Pool utilization stats (#5453)Add stats to record pool utilization such as open slots and used slots.,1
[AIRFLOW-3217] Button to toggle line wrapping in log and code views  (#4277),2
"[AIRFLOW-XXX] Dump logs in case of kube failure (#5472)Previously we were only dumping the logs in case of _success_, which wassomewhat pointless",2
"[AIRFLOW-3502] Update config template to reflect supporting different Celery pool implementation (#5477)The support to different Celery pool implementation has been addedin https://github.com/apache/airflow/pull/4308.But it's not reflected in the default_airflow.cfg yet, while it'sthe main portal of config options to most users.",1
[AIRFLOW-4845] Fix bug where runAsUser 0 doesn't get set in k8s security context (#5474),1
[AIRFLOW-XXX] Links to Pendulum in macros.rst (#5229)* [AIRFLOW-XXX] Links to Pendulum in `macros.rst`* Add links to the Pendulum library in the relevant part of `macros.rst`Signed-off-by: mr.Shu <mr@shu.io>* [ci skip],2
[AIRFLOW-XXX] Add Bonial International GmbH to who's using Airflow (#5484)[ci skip],1
[AIRFLOW-XXX] Add Grab to the list of Airflow users (#5485)[ci skip],1
[AIRFLOW-4524] Fix incorrect field names in view for Mark Success/Failure (#5486)Another mistake that wasn't caught from #5039 - we renamed the fieldsin the template (to be unique) but didn't update the view,5
[AIRFLOW-4767] Fix errors in the documentation of Dataproc Operator (#5487),1
[AIRFLOW-4857] Add templated fields to SlackWebhookOperator (#5490),1
[AIRFLOW-4860] Remove Redundant Information in Example Dags (#5497),2
"[AIRFLOW-4859] Extend list of pylint good-names (#5496)Nearly 10% of pylint errors are related to invalid-name of variable.So this commit propose to add cm, db, f, dr, op to pylint good-names.",5
[AIRFLOW-XXX] Add Kargo to list of Airflow users (#5500),1
"[AIRFLOW-3935] answer a TODO in airflow/executors/local_executor.py (#4752)To answer why 'raise e' was commented:1. This try-except is inside method execute_work() & there are other operations   after the try-except and after invoking this method.   Raising exception here will prevent all following steps from taking place.2. The exception itself is already marked properly by labelling state to be FAILED,   and the exception is printed out using self.log.error().",2
[AIRFLOW-4840] Fix pylint errors regarding logging-format-interpolation (#5466),2
[AIRFLOW-4862] Allow directly using IP address as hostname (#5501),1
[AIRFLOW-4769] Pass gcp_conn_id to BigtableHook (#5445),1
[AIRFLOW-4849] Add gcp_conn_id to cloudsqldatabehook class to use correctly CloudSqlProxyRunner class (#5478)* add gcp_conn_id to cloudsqldatabehook* Modified gcp sql hook test,3
[AIRFLOW-4237] Including Try Number of Task in Gantt Chart (#5037),2
"[AIRFLOW-4871] Allow creating DagRuns via RBAC UI (#5507)All this needed to enable the form was to add `can_add` to thepermission listThe run_id field is a required form (though the DB doesn't have it asnot nullable) - the scheduler requires it. so I have enabled therequired validation for it.The `validators_columns` attribute on the view was ignored by FABbecause we set `add_form` and `edit_form` directly, so I have removedthe property",5
[AIRFLOW-4864] Remove calls to load_test_config (#5502)We already set the environment variable in the test runner so thatairflow.configuration will do this -- we don't need to do it again,5
[AIRFLOW-4406] Fix a method name typo: NullFernet.decrpyt to decrypt (#5509)fixing typo from decrpyt to decrypt,2
[AIRFLOW-4564] ACI bugfixes and improvements (#5319),1
[AIRFLOW-4862] Fix bug for earlier change to allow using IP as hostname (#5513),1
[AIRFLOW-4885] Add virtualenv dependency (#5518),1
[AIRFLOW-XXX] Add OVH to the list of Airflow users (#5521)[ci skip],1
[AIRFLOW-XXX] Add LokSuvidha to list of companies using Airflow (#5520)[ci skip],1
[AIRFLOW-XXX] Add Growbots as the user of Airflow (#5523)[ci skip],1
[AIRFLOW-XXX] Add City of Toronto to official users list (#5526)[ci skip],1
"[AIRFLOW-4510] Don't mutate default_args during DAG initialization (#5277)While initializing a DAG, default_args is being mutated. If anotherDAG is created in the same file with the same default_args, it getsinitialized with the incorrect Timezone information.",5
[AIRFLOW-4494] Remove `shell=True` in DaskExecutor (#5273)Signed-off-by: MisLink <gjq.uoiai@outlook.com>,2
[AIRFLOW-4896] Make KubernetesExecutorConfig's default args immutable (#5534),5
[AIRFLOW-3360] Make the DAGs search respect other querystring parameters with url-search-params-polyfill for IE support (#5503),1
[AIRFLOW-4891] Extend list of pylint good-names (#5524),5
"[AIRFLOW-4478] Lazily instantiate default resources objects. (#5259)Instantiating `Resources` and its child classes takes non-negligibletime when users create many operators. To save time, don't create the resources object until it is needed.",1
[AIRFLOW-XXX] Add Huq Industries to company list in readme (#5532),1
[AIRFLOW-4895] Import Iterable from collections.abc to fix DeprecationWarning in airflow.utils (#5533),2
"[AIRFLOW-4797] Improve performance and behaviour of zombie detection (#5511)Moved query to fetch zombies from DagFileProcessorManager to DagBag class. Changed query to only look for DAGs of the current DAG bag. The query now uses index ti_dag_state instead of ti_state. Removed no longer required zombies parameters from many function signatures.The query is now executed on every call to DagBag.kill_zombies which is called when the DAG file is processed which frequency depends on scheduler_heartbeat_sec and processor_poll_interval (AFAIU). The query is faster than the previous one (see also stats below). It's also negligible IMHO because during DAG file processing many other queries (DAG runs and task instances are created, task instance dependencies are checked) are executed.",1
"[AIRFLOW-4876] Making tests in CoreTest rerunnable (#5508)- all related DagRun, TaskInstance and TaskFail objects are properly removed on tear down- replaced ""models.<ModelName>"" importing with ""<ModelName>"" direct usage- extracted test specific DAG ids into test suit constants- small linter improvements",1
[AIRFLOW-4900] Resolve incompatible version of Werkzeug (#5535)* [AIRFLOW-4900] Resolve incompatible version of WerkzeugPin Werkzeug version to >= 0.15 as flask requirement needs it.* Update setup.py,1
[AIRLFOW-XXX] Add Revolut to the list of Airflow users (#5544),1
[AIRFLOW-XXX] Adds Beamly to the list of users (#5537),1
[AIRFLOW-4911] Silence the FORBIDDEN errors from the KubernetesExecutor (#5547)* [AIRFLOW-4911] Silence the FORBIDDEN errors from the KubernetesExecutor* switch to WARN* add to show message from ApiException,1
[AIRFLOW-4919] DataProcJobBaseOperator dataproc_properties templated (#5554)* [AIRFLOW-4919] DataProcJobBaseOperator change dataproc_properties to be templatedAdd dataproc_properties to templated fields.,5
[AIRFLOW-4926] Fix example dags where its start_date is datetime.utcnow() (#5553),5
[AIRFLOW-4587] Replace self.conn with self.get_conn() in AWSAthenaHook (#5545),1
[AIRFLOW-XXX] adding a Cryptalizer.com to list of companies (#5529)[ci-skip],1
"[AIRFLOW-4904] Retrieve test config file from $AIRFLOW_TEST_CONFIG (#5540)It defaults to the same location but this allows the location to be changed.We may remove this test config file entirely in the future, but for now this lets thelocation of this file be controlled.",2
[AIRFLOW-4906] Improve debugging for the SparkSubmitHook (#5542),1
"[AIRFLOW-3761] Skip drop constraint for SQLlite, not necessary (#5538)",4
[AIRFLOW-XXX] Add Caesars Entertainment to list of users (#5536)[ci-skip],1
[AIRFLOW-4925] Improve css style for Variables Import file field (#5552),2
[AIRFLOW-4868] Fix typo in kubernetes/docker/build.sh (#5505),2
[AIRFLOW-3495] Validate one of query and query_uri passed to DataProcSparkSqlOperator (#5510)DataProcSparkSqlOperator and DataProcHiveOperator are working either with queryor query_uri. Passing both doesn't make sense so check at construction time.,1
[AIRFLOW-4905] Add colours to flake8 output (#5541),1
[AIRFLOW-1740] Fix xcom creation and update via RBAC UI (#5561),5
[AIRFLOW-XXX] Correct BaseSensorOperator docs (#5562),2
[AIRFLOW-4136] fix key_file of hook is overwritten by SSHHook connection (#5558)Prevent overwrite of key_file by connection if parameter was provided to SSHHook,1
[AIRFLOW-4934] Fix ProxyFix due to Werkzeug upgrade (#5563),0
[AIRFLOW-4934] Bump Flask to resolve Werkzeug ProxyFix (#5571),0
[AIRFLOW-4937] Fix lodash security issue with version below 4.17.13 (#5572)Issue fixed in https://github.com/lodash/lodash/pull/4336,0
[AIRFLOW-XXX] Add Premise to list of companies who use Airflow (#5568),1
[AIRFLOW-XXX] Add Instacart to list of companies who use Airflow (#5575),1
[AIRFLOW-4882] Make GCP tests and examples Pylint compatible (#5522),3
[AIRFLOW-4945] Use super() syntax (#5579),1
[AIRFLOW-4884] Roll up import_errors in UI (#5516),2
[AIRFLOW-4946] Use yield from syntax (#5580),1
[AIRFLOW-4949] Use OSError exception (#5583),0
[AIRFLOW-4947] Remove six types (#5581),4
[AIRFLOW-4944] Use new types syntax (#5578),1
[AIRFLOW-4943] Replace six assertion method with native (#5577),3
[AIRFLOW-4942] Drop six.next (#5576),4
[AIRFLOW-XXX] Disable intersphinx loading of `requests` modules (#5590)Their website is currently not responding making this inventoryunreachable which is causing all our builds to fail.,0
[AIRFLOW-4954] Remove unused variables from tests (#5588),3
[AIRFLOW-4865] Add context manager to set temporary config values in tests. (#5569),3
[AIRFLOW-4929] Pretty print JSON Variables in UI (#5573)- serialize JSON variables with newlines and indentation- use monospace font family for `val` textarea- set height of `val` textarea dynamically,1
[AIRFLOW-4962] Fix Werkzeug v0.15 deprecation notice for DispatcherMiddleware import (#5595),2
[AIRFLOW-XXX] Add ellisdon to list of companies that use airflow (#5599),1
[AIRFLOW-4939] Add default_task_retries config (#5570),5
[AIRFLOW-4959] Add .hql support for the DataProcHiveOperator (#5591),5
[AIRFLOW-4963] Avoid recreating task context (#5596),1
[AIRFLOW-4117] Travis CI uses multi-stage images to run tests (#4938),3
[AIRFLOW-XXX] Add proton.ai to list of airflow users (#5604)[ci-skip],1
[AIRFLOW-3998] Use nested commands in cli. (#4821),1
"AIRFLOW-3791: Dataflow - Support check status if pipeline spans on multiple jobs (#4633)* AIRFLOW-3791: DataflowSupport to check if job is already running before starting java jobIn case dataflow creates more than one job, we need to track all jobs for status* AIRFLOW-3791: DataflowSupport to check if job is already running before starting java jobIn case dataflow creates more than one job, we need to track all jobs for status* Update airflow/contrib/hooks/gcp_dataflow_hook.pyCo-Authored-By: Fokko Driesprong <fokko@driesprong.frl>* Update airflow/contrib/hooks/gcp_dataflow_hook.pyCo-Authored-By: Fokko Driesprong <fokko@driesprong.frl>* Update gcp_dataflow_hook.py* Update dataflow_operator.py* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflow* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowchange default for check if running* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name* Merge branch 'AIRFLOW-3791_Dataflow' of github.com:chaimt/airflow into AIRFLOW-3791_Dataflowmerge redundant code of _get_job_id_from_name",1
"[AIRFLOW-4583] Fixes type error in GKEPodOperator (#5612)Fixes `a bytes-like object is required, not 'str'` error in GKEPodOperator.",1
[AIRFLOW-4775] Fix incorrect parameter order in GceHook (#5613)Fixes incorrect parameter order in GceHook._wait_for_operation_to_completemethod and adds additional tests.,3
[AIRFLOW-4883] Kill hung file process managers (#5605),2
[AIRFLOW-4763] Allow list in DockerOperator.command (#5408),2
[AIRFLOW-4952] Remove unused arguments in tests (#5586),3
[AIRFLOW-XXX] Add next/prev ds not correct in faq (#5454)* [AIRFLOW-XXX] Add next/prev ds not correct in faq,1
[AIRFLOW-XXX] Update docs to accurately describe the precedence of remote and local logs (#5607),2
[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#5626),2
[AIRFLOW-5007] Remove override of python version to 3.6 in tests (#5628),3
[AIRFLOW-4995] Fix DB initialisation on MySQL (#5614)* [AIRFLOW-4995] Fix initdb for MySQL 8.0.16+,5
[AIRFLOW-5014] Fix sphinx doc problem and leaves API docs (#5636)This change fixes autodoc generated documentation problems but alsoleaves generated .rst files in _api folder so that it is easier todebug and fix problems like that in the future.,0
[AIRFLOW-4074] Cannot put labels on Cloud Dataproc jobs (#5606),5
[AIRFLOW-5000] Remove duplicate end_date and reorder template (#5618)* Remove the duplicate end_date in template context* Reorder template context key to make ds together,1
"[AIRFLOW-5004] Branch/image for CI builds is selected via TRAVIS_BRANCH (#5624)When building CI images you need to know whether you use v1-10-test orv1-10-stable branch (and use corresponding image).Currently the default branch is read from _default_branch.sh. This isnot a problem for master/v1-10-test distinction because those are indifferent ""physical"" branches, but v1-10-stable and v1-10-test areactually following one-another (v1-10-test moves first andwhen several commits pass, then stable branch follows).Therefore we need to utilise TRAVIS_BRANCH variable - this variableworks in two modes - for PRs it is the branch that the PR is goingto be merged to, whereas for triggered builds it's the branch fromwhich the build was triggered. This is perfect for us to determinewhich image should be used during build.(cherry picked from commit 0c6c3745d14479a0dea42c02ef8d01e99fd45b32)",1
[AIRFLOW-4997] Support for non-master branches (#5620)(cherry picked from commit 98c17d97947ef58b3dbc11b341c67c4ca562d5a7),5
[AIRFLOW-4929] Improve display of JSON Variables in UI (#5641),5
[AIRFLOW-5008] Fixed missing libmysql-client-dev in Oracle repos (#5629)(cherry picked from commit 2b6c545c823527c0ae836acb28444c4e0046d3f0),0
[AIRFLOW-5031] Added limit on tzlocal release (#5649),1
[AIRFLOW-4999] Local build and build_and_pull work on both images (#5621)(cherry picked from commit e054f146cca9c07f753a6e7da29a26d4289d24e8),1
[AIRFLOW-5002] Diagnostics of getopt fixed for zsh on MacOS (#5623),0
[AIRFLOW-5011] Add typehints for GCP Vision operators (#5632),1
[AIRFLOW-5005] Split kubernetes tests into separate jobs (#5625)(cherry picked from commit 87150e26fb3912dadaaa50b073d5f95ad4df1b0a),3
[AIRFLOW-5001] Moving building image to before_install phase (#5648)(cherry picked from commit 15d78b723db3bf05f845025fcceafe3170563063),5
[AIRFLOW-XXX] Better troubleshooting docs in CONTRIBUTING.md (#5642),2
[AIRFLOW-5021] move gitpython into setup_requires (#5640)Since this dependency is only used in setup.py,1
[AIRFLOW-4883] Bug-fix for Kill hung file process managers  (#5639)Previous PR (#5605) was missing some code after a rebase. This adds the codeand adds unit tests,3
[AIRFLOW-4883] Fix tests on Python 3.5,3
[Airflow 4923] Fix Databricks hook leaks API secret in logs (#5635)* Update databricks operator* Updated token auth to get from extra_dejson* Update test DatabricksHookTokenTest to use get host from 'extra',1
[AIRFLOW-4856] Make git sync run_as_user an config option (#5494)* [AIRFLOW-4856] change hard coded run_as_usertry to use worker_run_as_user* [AIRFLOW-4856] change hard coded run_as_useradd unit test* [AIRFLOW-4856] change hard coded run_as_usercreate new param git_sync_run_as_user* [AIRFLOW-4856] change hard coded run_as_useradd back remove option* [AIRFLOW-4856] change hard coded run_as_userfix Flake8* [AIRFLOW-4856] change hard coded run_as_userfix Flake8* [AIRFLOW-4856] change hard coded run_as_userfix unit test* [AIRFLOW-4856] change hard coded run_as_userchange the default value to it's old 65533,4
[AIRFLOW-4883] Fix tests on Python 3.5 (#5655)`assert_called` and `assert_called_once` are new in Py 3.6,1
[AIRFLOW-XXX] Fix Typos (#5658),2
[AIRLFOW-XXX] Fix constructor parameters docs (#5630),2
[AIRFLOW-5041] just force PYTHON_VERSION variable (#5660)There is no need for python3.6 to be installed in the host.,1
[AIRFLOW-XXX] fix copy/pasta in k8s request factory extract resources (#5657)* fix copy/pasta in k8s request factory extract resorces* fix test KubernetesPodOperatorTest.test_pod_resources,3
[AIRFLOW-4998] Run multiple queries in BigQueryOperator (#5619)Add support to run multiple queries provided in a list. This brings BigQueryOperator in line with other SQL operators and it's own documentation.Furthermore added type annotations and raise an Exception if `sql` is neither a string nor an iterable.,1
[AIRFLOW-XXX] Make parallelism lowercase (#5638),1
[AIRFLOW-3617] Add gpu limits option in configurations for executor and pod (#5643),5
"[AIRFLOW-4880] Add success, failure and fail_on_empty params to SqlSensor (#5488)* [AIRFLOW-4880] Add success, failure and fail_on_empty params",2
[AIRFLOW-5030] fix env var expansion for config key contains __ (#5650),5
[AIRFLOW-3370] Fix bug in Elasticsearch task log handler (#5667),0
[AIRFLOW-5053] Add support for configuring under-the-hood csv writer in MySqlToHiveTransfer Operator (#5669),1
[AIRFLOW-XXX] Ignore rbac node_modules when running pylint (#5670),1
"[AIRFLOW-4981][AIRFLOW-4788] Always use pendulum DateTimes in task in… (#5654)* [AIRFLOW-4981][AIRFLOW-4788] Always use pendulum DateTimes in task instance contextIn certain situations, like when using fixed cron schedules such as `012 * * *`, the `execution_date`, `prev_execution_date`, and`next_execution_date` variables in macros were native python `datetime`objects instead of, as stated in the docs, pendulum `DateTime` objects.",5
[AIRFLOW-4931] Add KMS Encryption Configuration to BigQuery Hook and Operators (#5567)Add KMS to BigQuery,1
[AIRFLOW-XXX] Add missing doc for annotations param of KubernetesPodOperator (#5666),1
[AIRFLOW-5042] Improve mocking in Dataproc operator tests (#5662),3
"[AIRFLOW-5050] Correctly delete FAB permission m2m objects in sync_perms (#5679)Without this fix we can end up in the situation where trying to deletethrows an error:    > psycopg2.IntegrityError: update or delete on table    > ""ab_permission_view"" violates foreign key constraint    > ""ab_permission_view_role_permission_view_id_fkey"" on table    > ""ab_permission_view_role""    > DETAIL:  Key (id)=(57) is still referenced from table    ""ab_permission_view_role"".",4
[AIRFLOW-5064] Switched to python 3.5 (#5678),5
[AIRFLOW-5063] Fix performance when switching between master/v1-10 (#5677),0
Reorganize sql to gcs operators. (#5504),1
"[AIRFLOW-5067] Update pagination symbols (#5682)From using lt & gt (< & >)to lsaquo & rsaquo (‹ & ›), which match the symbols used for first & last.",1
[AIRFLOW-5012] Add typehints for gcp_*_hook.py (#5634),1
"[AIRFLOW-5035] Replace multiprocessing.Manager with golang-""channel"" style (#5615)Under heavy load (and exasperated by having `--run-duration 600`) wefound that the multiprocessing.Manager processes could be left alive.They would consume no CPU as they were just polling on a socket, butthey would consume memory.Instead of trying to track down all the places we might have leaked aprocess I have just removed manager's from the scheduler entirely, andre-written the multiprocessing how I would if I was writing this ingolang channels - passing objects/messages over a single channel, andshutting down when done (so we don't need a ""Done"" signal, and we can.poll() on the channel to see if there is anything to receive.",4
[AIRFLOW-4953] Remove unused variables from core (#5587),1
[AIRFLOW-4951] Use new style classes (#5585),1
[AIRFLOW-4451] Allow templated named tuples (#5673),1
changing log level to be proper library to suppress warning for https://issues.apache.org/jira/browse/AIRFLOW-4590 (#5337),0
"[AIRFLOW-4948] Use items, values method instead six package (#5582)",1
[AIRFLOW-5057] Provide bucket name to functions in S3 Hook when none is specified (#5674)Note: The order of arguments has changed for `check_for_prefix`.The `bucket_name` is now optional. It falls back to the `connection schema` attribute.- refactor code- complete docs,2
[AIRFLOW-5010] Add typehints for core operators (#5631)* [AIRFLOW-5010] Add typehints for core operators* fixup! [AIRFLOW-5010] Add typehints for core operators,1
"[AIRFLOW-5038] skip pod deleted log message when pod deletion is disabled (#5656)When delete_worker_pods is set to False, scheduler should not logPod deleted message.",4
"[AIRFLOW-4961] Insert TaskFail.duration as int match DB schema column type (#5593)When writing a task failure record, convert 'duration' decimalvalue -> an int before persistence, to remove reliance on thedatabase doing this automatically and gracefully.",5
[AIRFLOW-5022] Fix DockerHook for registries with port numbers (#5644)`DockerHook` was failing to authenticate with private Dockerregistries if the registry's `Connection` specified a non-standardport number.,1
"[AIRFLOW-4822] Fix bug where parent-dag task instances are wrongly cleared (#5444)Full matching required in this case, so the regex should start and endwith ""^$"". Blurred matching might result in irrelevant task instances becleared.Also in this commit:* Added independent test dag: `clear_subdag_test_dag`* Polished related unit test: `test_subdag_clear_parentdag_downstream_clear`",3
[AIRFLOW-4811] Implement GCP DLP' Hook and Operators (#5539)Implement GCP DLP' Hook and Operators,1
"[AIRFLOW-XXX] Remove default/wrong values from test config. (#5684)Most of the values I've removed here are the current defaults, so wedon't need to specify them again.The reason I am removing them is that `email_backend` of`airflow.utils.send_email_smtp` has been incorrect since 1.7.2(!) buthasn't mattered until #5379 somehow triggered it. By removing thedefault values it should make it easier to update in future.",5
[AIRFLOW-5065] Add colors to console log (#5681)This commit adds custom formatter for Airflow console log to display colourswhen connected to a TTY,2
[AIRFLOW-2891] allow configurable docker_operator container name (#5689),2
[AIRFLOW-5077] Skip force pulling latest python in CI environment (#5690)The latest python will only be pulled by DockerHub when buildingmaster/v1-10-test - which means that it will eventually catchup with the latest python security releases but it will notslow down the CI builds.,3
"[AIRFLOW-5079] Checklicence test uses own, much smaller image (#5692)",1
[AIRFLOW-5078] User is asked if an image needs to be rebuild (#5691)This is not happening when the user explicitely wants to build imageor in CI environment (DockerHub always builds the image viahooks/build script.,1
[AIRFLOW-5075] Let HttpHook handle connections with empty host fields (#5686),0
[AIRFLOW-4289] fix spark_binary argument being ignored in SparkSubmitHook (#5564),1
[AIRFLOW-2891] Make DockerOperator container_name be templateable (#5696),2
[AIRFLOW-5084] Remove mypy.ini (#5697)`mypy.ini` has been accidentally committed to the repo inAIRFLOW-5063 - caused mypy to fail if the sources arelocally mounted or in IDE/local environment,0
[AIRFLOW-5091] Build epoch is fixed now (#5704)We do not need monthly build epoch as we rebuild from the scratchanyway every time new python patchlevel is released.This happens every 1-2 months.,1
[AIRFLOW-5089] Spanner compatibility bug causes missing imports (#5703),2
"[AIRFLOW-4883] Bug-fix to killing hung file processes (#5706)PRs #5615 and #5605 and fought a bit over this change, and this ishard (but not impossible) to test so we didn't notice.",3
[AIRFLOW-5092] Local CI pull and build also pulls python image (#5705)Needed as the pulled image will usually be rebuilt using thelatest python version.,3
[AIRFLOW-5085] we always pass the default branch name to the build (#5699)TRAVIS_BRANCH is set to TAG when TAG build runs. We should alwayssuse branch and we already have our current branch inhooks/_default_branch.sh and we can use it from there.This seems to be the only way as TRAVIS does not pass the branchin any variable - mainly because we do not know what branch weare in when building a TAG build,1
[AIRFLOW-XXX] Add sentry.io to list of airflow users (#5708)[ci-skip],1
[AIRFLOW-5101] Fix inconsistent owner value in examples (#5712),0
[AIRFLOW-5048] Improve display of Kubernetes resources (#5665),1
[AIRFLOW-4217] Remove all usage of the six library (#5715),4
[AIRFLOW-5108] More informative message when kerberos in ci fails (#5719),0
"utils: fix process races when killing processes (#5721)airflow.utils.helpers.reap_process_group() can throw uncaught OSErrorsif processes exit at the wrong time in its execution. Fix this bycatching all OSErrors that can arise due to a process race, andreturning from them when the error is ESRCH (process not found).",0
[AIRFLOW-4192] Remove tables from the task context variables (#5723),4
[AIRFLOW-XXX] Fix incorrect docstring parameter (#5729),2
[AIRFLOW-5111] Remove apt-get upgrade (#5722),1
[AIRFLOW-4813] Add the client_info parameter during GCP's client library initialization (#5728),5
[AIRFLOW-1772] Add support for cron expression in GoogleCloudStorageObjectUpdatedSensor (#5730),5
[AIRFLOW-5051] Better coverage integration (#5732),3
[AIRFLOW-5122] Normalize *_conn_id parameters in Bigquery operators (#5734),1
[AIRFLOW-5119] Enable building from scratch in CRON jobs (#5733),0
adding 'icon_url' on slack web hook and slack operator (#5724),1
[AIRFLOW-5083] Move image building to before_install for licence (#5695),4
[AIRFLOW-5107] Fix template_fields in GCS ACL operator (#5718)Fixes generation field in GoogleCloudStorageObjectCreateAclEntryOperator,1
[AIRFLOW-4443] Document LatestOnly behavior for external trigger (#5214),3
[AIRFLOW-XXX] Update changelog and updating for 1.10.4 (#5739),5
[AIRFLOW-5127] Gzip support for CassandraToGoogleCloudStorageOperator (#5738),1
[AIRFLOW-XXX] Fix sensors constructor parameters docs (#5742),2
[AIRFLOW-5128] Move provide_gcp_credential_file decorator to GoogleCloudBaseHook (#5741),5
[AIRFLOW-4746] Implement GCP Cloud Tasks' Hook and Operators (#5402)Implement GCP Cloud Tasks' Hook and Operators,1
[AIRFLOW-5103] Pass matching objects in GCSPrefixSensor along via XCom (#5714),0
[AIRFLOW-5125] Add gzip support for AdlsToGoogleCloudStorageOperator (#5737),1
[AIRFLOW-4992] Replace backports configparser by Python native configparser (#5617),5
[AIRFLOW-XXX] Mark CLI docs as reference (#5748),2
[AIRFLOW-4192] Remove end_date and latest_date from task context (#5725),5
[AIRFLOW-4161] BigQuery to Mysql Operator (#5711),1
[AIRFLOW-5130] Use GOOGLE_APPLICATION_CREDENTIALS constant from library (#5744),1
[AIRFLOW-5003] Making AWS Hooks pylint compatible (#5627),1
[AIRFLOW-XXX] Make string type uniform in docstrings (#5750),2
[AIRFLOW-5123] Normalize *_conn_id parameter in GCS operators (#5735),1
[AIRFLOW-5132] Add tests for fallback_to_default_project_id (#5746),3
"[AIRFLOW-4509] SubDagOperator using scheduler instead of backfill (#5498)Change SubDagOperator to use Airflow scheduler to scheduletasks in subdags instead of backfill.In the past, SubDagOperator relies on backfill schedulerto schedule tasks in the subdags. Tasks in parent DAGare scheduled via Airflow scheduler while tasks ina subdag are scheduled via backfill, which complicatesthe scheduling logic and adds difficulties to maintainthe two scheduling code path.This PR simplifies how tasks in subdags are scheduled.SubDagOperator is reponsible for creating a DagRun for subdagand wait until all the tasks in the subdag finish. Airflowscheduler picks up the DagRun created by SubDagOperator,create andschedule the tasks accordingly.",2
[AIRFLOW-4690] Make tests/api Pylint compatible (#5413),3
[AIRFLOW-4956] Fix LocalTaskJob heartbeat log spamming (#5589),2
[AIRFLOW-5045] Add ability to create Google Dataproc cluster with custom image from a different project (#5752),5
[AIRFLOW-5142] Fixed flaky cassandra test (#5758),3
[AIRFLOW-5143] Fix for potentially corrupted .jar (#5759),0
[AIRFLOW-5148] Add Google Analytics to the Airflow doc website (#5763),2
[AIRFLOW-5143] Caching works for Checklicence images (#5762),1
"Revert ""[AIRFLOW-5148] Add Google Analytics to the Airflow doc website (#5763)""This reverts commit 502ed749fee8c1c49e4a8f9180671e32b76a2dbb.",5
[AIRFLOW-5088][AIP-24] Add DAG serialization using JSON (#5701)It implements the method proposed in AIP-24 to serialize DAG. It will be used in DAG persistency in DB to solve webserver scalability issue.,0
"[AIRFLOW-5139] Allow custom ES configs (#5760)* AIRFLOW-5139 Allow custom ES configsWhile attempting to create a self-signed TLS connection between airflowand ES, we discovered that airflow does now allow users to modify theSSL state of the elasticsearchtaskhandler. This commit will allow usersto define ES settings in the airflow.cfg",5
Adding AloPeyk to the list of companies using Apache Airflow (#5773),1
[AIRFLOW-XXX] Add iS2.co to list of airflow users (#5772)[AIRFLOW-XXX] Add iS2.co to list of airflow users,1
[AIRFLOW-5159] Checklicence image is not built when not needed (#5774),5
[AIRFLOW-4835] Refactor operator render_template (#5461)- Refactors `BaseOperator.render_template()` and removes `render_template_from_field()`. The functionality could be greatly simplified into a single `render_template()` function.- Removes six usage.- Improves performance by removing two `hasattr` calls and avoiding recreating Jinja environments.- Removes the argument `attr` to `render_template()` which wasn't used.- Squashes multiple similar tests into two parameterized tests.- Adheres to 110 line length.- Adds support for templating sets.- Adds Pydoc.- Adds typing.,1
"[AIRFLOW-5100] Respect safe_mode configuration setting when parsing DAG files (#5757)The scheduler calls `list_py_file_paths` to find DAGs to schedule. It does sowithout passing any parameters other than the directory. This means thatit *won't* discover DAGs that are missing the words ""airflow"" and ""DAG"" evenif DAG_DISCOVERY_SAFE_MODE is disabled.Since `list_py_file_paths` will refer to the configuration if`include_examples` is not provided, it makes sense to have the same behaviourfor `safe_mode`.",1
[AIRFLOW-5153] Option to force delete non-empty BQ datasets (#5768)This gives the option to delete the dataset as well as its tables.,5
[AIRFLOW-5169] Pass GCP Project ID explicitly to StorageClient in GCSHook (#5783),1
[AIRFLOW-4230] BigQuery schema update options should be a list (#5766),5
[AIRFLOW-5135] Use gapic ClientInfo in GoogleCloudBaseHook (#5749)Use from google.api_core.gapic_v1.client_info import ClientInfobecause this object inherits fromfrom google.api_core.client_info import ClientInfoand implements one additional method used by Python SDKs.,1
[AIRFLOW-5124] Add gzip support for S3ToGoogleCloudStorageOperator (#5736),1
[AIRFLOW-4686] Make dags Pylint compatible (#5753),2
"[AIRFLOW-5104] Set default schedule for GCP Transfer operators (#5726)The GCS Transfer Service REST API requires that a schedule be set, even forone-time immediate runs. This adds code to`S3ToGoogleCloudStorageTransferOperator` and`GoogleCloudStorageToGoogleCloudStorageTransferOperator` to set a defaultone-time immediate run schedule when no `schedule` argument is passed.",4
[AIRFLOW-5131] Create scopes property in GoogleCloudBaseHook (#5745),5
"[AIRFLOW-5114] Fix gcp_transfer_hook behavior with default operator arguments (#5727)`GCPTransferServiceHook.wait_for_transfer_job` defeaults its `timeout`parameter to 60 and assumes it is an integer or at least comparable toone. This is a problem as some of the built-in operators that use itlike `S3ToGoogleCloudStorageTransferOperator` and`GoogleCloudStorageToGoogleCloudStorageTransferOperator` default their`timeout` param to `None`, and when they call this method with theirdefault value, it causes an error. Fix this by allowing`wait_for_transfer_job` to accept a timeout of `None` and fill inappropriate defaults. This also adds functionality to allow it to takea `timedelta` instead of an integer, allows seconds to be any real, asthere is really no need for them to actually be an integer, and fixesthe counting of time for determining timeout to be a bit more accurate.",5
[AIRFLOW-5165] make number of dataproc masters configurable (#5781),5
[AIRFLOW-4843] Allow orchestration via Docker Swarm (SwarmOperator) (#5489)* [AIRFLOW-4843] Allow orchestration via Docker Swarm (SwarmOperator)Add support for running Docker containers via Docker Swarmwhich allows the task to run on any machine (node) whichis a part of your Swarm clusterMore details: https://issues.apache.org/jira/browse/AIRFLOW-4843Built with <3 at Agoda!,0
[AIRFLOW-4222] Add cli autocomplete for bash & zsh (#5789),1
[AIRFLOW-5052] Added the include_deleted params to salesforce make_query (#5717),1
"[AIRFLOW-4908] Implement BigQuery Hooks/Operators for update_dataset, patch_dataset and get_dataset (#5546)Implement BigQuery Hooks/Operators for update_dataset, patch_dataset and get_dataset",5
[AIRFLOW-5133] Keep original env state in provide_gcp_credential_file (#5747),2
[AIRFLOW-5211] Add pass_value to template_fields for BigQueryValueCheckOperator (#5816),1
[AIRFLOW-5209] Bump Sphinx version to fix doc build (#5814),2
"[AIRFLOW-5210] Make finding template files more efficient (#5815)For large DAGs, iterating over template fields to find template files can be time intensive.Save this time for tasks that do not specify a template file extension.",2
"[AIRFLOW-5179] Remove top level __init__.py (#5818)The recent commit 3724c2aa to master introduced a __init__.py file inthe project root folder, which basically breaks all imports in localdevelopment (`pip install -e .`) as it turns the project root into apackage.[ci skip]",2
[AIRFLOW-5183] Preprare documentation for new GCP import paths (#5791),2
[AIRFLOW-XXX] Group references in one section (#5776),5
[AIRFLOW-5161] Static checks are run automatically in pre-commit hooks (#5777),1
[AIRFLOW-XXX] Remove duplicate lines from CONTRIBUTING.md (#5830),4
[AIRFLOW-4285] Update task dependency context defination and usage (#5079),5
[AIRFLOW-5227] Consistent licence for .sql files (#5829),2
[AIRFLOW-5229] Consistent licences to all other files (#5831),2
[AIRFLOW-5225] Consistent licence for all JS files (#5827),2
[AIRFLOW-XXX] Remove 'Setup Test Environment using MySQL' article (#5833),1
[AIRFLOW-5184] Move GCP Natural Language to core (#5792),4
[AIRFLOW-5233] Fixed consistency in whitespace (tabs/eols) + common problems (#5835)* [AIRFLOW-5233] Fixed consistency in whitespace (tabs/eols) + common problems,0
[AIRFLOW-5235] Fixes bug where K8s CI does not properly create user (#5838),1
[AIRFLOW-5152] Change back autodetect default value from False to True in GoogleCloudStorageToBigQueryOperator. (#5771)Set autodetect default value from false to be true to avoid breaking downstreamservices using GoogleCloudStorageToBigQueryOperator but not aware of the newlyadded autodetect field.This is to fix the current regression introduced by #3880,0
"[AIRFLOW-5056] Add argument to filter mails in ImapHook and related operators (#5672)- changes the order of arguments for `has_mail_attachment`, `retrieve_mail_attachments` and `download_mail_attachments`- add `get_conn` function- refactor code- fix pylint issues- add imap_mail_filter arg to ImapAttachmentToS3Operator- add mail_filter arg to ImapAttachmentSensor- remove superfluous tests- changes the order of arguments in the sensors + operators __init__",5
[AIRFLOW-5140] fix all missing type annotation errors from dmypy (#5664),0
[AIRFLOW-5239] Fix listing of pylint test scripts (#5844)List the two separate pylint scripts for use inside the Dockercontainers in CONTRIBUTING.md.,2
[AIRFLOW-XXX] Add Chao-Han to committer list (#5846),1
[AIRFLOW-5237] Less verbose output for CI builds (#5840),5
[AIRFLOW-5187] Move GCP Container to core (#5793)This commit moves GCP Container from contrib to core.For more information check AIP-21.,5
[AIRFLOW-XXX] Add Raízen to list of airflow users (#5848),1
[AIRFLOW-5185] Move GCP Video Intelligence to core (#5794)This commit moves GCP Video Intelligence from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5186] Move GCP Translate to core (#5795)This commit moves GCP Translate from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5244] Add list of standard FAB theme choices (#5849),1
[AIRFLOW-5200] Move GCP PubSub to core (#5803)This commit moves GCP PubSub from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5199] Move GCP Spanner to core (#5802)This commit moves GCP Spanner from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5196] Move Google DLP to core (#5800)This commit moves GCP DLP from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5195] Move GCP Dataflow to core (#5799)This commit moves GCP Dataflow from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5189] Move GCP Vision to core (#5796)This commit moves GCP Vision from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5201] Move GCP Functions to core (#5804)This commit moves GCP Functions from contrib to core.For more information check AIP-21.,5
[AIRFLOW-XXX] Group executors in one section (#5834),5
[AIRFLOW-4667] Make airflow/contrib/task_runner Pylint compatible (#5852),1
[AIRFLOW-5246] Remove unused source constructor parameter in BaseHook,1
[AIRFLOW-5248] Pylint fixes related to source constructor param removal,4
"[AIRFLOW-5234] Rst files have consistent, auto-added license",1
[AIRFLOW-5160] Remove example DAG count test (#5775),3
[AIRFLOW-5245] Add more metrics around the scheduler (#5853),1
[AIRFLOW-4665] Remove contrib/plugins from Pylint todo (#5851)This commit intends to remove files under contrib/plugins outof the Pylint todo list.,2
[AIRFLOW-4846] Allow kube git-sync mode to use existing secret for git credentials (#5475),1
[AIRFLOW-5182] remove unused incorrect import (#5867),2
"AIRFLOW-5258 ElasticSearch log handler, has 2 times of hours (%H and %I) in _clean_execution_date instead of %H and %M (#5864)",5
[AIRFLOW-5252] Move GCP Transfer to core (#5858)This commit moves GCP Transfer from contrib to core.For more information check AIP-21.,5
[AIRFLOW-XXX] Fixed Azkaban link (#5865)This was done in the top level README.me in abbb1eadab in 2016.,2
[AIRFLOW-5204] Shellcheck + common licences + executable shebangs in shell files (#5807)* [AIRFLOW-5204] Shellcheck + common licence in shell files,2
[AIRFLOW-5263] Show diff on failure of pre-commit checks (#5869),0
[AIRFLOW-5247] Move NPM dependencies up in the Dockerfile (#5870),2
[AIRFLOW-5257] ElasticSearch log handler errors when attemping to close logs (#5863),2
[AIRFLOW-5253] Move GCP KMS to core (#5859)This commit moves GCP KMS from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5254] Move GCP Tasks to core (#5860)This commit moves GCP Tasks from contrib to core.For more information check AIP-21.,5
[AIRFLOW-XXX] Add doc on specifying SSH Key in SSH connection (#5872),2
[AIRFLOW-5260] Allow empty uri arguments in connection strings (#5855),1
[AIRFLOW-5205] Xml files are checked with xmllint (#5808),5
"[AIRFLOW-1523] Clicking on Graph View should display related DAG run (#5866)Add execution_date_argUse execution_date_arg in graph, gantt, and Back To {parent.dag} links.Add check of execution date",5
[AIRFLOW-4013] Fix Mark Success/Failed picking all execution_date bug (#5616),0
[AIRFLOW-5268] Apply same DAG naming conventions as in literature (#5874),2
[AIRFLOW-5190] Move GCP Compute to core (#5797),4
[AIRFLOW-5255] Move GCP SQL to core (#5861)This commit moves GCP SQL from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5203] Move GCP BigTable to core (#5806)This commit moves GCP BigTable from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5202] Move GCP MLEngine to core (#5805)This commit moves GCP MLEngine from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5197] Move GCP Datastore to core (#5801)This commit moves GCP Datastore from contrib to core.For more information check AIP-21.,5
[AIRFLOW-5193] Move GCP Cloud Build to core (#5798)This commit moves GCP Cloud Build from contrib to core.For more information check AIP-21.,5
[AIRFLOW-XXX] Raise ValueError rather than assert in skip logic (#5875),2
[AIRFLOW-5073] Optionally treat NULL as failure and keep poking in SQL sensor (#5688)Change SQLSensor to treat NULL as fail. Add boolean allow_nullparameter to support legacy behavior passing NULL as success.,4
[AIRFLOW-5276] remove unused is_in helper function (#5878),1
"[AIRFLOW-5206] Common licence in all .md files, TOC + removed TODO.md (#5809)",2
[AIRFLOW-5269] Reuse session in Scheduler Job from health endpoint (#5873),1
[AIRFLOW-XXX] Add Agoda to the users list in README (#5882),1
[AIRFLOW-1498] Add optional analytics script to webserver html (#5850),1
[AIRFLOW-5241] Make all test class names consistent (#5847)Make all test class names consistent by starting with Test,3
[AIRFLOW-5284] Replace warn by warning (#5881),2
[AIRFLOW-5180] Added static checks (yamllint) + auto-licences for yaml file (#5790),2
[AIRFLOW-5027] Grab CloudWatch logs after ECS task has finished (#5645),5
[AIRFLOW-4768] Add timeout parameter to Cloud Video Intelligence operators (#5862),1
[AIRFLOW-5250] Fix dmypy error for gcp hooks (#5856),1
[AIRFLOW-4462] Use datetime2 column types when using MSSQL backend (#5707)Change missing columns to datetime2. All columns of datetime arenow changed to datetime2. Co-authored-by: mattinbits <3765307+mattinbits@users.noreply.github.com>Co-authored-by: sirVir <sirVir@users.noreply.github.com>pylint has poor support for alembic so alembic.op module willfail ci if not ignored.Fix code formatting.,0
"[AIRFLOW-4316] support setting kubernetes_environment_variables config section from env var (#5668)When AIRFLOW_KUBERNETES_ENVIRONMENT_VARIABLES__AIRFLOW_HOME is set inthe scheduler, AIRFLOW_HOME should be set for workers, not airflow_home.",1
[AIRFLOW-5251] add missing typing-extensions dep for py37 (#5857),1
"[AIRFLOW-5145] Don't show (confusing) is_encrypted checkbox in Variable screens (#5761)With webserver `rbac=True` and core fernet_key set, variable values are alwaysencrypted, but they are only marked as is_encrypted = true in the database ifthe user explicitly checks the Is Encrypted checkbox on the variable createscreen. If a variable is set up this way, then it is not correctly displayed inthe UI and calls to Variable.get return the cipher text instead of thedecrypted value.",1
[AIRFLOW-5296] Do Not Pickle DAGs by default (#5895),2
[AIRFLOW-5289] Add body to templated fields for gcp operators. (#5889),1
[AIRFLOW-5218] Less polling of AWS Batch job status (#5825)https://issues.apache.org/jira/browse/AIRFLOW-5218- avoid the AWS API throttle limits for highly concurrent tasks- a small increase in the backoff factor could avoid excessive polling- random sleep before polling to allow the batch task to spin-up  - the random sleep helps to avoid API throttling- revise the retry logic slightly to avoid unnecessary pause  when there are no more retries required,1
[AIRFLOW-XXX] Fix analytics doc (#5885),2
[AIRFLOW-5301] Remove not-yet-existing files from mounts (#5901)This should be only merged if breeze is not yet merged.,7
[AIRFLOW-5285] Pylint pre-commit filters out pylint_todo files (#5884)* [AIRFLOW-5285] Pylint pre-commit filters out pylint_todo files,2
[AIRFLOW-5288] Auto-remove temporary containers for static checks (#5887),4
[AIRFLOW-5287] Base image for chekclicence can be force pulled now (#5886),1
[AIRFLOW-5298] Move FileToGcs to core (#5898)For more information check AIP-21.,5
[AIRFLOW-5297] Move AdlsToGcs operator to core (#5897)For more information check AIP-21.,5
[AIRFLOW-5299] Move SQLToGCS to core (#5896)For more information check AIP-21.,5
[AIRFLOW-5294] Make GCP MLEngine pylint compatible (#5892)This commit make MLEngine operators and tests pylint compatible,3
[AIRFLOW-5302] Fix bug in none_skipped Trigger Rule (#5902),0
"[AIRFLOW-XXX] Create ""Using the CLI"" page (#5823)",1
[AIRFLOW-4771] Improve initialization of hook in the GCP operators (#5893)Hook should be initialized in execute method not in operator's constructor.,1
[AIRFLOW-5305] Sort extra links by name (#5905),2
[AIRFLOW-5309] Use assert_called_once or has_calls in tests (#5912)Using mock.assert_call_with method can result in flaky tests(ex. iterating through dict in python 3.5 which does notstore order of elements). That's why it's better touse assert_called_once_with or has_calls methods.,3
[AIRFLOW-5099] Add Google Cloud AutoML operators (#5720),1
[AIRFLOW-5274] dag loading duration metric name too long (#5890),2
Bump eslint-utils from 1.3.1 to 1.4.2 in /airflow/www (#5918)Bumps [eslint-utils](https://github.com/mysticatea/eslint-utils) from 1.3.1 to 1.4.2.- [Release notes](https://github.com/mysticatea/eslint-utils/releases)- [Commits](https://github.com/mysticatea/eslint-utils/compare/v1.3.1...v1.4.2)Signed-off-by: dependabot[bot] <support@github.com>,1
[AIRFLOW-5306] Fix the display of links when they contain special characters (#5904),2
[AIRFLOW-5304] Fix extra links in BigQueryOperator with multiple queries (#5906),1
[AIRFLOW-5300] Move GcsToService operators to core (#5899)[AIRFLOW-5300] Move GcsToService operators to coreFor more information check AIP-21.,5
[AIRFLOW-5158] Add Google Sheets hook (#5845),1
[AIRFLOW-5315] TaskInstance now only overwrites executor_config when explicitly told… (#5926)* TaskInstance now only overwrites executor_config when explicitly told to do so* flake8,5
[AIRFLOW-5316] Skip running check-apache-license without --all-files (#5917),2
[AIRFLOW-5226] Consistent licences for all jinja templates (#5828),5
[AIRFLOW-3611] Simplified development environment (#4932),5
[AIRFLOW-5317] Remove invalid arguments in tests for GCPTransferServiceWaitForJobStatusSensor (#5921)This commit removes unused  in tests for GCPTransferServiceWaitForJobStatusSensor.,3
[AIRFLOW-XXX] Add PayFit to the users list in README (#5927)And update old username,1
[AIRFLOW-4940] Add DynamoDB to S3 operator (#5663)Add an Airflow operator that replicates aDynamoDB table to S3.,5
[AIRFLOW-5323] Fixed CoreTest -> TestCore in documentation of Breeze,2
[AIRFLOW-5324] Fix hidden dependency on python3 in Breeze,0
[AIRFLOW-5325] Default python version if no python3 on path is 3.5,5
[AIRFLOW=5327] Fixed .bash* files mounting lost during rebase,2
[AIRFLOW-5326] Fixed regression of Apache check running always,1
[AIRFLOW-XXX] Better documentation about resource usage in Breeze,2
[AIRFLOW-5329] Added shared files folder,2
[AIRFLOW-XXX] Fix typo in BREEZE.rst,2
[AIRFLOW-5168] Add tests for Dataproc{*} Operators (#5929),1
[AIRFLOW-5292] Allow ECSOperator to tag tasks (#5891)This commit introduces a parameter for the `ECSOperator` in order topass along tags to any tasks it is running.,1
[AIRFLOW-3705] Fix PostgresHook get_conn to use conn_name_attr (#5841)Update PostgresHook's get_conn method to directly call the specifiedconn_name_attr rather that always using self.postgres_conn_id.Currently subclassing PostgresHook requires overriding thepostgres_conn_id attribute in order to establish a separate connection.Add an additional unit test for this case checking that the subclassedPostgresHook's get_conn calls the correct arguments and that the hookcalls the correction connection_id in get_connection.,1
[AIRFLOW-5322] Fix flaky test - GCP Transfer Service hook (#5931),1
[AIRFLOW-5333] Move init docs to class docs in PubSub (#5938),2
[AIRFLOW-5275] Add support for template parameters in DataprocWorkflowTemplateInstantiateOperator (#5877),5
[AIRFLOW-5118] Add ability to specify optional components in DataprocClusterCreateOperator (#5821),5
[AIRFLOW-XXX] Bump mixin-deep from 1.3.1 to 1.3.2 in /airflow/www (#5941)Bumps [mixin-deep](https://github.com/jonschlinkert/mixin-deep) from 1.3.1 to 1.3.2.- [Release notes](https://github.com/jonschlinkert/mixin-deep/releases)- [Commits](https://github.com/jonschlinkert/mixin-deep/compare/1.3.1...1.3.2)Signed-off-by: dependabot[bot] <support@github.com>,1
[AIRFLOW-5345] Allow SqlSensor's hook to be customized by subclasses (#5946)Due to security reasons we have an alternative PostgreSQL hook that fetches credentials dynamically.We would still like to use the `SqlSensor` functionality as is as our hook is still backed by sqlalchemy.By extracting SqlSensor's `connection.get_hook` into a method we can override the `_get_hook` method without affecting the functionality of the hook.,1
[AIRFLOW-5340] Fix GCP DLP example (#5945)Use do_xcom_push instead of deprecated xcom_push,1
[AIRFLOW-5335] Update GCSHook methods so they need min IAM perms (#5939),1
[AIRFLOW-XXX] Add FullContact to list of companies that use airflow (#5953),1
[AIRFLOW-4833] Allow to set Jinja env options in DAG declaration (#5943),2
[AIRFLOW-5341] Use more precise mock of time.sleep (#5950)Some tests were mocking 'time.sleep' instead of using more precise'airflow.module.path.time.sleep' mock.,1
[AIRFLOW-5346] Add system tests for GKECluster (#5947),3
[AIRFLOW-5330] Add project_id to Datastore hook and operators (#5935),1
[AIRFLOW-5351] Move all GCP Cloud SQL tests in 1 file (#5956),2
[AIRFLOW-5350] Fix bug in the num_retires field in BigQueryHook (#5955),1
[AIRFLOW-5148] Adding GA and privacy notice to website (#5930)Co-authored-by: kaxil <kaxilnaik@gmail.com>Co-authored-by: mik-laj <kamil.bregula@polidea.com>,1
[AIRFLOW-5303] Use project_id from GCP credentials (#5907),1
[Airflow-XXXX] Fix a comment error in _utils.sh,0
[AIRFLOW-5353] Simplify GKEClusterHook (#5960),1
[AIRFLOW-5320] Add system tests for PubSub (#5925),3
[AIRFLOW-5357] Fix Content-Type for exported variables.json file (#5962),2
[AIRFLOW-5335] Simplify GCSHook test (#5958),3
[AIRFLOW-5356] Fix GCP Datastore unit tests (#5961)This commit adds additional mocking for Datastore tests whichtests fallback_to_default_project_id decorator.,3
[AIRFLOW-5363] Fixed building docs in breeze,2
[AIRFLOW-5364] Fix missing port numbers for local ci scripts,0
[AIRFLOW-5314] Create test for new import paths (#5920),2
[AIRFLOW-XXXX] Update BigQuery View section URL to ViewDefinition (#5981),5
[AIRFLOW-5372] Apache license check runs locally on LICENCE changesLicence check for RAT runs too often (every time pre-commit is modified) and itshould only be run (locally) when any of *LICEN[S|C]E* files change. We anyhowrun full check on CI so this is local optimisation (it runs too long while youplay with .pre-commit-config.yaml - and we will probably be able to detect someproblems locally as well when new modules are added.,1
[AIRFLOW-5371] Remove yamllint as prerequisite to run pre-commit,1
[AIRFLOW-5384] Improve dst param info in FileToGCSOperator (#5985)This commit add more info about dst parameter to indicate that the pathmust include file name.,2
[AIRFLOW-XXX] Add Kroton as official user (#5984),1
[AIRFLOW-XXX] Added AMPATH Kenya to list of Airflow Users (#5983),1
"[AIRFLOW-5388] Add airflow version label to newly created buckets (#5987)To tag and track GCP resources spawned from Airflow, we havebeen adding airflow specific label(s) to GCP API service callswhenever possible and applicable.",1
[AIRFLOW-XXX] Added Bexs Bank to list of Airflow Users (#5988),1
[AIRFLOW-5360] Type annotations for BaseSensorOperator (#5966),1
[AIRFLOW-5365] No need to do image rebuild when switching master/v1-10-test (#5972),3
[AIRFLOW-5129] Add typehint to GCP DLP hook (#5980),1
[AIRFLOW-XXX] Adds BlueKiri and Logitrabel to companies using Airflow (#5991)[ci skip],1
[AIRFLOW-5389] better organized scripts for building CI docker deps,2
[AIRFLOW-5367] Remove hook initialization in ctor from BigtableTableWaitForReplicationSensor (#5974),5
"[AIRFLOW-4858] Deprecate ""Historical convenience functions"" in airflow.configuration (#5495)1. Issue old conf method deprecation warnings properly and remove current old conf method usages.2. Unify the way to use conf as `from airflow.configuration import conf`",5
[Airflow-XXXX] Fix a typo,2
[AIRFLOW-5386] Move Google Dataproc to core (#5986)For more information check AIP-21.,5
[AIRFLOW-5399] Add invoke operator for GCP Functions (#5995),1
[Airflow-4668] Make airflow/contrib/utils Pylint compatible (#5916),1
[AIRFLOW-XXX] Remove flake8 from PR templateSince we have pre-commit and flake8 on CI there seems to be noreason to keep this bullet point in PR template.,4
[AIRFLOW-5344] Add --proxy-user parameter to SparkSubmitOperator (#5948)`spark2-submit` supports `--proxy-user` parameter which should be handled by SparkSubmitOperator.```$ spark2-submit --help 2>&1 | grep proxy  --proxy-user NAME           User to impersonate when submitting the application.```,1
[AIRFLOW-4085] FileSensor now takes glob patterns for `filepath` (#5358),2
[AIRFLOW-4391] Fix tooltip for None-State Tasks in 'Recent Tasks' (#5909),0
[AIRFLOW-XXX] Add Chagelog for 1.10.5,2
[AIRFLOW-5347] Add system tests for GoogleCloudStorage (#5951),3
[AIRFLOW-5405] Fixed Unbound variable in force build,1
[AIRFLOW-4851] Refactor K8S codebase with k8s API models (#5481)* [AIRLFOW-4851] refactor Airflow kubernetes* [AIRFLOW-4851] refactor Airflow k8s models* [AIRFLOW-4851] Fix linting and tests* Refactor and add some tests* [AIRLFOW-4851] Add assertions to PodOperator tests,3
[AIRFLOW-5404] Switch back to using Lucas-C pre-commit-hooksThe fuzzy licence matching implemented by Jarek Potiukwas accepted and merged by Lucas-C in his pre-commithooks implementation (released today ver. 1.1.7)so we can switch back to it.,1
Fix webserver link in breeze.rst (#6013)- change webserver link to RST syntax- link resulted in a `about:blank#blocked` .,2
[AIRFLOW-XXX] Make Breeze The default integration test environment (#6001)* [AIRFLOW-XXX] Make Breeze The default integration test environmentCo-Authored-By: Chao-Han Tsai <milton0825@gmail.com>,3
[AIRFLOW-5358] Improved mocking of the project ID (#5964)* [AIRFLOW-5358] Improved mocking of the project ID,1
[AIRFLOW-5308] Pass credentials object to pandas_gbq (#5911),4
[AIRFLOW-XXX] Use full command in examples (#5973),1
[AIRFLOW-5409] Fix BigQuery hook tests (#6017),3
"Passed **kwargs to push_by_returning (#5810)Without **kwargs push_by_returning was giving error, so added that parameter.",2
[AIRFLOW-5403] Fix input check in GKE Operator (#6004),1
[AIRFLOW-5332] Add system tests for Datastore (#5937),5
[AIRFLOW-5361] Add system tests for BigQuery (#5968),3
[AIRFLOW-5319] Add system tests for Dataproc (#5924)* [AIRFLOW-5319] Add system tests for Dataproc,5
[AIRFLOW-3804] Extend MySQL to GCS operator tests (#5993)This commit adds additional test related to problem mentioned in initialissue https://issues.apache.org/jira/browse/AIRFLOW-3804\#,0
[AIRFLOW-5072] gcs_hook should download once (#5685)When a user supplied a filename the expected behaviour is that airflowdownloads the file and does not return it's content as a string.,2
[AIRFLOW-XXX] Add information about default pool to docs (#6019),2
[AIRFLOW-5049] Add validation for src_fmt_configs in bigquery hook (#5671)* AIRFLOW-5049 Add validation for src_fmt_configs in bigquery hookAdds validation for the src_fmt_configs arguments in the bigquery hook. Otherwise wrong src_fmt_configs would be silently ignored which is non-desireable.* [AIRFLOW-5049] Update - Add validation for src_fmt_configs in bigquery hookAdds a common method for validating the src_ftm_configs,5
[AIRFLOW-XXX] Adding walmart labs as user (#6027)[ci skip],1
[AIRFLOW-5343] Add pool_pre_ping to SQLAlchemy (#5949)SQLalchemy supports connection check while returning it from the pool. There is a need to allow this parameter (`pool_pre_ping`) while creating the connection to database.More info here: https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic,2
[AIRFLOW-5412] Add get_conn/get_client to hooks tests (#6018)* [AIRFLOW-5412] Add get_conn/get_client to hooks tests,3
[AIRFLOW-XXX] Fix incorrect GCP integration sections (#5999)[AIRFLOW-XXX] Fix incorrect GCP integration sections,0
[AIRFLOW-5318] Option to specify location of the new BQ dataset (#5923),5
[AIRFLOW-4758] Add GcsToGDriveOperator operator (#5822)* [AIRFLOW-4758] Add GcsToGDriveOperator operator,1
[AIRFLOW-4964] Add BigQuery Data Transfer Hook and Operator (#5769)* [AIRFLOW-4964] Add BigQuery Data Transfer Hook and Operator,1
[AIRFLOW-5411] Remove the noise produced while running failed pre-commits,0
fixup! [AIRFLOW-4964] Add BigQuery Data Transfer Hook and Operator (#5769),1
[AIRFLOW-5426] Adjust import path in Dataproc example (#6033),5
[AIRFLOW-5422] Add type annotations to GCP operators,1
[AIRFLOW-5423] Type annotations for GCP sensors (#6029),5
Add clarity to gcs_download_operator paramsFix flake8 errorsreformat comments,0
"[AIRFLOW-5430] Pin transitive dependency on marshmallow-sqlalchemymarshmallow-sqlalchemy, a transitive dependency from Flask-AppBuilder,broke py2 and py35 compatibility with its 0.19.0.",1
[AIRFLOW-XXX] Move Azure Logging section above operators (#6040),1
[AIRFLOW-XXX] Add S3 Logging section (#6039),2
[AIRFLOW-XXX] Add autogenerated TOC (#6038),1
pre-load requirements for airflow image,1
fix postrgres bug,0
[AIRFLOW-3511][AIRFLOW-3512] Add Cloud Memorystore integration,1
[AIRFLOW-5401] Add support for project_id from connection in GKEThis change adds support for reading project_id value fromconnection configuration GKE operators.,1
[AIRFLOW-2842] Add GoogleCloudStorageSynchronizeBuckets operator,1
[AIRFLOW-XXX] Simplify GCP operators listing,1
[AIRFLOW-5424] Type annotations for GCP hooks,1
fixup! [AIRFLOW-5424] Type annotations for GCP hooks,1
[AIRFLOW-5432] Remove colour logs from UIStreamLogWriter logs other logs that could contain escapes codes thus in web UIthe log is obfuscated. To fix it I added method to remove those codes. This commitalso improves handling of dictionary as an argument for formatted string.,1
[AIRFLOW-5375] Move dumb-init to devel requirements,1
[AIRFLOW-5368] Display DAG from the CLI,2
[AIRFLOW-5436] Remove log from init in BigQuery operators,1
[AIRFLOW-XXX] Simplify Qubole operators listing,1
[AIRFLOW-XXX] Add note about GKEPodOperator in KubernetesPodOperator (#6042),1
[AIRFLOW-XXX] Improve docstring of SQSHook (#6041),1
[AIRFLOW-XXX] Add external reference to all GCP operator guide (#6048),1
[AIRFLOW-5425] Use logging not printing in LoggingCommandExecutor (#6032)This will show proper message in log (yellow color) which will be harder to miss.,2
[AIRFLOW-5437] Do not override python when you rebuild ci_slim image (#6053),5
[AIRFLOW-5376] Add coverage package back to devel extras (#6054)* [AIRFLOW-5376] Add coverage package back to devel extras* fixup! [AIRFLOW-5376] Add coverage package back to devel extras,3
[AIRFLOW-4983] Add ability for DataflowPythonOperator to submit jobs w/ python3 (#5602),5
[AIRFLOW-XXX] Create KubernetesPodOperator guide (#6055),1
"[AIRFLOW-5441] Ownership of package*.json file group write is fixed (#6061)When you use Breeze and you specify ""--force-pull"" flag, the latest image fromDockerHub is pulled before you attempt to rebuild the image. Currently inbreeze we used an optimised version of ""fix-permission"" script that only fixespermissions of several files in the context (the workaround for differentroup write umask setting in DockerHub). However we did not have package.jsonand package-lock.json on the list so those files were always seen as ""changed""and npm was reinstalled for the first time (and only the first time) when theimage was force pulled.After fixing this, the of --force-pull breeze commands will be much faster -skipping the whole npm package reinstallation. Depending on your network speed,this might be between 20 seconds to several minutes as npm ci command wipes outeverything and downloads a lot of packages.",1
[AIRFLOW-3601] Update operators to BigQuery to support location (#6020),1
Update README.md (#6063)Add BigQuant to list of current users,1
[AIRFLOW-5443] Use alpine image in Kubernetes's sidecar (#6059),1
[AIRFLOW-5402] Remove deprecated logger (#6006)* [AIRFLOW-5402] Remove deprecated logger* Remove the related test* Less is more,3
[AIRFLOW-5359] Update type annotations in BaseOperator (#5965)* [AIRFLOW-5359] Update type annotations in BaseOperator,1
[AIRFLOW-XXX] Add Felix to doc (#6068),2
[AIRFLOW-XXX] Add pecan.ai to the users list (#6005),1
"[AIRFLOW-5437] Better python version detection/explanation. (#6060)We have fairly complex python version detection in our CI scripts.They have to handle several cases:1) Running builds on DockerHub (we cannot pass different environment   variables there, so we detect python version based on the image   name being build (airflow:master-python3.7 -> PYTHON_VERSION=3.7)2) Running builds on Travis CI. We use python version determined   from default python3 version available on the path. This way we   do not have to specify PYTHON_VERSION separately in each job,   we just specify which host python version is used for that job.   This makes a nice UI experience where you see python version in   Travis UI.3) Running builds locally via scripts where we can pass PYTHON_VERSION   as environment variable.4) Running builds locally for the first time with Breeze. By default   we determine the version based on default python3 version we have   in the host system (3.5, 3.6 or 3.7) and we use this one.5) Selecting python version with Breeze's --python switch. This will   override python version but it will also store the last used version   of python in .build directory so that it is automatically used next   time.This change adds necessary explanations to the code that works forall the cases and fixes some of the edge-cases we had. It alsoextracts the code to common directory.",4
[AIRFLOW-5446] Rewrite Google KMS Hook to Google Cloud Python (#6065),1
[AIRFLOW-5450] Switching comment from HTML to JinjaIt is causing trouble to the Google Analytics script.,1
[AIRFLOW-5390] Remove provide context (#5990),1
[AIRFLOW-XXX] adding Cyscale to list of companies (#6071),1
[AIRFLOW-5417] Fix DB disconnects during webserver startup (#6023),5
[AIRFLOW-5440] Static checks from docker file use dumb-init (#6056)This is needed so that you can easily kill such checks with ^CNot doing it might cause your docker containers run for a long timeand take precious resources.,1
"[AIRFLOW-4588] Add GoogleDiscoveryApiHook and GoogleApiToS3Transfer (#5335)- add documentation to integration.rstThe hook provides:- a get_conn function to authenticate to the Google API via an airflow connection- a query function to dynamically query all data available for a specific endpoint and given parameters. (You are able to either retrieve one page of data or all data)The transfer operator provides:- basic transfer between google api and s3- passing an xcom variable to dynamically set the endpoint params for a request- exposing the response data to xcom, but raises exception when it exceeds MAX_XCOM_SIZECo-authored-by: louisguitton <louisguitton@users.noreply.github.com>",1
[AIRFLOW-5455] Move BigQuery operators to coreFor more information check AIP-21.,5
[AIRFLOW-5457] Move GCS operators to core,1
"[AIRFLOW-5451] SparkSubmitHook don't set default namespace (#6072)* [AIRFLOW-5451] SparkSubmitHook don't set default namespaceWe only want to set the namespace if it isn't default.https://spark.apache.org/docs/latest/running-on-kubernetes.html#configurationThe default is already set by Spark, therefore we don't want to passit if is default. This also allows us to pass the namespace over theconf dict. Otherwise the namespace would be set twice.* Fix tests as well",3
[AIRFLOW-5468] Fix PubSub system test class name (#6085),3
[AIRFLOW-5465] Fix deprecated imports in examples (#6082),2
[AIRFLOW-5464] Fix GCP Memorystore example DAG (#6081),2
[AIRFLOW-XXX] Simplify AWS/Azure/Databricks operators listing (#6047),1
[AIRFLOW-5476] Fix typo in BREEZE.rst (#6094),2
[AIRFLOW-5480] Fix flaky impersonation (#6098),0
[AIRFLOW-XXX] Add Dentsu as Airflow Users (#6089),1
[AIRFLOW-5471] Fix docstring in GcpTransferServiceOperationsListOperator (#6091),1
[AIRFLOW-5479] Normalize gcp_conn method in GCP Kubernetes Hook (#6099),1
[AIRFLOW-5482] Deprecate Schedule Interval on task level (#6103)* [AIRFLOW-5482] Deprecate Schedule Interval on task level* Update baseoperator.py,1
[AIRFLOW-5428] Dataflow with one job is not done correctly (#6036),5
[AIRFLOW-5453] Improve reading inputs from Dataflow console,5
[AIRFLOW-5427] Add system tests for Dataflow,5
[AIRFLOW-XXX] Add prerequisite tasks for all GCP operators guide (#6049)* [AIRFLOW-XXX] Add prerequisite tasks for all GCP operators* fixup! [AIRFLOW-XXX] Add prerequisite tasks for all GCP operators,1
[AIRFLOW-5495] Remove unneeded parens in dataproc.py (#6105)* AIRFLOW-5495 removing the parens* [AIRFLOW-5495] Update airflow/gcp/operators/dataproc.pyCo-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-5459] Use a dynamic tmp location in Dataflow operator,1
[AIRFLOW-5445] Reduce the required resources for the Kubernetes's sidecar (#6062),1
[AIRFLOW-XXX] Display GCP integration in table (#6086),2
[AIRFLOW-5491] mark_tasks pydoc is incorrect (#6108),2
[AIRFLOW-5490] Fix incorrect None comparison (#6109),0
AIRFLOW-5489: Remove unneeded assignment of variable (#6106),4
AIRFLOW-5492: added missing docstrings (#6107),2
AIRFLOW-5496: delete unneeded variable assignment (#6110),4
[AIRFLOW-5487]Fix unused warning var (#6111),2
AIRFLOW-5484: fix PigCliHook has incorrect named parameter (#6112),2
[AIRFLOW-5488]Remove unused variables from tmp_configuration_copy method (#6114),5
AIRFLOW-5493:cli.py has unnecessary paren wrapping of scala variable (#6113),5
[AIRFLOW-XXX] Display AWS integration in table (#6087),2
"Revert ""[AIRFLOW-5488]Remove unused variables from tmp_configuration_copy method (#6114)"" (#6120)This reverts commit 31b7bc958ef3395a5f7307a852822eba81a5d663.",4
[AIRFLOW-5481] Allow Deleting Renamed DAGs (#6101),2
[AIRFLOW-5503] Fix tree view layout on HDPI screen (#6125)- Tree view layout on HDPI screens overlaps and doesn't make full use ofscreen real estate.,2
[AIRFLOW-5504] Improve project_id in GCP Kubernetes Hook (#6126),1
[AIRFLOW-XXX] Add note about moving GCP from contrib to core (#6119),4
[AIRFLOW-5472] Fix labels in GCS operator (#6117),1
[AIRFLOW-XXX] added NBC to users (#6127),1
[AIRLFOW-XXX] Display other integrations in single table (#6133),2
[AIRFLOW-5498] Move GCP Discovery hook to core (#6121),1
[AIRFLOW-3149] Support Dataproc cluster deletion on ERROR (#4064),0
[AIRFLOW-XXX] Update pydoc of mlengine_operator (#5419),1
[AIRFLOW-5162] GCS Hook Upload Method Improvement (#5770)* [AIRFLOW-5162] add to 'upload' method for gcs file dataAllows for local file uploads (parameter 'filename') and file content (parameter 'data') uploads to gcs by using Google's gcs api method 'upload_from_string' which allows for uploading files content as a string or bytes.* [AIRFLOW-5162] add ValueError exceptions and upload testsValueError exceptions are raised when the filename and data params areprovided or neither are provided. This avoids errors of local files andfile data. Tests assert file and data upload variants as well as assertexceptions.* [AIRFLOW-5162] fixed unit tests for gcs uploadsadded seperate unit tests for uploading string content and byte contentusing the upload_from_string gcp method. Prevents assertion being calledtwice when tests are seperated as well as organizes testing.* [AIRFLOW-5162] added type annotations and tests modificationsAdded type annotations for static tests and ease of use for users. Testsmodifications get rid of assertions for None responses and add tests forcalling upload functions once.* [AIRFLOW-5162] add pylint and flake8 fixes for type annotationstype annotation syntax requires a space between '=' sign for defaultassignment.* [AIRFLOW-5162] added called once and exception testsadded called once tests for upload methods for gzip data. This was addedto assert that the upload_from_string method was called correctly.Assertions for exceptions confirming the correct exception messages.* [AIRFLOW-5162] added fix to doc buildFixed indentation in docstrings so that the documentation can buildproperly. Added new lines after descriptions of the methodsfunctionalities.,1
[AIRFLOW-5256] Related pylint changes for common licences in python files (#5786),2
[AIRFLOW-5343] Remove legacy way of pessimistic disconnect handling (#6034)Based on discussions in https://github.com/apache/airflow/pull/5949it was figured out that there is already pessimistic disconnecttimeout handling. So instead of hand-written one only SQLAlchemyembedded way should be used.'sqlalchemy~=1.3' is in `setup.py` requirements and `pool_pre_ping`appeared in SQLAlchemy 1.2.,1
[AIRFLOW-XXX] Display Azure integration in table (#6132),2
[AIRFLOW-5447] Scheduler stalls because second watcher thread in default args,1
[AIRFLOW-4574] add option to provide private_key in SSHHook (#6104),1
[AIRFLOW-3871] Operators template fields can now render fields inside objects (#4743),1
[AIRFLOW-XXX] Fix typos in CONTRIBUTING.md (#6138),2
"[AIRFLOW-5369] Adds interactivity to pre-commits (#5976)This commit adds full interactivity to pre-commits. Whenever you run pre-commitand it detects that the image should be rebuild, an interactive question willpop up instead of failing the build and asking to rebuild with REBUILD=yesThis is much nicer from the user perspective. You can choose whether to:1) Rebuild the image (which will take some time)2) Not rebuild the image (this will use the old image with hope it's OK)3) Quit.Answer to that question is carried across all images needed to rebuild.There is the special ""build"" pre-commit hook that takes care about that.Note that this interactive question cannot be asked if you run onlysingle pre-commit hook with Dockerfile because it can run multiple processesand you can start building in parallel. This is not desired so instead we failsuch builds.",0
[AIRFLOW-XXX] Add example of running pre-commit hooks on single file (#6143)Or a list of files,2
"[AIRFLOW-4858] Deprecate ""Historical convenience functions"" in airflow.configuration (#6144)These places were missed in the original PR (#5495)",5
[AIRFLOW-5147] extended character set for for k8s worker pods annotations (#5819)* [AIRFLOW-5147] extended character set for for k8s worker pods annotations* updated UPDATING.md with new breaking changes* excluded pylint too-many-statement check from constructor due to its nature,4
[AIRFLOW-5514] No implicit optional flag for mypy (#6141),5
[AIRFLOW-5519] Fix sql_to_gcs operator missing multi-level default args by adding apply_defaults decorator  (#6146),1
[AIRFLOW-4928] Move config parses to class properties inside DagBag (#5557),2
[AIRFLOW-5521] Fix link to GCP documentation (#6150),2
[AIRFLOW-XXX] Fix incorrect units in docs for metrics using Timers (#6152),1
[AIRFLOW-5515] Add stacklevel to GCP deprecation warnings (#6142),2
[AIRFLOW-5513] Move example_pubsub_flow.py to GCP package (#6139),4
[AIRFLOW-XXX] Add link to GCP Example DAGs source code (#6148),2
[AIRFLOW-5475] Normalize gcp_conn_id in operators and hooks (#6093)Co-Authored-By: Kamil Breguła <kamil.bregula@polidea.com>,1
[AIRFLOW-XXX] Update documentation about variables forcing answer (#6158),1
[AIRFLOW-5528] end_of_log_mark should not be a log record (#6159),2
[AIRFLOW-XXX] Fix incorrect backticks in BREEZE.rst (#6161),0
[AIRFLOW-XXX] Fix Prerequisites link in BREEZE.rst (#6160),2
[AIRFLOW-XXX] Fix typo and format error (#6149),0
[AIRFLOW-5526] Update docs configuration due to migration of GCP docs (#6154)* [AIRFLOW-5526] Update docs configuration due to migration of GCP docs,2
[AIRFLOW-XXX] Add a third way to configure authorization (#6134),5
[AIRFLOW-5499] Move GCP utils to core (#6122),4
[AIRFLOW-5530] Fiix typo in AWS SQS sensors (#6012),2
[AIRFLOW-XXX] Fix backtick issues in .rst files & Add Precommit hook (#6162),1
[AIRFLOW-5434] Use hook to provide credentials in GKEPodOperator (#6050),1
[AIRFLOW-XXX] fix backticks in new file (#6164),2
[AIRFLOW-5531] Replace deprecated log.warn() with log.warning() (#6165)- Add pre-commit hook to detect this,1
[AIRFLOW-3388] Add support to Array Jobs for AWS Batch Operator (#6153),1
[AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator (#6051)* [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator* fixup! [AIRFLOW-5435] Add fallback for connection's project id in GKEPodOperator,1
[AIRFLOW-5534] Less verobosity and removal of context container,4
"[AIRFLOW-5533] Fixed failing CRON buildThis change optimises further image building and removes unnecessaryverbosity in building the images for CI builds.After this change is merged, only the necessary images are built foreach type of check:* Tests -> only CI* Static checks (with/without pylint) -> Only CI_SLIM* Docs -> only CI_SLIM* Licence checks -> Only CHECKLICENCEPreviously the right images only were built in ci_before_install.shbut then in case of static checks, the pre-commit build image stepalso rebuilt CHECKLICENCE and CI images - which was not necessaryand very long in case of CRON job - this caused the CRON job tofail at 10m of inactivity.",0
[AIRFLOW-5535] Fix name of VERBOSE parameter,2
[AIRFLOW-5536] Better handling of temporary output files,2
[AIRFLOW-5537] Yamllint is not needed as dependency on hostIt used to be needed for pre-commits but is not needed any moreas it is automatically installed as dependency in the virtualenvcreated by pre-commit,1
[AIRFLOW-5508] Add config setting to limit which StatsD metrics are emitted (#6130),1
[AIRFLOW-5419] - Use `sudo` to kill cleared tasks when running with impersonation (#6026)Marking task failed doesn't kill the task when it is run through impersonation(run_as_user),1
[AIRFLOW-4741] Optionally report task errors to Sentry (#5407)This commit intends to add Sentry to the core functionality ofAirflow. It takes an approach like Statsd for simple integration.The commit makes use of tagging and breadcrumbs for more errorinformation.,5
"[AIRFLOW-774] Fix long-broken DAG parsing Statsd metrics (#6157)Since we switched to using sub-processes to parse the DAG files sometimeback in 2016(!) the metrics we have been emitting about dag bag size andparsing have been incorrect.We have also been emitting metrics from the webserver which is going tobe become wrong as we move towards a stateless webserver.To fix both of these issues I have stopped emitting the metrics frommodels.DagBag and only emit them from inside theDagFileProcessorManager.(There was also a bug in the `dag.loading-duration.*` we were emittingfrom the DagBag code where the ""dag_file"" part of that metric was empty.I have fixed that even though I have now deprecated that metric. Thewebserver was emitting the right metric though so many people wouldn'tnotice)",0
[AIRFLOW-5398] Update contrib example DAGs to context manager (#5998)- replaces the explicit assignment of a dag to a task to be implicit via context manager- changes task dependencies to be defined via bit shifting operator instead of set_upstream/set_downstream- update docs for papermill to extract the code example from the code,4
[AIRFLOW-5419] Correctly use `sudo` to kill cleared tasks when running with impersonation (#6176)Bug fix to #6026,0
[AIRFLOW-4309] Remove Broken Dag error after Dag is deleted (#6102),4
[AIRFLOW-4068] Add GoogleCloudStorageFileTransformOperator (#6177),2
[AIRFLOW-5522] BQ list dataset tables operator (#6151)This operator will fetch the tables of the specified dataset.Signed-off-by: Mohannad Albanayosy <m.banayosi@gmail.com>,5
[AIRFLOW-XXX] Don't trust python-requests.org to run a valid HTTPS server (#6179)This is the 3rd or 4th time we've had a problem with the docs fromrequests causing the docs step of our build to fail. So lets switch toRTD.io instead as they are much better at this.,1
[AIRFLOW-5555] Remove Hipchat integration (#6184),4
[AIRFLOW-XXX] Add ASF integration tables (#6188),1
[AIRFLOW-5554] Require statsd 3.3.0 minimum (#6185)We need v3.3.0+ to support sending timedelta objects to stats.timing.,1
[AIRFLOW-XXX] Add vipul007ravi as user (#6181),1
[AIRFLOW-XXX] Add WASB Hook (#6189),1
[AIRFLOW-XXX] Sort and add missing GCP integrations (#6190),1
[AIRFLOW-XXX] Improve description OpenFaaS Hook (#6187)Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-5527] Improve mocking in GCP test to avoid db queriesSome test were trying to get credentials from db. This shouldbe mocked to assure that tests are stateless.,3
[AIRFLOW-5502] Move GCP base hook to core,1
[AIRFLOW-5342] Fix MSSQL breaking task_instance db migrationMSSQL does not allow altering columns to NOT NULL when the columnis used in an index. Therefore we drop the ti_pool index and recreateit after modifying the column.Co-authored-by: mattinbits <3765307+mattinbits@users.noreply.github.com>Co-authored-by: sirVir <sirVir@users.noreply.github.com>,1
[AIRFLOW-XXX] Add missing service integrations (#6192),1
"[AIRFLOW-5556] Add separate config for timeout from scheduler dag processing (#6186)As of right now, it uses dagbag_import_timeout to control time out ofDagFileProcessor, which is intended to control the timeout of loadingpython file, but the DagProcessor does a bit more so needs a longertimeout.",2
[AIRFLOW-XXX] Pin version of mypy so we are stable over time (#6198)mypy 0.730 was just released and started breaking changes in our testsuite. So that we don't start breaking with no changes on our side Ihave now pinned the version of mypy that we use,1
[AIRFLOW-5561] Relax httplib2 version required for gcp extra (#6194)As part of [AIRFLOW-3971] a minor version range dependency wasintroduced that means that only older versions of httplib2 canbe used.I'm relaxing this dependency as I tested with the latest 0.13.1and it works correctly,1
"[AIRFLOW-5553] Update mysql_to_gcs `bytes` check (#6183)The check for `bytes` type is now based on value type too, not just BQ schema.",5
[AIRFLOW-XXX] Add software integration tables (#6191),1
[AIRFLOW-XXX] Add protocol operators and hooks table (#6193),1
"[AIRFLOW-XXX] Improve style - missing comma, redundant dot (#6201)",1
[AIRFLOW-XXX] Use new import path in GCP table (#6204),2
[AIRFLOW-XXX] Add more ASF transfer operators (#6203),1
[AIRFLOW-XXX] Improve ASF operators table (#6202),1
[AIRFLOW-XXX] Add more AWS transfer operators (#6205),1
[AIRFLOW-XXX] Add more GCP transfer operators (#6206),1
[AIRFLOW-XXX] Add software transfer operators (#6207),1
[AIRFLOW-XXX] Add service transfer operators (#6209)Co-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-XXX] Add protocol transfer operators (#6208),1
"[AIRFLOW-5280] conn: Remove aws_default's default region name (#5879)The `aws_default` by default specifies the `region_name` to be`us-east-1` in its `extra` field. This causes trouble when the desiredAWS account uses a different region as this default value has priorityover the $AWS_REGION and $AWS_DEFAULT_REGION environment variables,gets passed directly to `botocore` and does not seem to be documented.This commit removes the default region name from the `aws_default`'sextra field. This means that it will have to be set manually, whichwould follow the ""explicit is better than implicit"" philosophy.",1
[AIRFLOW-5408] Fix env variable name in Kubernetes template,0
[AIRFLOW-XXX] Improve link to plugin page,2
[AIRFLOW-XXX] Update airflow commands (#6215),5
[AIRFLOW-5574] Fix Google Analytics script loading (#6218),0
[AIRFLOW-XXX] Update Alembic autogenerator to pass pylint (#6216),4
[AIRFLOW-XXX] Ignore FAB's db tables when auto-generating migrations (#6221),5
[AIRFLOW-5579] Fix astroid version,0
[AIRFLOW-XXX] Minor fix for BREEZE.rst,0
[AIRFLOW-5477] Rewrite Google PubSub Hook to Google Cloud Python (#6096),1
[AIRFLOW-XXX] Fix AzureContainerInstancesOperator example (#6225),1
[AIRFLOW-5543] Fix tooltip disappears in tree and graph view (#6174)when page scrolls,0
[AIRFLOW-5362] Reorder imports (#5944),2
"[AIRFLOW-XXX] Updated readme with ""Who's using"" information (#6238)",5
"Revert ""[AIRFLOW-5490] Fix incorrect None comparison (#6109)"" (#6222)This reverts commit 65600048b5b8f235d40a08080e5c7b1721d1d7d6.",4
[AIRFLOW-XXX] Make it clear that 1.10.5 wasn't accidentally omitted from UPDATING.md (#6240),5
[AIRFLOW-XXX] Extract operators and hooks to separate page (#6213),1
[AIRFLOW-XXX] Improve format in code-block directives (#6242),1
[AIRFLOW-XXX] Format Sendgrid docs (#6245),2
[AIRFLOW-XXX] Update Astronomer team (#6251),5
[AIRFLOW-5444] Fix action_logging so that request.form for POST is logged (#6064)Log request.values so both GET and POST are properly logged,2
[AIRFLOW-XXX] Add message about breaking change in DAG#get_task_instances in 1.10.4 (#6226),2
[AIRFLOW-4939] Simplify Code for Default Task Retries (#6233),5
[AIRFLOW-XXX] Add fundaments operators and hooks (#6249),1
[AIRFLOW-5592] Add lint for operators-and-hooks-ref.rst (#6255),1
[AIRFLOW-XXX] Highlight code blocks (#6243),5
[AIRFLOW-5585] Remove docker context from build,2
[AIRFLOW-XXX] Fix minor typo in Slack hook (#6260),1
[AIRFLOW-XXX] Improve linking in docs (#6244),2
[AIRFLOW-5586] Improve CLI error messaging (#6246),0
[AIRFLOW-XXX] Move examples note (#6250),4
[AIRFLOW-XXX] Update to new logo (#6066),2
[AIRFLOW-5604] Remove duplicated isort check (#6272),4
[AIRFLOW-5560] Allow no confirmation on reset dags (#6197)- update backfill command to take --yes- update clear command to use --yes rather than no_confirm,5
[AIRFLOW-5587] Move airflow.contrib.task_runner.cgroup_task_runner to core (#6248),1
[AIRFLOW-5599] Imporve Python 3 support in MLEngine integration (#6262),1
[AIRFLOW-5598] Improve MLEngine typehint (#6263),1
[AIRFLOW-5580] Add base class for system test (#6229)* [AIRFLOW-5580] Add base class for system testThis commit proposes base class for running system test in Airflow. Themain concepts is to create an example DAG and run it for test purpose. Thisis especially important in case of integration with third party services.,2
[AIRFLOW-5601] Use built-in pagination mechanism in MLEngine hook (#6267),1
[AIRFLOW-5600] Add MLEngine system tests (#6264),3
[AIRFLOW-5602] Use unittest.mock in MLEngine hook tests (#6268),3
[AIRFLOW-XXX] Split extra packages table in multiple (#6257),2
[AIRFLOW-XXX] Fix missing backtick in Breeze.rst (#6278),0
[AIRFLOW-XXX] Fix extra-packages tables (#6280),0
[AIRFLOW-5603] Add MLEngine version operators (#6271),1
[AIRFLOW-XXX] Fix heading levels (#6275),0
[AIRFLOW-5387] Fix show paused pagination bug (#6100),0
"[AIRFLOW-5102] Worker jobs should terminate themselves if they can't heartbeat (#6284)If a LocalTaskJob fails to heartbeat forscheduler_zombie_task_threshold, it should shut itself down.However, at some point, a change was made to catch exceptions inside theheartbeat, so the LocalTaskJob thought it had managed to heartbeatsuccessfully.This effectively means that zombie tasks don't shut themselves down.When the scheduler reschedules the job, this means we could have twoinstances of the task running concurrently.",1
[AIRFLOW-5597] Linkify urls in task instance log (#6265),2
Passing job location when initializing the operator (#6258),1
[AIRFLOW-5572] Clear task reschedules when clearing task instances (#6217),2
[AIRFLOW-XXX] Typo in FAQ - schedule_interval (#6291),2
[AIRFLOW-5609] Add MLEngine models operators (#6279),1
[AIRFLOW-5614] Enable Fernet by default (#6282),0
[AIRFLOW-4660] Make airflow/bin Pylint compatible (#6294),1
[AIRFLOW-5588] Add Celery's architecture diagram (#6247)* [AIRFLOW-5588] Add Celery's architecture diagram* fixup! [AIRFLOW-5588] Add Celery's architecture diagram* fixup! fixup! [AIRFLOW-5588] Add Celery's architecture diagram,1
[AIRFLOW-5622] Improve creating directories in SFTPHook (#6287)* [AIRFLOW-XXX] Improve creating directories in SFTPHook* fixup! [AIRFLOW-XXX] Improve creating directories in SFTPHook,1
[AIRFLOW-5617] Add fallback for connection's project id in MLEngine hook (#6286)* [AIRFLOW-5617] Add fallback for connection's project id in MLEngine hook* fixup! [AIRFLOW-5617] Add fallback for connection's project id in MLEngine hook* fixup! fixup! [AIRFLOW-5617] Add fallback for connection's project id in MLEngine hook* fixup! fixup! fixup! [AIRFLOW-5617] Add fallback for connection's project id in MLEngine hook,1
AIRFLOW-5608: Gracefully stop executor when SIGTERM is received by SchedulerJib (#6274),5
[AIRFLOW-XXX] Fix Documentation for adding extra Operator Links (#6301),2
[AIRFLOW-5584] Initialise hook in execute in Cloud SQL operators (#6236)* [AIRFLOW-5584] Initialise hook in execute in Cloud SQL operators,1
[AIRFLOW-5581] Cleanly shutdown KubernetesJobWatcher for safe Scheduler shutdown on SIGTERM (#6237),4
[AIRFLOW-4970] Add Google Campaign Manager integration (#6169)* [AIRFLOW-4970] Add Google Campaign Manager integration,1
[AIRFLOW-5624] Use mock decorator in MLEngine operators tests (#6292),3
[AIRFLOW-5625] Update MLEngine integration doc and typehint (#6293)* [AIRFLOW-5625] Update MLEngine integration doc and typehint,2
[AIRFLOW-XXX] Fix typo - AWS DynamoDB Hook (#6319),1
[AIRFLOW-5630] Improve BigQueryGetDataOperator to handle no rows (#6298),0
[AIRFLOW-5650] Add GSN Games into airflow users in README.md (#6323),2
[AIRFLOW-XXXX] Google Season of Docs updates to CONTRIBUTING doc (#6283)Co-authored-by: Elena Fedotova <lavel@mail.ru>Co-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>,1
[AIRFLOW-5626] Add labels to MLEngine resources (#6296),1
[AIRFLOW-5650] remove githubhandle (#6328),0
[AIRFLOW-5651] Replace GCP depreccated imports (#6325),2
[AIRFLOW-3783] Speed up Redshift to S3 UNload with HEADERs (#6309)replace the default unload_option 'PARALLEL OFF' by 'HEADER' as Redshift handlesheader with parallel mode nowCo-authored-by: Shreya Chakraborty <shrechak@users.noreply.github.com>,1
[AIRFLOW-4661] Make airflow/config_templates Pylint compatible (#6300),5
[AIRFLOW-5652] Add provider package to lint rules (#6326),1
[AIRFLOW-XXX] Fixed case problem with CONTRIBUTING.rst (#6329),0
"[AIRFLOW-5634] Don't allow editing of DagModelView (#6308)Most/all of these fields are not fur user editing, with the exception ofPause which is set in other ways, and if a user edits these it will justconfuse the system.So lets disable the FAB edit form.(Not to mention that the form was broken because it didn't accept thelast_scheduler_run as filled out.)",1
[AIRFLOW-5653] Log caught AirflowSkipException in task instance log (#6330)[AIRFLOW-5653] Log caught AirflowSkipException in task instance logLogging any caught AirflowSkipException with message in models/taskinstance.pyto remove confusion due to no task skipping information in the logs,2
[AIRFLOW-5497] Update docstring in airflow/utils/dag_processing.py (#6314),2
"Revert ""[AIRFLOW-4797] Improve performance and behaviour of zombie detection (#5511)""This reverts commit 2bdb053db618de7064b527e6e3ebe29f220d857b.",5
[AIRFLOW-4797] Use same zombies in all DAG file processors,2
"[AIRFLOW-5223] Use kind for Kubernetes in CI (#5837)This PR reimplements Kubernetes integration testing using kind,a tool for running local Kubernetes clusters using Docker container""nodes"". The ""nodes"" are deployed to a separate docker daemon(dind) started through docker-compose.",2
[AIRFLOW-5641] Support running git sync container as root (#6312),1
[AIRFLOW-5636] Allow adding or overriding existing Operator Links (#6302),2
[AIRFLOW-5657] Update the upper bound for dill (#6334),5
[AIRFLOW-5224] Add encoding parameter to GoogleCloudStorageToBigQuery… (#6297),1
[AIRFLOW-5126] Read aws_session_token in extra_config of the aws hook (#6303),1
"[AIRFLOW-5643] Reduce duplicated logic in S3Hook (#6313)S3Hook.load_bytes is duplicating the logic of S3Hook.load_file_obj.Instead, we should stay consistent : S3Hook.load_string is alreadydelegating the logic to S3Hook.load_bytes, so we can use the sameapproach to delegate to S3Hook.load_file_obj",2
[AIRFLOW-5684] docker-compose-kubernetes still used (#6353),1
"Revert ""AIRFLOW-5608: Gracefully stop executor when SIGTERM is received by SchedulerJib (#6274)"" (#6357)This reverts commit 7c9b44dbfc72df951aadbda006229977beb6d47c.",5
[AIRFLOW-5687] Upgrade pip to 19.0.2 (#6358),5
[AIRFLOW-XXXX] Include code tags for sample file names (#6360),2
[AIRFLOW-5339] Fix infinite wait for Spark subprocess,5
[AIRFLOW-5687] Fix Upgrade pip to 19.0.2 in CI build pipeline (#6361),0
[AIRFLOW-5649] Skips tests when relevant .py files are not changed (#6321),4
"[AIRFLOW-4574] SSHHook private_key may only be supplied in extras (#6163)* discussion on original PR suggested removing private_key option as init param* with this PR, can still provide through extras, but not as init param* also add support for private_key in tunnel -- missing in original PR for this issue* remove test related to private_key init param* use context manager to auto-close socket listener so tests can be re-run",1
[AIRFLOW-5415] Enable authentication for Druid hook (#5967),1
[AIRFLOW-5656] Rename provider to providers module (#6333),1
"[AIRFLOW-5640] Document and test `email` parameters of BaseOperator (#6315)* Refactored get_email_address_list to have a better  separation between string handling and other iterables.* Explicitely casting get_email_address_list argument  to a list in case the argument was an iterable. This  enables direct support for tuples, sets or the like.* Fixed type annotation of email parameter of  BaseOperator to show that iterables are directly  supported.* Added docstring entries for email, email_on_retry,  email_on_failure and queue in BaseOperator.",1
[AIRFLOW-5688] Merge multiple heads in alembic migrations (#6362),7
Add logo info to readme (#6349),5
[AIRFLOW-5680] Fixes Kubernetes hangs (#6347),0
[AIRFLOW-5694] Check for blinker in Sentry setup (#6365),1
[AIRFLOW-5702] Fix common docstring issues (#6372),0
"[AIRFLOW-5693] Support the ""blocks"" component for the Slack messages (#6364)- fix doc string issues",0
[AIRFLOW-5698] Organize Dataflow tests (#6368),3
[AIRFLOW-5699] Add more tests for Dataflow integration (#6369),5
[AIRFLOW-5707] Add type annotations to SFTPHook (#6378),1
[AIRFLOW-5665] Add path_exists method to SFTPHook (#6344)Co-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-5667] Improve type annotations in GCP (#6345),1
[AIRFLOW-5661] Fix create_cluster method of GKEClusterHook (#6339),1
[AIRFLOW-5695] use RUNNING_DEPS to check run from UI (#6367),1
[AIRFLOW-5712] Rename internal Dataflow classes (#6381),5
[AIRFLOW-4965] Handle quote exceptions in GCP operators (#6305),1
[AIRFLOW-5713] Remove dead code (#6382),4
[AIRFLOW-5379] Add Google Search Ads 360 operators (#6228),1
[AIRFLOW-4971] Add Google Display & Video 360 integration (#6170),1
[AIRFLOW-5696] Add GoogleCloudStorageToSFTPOperator (#6366),1
[AIRFLOW-5632] Rename ComputeEngine operators (#6306),1
"[AIRFLOW-5714] Collect SLA miss emails only from tasks missed SLA (#6384)Currently when a task in the DAG missed the SLA,Airflow would traverse through all the tasks in the DAGand collect all the task-level emails. Then Airflow wouldsend an SLA miss email to all those collected emails,which can add unnecessary noise to task owners thatdoes not contribute to the SLA miss.Thus, changing the code to only collect emailsfrom the tasks that missed the SLA.",4
"[AIRFLOW-5715] Make email, owner context available (#6385)",1
[AIRFLOW-XXX] Improve wording (#6391),1
[AIRFLOW-5710] Optionally raise exception on unused operator arguments. (#6332),1
AIRFLOW-5701: Don't clear xcom explicitly before execution (#6370),5
[AIRFLOW-5644] Simplify TriggerDagRunOperator usage (#6317)* [AIRFLOW-5644] Simplify TriggerDagRunOperator usage* Call timezone.parse in execute() to allow for templating,1
[AIRFLOW-5732] Add unit test for the version command (#6401)There's no unit test for the `airflow version` command for now. This PRadds a unit test for that command so as to avoid an accidentalregression.,3
[AIRFLOW-5474] Add Basic auth to Druid hook (#6095),1
[AIRFLOW-5073] Change SQLSensor to optionally treat NULL as keep poking (#6336),4
[AIRFLOW-XXX] Add resources & links to CONTRIBUTING.rst (#6405),2
[AIRFLOW-5088][AIP-24] Persisting serialized DAG in DB for webserver scalability (#5743)Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>,5
[AIRFLOW-XXX] Add Automattic to Airflow users (#6413),1
[AIRFLOW-5740] Fix Transient failure in Slack test (#6407)* [AIRFLOW-5740] Fix Transient failure in Slack testThe transient failure is caused by Dict Ordering* We were comparing stringWe were comparing strings,1
[AIRFLOW-5746] move FakeDateTime into the only place it is used (#6416)Co-authored-by: Jarek Potiuk <jarek.potiuk@polidea.com>,1
[AIRFLOW-5745] Breeze complete has now licence (#6415),5
[AIRFLOW-5747] `real_prefix misses the virtual env created with pyenv and venv (#6417),1
[AIRFLOW-5746] Fix problems with static checks (#6420),0
[AIRFLOW-5690] Change log level local_task_job.py (#6422),2
[AIRFLOW-5723] Simplify illegal arguments test (#6394),3
[AIRFLOW-5750] Licence check is done also for non-executable .sh (#6425),2
"[AIRFLOW-5748] Remove python auto-detection (#6423)The detection of python version is complex because we need to handleseveral cases - including determining the version from image nameon DockerHub, detecting python version from python in the environment,finally forced from python version. This caused multiple problemswith Travis where we run tests with different version (auto-detectedfrom current python - especially when python3 became present inTravis' python 2.7 images. Now all the jobs in Travis havePYTHON_VERSION forced and the code responsible for detecting currentpython version has been removed as it is not needed in this case.",4
"[AIRFLOW-5754] Improved RAT checking (#6429)All files are mounted in CI now and checked using the RAT tool.As opposed to only the runtime-needed files. This is enabled for CIbuild only as mounting all local files to Docker (especially on Mac)has big performance penalty when running the checks (slow osxfsvolume and thousands of small node_modules files generated make thecheck runs for a number of minutes). The RAT checks will by defaultuse the selective volumes but on CI they will mount the wholesource directory.Also latest version of RAT tool is used now and the output - listof checked files - is additionally printed as output of the RATcheck so that we are sure the files we expect to be there, areactually verified.",2
[AIRFLOW-XXX] Improve GCP documentation (#6433),2
[AIRFLOW-XXX] GSoD: Adding 'Create a custom operator' doc (#6348),2
[AIRFLOW-XXX] Clarify daylight savings time behavior (#6324),5
[AIRFLOW-5757] Improve test_on_kill test (#6430),3
[AIRFLOW-5727] SqoopHook: Build --connect parameter only if port/schema are defined (#6397),2
[AIRFLOW-XXX] Updates to Breeze documentation from GSOD (#6285)Co-authored-by: Elena Fedotova <lavel@mail.ru>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-5717] Add get_tree_map method to SFTPHook (#6387),1
[AIRFLOW-5663] Switch to real-time logging in PythonVirtualenvOperator (#6389)Co-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-5759] Don't allow additional arguments in BaseOperator (#6435),1
"[AIRFLOW-5749][AIRFLOW-4162] Support the ""blocks"" component for the Slack operators (#6418)Co-authored-by: bskim45 <bskim45@gmail.com>",1
[AIRFLOW-5764][depends on #6434] Avoid loading corrupted DAGs in a breeze environment (#6436),2
"Revert ""AIRFLOW-5701: Don't clear xcom explicitly before execution (#6370)""This reverts commit 74d2a0d9e77cf90b85654f65d1adba0875e0fb1f.",5
[AIRFLOW-5742] Move Google Cloud Vision to providers package (#6424),1
[AIRFLOW-5771] Make Alembic migrations lineair (#6442)Remove branchpoints to make it lineair again.,1
[AIRFLOW-5770] Add example for PythonVirtualenvOperator (#6441),1
[AIRFLOW-5793] add test to detect multiple alembic revision heads (#6449),3
[AIRFLOW-5766] Use httpbin.org in http_default (#6438)- the url for the http_default changes from google.com to httpbin.org- adjust example_http_operator,1
[AIRFLOW-5798] Set default ExternalTaskSensor.external_task_id (#6431)- fix type annotation for ExternalTaskSensor,0
[AIRFLOW-5631] Change way of running GCP system tests (#6299)* [AIRFLOW-5631] Change way of running GCP system testsThis commit proposes a new way of running GCP related system tests.It uses SystemTests base class and authentication is provided by acontext manager thus it's easier to understand what's going on.,1
[AIRFLOW-5671] rename DataProcHook to DataprocHook (#6404),5
[AIRFLOW-5676] Rename CloudSpannerHook to SpannerHook (#6409),1
[AIRFLOW-5678] Rename GCPTextToSpeechHook to CloudTextToSpeechHook (#6411),1
[AIRFLOW-5673] Rename GcfHook to CloudFunctionsHook (#6414),1
[AIRFLOW-5772] Use mock.patch.dict for manipulation of env vars (#6444),1
[AIRFLOW-5336] Add ability to make updating FAB perms on webserver in… (#5940)Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>,5
"[AIRFLOW-4930] Send druid ingestion spec as data, not json (#5559)* [AIRFLOW-4930] Send ingestion spec as data, not json* [AIRFLOW-4930] Add str annotation to druid job submit method",1
[AIRFLOW-5773] Migrate AWS Athena components to /providers/aws [AIP-21] (#6446),1
[AIRFLOW-5799] Freshworks readme update (#6453),5
[AIRFLOW-5729] Make InputDataConfig optional in Sagemaker's training config (#6398)* [AIRFLOW-5729] Make InputDataConfig optional in Sagemaker's training config* Added test checking training config without InputDataConfig,5
[AIRFLOW-5731] Make output format from list commands configurable (#6400),5
[AIRFLOW-5769] Move the S3_hook to /providers/aws/hooks (#6443),1
[AIRFLOW-5442] implementing get_pandas_df method for druid broker hook (#6057),1
[AIRFLOW-5082] Add subject in AwsSnsHook (#5694),1
[AIRFLOW-5562] Skip grant single DAG permissions for Admin role. (#6199)* [AIRFLOW-5562] Skip grant single DAG permissions for Admin role.- Admin role have all permissions so it does not need to be re-authorized.- Too many permissions for role is not good for view and performance.* [AIRFLOW-5562] Fix typo in last change.,4
[AIRFLOW-5815] Fix isort problem (#6468),0
[AIRFLOW-5741] Move Cloud Natural Language to providers (#6421),1
[AIRFLOW-XXX] FIX typo in Breeze.rst (#6477),2
[AIRFLOW-5787] Moving AWS SQS to /providers/aws (#6474),1
adding ciValue to README.md (#6479),2
"[AIRFLOW-XXX] Fix typo in ""airflow dags list_runs"" (#6475)",1
[AIRFLOW-XXX] Helpful list of do_xcom - affected operators added. (#6448),1
[AIRFLOW-5836] Pin azure-storage-blob version to <12 (#6486),5
[AIRFLOW-XXX] Fix typo in mysql_hook docstring (#6483),2
"[AIRFLOW-XXXX] Expose SQLAlchemy's connect_args and make it configurable (#6478)In many use cases users need to configure SQLAlchemy's connect_args (e.g. pass ssl.check_hostname=False to PyMySQL), and Airflow should expose this option and make it configurable.",5
[AIRFLOW-5758] Support the custom cursor classes for the PostgreSQL hook (#6432),1
[AIRFLOW-5814] Implementing Presto hook tests (#6491),3
[AIRFLOW-5839] Upgrade pre-commit and hooks to latest versions (#6492)We want to use latest pre-commit version as well as latestversion of the plugins used.,1
[AIRFLOW-5826] Added auto-generation of breeze output in BREEZE.rst (#6490)This is needed to keep breeze --help in sync with the documentation.It makes it easier for the follow-up changes needed for productionimage to keep the docs in sync with the code.,2
[AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator (#6488)* [AIRFLOW-XXX] Add How-To-Guide to GoogleCloudStorageToSFTPOperator,1
[AIRFLOW-5677] Rename GCPSpeechToTextHook to CloudSpeechToTextHook (#6410),1
[AIRFLOW-5827] Move build to later stage in pre-commit (#6493)This change is to move build stage of pre-commit as late as possible in pre-commitchain. This is useful if you do not want to rebuild the docker images neededto run pylint/mypy/flake8 - and still see the result of other checks immediatelyafter you run commit.,1
[AIRFLOW-5743] Move Google PubSub to providers package (#6476),1
[AIRFLOW-5838] Make all __init__ pylint compatible (#6503),5
[AIRFLOW-5658] Fix broken navigation links (#6374)Configuration and Version links are broken when running airflowbehind reverse proxy.Registering navigation links through their respective viewsensure that correct links are rendered in the menu.,2
[AIRFLOW-5852] Make tests.www pylint compatible (#6504),3
[AIRFLOW-5709] Fix regression in setting custom operator resources. (#6331),1
"[AIRFLOW-5830] Get rid of slim image (#6494)The slim image gave only very small gain on executing the tests in CI. Theimage was significantly smaller, but then for local development and testingyou needed both full CI and SLIM-CI image.This made the scripts and docker image needlessly complex - especiallyin the wake of coming Production image it turned to be prematureoptimisation really. While it sped-up (slightly - by 10-20 seconds) somestatic check jobs in Travis, it increased time needed by developersto have a working environment and to keep it updated every time it wasneeded (by minutes)Also having two separately managed images made it rather complex to joinsome of the Travis CI jobs (there is a follow-up change with getting ridof Checklicence image).With this change both static checks and tests are executed using singleimage. That also opens doors for further simplification of the scriptsand easier implementation of production image.",1
[AIRFLOW-5683] Add propagate_skipped_state to SubDagOperator (#6352)- fix pylint issues- refactor subdag_operator,2
[AIRFLOW-XXX] Add How-To-Guide to GCP PubSub (#6497),1
[AIRFLOW-5855] Fix broken reference in custom operator doc (#6508),2
"[AIRFLOW-5829] Get rid of the checklicence image (#6495)This change is a further step of simplifying the set of scriptsused by CI. The separate checklicence image was implemented as anoptimisation of the licence check time. The image to download wassmall and could be downloaded slightly faster in CI. However thatmade all the management script more complex and lead to havingseparate jobs for check licence and static checks. That lead toactually longer time of Travis jobs - because new machine had tobe spun-off for checklicence check only.With this change, the CI image is the only one left and it is slightlybigger (with RAT tool added) but the same image is used for all thetests - unit tests, static checks and checklicence checks.This also makes it easier to manage the images and decreases updateoverhead on the developers using Breeze.",1
[AIRFLOW-5846] Implementing Sqlite hook tests (#6509),3
"[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496)* [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing* Fixed problem that Kubernetes tests were testing latest master  rather than what came from the local sources.* Moved Kubernetes scripts to 'in_container' dir where they belong now* Kubernetes tests are now better suited for running locally* Kubernetes cluster is not deleted until environment is stopped* Kubernetes image is built outside of the container and passed as .tar* Kubectl version name is corrected in the Dockerfile* Kubernetes Version can be used to select Kubernetes versio* Running kubernetes scripts is now easy in Breeze* Instructions on how to run Kubernetes tests are updated* Better flags in Breeze are used to run Kubernetes environment/tests* The old ""bare"" environment is replaced by --no-deps switch",3
[AIRFLOW-5811] add metric for externally killed task count (#6466),1
"Revert ""[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6496)""This reverts commit 8e789a33a3c95fc91aa9be71d2d7329132dc25f9.",4
[AIRFLOW-5853] BigQuery - standardize task_ids in example_dag (#6505),2
[AIRFLOW-5819] Update AWSBatchOperator default value (#6473),1
[AIRFLOW-5817] Improve BigQuery operators idempotency (#6470),1
"[AIRFLOW-XXX] Fix typo in python_sensor.py docstring (#6521)I fixed typo from ""the the"" to ""the"".",2
"[AIRFLOW-5806] Simplify the xcom table (#6463)Remove the id column, since it isn't used anywhere",1
"[AIRFLOW-5874] Use poke_interval of 0 to make EMR tests run green (#6269)As part of introducing EmrRunJobFlows in a seperate PR, I noticed thatthe EMR tests were so slow they were yellow and that a poke_interval of0 is a valid poke_interval for the tests to run at.Practically speaking, though, this is only saving a dozen seconds inthe test suite at most.",3
[AIRFLOW-5711] Add fallback for connection's project ID in Dataflow integration (#6383)* [AIRFLOW-5711] Use keywords arguments as a parameter,2
[AIRFLOW-5867] Fix webserver unit_test_mode data type (#6517),5
[AIRFLOW-5875] Fix typo in example_qubole_operator.py (#6525),1
[AIRFLOW-5823] Add a new hook for ingesting data into Apache Pinot (#6482),5
[AIRFLOW-5782] Migrate AWS Lambda to /providers/amazon/aws [AIP-21] (#6518)* [AIRFLOW-5782] Migrate AWS Lambda to /providers/aws [AIP-21],1
[AIRFLOW-5803] Update S3Hook import paths [AIP-21] (#6465)* [AIRFLOW-5803] Rename S3Hook to AWSS3Hook and update import paths,2
"Revert ""[AIRFLOW-5803] Update S3Hook import paths [AIP-21] (#6465)""This reverts commit 2daf72e305e28baee90007a60dead8bb57122f6e.",4
[AIRFLOW-5883] Don't use .count() from sqlalchemy to count (#6532),1
[AIRFLOW-5786] Migrate AWS SNS to /providers/amazon/aws (#6502)* [AIRFLOW-5786] Move AWS Sns hook and operator according to AIP-21,1
[AIRFLOW-5803] Update S3Hook import paths [AIP-21] (#6535)* [AIRFLOW-5803] Rename S3Hook to AWSS3Hook and update import paths,2
[AIRFLOW-XXX] Update company name and user admin of the platform (#6528),1
[AIRFLOW-5870] Allow -1 for pool size and optimise pool query (#6520),1
[AIRFLOW-XXX] Fix typo in 5 files  docstring (#6537),2
[AIRFLOW-5716] Simplify DataflowJobsController logic (#6386),2
[AIRFLOW-5888] Use psycopg2-binary for postgres operations (#6533),1
[AIRFLOW-XXX] Improve the PubSub documentation (#6511),2
[AIRFLOW-5878] Use JobID to monitor statuses when running a Dataflow … (#6530)* [AIRFLOW-5878] Use JobID to monitor statuses when running a Dataflow template,5
[AIRFLOW-5869] BugFix: Some Deserialized tasks have no start_date (#6519),5
AIRFLOW-5854: Add support for `tty` parameter in Docker related operators (#6542),1
[AIRFLOW-5832] Add pagerduty hook (#6484),1
AIRFLOW-5824: AWS DataSync Hook and Operators added (#6512),1
[AIRFLOW-5117] Automatically refresh EKS API tokens when needed (#5731),5
[AIRFLOW-5894][part of AIRFLOW-5893] Group tests for the Dags command (#6543)* [AIRFLOW-5894] Group tests for the Dags command,2
[AIRFLOW-XXX] Smal typo = Trakcing => Tracking (#6557),2
[AIRFLOW-5896] Move email stuff from tests/core.py (#6545)* [AIRFLOW-5896] Move email stuff from tests/ccore.py,3
[AIRFLOW-5897] Allow setting -1 as pool slots value in webserver (#6550),1
[AIRFLOW-5899] fix dmypy errors (#6548),0
[AIRFLOW-XXX] Add render_template changes to UPDATING.md (#6546),5
[AIRFLOW-5876] Fetch all Dataflow jobs during searching (#6527),5
[AIRFLOW-5877] Improve job_id detection in DataflowRunner (#6529),5
[AIRFLOW-5900] avoid unnecessary system calls in heartbeat_callback (#6551),5
[AIRFLOW-5882] Add task_not_running dep (#6531),1
[AIRFLOW-5892] BashOperator does not create temporary shell script (#6541),1
[AIRFLOW-XXX] Fix typo in macros (#6559),2
[AIRFLOW-5885] List of tests is generated dynamically after you enter Breeze (#6536)The list of tests for autocomplete is now generated automatically when you enter Breeze.It will take some 40 seconds or so to generate the list and until it's done there areno autocompletions but they appear right after the list is ready.,3
[AIRFLOW-XXX] Local development environments - polish documentation (#6450)* [AIRFLOW-XXX] Updates to LOCAL_VIRTUALENV.rstCo-authored-by: Elena Fedotova <lavel@mail.ru>Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>,5
[AIRFLOW-5887] User is removed from CI images (#6540)The AIRFLOW_USER is not needed any more in CI images. It will be needed in Prodimages but it will be implemented differently there.,1
[AIRFLOW-5901] Group tests for the tasks and variables command (#6554),3
"[AIRFLOW-5873] KubernetesPodOperator fixes and test (#6524)- `security_context` was missing from docs of `KubernetesPodOperator`- `KubernetesPodOperator` kwarg `in_cluster` erroneously defaults toFalse in comparison to `default_args.py`, also default `do_xcom_push` was overwritten to False in contradiction to `BaseOperator`- `KubernetesPodOperator` kwarg `resources` is erroneously passed to `base_operator`, instead should only go to `PodGenerator`. The two have different syntax. (both on `master` and `v1-10-test` branches)- `kubernetes/pod.py`: classes do not have `__slots__` so they would accept arbitrary values in `setattr`- Reduce amount of times the pod object is copied before execution",1
[AIRFLOW-5886] Selective copying of sources in Docker image (#6538)This change further improves time of rebuilds for docker image when yoursources change (very useful in case of building kubernetes image). It adds onlydirectories that are needed (it is synchronised with .dockerignore and localmounts) and in the sequence that reflects frequency of changes. Also pipinstall is not done again after sources change (there is no point) so thebuild is much faster when only sources or test file change.,4
[AIRFLOW-5895] Move HDFS stuff from tests/core.py (#6544),3
[AIRFLOW-5917][part of AIRFLOW-5893] Group tests for the Webserver command (#6566),3
[AIRFLOW-5898] fix alembic crash due to typing import (#6547),2
[AIRFLOW-5905] Unify GCP file names (#6558),2
[AIRFLOW-5886] Fixed a bit too selective Dockerfile copy (#6570),2
[AIRFLOW-5691] Rewrite Dataproc operators to use python library (#6371),1
[AIRFLOW-5925] Relax funcsigs and psutil version requirements (#6580),1
[AIRFLOW-XXX] Fix the docstring for Dataproc get_job method (#6581),1
[AIRFLOW-5880] Enforce unique task ids (#6549),1
[AIRFLOW-XXX] Alphabetical table and remove duplicate (#6487),4
[AIRFLOW-5918][part of AIRFLOW-5893] Group tests for the Pools command (#6567),3
[AIRFLOW-5768] GCP cloud sql don't store ephemeral connection in db (#6440),5
[AIRFLOW-5926] Fix race-condition in TestCliWebServer (#6579)Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>,3
[AIRFLOW-XXX] Replace string with str in docstring for consistency (#6589),2
"[AIRFLOW-5928] Hive hooks load_file short circuit (#6582)If function load_file with parameter create and recreate areset to False, hql = '' and should not call functionHiveCliHook.run_cli",1
[AIRFLOW-5919] Group tests for the Users/Roles/Perms commands (#6568),1
[AIRFLOW-5718] Add SFTPToGoogleCloudStorageOperator (#6393),1
[AIRFLOW-5937] Group tests for the version/connection/db commands (#6587),5
[AIRFLOW-XXX] Remove duplicate docs (#6584),2
[AIRFLOW-5730] Enable get_pandas_df on PinotDbApiHook (#6399),5
[AIRFLOW-5942] Pin PyMSSQL to <3.0 (#6592),5
[AIRFLOW-5936] Allow explicit get_pty in SSHOperator (#6586),1
[AIRFLOW-5582] Add get_autocommit to JdbcHook (#6232)- add tests- update docsCo-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-5940] Remove redundant Code in models.dagrun (#6591),2
[AIRFLOW-5923] Use absolute paths in GCP system tests (#6571),3
[AIRFLOW-6002] Drop support for python 3.5 (#6595),1
[AIRFLOW-3489] Improve json data handling in PostgresToGcs operator (#6572),1
[AIRFLOW-5783] AIP-21 Move aws redshift into providers structure (#6539),1
[AIRFLOW-5073] Change SQLSensor to not treat NULL as success criteria (#5913)Remove allow_null parameter to decrease clutter.,2
[AIRFLOW-6009] Switch off travis_wait for regular tests (#6600),3
[AIRFLOW-6000] Fix the test randomly fail - TestCliConnections.test_cli_connections_add_delete (#6593),3
[AIRFLOW-XXX] Update list of pre-commits (#6603),5
[AIRFLOW-5781] AIP-21 Migrate AWS Kinesis to /providers/amazon/aws (#6588),1
[AIRFLOW-6001] Lazy load CLI commands (#6594)* [AIRFLOW-YYY] Lazy load API Client* [AIRFLOW-YYY] Introduce order in CLI's function names* [AIRFLOW-YYY] Create cli package* [AIRLFOW-YYY] Move user and roles command to seperate files* [AIRLFOW-YYY] Move sync_perm command to seperate file* [AIRLFOW-YYY] Move task commands to separate file* [AIRLFOW-YYY] Move pool commands to separate file* [AIRLFOW-YYY] Move variable commands to separate file* [AIRLFOW-YYY] Move db commands to separate file* fixup! [AIRLFOW-YYY] Move variable commands to separate file* [AIRLFOW-YYY] Move connection commands to separate file* [AIRLFOW-YYY] Move version command to separate file* [AIRLFOW-YYY] Move scheduler command to separate file* [AIRLFOW-YYY] Move worker command to separate file* [AIRLFOW-YYY] Move webserver command to separate file* [AIRLFOW-YYY] Move dag commands to separate file* [AIRLFOW-YYY] Move serve logs command to separate file* [AIRLFOW-YYY] Move flower command to separate file* [AIRLFOW-YYY] Move kerberos command to separate file* [AIRFLOW-YYY] Lazy load CLI commands* [AIRFLOW-YYY] Fix migration* fixup! [AIRFLOW-YYY] Fix migration* fixup! fixup! [AIRFLOW-YYY] Fix migration,0
[AIRFLOW-6017] Exclude PULL_REQUEST_TEMPLATE.md from RAT check (#6611),5
"[AIRFLOW-5804] Batch the xcom pull operation (#6461)* [AIRFLOW-5804] Batch the xcom pull operationRight now the xcom_pull of the task_instance will callXCom.get_one many times, therefore I'd like to changethis to XCom.get_many.Databases love batches* Fix flake8",0
"[AIRFLOW-3632] Only replace microseconds if execution_date is None in trigger_dag REST API (#6380)No need to require a user to pass in replace_microseconds to therequest body; instead we should use exactly the date that is given.We will still replace the microseconds on execution_date if none ispassed in (and the param is True, which is the default)",2
[AIRFLOW-5921] Add bulk_load_custom to MySqlHook (#6575),1
[AIRFLOW-2143] Fix TaskTries graph counts off-by-1 (#5297),0
[AIRFLOW-XXX] Fix typos from CLI refactor (#6616),4
[AIRFLOW-XXX] Add TokenAnalyst to Airflow Users (#6605),1
[AIRFLOW-6021] Replace list literal with list constructor (#6617),5
[AIRFLOW-6022] Move FS defn into awk BEGIN (#6619),4
[AIRFLOW-6023] Remove deprecated Celery configs (#6620),5
[AIRFLOW-6026] Use contextlib to redirect stderr and stdout (#6624),1
Add coding to fix Cyrillic output (#6631),0
"[AIRFLOW-5950] AIP-21 Change import paths for ""apache/cassandra"" modules (#6609)",2
[AIRFLOW-5947] Make the json backend pluggable for DAG Serialization (#6630),2
[AIRFLOW-6025] Add label to uniquely identify creator of Pod (#6621),1
[AIRFLOW-6035] Remove comand method in TaskInstance (#6629),4
[AIRFLOW-6034] Fix Deprecation Elasticsearch configs on Master (#6628),5
[AIRFLOW-5313] Add params support for awsbatch_operator (#5900),1
[AIRFLOW-6044] Standardize the Code Structure in kube_pod_operator.py (#6639)https://issues.apache.org/jira/browse/AIRFLOW-6044,0
[AIRFLOW-XXX] Fix minor typos in kubernetes_executor.py (#6641),2
[AIRFLOW-6045] Error on failed execution of compile_assets (#6640)https://issues.apache.org/jira/browse/AIRFLOW-6045,0
[AIRFLOW-6046] Fix Alembic migrations & sync it to 1.10.* (#6642),0
[AIRFLOW-XXX] Fix typo in gcp/hooks/test_base.py (#6646),3
[AIRFLOW-6048] Make K8s Pod Operator's `_set_*` methods static (#6645),1
[AIRFLOW-XXX] Fix incorrect docstring parameter (#6649),2
[AIRFLOW-6052] Add TypeHints to kubernetes_pod_operator (#6648),1
[AIRLFOW-6024] Do not use the logger in CLI (#6622),2
[AIRFLOW-6020] Fix python 3 KubernetesExecutor iteritems exception (#6614),0
[AIRFLOW-6054] Add a command that starts the database consoles (#6653),5
[AIRFLOW-6060] Improve conf_vars context manager (#6658)This commit adds try / finally clause to conf_vars contextmanager to assure that initail values are reseted in caseof an exception in yield.,1
[AIRFLOW-XXX] GSoD: How to make DAGs production ready (#6515),2
[AIRFLOW-6042] Fix folder path in SFTPToGcsExampleDagsSystemTest (#6637),5
[AIRFLOW-3682] Use aws_default in EMR related operators (#4465),1
"[AIRFLOW-6033] Fix UI Crash at ""Landing Times"" when task_id is changed (#6635)",4
[AIRFLOW-6049] set propagate True when needed in airflow test (#6647)when airflow.task handlers don't have StreamHandler,0
[AIRFLOW-5915] Add support for the new documentation theme (#6563),2
[AIRFLOW-6066] Added pre-commit checks for accidental debug stmts (#6662),0
[AIRFLOW-5726] Allow custom filename in RedshiftToS3Transfer (#6396),2
"[AIRFLOW-6069] Python host version in travis is set to 3.6 always (#6666)This will make the scripts more ""stable"" - no problems withfeatures missing in 3.5 for host scripts.Python version for all tests in container is controlled viaPYTHON_VERSION variable.",3
Clarified a grammatically incorrect sentence (#6667),5
[AIRFLOW-6063] Remove astroid dependency (#6659)Note this dependency was only meant to temporarily fix an issue with pylint and astroid.See https://github.com/PyCQA/pylint/issues/3123 for more information.,5
[AIRFLOW-XXX] Endesa readme update (#6671),5
[AIRFLOW-6051] Make DAG optional during displaying the log (#6650),2
[AIRFLOW-6073] Move Qubole Operator Link class to qubole_operator.py (#6668),1
[AIRFLOW-6041] Add user agent to the Discovery API client (#6636),1
[AIRFLOW-6010] Remove cyclic imports and pylint hacks (#6601),2
[AIRFLOW-6079] Make consistent use of timezone.parse (#6672),1
"[AIRFLOW-6055] Option for exponential backoff in Sensors (#6654)A new option ""exponential_backoff"" in Sensors, will increase the next poke or next reschedule time for sensors exponentially. Turned off by default.",1
"[AIRFLOW-5911] Simplify lineage API and improve robustness (#6564)This simplifies the lineage API which was needlessy cluttered.You can now set ""inlets='auto'"" rather than ""inlets={'auto': True}""and airflow will figure out what to do.Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>",1
[AIRFLOW-XXX] GSoD: Adding Task re-run documentation (#6295),2
[AIRFLOW-6047] Simplify the logging configuration template (#6644),5
[AIRFLOW-6103] Add .asf.yaml to control Github settings (#6689),1
[AIRFLOW-6088] pass DAG processing runtime as duration to stats (#6682),1
[AIRFLOW-3656] Show doc link for the current installed version (#6690),2
[AIRFLOW-6099] Add host name to task runner log (#6688),2
[AIRFLOW-6089] Reorder setup.py dependencies and add ci (#6681)* Reorder dependencies in setup.py,1
[AIRFLOW-6129] Fix pylint errors (#6691),0
[AIRFLOW-6131] Make Cassandra hooks/sensors pylint compatible (#6693)* [AIRFLOW-6131] Make Cassandra hooks/sensors pylint compatible,1
[AIRFLOW-6080] Upgrade mypy version and fix newly reported errors (#6674),0
[AIRFLOW-6136] Remove hardcoded project ID in example_dag (#6698),2
[AIRFLOW-6137] Remove refuse after CLI refactor (#6699),4
[AIRFLOW-6138] Fixed escaping of pre-commit dots (#6700),0
[AIRFLOW-6139] Consistent spaces in pylint enable/disable (#6701),0
[AIRFLOW-6133] Make Hive transfer operators pylint compatible (#6695)* [AIRFLOW-6133] Make Hive transfer operators pylint compatible,1
[AIRFLOW-6143] Remove master-failing pylint:disables (#6706),0
[AIRFLOW-6141] Remove ReadyToRescheduleDep if sensor mode == poke (#6704),4
[AIRFLOW-6144] Improve the log message of airflow scheduler (#6710),2
"[AIRFLOW-6043] Fix bug in UI when ""filtering by root"" to display section of dag  (#6638)Graph UI is broken when tasks are filtered by root ifroot is in the middle of chain of tasks.The initial of the problem is in `sub_dag` method that keeps referencesto the original DAG. `deepcopy` call is optimized and the optimization isnot 100% correct.",2
[AIRFLOW-XXX] Add Wrike as official user of airflow (#6711),1
[AIRFLOW-5194] Add error handler to action log (#5883),2
"[AIRFLOW-6142] Fix different local/Travis pylint results (#6705)* [AIRFLOW-6142] Fix different local/Travis pylint resultsSometimes Pylint on Travis CI gives still different results than the one runlocally. This was happening because we were using theAIRFLOW_MOUNT_SOURCE_DIR_FOR_STATIC_CHECKS=""true"" for static checks. This isneeded for checklicence check only - just to make sure that all source files(including scripts etc.) are mounted to the container.However this makes it slightly different when it comes to pylint checks. Wewould like to have it exactly identical when run locally and in CI so in caseof static checks we should rather useAIRFLOW_MOUNT_HOST_VOLUMES_FOR_STATIC_CHECKS=""true"" for all checks but theChecklicence one - same as used locally.This way running:pre-commit run pylint --all-filesShould always give the same results locally and in Travis.* Update scripts/ci/_utils.shCo-Authored-By: Felix Uellendall <feluelle@users.noreply.github.com>",1
"[AIRFLOW-XXX] Fix docstring minor issues in airflow/kubernetes/ (#6708)* Correct typo in pod_launcher* Remove param `secrets` in docstring since it's not specified/used* Always use triple double quotes around docstrings https://www.python.org/dev/peps/pep-0257/For consistency, always use """"""triple double quotes"""""" around docstrings",2
[AIRFLOW-6159] Change logging level of the heartbeat message to DEBUG (#6716),0
"[AIRFLOW-6004] Untangle Executors class to avoid cyclic imports (#6596)There are cyclic imports detected seemingly randomly by pylint checks when some    of the PRs are run in CI    It was not deterministic because pylint usually uses as many processors as    many are available and it splits the list of .py files between the separate    pylint processors - depending on how the split is done, pylint check might    or might not detect it. The cycle is always detected when all files are used.    In order to make it more deterministic, all pylint and mypy errors were resolved    in all executors package and in dag_processor.    At the same time plugins_manager had also been moved out of the executors    and all of the operators/hooks/sensors/macros because it was also causing    cyclic dependencies and it's far easier to untangle those dependencies    in executor when we move the intialisation of all plugins to plugins_manager.    Additionally require_serial is set in pre-commit configuration to    make sure cycle detection is deterministic.",5
[AIRFLOW-5834] Option to skip serve_logs process with workers (#6709),1
[AIRFLOW-6162] Add back serialization as a module (#6718),1
[AIRFLOW-6132] - Allow to pass in tags for the azure cloud (#6694)* AIRFLOW-6132* allow to pass in tags,4
[AIRFLOW-6095] Filter dags returned by task_stats (#6684)Add dag_ids parameter to task_stats so can filter by a set of dag_idspresent on the dags view. This is intended to speed up the response timeand reduce the size of the payload when running a large number of dags.,2
[AIRFLOW-5902] avoid unnecessary sleep to maintain local task job heart rate (#6553)sleep to maintain heart rate is already done by the hearbeat() call,5
[AIRFLOW-6007] Check providers instead of provider package (#6599),1
more GSOD improvements (#6585)Co-authored-by: Elena Fedotova <lavel@mail.ru>,1
[AIRFLOW-6169] Avoid unnecessary int-to-float conversion (#6724),5
[AIRFLOW-XXX] Add Changelog for 1.10.6 (#6728),4
[AIRFLOW-6140] Add missing types for some core classes (#6702),1
[AIRFLOW-6165] Housekeep utils.dates.date_range & add tests (#6720),3
[AIRFLOW-6058] Running tests with pytest (#6472)This commit runs Airflow's test suite using pytest.,3
[AIRFLOW-6058] Fixed remnant of nose tests (fixup) (#6733),0
[AIRFLOW-6180] Improve kerberos init in pytest conftest (#6735),5
"[AIRFLOW-6177] Log DAG processors timeout event at error level, not info (#6731)This case prevents a DAG from being scheduled so should be an error.",0
[AIRFLOW-6172] BigQuery - Move BigQuery hook Pandas data frame system tests to separate file (#6729)This is done due to the fact that these tests require authorization to BigQuery,1
[AIRFLOW-XXX] Add information how to configure pytest runner (#6736),1
[AIRFLOW-6189] Reduce the maximum test duration to 8 minutes (#6744),3
[AIRFLOW-XXX] Fix broken link in CONTRIBUTING.rst (#6747),2
"[AIRFLOW-6167] Escape column name in create table in hive (#6741)Hive query will fail if column name contains keyword, unless escaped.Fix is always escaping column name when creating table.",1
[AIRFLOW-6192] Stop creating Hook from SFTPSensor.__init__ (#6748),5
[AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715),1
[AIRFLOW-4935] Add function get_dataset_tables_list to BigQueryHook  (#5566),1
[AIRFLOW-XXX] Minor fix to CONTRIBUTING.rst (#6752),0
[AIRFLOW-6135] Extract DAG processing from SchedulerJob into separate class (#6697),2
[AIRFLOW-XXX] Fix a typo in TESTING.rst (#6757),3
[AIRFLOW-6199] Add GKE example with XCOM (#6755),1
[AIRFLOW-6196] Use new syntax for NamedTuple (#6751),1
[AIRFLOW-6185] SQLAlchemy Connection model schema not aligned with Alembic schema (#6754)* [AIRFLOW-6185] SQLAlchemy Connection model schema aligned with Alembic schema,5
[AIRFLOW-6197] Use tabulate to display DAG Runs (#6753),1
[AIRFLOW-6081] Refactor test_connection_command.py (#6676),3
[AIRFLOW-XXX] Add a structural dag validation example (#6727)* Add a structural dag validation example,5
[AIRFLOW-6120] Rename GoogleCloudBaseHook (#6734)* [AIRFLOW-6120] Rename GoogleCloudBaseHook,5
[AIRFLOW-6158] Upgrade sendgrid dependency (#6719),5
[AIRFLOW-6072] aws_hook: Outbound http proxy setting and other enhancements (#6686),1
[AIRFLOW-6018] Display task instance in table during backfilling (#6612)* [AIRFLOW-6018] Display task instance in table during backfilling,2
[AIRFLOW-6191] Adjust pytest verbosity in CI and local environment (#6746),3
[AIRFLOW-6193] Do not use asserts in Airflow main code (#6749)* [AIRFLOW-6193] Do not use asserts in Airflow main code,3
[AIRFLOW-5583] Extend the 'DAG Details' page to display the start_date / end_date (#6235),5
[AIRFLOW-5807] Move SFTP from contrib to providers. (#6464)* [AIRFLOW-5807] Move SFTP from contrib to core,4
[AIRFLOW-6203] BigQuery - parametrize hook tests (#6759),3
"[AIRFLOW-5931] Use os.fork when appropriate to speed up task execution. (#6627)* [AIRFLOW-5931] Use os.fork when appropriate to speed up task execution.  Rather than running a fresh python interpreter which then has to re-load  all of Airflow and its dependencies we should use os.fork when it is  available/suitable which should speed up task running, espeically for  short lived tasks.  I've profiled this and it took the task duration (as measured by the  `duration` column in the TI table) from an average of 14.063s down to  just 0.932s!* Allow `reap_process_group` to kill processes even when the ""groupleader"" has already exited.* Don't re-initialize JSON/stdout logging ElasticSearch inside forked processes  Most of the time we will run the ""raw"" task in a forked subprocess (the  only time we don't is when we use impersonation) that will have the  logging already configured. So if the EsTaskHandler has already been  configured we don't want to ""re""configure it -- otherwise it will  disable JSON output for the actual task!",5
"[AIRFLOW-6175] Fixes bug when tasks get stuck in ""scheduled"" state (#6732)There is a bug caused by scheduler_jobs refactor which leads to task failureand scheduler locking.Essentially when a there is an overflow of tasks going into the scheduler, thetasks are set back to scheduled, but are not removed from the executor'squeued_tasks queue.This means that the executor will attempt to run tasks that are in the scheduledstate, but those tasks will fail dependency checks. Eventually the queue isfilled with scheduled tasks, and the scheduler can no longer run.Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>, Kevin Yang <kevin.yang@airbnb.com>",1
[AIRFLOW-6183] Fix flaky GCS hook gzip test (#6739)* [AIRFLOW-6183] Fix flaky GCS hook gzip test,3
[AIRFLOW-3014] Increase max length of connection password column to 5000 (#6241),4
"[AIRFLOW-6216] Allow pytests to be run without ""tests"" (#6770)With this change you should be able to simply run `pytest` to run all the tests in the main airflow directory.This consist of two changes:* moving pytest.ini to the main airflow directory* skipping collecting kubernetes tests when ENV != kubernetes",3
[AIRFLOW-6056] Allow EmrAddStepsOperator to accept job_flow_name as alternative to job_flow_id (#6655),1
[AIRFLOW-6168] Allow proxy_fix middleware of webserver to be configurable (#6723),5
[AIRFLOW-XXX] Fix typos in gcp_authenticator.py (#6775),2
[AIRFLOW-XXX] Fix the trailing whitespace,0
"Revert ""[AIRFLOW-3014] Increase max length of connection password column to 5000 (#6241)"" (#6783)This reverts commit da7a353e17108a23973ad0584d64a37e4a26f9b4.This was already fixed by by #6754 which was already merged, and we ended up withtwo migration files (trying) to do the same thing.",1
[AIRFLOW-XXXX] Birdz by Veolia added to users (#6781),1
Update kubernetes doc with correct path (#6774),2
[AIRFLOW-6211] Use conda for local virtualenv (#6766),1
[AIRFLOW-6209] Drop gcp_service_account_keys option (#6768)* [AIRFLOW-6209] Drop gcp_service_account_keys option* fixup! [AIRFLOW-6209] Drop gcp_service_account_keys option,4
[AIRFLOW-5685] Loading AVRO file from GCS to BQ throwing ValueError (#6355)[AIRFLOW-5685] Loading AVRO file from GCS to BQ throwing ValueError,0
"[AIRFLOW-XXX] Add template_ext to custom operator example (#6787)Update Custom Operator -> Templating section, add template_ext field to HelloOperator example.",1
[AIRFLOW-5751] add get_uri method to Connection (#6426)Add a convenience method `get_uri` on `Connection` object to generate the URI for a connection.,1
[AIRFLOW-6195] Fixed TaskInstance attrs not correct on  UI (#6758),0
[AIRFLOW-XXX] Update airflow-jira release management script (#6772),5
[AIRFLOW-6230] Improve mocking in GCP tests (#6789),3
[AIRFLOW-6084] Add info endpoint to experimental api (#6651),5
[AIRFLOW-6121][API-21] Rename Cloud Build service class (#6771),5
[AIRFLOW-XXX] Add task lifecycle diagram to documentation (#6762),2
"[AIRFLOW-5889] Make polling for AWS Batch job status more resillient (#6765)- errors in polling for job status should not fail  the airflow task when the polling hits an API throttle  limit; polling should detect those cases and retry a  few times to get the job status, only failing the task  when the job description cannot be retrieved- added typing for the BatchProtocol method return  types, based on the botocore.client.Batch types- applied trivial format consistency using black, i.e.  $ black -t py36 -l 96 {files}",2
[AIRFLOW-XXX] Add link to XCom section in concepts.rst (#6791)Add link to XCom section in concepts.rst,2
"[AIRFLOW-5660] Attempt to find the task in DB from Kubernetes pod labels (#6340)Try to find the task in DB before regressing to searching every task, and explicitly warn about the performance regressions.Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>",5
[AIRFLOW-5744] Environment variables not correctly set in Spark submit operator (#6796),1
[AIRFLOW-6171] Apply .airflowignore to correct subdirectories (#6784)Fix the defect that applied .airflowignore rules from one subdirectoryto all other subdirectories scanned later.,0
[AIRFLOW-6181] Add InProcessExecutor (#6740)Adds new executor that is meant to be used mainlyfor debugging and DAG development purposes. Thisexecutor executes single task instance at time andis able to work with SQLLite and sensors.,1
[AIRFLOW-6241] Fix typo in airflow/gcp/operator/dataflow.py (#6806),5
"[AIRFLOW-XXX] Change instances ""Google cloud storage"" to ""Google Cloud Storage"". (#6359)GCS should be formatted as ""Google Cloud Storage"", so change instances ""Google cloud storage"" to ""Google Cloud Storage"".",4
[AIRFLOW-6238] Filter dags returned by dag_stats (#6803)Add dag_ids parameter to the dag_stats end point and only request thedags on the current page. This is intended to speed up the responsetimes for systems running a large number of DAGS.,2
[AIRFLOW-6239] Filter dags return by last_dagruns (#6804)Add dag_ids get parameter to last_dagruns endpoint so can filter by theset of dag_ids present on the dags view. This is intended to speed upthe response time on systems running a large number of dags.,2
[AIRFLOW-6122] Rename CloudKMS service classes (#6798),5
[AIRFLOW-6220] Remove redundant BigQuery hook tests (#6776),3
[AIRFLOW-5959][AIP-21] Move contrib/*/jira to providers (#6661),1
[AIRFLOW-1076] Add get method for template variable accessor (#6793)Support getting variables in templates by string. This is necessary whenfetching variables with characters not allowed in a class attributename. We can then also support returning default values when a variable doesnot exist.,1
[AIRFLOW-5463] Use same session to delete and add variable in set (#6807)Why:* In our system we had a postgres connection error during Variable.setresulting in the variable being deleted. The intention of this change isthat an error should leave the variable unchanged.,4
[AIRFLOW-6170] BranchPythonOperator does not do XCom push of returned value (#6726)* [AIRFLOW-6170] BranchPythonOperator XCom push,1
[AIRFLOW-XXX] Bump Jira version to fix issue with async (#6813),0
[AIRFLOW-6243]Optimize NamedHivePartitionSensor poke (#6810),5
[AIRFLOW-6247] Fix sort order in Alembic Migration template (#6809)* [AIRFLOW-6247] Fix sort order in Alembic Migration template,0
[AIRFLOW-6091] Add flushing in execute method for BigQueryCursor (#6683)If you execute multiple queries results of old ones will beflushed allowing to read results of recent execute withoutany issues.,0
[AIRFLOW-4824] Add charset handling for SqlAlchemy engine for MySqlHook (#6816)Airflow should handle various charsets for connections with MySQL dbs.This change allows to set charset in extra field of a connection whenusing SqlAlchemy engine.,1
[AIRFLOW-6254] obscure conn extra in logs (#6817),2
[AIRFLOW-6226] Always reset warnings in tests (#6785),3
[AIRFLOW-XXX] Bump npm from 6.4.1 to 6.13.4 in /airflow/www (#6815)Bumps [npm](https://github.com/npm/cli) from 6.4.1 to 6.13.4.- [Release notes](https://github.com/npm/cli/releases)- [Changelog](https://github.com/npm/cli/blob/latest/CHANGELOG.md)- [Commits](https://github.com/npm/cli/compare/v6.4.1...v6.13.4)Signed-off-by: dependabot[bot] <support@github.com>,1
"[AIRFLOW-6250] Ensure on_failure_callback always has a populate context (#6812)on_failure_callback almost always want to know the dag_id and taskinstance that failed. These info are in the context passed to on_failure_callback, which is passed in from handle_failure(). However, in some rare scenarios, if handle_failure is called in scheduler_job.py and backfill_job.py, the only argument passed is the error message. context is left as None.So in these cases, on_failure_callback will not even know what's the dag_id of the dag that just failed.This PR fixes this by setting context to get_template_context() if it's not given.",1
[AIRFLOW-6246] Add support None in mysql_to_gcs (#6808),1
[AIRLFOW-6240] BigQuery - remove unnecessary mocks in hook system tests (#6805),3
[AIRFLOW-XXX] Fix DebugExecutor docs (#6830),2
[AIRFLOW-6263] Fix broken WinRM integration (#6832),0
"[AIRFLOW-3152] Kubernetes Pod Operator should support init containers. (#6196)* Add support for init-containers to Kubernetes Pod OperatorEnables start-up related code to be added for an app container in K8s Pod operator.Add new init_container resource that can be attached to the K8s Pod.* Update init_container and fix testsFix the error in init_container and the associated tests.* Refactor and fix testsFix tests for init_containers* Fix init_container test errorsRemove unused mocks in init_container test* Fix init_container test errorsUpdate volume mount object used in init_container test* Fix init_container test errorsAdd the missing volume setup for the init_container test.* Fix init_container test failure.Fix the expected result in the init_container test.* Fix init_container test failuresUpdate expected results in the init_container tests* Update the KubernetesPodOperator guideUpdate the KubernetesPodOperator guide to document support for init containers* Update init-container testsFix test failures casued due python versions by sorting the output before assert test.* Update init-container to use k8s V1Container objectRemove custom object InitContainer.Allow users to pass List[k8s.V1Container] as init-container in K8sPodOperator* Add missing init_containers initalization in K8s pod operatorDue to rebase from master, certain sections of the kubernetes_pod_operator.py file was refactored which led to missing init_containers initalization in K8s pod operator. Add missing init_containers initalization in K8s pod operator. Update kubernetes pod operator configurations in init container test.",3
[AIRFLOW-6262] add on_execute_callback to operators (#6831),1
"[AIRFLOW-3189] Remove schema from get_uri response if None (#6833)""None"" was appended to uri if schema=None. Check was added ifschema is None.",1
[AIRFLOW-6083] Adding ability to pass custom configuration to lambda client. (#6678)There was no way to pass custom configuration from AwsLambdaHook to lambda client. This was an issue when for example default lambda timeout was too short.,0
[AIRFLOW-6206] Move and rename AWS batch operator [AIP-21] (#6764)- conform to AIP-21  - see https://issues.apache.org/jira/browse/AIRFLOW-4733  - see https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-21%3A+Changes+in+import+paths  - use airflow.providers.amazon.aws.operators.batch.AwsBatchOperator  - deprecate airflow.contrib.operators.awsbatch_operator.AWSBatchOperator- fix pylint for airflow/providers/amazon/aws/operators/batch.py,1
[AIRFLOW-6260] Drive _cmd config option by env var (#6801)This improves the ability to configure AirFlowusing Kubernetes best practices. You can providefor exemple AIRFLOW__CORE__SQL_ALCHEMY_CONN_CMDreferencing a shell script that computes theconnection string using Kubernetes secrets.And that script can be provided to the containerusing a configmap.Adding a unit test to check that an option thatshould NOT be overriden by a command is correctlyonly read from the configuration.,5
[AIRFLOW-6222] http hook logs response body for any failure (#6779),0
[AIRFLOW-4940] Simplify tests of DynamoDBToS3Operator (#6836)There's no point using something from multiprocessing when a `[]` willdo just fine.,1
[AIRFLOW-5349] Add schedulername option for KubernetesPodOperator (#6088),1
[AIRFLOW-5532] Fix imagePullSecrets in pod created from k8s executor (#6166)* [AIRFLOW-5532] Fix imagePullSecrets in pod created from k8s executor* Keeps using PodGenerator constructor for setting up image_pull_secrets,1
[AIRFLOW-6223] Refactor BigQuery hook tests (#6777),3
[AIRFLOW-5681] Allow specification of a tag or hash for the git_sync init container (#6350)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
[AIRFLOW-5458] Bump Flask-AppBuilder to 2.2.0 (#6607)This might also fix AIRFLOW-5462 (OAuth login issue)Same fix as was required for Superset (https://github.com/apache/incubator-superset/issues/7739),0
[AIRFLOW-6259] Reset page with each new search (#6828)Search incorrectly persists the page number which can confusinglyyield no results when making searches from 2nd page onwards,1
[AIRFLOW-6268] Prevent ajax calls when no dags (#6839)Prevent 4 AJAX calls on empty search result page of DAGs view.,2
[AIRFLOW-6256] BaseJob table was not deleted on db reset (#6818),1
[AIRFLOW-6130] Make Cassandra to GCS operator pylint compatible (#6692)* [AIRFLOW-6130] Make Cassandra to GCS operator pylint compatible,1
"[AIRFLOW-6038] AWS DataSync reworked (#6773)Added Amazon AWS how-to documentation scaffolding, plus example DAGs for AWS DataSync Operators with their respective how-to guides.Reworked AWS DataSync operators into a single, logically idempotent operator.",1
[AIRFLOW-6269] Add show config command in CLI (#6840),5
[AIRFLOW-XXX] Add simple guidelines to unit test writing (#6846),3
[AIRFLOW-XXX] Update README.md (#6848)Add Udacity to users,1
[AIRFLOW-6297] Add Airflow website link to UI docs (#6849),2
[AIRFLOW-6270] Remove good errors from docs building (#6845),2
"[AIRFLOW-6126] [AIP-21] Rename GCP Speech operators (#6827)Also these modules have been changed GcpTextToSpeechSynthesizeOperator, GCPSpeechToTextHook, GCPTextToSpeechHook",1
[AIRFLOW-5616] Switch PrestoHook from pyhive to prestosql-client to support transactions. (#6822)Replace the pyhive client with presto official client.,1
[AIRFLOW-5903] Remove serve_logs command from CLI (#6843)* [AIRFLOW-5903] Remove serve_logs command,2
"[AIRFLOW-6271] Printing log files read during load_test_config (#6842)Airflow should show where the configuration is read from. Especially for newuser it might be confusing as it is not logged anywhere and there are severalplaces the configuration might be read from. This is printed for the main config file, but not when load_test_config is used.",1
[AIRFLOW-6147] [AIP-21] Rename GoogleCloudStorageToS3Operator (#6826),1
[AIRFLOW-6101] [AIP-21] Rename dataflow service (#6852),5
[AIRFLOW-6100] [AIP-21] Rename bigquery service (#6854),5
[AIRFLOW-6104] [AIP-21] Rename datastore service (#6853),5
[AIRFLOW-5903] Remove serve_logs command from CLI (#6843) (#6857),2
[AIRFLOW-6272] Switch from npm to yarnpkg for managing front-end dependencies (#6844)It is:- quicker to install- easier to get repeatable results- Takes up less space (130MB/15k files vs 190MB/23k files)- nicer to user (has better help),1
[AIRFLOW-XXXX] Fix downgrade of db migration 0e2a74e0fc9f (#6859),5
[AIRFLOW-6267] Add View Creation to BigQueryOperator (#6838),1
[AIRFLOW-6086] Correctly pick up spark_binary from Connectin in SparkSubmitOperator (#6680)Remove default value of spark_binary from the Operator because it's better managed on spark hook.,1
"[AIRFLOW-6313] Unify example or doc dag owner (#6864)We already change Airflow DAG default owner to 'airflow'in https://github.com/apache/airflow/pull/4151 but someof our example DAGs and docs are still usingowner = 'Airflow', This patch to unify them",1
[AIRFLOW-6312] Unpin marshmallow-sqlalchemy for Py>3.5 (#6861) (#6866),1
[AIRFLOW-6317] BigQuery - move repeated variables to constants in hook tests (#6869),3
"[AIRFLOW-6106] [AIP-21] Rename GCP compute operators (#6872)fixed examp. dags, howto, testsfixed docstrings",2
[AIRFLOW-6314] BigQuery - improve BigQueryHook test (#6865)* [AIRFLOW-6314] BigQuery - improve BigQueryHook testMove methods for BigQueryHook under one testings classAdd new testsImprove mock assertions* fixup! [AIRFLOW-6314] BigQuery - improve BigQueryHook test,3
[AIRFLOW-6105] [AIP-21] Rename Bigtable operators (#6862),1
[AIRFLOW-6311] Remove python2-required encoding from settings (#6860),1
[AIRFLOW-6320] Add quarterly to crontab presets (#6873),1
"[AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868)Recently we hard code in tutorial.rst whichis hard to maintain, such as `set_upstream`is change to shift in tutorial.py but stillin tutorial.rst. Use sphinx is a better way",1
[AIRFLOW-6315] Create .airflow_db_initialised always in tests dir (#6867)* [AIRFLOW-6315] Create .airflow_db_initialised always in root tests directory* fixup! [AIRFLOW-6315] Create .airflow_db_initialised always in root tests directory,3
[AIRFLOW-6325] Clear cli hint (#6880),5
[AIRFLOW-XXX] Add docs showing usage of `Connection.get_uri` (#6863),1
[AIRFLOW-6323] Remove non-ascii letters from default config (#6878),5
[AIRFLOW-6333] Bump Pylint to 2.4.4 & fix/disable new checks (#6888),1
[AIRFLOW-6331] Pylint: Disable Missing Module Docstring (#6885),2
[AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor (#6877)* [AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor* fixup! [AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor* fixup! fixup! [AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor* fixup! fixup! fixup! [AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor* fixup! fixup! fixup! fixup! [AIRFLOW-6324] Hide celery commands when executor is not CeleryExecutor,0
[AIRFLOW-4113] Unpin boto3 (#6884),5
[AIRFLOW-XXX] Add Changelog for 1.10.7 (#6890),4
[AIRFLOW-XXX] Fix broken DAG Serialization Link (#6891),2
[AIRFLOW-6336] Make tests/utils pylint compatible (#6892),3
[AIRFLOW-6338] Make tests/sensors pylint compatible (#6893),3
[AIRFLOW-6339] Make tests/hooks pylint compatible (#6895),3
[AIRFLOW-6341] Make tests/models pylint compatible (#6897),3
[AIRFLOW-6332] Extract logging options to new section (#6887)* [AIRFLOW-6332] Extract logging options to new section,1
[AIRFLOW-6340] Make tests/contrib pylint compatible (#6896),3
[AIRFLOW-6330] Show cli help when param blank or typo (#6883)When enter Airflow cli with blank parameteror typo parameter or wrong parameter willshow Airflow cli help just like enter`Airflow [command] -h` command,2
[AIRFLOW-6337] Make tests/operators pylint compatible (#6894),3
[AIRFLOW-XXXX] Adds .mailmap to coalesce commits by person in the shortlog (#6879),2
[AIRFLOW-6343] Make tests/* pylint compatible (#6899),3
[AIRFLOW-6362] Fix typehint for CommandType (#6906),0
[AIRFLOW-6364] Move conn tests from test_core to test_connection (#6914),3
[AIRFLOW-6335] dag_processor_manager timeout logs should be ERROR not WARN (#6916),0
[AIRFLOW-6365] Remove tests/compat (#6919),3
[AIRFLOW-6350] security - spark submit operator logging+exceptions sh… (#6917)* [AIRFLOW-6350] security - spark submit operator logging+exceptions should mask passwords,4
[AIRFLOW-6351] security - ui - Add Cross Site Scripting defence (#6913),1
[AIRFLOW-6366] Fix migrations for MS SQL Server (#6920),0
[AIRFLOW-6363] Split complex tests in TestDag into smaller units (#6910),3
[AIRFLOW-5406] allow spark without kubernetes (#6921),1
[AIRFLOW-6229] SparkSubmitOperator polls forever if status json can't… (#6918),5
[AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901),0
[AIRFLOW-6348] security - cli.py is currently printing logs with pass… (#6915),4
[AIRFLOW-6368][Depends on 6367] Move conf tests from test_core to test_configuration (#6925),5
[AIRFLOW-6358] - log details of failed task (#6908),0
[AIRFLOW-6371] Move dag tests from test_core to test_dag (#6927),3
[AIRFLOW-6370] Skip Cassandra tests if cluster is not up (#6926),3
[AIRFLOW-XXXX] Fix GCSTaskHandler Comment Typo (#6928),2
[AIRFLOW-6374] Automate sending emails for Release management (#6932),5
[AIRFLOW-6357] Highlight nodes in Graph UI if task id contains dots (#6904),2
[AIRFLOW-6380] Create separate variable for each subcommands (#6937),1
[AIRFLOW-6379][depends on AIRFLOW-6377] Simplify `airflow task run` logic (#6936)* [AIRFLOW-6377] Decouple cli utils from argparse.Namespace,2
[AIRFLOW-XXX] Remove trailing whitespaces from UPDATING.md (#6940),5
[AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941),1
[AIRFLOW-6352] security - ui - add login timeout (#6912),2
[AIRFLOW-6319] Add support for AWS Athena workgroups (#6871),1
[AIRFLOW-6376] Extract order_queued_tasks_by_priority method (#6933),4
[AIRFLOW-6382] Extract provide/create session to session module (#6938)Extracting provide_session and create_session to separate modulereduces number of cyclic imports and make a disctinction betweensession and database,5
[AIRFLOW-5149] - skip SLA checks config (#6923)* [AIRFLOW-5149] - skip SLA checks config,5
[AIRFLOW-6318] Change python3 as Dataflow default interpreter (#6945),5
[AIRFLOW-XXX] add autoenv to gitignore (#6946),1
[AIRFLOW-6057] Update template_fields of the PythonSensor (#6656)Add op_args and op_kwargs to the template_fields of the PythonSensor.,1
[AIRFLOW-4445] mushroom cloud errors too verbose (#6952)* [AIRFLOW-4445] mushroom cloud errors too verbose,0
[AIRFLOW-6391] Move content of utils.tests to tests.test_utils (#6949),3
[AIRFLOW-XXX] add note warning that bash>4.0 is required for docs build script (#6947),2
[AIRFLOW-6356] clear/dag_state should not show logs from other dags (#6951),2
[AIRFLOW-XXX] Add tips for writing a note in UPDATIND.md (#6960),5
[AIRFLOW-6397] ensure sub_process attribute exists before trying to kill it (#6958),1
[AIRFLOW-6400] Fix pytest not working on Windows (#6964),1
[AIRFLOW-XXX] fix gcp keyfile_dict typo (#6962),2
[AIRFLOW-6394] Simplify github PR template (#6955),5
[AIRFLOW-6403] BigQuery - add tests for BigQueryCursor (#6967),3
[AIRFLOW-6398] improve flakey test test_mark_success_no_kill (#6959),3
[AIRFLOW-6409] BigQuery hook - add tests for run_query method (#6971),1
[AIRFLOW-XXXX] Add Documentation for check_slas flag (#6974),2
[AIRFLOW-6359] spark_submit_hook.py status polling interval config (#6909)* AIRFLOW-6359* pep checks* make travis CI refire* new section = bad* move poll_interval into the operator instead of .cfg* review comments,5
[AIRFLOW-XXX] Fix development packages installtion instructions (#6942),0
[AIRFLOW-6396] Use tempfile.TemporaryDirectory instead of custom one (#6957),2
[AIRFLOW-6110] [AIP-21] Rename natural_language service (#6968),5
[AIRFLOW-6261] flower_basic_auth eligible to _cmd (#6825)Make the configuration option flower_basic_auth from celeryavailable as the stdout of a command as it contains sensitiveinformation.,5
[AIRFLOW-6393] Ensure rendering of all lineage items and record source (#6953)Only lineage items obtained from XCom were rendered rather thanall. Additionally source tasks are recorded.,1
[AIRFLOW-6359] Make Spark status_poll_interval explicit (#6978),1
[AIRFLOW-6412] Add config file for ci-reporter Probot Integration (#6980),2
[AIRFLOW-6413] Add config file for Mergeable Github integration (#6981),7
[AIRFLOW-5385] spark hook does not work on spark 2.3/2.4 (#6976),1
[AIRFLOW-6108] [AIP-21] Rename GCP DLP operators (#6966),1
[AIRFLOW-6109] [AIP-21] Rename GCP function operators and hooks (#6977),1
[AIRFLOW-6392] Remove cyclic dependency baseoperator <-> helpers (#6950)There is a hidden cyclic dependency between baseoperator and helpers module.It's hidden by local import but it is detected when baseoperator/helpers areremoved from pylint_todo.txt (and it's really there).The dependency comes from BaseOperator using helpers and two helpers methods(chain and cross_downstream) using BaseOperator. This can be solved byconverting the chain and cross_downstream methods to be static methods inBaseOperator class.,1
[AIRFLOW-6327] http_hook: Accept json= parameter for payload (#6886)* [AIRFLOW-6327] http_hook: Accept json= parameter for payload* [AIRFLOW-6327] CODE REVIEW* [AIRFLOW-6327] CODE REVIEW 2* [AIRFLOW-6327] CODE REVIEW 3Co-Authored-By: dstandish <dstandish@users.noreply.github.com>Co-authored-by: dstandish <dstandish@users.noreply.github.com>,1
[AIRFLOW-6432] Raise appropriate exception in EmrAddStepsOperator when using job_flow_name and no cluster is found (#6898)* [AIRFLOW-6432] fixes in EmrAddStepsOperatorfix EmrAddStepsOperator broken ref & faulty test* changes after CR #1* Add exception and test case* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/operators/emr_add_steps_operator.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/contrib/hooks/emr_hook.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Update tests/contrib/operators/test_emr_add_steps_operator.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* changes after CR #2Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,4
[AIRFLOW-XXXX] More .mailmap (#6982),5
[AIRFLOW-6411] Raise exception on invalid task run option selection (#6979),1
[AIRFLOW-6205] The 'airflow db shell' works on mysql without port/passwd (#6763),4
[AIRFLOW-XXXX] add empty line so that hints are indented better (#6989),1
[AIRFLOW-6417] Disable approval requirements from mergeable (#6990),7
[AIRFLOW-XXXX] Add `.autoenv_leave.zsh` to .gitignore (#6986),1
[AIRFLOW-XXXX] Fix docstring in db_command.py (#6992),5
[AIRFLOW-6418] Remove SystemTest.skip decorator (#6991),5
[AIRFLOW-6421] Remove ci-reporter Probot app config (#6996),5
[AIRFLOW-XXXX] Fix typo in task_command.py (#6997),2
[AIRFLOW-6373] Make airflow/utils pylint compatible (#6929)This PR removes airflow/utils/* files from pylint todo listand implements required changes.,4
[AIRFLOW-XXXX] Add separator to pull request template (#7001),1
[AIRFLOW-XXXX] You can now paste JIRA ID only once (#7009),5
[AIRFLOW-6426] Fix error in example_gcs Dag (#7003),2
[AIRFLOW-6425] Serialization: Add missing DAG parameters to Json Schema (#7002),5
[AIRFLOW-6430] - BigQuery hook - add tests for BigQueryBaseCursor (#7010),3
[AIRFLOW-6427] Fix broken example_qubole_operator dag (#7005),2
[AIRFLOW-6112] [AIP-21] Rename GCP SQL operators and hooks (#7006)fixed tests,3
[AIRFLOW-XXXX] Update operation chaining documentation (#7018),2
[AIRFLOW-6428] Fix import path for airflow.utils.dates.days_ago in Example DAGs (#7007),2
[AIRFLOW-6353] security - ui - add click jacking defence (#6995),1
[AIRFLOW-6434] add return statement back to DockerOperator.execute (#7013),2
[AIRFLOW-6436] Create & Automate docs on Airflow Configs (#7015),5
[AIRFLOW-6433] reduce conf.get lookups in scheduler_job.py loops (#7012),5
[AIRFLOW-6385] BugFix: SlackAPIPostOperator fails when blocks not set (#7022),1
[AIRFLOW-6436] Add x_frame_enabled config in config.yml (#7024),5
[AIRFLOW-6115] [AIP-21] Rename GCP vision operators (#7020)ignred unused imports in gcp_vision_operator and fixed test_core_to_contrib,3
[AIRFLOW-6442] BigQuery hook - standardize handling http exceptions (#7028),1
[AIRFLOW-6444] Fix invalid argument error in example_dataflow dag (#7029),2
[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7032),1
[AIRFLOW-6446] Add Github Action to Welcome First time contributors (#7031)* [AIRFLOW-6446] Add Github actions to Welcome First time contributors* Update the welcome message,5
[AIRFLOW-6449] Remove Github actions until a known bug is fixed (#7035),0
[AIRFLOW-6447] Add GitHub Action to add Labels on Pull Requests (#7039),1
[AIRFLOW-XXXX] Fix Path for Github Action (#7040),0
[AIRFLOW-6436] Cleanup for Airflow configs doc generator code (#7036),2
[AIRFLOW-XXXX] Add more cross-reference (#7045),1
"[AIRFLOW-XXXX] Used fixed periodic auto-labeler from potiuk's repo (#7047)The periodic labeler had a bug that it was not using pagination.In effect the labeler was checking only 30 oldest request andif they all contained proper labels, it was not checking any morePRs.Details are available inhttps://github.com/paulfantom/periodic-labeler/pull/4But until it is merged, we switch to our own fixed version.",0
[AIRFLOW-XXXX] Decrease frequency of PR labelling Github Action (#7048)With the current frequency we are going to exhaust our 2000 freeminutes in about 10 days. We should decrease the frequency quickly.,5
[AIRFLOW-6459] Increase verbosity of pytest (#7049),3
[AIRFLOW-XXXX] Add Probot App to add labels on PR (#7053),1
[AIRFLOW-XXXX] Remove Github action to add label (#7055),1
[AIRFLOW-6460] Reduce timeout in pytest (#7051),3
[AIRFLOW-XXXX] Add more label configs for Github App (#7056),5
[AIRFLOW-6461] Remove silent flags in Dockerfile (#7052),2
[AIRFLOW-64624] Add cloud providers CLI tools in Breeze (#7059),1
[AIRFLOW-6465] Add bash autocomplete for airflow in Breeze (#7060),1
[AIRFLOW-XXXX] Add more files for area:dev label (#7058),2
"[AIRFLOW-6460] - Reverting ""Reduce timeout in pytest (#7051)"" (#7062)This reverts commit bf3fa1fda2f45946e2bf4b2d3377a5c0bc23664d.",4
[AIRFLOW-6462] Limit exported variables in Dockerfile/Breeze (#7057),2
"[AIRFLOW-6471] Add pytest-instafail plugin (#7064)We have a problem currently that if a test fails in CI we do not see thefailures immediately - only when it finishes, but when tests hang, sometimesthe details about failed tests are not shown immediately. We tried to increaseverbosity but it's not very helpful.The pytest-instafail plugin solves the problem without increasing verbosity.",1
[AIRFLOW-6470] Avoid pipe to file when do curl (#7063),2
[AIRFLOW-6416] Sort default connection by conn_id (#6987),5
[AIRFLOW-6437] Sql filters - remove in (NULL) (#7033),4
Fixed communication ids (#7016)Fixed communication ids to match the labels in the diagram.8 and 9 were swapped,0
[AIRFLOW-XXXX] Update types in docstrings (#7050),2
[AIRFLOW-XXXX] Add `airflow dags show` command guide (#7014),2
[AIRFLOW-6467] Use self.dag i/o creating a new one (#7067),1
[AIRFLOW-XXXX] Clarify wait_for_downstream and execution_date (#6999),5
[AIRFLOW-6476] Fix directories created by docker (#7066)Some of the files that are mounted to breeze might not be created when firsttime run and then docker will turn them into directories.We should delete dirs/touch files in both breeze script and static checkscripts to get it fixed,0
[AIRFLOW-XXXX] Add config to welcome first time contributors (#7071),5
[AIRFLOW-XXXX] Change company name (#7073)- rename Publicis Pixelpark to Digitas Pixelpark,4
[AIRFLOW-5621] - Failure callback is not triggered when marked Failed on UI (#7025),0
[AIRFLOW-6484] BigQuery hook - standardize handling http exceptions (missing) (#7076),1
[AIRFLOW-XXXX] Fix a typo in flower command (#7074),2
[AIRFLOW-XXXX] Boring cyborg automatically inserts issue link (#7078)Issue link in PR will be now automatically inserted by the boringcyborg probot integration. It also handles the case of document-onlychanges and it will keep the link updated if the title of the PRis updated.,5
[AIRFLOW-6485] BigQuery hook - add missing test for BIgQueryBaseCursor methods (#7077),3
[AIRFLOW-XXXX] Add missing relation to op4 (#7080),1
[AIRFLOW-XXXX] Add useful tips to first PR msg (#7082),1
[AIRFLOW-6456] Remove create_mock_args in CLI tests (#7044),3
[AIRFLOW-4428] error if exec_date before default_args.start_date in t… (#6948),5
[AIRFLOW-6387] print details of success/skipped task (#6956),5
[AIRFLOW-6360] 'Recent tasks' stats only show non-completed dagruns option (#7037),2
[AIRFLOW-XXXX] Fix typos and broken links in development docs (#7086),2
[AIRFLOW-6113] [AIP-21] Rename GCP transfer operator (#7000)changes in howto refs in cloud_storage_transfer_service.rst,4
[AIRFLOW-6381] Remove styling based on DAG id from DAGs page (#6985),2
[AIRFLOW-6498] BigQuery operator tests - add deprecation tests (#7093),3
[AIRFLOW-6116] [AIP-21] Rename MLEngine operators (#7021),1
[AIRFLOW-6489] Add BATS support for Bash unit testing (#7081)We have far too much bash code around that is not automatically tested.This is the first step to change it (simplifications and more tests are comingsoon).,3
[AIRFLOW-6347] BugFix: Can't get task logs when serialization is enabled (#7092),0
"[AIRFLOW-6491] Improve handling of Breeze parameters (#7084)While working on improving the way we run Kubernetes tests, we found out that Ineed to fix handling of parameters - we change Kubernetes version used via Kindand the old versions are no longer valid, however it was not properlyremoved/saved.We use the opportunity to add automated tests for that feature.(cherry picked from commit 38dea9132d1fa36f4fbe871e2ab037be5ad3fab2)",3
"[AIRFLOW-6475] Remove duplication of volume mount specs in Breeze.. (#7065)We had two sets of duplicated local volume mounts - one for ./breezeinteractive runs (with docker_compose) and the other with scripts that are usedto run static checks.The volumes from the .yaml for docker compose are now the ""source of truth""and the bash script parses the yaml and useis volume mounts from there.",1
"[AIRFLOW-6245] Add custom waiters for AWS batch jobs (#6811)- add AwsBatchWaiters  - the waiters are based on botocore, but not yet    available for AWS batch services- refactor AwsBatchOperator:  - use an optional waiters object to wait for    batch job status indicators  - split execute into submit_job and monitor_job  - use job_id with an optional init-parameter;    discard jobId and jobName (already has job_name)  - inherit from AwsBatchClient  - add notes to UPDATING.md- extract class for AwsBatchClient  - move responsibility for batch API calls and    response parsing to this client  - move responsibility for default wait and    polling to this client- rename BatchProtocol to AwsBatchProtocol [AIP-21]  - test backward compatibility  - add PROTOCOLS to tests/test_core_to_contrib.py  - add notes to UPDATING.md- split up polling for job status into steps:  - poll for a JobExists  - poll for a JobRunning  - poll for a JobComplete- use random jitter for wait-polling delays for  high concurrency job polling- modify the exponential backoff delay for the  existing polling functions- revise and update unit tests",3
[AIRFLOW-6118] [AIP-21] Rename Pubsub operators and hook (#7046)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules* adde deprecation warnings to the contrib modules* fixed tests* updated UPDATING.md,5
[AIRFLOW-6111] [AIP-21]  Rename GCP spanner operator and hook (#7004),1
Update the version of cattrs from 0.9 to 1.0 to support Python 3.8 (#7100),1
[AIRFLOW-6504] Allow specifying configmap for Airflow Local Setting (#7097),1
[AIRFLOW-XXXX] Description for how to create an user is incorrect in some docs (#7101),2
[AIRFLOW-6479] Update celery command calls (#7068),5
[AIRFLOW-6511] Remove BATS docker containers (#7103)The containers were not removed and you have to remove themwith `dockery system prune`. The --rm flag is added.,1
[AIRFLOW-XXXX] Clear debug docs (#7104),2
[AIRFLOW-6513] Add web_pdb/pudb support in post_mortem option (#7105),1
[AIRFLOW-XXXX] Update docs on example files for k8s (#7095),2
[AIRFLOW-XXXX] Update logging section of config docs (#7106),2
[AIRFLOW-XXXX] Add descriptions to kubernetes section of config docs (#7107),2
[AIRFLOW-XXXX] Move airflow-config-yaml pre-commit before pylint (#7108),5
[AIRFLOW-6516] BugFix: airflow.cfg does not exist in Volume Mounts (#7109),5
AIRFLOW-XXXX improve clarity of confirm message (#7110),5
[AIRFLOW-6517] make merge_dicts function recursive (#7111),1
[AIRFLOW-6451] self._print_stat() in dag_processing.py should be skippable by config option (#7096),5
[AIRFLOW-6326] Sort cli commands and arg (#6881),5
"[AIRFLOW-6452] scheduler_job.py - remove excess sleep/log/duration calls (#7089)no need for 2 sleep, _processor_poll_interval will already sleep",2
[AIRFLOW-6125] [AIP-21] Rename S3 operator and SFTP operator (#7112)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules* fixed tests,3
[AIRFLOW-4026] Add filter by DAG tags (#6489),2
[AIRFLOW-6495] Load DAG only once when running a task using StandardTaskRunner (#7090),1
[AIRFLOW-6490] Improve time delta comparison in local task job tests (#7083),3
[AIRFLOW-6519] Make TI logs constants in Webserver configurable (#7113),5
[AIRFLOW-5413] Refactor worker config (#7114),5
[AIRFLOW-6438] Filter DAGs returned by blocked (#7019)Add dag_ids GET parameter to /blocked end point to allow querying of justthe dags present on the current page.,2
[AIRFLOW-2279] Clear tasks across DAGs if marked by ExternalTaskMarker (#6633),2
[AIRFLOW-XXXX] Update start.rst to reference specific DAG (#7094),2
"Revert ""[AIRFLOW-6451] self._print_stat() in dag_processing.py should be skippable by config option (#7096)"" (#7129)This reverts commit 77b1bdc12ca5ddf043d4550d36948766b59f60ce.Reverts #7096 to do in a slightly different way (without a new config option), and reverting this so that the new change is easier to backport to 1.10 releases.",4
[AIRFLOW-6507] Replace the use of imp.load_source with another solution. (#7099),1
[AIRFLOW-6520] Simplify `airflow config` command (#7117),5
[AIRFLOW-5167] Update dependencies for GCP packages (#7116),5
[AIRFLOW-6528] disable flake8 W503 line break before binary operator (#7124),1
[AIRFLOW-6506] do_xcom_push defaulting to True (#7122),5
[AIRFLOW-XXXX] Remove unused line in k8s worker_configuration.py (#7121),5
[AIRFLOW-XXXX] Fix reference in concepts doc (#7135)Correcting reference in Concepts -> Cluster Policy doc from airflow_setting.py to airflow_local_settings.py,1
"[AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6516)* Fixed problem that Kubernetes tests were testing latest master  rather than what came from the local sources.* Kind (Kubernetes in Dcocker) is run in the same Docker as Breeze env* Moved Kubernetes scripts to 'in_container' dir where they belong now* Kubernetes cluster is reused until it is stopped* Kubernetes image is build from image in docker already + mounted sources* Kubectl version name is corrected in the Dockerfile* KUBERNETES_VERSION can now be used to select Kubernetes version* Running kubernetes scripts is now easy in Breeze* We can start/recreate/stop cluster using  --<ACTION>-kind-cluster* Instructions on how to run Kubernetes tests are updated* The old ""bare"" environment is replaced by --no-deps switch",5
[AIRFLOW-6537] Fix backticks in rst files (#7140),2
[AIRFLOW-6536] Make job_id parameter optional (#7136)It can be passed in the json parameter,2
[AIRFLOW-6539][AIP-21] Move Apache classes to providers.apache package (#7142)* [AIP-21] Move contrib.hooks.pinot_hook providers.apache.pinot.hooks.pinot* [AIP-21] Move contrib.hooks.spark_jdbc_hook providers.apache.spark.hooks.spark_jdbc* [AIP-21] Move contrib.hooks.spark_jdbc_script providers.apache.spark.hooks.spark_jdbc_script* [AIP-21] Move contrib.hooks.spark_sql_hook providers.apache.spark.hooks.spark_sql* [AIP-21] Move contrib.hooks.spark_submit_hook providers.apache.spark.hooks.spark_submit* [AIP-21] Move contrib.hooks.sqoop_hook providers.apache.sqoop.hooks.sqoop* [AIP-21] Move contrib.operators.druid_operator providers.apache.druid.operators.druid* [AIP-21] Move contrib.operators.spark_jdbc_operator providers.apache.spark.operators.spark_jdbc* [AIP-21] Move contrib.operators.spark_sql_operator providers.apache.spark.operators.spark_sql* [AIP-21] Move contrib.operators.spark_submit_operator providers.apache.spark.operators.spark_submit* [AIP-21] Move contrib.operators.sqoop_operator providers.apache.sqoop.operators.sqoop* [AIP-21] Move contrib.sensors.hdfs_sensor providers.apache.hdfs.sensors.hdfs* [AIP-21] Move hooks.druid_hook providers.apache.druid.hooks.druid* [AIP-21] Move hooks.hdfs_hook providers.apache.hdfs.hooks.hdfs* [AIP-21] Move hooks.hive_hooks providers.apache.hive.hooks.hive* [AIP-21] Move hooks.pig_hook providers.apache.pig.hooks.pig* [AIP-21] Move hooks.webhdfs_hook providers.apache.hdfs.hooks.webhdfs* [AIP-21] Move operators.druid_check_operator providers.apache.druid.operators.druid_check* [AIP-21] Move operators.hive_operator providers.apache.hive.operators.hive* [AIP-21] Move operators.hive_stats_operator providers.apache.hive.operators.hive_stats* [AIP-21] Move operators.pig_operator providers.apache.pig.operators.pig* [AIP-21] Move sensors.hive_partition_sensor providers.apache.hive.sensors.hive_partition* [AIP-21] Move sensors.metastore_partition_sensor providers.apache.hive.sensors.metastore_partition* [AIP-21] Move sensors.named_hive_partition_sensor providers.apache.hive.sensors.named_hive_partition* [AIP-21] Move sensors.web_hdfs_sensor providers.apache.hdfs.sensors.web_hdfs* Update docs* [AIP-21] Move sensors.hdfs_sensor providers.apache.hdfs.sensors.hdfs,1
"Display all containers logs (#7144)Before:If the container emits logs very early (after the start but before the attach),then those logs are lost.Now:Because attach is done before start, no logs are lost any more.",2
"[AIRFLOW-6119] [AIP-21] Rename GCS operators, hooks and sensors (#7125)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules:* GoogleCloudStorageHook* GoogleCloudStorageDeleteOperator* GoogleCloudStorageDownloadOperator* GoogleCloudStorageListOperator* GoogleCloudStorageCreateBucketOperator* GoogleCloudStorageBucketCreateAclEntryOperator* GoogleCloudStorageObjectCreateAclEntryOperator* GoogleCloudStorageObjectSensor* GoogleCloudStorageObjectUpdatedSensor* GoogleCloudStoragePrefixSensor* GoogleCloudStorageUploadSessionCompleteSensor",0
[AIRFLOW-6545] Add commit message validation (#7149),5
[AIRFLOW-4502] Add new CLI command - task_states_for_dag_run (#6993)Co-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-6102] [AIP-21] Rename Dataproc operators (#7151),1
"[AIRFLOW-XXXX] Remove trailing slash of JIRA issue link (#7155)Trailing slash causes weird problem. When you enter the issuevia this link, your URL in browser gets updated to an empty onewhich results in Error Page when you resolve the ticket.",0
"[AIRFLOW-1467] Dynamic pooling via allowing tasks to use more than one pool slot (depending upon the need) (#6975)* adding pool capacity required for each task for dynamic pooling* Added pool_capacity column migration script* removed test checkedin file* removed extra space* correct test_database_schema_and_sqlalchemy_model_are_in_sync test case* Added description for pool_capacity property for task instance* Modified test cases to include pool_capacity along with pool in task instances* Modified test cases to include pool_capacity along with pool in task instances* Removed Column.name property, since property value is same as actual variable* check for pool_capacity property to be always >= 1* removed unused variable ti* modified test cases for pool_capacity* modified test cases for pool_capacity",3
[AIRFLOW-XXXX] Add instructions to mark the boxes. (#7156)It was removed when we simplified the template but not everyoneknows that they should do it.,4
[AIRFLOW-6548] Restore GCS tests removed by migration (#7152)This PR restores tests that were accidentally removed inhttps://github.com/apache/airflow/pull/6077,4
"[AIRFLOW-6521] Add project param to BigQuery hook .getSchema method (#7118)BigQuery hook can now take project_id as a parameter, so that operators may specify project_ids other than the default specified in connection.",1
"[AIRFLOW-2516] Fix mysql deadlocks (#6988)Deadlocks were occuring in mysql when task_instance was modifiedby two queries at the same time. One query used state as selectioncriteria and updated it in the same query where second query justupdated the state for the same table. The first query locked stateindex first and primary index afterwards, the second query lockedprimary index first and state afterwards - leading to deadlocks.This change splits the first query into two independent ones.First query makes select FOR UPDATE and selects all the taskinstances to act on (this will lock primary index only)and second updates all affected task instances.Note that performance impact for that is neglectable because thisquery is only run once every scheduler loop and the second partof it (looping through task instances) will only happen in casethere are some manually modified DagRun states - so it is onlyrun to correct some wrong states of DagRun. This should happenvery infrequently.",5
"[AIRFLOW-6489] Separate integrations in tests (#7091)You can now choose which integration you want to start when you runBreeze as well as when CI tests are run. Now by default Breezeand CI runs without integrations, but you can add them viaBreeze flags or by environment variables when CI is executed.We have pytest markers now that mark tests that can be run forintegration, backend and runtime selected.Also we have now more test jobs - we have separate test runfor all non-integration tests (with less memory used by theintegrations) and separate jobs that run integration testsonly (more memory used for integrations but far less numberof tests to run)",1
"Revert ""[AIRFLOW-1467] Dynamic pooling via allowing tasks to use more than one pool slot (depending upon the need) (#6975)""This reverts commit 277d01d4c57fe2470a8518c58bf0df9bf21e8f5e.",4
[AIRFLOW-6543][AIP-21] Promotion of contrib classes to the core (#7145)* [AIP-21] Move contrib.hooks.fs_hook hooks.filesystem* [AIP-21] Move contrib.sensors.weekday_sensor sensors.weekday_sensor* [AIP-21] Move contrib.sensors.file_sensor sensors.filesystem,5
[AIRFLOW-6107] [AIP-21] Rename GCP container operators (#7154),1
"[AIRFLOW-6534] - BigQuery - move methods from BigQueryBaseCursor to BigQueryHook (#7131)* [AIRFLOW-6534] Add backward relation, update tests* Move create_empty_table method, simplify BigQueryCreateEmptyTableOperator* Move create_empty_dataset method, simplify BigQueryCreateEmptyDatasetOperator* Move get_dataset_tables  method, simplify BigQueryGetDatasetTablesOperator* Move delete_dataset method, simplify BigQueryDeleteDatasetOperator* Move create_external_table method, simplify BigQueryCreateExternalTableOperator* Move patch_table method* Move insert_all method* Move update_dataset method, simplify BigQueryUpdateDatasetOperator* Move patch_dataset method, simplify BigQueryPatchDatasetOperator* Move get_dataset_tables_list method* Move get_datasets_list method* Move get_dataset method, simplify BigQueryGetDatasetOperator* Move run_grant_dataset_view_access* Move run_table_upsert method* Move run_table_delete method, simplify BigQueryDeleteTableOperator* Move get_tabledata method, simplify TestBigQueryGetDataOperator* Move get_schema method* Move poll_job_complete method* Move cancel_query method* Move run_with_configuration method* Move run_load method* Move run_copy method* Move run_extract method* Move run_query method, simplify BigQueryExecuteQueryOperator* fixup! Move run_query method, simplify BigQueryExecuteQueryOperator* fixup! fixup! Move run_query method, simplify BigQueryExecuteQueryOperator* fixup! fixup! fixup! Move run_query method, simplify BigQueryExecuteQueryOperator",1
[AIRFLOW-XXXX] Update task lifecycle diagram (#7161),5
"[AIRFLOW-6522] Clear task log file before starting to fix duplication in S3TaskHandler (#7120)The same task instance (including try number) can be run on a workerwhen using a sensor in ""reschedule"" mode. Accordingly, this clears thelocal log file when re-initializing the logger so that the old loglines aren't uploaded again when the logger is closed.",2
"[AIRFLOW-6510] Fix druid operator templating (#7127)Remove manual parsing of druid specification from druid operatorand replace it with template_fields, as in most operators.The reason for this change is because current implementation doesnot work well with relative paths, plus template_fields appears tobe the most common pattern in the project.",1
"[AIRFLOW-6146] [AIP-21] Rename GCS operators regarding GDrive, BigQuery and SFTP (#7147)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules* adde deprecation warnings to the contrib modules* fixed tests* updated UPDATING.md",5
"[AIRFLOW-XXXX] Temporarily disable Kerberos to get test stability (#7165)Kerberos causes various stability issues while CI tests are run,so we are disabling the Kerberos-only test so that we can addmore diagnostics information.",5
"[AIRFLOW-XXXX]: Set test env vars in confttest.py, not Breeze entrypoint (#7164)These environment variables need to be set beforecollection/file-parsing happens otherwise the test collection will failwith a KeyError.When running under breeze these were set in the entrypoint, but to makenon-breeze behave the same I have moved them out of there and to the toplevel of conftest.py.",5
[AIRFLOW-6566][AIRFLOW-4029] Replace uses of imp still left with importlib. (#7174),2
[AIRFLOW-6570] Add dag tag for all example dag (#7176)We miss add some dag tag inhttps://github.com/apache/airflow/pull/6489This patch to add dag tag for all others,2
[AIRFLOW-6565] BigQuery - replace deprecated connection parameters (#7173),2
[AIRFLOW-XXXX] Move UPDATING changes into correct versions (#7166),4
[AIRFLOW-6552] Move Azure classes to providers.microsoft package (#7158),1
[AIRFLOW-6573] bump `text-unidecode` version for Python 3.7 (#7179),5
[AIRFLOW-XXXX] Fix typo in tests/conftest.py (#7181),5
[AIRFLOW-XXXX] Fix typo in UPDATING.md (#7182),5
[AIRFLOW-XXXX] Sync Updating.md with v1.10.* branch (#7183),5
[AIRFLOW-6564] Additional diagnostics information on CI check failure (#7172),0
[AIRFLOW-6563] Add end_date for marked tasks (#7171),5
[AIRFLOW-6575] Entropy source for CI tests is changed to unblocking (#7185)On Travis CI blocking entropy source slows startup time of a number ofcontainers. This change changes the entropy source to unblocking one.,4
[AIRFLOW-6553] Add upstream_failed in instance state filter to WebUI (#7159),0
[AIRFLOW-6568] Add Emacs related files to .gitignore (#7175),2
[AIRFLOW-XXXX] Move email configuration from the concept page (#7189)* [AIRFLOW-XXXX] Move email configration from the concept page* fixup! [AIRFLOW-XXXX] Move email configration from the concept page,5
"[AIRFLOW-5501] Make default `in_cluster` value in KubernetesPodOperator respect config (#6124)The default value of the parameter in_cluster of thekube_client.get_kube_client function isin_cluster=conf.getboolean('kubernetes', 'in_cluster'). Therefore, theexpected behavior is that when, in_cluster is not set, it takes thevalue in the configuration file.However, the default value of in_cluster in KubernetesPodOperator.py isFalse and in_cluster is passed as a parameter when calling thekube_client.get_kube_client function. Therefore, it changes theexpecting behavior by overwritting the default value. When in_cluster isnot set when initializing KubernetesPodOperator, the value of in_clusterin kube_client.get_kube_client is False and not the value which is inthe configuration file.Therefore, the default value of in_cluster in KubernetesPodOperator hasbeen changed to None and will not be passed to get_kube_client if it isnot overwritten so that it takes the configuration value as a defaultvalue.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",5
"[AIRFLOW-3607] Only query DB once per DAG run for TriggerRuleDep (#4751)This decreases scheduler delay between tasks by about 20% for larger DAGs,sometimes more for larger or more complex DAGs.The delay between tasks can be a major issue, especially when we have dags with many subdags, figures out that the scheduling process spends plenty of time independency checking, we took the trigger rule dependency which calls the db foreach task instance, we made it call the db just once for each dag_run",2
[AIRFLOW-6572] Move AWS classes to providers.amazon.aws package (#7178)* [AIP-21] Move contrib.hooks.aws_glue_catalog_hook airflow.contrib.hooks.aws_glue_catalog_hook* [AIP-21] Move contrib.hooks.aws_logs_hook airflow.contrib.hooks.aws_logs_hook* [AIP-21] Move contrib.hooks.emr_hook airflow.contrib.hooks.emr_hook* [AIP-21] Move contrib.operators.ecs_operator airflow.contrib.operators.ecs_operator* [AIP-21] Move contrib.operators.emr_add_steps_operator airflow.contrib.operators.emr_add_steps_operator* [AIP-21] Move contrib.operators.emr_create_job_flow_operator airflow.contrib.operators.emr_create_job_flow_operator* [AIP-21] Move contrib.operators.emr_terminate_job_flow_operator airflow.contrib.operators.emr_terminate_job_flow_operator* [AIP-21] Move contrib.operators.s3_copy_object_operator airflow.contrib.operators.s3_copy_object_operator* [AIP-21] Move contrib.operators.s3_delete_objects_operator airflow.contrib.operators.s3_delete_objects_operator* [AIP-21] Move contrib.operators.s3_list_operator airflow.contrib.operators.s3_list_operator* [AIP-21] Move contrib.operators.sagemaker_base_operator airflow.contrib.operators.sagemaker_base_operator* [AIP-21] Move contrib.operators.sagemaker_endpoint_config_operator airflow.contrib.operators.sagemaker_endpoint_config_operator* [AIP-21] Move contrib.operators.sagemaker_endpoint_operator airflow.contrib.operators.sagemaker_endpoint_operator* [AIP-21] Move contrib.operators.sagemaker_model_operator airflow.contrib.operators.sagemaker_model_operator* [AIP-21] Move contrib.operators.sagemaker_training_operator airflow.contrib.operators.sagemaker_training_operator* [AIP-21] Move contrib.operators.sagemaker_transform_operator airflow.contrib.operators.sagemaker_transform_operator* [AIP-21] Move contrib.operators.sagemaker_tuning_operator airflow.contrib.operators.sagemaker_tuning_operator* [AIP-21] Move contrib.sensors.aws_glue_catalog_partition_sensor airflow.contrib.sensors.aws_glue_catalog_partition_sensor* [AIP-21] Move contrib.sensors.emr_base_sensor airflow.contrib.sensors.emr_base_sensor* [AIP-21] Move contrib.sensors.emr_job_flow_sensor airflow.contrib.sensors.emr_job_flow_sensor* [AIP-21] Move contrib.sensors.emr_step_sensor airflow.contrib.sensors.emr_step_sensor* [AIP-21] Move contrib.sensors.sagemaker_base_sensor airflow.contrib.sensors.sagemaker_base_sensor* [AIP-21] Move contrib.sensors.sagemaker_endpoint_sensor airflow.contrib.sensors.sagemaker_endpoint_sensor* [AIP-21] Move contrib.sensors.sagemaker_training_sensor airflow.contrib.sensors.sagemaker_training_sensor* [AIP-21] Move contrib.sensors.sagemaker_transform_sensor airflow.contrib.sensors.sagemaker_transform_sensor* [AIP-21] Move contrib.sensors.sagemaker_tuning_sensor airflow.contrib.sensors.sagemaker_tuning_sensor* [AIP-21] Move operators.s3_file_transform_operator airflow.operators.s3_file_transform_operator* [AIP-21] Move sensors.s3_key_sensor airflow.sensors.s3_key_sensor* [AIP-21] Move sensors.s3_prefix_sensor airflow.sensors.s3_prefix_sensor* [AIP-21] Move contrib.hooks.sagemaker_hook providers.amazon.aws.hooks.sagemaker,1
[AIRFLOW-6584] Pin cassandra driver (#7194)3.21.0 release of Cassandra driver(https://pypi.org/project/cassandra-driver/3.21.0/) broke backwardscompatibility. We need to pin it to 3.20.2,2
[AIRFLOW-XXXX] Add link for operators guide in first PR message (#7188),1
[AIRFLOW-XXXX] Adds branching strategy to documentation (#7193),2
[AIRFLOW-6557] Add test for newly added fields in BaseOperator (#7162)Adding new field in BaseOperator requires some manual updatesin serialization code. This test detects new fields and informswhat should be done in case new field is added.,1
[AIRFLOW-6473] Show conf in response of dag_state cli command (#7186),2
"[AIRFLOW-6541] Use EmrJobFlowSensor for other states (#7146)Remove class constants of EmrBaseSensor: NON_TERMINAL_STATES,FAILED_STATE. Add new parameters: target_states and failed_states.Eliminate pylint warnings. Do not change default behaviour of updatedsensors.",5
"[AIRFLOW-XXXX] Increease verbosity of static checks in CI (#7200)The latest version of pre-commit supports showing execution timesfor particular checks. This was a feature requested inhttps://github.com/pre-commit/pre-commit/issues/1144 and theyfinally implemented it after long time saying ""no"" :).This commit enables it with --verbose flag - which is also usefulas it shows hook ids and some extra information printed bysome plugins.",5
[AIRFLOW-6583] (BigQuery) Add query_params to templated_fields (#7198),2
[AIRFLOW-XXXX] Adjust celery defaults to work with breeze (#7205),1
[AIRFLOW-XXXX] Add rebase info to contributing (#7201),5
[AIRFLOW-XXXX] Add mentoring information to contributing docs (#7202),2
[AIRFLOW-6592] Doc build is moved to test phase (#7208),3
[AIRFLOW-6296] add OdbcHook & deprecation warning for pymssql (#6850),2
[AIRFLOW-6594] Raise an exception when the GCP connection is misconfigured (#7209),5
"[AIRFLOW-6576] Fix scheduler crash caused by deleted task with sla misses (#7187)When a task with SLA is deleted from a DAG after the SLA miss is loggedbut before the notification was sent, scheduler will crash with anAirflowException",5
[AIRFLOW-6596] Enforce description should not be empty (#7211),1
[AIRFLOW-6595] Use TaskNotFound exception instead of AirflowException (#7210),1
[AIRFLOW-1467] Allow tasks to use more than one pool slot (#7160),1
[AIRFLOW-6589] BAT tests run in pre-commit on bash script changes (#7203),4
[AIRFLOW-5520] Add options to run Dataflow in a virtual environment (#6590),5
[AIRFLOW-XXXX] Update committers list (#7212),5
[AIRFLOW-6597] Surface ODBC conn_type in Webserver UI Connection Form (#7214),5
[AIRFLOW-5912] Expose lineage API (#7138)Lineage data is exposed via the experimental api endpointper dag.,2
[AIRFLOW-6605] fix warnings from mypy-0.761 (#7222),2
[AIRFLOW-5816] Add S3 to snowflake operator (#6469)- move Snowflake from contrib to providers- fix pylint issuesCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
[AIRFLOW-6607] Get rid of old local scripts for Breeze (#7225)Breeze is now mature enought to remove old scripts that weredoing the same what Breeze does now with command line.,4
[AIRFLOW-XXXX] Screenshot showing disk space configuration for OSX (#7226)Might be useful for people who want to reconfigure their Dockeron OSX.,2
[AIRFLOW-6603] Remove unnecessary pylint warnings (#7224)There are unnecessary pylint warnings in ODBC that arespamming the error messages. We need to silence the.,0
[AIRFLOW-6145] [AIP-21] Rename GCS related operators and hooks (#7220)PR contains changes regarding AIP-21 (renaming GCP operators and hooks):* renamed GCP modules* adde deprecation warnings to the contrib modules* fixed tests* updated UPDATING.md,5
"Revert ""[AIRFLOW-6596] Enforce description should not be empty (#7211)"" (#7219)This reverts commit d553813b0c35f212b2fa10a49e6a542d23e5ee83.",4
[AIRFLOW-6610] Move software classes to providers package (#7231)* [AIP-21] Move contrib.hooks.mongo_hook providers.mongo.hooks.mongo* [AIP-21] Move contrib.hooks.openfaas_hook providers.openfass.hooks.openfaas* [AIP-21] Move contrib.hooks.redis_hook providers.redis.hooks.redis* [AIP-21] Move contrib.operators.docker_swarm_operator providers.docker.operators.docker_swarm* [AIP-21] Move contrib.operators.redis_publish_operator providers.redis.operators.redis_publish* [AIP-21] Move contrib.operators.kubernetes_pod_operator providers.cncf.kubernetes.operators.kubernetes_pod* [AIP-21] Move contrib.sensors.bash_sensor sensors.bash* [AIP-21] Move contrib.sensors.celery_queue_sensor providers.celery.sensors.celery_queue* [AIP-21] Move contrib.sensors.mongo_sensor providers.mongo.sensors.mongo* [AIP-21] Move contrib.sensors.python_sensor sensors.python* [AIP-21] Move contrib.sensors.redis_key_sensor providers.redis.sensors.redis_key* [AIP-21] Move contrib.sensors.redis_pub_sub_sensor providers.redis.sensors.redis_pub_sub* [AIP-21] Move hooks.docker_hook providers.docker.hooks.docker* [AIP-21] Move hooks.mssql_hook providers.microsoft.mssql.hooks.mssql* [AIP-21] Move hooks.mysql_hook providers.mysql.hooks.mysql* [AIP-21] Move hooks.oracle_hook providers.oracle.hooks.oracle* [AIP-21] Move hooks.postgres_hook providers.postgres.hooks.postgres* [AIP-21] Move hooks.presto_hook providers.presto.hooks.presto* [AIP-21] Move hooks.samba_hook providers.samba.hooks.samba* [AIP-21] Move hooks.sqlite_hook providers.sqlite.hooks.sqlite* [AIP-21] Move operators.bash_operator operators.bash* [AIP-21] Move operators.docker_operator providers.docker.operators.docker* [AIP-21] Move operators.mssql_operator providers.microsoft.mssql.operators.mssql* [AIP-21] Move operators.mysql_operator providers.mssql.operators.mysql* [AIP-21] Move operators.oracle_operator providers.oracle.operators.oracle* [AIP-21] Move operators.papermill_operator providers.papermill.operators.papermill* [AIP-21] Move operators.postgres_operator providers.postgres.operators.postgres* [AIP-21] Move operators.presto_check_operator providers.presto.operators.presto_check* [AIP-21] Move operators.python_operator operators.python* [AIP-21] Move operators.sqlite_operator providers.sqlite.operators.sqlite* Update docs,2
[AIRFLOW-6608] Change logging level for PythonOperator Env exports (#7229),1
[AIRFLOW-XXXX] Fix Typo in scripts/ci/ci_run_airflow_testing.sh (#7235),3
[AIRFLOW-6588] write_stdout and json_format are boolean (#7199),5
[AIRFLOW-XXXX] Add up-to-date checker to BoringCyborg config (#7239),5
[AIRFLOW-6615] Remove double sorted in task_list CLI (#7240),4
[AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038),5
[AIRFLOW-6611] Add proxy_fix configs to default_airflow.cfg (#7236),5
[AIRFLOW-XXXX] Updated MyTaxi company name and links  (#7233),2
[AIRFLOW-XXXX] Add Rapido to Airflow Users list (#7244),1
[AIRFLOW-6591] Add cli option to stop celery worker (#7206)Now users can gracefully stop the Celery worker by sending aSIGTERM signal to Celery main process.,1
[AIRFLOW-6619] Add cluster_fields to BigQueryCreateEmptyTableOperator (#7242),1
[AIRFLOW-6620] Mock celery in worker cli test (#7243)Actual tests start celery worker and doesn't stop it. Sometimesit result in timeout of a test.,3
[AIRFLOW-6608] Change logging level for PythonOperator Env exports (#7246),1
[AIRFLOW-4204] Update super() calls (#7248),5
[AIRFLOW-6493] Add SSL configuration to Redis hook connections (#7234),1
[AIRFLOW-6625] Explicitly log using utf-8 encoding (#7247)This is the standard encoding supported by Airflow. We should be explicit about logging usingthis encoding.,1
[AIRFLOW-6634] Set PYTHONPATH in interactive Breeze (#7254)Breeze did not have PYTHONPATH set in the interactive Breezeentry - which is different than in tests and makes it difficultto run tests outside of the main directory.,3
[AIRFLOW-XXXX] Fix url in new contributors message (#7255),1
[AIRFLOW-6635] Speed up static checks (#7256),5
[AIRFLOW-6638] Remove flakiness test from test_serialized_db remove (#7258)The test was failing randomly because it returned the DAGs in random order andsometimes it failed because test_serialized_dag was removing only one filefrom the list of files but the file could be repeated incase a dag file had more than one DAG defined.,2
[AIRFLOW-6639] Remove duplicate Output format choices from CLI docs (#7259),2
[AIRFLOW-6642] Make local task job test less flaky (#7262)The heartbeat failed fast test is flaky - sometimes the time needed to run thetest is longer than expected.,3
[AIRFLOW-6641] Better diagnostics for kubernetes flaky tests (#7261)Kubernetes tests are sometimes flaky - they do not reach desired state and wedo not know why. We need to make it less flaky and have enough diagnostics tounderstand what's going on.,1
[AIRFLOW-6643] Fix flakiness of kerberos tests (#7264)Kerberos test were still flaky because checking for kerberos token happensbefore active waiting for kerberos to be ready.,1
[AIRFLOW-XXXX] Fix typo in error for when getting data flow jobs (#7260),5
[AIRFLOW-6644][AIP-21] Move service classes to providers package (#7265)* [AIP-21] Move contrib.hooks.cloudant_hook providers.cloudant.hooks.cloudant* [AIP-21] Move contrib.hooks.databricks_hook providers.databricks.hooks.databricks* [AIP-21] Move contrib.hooks.datadog_hook providers.datadog.hooks.datadog* [AIP-21] Move contrib.hooks.dingding_hook providers.dingding.hooks.dingding* [AIP-21] Move contrib.hooks.discord_webhook_hook providers.ddiscord.hooks.discord_webhook* [AIP-21] Move contrib.hooks.gdrive_hook providers.google.suite.hooks.drive* [AIP-21] Move contrib.hooks.jenkins_hook providers.jenking.hooks.jenkins* [AIP-21] Move contrib.hooks.opsgenie_alert_hook providers.opsgenie.hooks.opsgenie_alert* [AIP-21] Move contrib.hooks.pagerduty_hook providers.pagerduty.hooks.pagerduty* [AIP-21] Move contrib.hooks.qubole_check_hook providers.qubole.hooks.qubole_check* [AIP-21] Move contrib.hooks.qubole_hook providers.qubole.hooks.qubole* [AIP-21] Move contrib.hooks.salesforce_hook providers.salesforce.hooks.salesforce* [AIP-21] Move contrib.hooks.segment_hook providers.segment.hooks.segment* [AIP-21] Move contrib.hooks.snowflake_hook providers.snowflake.hooks.snowflake* [AIP-21] Move contrib.hooks.vertica_hook providers.vertica.hooks.vertica* [AIP-21] Move gcp.hooks.gsheets providers.google.suite.hooks.sheets* [AIP-21] Move hooks.slack_hook providers.slack.hooks.slack* [AIP-21] Move hooks.zendesk_hook providers.zendesk.hooks.zendesk* [AIP-21] Move contrib.operators.databricks_operator providers.databricks.operators.databricks* [AIP-21] Move contrib.operators.dingding_operator providers.dindding.operators.dingding* [AIP-21] Move contrib.operators.discord_webhook_operator providers.discord.operators.discord_webhook* [AIP-21] Move contrib.operators.jenkins_job_trigger_operator providers.jenking.operators.jenkins_job_trigger* [AIP-21] Move contrib.operators.opsgenie_alert_operator providers.opsgenie.operators.opsgenie_alert* [AIP-21] Move contrib.operators.qubole_check_operator providers.qubole.operators.qubole_check* [AIP-21] Move contrib.operators.qubole_operator providers.qubole.operators.qubole* [AIP-21] Move contrib.operators.segment_track_event_operator providers.segment.operators.segment_track_event* [AIP-21] Move contrib.operators.slack_webhook_operator providers.slack.operators.slack_webhook* [AIP-21] Move contrib.operators.vertica_operator providers.vertica.operators.vertica* [AIP-21] Move contrib.sensors.datadog_sensor providers.datadog.sensors.datadog* [AIP-21] Move contrib.sensors.qubole_sensor providers.qubole.sensors.qubole* [AIP-21] Move operators.slack_operator providers.slack.operators.slack* Update docs,2
[AIRFLOW-XXXX] Improve docstring of AwsHook (#7263),1
"[AIRFLOW-2923][AIRFLOW-1784] Implement LatestOnlyOperator as BaseBranchOperator (#5970)LatestOnlyOperator is a special case of a BranchOperator, thus it should inherit from it.This fixes an issue where the skipping behaviour of LatestOnlyOperator is inconsistent with other operators,by forcefully skipping all downstream tasks recursively ignoring trigger rules.",1
[AIRFLOW-XXXX] Update installation doc to suggest installing globally as fallback (#7221),2
[AIRFLOW-6646][AIP-21] Move protocols classes to providers package (#7268)* [AIP-21] Move contrib.hooks.ftp_hook providers.ftp.hooks.ftp* [AIP-21] Move contrib.hooks.grpc_hook providers.grpc.hooks.grpc* [AIP-21] Move contrib.hooks.imap_hook providers.imap.hooks.imap* [AIP-21] Move contrib.hooks.ssh_hook providers.ssh.hooks.ssh* [AIP-21] Move contrib.hooks.winrm_hook providers.microsoft.winrm.hooks.winrm* [AIP-21] Move contrib.operators.grpc_operator providers.grpc.operators.grpc* [AIP-21] Move contrib.operators.ssh_operator providers.ssh.operators.ssh* [AIP-21] Move contrib.operators.winrm_operator providers.microsoft.winrm.operators.winrm* [AIP-21] Move contrib.sensors.imap_attachment_sensor providers.imap.sensors.imap_attachment* [AIP-21] Move hooks.http_hook providers.http.hooks.http* [AIP-21] Move hooks.jdbc_hook providers.jdbc.hooks.jdbc* [AIP-21] Move contrib.sensors.ftp_sensor providers.ftp.sensors.ftp* [AIP-21] Move operators.email_operator providers.email.operators.email* [AIP-21] Move operators.http_operator providers.http.operators.http* [AIP-21] Move operators.jdbc_operator providers.jdbc.operators.jdbc* [AIP-21] Move sensors.http_sensor providers.http.sensors.http* Update docs,2
[AIRFLOW-6451] self._print_stat() in dag_processing.py should be skippable (#7134),2
[AIRFLOW-6655] Move AWS classes to providers (#7271),1
[AIRFLOW-6654] AWS DataSync - bugfix when creating locations (#7270),1
[AIRFLOW-6627] Email with incorrect DAG not delivered (#7250),2
[AIRFLOW-6656] Fix AIP-21 moving (#7272),4
[AIRFLOW-6424] Added a operator to modify EMR cluster (#7213)* [AIRFLOW-6424] Added a operator to modify EMR cluster* [AIRFLOW-6424] Updated docs for EMR modify cluster operator* [AIRFLOW-6424] Removed sanity check,4
[AIRFLOW-6661] Fail after 50 failing tests (#7277)When we have a lot of failed tests due to environment problems the log offailure is too long and gets truncated so that we do not see the root cause. Weshould fail pytest after X failures (will try with x=50),1
[AIRFLOW-6660] Fix deprecation warning in check_environment.sh (#7275),2
"[AIRFLOW-6658] Remove contrib/{hooks,sensors} directory (#7273)* Remove tests/contrib/hooks* Remove tests/contrib/sensors",3
[AIRFLOW-6659] Move AWS Transfer operators to providers package (#7274)* [AIP-21] Move contrib.operators.dynamodb_to_s3 providers.amazon.aws.operators.dynamodb_to_s3* [AIP-21] Move contrib.operators.hive_to_dynamodb providers.amazon.aws.operators.hive_to_dynamodb* [AIP-21] Move contrib.operators.imap_attachment_to_s3_operator providers.amazon.aws.operators.imap_attachment_to_s3* [AIP-21] Move contrib.operators.mongo_to_s3 providers.amazon.aws.operators.mongo_to_s3* [AIP-21] Move contrib.operators.s3_to_sftp_operator providers.amazon.aws.operators.s3_to_sftp* [AIP-21] Move contrib.operators.sftp_to_s3_operator providers.amazon.aws.operators.sftp_to_s3* [AIP-21] Move operators.gcs_to_s3 providers.amazon.aws.operators.gcs_to_s3* [AIP-21] Move operators.google_api_to_s3_transfer providers.amazon.aws.operators.google_api_to_s3_transfer* [AIP-21] Move operators.redshift_to_s3_operator providers.amazon.aws.operators.redshift_to_s3* [AIP-21] Move operators.s3_to_redshift_operator providers.amazon.aws.operators.s3_to_redshift,1
[AIRFLOW-6670][depends on AIRFLOW-6669] Move contrib operators to providers package (#7286),1
[AIRFLOW-6662] Switch to --init docker flag for signal propagation (#7278)We are now using native --init flag of docker run and init: parameterof docker compose to pass signals and reap child processes,4
[AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291),2
[AIRFLOW-6677] Remove deprecation warning from SQLAlchmey (#7289),2
[AIRFLOW-6667] Resolve serialize-javascript advisory (#7282),0
[AIRFLOW-6632] Bump dagre-d3 to resolve lodash CVE advisory https://npmjs.com/advisories/577 (#7280),0
[AIRFLOW-XXXX] Update .mailmap with some missing authors (#7290),5
[AIRFLOW-6674] Move example_dags in accordance with AIP-21 (#7287),2
"[AIRFLOW-6682] Move GCP classes to providers package (#7295)* [AIP-21] Move gcp.hooks.automl providers.google.cloud.hooks.automl* [AIP-21] Move gcp.hooks.bigquery providers.google.cloud.hooks.bigquery* [AIP-21] Move gcp.hooks.bigquery_dts providers.google.cloud.hooks.bigquery_dts* [AIP-21] Move gcp.hooks.bigtable providers.google.cloud.hooks.bigtable* [AIP-21] Move gcp.hooks.cloud_build providers.google.cloud.hooks.cloud_build* [AIP-21] Move gcp.hooks.cloud_memorystore providers.google.cloud.hooks.cloud_memorystore* [AIP-21] Move gcp.hooks.cloud_sql providers.google.cloud.hooks.cloud_sql* [AIP-21] Move gcp.hooks.cloud_storage_transfer_service providers.google.cloud.hooks.cloud_storage_transfer_service* [AIP-21] Move gcp.hooks.compute providers.google.cloud.hooks.compute* [AIP-21] Move gcp.hooks.dataflow providers.google.cloud.hooks.dataflow* [AIP-21] Move gcp.hooks.datastore providers.google.cloud.hooks.datastore* [AIP-21] Move gcp.hooks.dlp providers.google.cloud.hooks.dlp* [AIP-21] Move gcp.hooks.functions providers.google.cloud.hooks.functions* [AIP-21] Move gcp.hooks.gcs providers.google.cloud.hooks.gcs* [AIP-21] Move gcp.hooks.kms providers.google.cloud.hooks.kms* [AIP-21] Move gcp.hooks.kubernetes_engine providers.google.cloud.hooks.kubernetes_engine* [AIP-21] Move gcp.hooks.mlengine providers.google.cloud.hooks.mlengine* [AIP-21] Move gcp.hooks.spanner providers.google.cloud.hooks.spanner* [AIP-21] Move gcp.hooks.speech_to_text providers.google.cloud.hooks.speech_to_text* [AIP-21] Move gcp.hooks.text_to_speech providers.google.cloud.hooks.text_to_speech* [AIP-21] Move gcp.hooks.translate providers.google.cloud.hooks.translate* [AIP-21] Move gcp.hooks.discovery_api providers.google.cloud.hooks.discovery_api* [AIP-21] Move gcp.hooks.video_intelligence providers.google.cloud.hooks.video_intelligence* [AIP-21] Move gcp.sensors.bigquery providers.google.cloud.sensors.bigquery* [AIP-21] Move gcp.sensors.bigquery_dts providers.google.cloud.sensors.bigquery_dts* [AIP-21] Move gcp.sensors.bigtable providers.google.cloud.sensors.bigtable* [AIP-21] Move gcp.sensors.cloud_storage_transfer_service providers.google.cloud.sensors.cloud_storage_transfer_service* [AIP-21] Move gcp.operators.automl providers.google.cloud.operators.automl* [AIP-21] Move gcp.operators.bigquery providers.google.cloud.operators.bigquery* [AIP-21] Move gcp.operators.bigquery_dts providers.google.cloud.operators.bigquery_dts* [AIP-21] Move gcp.operators.bigtable providers.google.cloud.operators.bigtable* [AIP-21] Move gcp.operators.cloud_build providers.google.cloud.operators.cloud_build* [AIP-21] Move gcp.operators.cloud_memorystore providers.google.cloud.operators.cloud_memorystore* [AIP-21] Move gcp.operators.cloud_sql providers.google.cloud.operators.cloud_sql* [AIP-21] Move gcp.operators.cloud_storage_transfer_service providers.google.cloud.operators.cloud_storage_transfer_service* [AIP-21] Move gcp.operators.compute providers.google.cloud.operators.compute* [AIP-21] Move gcp.operators.datastore providers.google.cloud.operators.datastore* [AIP-21] Move gcp.operators.dlp providers.google.cloud.operators.dlp* [AIP-21] Move gcp.operators.functions providers.google.cloud.operators.functions* [AIP-21] Move gcp.operators.gcs providers.google.cloud.operators.gcs* [AIP-21] Move gcp.operators.kubernetes_engine providers.google.cloud.operators.kubernetes_engine* [AIP-21] Move gcp.operators.mlengine providers.google.cloud.operators.mlengine* [AIP-21] Move gcp.operators.spanner providers.google.cloud.operators.spanner* [AIP-21] Move gcp.operators.speech_to_text providers.google.cloud.operators.speech_to_text* [AIP-21] Move gcp.operators.tasks providers.google.cloud.operators.tasks* [AIP-21] Move gcp.operators.text_to_speech providers.google.cloud.operators.text_to_speech* [AIP-21] Move gcp.operators.translate providers.google.cloud.operators.translate* [AIP-21] Move gcp.operators.translate_speech providers.google.cloud.operators.translate_speech* [AIP-21] Move gcp.operators.video_intelligence providers.google.cloud.operators.video_intelligence* [AIP-21] Move operators.adls_to_gcs providers.google.cloud.operators.adls_to_gcs* [AIP-21] Move operators.bigquery_to_bigquery providers.google.cloud.operators.bigquery_to_bigquery* [AIP-21] Move operators.bigquery_to_gcs providers.google.cloud.operators.bigquery_to_gcs* [AIP-21] Move operators.bigquery_to_mysql providers.google.cloud.operators.bigquery_to_mysql* [AIP-21] Move operators.cassandra_to_gcs providers.google.cloud.operators.cassandra_to_gcs* [AIP-21] Move operators.gcs_to_bq providers.google.cloud.operators.gcs_to_bigquery* [AIP-21] Move operators.gcs_to_gcs providers.google.cloud.operators.gcs_to_gcs* [AIP-21] Move operators.gcs_to_sftp providers.google.cloud.operators.gcs_to_sftp* [AIP-21] Move operators.local_to_gcs providers.google.cloud.operators.local_to_gcs* [AIP-21] Move operators.mssql_to_gcs providers.google.cloud.operators.mssql_to_gcs* [AIP-21] Move operators.mysql_to_gcs providers.google.cloud.operators.mysql_to_gcs* [AIP-21] Move operators.postgres_to_gcs providers.google.cloud.operators.postgres_to_gcs* [AIP-21] Move operators.sql_to_gcs providers.google.cloud.operators.sql_to_gcs* Update docs* [AIP-21] Move gcp.hooks.base providers.google.cloud.hooks.base* Move system tests to providers package* Fix deprecation message* Remove gcp.{operators,hooks,sensors}* Move gcp.example_dags to providers.google.cloud.example_dags* Move gcp.utils to providers.google.cloud.utils* Remove airflow.gcp* Fix import paths in tests* Remove reference to airflow.gcp* Fix utils path in test_mlengine_utils.py",3
[AIRFLOW-6687] Switch kubernetes tests to example_dags (#7299)* [AIRFLOW-6687] Switch kubernetes tests to example_dagsThe files were moved during AIP-21 and our tests do not runkubernetes tests for optimisations that's why it's not beennoticed,3
[AIRFLOW-6697] fix modal_backdrop z-index (#7313),4
[AIRFLOW-XXXX] Update GitLab team members (#7308)@tlapiana is no longer with the company and @m_walker is new on the team!,1
[AIRFLOW-XXXX] Fix email configuration link in CONTRIBUTING.rst (#7311),2
[AIRFLOW-6694] Fixed import to print_stuff in kubernetes tests (#7306)The import was missing helper.,2
[AIRFLOW-6698] Add shorthand notation for lineage (#7314)This adds shorthand notation to define dags that have lineagesupport. | for piping between operators and > and < for setting(static) lineage defintions.,1
[AIRFLOW-6705] Less chatty integration/backend checks (#7325),5
[AIRFLOW-6709] Fixed failing git sync (#7332),0
[AIRFLOW-6636] Avoid exceptions when printing task instance (#7257)Sometimes (mainly in tests) tasks do not have start/execution/end date set andprinting task instance information throws an exception which cover the realproblems and makes diagnostics difficult.,1
[AIRFLOW-XXXX] Consistency fixes in new documentation (#7207)Co-authored-by: efedotova <47426163+efedotova@users.noreply.github.com>,1
[AIRFLOW-6706] Lazy load operator extra links (#7327),2
[AIRFLOW-6626] Add email on failure or retry to default config (#7249),5
[AIRFLOW-6672] AWS DataSync - better logging of error message (#7288),0
[AIRFLOW-6705] One less chatty message at breeze initialisation (#7326)This message was still left preventing nice .... progress,5
[AIRFLOW-6666] Resolve js-yaml advisory (#7283),0
[AIRFLOW-5843] Add conf option to Add DAG Run view (#7281),1
[AIRFLOW-6258] Add CloudFormation operators to AWS providers (#6824),1
[AIRFLOW-6527] Make send_task_to_executor timeout configurable (#7143),5
[AIRFLOW-6703] The base_log_folder option moved to logging in ci (#7320)The option is raising deprecation warnings currently,2
[AIRFLOW-6701] Rat is downloaded from stable backup/mirrors (#7323)Also curl options are now using long format and include --failto protect against some temporary errors (5xx). Also RAT downloaduses now two possible sources of downloads and fallbacks to thesecond if first is not available.,1
"[AIRFLOW-6662] install dumb init (#7300)* Revert ""[AIRFLOW-6662] Switch to --init docker flag for signal propagation (#7278)""This reverts commit d1bf343ffec505270d4b2ee4b0e9fa5dbbedd891.* [AIRFLOW-6662] return back the dumb-init - installed by aptWe had stability problems with tests with --init flag so we aregoing back to it",5
[AIRFLOW-6612] Use builtin method to run flower instead of executing cmd (#7237)Instead of os.execvp to run flower command we can use flower'sexecute_from_commandline method.,1
[AIRFLOW-3349] Use None instead of False as value for encoding in StreamLogWriter (#7329),2
[AIRFLOW-6707] Simplify Connection.get_hook method (#7328)* [AIRFLOW-6707] Simplify Connection.get_hook method* fixup! [AIRFLOW-6707] Simplify Connection.get_hook method,1
[AIRFLOW-6694] Fixed import to print_stuff in kubernetes tests (#7306) (#7334),3
[AIRFLOW-6710] Lazy initialise executor modules (#7333),5
[AIRFLOW-6686] Fix syntax error constructing list of process ids (#7298)Construction of list of pids passed in kill command raises a Syntax error becauseordering of the arguments to `map` function doesn't conform to functionparameter definition.A list comprehension replaces the existing `map` function to make the codeforward compatible with Python 3 and at the same time expand to a list of pids.,1
[AIRFLOW-6702] Dumping kind logs to file.io. (#7319)* [AIRFLOW-6702] Dumping kind logs works nowKubernetes Kind logs are now always dumped to file.ioservice.,2
"[AIRFLOW-6699] Parameterize weekday sensor tests (#7316)Using the parameterized library, consolidate the true or ""happy path""tests, reducing overall code to maintain and showing each test case inone list of tuples.",3
"[AIRFLOW-6693] Make page titles distinguishable from another (#7305)* [AIRFLOW-6693] Make page titles distinguishable from anotherImprove navigation in the browser history and when having many openAirflow tabs in the browser. This patch includes the DAG name in thetitle (where applicable) and puts it in front so that tabs can betold apart even when having many tabs open. Likewise, if the index pageis filtered, the search term is added in front.* Incorporate review suggestions",1
[AIRFLOW-6692] Generate excluded_patterns in docs/conf.py (#7304),5
[AIRFLOW-6680] Last changes for AIP-21 (#7301)* Move postgres tests from test_operators to test_postgres* Move myqsl tests from test_operators to test_mysql* Move myqsl tests from test_operators to test_mysql* Move myqsl_to_hive tests from test_operators* Move GCP system tests to providers.google.cloud* Move PythonVirtualenvOperator to test_python* Move GenericTransfer tests to test_generic_transfer* Move MSSQL tests to providers.microsoft.mssql.operators* Remove duplicate test_to_gcs_to_s3* [AIP-21] Move operators.presto_to_mysql operators.mysql.operators.presto_to_mysql* Move Cassandra hook tests to test_cassandra* Improve strcutre of cassandra senssor testss* Move TestHiveCli to hooks.test_hive,3
[AIRFLOW-6708] Set unique logger names (#7330),2
[AIRFLOW-6715] Fix Google Cloud DLP Example DAG (#7337),2
[AIRFLOW-4364] Make all code in airflow/providers/amazon pylint compatible (#7336),1
[AIRFLOW-6613] center dag on graph view load (#7238),2
[AIRFLOW-XXXX] Add cross-reference between connection and fernet (#7190)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
[AIRFLOW-6714] Remove magic comments about UTF-8 (#7338),4
[AIRFLOW-6717] Remove non-existent field from templated_fields (#7340),4
[AIRFLOW-XXXX] Fix Static Checks on CI (#7342),0
[AIRFLOW-6718] Fix more occurrences of utils.dates.days_ago (#7341),5
[AIRFLOW-6716] Fix AWS Datasync Example DAG (#7339),2
[AIRFLOW-6720] Enforce alphabetical order for CONN_TYPE_TO_HOOK (#7344),1
"[AIRFLOW-4681] Make sensors module pylint compatible (#7309)Remove all references to sensor modules from pylinttodo.txt and beginmaking changes, including using local variables where class ones are notneeded. Where possible, clarify some local variables names, such as thesnakebite client in the HDFSSensor module.Ignore corresponding pylint checks for higher impact code.",1
[AIRFLOW-6691] Add tests that protect the deprecated packages (#7303)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,3
[AIRFLOW-6711] Drop plugin support for stat_name_handler (#7335),0
[AIRFLOW-6720] Detect missing tests for providers package (#7346),1
[AIRFLOW-6676] added GCSDeleteBucketOperator (#7307),4
[AIRFLOW-6683] REST API respects store_serialized_dag setting (#7296)Make REST API respect core.store_serialized_dags setting,1
[AIRFLOW-6725] Simplify chaining operation in DagFileProcessorManager (#7349),2
[AIRFLOW-XXXX] Move LatestOnlyOperator change to Master (#7350),4
[AIRFLOW-6678] Pull event logs from Kubernetes (#7292)Adds an option (defaults to True) to pull and log events from aKubernetes pod that fails.,0
[AIRFLOW-6630] Resolve handlebars advisory (#7284),0
[AIRFLOW-XXXX] Add scheduler in production section (#7351),1
[AIRFLOW-XXXX] Add pre-commit check for utf-8 file encoding (#7347)Note: From Python 3.x onwards the explicit utf-8 header is no longer required. It is utf-8 encoded by default.,1
[AIRFLOW-6727] Fix minor bugs in Release Management scripts (#7355),0
[AIRFLOW-XXXX] Add versions_added field to configs (#7354),5
[AIRFLOW-XXXX] Add note about  docs/autoapi_templates/index.rst file,2
"Revert ""[AIRFLOW-XXXX] Add note about  docs/autoapi_templates/index.rst file""This reverts commit fe6b17034fef83b92be87c77e0a18eeea264b6f9.",4
[AIRFLOW-XXXX] Add user and DAGs folder notes to BREEZE.rst (#7362)Co-authored-by: Matthew Bowden <bowdenm@spu.edu>,2
[AIRFLOW-XXXX] Add notes about airflow.providers and docs (#7360)Co-Authored-By: Jarek Potiuk <jarek.potiuk@polidea.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,2
"[AIRFLOW-XXXX] Remove ""Core and community package"" section (#7361)",4
[AIRFLOW-6736] Fix the repo/branch that is used in PRs for git sync (#7368)Kubernetes Git test uses always apache/airflow:master as source of filesThis made it impossible to test how the Kubernetes GitSync test will behavewhen merged. The git sync should always be done with the original repo/branchor from the TRAVIS_BRANCH if this is a push build,7
"[AIRFLOW-6739] Update tutorial.rst (#7369)Traceback from sqlite3.OperationalError: no such table: slot_pool when tutorial being followed on a new installation, include the initdb step",5
[AIRFLOW-XXXX] Fix location of kubernetes tests (#7373),3
"[AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359)This parameter is deprecated by werkzeug, see:https://github.com/pallets/werkzeug/blob/0.16.1/src/werkzeug/middleware/proxy_fix.py#L113-L120However, it is also broken. The value is acquired as string from theconfig, while it should be int like the other `x_*` attributes. Thosewere fixed in #6901, but `num_proxies` was forgotten.I think we can safely remove it because:* There is virtually no possibility that someone is using that parameter  in their config without raising an exception.* The configuration variable is not present in Airflow's docs or   anywhere else anymore. The removed line is the only trace of it.More details:https://issues.apache.org/jira/browse/AIRFLOW-6740",0
[AIRFLOW-6738] Upload container diagnostics always (#7372),5
"[AIRFLOW-6733] Extend not replace template (#7366)* [AIRFLOW-6733] Extend, rather than replace, the base_templateThis commit just moves the existing template to airflow/master.html(without further changes)* [AIRFLOW-6733] Only change blocks we have customizedThis makes is easier to see which parts of the template we have changed.",4
[AIRFLOW-6734] Use configured base_template instead of hard-coding (#7367)Flask/FAB gives us a `base_template` variable that we should use insteadof hard-coding a specific template to extend.,1
[AIRFLOW-6695] Can now pass dagrun conf when triggering dags via UI (#7312),2
[AIRFLOW-6737] Enable kubernetes diagnostics for all kubernetes tests (#7371)The logs are now sent in the bash scripts when all tests are complete,3
[AIRFLOW-6683] Run REST API tests when DAGs are serialized (#7352),2
[AIRFLOW-6751] Pin Werkzeug < 1.0.0 release - 1.0.0 is not compatible (#7377),5
[AIRFLOW-6755] Fix snowflake hook bug and tests (#7380),3
[AIRFLOW-6756] Drop also deprecated tables in reset (#7381),1
[AIRFLOW-XXXX] Update autolabeler config (#7379),5
[AIRFLOW-6728] Change various DAG info methods to POST (#7364)If the number of dags was large and/or the length of the DAG ids were too large this would exceed the maximum possible query string limit.To work around that we have made these endpoints always make POST requests,1
[AIRFLOW-XXXX] Add section for 1.10.8 in Updating.md (#7384),5
[AIRFLOW-XXXX] Add Changelog for 1.10.8 (#7383),4
[AIRFLOW-XXXX] Add Changelog & Updating.md section for 1.10.9 (#7385),5
"[AIRFLOW-6761] Fix WorkGroup param in AWSAthenaHook (#7386)Unknown parameter in input: ""Workgroup"", must be one of: QueryString, ClientRequestToken, QueryExecutionContext, ResultConfiguration, WorkGroup",1
[AIRFLOW-6758] Skip git version retrieval in case of invalid git (#7382)This happens when you have shared clone of the repository,5
"[AIRFLOW-6762] Fix link to ""Suggest changes on this page"" (#7387)",4
[AIRFLOW-XXXX] Fix typo commiter => committer (#7392)https://github.com/apache/airflow-site/pull/247,2
[AIRFLOW-XXXX] Add explicit info about JIRAs for code-related PRs (#7318)* [AIRFLOW-XXXX] Add explicit info about JIRAs for code-related PRs* fixup! [AIRFLOW-XXXX] Add explicit info about JIRAs for code-related PRsCo-Authored-By: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
"[AIRFLOW-6766] Fix ""cannot import ensure_text"" error for pre-commit (#7393)* [AIRFLOW-6766] Fix ""cannot import ensure_text"" error for pre-commitAs of today Travis bundles six version 1.11.0 with their python3.6 image and it misses ensure_text method. Bumping to 1.14+solves the problem.",0
[AIRFLOW-6764] Fixed environment installation for Linux (#7390),0
[AIRFLOW-6767] Correct name for default Athena workgroup (#7394)* Change default wg name* Change workgroup in tests,3
[AIRFLOW-5176] Add Azure Data Explorer (Kusto) operator (#5785),1
[AIRFLOW-6770] Run particular test using breeze CLI bug fix (#7396)* [AIRFLOW-6770] Run particular test using breeze CLI bug fix* [AIRFLOW-6770] Fix typo in travis config* [AIRFLOW-6770] Fix variable name and remove unnecessary travis command,4
[AIRFLOW-5413] Allow K8S worker pod to be configured from JSON/YAML file (#6230)* [AIRFLOW-5413] enable pod config from file* Update airflow/kubernetes/pod_generator.pyCo-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
[AIRFLOW-6065] Add Stackdriver Task Handler (#6660),0
[AIRFLOW-5231] Fix S3Hook.delete_objects method (#7375),4
[AIRFLOW-5590] Add run_id to trigger DAG run API response (#6256),1
[AIRFLOW-6781] Enforce example_dags and guide for Google integrations (#7404),2
[AIRFLOW-6505] Let emoji encoded properly for json.dumps() (#7399),5
"[AIRFLOW-6344] Fix travis CI for tag builds (#7411)Don't try to find changed files unless we are building a pull request.This only caused a problem on build of tags, but we were also doing thisfor master/branch builds, but it was always saying finding no fileschanged.By checking this early we can make the other conditions in this functionsimpler.",1
[AIRFLOW-2906] Add support for DataDog's dogstatsd when emitting metrics (#7376),5
[AIRFLOW-XXXX] Add ShopBack as an Airflow user (#7418),1
"[AIRFLOW-6590] Use batch db operations in jobs (#7370)* [AIRFLOW-6590] Use batch db operations in jobsThe PR changes numerous single selects / updates in base,scheduler, and backfill jobs to bulk operations.* fixup! [AIRFLOW-6590] Use batch db operations in jobs* fixup! fixup! [AIRFLOW-6590] Use batch db operations in jobs",5
[AIRFLOW-6800] Close file object after parsing ssh config (#7415),5
[AIRFLOW-6802] Fix bug where dag.max_active_run wasn't always honored by scheduler (#7416)commit 50efda5c69c1ddfaa869b408540182fb19f1a286 introduced a bug thatprevents the scheduler from enforcing max active run config for allDAGs.this commit fixes the regression as well as the test.,3
"[AIRFLOW-6531] Initial Yandex.Cloud Dataproc support (#7252)* [AIRFLOW-6531] Initial Yandex.Cloud Dataproc support* [AIRFLOW-6531] Move from contrib to providers. Drop py2 support* [AIRFLOW-6531] Add service account support to YC connection* [AIRFLOW-6531] Use Dataproc wrapper* [AIRFLOW-6531] Move base classes to init to test ""test_providers_modules_should_have_tests""* [AIRFLOW-6531] Add type annotations. Remove op base class* [AIRFLOW-6531] Add cluster id to templated fields",1
[AIRFLOW-3607] fix scheduler bug related to concurrency and depends on past (#7402)commit 50efda5c69c1ddfaa869b408540182fb19f1a286 introduced a bug thatprevents scheduler from scheduling tasks with the following properties:* has depends on past set to True* has custom concurrency limit,1
"[AIRFLOW-XXXX] Speed up mypy runs. (#7421)This PR does two things:1. It enables the mypy cache (default folder name .mypy_cache) so that   repeated runs locally are quicker2. It _disables_ passing only the changed files in.Point 2 seems counter-intuitave, but in my testing running with allfiles (airflow docs tests) was about twice as fast as without. Myhypothesis for why this happens is that when mypy is checking file x, ithas to check dependencies/imports for it too, and when we havepass_filenames set runs multiple processes in parallel, and each of themhave to do this work!Timings before and after:- Before:  For all files  ```  ❯ time pre-commit run mypy -a  Run mypy.................................................................Passed  pre-commit run mypy -a  0.31s user 0.07s system 2% cpu 17.140 total  ```  With only a single file  ```  ❯ time pre-commit run mypy --files airflow/configuration.py  Run mypy.................................................................Passed  pre-commit run mypy --files airflow/configuration.py  0.30s user 0.07s system 5% cpu 6.724 total  ```- After:  With a clean cache (`rm -rf .mypy_cache`):  ```  $ time pre-commit run mypy  Run mypy.................................................................Passed  pre-commit run mypy -a  0.26s user 0.10s system 2% cpu 17.226 total  ```  Clean cache with single file:  ```  $ time pre-commit run mypy  --file airflow/version.py  Run mypy.................................................................Passed  pre-commit run mypy --file airflow/version.py  0.23s user 0.07s system 4% cpu 7.091 total  ```  Repeated run (cache folder exists):  ```  $ time pre-commit run mypy  --file airflow/version.py  Run mypy.................................................................Passed  pre-commit run mypy --file airflow/version.py  0.23s user 0.05s system 6% cpu 4.178 total  ```  and for all files  ```  airflow ❯ time pre-commit run mypy  -a  Run mypy.................................................................Passed  pre-commit run mypy -a  0.25s user 0.09s system 6% cpu 4.833 total  ```",5
"[AIRFLOW-XXXX] Prevent Docker cache-busting on when editing www templates (#7427)There is two parts to this PR:1. Only copying www/webpack.config.js and www/static/ before running the   asset pipeline2. Making sure that _all_ files (not just the critical ones) have the   same permissions.The goal of both of these is to make sure that the docker build cache for the ""expensive""operations (installing NPM modules, running asset pipeline, installing python modules)isn't run when it isn't necessary.",1
"Revert ""[AIRFLOW-XXXX] Prevent Docker cache-busting on when editing www templates (#7427)""This reverts commit 3eb30ed12ced9b776e2588f694d90412071f41f0.",4
"[AIRFLOW-6765] Add CLI ""airflow dags test"" command (#7426)* Add DAG return type* Add airflow dag test CLI command for running a complete DAG with CLI* Added new command note to updating.md* Add airflow dags test example to testing.rst* Fix rst inline code mistake",0
[AIRFLOW-6818] Prevent Docker cache-busting on when editing www templates (#7432)There is two parts to this PR:1. Only copying www/webpack.config.js and www/static/ before running the   asset pipeline2. Making sure that _all_ files (not just the critical ones) have the   same permissions.,2
[AIRFLOW-XXXX] Add Geekie to Airflow Users list (#7434),1
[AIRFLOW-6816] Simplified common functions in breeze scripts (#7430),1
[AIRFLOW-6801] Make use of ImportError.timestamp (#7425),2
[AIRFLOW-6405] Add GCP BigQuery Table Upsert Operator (#7126)* [AIRFLOW-6405] Add GCP BigQuery Table Property Upsert Operator* [AIRFLOW-6405] Remove unnecessary checks from BQ hook,1
[AIRFLOW-6759] Added MLEngine operator/hook to cancel MLEngine jobs (#7400)* [AIRFLOW-6759] Added MLEngine operator/hook to cancel MLEngine jobs* Update airflow/providers/google/cloud/hooks/mlengine.pyAdded types for `job_id`Co-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Updates cancel_job doc* Update airflow/providers/google/cloud/hooks/mlengine.pycleaner formatingCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* removed redundant error checkingCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,0
[AIRFLOW-6818] Fix for old versions of git on Dockerhub builds (#7440)Dockerhub uses an old version of git (2.7.4 as of our testing in Feb2020) so needs a slightly different syntax for git ls-files with anexclusion. This works with current-latest git (2.25.0) too,3
"[AIRFLOW-XXXX] Less verbose docker builds`-t` to xargs makes it echo the command it runs, we don't really needthat and it's overly verbose in our build logs",2
[AIRFLOW-XXXX] Typo in example_bigquery DAG (#7429)[AIRFLOW-XXXX] Typo in example_bigquery DAG,2
[AIRFLOW-XXXX] Fix breeze build-docs (#7445),2
[AIRFLOW-XXXX] Add known issue  - example_dags/__init__.py (#7444),5
[AIRFLOW-6792] Remove _operator/_hook/_sensor in providers package and add tests (#7412),3
[AIRFLOW-6804] Add the basic test for all example DAGs (#7419)Co-Authored-By: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,2
[AIRFLOW-XXXX] - add communication chapter to contributing (#7204),1
[AIRFLOW-6793] Respect env variable in airflow config command (#7413),5
[AIRFLOW-6823] Fix root owned files automatically in Breeze (#7449),2
[AIRFLOW-XXXX] Change title and add some more helpful information (#7455),5
[AIRFLOW-1202] Create Elasticsearch Hook (#7358),1
[AIRFLOW-6820] split breeze into functions (#7433),1
[AIRFLOW-6820] fixup! fix unbound variable (#7459),0
[AIRFLOW-5470] Add Apache Livy REST operator (#6090),1
[AIRFLOW-6828] Stop using the zope library (#7448),1
[AIRFLOW-XXXX] Remove travis config warnings (#7467)Building on my fork on travis-ci.com I noticed that there is a newconfig validation feature and these two things were giving warnings,2
[AIRFLOW-6841] Fixed unbounded variable on Mac (#7465),0
"[AIRFLOW-6839] even more mypy speed improvements (#7460)Require_serial:true is better choice than pass_filename: false as it canspeed-up mypy for single file changes.Significant gains can be achieved for single file changes and no cache for allother files. This is majority of cases for our users who have pre-commitsinstalled as hooks because most people change only few files and never runcheck with --all-filesWhen just one file is changed and no cache is built, the difference is drastic:require_serial: true = 4spass_filenames: false =  13s",4
"[AIRFLOW-6721] Organize Apache Hive tests (#7468)* [AIRFLOW-6721] Move WebHdfsSensor tests to own moduleMove the WebHdfsSensor tests out of the operator tests and into aseparate module.* [AIRFLOW-6721] Move TestHiveEnvironment to test moduleMovee the TestHiveEnvironment class and setUp method out to the hivetest module so that it can be used in other hive tests.* [AIRFLOW-6721] Move Hive Stats tests to separate moduleMove skipped test for HiveStatsCollectionOperator to the test_hive_statsmodule to make it easier to locate.* [AIRFLOW-6721] Move NamedHivePartitionSensor testsMove the NamedHivePartitionSensor tests from the operators module intothe test module for NamedHivePartitionSensor.* [AIRFLOW-6721] Move HivePartitionSensor testsMove the HivePartitionSensor tests from the operators module intothe a new test module.* [AIRFLOW-6721] Move HiveToMySqlTransfer testMove the HiveToMySqlTransfer test in the operators module intothe existing test module and use the shared test environment forthese tests.* [AIRFLOW-6721] Move HdfsSensor test to separate moduleMove the HdfsSensor test in the operators module into a new testmodule and use the shared test environment.* [AIRFLOW-6721] Move Hive2SambaOperator testMove the Hive2SambaOperator test in the operators module into theexisting test module and use the shared test environment.* [AIRFLOW-6721] Move Hive MetastorePartitionSensor testMove the MetastorePartitionSensor test from the operators moduleinto a new module and use the shared test environment.* [AIRFLOW-6721] Move Hive PrestoToMySqlTransfer testMove the PrestoToMySqlTransfer test from the operators moduleinto an existing test module and use shared Hive test environment.* [AIRFLOW-6721] Move PrestoCheckOperator testMove the PrestoCheckOperator test from the Hive operators moduleinto a new Presto operators test module while still using theshared Hive test environment.* [AIRFLOW-6721] Move Hive SqlSensor testMove the SqlSensor in the Hive operators module into the existingSqlSensor test module and label it as such. Use the shared testenvironment for all tests, ensuring that original setUp propertiesare preserved for the existing tests.",3
[AIRFLOW-6842] Skip fixing ownership on Mac (#7469),0
[AIRFLOW-6558] Campaign Manager operators for conversions (#7420),1
[AIRFLOW-6821] Success callback not called when task marked as success from UI (#7447),5
"[AIRFLOW-5391] Do not re-run skipped tasks when they are cleared (#7276)If a task is skipped by BranchPythonOperator, BaseBranchOperator or ShortCircuitOperator and the user then clears the skipped task later, it'll execute. This is probably not the rightbehaviour.This commit changes that so it will be skipped again. This can be ignored by running the task again with ""Ignore Task Deps"" override.",1
[AIRFLOW-6854] Fix missing typing_extensions on python 3.8 (#7474)All import from `typing_extensions` should be done through`airflow.typing_compat` to avoid breakage under python >= 3.8,4
[AIRFLOW-6346] Enhance dag default_view and orientation (#6900)* Only DEFAULT_VIEW_PRESETS are acceptable* Only ORIENTATION_PRESETS are acceptable* Use webserver.dag_default_view in airflow.cfg as default value* Remove DAG and DagModel function get_default_view,1
[AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324),2
[AIRFLOW-XXXX] Add instructions for logging to localstack S3 (#7461),2
[AIRFLOW-6861] Remove pool full on TaskInstance (#7480),4
[AIRFLOW-XXXX] Document new session handling guidelines (#7483),1
[AIRFLOW-6866] Fix wrong export for Mac on Breeze (#7485),0
[AIRFLOW-6763] Make systems tests ready for backport tests (#7389)We will run system test on back-ported operators for 1.10* series of airflowand for that we need to have support for running system tests using pytest'smarkers and reading environment variables passed from HOST machine (to passcredentials). This is the first step to automate system tests execution.,3
[AIRFLOW-XXXX] Small BREEZE.rst update (#7487),5
[AIRFLOW-4973] Add Cloud Data Fusion Pipeline integration (#7486),5
[AIRFLOW-6863] Make airflow/task and airflow/ti_deps pylint compatible (#7482),1
[AIRFLOW-6858] Decouple DagBag and Executor (#7479),2
[AIRFLOW-6867] Decouple DagBag and TaskInstance (#7488),2
[AIRFLOW-XXXX] Add architecture to k8sexec docs (#7406)Adds an architecture section the KubernetesExecutor documentation,2
[AIRFLOW-6850] Handle no access to code in UI (#7472),0
"[AIRFLOW-XXXX] Clarify dag_id parameter docstring (#7463)I spent a while chasing this just now, wondering whether `:` is valid before finding `validate_key()` in helpers.I think it's helpful to surface this here directly.",5
[AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457),2
[AIRFLOW-XXXX] Update tests info in CONTRIBUTING.rst (#7466),5
[AIRFLOW-XXXX] Remove duplication in BaseOperator docstring (#7321),2
[AIRFLOW-XXXX] Remove redundant wording in docstrings (#7497),2
[AIRFLOW-6790] Add basic Tableau Integration (#7410),1
[AIRFLOW-XXXX] Remove duplicate line from breeze (#7491),4
"[AIRFLOW-6817] remove imports from `airflow/__init__.py`, replaced implicit imports with explicit imports, added entry to `UPDATING.MD` - squashed/rebased (#7456)",5
[AIRFLOW-XXXX] Remove unused local variable value (#7496),1
[AIRFLOW-6872] Fix: Show Git Version in UI (#7493),0
[AIRFLOW-6879] Fix Failing CI: Update New import paths (#7500),2
[AIRFLOW-XXXX] Fix grammar in UPDATING.md (#7509),5
"[AIRFLOW-4030] second attempt to add singularity to airflow (#7191)* adding singularity operator and testsSigned-off-by: Vanessa Sochat <vsochat@stanford.edu>* removing encoding pragmas and fixing up dockerfile to pass lintingSigned-off-by: Vanessa Sochat <vsochat@stanford.edu>* make workdir in /tmp because AIRFLOW_SOURCES not defined yetSigned-off-by: Vanessa Sochat <vsochat@stanford.edu>* curl needs to follow redirects with -LSigned-off-by: Vanessa Sochat <vsochat@stanford.edu>* moving files to where they are supposed to be, more changes to mock, no clueSigned-off-by: vsoch <vsochat@stanford.edu>* removing trailing whitespace, moving example_dag for singularity, adding licenses to empty init filesSigned-off-by: vsoch <vsochat@stanford.edu>* ran isort on example dags fileSigned-off-by: vsoch <vsochat@stanford.edu>* adding missing init in example_dags folder for singularitySigned-off-by: vsoch <vsochat@stanford.edu>* removing code from __init__.py files for singularity operator to fix documentation generationSigned-off-by: vsoch <vsochat@stanford.edu>* forgot to update link to singularity in operators and hooks refSigned-off-by: vsoch <vsochat@stanford.edu>* command must have been provided on init of singularity operator instanceSigned-off-by: vsoch <vsochat@stanford.edu>* I guess I'm required to have a task_id?Signed-off-by: vsoch <vsochat@stanford.edu>* try adding working_dir to singularity operator type definitionsSigned-off-by: vsoch <vsochat@stanford.edu>* disable too many arguments for pylint of singularity operator initSigned-off-by: vsoch <vsochat@stanford.edu>* move pylint disable up to line 64 - doesnt catch at end of statement like other examplesSigned-off-by: vsoch <vsochat@stanford.edu>* two spaces before inline commentSigned-off-by: vsoch <vsochat@stanford.edu>* I dont see task_id as a param for other providers, removing for singularity operatorSigned-off-by: vsoch <vsochat@stanford.edu>* adding debug printSigned-off-by: vsoch <vsochat@stanford.edu>* allow for return of just image and/or linesSigned-off-by: vsoch <vsochat@stanford.edu>* dont understand how mock works, but the image should exist after its pulled....Signed-off-by: vsoch <vsochat@stanford.edu>* try removing shutil, the client should handle pull folder insteadSigned-off-by: vsoch <vsochat@stanford.edu>* try changing pull-file to same uri that is expected to be pulledSigned-off-by: vsoch <vsochat@stanford.edu>* import of AirflowException moved to exceptionsSigned-off-by: vsoch <vsochat@stanford.edu>* DAG module was moved to airflow.modelsSigned-off-by: vsoch <vsochat@stanford.edu>* ensure pull is called with pull_folderSigned-off-by: vsoch <vsochat@stanford.edu>",4
[AIRFLOW-6888] Replace List creation in experimental/trigger_dag.py (#7511),2
[AIRFLOW-6889] Change mutable argument value in OpsgenieAlertHook (#7512),1
[AIRFLOW-6890] AzureDataLakeHook: Move DB call out of __init__ (#7513),5
[AIRFLOW-6892] Fix broken non-wheel releases (#7514),0
[AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123)* [AIRFLOW-6062] Executor would only delete pods in its own namespace* add tests* clean up PR* static tests* Update airflow/executors/kubernetes_executor.pyCo-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>* Update airflow/executors/kubernetes_executor.pyCo-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>* Update airflow/executors/kubernetes_executor.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
[AIRFLOW-6894] Prevent db query in example_dags (#7516),2
[AIRFLOW-5629] Implement Kubernetes priorityClassName in KubernetesPodOperator (#7395),1
[AIRFLOW-6663] Prepare backporting packages (#7391),5
[AIRFLOW-6204] Create GoogleSystemTest class (#7439)fixup! [AIRFLOW-6204] Create GoogleSystemTest class,5
[AIRFLOW-6628] DAG auto-complete now suggests from all acessible DAGs (#7251)The auto complete on the dag search box utilises the page search contextwhich prevents auto completing dags which dont match the current query.On the dags page with no search provided it executes a query that loadsevery dag_id in the system to pass to the typeahead widget. On systemswith a lot of dags this makes the page large and slow.Add a JSON endpoint to provide correct auto complete behaviour andreduce page load time.,1
[AIRFLOW-6474] `list_dag_runs` CLI can now filter by execution date range (#7446),5
[AIRFLOW-6830] Add Subject/MessageAttributes to SNS hook and operator (#7451),1
"[AIRFLOW-6817] Lazy-load `airflow.DAG` to keep user-facing API untouched (#7517)`from airflow import DAG` is _such_ a common pattern that it's worthmaking sure this stays working.Since we are now on Python 3 we can use PEP-562 to have officiallysupported lazy-loading of attributes on a module.Since the example_dags will be referred to/copied by users I haveupdated them all back to import from the airflow module, but have leftany internal uses untouched.",1
[AIRFLOW-6897] Simplify DagFileProcessorManager (#7521),2
[AIRFLOW-XXXX] correct path to deploy_airflow_to_kubernetes.sh in TESTING.rst (#7522),3
[AIRFLOW-6516] Allow different configmaps for airflow.cfg & airflow_l… (#7518)* [AIRFLOW-6516] Allow different configmaps for airflow.cfg & airflow_local_settings.py (#7109)* Update test_worker_configuration.py,5
[AIRFLOW-6895] AzureFileShareHook: Move DB call out of __init__ (#7519)* [AIRFLOW-6895] AzureFileShareHook: Move DB call out of __init__* Mock client* Fix test,3
[AIRFLOW-6896] AzureCosmosDBHook: Move DB call out of __init__ (#7520)* [AIRFLOW-6896] AzureCosmosDBHook: Move DB call out of __init__* Fix tests* fixup* Fix test,3
[AIRFLOW-6873] fix mypyd errors and add new type annotations (#7495),1
[AIRFLOW-6838] Introduce real subcommands for Breeze (#7515)This change introduces sub-commands in breeze tool.It is much needed as we have many commands nowand it was difficult to separate commands from flags.Also --help output was very long and unreadable.With this change help it is much easier to discoverwhat breeze can do for you as well as navigate with it.Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-6905] Update pin.svg with new pinwheel (#7524),1
[AIRFLOW-6906] Fix flaky tests in kubernetes tests (#7525),3
[AIRFLOW-6593] Add GCP Stackdriver Alerting Hooks and Operators (#7322),1
[AIRFLOW-6908] Lazy load AirflowException (#7528),5
[AIRFLOW-XXXX] Update docs on starting Kubernetes tests (#7530),3
[AIRFLOW-6909] Prepare backport packages on post-test stage (#7529),3
[AIRFLOW-5924] Automatically unify bucket name and key in S3Hook (#6574)- change provide_bucket_name to provide bucket name also for function with keys- refactoring,4
[AIRFLOW-6910] Fix kill_zombie method call (#7531),0
[AIRFLOW-6907] Simplify SchedulerJob (#7527),5
[AIRFLOW-XXXX] Fix outdated doc on settings.policy (#7532),1
[AIRFLOW-6864] Make airfow/jobs pylint compatible (#7484)fixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! fixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatiblefixup! [AIRFLOW-6864] Make airfow/jobs pylint compatible,1
[AIRFLOW-XXXX] Add instructions on using template_searchpath (#7540),1
[AIRFLOW-6824] EMRAddStepsOperator problem with multi-step XCom (#7443),0
[AIRFLOW-6382] Add reason for pre-commit rule (#7538),1
"[AIRFLOW-6919] Make Breeze DAG-test friedly (#7539)Originally Breeze was used to run unit and integration tests, recently systemtests and finally we make it a bit more friendly to test  your DAGs there. Youcan now install any older airflow version in Breeze via--install-airflow-version switch and ""files/dags"" folder is mounted to""/files/dags"" and this folder is used to read the dags from.",2
[AIRFLOW-6887] Do not check the state of fresh DAGRun (#7510),2
[AIRFLOW-XXXX] Prohibit non-finished PR (#7543),5
[AIRFLOW-XXXX] Add more .mailmap entries (#7545),1
[AIRFLOW-XXXX] Block PR Merge if it has DONT-MERGE in title (#7550),7
[AIRFLOW-XXXX] Fix broken static check failure on CI (#7551),0
[AIRFLOW-6843] Add delete_option_kwargs to delete_namespaced_pod (#7523),4
[AIRFLOW-XXXX] Fix typo in BREEZE.rst (#7554),2
[AIRFLOW-XXXX] Fix broken static check for BREEZE.rst (#7555),0
"[AIRFLOW-5354] Reduce scheduler CPU usage from 100% (#7552)A previous change ended up with the scheduler busily checking if theDagFDagFileProcessorAgent had collected any dags. This simple changetakes the CPU usage of the scheduler from an entire core, to barelyanything (dropped below 5%).Time for 10 dag runs of 9 dags with 108 total tasks: 50.3581s (±9.538s)vs master of Time for 10 dag runs of 9 dags with 108 total tasks: 49.6910s (±7.193s)The change is is basically no overall change, and is a quick fix fornow, and bigger changes are in store around DAG parsing anyway.",2
[AIRFLOW-XXXX] Fix typo in BREEZE.rst (#7556)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
[AIRFLOW-XXXX] Fix typo in BREEZE.rst (#7558),2
[AIRFLOW-6942] Use tabulate to display DagBag Report (#7566),2
"[AIRFLOW-6454] And script to benchmark scheduler dag-run time (#7537)This script will run the SchedulerJob for the specified dags ""to completion"".That is it creates a fixed number of DAG runs for the specified DAGs (fromthe configured dag path/example dags etc), disable the scheduler fromcreating more, and then monitor them for completion. When the file task ofthe final dag run is completed the scheduler will be terminated.The aim of this script is to have a benchmark for real-world schedulerperformance -- i.e. total time take to run N dag runs to completion.It is recommended to repeat the test at least 3 times so that you can getsomewhat-accurate variance on the reported timing numbers.",1
"[AIRFLOW-6932] Add restart-environment command to Breeze (#7557)When you switch between versions of Aiflow installed, you want to delete thedatabase so that the scripts for resetdb work",1
[AIRFLOW-6936] Only register signal hanlders when ScheduleJob is started (#7560),5
[AIRFLOW-5659] Add support for ephemeral storage on KubernetesPodOperator (#6337),1
[AIRFLOW-6940] Improve test isolation in test_views.py (#7564),3
[AIRFLOW-6938] Don't read dag_directory in SchedulerJob (#7562),2
[AIRFLOW-6857] Bulk sync DAGs (#7477),2
[AIRFLOW-XXXX] Fix typos in docs directory (#7571),2
[AIRFLOW-6939] Executor configuration via import path (#7563),2
[AIRFLOW-6862] Do not check the freshness of fresh DAG (#7481),2
[AIRFLOW-6856] Bulk fetch paused_dag_ids (#7476),2
[AIRFLOW-XXXX] Update LICENSE versions and remove old licenses (#7553),4
[AIRFLOW-6730] Use total_seconds instead of seconds (#7363)* [AIRFLOW-6730] Use total_seconds instead of seconds* adds tests and fixes types issue* fix test,3
[AIRFLOW-6856] BugFix: Paused Dags still Scheduled (#7578),2
[AIRFLOW-XXXX] Add Tink as an Airflow user (#7581),1
"[AIRFLOW-6949] Respect explicit `spark.kubernetes.namespace` conf to SparkSubmitOperator (#7575)This means the value from the Operator/dag file takes precedence overthe connectionThe previous behaviour was to emit one line from the conf arg, but thena later one from the connection:```--conf spark.kubernetes.namespace=airflow \--conf spark.kubernetes.namespace=default \```",5
[AIRFLOW-6937] Inline _get_simple_dags method in SchedulerJob (#7561),2
[AIRFLOW-XXXX] Add airflow/utils/dag_processing.py to boring-cyborg.yml (#7586),5
[AIRFLOW-6950] Remove refresh_executor_config from ti.refresh_from_db (#7577),5
[AIRFLOW-6948] Remove ASCII Airflow from version command (#7572),4
[AIRFLOW-6955] Only register signal handlers when DagFileProcessorManager is started (#7582),2
[AIRFLOW-6941] Add queries count test for create_dagrun (#7565),2
[AIRFLOW-6957] Make retrieving Paused Dag ids a separate method (#7587),2
[AIRFLOW-6952] Use property for dag default_view (#7579),2
[AIRFLOW-XXXX] Fix typo in bigquery_dts.rst (#7588),2
[AIRFLOW-6933] Pass in env vars for all operators (#7576),1
"[AIRFLOW-6871] optimize tree view for large DAGs (#7492)This change reduces page size by more than 10X andreduces page load time by 3-5X. As a result, thetree view can now load large DAGs that were causing5XX error without the patch.List of optimizations applied to the view handler:* only seralize used task instance attributes to json instead of the  whole ORM object* encode task instance attributes as array instead of dict* encode datetime in unix timestamp instead of iso formmat string* push task instance attribute construction into client side JS* remove redundant task instance attributes* simplify reduce_nodes() logic, remove unnecessary if statements* seralize JSON as string to be used with JSON.parse on the client side  to speed up browser JS parse time* remove spaces in seralized JSON string to reduce payload size",5
"[AIRFLOW-XXXX] Fix tutorial that initialize the database tables for 2.0.0 (#7590)The aiflow initdb part of the commit 61455c69ddb5677ce02af626fbbeaf4bc29c15d3was added to tutorial. However, if using 'initdb' command in version 2.0.0as current document, it doesn't work like below. Because `db init` is usedinstead of `initdb` in v2.0.0, so must fix it.```  $ airflow initdb  airflow command error: argument subcommand: invalid choice: 'initdb'  (choose from 'config', 'connections', 'dags', 'db', 'kerberos', 'pools',  'roles', 'rotate_fernet_key', 'scheduler', 'sync_perm', 'tasks', 'users',  'variables', 'version', 'webserver'), see help above.```",1
[AIRFLOW-6869][WIP] Bulk fetch DAGRuns for _process_task_instances (#7489),2
[AIRFLOW-5908] Add download_file to S3 Hook (#6577),1
[AIRFLOW-XXXX] Add -- to rm in install_released_airflow_version (#7548)Adding this -- after rm -f helps in case when you have some weirdfiles that have spaces and -W in names.,2
[AIRFLOW-6924] Fix Google DLP operators return types (#7546),1
[AIRFLOW-XXXX] Fix path in kubernetes.rst  (#7594),0
[AIRFLOW-6958] Fix Intermittent CI failure (#7592),0
[AIRFLOW-6881] Bulk fetch DAGRun for create_dag_run (#7502),2
[AIRFLOW-6472] Correct short and long option in cli (#7148)* [AIRFLOW-6472] Correct short and long option in cli* Make Airflow cli short options be single character only* Make Airflow cli long options be kebab-case style* Add test for airflow/bin* Update UPDATING.md,5
[AIRFLOW-XXXX] Add Docker installation in WSL (#7591)Add Docker setting in WSL and troubleshooting methodbecause of volume mount problem in WSL and dockerI referenced document with :https://nickjanetakis.com/blog/setting-up-docker-for-windows-and-wsl-to-work-flawlessly,2
[AIRFLOW-6966] Move reap_process_group to process_utils (#7601),4
[AIRFLOW-6962] Fix compeleted to completed (#7600),0
"[AIRFLOW-6969] Move backport build to test (#7603)When one of the test builds fails even restarting it does not help to succeedthe whole job if there is ""post-test"" stage. The post-test job will remain incancelled state after the job is restartedWe should move prepare-backport-packages to test phase (it is very fast).",3
[AIRFLOW-6961] Create tags in bulk during bulk_sync_to_db (#7599),5
[AIRFLOW-2325] Add CloudwatchTaskHandler option for remote task logging to Cloudwatch (#7437),2
"[AIRFLOW-XXXX] Fix typo from upstream to downstream (#7595)This is how it used to be:Each DAG Run will contain a task_1 Task Instance and a task_2 Task instance. Both Task Instances willhave ``execution_date`` equal to the DAG Run's ``execution_date``, and each task_2 will be *upstream* of(depends on) its task_1.if task_2 depends on task_1 this means task_2 is set downstream of tastk_1  - wondering if I am missing something here - or misreading it",1
[AIRFLOW-XXXX] Add Ternary Data to README.md (#7606),2
[AIRFLOW-6860] Default ignore_first_depends_on_past to True (#7490),5
[AIRFLOW-6971] Fix return type in CloudSpeechToTextRecognizeSpeechOperator (#7607),1
[AIRFLOW-6979] Fix breeze test-target specific test param issue (#7614),0
[AIRFLOW-6956] Extract kill_child_processes_by_pids from DagFileProcessorManager (#7584),2
"[AIRFLOW-XXXX] Add Wildlifestudios to airflow users listWildlifestudios uses Airflow heavily (more than 50 distinct monthly active users) for its data pipelines (+100 DAGs), which process billions of mobile game events, everyday.",2
[AIRFLOW-6970] Improve GCP Video Intelligence system tests (#7604),3
[AIRFLOW-6926] Fix Google Tasks operators return types and idempotency (#7547),1
[AIRFLOW-6040] ReadTimoutError should not raise exception (#7616),0
[AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608),1
[AIRFLOW-6977] Fix BigQuery DTS example DAG (#7612),2
[AIRFLOW-6973] Make GCSCreateBucketOperator idempotent (#7609),1
[AIRFLOW-XXXX] Add docs to add Dag Tags (#7627),2
[AIRFLOW-6915] Add AI Platform Console Link for MLEngineStartTrainingJobOperator (#7535),5
[AIRFLOW-6984] Improve setup/teardown in SFTP-GCS system tests (#7623),3
[AIRFLOW-6990] Improve system tests for Google Marketing Platform (#7631),3
[AIRFLOW-6497] Avoid loading DAGs in the main scheduler loop (#7597)* [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler* fixup! [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler* fixup! fixup! [AIRFLOW-6497] Avoid loading DAGs in the main loop of the scheduler,2
[AIRFLOW-6998] Fix failing system tests in CI (#7636),3
[AIRFLOW-7001] Time zone removed from MySQL TIMSTAMP field inserts (#7641)For mysql we should store timestamps as naive values.Timestamp in MYSQL is not timezone aware. In MySQL 5.6timezone added at the end is ignored but in MySQL 5.7inserting timezone value fails with 'invalid-date',5
[AIRFLOW-5828] Move build logic out from hooks/build (#7618)This is the final step of simplifying the Breeze scripts bymoving all the logic out from Travis' hooks/build,1
[AIRFLOW-6973] Make GCSCreateBucketOperator idempotent (fix) (#7624),0
[AIRFLOW-7006] Fix missing +e in Breeze script (#7648),0
"[AIRFLOW-7002] Get rid of yaml ""parser"" in bash (#7646)Mounts in docker compose file is now generated from an env variable",2
[AIRFLOW-6967] Add tests to avoid performance regression in DagFileProcessor (#7602),2
[AIRFLOW-7005] Added exec command to Breeze (#7649),1
[AIRFLOW-5842] Swtch to Debian buster image as a base (#7647),5
[AIRFLOW-6877] Add cross-provider dependencies as extras (#7506)Cross-provider dependencies are now extras in the backportpackages.,1
[AIRFLOW-7012] Split tests into providers/core to save memory (#7657),1
[AIRFLOW-7009] Use keywords arguments for DagFileStat (#7651),2
[AIRFLOW-XXXX] Fix gitignore (#7660),0
[AIRFLOW-7010] Skip in-container checks for Dockerhub builds (#7652),2
[AIRFLOW-5922] Add option to specify the mysql client library used in MySqlHook (#6576)- add mysql-connector-python library- fix pylint issues- add docs for using mysql driver in airflow db init,5
[AIRFLOW-XXXX] Fix upsert operator in BQ example DAG (#7666),2
[AIRFLOW-2911] Add job cancellation capability to Dataflow service (#7659),5
[AIRFLOW-7018] fixing travis's job name escaping problem (#7668),0
[AIRFLOW-5013] Add GCP Data Catalog Hook and operators (#7664),1
[AIRFLOW-7021] Avoid multiple calls to conf.getboolean in _process_dags (#7672),2
[AIRFLOW-7023] Remove duplicated package definitions in setup.py (#7675),1
[AIRFLOW-7015] Detect Dockerhub repo/user when building on Dockerhub (#7673),2
[AIRFLOW-XXXX] Small grammar fixes in Best Practices doc (#7670),2
[AIRFLOW-6542] Add spark-on-k8s operator/hook/sensor (#7163),1
[AIRFLOW-7024] Add the verbose parameter support to SparkSqlOperator (#7676),1
[AIRFLOW-6443] Increase pool name length & error if name is too long (#7658),0
[AIRFLOW-6980] Improve system tests and building providers package (#7615)This PR removes initdb from system tests setup as it seems unneccessary operation.Also some automatic code changes has been added before building providers package.fixup! [AIRFLOW-6980] Improve system tests and building providers packagefixup! [AIRFLOW-6980] Improve system tests and building providers packagefixup! fixup! [AIRFLOW-6980] Improve system tests and building providers package,1
[AIRFLOW-7016] Sort dag tags in the UI (#7661),2
[AIRFLOW-4438] Add Gzip compression to S3_hook (#7680),1
[AIRFLOW-7032] Remove airflow/gcp folder from doc build (#7681),2
[AIRFLOW-6809] Add tests for presto operators (#7422),1
[AIRFLOW-7037] Fix Incorrect Type Annotation for Multiprocessing Connection (#7687),0
[AIRFLOW-7040] Move tests/utils/contrib packages to tests/utils (#7690),3
[AIRFLOW-7041] make bowler dependency local (#7691),1
[AIRFLOW-7017] Respect default dag view in trigger dag origin. (#7667),2
[AIRFLOW-XXXX] Fix typo in tests/jobs/test_base_job.py (#7698),3
[AIRFLOW-XXXX] Fix docstring in TestSchedulerJob (#7699),3
[AIRFLOW-XXXX] Remove already solved items from pylint_todo.txt (#7697),5
[AIRFLOW-XXXX] Replace conversion from list to set (#7696),1
[AIRFLOW-XXXX] Update to the latest version of pre-commit-hooks (#7702),1
[AIRFLOW-XXXX] Change CONTRIBUTING.md to CONTRIBUTING.rst (#7695),4
[AIRFLOW-XXXX] Fix broken link in backport_packages/README.md (#7700),2
[AIRFLOW-XXXX] Wrap line under 90 characters (#7701),5
[AIRFLOW-6481] Fix bug in SalesforceHook (#7703)Explicitly cast the columns that are deemed as possibly string to strtype because accessing .str on Python dict raises an error.Update the corresponding test and re-formatted the code.Co-authored-by: Teddy Hartanto <teddy.hartanto@ninjavan.co>,3
[AIRFLOW-7034] Remove feature: Assigning Dag to task using Bitshift Op (#7685)* [AIRFLOW-7034] Remove feature: Assigning Dag to task using Bitshift Operator* fixup! [AIRFLOW-7034] Remove feature: Assigning Dag to task using Bitshift Operator* fixup! fixup! [AIRFLOW-7034] Remove feature: Assigning Dag to task using Bitshift Operator* fixup! fixup! fixup! [AIRFLOW-7034] Remove feature: Assigning Dag to task using Bitshift Operator,1
[AIRFLOW-7013] Automated check if Breeze image needs to be pulled (#7656),5
[AIRFLOW-5944] Rendering templated_fields without accessing DAG files (#6788),2
[AIRFLOW-6954] Use DagRunType instead of ID_PREFIX in run_id (#7583)fixup! [AIRFLOW-6954] Use DagRunType instead of ID_PREFIX in run_id,1
[AIRFLOW-7047] Fixe build providers dependencies pre-commit on Mac (#7705),1
[AIRFLOW-7025] Fix SparkSqlHook.run_query to handle its parameter properly (#7677),2
[AIRFLOW-6884] Make SageMakerTrainingOperator idempotent (#7598),5
[AIRFLOW-7022] Simplify DagFileProcessor.process_file method (#7674),2
[AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML (#7619)* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML CODE REVIEW* [AIRFLOW-6975] Base AWSHook AssumeRoleWithSAML CODE REVIEW 2Co-authored-by: Bjorn Olsen <BjornOlsen@capitecbank.co.za>,1
[AIRFLOW-6989] Display Rendered template_fields without accessing Dagfiles (#7633),2
[AIRFLOW-7056] Selective backport packages build (#7712),5
[AIRFLOW-6724] Add Google Analytics 360 Accounts Retrieve Operator (#7630)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,1
[AIRFLOW-7054] Breeze has an option now to reset db at entry (#7710),1
[AIRFLOW-5946] DAG Serialization: Store source code in db (#7217)* DAG serialization improvement: the DAG's source code is now stored in the dag_code table and is queried from here when the Code view is opened for the DAG. The webserver no longer needs access to the dags folder in the shared filesystem.,5
[AIRFLOW-7003] Lazy load all plguins (#7644),2
[AIRFLOW-XXXX] Update temp link to a fixed link (#7715),2
[AIRFLOW-XXXX] Remove the defunct limitation of Dag Serialization (#7716),2
[AIRFLOW-7029] Use separate docker image for running license check (#7678)Each stage of the CI tests needs to pull our `ci` image. By removingjava from it we can save 1-2minutes from each test stage. This is partof that work.,1
[AIRFLOW-7001] Further fix for the MySQL 5.7 UtcDateTime (#7655)The original fix from the commitb4215f634c0dd60f101b79d3d5bf493603bc6cbc was wrong. It converted to utc initially but then make_naivehad used TIMEZONE and converted it to the lcoal timezone ratherthan UTC.,1
[AIRFLOW-XXXX] Add Rakuten in Airflow Users list (#7686),1
"[AIRFLOW-7019] Show un/pause errors in dags view. (#7669)Pausing and unpausing dags in the dags view is asynchronous, and thereis currently no indication to the user if the operation fails. Thispatch updates the paused input and highlights it in red when pausing orunpausing fails.",0
[AIRFLOW-7055] Verbose logging option for google provider (#7711),1
[AIRFLOW-7062] Fix pydruid release breaking the build (#7720),4
[AIRFLOW-7061] Rename openfass to openfaas (#7721),5
[AIRFLOW-5705] Add secrets backend and support for AWS SSM (#6376),1
[AIRFLOW-7051] Implement Vertica Hook Tests (#7707),3
[AIRFLOW-7063] Fix dag.clear() slowness caused by count (#7723),1
"[AIRFLOW-6946] Switch to MySQL 5.7 in 2.0 as base (#7570)Switch to MySQL 5.7 in tests.Fixes the utf8mb4 encoding issue where utf8mb4 encodingproduces too long keys for mysql to handle in XCom table.You can optionally specify a separate option to setencoding differently for the columns that are part of theindex - dag_id, task_id and key.",2
[AIRFLOW-7058] Add support for different DB versions (#7717),5
[AIRFLOW-7066] Use sphinx syntax in concepts.rst (#7729),1
[AIRFLOW-6530] Allow Custom Statsd Client (#7227)Co-authored-by: Usman Arshad <usman.arshad@skyscanner.net>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Craig Rosie <craigrosie7@gmail.com>,1
[AIRFLOW-XXXX] Updated supported versions of backends (#7727),1
[AIRFLOW-5664] Store timestamps with microseconds precision (#6354)Microseconds value is lost in the conversion to timestampusing time.mktime.Timestamp is now computed to be precise up to microseconds.,1
[AIRFLOW-6987] Avoid creating default connections (#7629),1
[AIRFLOW-4175] S3Hook load_file should support ACL policy paramete (#7733)- Added acl_policy parameter to all the S3Hook.load_*() and S3Hook.copy_object() function                     - Added unittest to test the response permissions when the policy is passed                     - Updated the docstring of the functionCo-authored-by: retornam <retornam@users.noreply.github.com>,1
[AIRFLOW-5705] Fix bug in Secrets Backend (#7742),0
[AIRFLOW-XXXX] Add label for Secrets for matching filepaths (#7746),2
[AIRFLOW-5705] Fix bugs in AWS SSM Secrets Backend (#7745),0
[AIRFLOW-7073] GKEStartPodOperator always use connection credentials (#7738),1
"[AIRFLOW-6855]: Escape project_dataset_table in SQL query in gcs to bq … (#7475)* [AIRFLOW-6855]: Escape project_dataset_table in SQL query in gcs to bq operatorWithout escaping, if the project is specified in project_dataset_table and contains a -,the query will fail with an error.* Make string formatting in gcs_to_bigquery f-strings, add unit tests.* pylint appeasement, hopefully.* More not understanding how mocking works in python...* Add task ids to tests, remove tst_gcs_to_bigquery.py as a missing test file.* maybe more correct mocking of class vairables.",2
[AIRFLOW-XXXX] Fix typo in UPDATING.md (#7751),5
[AIRFLOW-7079] Remove redundant code for storing template_fields (#7750),4
[AIRFLOW-7072] Task instance email_alert render only if need (#7736)Task instance email_alert render html_content_err templateonly if need,2
[AIRFLOW-7074] Add Permissions to view SubDAGs (#7752),2
[AIRFLOW-5610] Add ability to specify multiple objects to copy in GCSToGCSOperator (#7728),1
[AIRFLOW-6014] - handle pods which are preempted and deleted by kuber… (#6606)* [AIRFLOW-6014] - handle pods which are preempted and deleted by kubernetes but not restarted,4
[AIRFLOW-7081] Remove env variables from GCP guide (#7755),4
[AIRFLOW-7080] Adds API endpoint to return a DAG's paused state (#7737)Adds an additional endpoint to the experimental APIto return the paused state of a DAG.,2
[AIRFLOW-7064] Add CloudFirestoreExportDatabaseOperator (#7725),5
[AIRFLOW-5421] Add Presto to GCS transfer operator (#7718),1
[AIRFLOW-7000] Allow passing in env var JSON dict in task_test (#7639)Co-authored-by: Haim Grosman <haim.grosman@airbnb.com>,3
[AIRFLOW-XXXX] Add cross-references for operators guide (#7760),1
[AIRFLOW-XXXX] Fix reference to GCP classes in guides (#7762),0
[AIRFLOW-7088] Avoid reading store_dag_code option in loop (#7764),2
[AIRFLOW-7089] Create kill method in AbstractDagFileProcessorProcess (#7765),2
"[AIRFLOW-7085] Cache credentials, project_id in GCP Base Hook (#7759)",1
[AIRFLOW-7087] Rename _execute_helper to _run_scheduler_loop (#7763),1
[AIRFLOW-7082] Remove catch_http_exception decorator in GCP hooks (#7756)* [AIRFLOW-7082] Remove catch_http_exception decorator in GCP hooksThis decorator unables to catch 409 or similar error from Google APIs* fixup! [AIRFLOW-7082] Remove catch_http_exception decorator in GCP hooks* fixup! fixup! [AIRFLOW-7082] Remove catch_http_exception decorator in GCP hooks* fixup! fixup! fixup! [AIRFLOW-7082] Remove catch_http_exception decorator in GCP hooks,1
[AIRFLOW-XXXX] Add HomeToGo in Airflow users list (#7769),1
[AIRFLOW-4679] Make airflow/operators Pylint compatible (#7757),1
[AIRFLOW-4672] Make airflow/hooks Pylint compatible (#7767),1
[AIRFLOW-7084] Lazy initialize plugins for each entrypoint (#7758),1
[AIRFLOW-7069] Fix cloudsql system tests (#7770),3
"Remove JIRA requirements from PRs (#7771)As a part of our move from JIRA tickets to GitHub issues, we should nolonger force users to add JIRA ticket numbers to the titles of their PRs",1
[AIRFLOW-6885] Delete worker on success (#7507)Users now have the option to only delete worker pods when they are successfulCo-authored-by: Daniel Imberman <daniel@astronomer.io>,1
[AIRFLOW-7076] Add support for HashiCorp Vault as Secrets Backend (#7741),1
[AIRFLOW-5705] Make AwsSsmSecretsBackend consistent with VaultBackend (#7753),1
[AIRFLOW-6959] Use NULL as dag.description default value (#7593),2
"[AIRFLOW-6732] Add GoogleAdsHook and GoogleAdsToGcsOperator (#7692)* [AIRFLOW-6732] Add GoogleAdsHook and GoogleAdsToGcsOperatorProvide ability to connect to Google Ads API using service account and execute arbitrary search (API call) using Google Ads Query language.Convert the API results to CSV and store in designated GCS bucket.Add google-ads to setup.py as a gcp package.* Update operators-and-hooks-ref for Google Ads hook and operator* Make updates for minor  PR comments- Add api_version as an attribute of GoogleAdsHook- Update CLIENT_IDS in example_google_ads DAG to make it clear that they are dummy values- Add Google Ads API link in hook and operator docstrings* Add tests* Add flush to tempfileRequired to ensure the csvfile within the context manager is correctly flushed before being uploaded to GCS. Without this flush, the written data is not correctly saved before upload* Fix docs* Move code to google.ads directory* fixup! Move code to google.ads directory* fixup! fixup! Move code to google.ads directory* fixup! fixup! fixup! Move code to google.ads directory* fixup! fixup! fixup! fixup! Move code to google.ads directoryCo-authored-by: Todd de Quincey <todddequincey@Todds-Air.broadband>Co-authored-by: Todd de Quincey <todd.dequincey@infectiousmedia.com>Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>",5
[AIRFLOW-6978] Add PubSubPullOperator (#7766)* Fix typo.* Type checking: pass empty dict instead of None to operators in test scenarios.* Reformat some whitespace.* Fix PubSubHook.pull return value type annotation.* PubSubHook.acknowledge: Implement acknowledging by passing list of ReceivedMessage objects.* Refactor PubSubPullSensor.* Implement PubSubPullOperator.* Remove unused constant from PubSub tests.* Fix Sensor contract: some do have a return value.* Reformat whitespace in PubSub tests.* PubSub: Add tests for PubSubPullOperator.* Fix PubSubPullSensor tests after refactoring.* Sort imports.* Fix pylint complaining about callback interface.* Update airflow/providers/google/cloud/operators/pubsub.pyCo-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>* Update airflow/providers/google/cloud/hooks/pubsub.pyCo-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>* Update airflow/providers/google/cloud/hooks/pubsub.pyCo-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>* Update airflow/providers/google/cloud/operators/pubsub.pyCo-Authored-By: Kamil Breguła <mik-laj@users.noreply.github.com>* Reorder PubSub messages_callback argument.* Update docstring.* Reformat mutually exclusive argument handling logic in PubSubHook.* PubSub: Deprecate return_immediately argument.* Implement example DAG and system test for PubSubPullOperator.* Fix docstring formatting.* PubSubPullOperator: Fix docs.* Apply suggestions from code reviewCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Aleksander Nitecki <aleksander.nitecki@polidea.com>Co-authored-by: ANiteckiP <60920935+ANiteckiP@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
[AIRFLOW-XXXX] document system tests mechanism better (#7774),1
[AIRFLOW-7098] Simple salesforce release 1.0.0 breaks the build (#7775),4
[AIRFLOW-6752] Add GoogleAnalyticsRetrieveAdsLinksListOperator (#7748)fixes after reviewCo-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,0
[AIRFLOW-6860] Default ignore_first_depends_on_past to True (#7610),5
[AIRFLOW-XXXX] Fix typo in ci_prepare_backport_packages.sh (#7778),2
"Remove Jira title requirement from Mergable check (#7780)Follow up to #7771, where we missed one final check",1
"Enable Github issues (#7779)As we don't want people using GitHub issues for asking for help, I havetried to make this clear in the issue template.Lets see if it works or not...",1
[AIRFLOW-7100] Add GoogleAnalyticsGetAdsLinkOperator (#7781)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,2
Fix formatting in feature_request issue template (#7784)To make it bold there needs to be no space inside the `**`,1
Remove references to Jira for tickets/PR from docs (#7783),2
[AIRFLOW-7097] Install gcloud beta componensts in CI image (#7772),2
Fixes unclean installation of Airflow 1.10 (#7796)Removal of airrflow installed via -e . leaves .egg-info folderthat makes subsequent airflow installation not clean.This folder should be removed before install,4
Fix elasticsearch breaking the build (#7800),4
Bring back reset db explicitly called at CI entry (#7798)* Fix elasticsearch breaking the build* Bring back reset db explicitly called at CI entry,1
Remove arg lookup map in CLIFactory (#7801),4
[AIRFLOW-7067] Pinned version of Apache Airflow (#7730),5
Add ability to specify a maximum modified time for objects in GCSToGCSOperator (#7791),1
Introduce types in airflow.bin.cli (#7803),5
[AIRFLOW-7099] Improve system test for cloud transfer service (#7794),3
Ask users reporting UI bugs to include photo or video (#7790),0
Verify that all providers packages are included (#7793),1
Change name of the common environment initialization function (#7805),1
Remove CLIFactory - reduce indendation (#7804),5
Add backwards compatibility for chain and cross_downstream (#7807),1
[AIRFLOW-XXXX] Clarifying wording on DAG runs (#7744),1
Test installation of all provider packages (#7797),1
Remove airflow.bin package (#7808),4
[AIRFLOW-XXXX] Add guide for Travis CI and IDE setup (#7625),1
[AIRFLOW-7104] Add Secret backend for GCP Secrets Manager (#7795),1
Imrove support for laatest API in  MLEngineStartTrainingJobOperator (#7812),5
Remove redundant parentheses (#7814),4
Make get_user_roles staticmethod (#7815),1
Make AirflowConfigParser._warn_deprecate as staticmethod (#7816),5
Add missing docstring in BigQueryHook.create_empty_table (#7817),1
Fix typo in GCP credentials_provider's docstring (#7818),2
Add call to Super call in apache providers (#7820),1
Add call to Super call in microsoft providers (#7821),1
Add call to Super class in 'ftp' & 'ssh' providers (#7822),1
Add call to Super class in 'google' providers (#7823),1
Add missing call to Super class in 'salesforce' provider (#7824),1
Add missing call to Super class in 'cncf' & 'docker' providers (#7825),1
"Add missing call to Super class in 'http', 'grpc' & 'slack' providers (#7826)",1
"Add missing call to Super class in 'amazon', 'cloudant & 'databricks' providers (#7827)",1
Add missing call to Super class in remaining providers (#7828),1
Change from Instance attribute to variable in JdbcOperator.execute (#7819),5
Move Dockerfile to Dockerfile.ci (#7829),2
Improve example DAG for ML Engine (#7810),2
Make airflow/providers pylint compatible (#7802),1
Improve Google PubSub hook publish method (#7831),1
Pylint: Enable deprecated-string-function check (#7839),1
Pylint: Enable deprecated-sys-function check (#7838),5
Pylint: Enable dict-*-not-iterating check (#7840),0
Pylint: Enable deprecated-* check (#7842)Enable the following checks:- deprecated-pragma- deprecated-str-translate-call- deprecated-itertools-function- deprecated-types-field- deprecated-operator-function- deprecated-urllib-function,1
Skip Installation on Travis (#7845),2
[AIRFLOW-6624] Improve webserver command with pidfile checking (#7245)* [AIRFLOW-6624] Improve webserver command with pidfile checkingWhen running webserver as daemon it can happend that it will not startdue to existing pidfile. This PR improves whole webserver command and addspidfile checking.* fixup! [AIRFLOW-6624] Improve webserver command with pidfile checking,2
Remove super().__init__() call in backported hooks (#7833),1
[AIRFLOW-7105] Unify Secrets Backend method interfaces (#7830),5
Add Colgate-Palmolive to users list (#7848),1
"[AIRFLOW-7053] Fix success/failed error in graph without dagrun (#7709)Graph view now will raise an error when clickmake task instance success/failed, with theinit state and no dag run, we should make somehint instead of raise error in the webserver",0
Run Dataflow for ML Engine summary in venv (#7809),5
Bump Boto3 (#7851),5
Fix example DAG for MLEngine in backport package (#7813),2
Remove unused import - BowlerTool (#7865),2
Standardize SecretBackend class names (#7846)- AwsSsmSecretsBackend -> SystemsManagerParameterStoreBackend- CloudSecretsManagerSecretsBackend -> CloudSecretsManagerBackend- VaultSecrets -> VaultBackend- EnvironmentVariablesSecretsBackend -> EnvironmentVariablesBackend- MetastoreSecretsBackend -> MetastoreBackend,5
[AIRFLOW-6833] HA for webhdfs connection (#7454),5
Welcome users on creating their first issue (#7867),0
BugFix: Show task_id in the Graph View tooltip (#7859),0
add kubernetes instructions to bug_report (#7789)Co-authored-by: Daniel Imberman <daniel@astronomer.io>,0
Fix static check failure: end-of-file-fixer (#7874)https://github.com/apache/airflow/pull/7789 introduced this bug by adding a new line at the end of the file,2
"[AIRFLOW-4363] Fix JSON encoding error (#7628)From the docker-py code comments for APIClient pull,the decode parameter should be set to True, when thestream parameter is also set to True. This will allowdecoding JSON data returned from the docker registryserver into dictsSigned-off-by: Raymond Etornam <retornam@users.noreply.github.com>",1
Fix grammar in setup.py (#7877),1
[AIRFLOW-5801] Get GCP credentials from file instead of JSON blob (#7869)* fix assumption of getting gcp credentials from file instead of JSON blob* fix deprecation warning,2
Restore DaskExecutor tests (#7786)Closes https://github.com/dask/dask/issues/5803. See https://github.com/dask/dask/issues/5803#issuecomment-601788441 for context.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
Load all job models at once (#7878),5
Remove Presto check operators (#7884),1
Don't schedule dummy tasks (#7880),5
Remove references to airflow.contrib in tests (#7882),3
bumping simple-salesforce to 1.0.0 (#7857),1
Improve authorization in GCP system tests (#7863),3
Fix CloudMemorystoreCreateInstanceAndImportOperator operator (#7856),1
Improve example DAGs for Cloud Memorystore (#7855),2
Display DAG filepaths in airflow dags list command (#7886),2
Improve setUp/tearDown in Cloud Firestore system test (#7862),3
Improve setUp/tearDown in PrestoToGCSSystemTest (#7860),5
Fix CloudSecretsManagerBackend invalid connections_prefix (#7861),0
Update Celery to 4.4.2 (#7754),5
Improve idempotency in MLEngineHook.create_model (#7811),1
[AIRFLOW-6399] Serialization: DAG access_control field should be decorated field in DAG serialization (#7879)Co-authored by: Louis Simoneau <simoneau.louis@gmail.com>Co-authored-by: Long Le Xich <codenamelxl@users.noreply.github.com>,1
Change signature of GSheetsHook methods (#7853),1
[Doc] Separate supported Postgres versions with comma (#7892),1
Fix Elastic DAG for TestDagFileProcessorQueriesCount (#7889),3
"[AIRFLOW-4453] Make behavior of `none_failed` consistent with documentation (#7464)The documentation for the `none_failed` trigger rule(https://airflow.apache.org/docs/stable/concepts.html#trigger-rules)describes its behavior as ""all parents have not failed (`failed` or`upstream_failed`) i.e. all parents have succeeded or been skipped.""With that definition in mind, there is no reason that the check for`none_failed` should ever have to check for skipped upstream tasks orset the current task to `skipped`. The current behavior causes the ruleto skip if all upstream tasks have skipped, which is more than a littleconfusing. This fixes the behavior to be consistent with the documentation.Co-authored-by: root <jcroteau@liveramp.com>",2
[Doc] Fix broken link in README.md (#7893),2
Add _access control to validate deserialized DAGs (#7896),2
Extract DAG cycle tester (#7897),3
Requirements now depend on python version (#7841),1
Enable super init not called check in pylint (#7834),5
Decouple DAG and SerializedDagModel (#7898),2
Allow hvac pakage installation using 'hashicorp' extra (#7915),1
[AIRFLOW-5825] SageMakerEndpointOperator is not idempotent (#7891),1
Fixed automated check for image rebuild (#7912),0
Add download/upload operators for GCS and Google Sheets (#7866)* Add download/upload operators for GCS and Google Sheets* fixup! Add download/upload operators for GCS and Google Sheets* fixup! fixup! Add download/upload operators for GCS and Google Sheets,1
Install version is not persistent in breeze (#7914),2
Get Airflow Connection from Environment Variables (#7923),1
Make BaseSecretsBackend.build_path generic (#7948)Currently the arguments required for it are `connections_prefix` and `conn_id` . Changing this to `path_prefix` and `secret_id` allows using that method for retrieving variables too,1
Don't use DagBag in set_is_paused method (#7894),1
Get Airflow Variables from Hashicorp Vault (#7944),1
Get Airflow Variables from GCP Secrets Manager (#7946),1
Get Airflow Variables from AWS Systems Manager Parameter Store (#7945),2
Fixes too high parallelism in CI (#7947),0
Remove Prepare & test backport package for 1.10.6 test from CI (#7950),3
Fix typo for store_serialized_dags config (#7952),5
Remove unnecessary messages in CI (#7951),4
[AIRFLOW-7113] Fix gantt render error (#7913)Co-authored-by: Cong Zhu <cong.zhu@airbnb.com>,0
Use mock.patch in TestExecutorLoader.test_should_support_plugins (#7949),3
Remove GKEStartPodOperator when backporting (#7908),1
[AIRFLOW-XXXX] fix pools doc for LocalExecutor (#7643)from this ticket : https://issues.apache.org/jira/browse/AIRFLOW-584,0
Further decrease of amount of parallelism (#7991),5
Improve system tests for Cloud Build (#8003),3
Add link to Airflow website in README (#7956)There's no simple way to move from Airflow repository to Airflow website. So I've added a link to airflow.apache.org and fixed link to documentation.,2
close sftp connection without error (#7953)check self.conn is None and close connection without error likeSSHHook,1
"Enable celery command in any environment (#7902)Set celery command visible even if the executor is not CeleryExecutorInstead, raise ArgumentError and display help message when the executor isnot CeleryExecutor",0
Remove sql like function in base_hook (#7901),1
[AIRFLOW-4688]: Fix Pylint checks on modules under `scripts` folder (#7850),0
"Move out get_python_source from www, Move get_dag to www.utils (#7899)",2
[AIRFLOW-7075] Operators for storing information from GCS into GA (#7743),5
[AIRFLOW-6574] Adding private_environment to docker operator. (#7671)The docker operator currently does not have a means to pass in anenvironment dict that is not exposed to the frontend.- Updating docs and ensuring code is flake8.- Adding a test and updating documentation.,2
Upgrading to latest requirements is eager (#7980)We have eager strategy in upgrade script - this way we will keep up with thelatest requirements as seen by anyone trying to install airflow from thescratch. And our tests will be using it.,1
[AIRFLOW] Force PGPORT env. var to be a string (#7773)Because subprocess.Popen loops through all the env vars and crashes if a value of an env var is not a string.,1
Fixed conditions for upgrade to latest requirements (#8013),1
[AIRFLOW-6685] ThresholdCheckOperator (#7353)* [AIRFLOW-6685] Data Quality Check operators* removed .get_connection to get hook in get_sql_value* added tests for get_sql_value* threshold check operator and tests added to checkoperator file,2
Mark trigger-controller-dag test as xfail (#8015),0
Add missing docstring in check operator (#8020),1
Run DB shells in PTY (#8004),5
Pinning max pandas version to 2.0 (lesser than) to allow pandas 1.0. (#7954),1
Fix Flaky TriggerDAG UI test (#8022),3
Individual package READMEs (#8012),5
Fix accidental hard dependency on dask in BackfillJob (#8025)PR #7786 pulled this back in in such a way that made dask a harddependency on `airflow backfill` commands.,0
Add more refactor steps for providers.google (#8010)* Add more refactor steps for providers.google* fixup! Add more refactor steps for providers.google,1
Rename CloudBaseHook to GoogleBaseHook and move it to google.common (#8011),4
Update TESTING.rst (#8029)Updating TESTING.rst with minor grammatical corrections,3
Update BREEZE.rst (#8028)Updating BREEZE.rst with minor grammatical corrections,5
Updating README.MD (#8027)Updating with minor grammatical corrections,5
Allow setting Airflow Variable values to empty string ('') (#8021),1
Prevent sequential scan of task instance table (#8014)The exact dag_id is known so no need to perform a like here which causeda sequential scan.,1
"Allow DateTimePicker to actually pick times too. (#8034)This is reported upstreadm as dpgaspar/Flask-AppBuilder#685 but thereappears to be 0 appetite for fixing it there. Since we can't easilychange the code in FAB I have had to customize the Widget, and give it adifferent class namehttps://github.com/eonasdan/bootstrap-datetimepicker/ is MIT licensed.",5
Unify Google class/package names (#8033),5
[AIRFLOW-5907] Add S3 to MySql Operator (#6578),1
Fix typo in scripts/perf/scheduler_dag_execution_timing.py (#8038),2
Generate requirements are now sorted (#8040),1
Fix example code in the documentation (#8042),2
"Use same tooltip for Graph and Tree views for TaskInstances (#8043)They had _similar_ info, but they weren't identical and should havebeen.To not include the 250kb of moment-timezone in more than one Webpackoutput bundle I changed how that is imported/handled. There may be abetter way of doing this in webpack, but I couldn't find it, and thisisn't horrible.",1
Update instructions to prepare backport packages (#8037),5
Add Jiajie Zhong to committers list (#8047),1
"[AIRFLOW-7045] Update SQL query to delete RenderedTaskInstanceFields (#8051)This is because ""The composite IN construct is not supported by all backends""Based on discussion in https://github.com/apache/airflow/pull/6788#discussion_r391268396",1
Add Google Ads list accounts operator (#8007),1
Fix airflow.www.views import (#8050),2
Unify flag name for long running tests (#8045),3
Add missing flag to xargs in containers utils (#8065),1
Add 1.10 import fallback in GCS-Presto system test (#8066),3
[AIRFLOW-XXXX] Add Gojek as an Airflow user (#8070)Gojek uses Airflow as a task automation scheduler and ETL tool on our data warehouses and machine learning pipelines.,1
[AIRFLOW-5800] Add a default connection entry for PinotDbApiHook (#6457),5
Add Production Docker image support (#7832),1
Fix Example in config_templates for Secrets Backend (#8074),5
Add backticks in IMAGES.rst command description (#8075),1
Change version_added for store_dag_code config (#8076),5
Add spark_kubernetes system test (#7875)fix reformatfix default connection sort,0
Remove JIRA ticket insert by bot in PR template (#7835),4
"Call delete_dag on subdag without AttributeError (#8017)The DagRun object does not have a task_id attribute, DagRuns are deleted in theblock above so no need to do it here or remove ones belonging to theparent.",4
"[AIRFLOW-7026] Improve SparkSqlHook's error message (#7749)* Replace self._conn.host in the error message with  self._master, because the former is unused in  SparkSqlHook actually.* Add self._sql into the error message, because it's  the executed query or a file that contains it.",2
"[AIRFLOW-6982] add native python exasol support (#7621)* [AIRFLOW-6982] add native python exasol supportThis adds exasol DB support, including a hook, connection type &operator. The [pyexasol](https://github.com/badoo/pyexasol) library isused to interact with the database.* Add exasol to EXTRAS documentation* Add exasol requirements to requirements files* Add exasol to backport packages setupCo-authored-by: Jan Omar <jan.omar@wooga.net>",1
Add Polidea as Airflow user (#8077),1
[AIRFLOW-7048] Allow user to chose timezone to use in UI (#8046)Co-authored-by: Sam Black <samblackk@users.noreply.github.com>,1
[AIRFLOW-7117] Honor self.schema in sql_to_gcs as schema to upload (#8049),5
Improve process terminating in scheduler_job (#8064),1
"BugFix: Datetimepicker is stuck on the UI (#8092)Datetimepicker was visible on page load on Graph view, Log View, TreeView, and Gantt chart, this was due to a fight between the old versionshipped with FAB and the version we now use.",1
Allow compile_assets.sh script to be run from any directory (#8097),1
"Housekeeping of auth backend & Update Security doc (#8071)- All authentication backends in `airflow/contrib/auth`  are base on the `user` model which has already been  removed earlier (https://github.com/apache/airflow/pull/4577#issuecomment-607202089)- Security related documentation is out-dated,  especially the LDAP and OAuth integration sections.  It still directs user to the configuration of ""old"" web UI  which has already been removed in master branch.",4
Add Local and Sequential Executors to Doc (#8084),2
"[AIRFLOW-5277] Gantt chart respects per-user the Timezone UI setting (#8096)`new Date()` in javascript was always returning user-local time, so weswitch over to using MomentJS to render the ticks.I also had to fix a bug in the `redraw` function here (previouslyunused, now used when the user selects a different timezone.)",1
Fix timezones displayed in Task Instance tooltip (#8103)I somehow made it so that both sets of dates were displayed only inbrowser local time.,5
Add QuintoAndar as Airflow user (#8101),1
"Handle DST better in Task Instance tool tips (#8104)We displayed the zone ""name"" based on the current time, which could leadto confusion when the date to be displayed was not in the samedaylight-savings state as ""now"".",5
[AIRFLOW-4529] Add support for Azure Batch Service (#8024),1
Make models/pool.py pylint compatible (#8068)* Make models/pool.py pylint compatible* Fixed for isortCo-authored-by: matsubara <matsubara@matsubaranoMacBook-Pro.local>,0
Add support for custom task runner (#8085),1
Fix enter Breeze with multiple container name include mysql (#8094),0
[AIRFLOW-6929] Add OpenAPI spec (#7549),1
Fix typo in .pre-commit-config.yaml (#8126),5
Fix minor issues with Announcements Dev Scripts (#8141),0
[AIRFLOW-6822] AWS hooks should cache boto3 client (#7541),1
Make the default TI pool slots '1' (#8153),1
"Improve tests for Storing Dag Code in DB (#8152)Original test does not cover the real case where DAG model actually calls `dag.sync_to_db()`. This is where the subdag fileloc is set to parent dag's fileloc, so it is important to change this behavior",4
"[AIRFLOW-7106] Cloud data fusion integration - Allow to pass args to start pipeline (#7849)* Add the possibility of passing args to data fusion start pipeline* [AIRFLOW-7106] Cloud data fusion integration - Allow to pass args to start pipeline* [AIRFLOW-7106] modified arguments type from string to dict, fixed tests* [AIRFLOW-7106] reverted wrong comment on tests* Update airflow/providers/google/cloud/hooks/datafusion.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* [AIRFLOW-7106] fixed static checks* reverting conftest.py* removed trailing whitespaces* added hook test for start pipeline with parameters* fixed test on google data fusion hook* fixed all pre-commit testsCo-authored-by: Davide Malagoli <malagoli@malagoli-macbookpro.roam.corp.google.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>",3
Fix broken Triggering Dag from UI functionality (#8148),1
Fix Updating & Deleting DagCode (#8161),2
"Fix 500 error in Security screens (#8165)PR #8046 used our CustomSQLAInterface more widely, and we weren'tcorrectly implementing the interface right in the constructor, which wasonly a problem for models which use Relations, which we don't inAirflow, but FAB does for it's user/permissions models.",1
[AIRFLOW-6914] Add a default robots.txt (#7653)Signed-off-by: Raymond Etornam <retornam@users.noreply.github.com>,1
"Revert ""[AIRFLOW-6914] Add a default robots.txt (#7653)"" (#8173)This reverts commit 751744237c25f793a34674897435d3c2f75860da.",4
Fix typo in doc of DagFileProcessorManager.emit_metrics (#8175),2
"Improve add_dag_code_table migration (#8176)This migration had a few problems that could cause us trouble in thefuture:1. It was importing the live model definition, which could break in the   future (if for instance it ever got a new column added, when this   migration runs it would try to use that new definition, but the   column won't exist yet).2. It was selecting the (large) `data` column needlessly3. It was dropping and re-creating the index but that is only needed on   MSSQL, not for MySQL or Postgresql.",1
Add tests for Security Views (#8180)This would avoid cases like the one that was solved in https://github.com/apache/airflow/pull/8165,3
Fix Viewing Dag Code for Stateless Webserver (#8178)Porting it from https://github.com/apache/airflow/commit/5e6aa3cc9c78d6be6d9578222469acab57251ef7 (v1-10-test),3
[AIRFLOW-6778] Add a configurable DAGs volume mount path for Kubernetes (#8147),2
Display Graph with TI statuses after backfilling (#7776),1
[AIRFLOW-6561] Add possibility to specify default resources for airflow k8s workers (#7168)Co-authored-by: Stijn De Haes <stijndehaes@gmail.com>,1
Consistent formatting in CSS files (#8182),2
Mount ${HOME}/.aws in breeze environemnt if --forward-credentials (#8183),5
Add support for AWS Secrets Manager as Secrets Backend (#8186)Allow retrieving Airflow Connections and Variables from AWS Secrets Manager (https://aws.amazon.com/secrets-manager/),1
"Revert ""[AIRFLOW-6929] Add OpenAPI spec (#7549)"" (#8217)This reverts commit 11f1e0cad996d5596e3e4fb440eb4ec52c024f70.The OpenAPI spec as added in has problems and needs revisiting (typos,wrong IDs, wrong types)",0
[AIRFLOW-5662] Reduce DB queries used to emit pool usage metrics (#6342)* [AIRFLOW-5662] fix incorrect naming and batch db call for scheduler metrics* report used slots value for used slots metrics (was sending occupied_slots)* move global scheduler metrics calculation and reporting outside of pool loop* optimize pool slots metrics report into constant timeCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,4
Fix formatting in AWS Connections docs (#8223),2
[AIRFLOW-7049] Persistent display/filtering of DAG status (#8106),2
Move DAG._schedule_interval logic out of DAG.__init__ (#8225)closes https://github.com/apache/airflow/issues/8166,0
Make Gantt tooltip the same as Tree and Graph view (#8220)To make this work I needed to change the fields exposed via the jsonrepresentation to those expected via the `tiTooltip` function.Closes #8210,1
CSS linting integrated into pre-commit (#8218),5
[AIRFLOW-6515] Change Log Levels from Info/Warn to Error (#8170),0
Fix stylelint violation (#8236),0
Add Changelog and Updating note for 1.10.10 (#8235),5
Honor schema type for MySQL to GCS data pre-process (#8090),5
Simplify DAG.set_dag_runs_state method (#8232)* Simplify DAG.set_dag_runs_state method* fixup! Simplify DAG.set_dag_runs_state method,2
Docker image build include now releses 1.10.10 version (#8234)It also installs properly on Mac as well as it auto-detectsif yarn prod is needed - based on presence of properpackage.json in either www or www_rbac which makes it simplerfor remote installations.,1
Upgraded to latest version of requirements (#8239),1
Proper version is displayed when running prod image via Breeze (#8229),1
Fix indentation (#8250),0
Add documentation for CLI command airflow dags test (#8251),3
RBAC ui: fix missing task runs being rendered as circles instead of squares (#8253),1
RBAC ui: Fix missing Y-axis labels with units in plots (#8252),0
Update to latest requirements 2020.04.12 (#8262),1
Make launch_type parameter optional (#8248)It is not a required parameter for run_task methodSee: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS.Client.run_task,3
Make autocomplete work better for zsh (#8260),1
Add 'Version Added' on Secrets Backend docs (#8264),2
Fix typo in local_task_job (#8263),2
[Docs] Simplify language re roll-your-own secrets backend (#8257),2
Help for breeze commands contain relevant flags. (#8261)* Help for breeze commands contain relevant flags.* fixup! Help for breeze commands contain relevant flags.* fixup! fixup! Help for breeze commands contain relevant flags.,0
Added GoogleDisplayVideo360UploadLineItemsOperator (#8216)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,1
Add mypy plugin for decorators. (#8145)* Preserve types for decorators that don't change signatures* Augment types for decorators that change signatures* Drop redundant type checks,4
Convert properties with query to real methods (#7900)* Convert properties with query to real methods,5
Fix tests and documentation for CLI 'connections add' (#8155),1
Fix non updating DAG code by checking against last modification time (#8266),2
Less aggressive eager upgrade of requirements (#8267)With this change requirements are only eagerly upgraded whengenerating requirements when setup.py changes. They are alsoeagerly upgraded when you run ./breeze generate-requirementslocally. Still the cron job will use the eager update mechanismwhen building the docker image which means that CRON jobs willstill detect cases where upgrede of requirements causes failureeither at the installation time or during tests.,3
Add Movember to users list (#8289),1
fixed typo (#8294),2
Added Facebook Ads Operator #7887 (#8008),1
Raise exception when GCP credential doesn't support account impersonation (#8213),1
"Fix confusing ""heartbeat"" of DagFileProcessor class (#8298)This method was called ""heartbeat"" and was called from withinSchedulerJob, but it does something very different to a job heartbeat --and is also not called in ""async"" mode, just in sync/sqlite mode, so Ithink this name provides a clearer indication of what it does",1
Remove unused code from Gannt View (#8302)After https://github.com/apache/airflow/pull/8220 we do not need task_types and extra_links variables,2
Create guide for BigQuery operators (#8276),1
"Remove unused Scheduler ""authenticate"" config (#8310)",5
Remove deprecated `secure_model` config (#8309)* Remove deprecated `secure_model` config`secure_mode` config was only available for Flask-admin UI* fixup! Remove deprecated `secure_model` config,5
Add simple tests for SqliteOperator (#8307)Improve code coverage by adding a few simple tests for theSqliteOperator.Signed-off-by: Remy Suen <remy.suen@gmail.com>,1
"Fix TOC on ""How-to Guides/GCP"" (#8295)",0
fix typo in DAG Serialization documentation (#8317),2
Production image is now built automatically in Dockerhub (#8314),2
Expose Airflow Webserver Port in Production Docker Image (#8228),2
Reorder middleware - ProxyFix and BaseUrl (#8157),0
Use Github Actions to run CI (#8376)* Use Github Actions to run CI* Fix backports build,0
[AIRFLOW-6885] Change delete-on-success to delete-on-failure (#8312)* Change delete-on-success to delete-on-failureIt makes more sense to by-default not delete failed podsUsers should explicitly state they want these pods deleted.This feature has not yet been released so ethis will not be a breakingchange* deps* depsCo-authored-by: Daniel Imberman <daniel@astronomer.io>,4
Fix Extra Links in Gannt View (#8308)Extra link didn't appear after changes in  https://github.com/apache/airflow/pull/8220 for Gantt View,4
Add Github Actions badge to README (#8386),1
"Add Dataproc SparkR Example (#8240)* GCP SparkR ExampleAllows you to schedule R, and sparkR jobs on a dataproc cluster.The functionality to run that kind of job is already in dataproc,but it was not so clear how to do that from Airflow.* Update airflow/providers/google/cloud/example_dags/example_dataproc.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>* Make sure R file finds correct libraryCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>",2
Move backport packages to GA (#8391),4
"Add migration waiting script and log cleaner (#8219)* Add migration waiting script and log cleanerThis PR creates a ""migration spinner"" that allows the webserver to wait for all database migrations to complete before starting up. Is a necessary component before we can merge the helm chart.* Update airflow/cli/cli_parser.pyCo-Authored-By: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>",5
Display docs errors summary (#8392),0
Add airflow info command (#8290),5
Add qoala as officially using airflow (#8401)We're using it as part of our ETL and other data related scheduling,5
Handle no Dagrun in DagrunIdDep (#8389),2
"[AIRFLOW-8187] Extend elastic DAG with a binary tree, grid, star (#8277)",2
"Fix timing-based flakey test in TestLocalTaskJob (#8405)This test suffered from timing-based failures, if the ""main"" processtook even fractionally too long then the task process would have alreadycleaned up it's subprocess, so the expected callback in the main/testprocess would never be run.This changes is so that the callback _will always be called_ in the testprocess if it is called at all.",3
Add colors to airflow config command (#8404),5
Test script tries to reset db regardless of db check (#8403),5
"Allow ""falsey"" default arguments in CLI Parser (#8398)In #8219 we noticed that we couldn't set a `default=0` because of the`and v` check. The ""add_argument"" function in python avoid this by using**kwargs, but we want type checking so can't directly use the samethere.This uses the same pattern that configparser does to allow falsey (0,False) and even `None` as valid values, distinct from not being passed.",4
"Fixes check_migrations commands (#8407)Fixes bug where old ""wait_for_migrations"" function nameis left in the lazy loader of the cli_parserCo-authored-by: Daniel Imberman <daniel@astronomer.io>",1
Fix building image manifest (was removed by accident with prod img) (#8408),4
Fix Migration for MSSQL (#8385),0
Update the tree view of dag on Concepts Last Run Only (#8268)Resolves #8246,0
"Fix error message in production entrypoint.sh (#8396)Fix non-existent BACKEND environment variable, replace with DB_URL",5
Less frequent dockerhub builds (#8400),2
Clean up temporary files in Dataflow operators (#8313),1
Use less fancy tables in CLI by default (#8409),1
"Add a dedicated ""free disk space"" step to fix CI (#8426)",0
Make doc clearer about Airflow Variables using Environment Variables (#8427),1
Fix subcommand error when running production image without argument (#8415)Co-authored-by: Liang Hao <liahao@tesla.com>,1
"stop rendering some class docs in wrong place (#8095)* stop rendering some class docs in wrong placeDocs generated for providers.yandex.hooks incorrectly include docsfor airflow.exceptions.AirflowException andairflow.hooks.base_hook.BaseHook in the yandex module, as ifthose classes had been defined in that module.This is almost certainly a bug in autoapi or one of the librariesit uses, but I haven't tracked down the root cause. In the meantime,importing the modules and then referring to the classes usingmodulename.Classname avoids the issue.",0
Remove duplicate dependency ('curl') from Dockerfile (#8412),2
fixed typo in confirm script (#8419)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,5
Add back-compat modules from 1.10.10 for SecretsBackends (#8413),1
[AIRFLOW-5156] Added auth type to HttpHook (#8429),1
[AIRFLOW-5156] Fixed doc strigns for HttpHook (#8434),1
Fix performance degradation when updating dagrun state (#8435)Co-authored-by: Dan Frank <dan.frank@coinbase.com>,2
Added location parameter to BigQueryCheckOperator (#8273),1
Fix formatting of Pool docs in concepts.rst (#8443),2
Not pulling base python images does not work for bugfixes (#8437),0
Improve language in Pod Mutation Hook docs (#8445),2
Make KubernetesPodOperator clear in docs (#8444)Avoid confusion with https://kubernetes.io/docs/concepts/extend-kubernetes/operator/,2
Fix formatting in Lineage docs (#8446),2
Simplify mocking in BigQueryHook tests (#8450),3
[AIRFLOW-1536] Set a default value or read from cfg/args for setting DaemonContext in worker (#7724),1
Added more precise Python requirements to README.md (#8455),2
Remove unittest.main() from tests (#8454),3
Remove unrelated EC2 references in ECSOperator (#8451),1
Allow only adding labels on PR Creation (not Updation) (#8461),5
Allow multiple extra_packages in Dataflow (#8394)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,5
Fix case when the pidfile is empty during the pidfile check (#8462),2
Return non-zero error code when variable is missing(#8438),0
"Divide commands into ""Actions""/""Groups"" sections (#8456)",5
Allow retrieving Connections from Secrets Backend using CLI (#8440)Co-Authored-By: Kaxil Naik <kaxilnaik@gmail.com>,1
Fix Snowflake hook conn id (#8423)Do not set manually to ensure the base `DbApiHook` correctly interpretsarguments based on `conn_name_attr` and `default_conn_name`.,5
Add Airflow Users for Optum (#8478),1
Pin Hadolint to version released 2020.04.20 (#8485),5
Fix awkward log info in dbapi_hook (#8482)Co-authored-by: Liang Hao <liahao@tesla.com>,5
"Pre-commit checks in Github Actions have colour and are simpler (#8486)We have now back color in pre-commit checks and we disabled verboseflag. The main reason for verbose flag was to show execution timeof each commit, but this is not needed in Github Actions as wehave the ""Show Timestamp"" option in job log which givesus all that we need. Also the id of failing pre-commit checkis displayed whenever it fails so we do not need --verbose flagany more.",0
Update to latest pygrep pre-commit hook (#8489),1
Get rid of Travis CI from the docs (#8488),2
Fix typo in UPDATING.md (#8493),5
Use existing DagBag for 'dag_details' & `trigger` Endpoints (#8501),2
Add scripts/list-integrations.py (#8469),1
[AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook (#8380)* [AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook* [AIRFLOW-7059] pass hive_conf to get_pandas_df in HiveServer2Hook* Use Github Actions to run CI (#8376)* Use Github Actions to run CI* Fix backports buildCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,0
Use python client in BQ hook create_empty_table/dataset and table_exists (#8377)* Use python client in BQ hook create_empty_table method* Refactor table_exists and create_empty_dataset* Add note in UPDATING,5
WTForms 2.3.0 break our Flask apps (#8512)* WTForms 2.3.0 break our Flask apps,4
Remove unused session variable from www/view.py (#8504),1
List of integrations is now maintained in one place. (#8496),5
Add installation description for repeatable PyPi installation (#8513),1
"Fix too long comment in setup.py (#8515)Fixes Bad commit via Github ""suggestion"" #8512",0
"Add ""please use search"" in bugfix template (#8492)* Please use search before creating bug issue* fixup! Please use search before creating bug issue",0
Add support for caching of image in GitHub's registry (#8497),1
Remove WTforms from setup.py (#8522),1
"Optimize GitLab CI configuration (#8499)* tests are not executed for doc-only changes* images will be (once merged) downloaded from GitHub Registry so likely much faster* we have a ""scheduled"" nightly build that will build everything from scratch and check if no requirements have been broken* improved split of static checks between two static check jobs - to utilise parallelism better.* reorganised some fast jobs (requirements, prod image) that do not depend on tests so that they can run earlier* shorter names for jobs so that they are nicer to view in the actions view* matrix definitions of the jobs so that we can manage them better",1
Move some tests to quarantine (#8511),3
Added GoogleDisplayVideo360DownloadLineItemsOperator (#8174)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,1
Pass location using parmamter in Dataflow integration (#8382),5
"Add Simply Business to the documentation-list of ""companies using Airflow"" (#8516)* moved to a better place* renamed name",1
Development PR template target issue if exists (#8524),0
[AIRFLOW-4357] Fix SVG tooltip positioning with custom scripting (#8269),0
Add Zerodha to Airflow users list (#8532),1
[AIRFLOW-8474]: Adding possibility to get job_id from Databricks run (#8475)* [AIRFLOW-8474]: Adding possibility to get job_id from Databricks runCo-Authored-By: Jiajie Zhong <zhongjiajie955@hotmail.com>,1
[AIRFLOW-7111] Add generate_presigned_url method to S3Hook (#8441),1
fix help message display for dags test subcommand (#8552),3
Fix --forward-credentials flag in Breeze (#8554),0
Add hook and operator for Google Cloud Life Sciences (#8481),1
The CRON job now is working and triggers builds on DockerHub (#8549)The CRON job from previous runs did not have everything workingafter the emergency migration to Github Actions.This change brings back following improvements:* rebuilding images from the scratch in CRON job* automatically upgrading all requirements to test if they are new* pushing production images to github packages as cache* pushing nightly tag to github,1
Add test to guard against command arg help message regression (#8561)follow up for PR #8552.,3
[AIRFLOW-6281] Create guide for GCS to GCS transfer operators (#8442),1
Add example DAG for ECSOperator (#8452),1
This test (test_mark_success_on_success_callback) fails often (#8563)I am moving the test to quarantine for now.,3
"Retry initialization of environment in case it fails (#8555)Sometimes the docker-compose environment fails to initialise. It isone of two problems:* DNS fwd/rev mismatch on checking the docker compose integration* Missing name for the docker compose intagrationThis is likely due to some race conditions when setting up dockercompose network and DNS. It happens very rarely and it seems thatit's not easily reproducible. It is very likely that detectingthis case, cleaning the docker environment and retrying willwork fine. This is what this PR tries to attempt",1
Building backport packages is now done inside Breeze container (#8558)This way you do not have to worry about setting up your environment.Fixes #8537,0
"Fixed SOURCE_BRANCH set by tag names (#8562)SOURCE_BRANCH was set to TAG_NAME in Dockerhub. Now that we changedDockerHub to build from tags, it caused AIRFLOW_BRANCH argto be set to master-nightly instead of master and we had a cachemiss.SOURCE_BRANCH variable is not realy needed any more. Removed it",4
"Improve idempodency in CloudDataTransferServiceCreateJobOperator (#8430)* Fix issue #8285- add JOB_NAME to available request body keys- check error on creating new transfer- add tests- add gen_job_name function, that generates suffix if name was deleted (this behavior is not completed)- add method enable_transfer_job- add gen_job_name function, that generates suffix if job was deleted- change CloudDataTransferServiceCreateJobOperator documentation- pylint minor fix- code review fix on gen_job_name function- Fix build docs error- Rename some inner variables for pylint- Remove unused import- Stylefix after precommit- refactor test- refactor gen_job_name() method",4
[AIRFLOW-5850] Capture task logs in DockerSwarmOperator (#6552)* [AIRFLOW-5850] Capture task logs in DockerSwarmOperator* [AIRFLOW-5850] Fix the mock in the docker swarm tests* [AIRFLOW-5850] Squash me: Remove nested blocks in docker swarm operator,1
AIRFLOW-6062 Watch worker pods from all namespaces (#8546),1
"Monkey patch greenlet celery pools (#8559)Celery pools of type eventlet and gevent use greenlets,which requires monkey patching the app:https://eventlet.net/doc/patching.html#monkey-patchOtherwise task instances hang on the workers and are neverexecuted.",1
File should be specified in docker compose before command (#8569),2
Add test_localtaskjob_maintain_heart_rate to quarantine (#8566),3
[AIRFLOW-4438] Add Gzip compression to S3_hook (#8571)Fixes a bug introduced in #7680 with passing filename as string,2
User-friendly error messages when the configuration is incorrect (#8463)* Clearer error messages when the configuration is incorrect,5
Bash command for production image (#8579),5
Add Local Filesystem Secret Backend (#8436),5
Fix json string escape in tree view (#8551)close #8523.,5
Update documentation with clarification about v1-10-stable (#8588)The v1-10-test branch does not have it's own cron job. TheDockerHub build is triggered after manual pushing of v1-10-stable.,2
Pin Pylint to 2.4.4 (#8592)Pylint 2.5.0 was released which has added bunch of new rules & checks.Pinning to 2.4.4 till we check the new rules & disable the ones we don't want to use,1
[AIRFLOW-6796] Clean up DAG serializations based on last_updated (#7424)DAG serializations were previous deleted based on whether theDagFileProcessorManager had processed a particular python file.  Thischanges that to be based on the last time a DAG was processed by thescheduler.Also moves cleaning up of stale dags to the DagFileProcessorManager tosupport long running schedulers,1
Remove WTforms from setup.py (#8590),1
Fix typo in error message - systems -> system (#8584),5
[AIRFLOW-8472]: `PATCH` for Databricks hook `_do_api_call` (#8473)Support of `PATCH` for Databricks hook `_do_api_call` function,1
Stop DockerSwarmOperator from pulling Docker images (#8533)Co-authored-by: Jonas Natzer <jonas.natzer@celus.io>,2
Add back-compat secrets to TestMovingCoreToContrib (#8593),3
fix: aws hook should work without conn id (#8534)This patch makes behavior of hook consistent with documentation.AWS hooks should support falling back to using default credential chainlookup behavior when connection id is not specified.add test for conn_id equals Nonemore elegant way to set role session name,1
Fix example for Local Filesystem Secrets Backend (#8597),5
"Fixed optimistions of non-py-code builds (#8601)Count of changed files returned exit code 1 not 0 when therewas no files matching the pattern, so the whole optimisationdid not work as intended.",1
Chown should work now when building the documentation (#8600),2
Refactor BigQueryHook dataset operations (#8477)* Refactor database methods of BigQueryHook* fixup! Refactor database methods of BigQueryHook* Apply suggestions from code reviewCo-Authored-By: Michał Słowikowski <michalslowikowski00@gmail.com>* fixup! fixup! Refactor database methods of BigQueryHook* fixup! fixup! fixup! Refactor database methods of BigQueryHook* Pass credentials to BQ clientCo-authored-by: Michał Słowikowski <michalslowikowski00@gmail.com>,4
"Split and improve BigQuery example DAG (#8529)* Simplify existing example DAG and add consistent UI colors* Move BigQueryConsoleLink to top* Split DAG into two separate* Add transfer example* Fix check interval operatorCould not cast literal {{ macros.ds_add(ds, -1) }} to type DATE at [1:51]'}], 'state': 'DONE'}}* Add BigQuery queries example* fixup! Add BigQuery queries example",1
Allow to define custom XCom class (#8560)* Allow to define custom XCom classcloses: #8059,1
Pin Flask-Appbuilder to 2.3.2 (#8602),5
Fix sphinx add_stylesheet deprecation warning (#8627)See https://www.sphinx-doc.org/en/master/extdev/appapi.html#sphinx.application.Sphinx.add_css_file for more information.,5
Add http system test (#8591),3
"[AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625)PostgresHook's parent class, DbApiHook, implements upsert in its insert_rows() methodwith the replace=True flag. However, the underlying generated SQL is specific to MySQL's""REPLACE INTO"" syntax and is not applicable to PostgreSQL.This pulls out the sql generation code for insert/upsert out in to a method that is thenoverridden in the PostgreSQL subclass to generate the ""INSERT ... ON CONFLICT DOUPDATE"" syntax (""new"" since Postgres 9.5)",1
"Fix the process of requirements generations (#8648)Right now requirements will be only checked during theCI build if the setup.py has changed and if yes, clear instructionswill be given. The diff will still be printed otherwise butit will not cause the job to fail",0
"Reduce response payload size of /dag_stats and /task_stats (#8633)Their response format is like {""example_dag_id"": [{""state"": ""success"", ""dag_id"": ""example_dag_id""}, ...], ...}The dag_id is already used as the ""key"", but still repeatedly appear in each element,which makes the response payload size unnecessarily bigger",1
"Improve template capabilities of EMR job and step operators (#8572)Allow EmrCreateJobFlowOperator and EmrAddStepsOperatorto receive their 'job_flow_overrides', and 'steps'arguments respectively as Jinja template filenames.This is similar to BashOperator's capability ofreceiving a filename as its 'bash_command' argument.",2
Enhanced documentation around Cluster Policy (#8661),2
[AIRFLOW-4363] Fix JSON encoding error (#8287),0
"Add check for pre-2.0 style hostname_callable config value (#8637)Changes deprecated config check rules. Now uses regex to look for an old pattern in the val. Updates 'hostname_callable'.This lets us pull the change back in to 1.10.x, so that by the time 2.0 is around people will have had time and notice to update, without reading (the now quite long) UPDATING.md.Depends on #8463",5
"Fix displaying Executor Class Name in ""Base Job"" table (#8679)",0
Persist start/end date and duration for DummyOperator Task Instance (#8663)Otherwise the behaviour in UI is incorrectAddressing issue https://github.com/apache/airflow/issues/8662,0
"Ensure ""started""/""ended"" in tooltips are not shown if job not started (#8667)",5
"Add support for fetching logs from running pods (#8626)When using KubernetesExecutor without any centralized PV for log storage, one has to wait until the logs get uploaded to cloud storage before viewing them on UI. With this change, the webserver will try to fetch logs from running worker pods and display them.",1
Remove _get_pretty_exception_message in PrestoHookFix #8530,0
Improve tutorial - Include all imports statements (#8670),2
Group Google services in one section (#8623),5
Refactor test_variable_command.py (#8535),3
Add system test and docs for Facebook Ads operators (#8503),1
"Fix connection add/edit for spark (#8685)connection add/edit UI pages were not working correctly for Spark connections. The root-cause is that ""spark"" is not listed in models.Connection._types.So when www/forms.py tries to produce the UI,""spark"" is not available and it always tried to ""fall back"" to the option listwhose first entry is ""Docker""In addition, we should hide irrelevant entries forspark connections (""schema"", ""login"", and ""password"")",4
"Sort connection type list in add/edit page alphabetically (#8692)Currently the connection type list in the UI is sorted in the original order of`Connection._types`, which may be a bit inconvenient for users.It would be better if it can be sorted alphabetically.",1
Support k8s auth method in Vault Secrets provider (#8640),1
Add system test for gcs_to_bigquery (#8556),3
[AIRFLOW-7008] Add perf kit with common used decorators/contexts (#7650),1
Invalid output in test_variable assertion (#8698),3
Change provider:GCP to provider:Google for Labeler Bot (#8697),1
Remove config side effects from tests (#8607)* Remove config side effects* Fix LatestOnlyOperator return type to be json serializable* Fix tests/test_configuration.py* Fix tests/executors/test_dask_executor.py* Fix tests/jobs/test_scheduler_job.py* Fix tests/models/test_cleartasks.py* Fix tests/models/test_taskinstance.py* Fix tests/models/test_xcom.py* Fix tests/security/test_kerberos.py* Fix tests/test_configuration.py* Fix tests/test_logging_config.py* Fix tests/utils/test_dag_processing.py* Apply isort* Fix tests/utils/test_email.py* Fix tests/utils/test_task_handler_with_custom_formatter.py* Fix tests/www/api/experimental/test_kerberos_endpoints.py* Fix tests/www/test_views.py* Code refactor* Fix tests/www/api/experimental/test_kerberos_endpoints.py* Fix requirements* fixup! Fix tests/www/test_views.py,3
Check consistency between the reference list and howto directory (#8690),5
Prevent clickable sorting on non sortable columns in TI view (#8681)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,2
Import Connection directly from multiprocessing.connection. (#8711)Co-authored-by: James Timmins <james@astronomer.io>,2
Fix typo in Google Display & Video 360 guideCo-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,2
Carefully parse warning messages when building documentation (#8693),2
Support num_retries field in env var for GCP connection (#8700),1
Add __repr__ for DagTag so tags display properly in /dagmodel/show (#8719),2
Add guide for Apache Spark operators (#8305),1
Fix pickling failure when spawning processes (#8671)* Pull processor_factory out of _execute and move to the class scope.* Change the value of pickle_dags from True to False which is the default value of pickle_dags to be passed.* Fix how to reference FakeDagFileProcessorRunner class due to the place where it's defined is changed.* Fix configuration inheritance issue using  multiprocessing with spawn mode.* Add a new CI entry for spawn-multiprocessing method.* Add testcases for multiprocessing with spawn mode.Co-authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>Co-authored-by: James Timmins <james@astronomer.io>,5
Support all RuntimeEnvironment parameters in DataflowTemplatedJobStartOperator (#8531),5
Add  jinja template test for AirflowVersion (#8505),3
Avoid loading executors in jobs (#7888),5
Optimize count query on /home (#8729),5
"Correctly deserialize dagrun_timeout field on DAGs (#8735)We weren't deserializing this correctly (it was left as a float) butnothing _was_ using it, and we hadn't explicitly tested it.We already have example dags with this field, so we just need to checkfor this field.",2
Stop Stalebot on Github issues (#8738),0
Make loading plugins from entrypoint fault-tolerant (#8732),1
Ensure test_logging_config.test_reload_module works in spawn mode. (#8741)Co-authored-by: James Timmins <james@astronomer.io>,1
Latest debian-buster release broke image build (#8758),3
Add google_api_to_s3_transfer example dags and system tests (#8581)- add amazon system helper for easier testing amazon aws systems / services- fix TESTING docs,2
Add google_api_to_s3_transfer docs howto link (#8761),2
Add documentation for SpannerDeployInstanceOperator (#8750),1
"[AIRFLOW-4568]The ExternalTaskSensor should be configurable to raise an Airflow Exception in case the poked external task reaches a disallowed state, such as f.i. failed (#8509)Added failed_states for ExternalTaskSensor to avoid waiting for the failure scenarios till timeout value",0
Add SQL query tracking for pytest (#8754),3
Added SDFtoGCSOperator (#8740)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,1
Patch Pool.DEFAULT_POOL_NAME in BaseOperator (#8587)Co-authored-by: Vishesh Jain <visheshj@twitter.com>,1
Support same short flags for `create user` as 1.10 did for `user_create` (#8763)This will make it easier to transition between versions.* fixup! Support same short flags for `create user` as 1.10 did for `user_create`,1
Add WorldRemit as Airflow user (#8786),1
fix typing errors reported by dmypy (#8773),0
"Update example SingularityOperator DAG (#8790)The main thing I was fixning here was `start_date=utcnow()` which isalways going to be wrong (discovered via a test in #8772).While I was updating the DAG I updated it to use context manager, andshift operators.",1
Backport packages are renamed to include backport in their name (#8767),5
Fixed test-target command (#8795)Wrong parameter sequence were passed to the run script.,1
Make celery worker_prefetch_multiplier configurable (#8695),5
[AIP-31] Implement XComArg to pass output from one operator to the next (#8652)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>Co-authored-by: Evgeny Shulman <evgeny.shulman@databand.ai>,5
Add default `conf` parameter to Spark JDBC Hook (#8787),1
Fix docs on creating CustomOperator (#8678),1
Document default timeout value for SSHOperator (#8744),1
[AIRFLOW-5906] Add authenticator parameter to snowflake_hook (#8642),1
Add imap_attachment_to_s3 example dag and system test (#8669),3
"Correctly store non-default Nones in serialized tasks/dags (#8772)The default schedule_interval for a DAG is `@daily`, so`schedule_interval=None` is actually not the default, but we were notstoring _any_ null attributes previously.This meant that upon re-inflating the DAG the schedule_interval wouldbecome @daily.This fixes that problem, and extends the test to look at _all_ theserialized attributes in our round-trip tests, rather than just the fewthat the webserver cared about.It doesn't change the serialization format, it just changes what/whenvalues were stored.This solution was more complex than I hoped for, but the test case intest_operator_subclass_changing_base_defaults is a real one that theround trip tests discovered from the DatabricksSubmitRunOperator -- Ihave just captured it in this test in case that specific operatorchanges in future.",4
"Correctly restore upstream_task_ids when deserializing Operators (#8775)This test exposed a bug in one of the example dags, that wasn't caughtby #6549. That will be a fixed in a separate issue, but it caused theround-trip tests to fail hereFixes #8720",0
Allow passing backend_kwargs to AWS SSM client (#8802),4
Added Upload Multiple Entity Read Files to specified big query dataset (#8610)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,5
Remove old airflow logger causing side effects in tests (#8746),3
Add comments to breeze scripts (#8797),1
Add separate example DAGs and system tests for google cloud speech (#8778),3
"Avoid color info in response of /dag_stats & /task_stats (#8742)* Avoid color info in response of /dag_stats & /task_statsCurrently the color for each state is repeatedly appearingin the response payload of endpoints /dag_stats and /task_stats.This can be avoided by having the mapping between state and color in thestatic file, and map each state into different color at client side afterclient receives the necessary info, instead of passing duplicatedcolor information in the response payload.This significantly reduces the size of data to be transferred fromserver to client.",5
Support cron presets in date_range function  (#7777),1
[AIRFLOW-6921] Fetch celery states in bulk (#7542),5
Useful help information in test-target and docker-compose commands (#8796)There was no useful information printed in test-target anddocker-compose commands. It's fixed now,0
Fix bash command in performance test dag (#8812),2
"[AIRFLOW-7068] Create EC2 Hook, Operator and Sensor (#7731)Co-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>",1
"Option to set end_date for performance testing dag. (#8817)I wanted an option to run a specific number of dag runs to completion,so this feature lets me control the end_date of the dag without havingto know exactly what value it would have.The ""is string"" check is more simplistic than it needs to be, but it'sGood Enough for now for an optional feature.",2
"[AIRFLOW-4549] Allow skipped tasks to satisfy wait_for_downstream (#7735)Previously, tasks that were in SUCCESS or SKIPPED state satisfy thedepends_on_past check, but only tasks that were in the SUCCESS statesatisfy the wait_for_downstream check. The inconsistency in behaviormade the API less intuitive to users.",1
Synchronize extras between airflow and providers (#8819),1
Add option to propagate tags in ECSOperator (#8811)Co-authored-by: Joao Ponte <jpe@plista.com>,1
Use fork when test relies on mock.patch in parent process. (#8794)* Use 'fork' in test bc 'spawn' breaks mocks.* Use fork when making process w test_scheduler_executor_overflow.,3
[AIRFLOW-1156] BugFix: Unpausing a DAG with catchup=False creates an extra DAG run (#8776),1
Fix typo. 'zobmies' => 'zombies'. (#8832),2
Add support for non-default orientation in `dag show` command (#8834),2
"Access function to be pickled as attribute, not method, to avoid error. (#8823)* Access function to be pickled as attribute, not method, to avoid error.* Access type attribute to allow pickling.* Use getattr instead of type(self) to fix linting error.",0
Refactor BigQuery check operators (#8813)* Refactor BigQuery check operatorsThis commit applies some code formatting to existing BigQuerycheck operators. It also adds location parameter toBigQueryIntervalCheckOperator and BigQueryValueCheckOperator.* fixup! Refactor BigQuery check operators,1
Fix Flake8 errors (#8841),0
Fix template fields in Google operators (#8840),1
Azure storage 0.37.0 is not installable any more (#8833)2020.05.12 release of the azure-storage is not installable any more(it is deprecated). For now we should switch to latest workingversion,1
[AIRFLOW-4543] Update slack operator to support slackclient v2 (#5519)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <8811558+kaxil@users.noreply.github.com>,1
[AIRFLOW-2310] Enable AWS Glue Job Integration (#6007)Co-authored-by: olalekanelesin <elesin.olalekan@gmail.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Refactor BigQuery hook methods to use python library (#8631)* Refactor create_external_table* Refactor patch_table and add update_table* Refactor run_grant_dataset_view_access* Refactor run_upsert_table* Refactor insert_all* Refactor delete_table* Refactor list_rows* Fix types* Fix test* Refactor poll_job_complete* Refactor cancel_query* Refactor run_with_configuration* Refactor run_* methods* Fix self.project_id issue* Fixup run_table_upsert,1
Remove duplicate code from perf_kit (#8843),4
Update GoogleBaseHook to not follow 308 and use 60s timeout (#8816),1
Fix Environment Variable in perf/scheduler_dag_execution_timing.py (#8847),2
Correctly pass sleep time from AWSAthenaOperator down to the hook. (#8845)Sleep time in AthenaHook was defined as a kwarg-only key.This one change makes tests go from 270s to 0.3s :D,3
"The librabbitmq library stopped installing for python3.7 (#8853)When preparing backport relases I found that rabbitmq was notincluded in the ""devel_ci"" extras. It turned out that librabbitmq wasnot installing in python3.7 and the reason it turned out to bethat librabbitmq is not maintained for 2 years already and ithas been replaced by py-amqp library. The pythhon py-amqplibrary has been improved using cython compilation, so itbecame production ready and librabbitmq has been abandoned.We are switching to the py-amqp library here and addingrabbitmq back to ""devel_ci"" dependencies.Details in: https://github.com/celery/librabbitmq/issues/153",0
"Convert tests/jobs/test_base_job.py to pytest (#8856)I would like to (create) and use a pytest fixture as a parameter, butthey cannot be used on unittest.TestCase functions:> unittest.TestCase methods cannot directly receive fixture arguments as> implementing that is likely to inflict on the ability to run general> unittest.TestCase test suites.",3
Add metric for monitoring email notification failures (#8771),0
"Relax Flask-Appbuilder version to ~=2.3.4 (#8857)""Bump jQuery to 3.5"" was reverted. And so we can upgrade and remove email_validator dependencySee also: https://github.com/dpgaspar/Flask-AppBuilder/blob/master/CHANGELOG.rst#improvements-and-bug-fixes-on-234",4
Add AWS EMR System tests (#8618)- add create_emr_default_roles to amazon_system_helpers.py,5
Do not create a separate process for one task in CeleryExecutor (#8855),1
Make Custom XCom backend a subsection of XCom docs (#8869),2
"Don't use ProcessorAgent to test ProcessorManager (#8871)Some of our tests (when I was looking at another change) were using theProcessorAgent to run and test the behaviour of our ProcessorManager incertain cases. Having that extra process in the middle is not criticalfor the tests, and makes it harder to debug the problem when ifsomething breaks.To make this possible I have made a small refactor to the loop ofDagFileProcessorManager (to give us a method we can call in tests thatdoesn't do `os.setsid`).",1
"Create log file w/abs path so tests pass on MacOS (#8820)* Set conf vals as env vars so spawned process can access values.* Create custom env_vars context manager to control simple environment variables.* Use env_vars instead of conf_vars when using .* When creating temporary environment variables, remove them if they didn't exist.",4
"Fix list formatting of plugins doc. (#8873)This was causing it to be picked up as a `<dl>/<dd>` containing a list,instead of a paragraph and a list.```<dl class=""simple"">  <dt>This will create a hook, and an operator accessible at:</dt>  <dd>    <ul class=""simple"">      <li><p><code><span class=""pre"">airflow.hooks.my_namespace.MyHook</span></code></p></li>      <li><p><code><span class=""pre"">airflow.operators.my_namespace.MyOperator</span></code></p></li>    </ul>  </dd></dl>```",1
Add EMR operators howto docs (#8863),2
"Fix KubernetesPodOperator pod name length validation (#8829)* Fix KubernetesPodOperator pod name length validation* Add test, verify Exception is raised",3
Added automated release notes generation for backport operators (#8807)We have now mechanism to keep release notes updated for thebackport operators in an automated way.It really nicely generates all the necessary information:* summary of requirements for each backport package* list of dependencies (including extras to install them) when package  depends on other providers packages* table of new hooks/operators/sensors/protocols/secrets* table of moved hooks/operators/sensors/protocols/secrets with  information where they were moved from* changelog of all the changes to the provider package (this will be  automatically updated with incremental changelog whenever we decide to  release separate packages.The system is fully automated - we will be able to produce release notesautomatically (per-package) whenever we decide to release new version ofthe package in the future.,1
"Spend less time waiting for DagFileProcessor processes to complete (#8814)In debugging another test I noticed that the scheduler was spending along time waiting for a ""simple"" dag to be parsed. But upon closerinspection the parsing process itself was done in a few milliseconds,but we just weren't harvesting the results in a timely fashion.This change uses the `sentinel` attribute of multiprocessing.Connection(added in Python 3.3) to be able to wait for all the processes, so thatas soon as one has finished we get woken up and can immediately harvestand pass on the parsed dags.This makes test_scheduler_job.py about twice as quick, and also reducesthe time the scheduler spends between tasks .In real work loads, or where there are lots of dags this likely won'tequate to much such a huge speed up, but for our (synthetic) elasticperformance test dag.These were the timings for the dag to run all the tasks in a single dagrun to completion., with PERF_SCHEDULE_INTERVAL='1d' PERF_DAGS_COUNT=1I also havePERF_SHAPE=linear PERF_TASKS_COUNT=12:**Before**: 45.4166s**After**: 16.9499sPERF_SHAPE=linear PERF_TASKS_COUNT=24:**Before**: 82.6426s**After**: 34.0672sPERF_SHAPE=binary_tree PERF_TASKS_COUNT=24:**Before**: 20.3802s**After**: 9.1400sPERF_SHAPE=grid PERF_TASKS_COUNT=24:**Before**: 27.4735s**After**: 11.5607sIf you have many more dag **files**, this likely won't be your bottleneck.",2
JIRA and Github issues explanation (#8539),0
"Speed up TestAwsLambdaHook by not actually running a function (#8882)Moto's mock_lambda _actually runs the code_ in a docker container. Thisis useful if you are testing a Lambda function but is massively overkillfor testing that we make a request to a function -- Airflow doesn't carewhat the function does.This is our slowest individual test in CI right now, taking 20s onGithub Actions.",3
Check for same task instead of Equality to detect Duplicate Tasks (#8828),2
Fix master failing on generating requirements (#8885)By default github actions checks out only latest commit but in order tosee if there are any changes since the last readme generatedwe need to see the whole history so we need to fetch it all.We also skip generating the new README in case there is only onecommit in the history since the last release. The nature of readmegeneration is that the commit with the README itself will neverbe in the list of commits for the previous release so there isalways at least one commit more than the one listed in the readme.,1
Regenerate readme files for backport package release (#8886),2
Updated docs for experimental API /dags/<DAG_ID>/dag_runs (#8800),2
"[AIRFLOW-6535] Add AirflowFailException to fail without any retry (#7133)* use preferred boolean check idiomCo-Authored-By: Jarek Potiuk <jarek@potiuk.com>* add test coverage for AirflowFailException* add docs for some exception usage patterns* autoformatting* remove extraneous newline, poke travis build* clean up TaskInstance.handle_failureTry to reduce nesting and repetition of logic for different conditions.Also try to tighten up the scope of the exception handling ... it lookslike the large block that catches an Exception and logs it as a failureto send an email may have been swallowing some TypeErrors coming outof trying to compose a log info message and calling strftime onstart_date and end_date when they're set to None; this is why I've addedlines in the test to set those values on the TaskInstance objects.* let sphinx generate docs for exceptions module* keep session kwarg last in handle_failure* explain allowed_top_level* add black-box tests for retry/fail immediately cases* don't lose safety measures in logging date attrs* fix flake8 too few blank lines* grammar nitpick* add import to AirflowFailException exampleCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",0
Add Snowflake system test (#8422),3
"Monitor pods by labels instead of names (#6377)* Monitor k8sPodOperator pods by labelsTo prevent situations where the scheduler starts asecond k8sPodOperator pod after a restart, we now checkfor existing pods using kubernetes labels* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* add docs* Update airflow/kubernetes/pod_launcher.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Daniel Imberman <daniel@astronomer.io>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
Added SalesforceHook missing method to return only dataframe (#8565) (#8644)* add feature for skipping writing to file* add SalesforceHook missing method to return dataframe onlyfunction write_object_to_file is divided to object_to_df which returns df and then the write_object_to_file can uses object_to_df as the first step before exporting to file* fixed exception message* fix review comments - removed filename check for None,2
"Prepare release candidate for backport packages (#8891)After preparing the 2020.5.19 release candidate andreviewing the packages, some changes turned out to be necessary.Therefore the date was changed to 2020.5.20 with the folowingfixes:* cncf.kubernetes.example_dags were hard-coded and added for all  packagesa and they were removed* Version suffix is only used to rename the binary packages not for  the version itself* Release process description is updated with the release process* Package version is consistent - leading 0s are skipped in month  and day",5
"Avoid failure on transient requirements in CI image (#8892)When you build from the scratch and some transient requirementsfail, the initial step of installation might fail.We are now using latest valid constraints from the DEFAULT_BRANCHbranch to avoid it.",3
Allow setting the pooling time in DLPHook (#8824)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
"Fix task and dag stats on home page (#8865)d.dag_id is not a valid attribute. in order to use dag_id variablein a closure callback, it needs to be passed in as a fuction so theright value can be captured for each for loop.",4
Release candidate 2 for backport packages 2020.05.20 (#8898)Release candidate 2 for backport packages 2020.05.20,5
"Fix race in Celery tests by pre-creating result tables (#8909)We noticed our Celery tests failing sometimes with> (psycopg2.errors.UniqueViolation) duplicate key value violates unique> constraint ""pg_type_typname_nsp_index""> DETAIL:  Key (typname, typnamespace)=(celery_tasksetmeta, 2200) already existsIt appears this is a race condition in SQLAlchemy's ""create_all()""function, where it first checks which tables exist, builds up a list of`CREATE TABLE` statements, then issues them. Thus if two celery workerprocesses start at the same time, they will find the the table doesn'tyet exist, and both try to create it.This is _probably_ a bug in SQLA, but this should be an easy enough fixhere, to just ensure that the table exists before launching any Celery tasks.",0
[AIRFLOW-6586] Improvements to gcs sensor (#7197)* [AIRFLOW-6586] Improvements to gcs sensorrefactors GoogleCloudStorageUploadSessionCompleteSensor to use set instead of number of objectsadd poke mode only decoratorassert that poke_mode_only applied to child of BaseSensorOperatorrefactor testsremove assert[AIRFLOW-6586] Improvements to gcs sensorrefactors GoogleCloudStorageUploadSessionCompleteSensor to use set instead of number of objectsadd poke mode only decoratorassert that poke_mode_only applied to child of BaseSensorOperatorremove assertfix static checksadd back inadvertently remove requirementspre-commitfix typo* gix gcs sensor unit test* move poke_mode_only to base_sensor_operator module* add sensor / poke_mode_only docs* fix ci check add sensor how-to docs* Update airflow/providers/google/cloud/sensors/gcs.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/sensors/base_sensor_operator.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/sensors/base_sensor_operator.pyCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>* simplify class decorator* remove type hint* add note to UPDATING.md* remove unecessary declaration of class member* Fix to kwargs in UPDATING.mdCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
UX Fix: Prevent undesired text selection with DAG title selection in Chrome (#8912)Negate user-select in Firefox where behavior is already as desired,1
"Use Debian's provided JRE from Buster (#8919)Installing the JDK (not even the JRE) from Sid is starting to break onBuster as the versions of packages conflict:> The following packages have unmet dependencies:> libgcc-8-dev : Depends: gcc-8-base (= 8.4.0-4) but 8.3.0-6 is to be installed>                Depends: libmpx2 (>= 8.4.0-4) but 8.3.0-6 is to be installedThis changes our CI docker images to:1. Not install something from Sid (unstable, packages change/get   updated) when we are using Buster (stable, only security fixes).2. Installed the JRE, not the JDK. We don't need to compile Java code.",0
Fix incorrect Env Var to stop Scheduler from creating DagRuns (#8920),2
Re-run all tests when Dockerfile or Github worflow change (#8924)Fixes #8921,0
Remove unused self.max_threads argument in SchedulerJob (#8935),1
Added Greytip to Airflow Users list (#8887),1
"Hive/Hadoop minicluster needs JDK8 and JAVA_HOME to work (#8938)Debian Buster only ships with a JDK11, and Hive/Hadoop fails in odd,hard to debug ways (complains about metastore not being initalized,possibly related to the class loader issues.)Until we rip Hive out from the CI (replacing it with Hadoop in a seprateintegration, only on for some builds) we'll have to stick with JRE8Our previous approach of installing openjdk-8 from Sid/Unstable startedfailing as Debian Sid has a new (and conflicting) version of GCC/libc.The adoptopenjdk package archive is designed for Buster so should bemore resilient",5
Fix DagRun Prefix for Performance script (#8934),0
Remove side-effect of session in FAB (#8940),4
Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910)Currently there is no way to determine the state of a TaskInstance in the graph view or tree view for people with colour blindnessApproximately 4.5% of people experience some form of colour vision deficiency,1
Fix docstring in DagFileProcessor._schedule_task_instances (#8948),2
"Remove singularity from CI images (#8945)The singularity operator tests _have always_ used mocking, so we wereadding 700MB to our docker image for nothing.Fixes #8774",0
"Update example webserver_config.py to show correct CSRF config (#8944)CSRF_ENABLED does nothing.Thankfully, due to sensible defaults in flask-wtf, CSRF is on bydefault, but we should set this correctly.Fixes #8915",0
Add note in Updating.md about the removel of DagRun.ID_PREFIX (#8949),2
"Python base images are stored in cache (#8943)All PRs will used cached ""latest good"" version of the pythonbase images from our GitHub registry. The python versions inthe Github Registry will only get updated after a masterbuild (which pulls latest Python image from DockerHub) buildsand passes test correctly.This is to avoid problems that we had recently with Pythonpatchlevel releases breaking our Docker builds.",2
"Don't hard-code constants in scheduler_dag_execution_timing (#8950)Slight ""improvement"" on #8949",1
"Make scheduler_dag_execution_timing grok dynamic start date of elastic dag (#8952)The scheduler_dag_execution_timing script wants to run _n_ dag runs tocompletion. However since the start date of those dags is Dynamic (`now- delta`) we can't pre-compute the execution_dates like we were before.(This is because the execution_date of the very first dag run would be`now()` of the parser process, but if we try to pre-compute that inthe benchmark process it would see a different value of now().)This PR changes it to instead watch for the first _n_ dag runs to becompleted. This should make it work with more dags with less changes tothem.",4
Cache 1 10 ci images (#8955)* Push CI images to Docker packcage cache for v1-10 branchesThis is done as a commit to master so that we can keep the two branchesin syncCo-Authored-By: Ash Berlin-Taylor <ash_github@firemirror.com>* Run Github Actions against v1-10-stable tooCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Pin Version of Azure Cosmos to <4 (#8956)Old Repo: https://github.com/Azure/azure-cosmos-pythonNew Repo: https://github.com/Azure/azure-sdk-for-python/tree/master/sdk/cosmos/azure-cosmosazure-cosmos==4.0.0 was released on 20 May 2020 that breaks Airflow,4
Pin google-cloud-datacatalog to <0.8  (#8957)`field_path` was renamed to `tag_template_field_path` in >=0.8 and there might be other unknown errors,0
[AIRFLOW-5262] Update timeout exception to include dag (#8466)* [AIRFLOW-5262] Update timeout exception to include dag* PR comment: extract dag id in log to variable,2
Add context to execution_date_fn in ExternalTaskSensor (#8702)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,5
Add support for spark python and submit tasks in Databricks operator(#8846),1
Fix typo in test_project_structure (#8978),3
Remove duplicate line from CONTRIBUTING.rst (#8981),4
Flush pending Sentry exceptions before exiting (#7232)Fixes AIRFLOW-6569 by explicitly flushing pending exceptions priorto calling `os._exit` within the forked task runner.,1
Support YAML input for CloudBuildCreateOperator (#8808),5
Add secrets to test_deprecated_packages (#8979),3
Fix formatting code block in TESTING.rst (#8985),3
Old json boto compat removed from dynamodb_to_s3 operator (#8987),1
Fix references in docs (#8984),2
Fix migration message (#8988),0
Remove defunct code from setup.py (#8982)* Remove defunct code from setup.py,1
added Paranabanco to official company list (#8990),1
Assign area:webserver label to webserver_command.py (#8998),5
[AIRFLOW-8902] Fix Dag Run UI execution date with timezone cannot be saved issue (#8902)Closes #8842,0
All classes in backport providers are now importable in Airflow 1.10 (#8991)* All classes in backport providers are now importable in Airflow 1.10* fixup! All classes in backport providers are now importable in Airflow 1.10* fixup! fixup! All classes in backport providers are now importable in Airflow 1.10,2
Refactor BigQuery operators (#8858)* Refactor BigQueryCreateEmptyTableOperator* Refactor BigQueryCreateExternalTableOperator* Refactor BigQueryDeleteDatasetOperator* Refactor BigQueryCreateEmptyDatasetOperator* Refactor BigQueryGetDataOperator* BigQueryGetDatasetTablesOperator* Refactor BigQueryPatchDatasetOperator* Refactor BigQueryUpdateDatasetOperator* Refactor BigQueryDeleteTableOperator* Refactor BigQueryUpsertTableOperator* Apply cr suggestions* fixup! Apply cr suggestions,0
Finding cross-provider dependencies fails when encoding wrong (#9012)This forces encoding of read python files to utf-8,2
Move setup order check back to pre-commit (#9010)* Move setup order check back to pre-commitThe order check used to be working from pre-commit but then itwas moved to be regular test case. That was a mistakeThe test is super-fast and actually making it use assertEqualswas not very useful and it was very late when you found it out.I changed it to be normal python script which made it works again(it did not work when it was a test because pre-commit does notrun tests - it runs python scripts).The messages printed now are much more informative as well.,5
Better content of backport packages CHANGELOG and INSTALL files (#9013)* Better content of backport packages CHANGELOG and INSTALL filesThe content of Backport Packages CHANGELOG.txt and INSTALL fileshas been updated to reflect that those are not full Airflowreleases.1) Source package:- INSTALL contains only references to preparing backport packages- CHANGELOG.txt contains combined change log of all the packages2) Binary packages:- No INSTALL- CHANGELOG.txt contains changelog for this package only3) Whl packagesNo change* Update backport_packages/INSTALL,5
Fixed name of 20 remaining wrongly named operators. (#8994),1
Preparing for RC3 relase of backports (#9026),5
Add ADDITIONAL_PYTHON_DEPS (#9031)* add build-arg ADDITIONAL_PYTHON_DEPS* Add ADDITIONAL_PYTHON_DEPS example and descriptionCo-authored-by: Fabian Witt <fabian.witt@redheads.de>,1
Add ADDITIONAL_AIRFLOW_EXTRAS (#9032)* Add build-arg ADDITIONAL_AIRFLOW_EXTRAS* Add ADDITIONAL_AIRFLOW_EXTRAS example and description,1
Filter dags by clicking on tag (#8897)Co-authored-by: Zacharya <zacharya19@gmail.com>,2
"[AIRFLOW-5615] Reduce duplicated logic around job heartbeating (#6311)Both SchedulerJob and LocalTaskJob have their own timers and decide whento call heartbeat based upon that. This makes those functions harder tofollow, (and the logs more confusing) so I've moved the logic to BaseJob",2
Additional python extras and deps can be set in breeze (#9035)Closes #8604Closes #8866,1
Add query count test for LocalTaskJob (#8922)* Add query count test for LocalTaskJob* fixup! Add query count test for LocalTaskJob,3
Add script_args for S3FileTransformOperator (#9019)Co-authored-by: Andrej Svec <asvec@slido.com>,2
Add a tip to trigger DAG screen (#9049),2
[AIRFLOW-6231] Display DAG run conf in the list view (#6794),5
detect incompatible docker server version in breeze (#9042),2
"Cancel queued/running builds on second push to PR (#9050)This uses an action from the marketplace to cancel any running buildsfor our main ""CI"" workflow (the only one we have at the moment)",1
"Use production image for k8s tests (#9038)* Use production image for k8s testsThe CI image has become too large to load into KinD,it also only really makes sense to use the production image forintegration tests* nitCo-authored-by: Daniel Imberman <daniel@astronomer.io>",3
Profile hostname for celery executor (#8624)Co-authored-by: yingbo_wang <yingbo.wang@airbnb.com>,2
add example dag and system test for GoogleSheetsToGCSOperator (#9056)* add example dag and system test for sheets_to_gcs* remove sheets_to_gcs in missing example dags,2
Add Company to Airflow Users list (#9061),1
Allow testing any executor with scheduler_dag_execution_timing.py (#9062),2
Add correct description for dst param in LocalFilesystemToGCSOperator (#9055),5
Added test for bigquery sensor (#8986)* added small test for bigquery sensor* removed file from missing_test_files,3
"Add SlackAPIFileOperator impementing files.upload from Slack API (#9004)* Added SlackAPIFileOperator* Added usage example in docstring* Added tests for SlackAPIFileOperator, fixed extra line and added missing type hinting in operators/slack.py* Fixed import order for isort* Refactor conn_id for slack_conn_idCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Remove # from channel name for default channel nameCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Mikaël Ducharme <mikaelducharme@effenco.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",4
add separate example dags and system tests for GCSToGoogleSheetsOperator (#9066)* add separate example dag and system test for GCSToGoogleSheetsOperator* remove gcs_to_sheets from missing example dags* fix doc error,0
Add example dag and system test for LocalFilesystemToGCSOperator (#9043),5
Add Delete/Create S3 bucket operators (#8895),1
"Improve test for the next_execution cli command (#9058)* Improve test for the next_execution cli commandIt still tests the same functionality, but it is now more efficient andreadable. The changes are: - The DB is cleaned only once instead of eight times - Use a fixed datetime instead of timezone.utcnow (deterministic) - Use redirect_stdout instead of subprocess - Clean up pylint warning - Test None output once instead of four times* Address PR commentsIssues addressed: - Use create_session - Use DagRunType.MANUAL.value - Remove DagRuns created by the test* Remove test from quarantineCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>",3
Improve SchedulerJob code style (#9018)* Small style changes in BaseJob* Small code improvements in SchedulerJob* fixup! Small code improvements in SchedulerJob* fixup! fixup! Small code improvements in SchedulerJob,1
Provide_context coma is added only when there is not one already (#9064),1
Remove not-existing files in pylint_todo.txt (#9073),5
Remove Travis CI badge from README (#9074)* Remove Travis CI badge from README* fixup! Remove Travis CI badge from README,4
"Test that DagFileProcessor can operate against on a Serialized DAG (#8739)As part of the scheduler HA work we are going to want to separate theparsing from the scheduling, so this changes the tests to ensure thatthe important methods of DagFileProcessor can do everything the need towhen given a SerializedDAG, not just a DAG. i.e. that we have correctlyserialized all the necessary fields.",2
Adds hive as extra in pyhive (#9075)Seems that apache hive needs to install [hive] extra of pyhivein order to be usable ¯\_(ツ)_/¯.Fixes: #8933,0
Prevents failure on fixing permissions for files with space in it (#9076),2
Support properties in plugins (#9002),1
Add metric for job start/end task run (#8680)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,1
"Create guide for Dataproc Operators (#9037)* added documentation for dataproc* added more update information for updateMask* Added link to information about cluster config api request* Apply naming convention* Set all dedents from 4 to 0* Adjust dedent to 4, for operators* removed dataproc guide from test_missing_guides",3
Test queries when number of active DAG Run is not zero (#9082),1
"Add displaying multiple dates in airflow next_execution command (#9072)The ""next_execution"" cli sub-command now accepts an optional number ofexecutions to be returned. This is particularly useful for checkingnon-regular schedule intervals, such as those created by some cronexpressions.Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
Update Breeze Documentation to have WSL 2 Instructions instead of WSL 1 (#9057),2
Shorten command help and move long help to command description (#9070)* add description to subcommands and move them to respective subcommands* move some command help to description* add default description* improve text* move some help text to description* improve code,1
Improve TestCliConfig in local environment (#9085),5
Don't create empty modules for plugins (#9078),1
Add BigQueryInsertJobOperator (#8868)* Add BigQueryInsertJobOperator* fixup! Add BigQueryInsertJobOperator* fixup! fixup! Add BigQueryInsertJobOperator* fixup! fixup! fixup! Add BigQueryInsertJobOperator,1
#8525 Add SQL Branch Operator  (#8942)* Add SQL Branch OperatorSQL Branch Operator allow user to execute a SQL query in any supported backend to decide whichbranch to follow. The SQL branch operator expect query to return True/False (Boolean) or0/1 (Integer) or true/y/yes/1/on/false/n/no/0/off (String).,1
"Add OpenAPI specification (II) (#8721)* Add OpenAPI spec (#7549)* Fix typo in name of pre-commit hook* Chaange type for DAGID, DAGRunID, TaskID* Fix typo in summary - POST /pools* Fix typo in description - FileToken parameter* Fix typo - singular/plural form - variables* Make EventLog endpoints read-only* Use ExcutionDate in DagRuns endpoints* Use custom action to control task instances* Typo in  DELETE Task instance* Remove unused schema - DagStructureCollection* Fix typo - singular/plural form - import errors* Add endpoint - POST /dagRuns* Remove job_idWe do not have endpoints to download jobs, because this is an implementation detail, so this field has big no value.* Add filters to GET /taskInstances* Fix typo - upadtePool => updatePool* Rename ""Create a DAG Run"" to ""Trigger a DAG Run""* Use Pool name as a parameter* Add filter to GET /dagRuns* Remove invalid note ion start_date field* Uss POST instead of PATCH for custom action* Remove DELETE /taskInstances endpooint* Rename Xcom Value to xcom Entry* Fix typo in XCCOM Entry endpoint* Change operationID: patchConnection => updaateConnection* Make execution_date optionall in DAGRunThis field can be filled with the current date when creating the DAG Run* Unify connection ID* Use URL with HTTPS and without www.* Fix typo - at database => in database* Fix typo = Raange -> Raange* Fix typo - the specific DAG => a DAG* Fix typo - getXComVEntry => getXComVEntry* Unify collection names - xcomEntries* Move TaskInstance resource under DagRun resource* Fix typo - change tag - TaskInstance => TaskInstanceCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Use path paramaters for /variables/lookup/ endpoint* Use consistent names for IDs* Use new style for filter parameters* Remove unused path parameter* Use ~ as a wildcard charaacter* Add batch endpoints for TaskInstance and DagRuns* Fix typo - response in trigger dag endpoint* Fix typo - Qqueue => Queue* Set dry_run = True in ClearTaskInstance* Mark all fieldss (expcet state) of DagRun as read-only* Use __type as a discriminator* Fix typo - ""The Import Error ID."" => ""The Event Log ID.""* Fix typo - Self referential in EventLogCollection* Rename fieldss - dttm => when* remove fields - pool_id* Fix typo - change request body in PATCH /pools/{pool_name}* Use DAG Run ID as a primary identifier* Fix typo - Change type of query to string* Unify fields names in collections* Use variable key as a primary id* Move collection to /variables* Mark passord as a write only* Fix typo - updaateConnection => updateConnection* Change is_paused/is_subdag to boolean* Fix typo - clearTaskInstaance => clearTaskInstance* Fix typo - DAAG => DAG* Fix typo - many => multiple* Fix typo - missing ""a""* Fix typo - variable by id => variable by key* Fix typo - updateXComEntries => updateXComEntry* Fix typo - missing ""a""* Use dag_run_id as a primary ID* Fix typo - objectss => objects, DAG IDS => DAG IDs* Allows create DAG Run with custom conf/execution_date/dag_run_id* Add new trigger rule, fix typo in dag run state* Add request body to POST/PATCH /dags/{dag_id}* Rename collection fields - dag_model => dags* Fix typo - /clearTaskInstanaces -> /clearTaskInstances* Improve wording - wildcard* Returns owners as a array* Return only references in clear task instances* Remove support for application/x-www-form-urlencoded* fixup! Use __type as a discriminator* Add file_token fields* Move description of variable collections* Return SUB DAG in DAG structure* Fix typo - sucess => sucess, Apache Foundation => Apache Software Foundation, Airfow => Apache Airflow* Improve description of get logs endpoint* Fix typo - Get all XCom entry => Get all XCom entries* Add crossreference between /dags/{dag_id}/structure and /dags/{dag_id}* Remove all form-urllencoded request bodies* Rename parameter - NoChunking => FullContent* Improve description of batch endpoints* Remove request body for GET endpoint* Use allOf insteaad of oneOf* Rename key => xcom_key* Use lowercase letters in query string parameter - Queue -> queue* Change type of conf to object* Change allOf into oneOf for ScheduleIntervalCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",4
Enable configurable git sync depth  (#9094)Enable configurable git sync depth,5
"Don't reuse MY_DIR in breeze to mean different folder from ci/_utils.sh (#9098)scripts/ci/*.sh uses MY_DIR to mean scripts/ci, but in `breeze` MY_DIRis the same as AIRFLOW_SOURCES. When jumping back-and-forth betweenci/_utils.sh, breeze, and ci/ci_*.sh it can be confusing to keep trackof what is what.This changes `breeze` to use `AIRFLOW_SOURCES` to referrer to the toplevel folder instead -- that means I don't have to keep as much contextin my head",1
Updated missing parameters for docker image building (#9039),2
You can push with Breeze as separate command and to cache (#8976)Breeze had --push-images switch to also push images to repobut it was often needed to build and push images separately.We have now a possibility to push an already built image withseparate push-image command instead and also you can chooseto push to cache registry in GitHub rather than to DockerHubwith --registry-cache switch.,1
Allow using Airflow with Flask CLI (#9030),1
"Produce less verbose output when building docker mount options (#9103)The previous method of generating this list had two ""problems""/nigglesthat this PR solves, when running with VERBOSE=true- Firstly, LOCAL_MOUNTS was set at the top level, so running with  `set -x` produced 30 extra lines of output.- Because of the `while read` used, it created 4 or 5 lines _per_ mount,  resulting in a lot verbose output.Nothing I've changed here is ""critical"", it's just making it a biteasier to see with the debug output what is going on, by running fewercommands.I have also expanded the BATS test a little bit to check each pair (`-v`and its following option)",3
"Fix handling of subprocess error handling in s3_file_transform and gcs (#9106)As outlined in Issue 9104 the python subprocess return code can be less than 0.The previous version only captures errors of the subprocess itself, not the negative error codes, as cause by host SIGKILL, SIGHUP, ... see https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess.returncodeWe now treat all non-zero returncodes as a script failure.",0
Add fudament for API based on connexion (#8149),1
Add filepaths for API label in BoringCyborg Bot (#9116),2
Add query count test for SchedulerJob (#9088)* Add query count test for SchedulerJob* fixup! Add query count test for SchedulerJob,3
Remove Hive/Hadoop/Java dependency from unit tests (#9029),3
Move TestDagFileProcessorQueriesCount to quarantine (#9119),3
Add snowflake to slack operator (#9023)Addition of a new `snowflake_to_slack` operator. The operatorallows you to run a SQL statement against Snowflake and renderthe results into a Slack message.* Add snowflake_to_slack operator to example dag* Add type hints to operator and clean example dag,2
"Kubernetes Cluster is started on host not in the container (#8265)Tests requiring Kubernetes Cluster are now moved out ofthe regular CI tests and moved to ""kubernetes_tests"" folderso that they can be run entirely on host without havingthe CI image built at all. They use production imageto run the tests on KinD cluster and we add toolingto start/stop/deploy the application to the KinD clusterautomatically - for both CI testing and local development.This is a pre-requisite to convert the tests to convert thetests to use the official Helm Chart and Docker images orApache Airflow.It closes #8782",2
"Fixes a bug where `build-image` command did not calculate md5 (#9130)Even if image was rebuilt, the md5 files were not updated",5
Fix the command in the documentation - airflow dags backfill (#9128),2
Fix INTEGRATIONS[*]: unbound variable error in breeze (#9135),0
"Cope with multiple processes get_remote_image_info in parallel (#9105)When I'd made a change to a large number of python files, running theflake8 pre-commit hook would fail without obvious error (as in no errorwas printed, but exit code was 1).In debugging this I switch the pre-commit to `require_serial: true` andthe problem went way - the fix for this is:- Don't redirect stderr to /dev/null (that silences both our VERBOSE  trace output, and the errors from docker)- Use `--cidfile` option to docker to create a random name and write the  created container ID to a file",2
Remove remnant kubernetes stuff from breeze scripts (#9138),4
Restrict google-cloud-texttospeach to <v2 (#9137)Version 2 of this library causes the following pylint errors:> airflow/providers/google/cloud/hooks/text_to_speech.py:59:27: E1123: Unexpected keyword argument 'client_info' in constructor call (unexpected-keyword-arg)> airflow/providers/google/cloud/hooks/text_to_speech.py:99:15: E1123: Unexpected keyword argument 'input_' in method call (unexpected-keyword-arg),1
Use static binary linked docker client in CI image (#9126),2
Add run_type to DagRun (#8227)* Add run_type to DagRunfixup! Add run_type to DagRunfixup! fixup! Add run_type to DagRunfixup! fixup! fixup! Add run_type to DagRunfixup! fixup! fixup! Add run_type to DagRunfixup! Add run_type to DagRunfixup! Add run_type to DagRunAdjust TriggerDagRunOperatorfixup! Adjust TriggerDagRunOperatorAdd indexMake run_type not nullableAdd type check for run_typefixup! Add type check for run_type* fixup! Add run_type to DagRun* fixup! fixup! Add run_type to DagRun* Fix migration* fixup! Fix migration,0
Fix sql_to_gcs hook gzip of schema_file (#9140),2
"Remove vendored nvd3 and slugify libraries (#9136)We pulled in them because slugify _used_ to default to the GPL'd`unidecode` module, but since Slugify 3.0[1] it has used text-unidecodeby first (and only installs the GPL library by an optional extra, not bydefault) so we can now use it.This lets us upgreade text-unidecode from 1.2 to the latest 1.3, whichis the version one of dbt's dependencies needs.[1]: https://github.com/un33k/python-slugify/blob/4.0.0/CHANGELOG.md#300",4
Add link to Swagger UI to navbar (#9144),2
Replaces cloud-provider CLIs in CI image with scripts running containers (#9129)The clis are replaced with scripts that will pull and rundocker images when they are needed.Added Azure CLI as well.Closes: #8946 #8947 #8785,1
Split utils sh (#9132)* Split _utils.sh into separate files,2
Add airflow plugin command (#9001),1
[AIRFLOW-6290] Create guide for GKE operators (#8883),1
[AIRFLOW-3607] Optimize dep checking when depends on past set and concurrency limit,1
Add 'main' param to template_fields in DataprocSubmitPySparkJobOperator (#9154),5
[AIRFLOW-5500] Fix the trigger_dag api in the case of nested subdagsCo-authored-by: Charles Bournhonesque <charles.bournhonesque@benevolent.ai>,2
Add 3.8 to the test matrices (#8836),3
Fix typo. 'Depreciation' to 'deprecation'. (#9160),2
[AIRFLOW-XXXX] remove vestigial sentence fragment in changelog (#8864)Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Parameterized bash/python in the prod image (#9157),2
"Add Deseret Digital Media to list of ""who uses Airflow"" (#9163)",1
Add kernel capabilities in DockerOperator(#9142),2
Fix xcom in DockerOperator when auto_remove is used (#9173),1
Check GCP guides on docs build stage on CI (#9171),2
Add note about using dag_run.conf in BashOperator (#9143),1
"Don't use the term ""whitelist"" - language matters (#9174)It's fairly common to say whitelisting and blacklisting to describedesirable and undesirable things in cyber security. However just becauseit is common doesn't mean it's right.However, there's an issue with the terminology. It only makes sense ifyou equate white with 'good, permitted, safe' and black with 'bad,dangerous, forbidden'. There are some obvious problems with this.You may not see why this matters. If you're not adversely affected byracial stereotyping yourself, then please count yourself lucky. For someof your friends and colleagues (and potential future colleagues), thisreally is a change worth making.From now on, we will use 'allow list' and 'deny list' in place of'whitelist' and 'blacklist' wherever possible. Which, in fact, isclearer and less ambiguous. So as well as being more inclusive of all,this is a net benefit to our understandability.(Words mostly borrowed from<https://www.ncsc.gov.uk/blog-post/terminology-its-not-black-and-white>)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>",2
Add PR/issue note in Contribution Workflow Example (#9177)* Add PR/issue note in Contribution Workflow Example* Update CONTRIBUTING.rstCo-authored-by: Eric Lopes <nullhack@users.noreply.github.com>* Update CONTRIBUTING.rstCo-authored-by: Eric Lopes <nullhack@users.noreply.github.com>* Update CONTRIBUTING.rstCo-authored-by: Eric Lopes <nullhack@users.noreply.github.com>,1
Validate only task commands are run by executors (#9178),1
"Don't use the `|safe` filter in code, it's risky (#9180)Most things already use the `Markup` class to correctly escape problemareas, this commit just fixes the last instances so that we can assertthat `|safe` is never used.",1
Fixes failure of the build scripts when remote repo does not exist (#9188),0
Improved cloud tool available in the trimmed down CI container (#9167)* Improved cloud tool available in the trimmed down CI containerThe tools now have shebangs which make them available forpython tools. Also /opt/airflow is now mounted from thehost Airflow sources which makes it possible for the tools tocopy files directly to/from the sources of Airflow.It also contains one small change for Linux users - the filescreated by docker gcloud are created with root user so in order to fixthat the directories mounted from the host are fixed when you exitthe tool - their ownership is changed to be owned by the host user,1
Allows using private endpoints in GKEStartPodOperator (#9169),1
Update AWS connection example to show how to set from env var (#9191)The trailing `@` wasn't obvious/documented anywhere (and took me sometrial and error to work out) so to save time for the next person let'sadd it to the docs,2
Query TaskReschedule only if task is UP_FOR_RESCHEDULE (#9087)* Query TaskReschedule only if task is UP_FOR_RESCHEDULE* Query for single TaskReschedule when possible* Apply suggestions from code reviewCo-authored-by: Stefan Seelmann <mail@stefan-seelmann.de>* Adjust mocking in tests* fixup! Adjust mocking in tests* fixup! fixup! Adjust mocking in testsCo-authored-by: Stefan Seelmann <mail@stefan-seelmann.de>,3
Set conn_type as not-nullable (#9187),1
Call super.tearDown in SystemTest tearDown (#9196),5
Remove httplib2 from Google requirements (#9194)* Remove httplib2 from Google requirements* fixup! Remove httplib2 from Google requirements,1
Fix typo in BREEZE.rst (#9199)Changed 'y' to 'by' since it was incorrect.,4
Support additional apt dependencies (#9189)* Add ADDITONAL_DEV_DEPS and ADDITONAL_RUNTIME_DEPS* Add examples for additional apt dev and runtime dependencies* Update comment* Fix typo,2
n Improved compatibility with Python 3.5+ - Convert signal.SIGTERM to int (#9207)Co-authored-by: Jiening Wen <phill84@Jienings-MacBook-Pro.local>,1
Add OrangeBank to the official users of AirFlow (#9210)Adding OrangeBank to the official users of AirFlow.,1
Upgrade pendulum to latest major version ~2.0 (#9184),3
Add S3ToRedshift example dag and system test (#8877)- add howto docs for S3ToRedshift example dag- add terraform which runs terraform CLI commands in an isolated docker containerNOTE: This system test uses terraform to provide the infrastructure needed to run this example dag.,2
Add metavar to CLI arguments (#9077),1
Make generated job_id more informative in BQ insert_job (#9203)* Make generated job_id more informative in BQ insert_job* fixup! Make generated job_id more informative in BQ insert_job* fixup! fixup! Make generated job_id more informative in BQ insert_job* fixup! fixup! fixup! Make generated job_id more informative in BQ insert_job,5
Add test for BQ operations using location (#9206),1
"Fix up coredns pods in KIND CI (#9224)These were passing for a while, but seemingly broke after _something_outside of the airflow code base changed (DNS of Github Actions runners?Phase of the moon) so adding this config map that we had when KINDinside docker, rather than in the host.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Daniel Imberman <daniel.imberman@gmail.com>",2
Disable KIND git-sync tests for now (#9229)There is some DNS problem causing it to be unable to resolve github.comin the container.To unblock tests we are disabling the git-sync mode tests for themoment.,3
"Correctly restore colour in logs after format arg (#9222)The ""\e[22m"" escape sequence has been tested on Konsole, iTerm2 andTerminal.app",3
"Make it possible to silence warnings from Airflow (#9208)If we blindly set the warnings filter, it is _impossible_ to silencethese warnings. This is the approach suggested in https://docs.python.org/3/library/warnings.html#overriding-the-default-filter",2
"Upload kind logs to Github Actions artifact (#9230)* Upload kind logs to Github Artifacts* get_ci_environment needed to correctly dump kind logsWithout this the ""setup cluster"" step dumps the logs as`kind_logs_2020-06-11_default_default.tar.gz`Since this is now used outside of just building images, I have moved thefunction to _initialization.sh* Github already compresses artifacts for download.If we upload a .tar.gz, the download is a .zip file containing that.tar.gz, which isn't idealCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",5
Add generic CLI tool wrapper (#9223)* Add generic  CLI tool wrapper* Pas working directory to container* Share namespaces between all containers* Fix permissions hack* Unify code styleCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>* Detect standalone execution by checking symboli link* User friendly error message when env var is missing* Display error to stderr* Display errors on stderr* Fix permission hack* Fix condition in if* Fix missing env-file* TEST: Install airflow without copying ssources* Update scripts/ci/in_container/run_prepare_backport_readme.shCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Additional apt dependencies options in breeze (#9231),1
Add readonly connection API endpoints (#9095)* add connection schema with tests* add endpoints for connection* update patch* update endpoint methods* add readonly connection endpoints* improve base schema and add tests* update spec set connection id to string* update type hint* improve base schema* add pre_load processing to return data to normal* remove pre_load processing as it is not needed* readonly endpoints* improve code* handle exception and improve code* improve pagination test* add nullable to spec and improve code* remove base and add parameterized tests* add pagination limit test,3
Add missing variable in run_cli_tool.sh (#9239),1
Fix typo in test_connection_schema.py (#9241),3
Use Markup for htmlcontent for landing_times (#9242),1
CI: Propogate Exit Code Correctly (#9247)This was unfortunately broken since #9138Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Further validation that only task commands are run by executors (#9240),1
Fix null conn_type for TestDiscordWebhookHook & TestGoogleApiToS3Transfer (#9257),3
"Update (previously null) imap_default conn_type (#9256)This is a test fixup to #9187, and updates the possibly existingimap_default connection, and adds that hook type to the various lists",1
Add conn_type to Fix failing Livy Tests (#9258),3
Fix failing TestGoogleDiscoveryApiHook & SnowflakeExampleDagsSystemTest (#9259),5
Fix failing TestSlackWebhookHook (#9260)`conn_type` was enforced by #9187,1
Fix PagerDuty and OpsGenie tests (#9261)conn_type was enforced by #9187,1
Fix Azure container registry hook tests (#9262)conn_type was enforced by #9187,1
Increase the number of expected queries on index view to 38 (#9263)I am not sure what bumped this number from 37 to 38 but fixing the test for now. We can investidate it later,5
JSON escape text in test_list_dagrun_includes_conf (#9264)Needed because of the change in https://github.com/apache/airflow/pull/9180,4
Add task instance mutation hook (#8852)* Add task instance mutation hook* add merge* update docs* fix* add missing import* fix lint* test state as well* persist state* fix lint,0
Add test_remove_unused_code to Quarantined test (#9268)This test is passing locally and on breeze. Not sure of a reason why it is failing on CI,0
Fix typo in test_dask_executor.py (#9269),3
Remove generating temp remote manifest file in project dir (#9267),2
Fix Failing test for JSON Formatter on Python 3.8 (#9278),5
Update pre-commit-hooks repo version (#9195)- use official isort pre-commit-hook- use official yamllint pre-commit-hook- run isort pre-commit-hook on all python files instead of files ending with py,2
Remove trailing comma in setup_backport_packages.py (#9284),1
"Merge comparisons with ""in"" operator in DagBag (#9281)This is faster, less verbose, and more readable.",2
"Allow Lazy Logging (#9283)It is recommended to leave string interpolation to the logging method itself. For more details, see http://www.python.org/dev/peps/pep-0282.",2
Make airflow/settings.py Pylint compatible (#9286),1
Fix cyclic imports (#9292)Fixes the following error:```************* Module airflow.providers.yandex.hooks.yandexairflow/providers/yandex/hooks/yandex.py:1:0: R0401: Cyclic import (airflow.settings -> airflow.utils.orm_event_handlers -> airflow.utils.sqlalchemy -> airflow.utils.timezone) (cyclic-import)```,2
Add dev script to compare GH issues against merges (#9270)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,7
Refactor create_app in airflow/www/app.py (#9291),1
Decrypt secrets from SystemsManagerParameterStoreBackend (#9214),5
"Use actions/cache@v2 (#9293)We upgraded to v2 in https://github.com/apache/airflow/pull/8265 for other places in `.github/workflows/ci.yml`. This was left, so fixing it.",0
Make airflow/models/skipmixin.py Pylint compatible (#9289),1
Make experimental/endpoints.py Pylint compatible (#9287),1
Make airflow/models/errors.py Pylint compatible (#9288),0
"Send Celery tasks from main process when sync_parallelism is 1 (#9253)In attempting to debug some other behaviour I discovered that we spawn a""pool"" of 1 process (main process, plus another) -- which makes ithard to use a debugger against.This changes it so that if a single process is configured that it isjust run directly, preserving the ability to step through with adebugger",0
"Fix tree view if config contains "" (#9250)If you run DAG with `{""\"""": """"}` configuration tree view will be broken:```tree:1 Uncaught SyntaxError: Unexpected string in JSON at position 806    at JSON.parse (<anonymous>)    at tree?dag_id=hightlight_test&num_runs=25:1190```JSON.parse is given incorrectly escaped json string.",5
Wait for pipeline state in Data Fusion operators (#8954)* Wait for pipeline state in Data Fusion operatorsfixup! Wait for pipeline state in Data Fusion operatorsfixup! fixup! Wait for pipeline state in Data Fusion operatorsfixup! fixup! fixup! Wait for pipeline state in Data Fusion operators* Use quote to encode url parts* fixup! Use quote to encode url parts,1
"Resolve upstream tasks when template field is XComArg (#8805)* Resolve upstream tasks when template field is XComArgcloses: #8054* fixup! Resolve upstream tasks when template field is XComArg* Resolve task relations in DagRun and DagBag* Add tests for serialized DAG* Set dependencies only in bag_dag, refactor tests* Traverse template_fields attribute* Use provide_test_dag_bag in all tests* fixup! Use provide_test_dag_bag in all tests* Use metaclass + setattr* Add prepare_for_execution method* Check signature of __init__ not class* Apply suggestions from code reviewCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/models/baseoperator.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Include some missing RBAC roles on User and Viewer roles (#9133),1
"Remove duplicated log line from `db upgrade` (#9305)Fixes these logs -- we only want one of them    [2020-06-15 12:19:38,673]  3613796 {{airflow.utils.db db.py:610}} INFO - Creating tables    [2020-06-15 12:19:38,673]  3613796 {{airflow.utils.db db.py:565}} INFO - Creating tables",1
Add /version endpoint (#9296),1
Keep consistent system test names (#9272),3
Fix failing tests from #9250 (#9307),3
Rename CloudBuildCreateBuildOperator to CloudBuildCreateOperator (#9314),5
Get all pod logs on k8s launching failure (#9317)Co-authored-by: Daniel Imberman <daniel@astronomer.io>,0
Close/Flush byte stream in s3 hook load_string and load_bytes (#9211),1
Add nullable and required to some fields in OpenAPI spec #9315 (#9315),1
Add event log endpoints (#9227),2
Add schema and read-only endpoints for Pools (#9097),1
Fix broken CI image optimisation (#9313)The commit 5918efc86a2217caa641a6ada289eee1c21407f8 brokeoptimisation of the CI image - using the Apache Airflowmaster branch as a base package installation source from PyPI.This commit restores it including removal of theobsolete CI_OPTIMISED arg - as now we have a separateproduction and CI image and CI image is by defaultCI_OPTIMISED,4
AWSBatchOperator <> ClientHook relation changed to composition (#9306),4
Add schema and read-only endpoints for Import errors (#9217)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add support for latest Apache Beam SDK in Dataflow operators (#9323)* A* Add support for Apache Beam latest SDK in Dataflow operators* fixup! Add support for Apache Beam latest SDK in Dataflow operators* fixup! fixup! Add support for Apache Beam latest SDK in Dataflow operators* fixup! fixup! fixup! Add support for Apache Beam latest SDK in Dataflow operators,1
Improve production image iteration speed (#9162)For a long time the way how entrypoint worked in ci scriptswas wrong. The way it worked was convoluted and short of blackmagic. This did not allow to pass multiple test targets andrequired separate execute command scripts in Breeze.This is all now straightened out and both production andCI image are always using the right entrypoint by defaultand we can simply pass parameters to the image as usual withoutescaping strings.This also allowed to remove some breeze commands andchange names of several flags in Breeze to make them moremeaningful.Both CI and PROD image have now embedded scripts for logcleaning.History of image releases is added for 1.10.10-*alpha quality images.,1
Fixes unbound variable on MacOS (#9335)Closes #9334,0
"clarify breeze initialize virtualenv instructions (#9319)* you need to activate virtualenv, not enter breeze, before running the command",1
Make hive macros py3 compatible (#8598)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,1
Introduce 'transfers' packages (#9320)* Consistent naming of transfer operatorsTransfer operators have consistent names and are grouped inthe 'transfer' packages.* fixup! Consistent naming of transfer operators* Introduces 'transfers' packages.Closes #9161 and #8620* fixup! Introduces 'transfers' packages.* fixup! fixup! Introduces 'transfers' packages.* fixup! fixup! fixup! Introduces 'transfers' packages.,0
Add GrowthSimple to list of Airflow users (#9337),1
Fix typo in CONTRIBUTING.rst (#9340),2
Fix retries causing constraint violation on MySQL with DAG Serialization (#9336)The issue was caused because the `rendered_task_instance_fields` table did not have precision and hence causing `_mysql_exceptions.IntegrityError`.closes https://github.com/apache/airflow/issues/9148,0
Fix TestDagCode.test_remove_unused_code test (#9344),3
"Refactor CeleryExecutor to avoid duplication of code in test (#9345)The test code had duplicated most of the code in ""trigger_tasks"" --meaning we weren't strictly speaking testing the executor anymore.This removes the duplicated code in the test by refactoring the methodin the executor and calling that instead.",4
Add HashiCorp Vault Hook (split-out from Vault secret backend) (#9333)Split-off vault hook from vault secret backend,1
Merging multiple sql operators (#9124)* Merge various SQL Operators into sql.py* Fix unit test code format* Merge multiple SQL operators into one1. Merge check_operator.py into airflow.operators.sql2. Merge sql_branch_operator.py into airflow.operators.sql3. Merge unit test for both into test_sql.py* Rename test_core_to_contrib Interval/ValueCheckOperator to SQLInterval/ValueCheckOperator* Fixed deprecated class and added check to test_core_to_contrib,3
Move out metastore_browser from airflow.contrib (#9341),4
Add Production Helm chart support (#8777)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Fix failing tests from #9124 (#9356),3
Add 'helm-chart' label for PRs touching Airflow Helm Chart (#9359),2
Fix Airflow Stable version in README.md (#9360),2
Remove redundant count query in BaseOperator.clear() (#9362)We already fetch the results using `qry.all()` so running `qry.count()` is just redundant.,1
Use more effective count queries in API endpoints (#9361),1
implement API v1 for variables (#9273)* implement api v1 for variables* add test for limit parameter default value* change offset behavior* address review feedback* addressed more review feedbacks* return total item count in db for total_entries field,5
Detect automatically the lack of reference to the guide in the operator descriptions (#9290)Co-authored-by: ivan.afonichkin <ivan.afonichkin@transferwise.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Force order in list API endpoints (#9366),1
Add info about BaseOperatorMeta to UPDATING.md (#9369),5
Use strict API  schemas (#9365),1
Adds GCP Secret Manager Hook (#9368)* Adds GCP Secret Manager Hook,1
"Fixed crashing webserver after /tmp is mounted from the host (#9378)The bug was introduced in f17a02d33047ebbfd9f92d3d1d54d6d810f596c1Gunicorn uses a lot of os.fchmod in /tmp directory and it can create someexcessive blocking in os.fchmodhttps://docs.gunicorn.org/en/stable/faq.html#how-do-i-avoid-gunicorn-excessively-blocking-in-os-fchmodWe want to switch to use /dev/shm in prod image (shared memory) to makeblocking go away and make independent on the docker filesystem used (osxfs hasproblems with os.fchmod and use permissions as well).Use case / motivationAvoiding contention might be useful = in production image.This can be done with:GUNICORN_CMD_ARGS=""--worker-tmp-dir /dev/shm""",1
Fixes Breeze 'tests' command (#9384)Fixes the 'tests' command allows to run individual tests immediatelyfrom the host without entering the container. It's been brokenin 7c12a9d4e0b6c1e01fee6ab227a6e25b5aa5b157,3
Replace old Airflow screenshots with new images (#9393),1
Fix in-breeze CLI tools to work also on Linux (#9376)Instead of creating the links in the image (which did not work)the links are created now at the entry to the breeze image.The wrappers were not installed via Dockerfile and the ownershipfixing did not work on Linux,1
Unpin Apache Beam (#9390),5
Prepare backport release candidate for 2020.6.23rc1 (#9370)* Prepare backport release candidate for 2020.6.23rc1* fixup! Prepare backport release candidate for 2020.6.23rc1* fixup! fixup! Prepare backport release candidate for 2020.6.23rc1,5
Use current_app.dag_bag instead of global variable (#9380)* Use current_app.dag_bag instead of global variable* fixup! Use current_app.dag_bag instead of global variable* fixup! fixup! Use current_app.dag_bag instead of global variable,2
Add MySqlToS3Operator (#9054)* mysql_to_s3_operator* moved and blank line* documentation indent and dict none* directories changes* fixes* fixes* documentation change* docs* docs and pd_csv_kwargs* Update airflow/providers/amazon/aws/operators/mysql_to_s3.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* pd_csv_kwargs* dependencies and docs* contributing and docs* Update airflow/providers/amazon/aws/operators/mysql_to_s3.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/providers/amazon/aws/operators/mysql_to_s3.pyCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>* feluelle suggestion* import airflowexception* line too long fixCo-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Add more authentication options for HashiCorp Vault classes (#8974)The following authentication are now available:* aws_iam* azure* gcp with gcp_keyfile_dict* radiusAdditionally separate auth_mount_point can now be specifiedwhen instantiating both Secret Backend and Hook.,1
Remove outdated exclude pattenr in docs/conf.py (#9399),5
Move MySqlToS3Operator to transfers (#9400)* Move MySqlToS3Operator to transfers* fixup! Move MySqlToS3Operator to transfers,1
Add readonly endpoints for DagRuns (#9153),2
Final cleanup for 2020.6.23rc1 release preparation (#9404),4
Properly propagated warnings in operators (#9348)* Test warnings are properly propagated* Adjust deprecation warnings* Separate tests and deprecated classes lists,3
Decouple parameters formatting and endpoint logic (#9405),2
Fixes location of temporary file created in tests (#9403)Solves small Issue created in a60f589aa251cc3df6bec5b306ad4a7f736f539fThe file was created in the sources 'file' when run from IDE whichmight be accidentally committed. This change prevents it.,4
Fixed release number for fresh release (#9408),0
Fix deprecation messages in airflow.utils.helpers (#9398),0
Don't use connection to store task handler credentials (#9381),0
Extract TaskLogReader from views.py (#9391)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
add guidance re yarn build for local virtualenv development (#9411),1
Warn about incompatible plugins (#9416),2
Restore airflow.www.app.csrf to avoid breaking change (#9402)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,4
Make airflow/models/variable.py Pylint compatible (#9422),1
Remove incorrect docstrings in check_migrations (#9428),2
Remove unused variables in backport_packages (#9425)There were few instance where we didn't need all the variables from the returned functions. We can remove them.,4
Make airflow/models/pool.py Pylint compatible (#9423),1
Fix pylint issue in some airflow/www/* files (#9421),2
Remove empty file: test_hdfs_sensor.py (#9426)This contents of this file were moved to tests/providers/apache/hdfs/sensors/test_hdfs.py in https://github.com/apache/airflow/commit/0481b9a95786a62de4776a735ae80e746583ef2b,3
Simplify chained SQL Queries in Connexion API Endpoints (#9424)- The SQLAlchemy Queries can be nested- The comma filters command can take multiple filters,5
Fixed rendering of IMAGES.rst (#9433),0
In case of worktree .git might be a file - rat-check fails with it (#9435),0
Avoid broad exceptions when json.loads is used (#9432),1
Fix grammar in test_name (#9441)test_should_display_helps -> test_should_display_help,3
Fix typo in CONTRIBUTING.rst (#9437),2
Add options to extend list of sensitive keywords (#9397),1
Fix typo - init_wsg_middleware => init_wsgi_middleware (#9434),5
Remove unused tests/bin folder (#9440)`airflow.bin` package was removed in https://github.com/apache/airflow/commit/8465d66f05baeb73dd4479b019515c069444616e but this test folder wasn't removed,4
Add invitation to #documentation channel when docs build fails (#9439),0
Make airflow/models/base.py Pylint Compatible (#9442),1
Better documentation for backport packages (#9445)* Better documentation for backport packages* Update README.md,2
Fix Custom Sensitive Variable fields feature (#9446),0
Fixes pushing prod image directly from breeze (#9449),0
Show Dag's Markdown docs on Tree View (#9448),2
Update README to remove Py 3.8 limitation for Master (#9451),4
Enforce code-block directives in doc (#9443),2
Add copy button to Code Blocks in Airflow Docs (#9450),2
Add reference to the ASF Code of Conduct (#9453)* Add reference to the ASF Code of Conduct* Update CONTRIBUTING.rst,5
"Add reference to the ASF CoC for First Time Contributors (#9454)Similar to https://github.com/apache/airflow/issues/9453 but this would inform all the ""First Time Contributors""",5
Grammar correctness in communication (#9455),5
Add PyDocstyle Precommit Hook (#9456),1
"Enable & Fix ""Missing docstring in public module"" PyDocStyle check (#9457)",2
Enable & Fix Whitespace related PyDocStyle Checks (#9458),2
"Select Checks to exclude instead of include for PyDocStyle (#9459)Do the reverse of what we have in other Pull Request so it will be easy for us to in future to remove this ""ignore checks"" one by one",4
"Enable & Fix ""Docstring Content Issues"" PyDocStyle Check (#9460)",2
Remove DELETE /importErrors/{import_error_id} endpoint (#9325),2
Add pydocstyle to Breeze Autocomplete (#9462),2
fix typing error for utils.dates.date_range usage (#9429)* fix typing error for utils.dates.date_range usage* fix type annotation in timezone util,0
More user-friendly message on incorrect configuration (#9436)* More user-friendly message on incorrent configuration* Update tests/test_configuration.py* Update tests/test_configuration.py,5
Enable 'Public function Missing Docstrings' PyDocStyle Check (#9463),2
Fix function name in airflow/stats.py (#9466),1
Move modules in `airflow.contrib.utils.log` to `airflow.utils.log` (#9395),2
Disable schema  ordering (#9471),5
Add __init__ method to Variable class (#9470),5
Add unit tests for OracleOperator (#9469),1
Move out weekday from airflow.contrib (#9388)* Move out weekday from airflow.contrib* Add changelog about weekday enum refactor into UPDATING.md,5
Add AWS ECS system test (#8888),3
Pylint fixes and deprecation of rare used methods in Connection (#9419),1
Remove redundant code from breeze initialization (#9375),5
Remove unused recurse_tasks function (#9465),1
Fix typo in helm chart upgrade command for 2.0 (#9484),2
Correct command for starting Celery Flower (#9483),5
Remove redundant parentheses in /test_datacatalog.py (#9481)string literal doesn't need parentheses,5
"Fix typo in the word ""default"" in www/forms.py (#9480)`defualt_timezone` -> `defualt_timezone`",2
Add CRUD  endpoint for connections (#9266),1
Add link to ADC in use-alternative-secrets-backend.rst (#9478),1
Add more .mailmap entries (#9489),1
Remove need of datetime.timezone in test_views.py (#9479),3
Replace deprecated wtforms HTMLString with markupsafe.MarkUp (#9487)WTForms uses `MarkUp` to escape strings now and removed their internal class HTMLString in Master. Details: https://github.com/wtforms/wtforms/pull/400That change previously broke Airflow for new users (in 2.3.0). However on users request they added `HTMLString` that just passes all args to `markupsafe.MarkUp` back for temporary Backward compatbility with deprecation warning in 2.3.1. Details: https://github.com/wtforms/wtforms/issues/581,0
[AIRFLOW-8057] [AIP-31]  Add @task decorator (#8962)Closes #8057. Closes #8056.,1
[AIRFLOW-9347] Fix QuboleHook unable to add list to tags (#9349),1
Move python import path from operationId into x-openapi-router-controller (#9495),2
Add extra links endpoint (#9475),2
"Replace ""bail"" with ""cancel"" in Web UI (#9499)""Bail"" is a word that's unlikely to be widely understood by non-native Englishspeakers. ""Cancel"" is used much more frequently in sofwtare interfaces and is likely to be moreunderstood.",1
Add stats to backport packages (#9501),1
Add Redoc Open API preview (#9504),2
Read only endpoint for XCom #8134 (#9170)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,5
Add read-only Task endpoint (#9330)Add API endpoints for tasks and DAG detailsCo-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,2
Remove non-existent chart value from readme (#9511)This was accidentally left over when this was extracted fromAstronomer's chart.,2
Cancel queued/running builds on second push to PR (#9513)We need to Cancel builds on PRs too.,1
Fix logging issue when running tasks (#9363),1
Add query count tests for _run_raw_task (#9509),1
"Use literal syntax instead of function calls to create data structure (#9516)It is slower to call e.g. dict() than using the empty literal, because the name dict must be looked up in the global scope in case it has been rebound. Same for the other two types like list() and tuple().",1
"Fixes treatment of open slots in scheduler (#9316) (#9505)Makes scheduler count with number of slots required by tasks.If there's less open slots than required, a task isn't taken to a queue.",1
nitpick fix (#9527),0
Remove reimported AirflowException class (#9525)It is imported at the top of the file and L1060 too,2
Add missing precommit-hook ids to breeze-complete (#9524),1
Remove kwargs from Super calls in AWS Secrets Backends (#9523)We don't want pass to kwargs to `BaseSecretsBackend` nor `LoggingMixin`,2
"Fix typo in test_views.py (#9522)""parmeters_only"" -> ""parameters_only""",2
YAML file format in LocalFilesystemBackend (#9477)Co-authored-by: Vinay <vinay@synctactic.ai>,5
Extract common date log logic in _run_raw_task (#9512)Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
"Gunicorn works better if temporary folder uses tmpfs (#9534)This is discussed in the documentation of gunicorn.You can find more information here: https://docs.gunicorn.org/en/stable/faq.html#how-do-i-avoid-gunicorn-excessively-blocking-in-os-fchmodSince we are using docker, we always have shared memoryavailable (at least 64MB).Closes #9379",2
Add RushOwl to Airflow users (#9536),1
Add read-only Config endpoint (#9497)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>Co-authored-by: Tomek Urbaszek <turbaszek@apache.org>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Fix typo of resultBackendConnection in chart README (#9537),2
"Make Production Dockerfile OpenShift-compatible (#9545)OpenShift (and other Kubernetes platforms) often use the approachthat they start containers with random user and root group. This isdescribed in the https://docs.openshift.com/container-platform/3.7/creating_images/guidelines.htmlAll the files created by the ""airflow"" user are now belonging to'root' group and the root group has the same access to thosefiles as the Airflow user.Additionally, the random user gets automatically added/etc/passwd entry which is name 'default'. The name of the usercan be set by setting the USER_NAME variable when starting thecontainer.Closes #9248Closes #8706",1
"Show ""Task Reschedule"" table in Airflow Webserver (#9521)",2
show correct duration on graph view for running task (#8311) (#8675)* show correct duration on graph view for running task (#8311)* fix invalid end date  (#8311)* Update airflow/www/static/js/task-instances.jsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Deprecate contrib modules (#9540)* Deprecate contrib modules* Update test for deprecated classes to add Weekday* Fix typo in class name,2
Bump Pylint to 2.5.3 (#9294),5
Move out sendgrid emailer from airflow.contrib (#9355),4
Check all deprecation messages in airflow.contrib (#9552),5
"More sensible docker caching strategy for Prod images (#9547)Local caching is now default strategy when buildingthe Production image.You can still change it to pulled - similar to CI buildsby providing the right build flag and this is whatis used in CI by default. The flags in Breeze are now updatedto be more eplanatory and friendly (build-cache-*) and a flagfor ""disabled"" cache option is added as well.Also the Dockerfile and Dockerfile.ci files are not neededany more in the docker context. They used to be needed whenwe built the Kubernetes image in the container, but sincewe are now using production image directly - we do not needthem any nmore.Combining setting the default strategy to local and removingthe Dockerfile from the context has the nice effect that youcan iterate much faster on the Production image withouttriggering rebuilds of half of the docker imageas soon as the Dockerfile changes.",4
Clean up airflow.contrib in Kubernetes docs (#9551),2
Expose option: look_for_keys in ssh_hook via extras (#8793),1
Add unit tests for PigOperator (#9560),1
Remove almost all references to airflow.contrib (#9559),4
Detect references to deprecated classes in test_core_to_contrib.py (#9553),3
Fix the default value for store_dag_code (#9554)related to #8255 (fixes the issue mentioned with `store_dag_code` but does not address Config interpolation)The default value of `store_dag_code` should be same as `store_serialized_dags` setting.  But if the value is set it should use that value,1
Fix failing test in DagCode (#9565)PR https://github.com/apache/airflow/pull/9554 introduced this error and because of Github issue currently (github is down / has degraded performance) the CI didn't run fully,1
Allow changing Task States Colors (#9520),4
Reload gunicorn when plugins has beeen changed (#8997),4
Add tests for spark_jdbc_script (#9491)* Add tests for spark_jdbc_script* Remove test file from missing tests files list,2
"""build-essential"" are needed for proper install on Linux (#9573)On a fresh install of Windows Subsystem for Linux (Ubuntu 20.04), created a conda environment and tried to follow the above guide  to install airflow but was getting gcc errors. Installing build-essential solved the issue.  On Windows 10, with a clean conda environment, setproctitle fails to install with error ""Microsoft Visual C++ 14.0 is required"". Suggests to install ""Microsoft Visual C++ Build Tools"".  I got this both on Python version 3.8.3 and 3.5.6May be add some prerequisites about Windows and Python versions?",1
Fix failing tests from #8997 (#9576),3
Raise exception on invalid type in pre_commit_yaml_to_cfg.py (#9577),5
Add docs about reload_on_plugin_change opiton (#9575),4
Add docs on using DAGRun.conf (#9578)closes https://github.com/apache/airflow/issues/8900,0
Fix typo in password (#9579)`pasword` -> `password`,4
Fix typos in cloud_memorystore.rst (#9581),2
Add log endpoint (#9331)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,2
Add CRUD Endpoints for pools (#9329),1
Remove PATCH /dags/{dag_id}/dagRuns/{dag_run_id} endpoint(#9476)Dag Runs are immutable for third-party applications.,1
Add TruFactor to Airflow users list (#9584),1
Add XCom.get_one() method back (#9580),1
Add more info on dry-run CLI option (#9582)fixes  https://github.com/apache/airflow/issues/9561,0
Use pfromat instead of str to render arguments in WebUI (#9587),1
"Change worker_refresh_interval fallback to default of 30 (#9588)The default value for worker_refresh_interval is 30, so we should align the fallback to 30 too",1
Add template_ext to BigQueryInsertJobOperator (#9568),1
Extend BigQuery example with include clause (#9572),1
Fix failing test in test_webserver_command (#9589),3
"Remove redundant airflowVersion from Helm Chart readme (#9592)We no longer use `airflowVersion` , we instead use `defaultAirflowRepository` and `defaultAirflowTag`",1
Fix broken link in chart/README.md (#9591)`CONTRIBUTING.md` -> `../CONTRIBUTING.rst`,2
Fix regression in SQLThresholdCheckOperator (#9312),1
Fix typo in the word 'available' (#9599)`avaible` -> `available`,2
Move XCom tests to tests/models/test_xcom.py (#9601)Move XCom tests from `tests/models/test_cleartasks.py` to `tests/models/test_xcom.py`,3
Fix typo in tutorial.rst (#9605),2
Switches to Helm Chart for Kubernetes tests (#9468)The Kubernetes tests are now run using Helm chartrather than the custom templates we used to have.The Helm Chart uses locally build production imageso the tests are testing not only Airflow but alsoHelm Chart and a Production image - all at thesame time. Later on we will add more testscovering more functionalities of both Helm Chartand Production Image. This is the first step toget all of those bundle together and becometestable.This change introduces also 'shell' sub-commandfor Breeze's kind-cluster command andEMBEDDED_DAGS build args for production image -both of them useful to run the Kubernetes testsmore easily - without building two imagesand with an easy-to-iterate-over-testsshell command - which works without anyother development environment.Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Daniel Imberman <daniel@astronomer.io>,1
Update Breeze documentation (#9608)* Update Breeze documentation,2
Fix quarantined tests - TestCliWebServer (#9598),3
Add docs to change Colors on the Webserver (#9607)Feature was added in https://github.com/apache/airflow/pull/9520,1
"Change default auth for experimental backend to deny_all (#9611)In a move that should surprise no one, a number of users do not read,and leave the API wide open by default. Safe is better than powned",1
Removes importlib usage - it's not needed (fails on Airflow 1.10) (#9613),0
Restrict changing XCom values from the Webserver (#9614),4
Update docs about the change to default auth for experimental API (#9617),4
Change 'initiate' to 'initialize' in installation.rst (#9619)`Initiating Airflow Database` -> `Initializing Airflow Database`,5
Replace old Variables View Screenshot with new (#9620),1
Replace old SubDag zoom screenshot with new (#9621),1
Fix using .json template extension in GMP operators (#9566),1
Fix docstrings in exceptions.py (#9622),2
Task logging handlers can provide custom log links (#9354)Use a mixin to define log handlers based on remote services. The mainchanges are: - Create RemoteLoggingMixin to define remote log handlers. - Remove explicit mentions to Elasticsearch in dag.html. - Rename the /elasticsearch endpoint in views.py to   /redirect_to_remote_log and dispatch the remote URL building to the   log handler.Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Improve queries number SchedulerJob._process_executor_events (#9488),1
Move S3TaskHandler to the AWS provider package (#9602),1
Use supports_read instead of is_supported in log endpoint (#9628),2
Updated link to official documentation (#9629)The link to official documentation should point to the documentation page instead of the home page.,2
Fixing typo in chart/README.me (#9632)* Fixing typo in readme,2
Customizable page size limit in API (#9431)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Allow setting Hashicorp Vault token from File (#9644),2
Fixed failing Kubernetes tests after deny_all for experimental API (#9647)The tests were broken by #9611,3
"Test are triggered now on more changes. (#9646)Sometimes tests were not triggered when they should be.This change will cause the tests to be triggered when anythingchanges in ""airflow"" or ""charts"" additionally to what we hadbefore.",5
Support .airflowignore for plugins (#9531),1
Remove duplicate License lines in airflow/lineage/entities.py (#9659)The license details are repeated twice,4
Make airflow/models/xcom.py Pylint Compatible (#9658),1
Make airflow/models/kubernetes.py Pylint Compatible (#9673),1
Make airflow/logging_config.py Pylint Compatible (#9672),5
"The fix_ownership works independently of backend choice (#9664)The script failed on a ""clean"" installation if the imagerequired cleaning and the database was not started.",5
"Add git sync option and unit tests for the Helm chart (#9371)* add git sync sidecars* add a helm test* add more tests* allow users to provide git username and pass via  a k8s secrets* set default values for airflow worker repository & tag* change ci timeout* fix link* add credentials_secret to airflow.cfg configmap* set GIT_SYNC_ADD_USER on kubernetes worker pods, set uid* add fsGroup to webserver and kubernete workers* move gitSync to dags.gitSync* rename valueFields* turn off git sync and dag persistence by default* provide option to specify known_hosts* add git-sync details into the chart documentation* Update .gitignoreCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* make git sync max failures configurable* Apply suggestions from code reviewCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* add back requirements.lockCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>",1
Fix use of GCP credentials in StackdriverTaskHandler (#9668),0
Fix pylint issues in airflow/models/dagbag.py (#9666),2
Remove side effects from tests (#9675)Add setUp and tearDown methods to clear tabels,1
Fix tests: Add Default Conns back only when needed (#9679),1
Simplify DagBag - remove dead code related to SerializedDag (#9676),2
Move provider's log task handlers to the provider package (#9604),1
Fix typo in pre_commit_breeze_cmd_line.sh (#9682)`genereate` -> `generate`,2
Upgrade to latest pre-commit checks (#9686),3
Use parallel process for several Pre-Commits checks (#9681)Many of the pre-commits don't require `require_serial: true`. This PR removes those to use the default of `require_serial: false`,1
More robust and re-runnable autocomplete setup in Breeze (#9685)* More robust and re-runnable autocomplete setup in Breeze* Update breezeCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,5
Update some dependencies (#9684),5
"Revert ""Update some dependencies (#9684)"" (#9693)This reverts commit fd62b1c5262086597db2aa439a09d86794a33345.",5
Remove XCom CUD endpoints (#9661),4
"Tests should also be triggered when there is just setup.py change (#9690)So far tests were not triggered when only requirements changed,but this is quite needed in fact.",4
Update FlaskAppBuilder to v3 (#9648),5
Some Pylint fixes in airflow/models/taskinstance.py (#9674),0
Update migrations to ensure compatibility with Airflow 1.10.* (#9660)closes https://github.com/apache/airflow/issues/9640,0
Fix _process_executor_events method to use in-memory try_number (#9692),1
use the correct claim name in the webserver (#9688),1
Update Thumbtack points of contact in Airflow Users list (#9701)The previously-listed person is no longer at the company,1
generate go client from openapi spec (#9502)* generate go client from openapi spec* move openapi codegen to seperate workflow,1
[AIRFLOW-XXXX] Remove unnecessary docstring in AWSAthenaOperator,1
Add health API endpoint  (#8144) (#9277),1
Add AWS StepFunctions integrations to the aws provider (#8749),1
Move gcs & wasb task handlers to their respective provider packages (#9714),1
Allow AWSAthenaHook to get more than 1000/first page of results (#6075)Co-authored-by: Dylan Joss <dylanjoss@gmail.com>,1
Add Dag Runs CRUD endpoints (#9473),1
Make airflow/migrations/env.py Pylint Compatible (#9670),1
Get Airflow configs with sensitive data from Secret Backends (#9645),5
YAML file supports extra json parameters (#9549)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Vinay <vinay@synctactic.ai>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
fix grammar in prereq tasks gcp operator docs (#9728),2
Add The Climate Corporation to user list (#9726),1
Add Qingping Hou to committers list (#9725),1
Add new fantastic team member of Polidea. (#9724),1
Error in description after deployment (#9723)* Error in description after deploymentCo-authored-by: Daniel Debny <daniel.debny@polidea.com>,0
Skip one version of Python for each test.Skip one version of Python for each test.,3
Add read-only endpoints for DAG Model (#9045)Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,2
Ensure Kerberos token is valid in SparkSubmitOperator before running `yarn kill` (#9044)do a kinit before yarn kill if keytab and principal is provided,1
Update example DAG for AI Platform operators (#9727),1
Fix warning about incompatible plugins (#9704)One condition was bad and warns when the plugin is for admin and FAB flask.,2
Update local_task_job.py (#9746)Removing the suicide joke.,4
Tests are working for newly added backport providers (#9739)* Tests are working for newly added backport providers,1
"Pre-create Celery db result tables before running Celery worker (#9719)Otherwise at large scale this can end up with some tasks failing as theytry to create the result table at the same time.This was always possible before, just exceedingly rare, but in largescale performance testing where I create a lot of tasks quickly(especially in my HA testing) I hit this a few times.This is also only a problem for fresh installs/clean DBs, as once thesetables exist the possible race goes away.This is the same fix from #8909, just for runtime, not test time.",3
Support extra config options for Sentry (#8911)For now only dsn can be configured through the airflow.cfg. Need support 'http_proxy' option for example (it can't be configured through the environment variables). This change implements solution for supporting all existed (and maybe future) options for sentry configuration.,5
Use namedtuple for TaskInstanceKeyType (#9712)* Use namedtuple for TaskInstanceKeyType,1
Add TargetQueryValue to KEDA Autoscaler (#9748)Co-authored-by: Daniel Imberman <daniel@astronomer.io>,1
Add unit tests for mlengine_operator_utils (#9702),1
Mask other forms of password arguments in SparkSubmitOperator (#9615)This is a follow-up to #6917 before modifying the masking code.Related: #9595.,1
Use absolute paths in howto guides (#9758),1
Fix StackdriverTaskHandler + add system tests (#9761)Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,3
Check project structure in sensors/transfers directories (#9764),5
Add tests for yandex hook (#9665),1
improve type hinting for celery provider (#9762),1
Add ME-Br to who uses Airflow list (#9770),1
Add 1.10.11 Changelog & Update UPDATING.md (#9757),5
Links Breeze documentation to new Breeze video (#9768),1
Fix is_terminal_support_colors functtion (#9734),1
Add type hinting for discord provider (#9773),1
"Fix typo in the word ""Airflow"" (#9772)",2
Add Google Stackdriver link (#9765),2
Improve type hinting to provider microsoft  (#9774),1
Unit tests jenkins hook (#9767),1
Fixes failing formatting of DAG file containing {} in docstring (#9779),2
Upgrade to latest isort (5.0.8) (#9782),3
Add API Endpoint - DagRuns Batch (#9556)Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>,2
Improve typing coverage in scheduler_job.py (#9783),3
Enable pretty output in mypy (#9785),0
provide_session keep return type (#9787),1
Refactor Google operators guides (#9766)* Refactor Google guides* fixup! Refactor Google guides* fixup! fixup! Refactor Google guides,4
Fix small errors in image building documentation (#9792),2
Backfill reset_dagruns set DagRun to NONE state (#9756),2
Add DAG Source endpoint (#9322),2
The group of embedded DAGs should be root to be OpenShift compatible (#9794),2
Add docs for replace_microseconds parameters in trigger DAG endpoint (#9793),2
Add multiple file upload functionality to GCS hook (#8849)Co-authored-by: Timothy Healy <healz@timothys-air.lan>,1
Keep functions signatures in decorators (#9786),1
Use paths relative to root docs dir  in *include directives (#9797),2
Add Migration guide from the experimental API to the REST API (#9771)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Update paths in .github/boring-cyborg.yml (#9799)* Update paths in .github/boring-cyborg.yml* fixup! Update paths in .github/boring-cyborg.yml,5
Minor typo fix in OpenAPI specification (#9809),0
Enable annotations to be added to the webserver service (#9776),1
Make airflow package type check compatible (#9791),1
Update README to add Py 3.8 in supported versions (#9804),1
Remove unnecessary comprehension (#9805),4
Add type annotations for redis provider (#9815),1
Remove package.json and yarn.lock from the prod image (#9814)Closes #9810,5
"For now cloud tools are not needed in CI (#9818)Currently there is ""unbound"" variable error printed in CI logsbecause of that.",2
Python 3.8.4 release breaks our builds (#9820),4
"Allow `replace` flag in gcs_to_gcs operator. (#9667)* Allow `replace` flag in gcs_to_gcs operator.If we are not replacing, list all files in the Destination GCS bucket and only keep those files which are present in Source GCS bucket and not in Destination GCS bucket",2
Add kylin operator (#9149)Co-authored-by: yongheng.liu <yongheng.liu@kyligence.io>,1
Fix SqlAlchemy-Flask failure with python 3.8.4 (#9821),0
Add API Reference docs (redoc) to sphinx (#9806),2
Add Google Deployment Manager Hook (#9159)Co-authored-by: Ephraim Anierobi <4122866+ephraimbuddy@users.noreply.github.com>,1
Remove HTTP guide index in docs (#9796),2
Improve type hinting to provider cloudant (#9825)Co-authored-by: Refael Y <refael@seadata.co.il>,5
Add option to delete by prefix to S3DeleteObjectsOperator (#9350)Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Add CloudVisionDeleteReferenceImageOperator  (#9698),4
Add note in Updating.md about the change in `run_as_user` default (#9822)Until Airflow 1.10.10 the default run_as_user config (https://airflow.readthedocs.io/en/1.10.10/configurations-ref.html#run-as-user) which defaulted it to root user `0` (https://github.com/apache/airflow/blob/96697180d79bfc90f6964a8e99f9dd441789177c/airflow/contrib/executors/kubernetes_executor.py#L295-L301)In Airflow 1.10.11 we changed it to `50000`,4
Improve typing in airflow/models/pool.py (#9835),1
Remove global variable with API auth backend (#9833),4
Fix Writing Serialized Dags to DB (#9836),5
Update gcp to google in docs (#9839)Co-authored-by: Ashwin Shankar <ashankar@slack-corp.com>,2
BigQueryTableExistenceSensor needs to specify keyword arguments (#9832),2
Add guide for AI Platform (previously Machine Learning Engine) Operators  (#9798),1
Change DAG.clear to take dag_run_state (#9824)* Change DAG.clear to take dag_run_state* fix lint* fix tests* assign var* extend original clause,1
Rename DagBag.store_serialized_dags to Dagbag.read_dags_from_db (#9838),5
Update more occurrences of gcp to google (#9842),5
Add Dynata to the Airflow users list (#9846),1
"Fix S3FileTransformOperator to support S3 Select transformation only (#8936)Documentation for S3FileTransformOperator states that userscan skip transformation script if S3 Select experession isspecified, but in this case the created file is alwayszero bytes long.This fix changes the behaviour, so in case of no transformationgiven, the source file (a result of S3Select) is uploaded.",2
Fix DagRun.conf when using trigger_dag API (#9853)fixes https://github.com/apache/airflow/issues/9852,0
"Helm chart can now place arbitrary config settings in to airflow.cfg (#9816)Rather than only allowing specific pre-determined config settings, thischange allows the user to place _any_ config setting they like in thegenerated airflow.cfg, including overwriting the ""generated defaults"".This providers a nicer interface for the users of the chart (even if thecould already set these via the env vars).",1
Fix typo in datafusion operator (#9859)Co-authored-by: michalslowikowski00 <michal.slowikowski@polidea.com>,1
Fix Experimental API Client (#9849),0
Add imagePullSecrets to the create user job (#9802)So that it can pull the specified image from a private registry.,1
Group CI scripts in subdirectories (#9653)Reviewed the scripts and removed some of the old unused ones.,1
Add Snowflake support to SQL operator and sensor (#9843)* Add Snowflake support to SQL operator and sensor* Add test for conn_type to valid hook mapping* Improve code quality for conn type mapping test,3
Add log of affected sql rows in PostgresOperator (#9841)Co-authored-by: Johan Eklund <jeklund@zynga.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,1
Reorganizing of CI tests (#9654)* we come back to idea of having one CI workflow* cancel and openapi are incorporated into that CI workflow* cancel retrieves workflow id automatically (works for forks)* static checks are now merged into one job* less dependencies between jobs so that waiting is minimised* better name for check if tests should be run* separated out script for tests should be run check,1
Fix check_integration pre-commit test (#9869),3
Improve type annotations for Ftp provider (#9868),1
Fix typo in Task Lifecycle section (#9867),2
"Added ""all"" to allowed breeze integrations and tried to clarify on fail (#9872)",0
TimeSensor should respect the default_timezone config (#9699),5
Add __repr__ to SerializedDagModel (#9862)Before: `<airflow.models.serialized_dag.SerializedDagModel at 0x7fab30d68c50>`After: `<SerializedDag: example_xcom_args>`,2
Add type hinting for mongo provider (#9875),1
Add guide for Cassandra Operators (#9877),1
Increase typing for Apache and http provider package (#9729),1
improve typing for datadog provider (#9775),1
Add drop_partition functionality for HiveMetastoreHook (#9472),1
Deprecate experimental API (#9888),5
Make Secret Backend docs clearer about Variable & Connection View  (#8913),2
"Don't Update Serialized DAGs in DB if DAG didn't change (#9850)We should not update the ""last_updated"" column unnecessarily. This is first of  few optimizations to DAG Serialization that would also aid in DAG Versioning",2
"Update Serialized DAGs in Webserver when DAGs are Updated (#9851)Before this change, if DAG Serialization was enabled the Webserver would not update the DAGs once they are fetched from DB. The default worker_refresh_interval was `30` so whenever the gunicorn workers were restarted, they used to pull the updated DAGs when needed.This change will allow us to have a larged worker_refresh_interval (e.g 30 mins or even 1 day)",1
Constraint files are now maintained automatically (#9889)* Constraint files are now maintained automatically* No need to generate requirements when setup.py changes* requirements are kept in separate orphan branches not in main repo* merges to master verify if latest requirements are working and  push tested requirements to orphaned branches* we keep history of requirement changes and can label them  individually for each version (by constraint-1.10.n tag name)* consistently changed all references to be 'constraints' not  'requirements',1
Improve KubernetesPodOperator guide (#9079),1
Refactor AwsBaseHook._get_credentials (#9878),1
improve typing for openfaas provider (#9883)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add new committers: Ry Walker & Leah Cole to project.rst (#9892),1
TimeSensor should respect DAG timezone (#9882),2
UX Enhancement: Separate actions from links in DAG navigation (#9894),2
Use warning directive in deprecation warning (#9890),2
Pin google-cloud-container to <2 (#9901),5
"Remove type hint causing DeprecationWarning in Firestore operators (#9819)* Import Iterable from collections.abc in firestore operatorsDeprecationWarning: Using or importing the ABCs from 'collections'instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working* Remove the type hint",4
Fix insert_job method of BigQueryHook (#9899)The method should submit the job and wait for the result.Closes: #9897,1
Simplify pull request template (#9896)Remove the checklist of always checked points.,4
Fix dag.clear usages after change from #9824 (#9909)#9824 introduced changes in the signature of dag.clear(...) but not all occurrences of invocation were adjusted.,2
"Add unit tests for CassandraTableSensor, CassandraRecordSensor and WebHdfsSensor (#9874)",3
Increase typing coverage for Elasticsearch (#9911),3
Add function to get current context (#9631)Support for getting current context at any code location that runsunder the scope of BaseOperator.execute function. This functionalityis part of AIP-31.Co-authored-by: Jonathan Shir <jonathan.shir@databand.ai>,5
Use pipe pylint result to sort -u for error deduplication (#9893)Due to an pylint issue https://github.com/PyCQA/pylint/issues/3584single error/warning is printed multiple times when usingpylint in multiprocessing mode,1
Add typing for grpc provider (#9884),1
Add support for impersonation in GCP hooks (#9915)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,1
[AIRFLOW-6931] Fixed migrations to find all dependencies for MSSQL (#9891),0
Dump Pod as YAML in logs for KubernetesPodOperator (#9895),1
Import ABC from collections.abc (#9649),2
Tests are cancelled if any of faster checks fail (#9917),0
Postgres tests were skipped by mistake (#9923),3
Fix calling `get_client` in BigQueryHook.table_exists (#9916)Adding `project_id` argument to `get_client` method otherwise this call always falls back to the default connection id.,1
Nightly tag was not pushed on scheduled run (#9924),1
Add unit test for test_sql_to_gcs (#9920),3
Allows to configure logging for third-party libraries (#9657),2
point go client mod path to new repo (#9922)* point go client mod path to new repo,1
"Update Spark submit operator for Spark 3 support (#8730)In spark 3 they log the exit code with a lowercasee, in spark 2 they used an uppercase E.Also made the exception a bit clearer when runningon kubernetes.",1
Shorter/more meaningful header for official images (#9925),5
"Breeze / KinD  - support earlier k8s versions, fix recreate and kubectl versioning (#9905)",1
More strict rules in mypy (#9705) (#9906)Signed-off-by: Raymond Etornam <retornam@users.noreply.github.com>,1
Add response_filter parameter to SimpleHttpOperator (#9885),1
"Python base image version is retrieved in the right place (#9931)When quick-fixing Python 3.8.4 error #9820 PYTHON_BASE_IMAGE_VERSIONvariable was added but it was initialized too early in Breeze andit took the default version of Python rather than the one chosenby --python switch. This caused the generated requirements(locally by Breeze only) to generate wrong set of requirementsand images built locally for different python versions werebased on default Python version, not the one chosen by --pythonswitch.",1
"Clean up tmp directory when exiting from breeze shell (#9930)Since we are mountign tmp dir now to inside container, someof the remnants of what's going on inside remains after exit.This is particularly bad if you are using tmux (some of thedirectories remaining there prevent tmux from re-run)This change cleans up /tmp directory on exit from Breeze command.It does it from inside container so that we clean up allroot-owned files without sudo.",2
Pin github checkout action to v2 (#9938),5
Improve command examples in docs (#9934),2
Add Google Authentication for experimental API (#9848),1
apply_default keeps the function signature for mypy (#9784),1
Add type annotations to providers/vertica (#9936)Co-authored-by: Johan Eklund <jeklund@zynga.com>,1
Add Nielsen to Airflow users list (#9954),1
Fix link to CI.rst document (#9953),2
Improve signature for core operaotrs (#9944),1
Add more information about using GoogleAdsHook (#9951)This hook requires two connections and it's not obvious howto use it and what is the purpose of each connection.,1
Add DateTimeSensor (#9697)* Add DateTimeSensor,5
Fix CI: Don't add & fetch remote if source branch == target branch (#9961),1
Fix CI: Fetch target only when source repo != target repo (#9962),1
Add get_blobs_list method to WasbHook (#9950),1
Fix OpenShift Guidelines link in IMAGES.rst (#9978),2
Add Badges for Airflow Docker (#9979),2
Fix typo in airflow/cli/cli_parser.py (#9980)`action_subcommnads` -> `action_subcommands``group_subcommnands` -> `group_subcommands`,2
Fix various typos in airflow/cli/commands (#9983),2
Stop using start_date in default_args in example_dags (#9982),2
Bump actions/setup-python version to 2 (#9984)v2 was released on 21 March,1
Bump tableauserverclient to 0.12 (#9988),1
Relax requirement to allow latest version of flask-caching (#9989)Current available version is 1.9.0,3
Relax requirement to allow latest version of jinja2 (#9991)The latest available version of jinja2 is 2.11.2Fixes in 2.11.2: https://github.com/pallets/jinja/blob/master/CHANGES.rst#version-2112,4
Relax requirement to allow latest version of tenacity (#9992),3
Bump mysqlclient to 2.0.1 (#9987)mysqlclient was pinned as the functionality of Connection object of MySQL had changed and that it dropped python2.mysqlclient has added context manager interface to Connection which closes the connection on `__exit__` in 2.0.0 and Airflow Master no longer uses Python 2,1
"Revert ""Bump mysqlclient to 2.0.1 (#9987)"" (#9997)This reverts commit c4388127f1c47e1abe512e1ef104c9835a150a04.",4
Remove unnecessary environment variable from CI workflow (#9998),1
Fix typo in api_connexion/openapi/v1.yaml (#9986)`startd_ate_lte` -> `start_date_lte`,5
Avoid sharing session with RenderedTaskInstanceFields write and delete (#9993)Sharing session with RTIF leads to idle-in-transaction timeout error when DAG serialization is enabled and task running duration exceeds the idle-in-transaction timeout setting of the database.,5
Adds separate scheduled-only workflow to cancel duplicates (#9999)Unfortunately cancelling workflows does not work from the workflowsexecuted by forks because their tokens do not allow doing that(they are read only). So we have to run a separate cron-triggeredaction (in the context of apache/airflow repository to cancelall duplicate workflows. This action stops all the workflowsrunning from the same fork/branch except the last one.,1
"Fix typos in README.md (#10000)This commit fixes three typing errors in README.md, consisting of one incorrect spelling of the word released and two incorrect version notations.",2
Add missing x-openapi-router-controller to DAG Run endpoint (#9945),1
Updates the slack WebClient call to use the instance variable - token (#9995)Co-authored-by: Alok Shenoy <ashenoy@coursera.org>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Stop using start_date in default_args in example_dags (2) (#9985),2
Fix cron schedule on cancelling workflow (#10002),1
Add unit tests for GcpBodyFieldValidator in google cloud providers (#10003),1
Add unit tests for MsSqlHook (#10006),1
Fix Markdown escape in UPDATING.md (#10010),5
Add unit tests for GcpBodyFieldSanitizer in Google providers (#9996),1
Add typing to ImapHook (#9887),1
Add EBANX company to README.md (#10012),2
"Cancel duplicate runs and HEAD runs that failed at specific jobs (#10008)This change implements canceling of the workflows in case some ofthe important (prerequisite) jobs failed - such as static checksor docs. The scheduled workflow will run periodically and checkif there are no duplicate workflow runs (it will cancel all butthe most recent one). It will also check for the most recentones and will cancel them if one of the specified jobs (matchedby regular expression) failed.The scenario that we want to handle here:* we want to start tests as soon as possible so that we do not  have to wait for static checks and doc builds* on the other hand as soon as one of the ""quicker"" jobs fail  we want to fail whole workflow so that the workers can be  freed up for other runs.This has to be done as a scheduled run because forked runsdo not have permissions to cancel any of the other runs inthe main repository.",1
Add unit tests for samba provider (#9959),1
Add airflow config get-value command (#9932),1
Use consistent message in SchedulerJob._process_executor_events (#9929),1
Simplify if clauses in ExternalTaskSensor (#9968)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,1
Create a short-link for Airflow Slack Invites (#10034),2
Introduce BaseExecutor.validate_command to avoid duplication (#10033),5
Move e-mail operator to core (#10013),1
Set pytest version to be < 6.0.0 due to breaking changes (#10043)The latest pytest version 6.0.0 released yesterday (2020-07-28) does not work in conjunction with the version of pylint (2.4.3) we are using.,1
Adding new SageMaker operator for ProcessingJobs (#9594),1
Fix docstrings in BigQueryGetDataOperator (#10042),5
Add typing for jira provider (#10005),1
Fix PythonVirtualenvOperator not working with Airflow context (#9394)- automatically add dill requirement if use_dill=True- add howto docs- refactorCo-authored-by: Luis Magana <maganaluis@users.noreply.github.com>,1
UI Graph View: Focus upstream / downstream task dependencies on mouseover (#9303)* graph view mouseover task should increase stroke width of upstream / downstream* mouseover should focus dependencies on mouseover in graph view,1
Burst Virtualenv Cache for Kubernetes Testing (#10070),3
"Update writing in timezone.rst (#10066)The questionable symbol changed in ""naive""",4
Improve docstring note about GKEStartPodOperator on KubernetesPodOperator (#10049)Improve recommendation note for GKEStartPodOperatorCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Allow `image` in `KubernetesPodOperator` to be templated (#10068)fixes https://github.com/apache/airflow/issues/10063,0
Split Display Video 360 example into smaler DAGs (#10077),2
Fix typo on reattach property of kubernetespodoperator (#10056),1
"Fix typo in Athena sensor retries (#10079)Understanding that it is an attribute name, which could have downstreamconsequences, correct the spelling of max_retries and reword some of thedocstring.",2
"Pin google-cloud-kms to ..,<2.0.0 due to breaking changes (#10088)",4
Group UPDATING.md  entries into sections (#10090),5
Move the contribution workflow to the beginning of the file (#10092),2
Combine entries in logging configuration section (#10094),5
Remove `args` parameter from provider operator constructors (#10097),1
Minor fixes in CONTRIBUTING.rst (#10101),0
Move Naming Conventions section in CONTRIBUTING.rst (#10103),4
"Create ""major changes"" section in UPDATING.md (#10100)",5
More user-oriented change titles in Python API sections (#10099),4
Update .asf.yaml (#10110),5
Add unit tests for mlengine_prediction_summary (#10022),3
Fix hook not passing gcp_conn_id to base class (#10075)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,4
Fix sensor not providing arguments for GCSHook (#10074)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,1
Combine entries in UPDATING.md file (#10102),2
Add migration guide for CLI commands (#10078),1
Add missing params to GCP Pub/Sub creation_subscription (#10106)Add missing params to GCP Pub/Sub creation_subscription hook/operator,1
Fixes flaky kubernetes Pod Operator tests (#10111)All Pod Operator tests were using the same task id which made itflaky as sometimes failed tests were still being deleted whilesubsequent success tests already started. That lead to random 404(not found) errors when the failed test was picked up first.,3
Make conn_id unique in Connections table (#9067),1
[AIRFLOW-4541] Replace os.mkdirs usage with pathlib.Path(path).mkdir (#10117)`makedirs` is used in `airlfow.utils.file.mkdirs`  - it is replaced with pathlib now with python3.5+,2
Get rid of pydruid limitation (#9965)Pydruid version 0.5.8 failed on python 3.7 but pydruid 0.5.11fixed it apparently.,0
Add typing annotations to Segment provider (#10120),1
Add try clause to DataFusionHook.wait_for_pipeline_state (#10031)Sometimes it may happen that the pipeline is not visible instantly inDataFusion so retrieving it will result in 404closes: #10030,5
Move stable REST API migration guide to UPDATING.md (#10098),5
Add additional Cloud Datastore operators (#10032)This PR adds more operators for Google Cloud Datastoreservice. It also adds missing tests and how-to guides.,3
Moved webserver background to Quarantine (#10114),4
Delete irrelevant entries from UPDATING.md (#10093),5
Add S3KeysUnchangedSensor (#9817)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Add Legacy command displaying new CLI counterparts (#10115),1
"[GH-9708] Add type coverage to Sendgrid module (#10134)Declare a ""custom"" AddressesType that takes a string or iterable ofstrings, which covers to, cc, and bcc, then declare this and other datatypes for the sendgrid plugin.",5
Status of quarantined tests is stored in Github Issue (#10119),0
Fixed mistyped quarantine.yaml extension (#10139),0
Improve Typing coverage of amazon/aws/athena (#10025)Co-authored-by: Johan Eklund <jeklund@zynga.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,3
Replace file.io with artifacts (#10137),2
Fixes quarantine parsing teething issues (#10145)* wrong issue id (from tests)* comment field was copied from status,3
Retry max 3 times if failing to initialize integration (#10146),5
Do not use .format() in log messages (#10150),2
Update celery and kombu versions (#9496),5
"Use conn_name_attr for SqliteHook connection (#10156)The DbApiHook allows for a conn_name_attr to be changed in subclasses,however SqliteHook's `get_conn` method is always calling the main classattribute. Find the correct attribute and use this to establish theconnection.Allow attr setting outside init for test caseCloses #10147",3
Add type annotations for Sqlite (#10157)Add type annotations for Sqlite's hook and operator.Part of #9708,1
Add type annotation to providers/jenkins (#9947)Part of #9708,1
Add correct signatures for operators in google provider package (#10144),1
Bring back code coverage (#10143)Fixes #10138,0
Remove coverage HTML report artifacts (#10168),3
Documentation artifact are also uploaded as GitHub Actions Artifacts (#10158),2
Add Apache Airflow CODE_OF_CONDUCT.md (#9715),1
Updated parameter definition docs for filesystem.py (#10159),5
Disable wiki. (#10173),5
Remove stat_name_handler attribute in plugins.rst (#10174),0
Improve heading on Email Configuration page (#10175),5
Add docs for airflow config command (#10177)* Add docs for airflow config command* Update docs/howto/email-config.rst,5
Add correct signatures for operators in amazon provider package (#10167),1
Enforce keyword only arguments on apache operators (#10170),1
prevent DAG callback exception from crashing scheduler (#10096),2
Improve handling Dataproc cluster creation with ERROR state (#9593)Handle cluster in DELETING stateExtend testsfixup! Extend testsfixup! fixup! Extend testsfixup! fixup! fixup! Extend tests,3
"Fix BaseSensorOperator soft_fail mode to respect downstream tasks trigger_rule (#8867)Fixes the BaseSensorOperator to make respect the trigger_rule in downstream tasks, when setting soft_fail=""True"".",0
Changes to all the constructors to remove the args argument (#10163),4
Add thredup to list of Airflow users (#10198),1
Add type annotations to S3 hook module (#10164),1
Update guide for Google Cloud Secret Manager Backend (#10172),5
Type annotation for Docker operator (#9733)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,1
Update JS packages to latest versions (#9811) (#9921)Signed-off-by: Raymond Etornam <retornam@users.noreply.github.com>,1
Add correct signature to all operators and sensors (#10205)* add correct signature to operators in providers package* add keyword only to operators and sensors outside provider package* remove unused type ignore,1
You can sync your fork master with apache/airflow master via UI (#10209)We are using newly added feature of GitHub to add manually triggeredworkflow to enable manually-triggered force-syncing of your forkwith apache/airflow.,1
"Pylint checks should be way faster now (#10207)* Pylint checks should be way faster nowInstead of running separate pylint checks for tests and main sourcewe are running a single check now. This is possible thanks to anice hack - we have pylint plugin that injects the right""# pylint: disable="" comment for all test files while readingthe file content by astroid (just before tokenization)Thanks to that we can also separate out pylint checksto a separate job in CI - this way all pylint checks willbe run in parallel to all other checks effectively halfingthe time needed to get the static check feedback and potentiallycancelling other jobs much faster.* fixup! Pylint checks should be way faster now",0
"Fix chart: parameterize namespace (#10213)Replace fixed namespace ""airflow"" with variable {{ .Release.Namespace }}",0
Docs: Separate page for each Secrets backend (#10211),2
"Improves stability of reported coverage and makes it nicer (#10208)* Improves stability of reported coverage and makes it nicerWith this change we only upload coverage report in the casewhen all tests were successuful and actually executed. This meansthat coverage report will not be run when the job gets canceled.Currently a lof of coverage reports gathered contain far lesscoverage because when static check fails or docs some test jobscould already submit their coverage - resulting in partial coveragereports.With this change we also remove comment from coverage reportand replace it with (for now) informational status message publishedto github. If we see that it works, we can change it to aPR-failing status if coverage drops for a given PR.This way we might get our coverage monotonously increasing :).",1
Add airflow connections export command (#9856) (#10081),1
"Handle IntegrityError while creating TIs (#10136)While doing a trigger_dag from UI, DagRun gets created first and then WebServer starts creating TIs. Meanwhile, Scheduler also picks up the DagRun and starts creating the TIs, which results in IntegrityError as the Primary key constraint gets violated. This happens when a DAG has a good number of tasks.Also, changing the TIs array with a set for faster lookups for Dags with too many tasks.",2
Merge similar sections on docs/howto/connection/index.rst (#10224),2
Add missing headinsg on docs/security.rst (#10225),2
Fixed wrong name of workflow in cancel step (#10219),1
Add MeuVendoo to Airflow Users (#10226),1
BoringCyborg Bot: Fix Automated Labels for serialized & secrets (#10228)- docs/howto/use-alternative-secrets-backend.rst has been renamed & split into docs/howto/secrets-backend/*- Add more paths for Serialization,1
Add Apache License to .github/workflows/repo-sync.yml (#10229)`.github/workflows/repo-sync.yml` was missing Apache license,5
Fix typo in .github/ISSUE_TEMPLATE/bug_report.md (#10231)`sytle` -> `style`,0
Fix typos in docs/howto/secrets-backend (#10233)`airflw` -> `airflow``Maanager` -> `Manager``mysq` -> `mysql`,2
Add airflow connections get command (#10214),1
Add labels param to Google MLEngine Operators (#10222),1
"Replace remaining uses of ""bail"" (#10217)",1
"Add Playsimple Games to ""Who uses Apache Airflow?"" (#10253)",1
Fix more typos in docs/ (#10251),2
"Remove Unnecessary list literal in Tuple for Kylin Operator (#10252)It is unnecessary to use a list or tuple literal within a call to tuple.Before:```In [1]: tuple([""ERROR"", ""DISCARDED"", ""KILLED"", ""SUICIDAL"", ""STOPPED""])Out[1]: ('ERROR', 'DISCARDED', 'KILLED', 'SUICIDAL', 'STOPPED')```After:```In [4]: (""ERROR"", ""DISCARDED"", ""KILLED"", ""SUICIDAL"", ""STOPPED"",)Out[4]: ('ERROR', 'DISCARDED', 'KILLED', 'SUICIDAL', 'STOPPED')```",0
Add whitespace around operator in docs/build (#10250),2
Triggering DAG with Future Date (#10249)THis will allow linking this section,2
Fix typo in docs/stable-rest-api/redoc.rst (#10248)`shpinx` -> `Sphinx`,2
Fix link for the Jinja Project in docs/tutorial.rst (#10245)`http://jinja.pocoo.org/docs/dev/` -> `https://jinja.palletsprojects.com/`,2
Add 'apache.beam' to docs/installation.rst (#10244)Also replace `'apache_beam'` to `apache.beam` for extras,2
Improve documentation in docs/start.rst (#10243)`lay` -> `create`Remove ` # if you build with master` as this is the document for Master itself,2
Move celery-exclusive feature to CeleryExecutor page (#10242),4
Add tip about airflow config command on docs/howto/secrets-backend/index.rst (#10239),2
Set language on code-block on docs/howto/email-config.rst (#10238)* Set language on code-block on docs/howto/email-config.rst* fixup! Set language on code-block on docs/howto/email-config.rst,5
Add system tests for CloudSecretManagerBackend (#10235)* Add system tests for CloudSecretManagerBackend* fixup! Add system tests for CloudSecretManagerBackend,3
"Remove redundant ""and_"" condition when using filter (#10232)Multiple criteria may be specified as comma separated; the effect is that they will be joined together using the and_() function ( https://docs.sqlalchemy.org/en/13/orm/query.html#sqlalchemy.orm.query.Query.filter)",2
Create separate section for Cron Presets (#10247),1
Improve guide about Google Cloud Secret Manager Backend (#10257),1
Update example on docs/howto/connection/index.rst (#10236)* Upddate example on docs/howto/connection/index.rst* fixup! Upddate example on docs/howto/connection/index.rst,2
Add Syntax Highlights to code-blocks in docs/best-practices.rst (#10258),2
Disable sentry integration by default (#10212)* Disable sentry integration by default,1
Increse number of runs for quarantined tests (#10220),3
Fix redirects URLs (#10259),0
"Create ""Managing variable"" in howto directory (#10241)",1
Fix Warning when using a different Sphinx Builder (#10262),1
Fix various typos in the repo (#10263),2
Add Missing Apache Providers to docs/installation.rst (#10265),2
Added DataprepGetJobsForJobGroupOperator (#10246),5
Fixed GitHub Actions badge (#10268),0
Add Amazon SES hook (#10004)- refactor airflow.utils.email and add typing,1
"Revert ""Add Amazon SES hook (#10004)"" (#10276)This reverts commit f06fe616e66256bdc53710de505c2c6b1bd21528.",4
Add more columns to airflow connections get (#10269),1
Update Gojek in who uses list (#10281),1
Makes multi-namespace mode optional (#9570)Running the airflow k8sexecutor with multiple namespace abilitiesrequires creating a ClusterRole which can break existing deploymentsCo-authored-by: Daniel Imberman <daniel@astronomer.io>,4
Remove duplicate line from 1.10.10 CHANGELOG (#10289),4
Fix KubernetesPodOperator reattachment (#10230),1
"Add type annotations to AwsGlueJobHook, RedshiftHook modules (#10286)",1
Add Authentication for Stable API (#10267),1
"Add reconcile_metadata to reconcile_pods (#10266)metadata objects require a more complex merge strategythen a simple ""merge pods"" for merging labels and otherfeatures",7
Add unittest for WasbTaskHandler (#10284),0
Use Hash of Serialized DAG to determine DAG is changed or not (#10227)closes #10116,4
Run create-user-job as user with specified id (#10291)In secured cluster there is a need to run this job with specific user id,1
Docs: Clarify DAG to image language (#10296),2
DbApiHook: Support kwargs in get_pandas_df (#9730)* DbApiHook: Support kwargs in get_pandas_df* BigQueryHook: Support kwargs in get_pandas_df* ExasolHook: Support kwargs in get_pandas_df* PrestoHook: Support kwargs in get_pandas_df* HiveServer2Hook: Support kwargs in get_pandas_df,1
Fixes name of pre-commit cache for multiple branches (#10299),0
Clarify connection docs (#10294),2
Enable Sphinx spellcheck for doc generation (#10280),2
Add ClusterPolicyViolation support to airflow local settings (#10282)This change will allow users to throw other exceptions (namely `AirflowClusterPolicyViolation`) than `DagCycleException` as part of Cluster Policies.This can be helpful for running checks on tasks / DAGs (e.g. asserting task has a non-airflow owner) and failing to run tasks aren't compliant with these checks.This is meant as a tool for airflow admins to prevent user mistakes (especially in shared Airflow infrastructure with newbies) than as a strong technical control for security/compliance posture.,1
Use more human readable table heading labels on DAG details (#10305),2
Restrict google-cloud-dataproc to <2.0.0 (#10307),5
Limit all google-cloud api to <2.0.0 (#10317)Google Cloud APIs introduced breaking changes in 2.0.0(https://github.com/googleapis/python-container/blob/master/UPGRADING.md)and they already caused a number of changes. We should (for now - beforewe migrate to 2.0+ ) limit all our google-cloud deps to <2.0.0Fixes #10316,0
Implement Google BigQuery Table Partition Sensor (#10218),2
Unpin pytest (#10314)The pytest bug in 6.0.0 has been fixed in 6.0.1.See changelog for details: https://docs.pytest.org/en/stable/changelog.html#pytest-6-0-1-2020-07-30,3
"Added ""sharded"" word to spellchecker (#10320)",1
Respect DAG Serialization setting when running sync_perm (#10321)We run this on Webserver Startup and when DAG Serialization is enabled we expect that no files are required but because of this bug the files were still looked for.,2
Catch Permission Denied exception when getting secret from GCP Secret Manager. (#10326),1
Fix clear future recursive when ExternalTaskMarker is used (#9515),1
Webserver: Sanitize values passed to origin param (#10334),2
Improve idempotency of BigQueryInsertJobOperator (#9590)Co-authored-by: Jacob Ferriero <jferriero@google.com>,1
"CI: Fix failing docs-build (#10342)CI is failing because of incorrect spelling ""everytime"", it should be ""every time""",1
Add ingress to the helm chart (#10064)Co-authored-by: Alikhan <alikhan.tagybergen@tomtom.com>Co-authored-by: alikhtag <43503284+alikhtag@users.noreply.github.com>,1
"Fixes rat-check pre-commit in case Airflow is added as subrepo (#10347)In case Apache Airflow directory is added as subrepo, a new.gitrepo file is created in the Airflow sources. When you try torun pre-commit checks, the RAT check fails in this case.Adding it to .rat-excludes fixes the problem",0
Breeze was slightly too chatty when there was no dirs created (#10346),1
More informative description of Breeze's --verbose flag. (#10348),5
Breeze: More fancy environment checking (#10329)* More fancy environment checking* fixup! More fancy environment checking,0
Improve language of a BaseSensorOperator in UPDATING.md (#10332),5
Add Bigtable Update Instance Hook/Operator (#10340)Add Bigtable Update Instance Hook/Operator,1
"Add type annotations for mlengine_operator_utils (#10297)Add type annotations, including a few changes to ensure the right typesare passed through. Specifically, if region is not given, it must beprovided in the DAG's default_args.",2
add more precise type hint for task callbacks (#10355),1
Add test for GCSTaskHandler (#9600) (#9861)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,0
Fix AwsGlueJobSensor to stop running after the Glue job finished (#9022)* Extract get_job_state and fix poke of AwsGlueJobSensor* Save hook and reuse in GlueJobSensor* Add descriptions for some functions* Fix tests according to changed function definition* Fix too long line* Add type hints and apply review* Fix type errorCo-authored-by: JB Lee <jb.lee@sendbird.com>,5
Add redbubble link to Airflow merch (#10359)* Add redbubble link to Airflow merch* Update README.mdCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Docs: Seperate page for each security topic (#10352),2
Add to CONTRIBUTING.rst link to backport packages troubleshooting (#10360),2
Add typing coverage to mysql providers package (#10095),1
"Use check_output to capture in celery task (#10310)See: https://docs.python.org/3/library/subprocess.html#subprocess.CalledProcessErrorThe check_call does not set output to the subprocess.CalledProcessError so the log.error(e.output) code is always None.By using check_ouput, when there is CalledProcessError, it will correctly log the error output",0
Simplified GCSTaskHandler configuration (#10365),5
Pylintable list-integrations.py (#10378)Extracted from #10368,4
Simplify cron preset language in docs (#10370),2
Too much was happening in this pre-commit script (#10345),5
You can disable spellcheck or documentation when building docs. (#10377)This cleans up the document building process and replaces itwith breeze-only. The original instructions with`pip install -e .[doc]` stopped working so there is nopoint keeping them.Extracted from #10368,4
Docs: Fix spacing bug in 'Dag Run' (#10372),1
Update github flags for Breeze (#10384)Part of #10368,5
"Expand JenkinsJobTriggerOperator unit tests (#10353)Using the parameterized library, add unit test coveragefor JenkinsJobTriggerOperator parameters, covering parametersas strings or as a list of strings.",2
Docker images are now consistently labelled and a bit smaller (#10387)Extracted from #10368,4
Add guide about custom API authentication (#10312),1
Consistently refer to section names (#10369),5
Correct verb tense for re-running task doc. (#10371),2
Add basic auth API auth backend (#10356),1
Moved description of page size limit to security/ (#10392)* Moved description of page size limit to security/* fixup! Moved description of page size limit to security/,4
Capitalize 'Python' properly in Concepts docs (#10398),2
"When precommits are run, output is silenced (#10390)The output of pre-commit builds on both CI and locallyis now limited to only show errors, unless verbosevariable is set.We are utilising aliases if possible but in case ofpre-commits they are run in non-interactive shell whichmeans that aliases do not work as expected so we haveto run a few functions directly in other toshow spinner.Extracted from #10368",4
Add back 'refresh_all' method in airflow/www/views.py (#10328)closes https://github.com/apache/airflow/issues/9749,0
Update celery.rst (#10400)Observed a change. Suggesting the same.,4
Improve headings on docs/executor (#10396)* Improve heading on docs/executor* Update docs/executor/index.rst,2
Correct typo in best-practices.rst (#10401),2
Kubernetes image is extended rather than customized (#10399)The EMBEDDED dags were only really useful for testingbut it required to customise built production image(run with extra --build-arg flag). This is not neededas it is better to extend the image instead with FROMand add dags afterwards. This way you do not haveto rebuild the image while iterating on it.,2
"Replaced aliases for common tools with functions. (#10402)This allows for all the kinds of verbosity we want, includingwriting outputs to output files, and it also works out-of-the-boxin git-commit non-interactive shell scripts. Also as a side effectwe have mocked tools in bats tests, which will allow us to writemore comprehensive unit tests for the bash scripts of ours(this is a long overdue task).Part of #10368",3
Constraint CI scripts are now separated out (#10404)Part of #10368,5
Move docker-compose ci.yml to ga.yml as it is GITHUB_* only (#10405),5
Fixes optimisation where doc only change should build much faster (#10344),4
Remove run-ons from scheduler docs. (#10397),2
BugFix: K8s Executor Multinamespace mode is evaluated to true by default (#10410),0
Make Kubernetes tests pass locally (#10407)* Make Kubernetes tests pass locallyCurrently Kuberentes tests only all pass within breeze.This PR makes them read the local path so they can pass in anysystem.* static tests,3
Make KubernetesExecutor recognize kubernetes_labels (#10412)KubernetesExecutor needs to inject `kubernetes_labels` configsinto the worker_config,5
Group logging & monitoring guides in one section (#10394),2
"Use sys.exit() instead of exit() (#10414)The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from `site.py`. However, if the interpreter is started with the `-S` flag, or a custom `site.py` is used then `exit` and `quit` may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.Previously, `exit()` was used and wouls fail if the interpreter is passed the `-S` option.",4
Unnecessary use of list comprehension (#10416)It is better to use list constructor which is faster and more readable,1
"Fix broken breeze script (#10418)Previosuly it was failing with `unbound variable AIRFLOW_PROD_BASE_TAG` and failing because it could not find ""kind"" binary",1
"CI Images are now pre-build and stored in registry (#10368)* CI Images are now pre-build and stored in registryWith this change we utilise the latest pull_request_targetevent type from Github Actions and we are building theCI image only once (per version) for the entire run.This safes from 2 to 10 minutes per job (!) depending onhow much of the Docker image needs to be rebuilt.It works in the way that the image is built only in thebuild-or-wait step. In case of direct push run orscheduled runs, the build-or-wait step builds and pushesto the GitHub registry the CI image. In case of thepull_request runs, the build-and-wait step waits untilseparate build-ci-image.yml workflow builds and pushesthe image and it will only move forward once the imageis ready.This has numerous advantages:1) Each job that requires CI image is much faster because   instead of pulling + rebuilding the image it only pulls   the image that was build once. This saves around 2 minutes   per job in regular builds but in case of python patch level   updates, or adding new requirements it can save up to 10   minutes per job (!)2) While the images are buing rebuilt we only block one job waiting   for all the images. The tests will start running in parallell   only when all images are ready, so we are not blocking   other runs from running.3) Whole run uses THE SAME image. Previously we could have some   variations because the images were built at different times   and potentially releases of dependencies in-between several   jobs could make different jobs in the same run use slightly   different image. This is not happening any more.4) Also when we push image to github or dockerhub we push the   very same image that was built and tested. Previously it could   happen that the image pushed was slightly different than the   one that was used for testing (for the same reason)5) Similar case is with the production images. We are now building   and pushing consistently the same images accross the board.6) Documentation building is split into two parallel jobs docs   building and spell checking - decreases elapsed time for   the docs build.7) Last but not least - we keep the history of al the images   - those images contain SHA of the commit. This means   that we can simply download and run the image locally to reproduce   any problem that anyone had in their PR (!). This is super useful   to be able to help others to test their problems.* fixup! CI Images are now pre-build and stored in registry* fixup! fixup! CI Images are now pre-build and stored in registry* fixup! fixup! fixup! CI Images are now pre-build and stored in registry* fixup! fixup! fixup! CI Images are now pre-build and stored in registry",1
Enable optimisation of image building. (#10422)Follow up after #10368,0
Fix typo in KubernetesPodOperator (#10419)`kubernetesOperator` -> `KubernetesPodOperator`,1
Fix failing breeze (#10424)Breeze failed after #10368,0
Switch to released cancel-workflow-runs action (#10423)Follow up after #10368,1
Add architecture diagram for basic Airflow deployment (#10428)This is primarily intended for showing the architecture for a basic Airflow development setup.,1
Dataflow operators don't not always create a virtualenv (#10373),1
Amazon SES Hook (#10391)* Add Amazon SES hook* Add SES Hook to operators-and-hooks documentation.* Fix arguments for parent class constructor call (PR feedback)* Fix indentation in operators-and-hooks documentation* Fix mypy error for argument on call to parent class constructor* Simplify logic on constructor (PR feedback)* Add custom headers and other relevant options to hook* Change pylint exception rule to apply it only to function instead of module (PR feedback)* Fix spellcheck error* Vendorize airflow.utils.emaail* fixup! Vendorize airflow.utils.emaailCo-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,0
Fix Breeze failure on MacOS (#10440)Fixes #10438,0
Add AzureBaseHook (#9747)- refactor/change azure_container_instance to use AzureBaseHook- add info to operators-and-hooks-ref.rst- add howto docs for connecting to azure- add auth mechanism via json config- add azure conn type,1
Add update endpoint for DAG (#9101) (#9740),2
Building backport packages generates README files (#10445),2
Be nice to fork repositories when it comes to scheduled events (#10448)Only runs scheduled CI runs in the 'apache/airflow' forks,1
Make system test work with 1.10 (#10444),1
"Do not override in_container scripts when building the image (#10442)After #10368, we've changed the way we build the imageson CI. We are overriding the ci scripts that we useto build the image with the scripts taken from masterto not give roque PR authors the possibiility to runsomething with the write credentials.We should not override the in_container scripts, howeverbecause they become part of the image, so we should usethose that came with the PR. That's why we have to movethe ""in_container"" scripts out of the ""ci"" folder andonly override the ""ci"" folder with the one frommaster. We've made sure that those scripts in ciare self-contained and they do not need reach outside ofthat folder.Also the static checks are done with local files mountedon CI because we want to check all the files - not onlythose that are embedded in the container.",2
Change provider configuration keys for OAuth (#9759),5
Fix port number in webserver for kind setup (#10452),1
Mount gcloud kubeconifg to breeze (#10439)* Mount gcloud kubeconifg to breeze* Update scripts/ci/docker-compose/forward-credentials.ymlCo-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
Fixes problem with Python image not ready to be pushed (#10430)When we pull image from base registry we should also pushit to the GitHub Registry so that we can track theorigin of the run.Note that it merely pushes the tag - the image will bedirectly pulled from the registry so this will only updatethe tag in registry.Bug introduced in #10368,0
"Fix typo in ""wait_for_done"" (#10458)`waitfordone` -> `wait_for_done`",2
Fix identation in executor_config example (#10467),5
"Remove redudandant checks in test_views.py (#10464)- `self.check_content_in_response` already checks that response code is 200- `self.assertEqual(None, ...)` -> `self.assertIsNone(...)`- Fix typo: ""succcess"" -> `success`",2
Test exact match of Executor name (#10465)Use `self.assertEqual` instead of `self.assertIn` to do an exact match of string name instead of partial match,3
"Use assertEqual instead of assertTrue in tests/utils/test_dates.py for proper diff (#10457)assertEqual will show show the proper diff instead of just ""False is not True"" error",0
Replace assigment with Augmented assignment (#10468),5
Add Type Annotations & Docstrings to airflow/models/dagrun.py (#10466),2
Fix typos in scripts/perf/scheduler_dag_execution_timing.py (#10463),2
Remove mentions of Airflow Gitter (#10460),4
Fix typo in timed_out (#10459)`timeouted` -> `timed_out`,2
"Stops running workflow_run for scheduled runs in forks (#10473)There is still one build running for forks regularly, even thoughwe disabled all scheduled runs in #10448, there is still onecase with nightly build that we should disable.The run is the ""workflow_run""executed for the nightly scheduled ""CI Build"" run that still getstriggered.This change skips those run in forjs in case the ""source event""is ""schedule""",1
Fixes quoting bug introduced in #10473 (#10477)Copy pasted curly quotes from a web page <facepalm>,0
Fix broken Kubernetes PodRuntimeInfoEnv (#10478)closes https://github.com/apache/airflow/issues/10456,0
Change Support Request template to a link to Slack (#10480)* Change Support Request template to a link to Slack* Update .github/ISSUE_TEMPLATE/config.ymlCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
Fixes S3ToRedshift COPY query (#10436)* fix: 🐛 Wrong S3 URI on COPY queryThe S3 URI on COPY query was appending the target Redshift table to theS3 object key.* test: 💍 Fixed typo on test queryThe COPY query that the operator used is the same query the test uses.,1
"Optimise production image building during k8s tests on CI (#10476)We do not have to rebuild PROD images now because we changedthe strategy of preparing the image for k8s tests instead ofembedding dags during build with EMBEDDED_DAGS build arg weare now extending the image with FROM: clause and add dagson top of the PROD base image. We've been rebuilding theimage twice during each k8s run - once in Prepare PROD imageand once in ""Deploy airflow to cluster"" both are not neededand both lasted ~ 2m 30s, so we should save around 5m for everyK8S jobi (~30% as the while K8S test job is around 15m).",3
Fix broken Markdown refernces in Providers README (#10483)`#provider-class-summary` -> `#provider-classes-summary`,1
"Move perf_kit to tests.utils (#10470)Perf_kit was a separate folder and it was a problem when we tried tobuild it from Docker-embedded sources, because there was a hidden,implicit dependency between tests (conftest) and perf.Perf_kit is now moved to tests to be avaiilable in the CI imagealso when we run tests without the sources mounted.This is changing back in #10441 and we need to move perf_kitfor it to work.",1
"Fix duplicate task_ids in example_http.py (#10485)`task_get_op` was previously defined twice, hence the first one was not used when the DAG was run",1
Fix typo in Facebook Ads Provider (#10484)`missings_keys` -> `missing_keys`,1
Remove old configuration from BoringCyborg (#10490)Signed-off-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,5
Removed the prerequisite for perf-kit path augmentation (#10492),4
Make macros.hive pylint compatible (#10495),1
Make Stats pylint compatible (#10496),1
"GitHub Registry is now lowercase (#10489)GitHub Registry must be lowercase and we are now using it tostore images/packages. We lowercase it now and we alsomade sure that GITHUB_REGISTRY variables are only usedwhen GITHUB_REGISTRY is set to ""true""",1
Add support for creating multiple replicated clusters in Bigtable hook and operator (#10475)* Add support for creating multiple Bigtable replicas* Flake8 fix,0
Implement impersonation in google operators (#10052)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,1
Fixes uploading of doc artifacts. (#10441)Requires #10470 and #10472,1
"Mounting from sources is disabled for tests (#10472)We had to enable mounting from sources for a short whilebecause we had to find a way to add new scripts to the""workflow_run"" workflow we have. This also requiresthe #10470 to be merged - perf_kit to be moved to tests.utils becauseit was in a separate directory and image without mounting sourcescould not run the tests.It also partially addresses the #10445 problem wherethere was difference between sources in the image and comingfrom the master. This comes from GitHub running merge onnon-conflicting changes in the PR and something that willbe addressed shortly.The issue #10471 discusses this in detail.",0
Add instructions to verify the release candidate (#10493),5
Updating Auth0 contributors (#10432),5
Make models/crypto.py Pylint-compatible (#10500),1
Alphabetize committer list (#10512),5
Add Hurb.com as Airflow User (#10518),1
Cleanup Astronomer contributors list in README (#10520),4
Fix impersonation related bug in bigtable tests (#10521)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,3
"Sets default timeout for the job waiting for images (#10517)In normal circumstances those jobs will wait for a short time(4-15 minutes depenfding on the state of the base image).However there might be some cases when there are a lot of jobsor when there is some queueing problems in GitHub thatthe ""Build Images"" job will be queued and not start quickly.This happened on 24th of August 2020 for example when severaljobs failed because the ""Build Image"" was queued and onlyrun after the ""CI Build"" job timed out.Usually those situations tends to be resolved by GitHub supportor they resolve themselves as the jobs will be finishing andfreing the queue. However in those cases we should give thewaiting job as much time as GitHub Action allows by defaultfor the job to run (360 minutes). This is no harm - we canalwayc cancel those jobs manually and they are just twojobs running so it should not cause any problem.Note that if someone would see that the job is running fora long time - the contributor will likely push amendedcommit and it will also cancel such waiting job, sothis is even less likely to have long runnning waiting jobs.",1
Improve direct impersonation documentation (#10506)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,2
Make www/views.py pylint compatible (#10498),1
Updated documentation for the CI with mermaid sequence diagrams (#10380),2
"Remove all ""noinspection"" comments native to IntelliJ (#10525)We have already fixed a lot of problems that were markedwith those, also IntelluiJ gotten a bit smarter on notdetecting false positives as well as understand morepylint annotation. Wherever the problem remainedwe replaced it with # noqa comments - as it isalso well understood by IntelliJ.",0
"Fix typo in the word ""release"" (#10528)`relase` -> `release`",2
Make models/taskinstance.py pylint compatible (#10499),1
Make configuration.py Pylint compatible (#10494),5
Make www/utils.py pylint-compatible (#10497),1
Fix typo in PostgresHook (#10529)`PostrgresHook` -> `PostgresHook`,1
Improving descriptions in OpenAPI (#10417)* Improving descriptions in OpenAPI* fixup! Improving descriptions in OpenAPI* fixup! fixup! Improving descriptions in OpenAPI,1
"Fix typo in ""Success"" (#10537)""Sucess"" ->  ""Success""",2
"Fix typo in ""Cloud"" (#10534)`CLoud` -> `Cloud`",2
Remove unreachable code in test_user_command.py (#10526),3
"Add a possibility to switch back to building images by secret (#10509)You can now define secret in your own fork:AIRFLOW_GITHUB_REGISTRY_WAIT_FOR_IMAGEIf you set it to ""false"", it skips building images in separateworkflow_run - images will be built in the jobs run in theCI Build run and they won't be pushed to the registry.Note - you can't have secrets starting with GITHUB_, that's whythe AIRFLOW_* prefix",0
Fix TestAWSDataSyncOperatorUpdate.__init__ method (#10536)`__init` -> `__init__`,5
PyDocStyle: Enable D403: Capitalized first word of docstring (#10530),2
Updated REST API call so GET requests pass payload in query string instead of request body (#10462)* Updated REST API call so GET requests pass payload in query string instead of request body* Updated comparisons to use in to follow better standards* Added whitespace for pylint failure* Update Databricks hooks tests to reflect new payload* Fixed trailing whitespace in unit testCo-authored-by: Steven Yu <steven@databricks.com>,5
Remove mlsd function from hooks/ftp.py (#10538)This function was backported for compatibility with Py 2.7. FTP.mlsd was added in Python 3.3https://docs.python.org/3.6/library/ftplib.html#ftplib.FTP.mlsd,2
Improve descriptions in OpenAPI Spec file (#10539),2
Fix typo Sucess to Success (#10540),2
PyDocStyle: No whitespaces allowed surrounding docstring text (#10533),2
Add update mask to patch dag endpoint (#10535),2
Enable Black on Connexion API folders (#10545),0
Fix failing Black test on connexion (#10547),3
Add introduction to Stable RESTT API (#10548),1
The PIP version is not pinned to 19.0.2 any more (#10542)Fixes #10516,0
Helm Docker image sources are now included in the Airlfow codebase (#9650)We can now build all the images from Airlfow sources ina reproducible fashion and our users can use the helm chartbased on the images build from official images + code inAirflow Codebase.We also have consistent versioning scheme based oncalver version of releasing the images coupled withthe version of the original package.Part of #9401,2
Enable Black on Providers Packages (#10543),1
Bring back some inclusions before we solve cyclic deps problems (#10551),0
Add Airflow 1.10.12 Changelog & Updating guide (#10558),5
Port isort config from pre-commit to setup.cfg (#10557)Based on https://github.com/apache/airflow/pull/10543#issuecomment-680231216,0
Update configs added in 1.10.12 (#10561),1
Migrate companies list to INTHEWILD.md (#10563),5
Add content to file (forgot to save before committing) (#10565),2
Add License to INTHEWILD.md (#10570),1
Move more operators/hooks/sensors to fundamentals (#10567),1
Update CHANGELOG complete class name (#10573),4
Untangle cyclic deps configuration <> secrets (#10559),5
"[k8s] Store the raw ti key info to pod annotations (#10568)The value of annotations can store the raw dag_id, task_id andexecution_date so that k8s executor can easily map pod event backto the task instance",5
Add OpenSlate to INTHEWILD.md (#10581),1
Add Jobrapido to INTHEWILD.md  (#10583)Co-authored-by: Mattia Giupponi <mattia.giupponi@jobrapido.com>,1
Improve .mailmap (#10582),1
updated official docker images in README (#10579),2
Spark-on-K8S sensor - add driver logs (#10023),2
Enhanced the Kubernetes Executor doc  (#10433)A simple architecture diagram to show the Airflow setup when used with the Kubernetes executor,1
Improve Docstring for AWS Athena Hook/Operator (#10580),1
Fix typo in Custom XCom backend (#10588),2
Remove Outdated SQLCheckOperator Docstring (#10589)This method is now implemented,2
Add info about update mask to API doc introduction (#10572),2
Add Airflow 1.10.12 & 'black' to breeze-complete (#10592),1
Documentation for Google Cloud Data Loss Prevention (#8201) (#9651),5
DockerOperator extra_hosts argument support added (#10546),1
URL encode execution date in the Last Run link (#10595)Fixes #10434,0
"Nightly tag push is not skipped in scheduled builds (#10597)With recent refactors, nightly tag was not pushed onscheduled event because it was depending on pushing imagesto github registry. Pushing images to github registry isskipped on scheduled builds, so pushing tag was also skipped.",1
Add Currency to INTHEWILD.md (#10607),1
Made use of authentication consistent (#10610)Fixed a couple of places where authorization was used instead of authentication,1
Added a logging and monitoring architecture diagram and page (#10609),2
Fix downstream rendering in WebUI (#10612)closes #10611,0
Revert Clean up DAG serializations based on last_updated (#7424) (#10613)This PR reverts the behavior of https://github.com/apache/airflow/pull/7424,4
Update Google Cloud branding (#10615),5
Fix Google DLP example and improve ops idempotency (#10608),1
Wrong key in DAGs Persistent Volume Claim (#10627)Co-authored-by: Flavien Dereume-Hancart <flavien@LL-PC0BE1K9-1.goiba.net>,2
Removed bad characters from AWS operator (#10590),1
Fix broken master - DLP (#10635),0
Add airflow cheat-sheet command (#10619),1
Improve output of check_environment.sh (#10631),1
Update scheduler deployment - dags volume mount (#10630),2
Helm Chart is using 1.10.12 image by default (#10639),1
"Exclude CSRF tokens in Log's attribute ""extra"" in database (#10640)",5
"Move roles to CONTRIBUTING.rst (#10327)* feat: add initial roles, committers and contributors* refactor: move roles to after contributions* fix: remove extra spaces* fix: fix text according to PR discussion* refactor: move roles to 2nd point* Update CONTRIBUTING.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
Update Google Cloud branding (#10642),5
Improve logging & monitoring docs (#10618)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
"Fix typos: duplicated ""the"" (#10647)",2
Update INTHEWILD.md (#10649)Added Datasprints as INTHEWILD markdown section,5
fix type hints for s3 hook read_key method (#10653),1
Adds pip-wheel metadata in .gitignore (#10657),5
Small fixes in Breeze/Static check/docs documentation (#10658),2
"Implement Google Shell Conventions for breeze script … (#10651)Part of #10576First (and the biggest of the series of commits to introduceGoogle Shell Conventions in our bash scripts.This is about the biggest and the most complex breeze scriptso it is rather huge but it is difficult to split it intosmaller pieces.The rules implemented (from the conventions): * constants and exported variables are CAPITALIZED, where   local/temporary variables are lowercase * following the shell guide, once all the variables are set to their   final values (either from exported variables, calculation or --switches   ) I have a single function that makes all the variables read-only. That   helped to clean-up a lot of places where same functions was called   several times, or where variables were defined in a few places. Now the   behavior should be rather consistent and we should easily catch some   duplications * function headers (following the guide) explaining arguments,   variables expected, variables modified in the functions used. * setting the variables as read-only also helped to clean-up the ""ifs""   where we often had "":=}"" in variables and != """" or == """". Those are   replaced with `=}` and tests are replaced with `-n` and `-z` - also   following the shell guide (readonly helped to detect and clean all   such cases). This also should be much more robust in the future. * reorganized initialization of those constants and variables - simplified   a few places where initialization was overlapping. It should be much more   straightforward and clean now * a number of internal function breeze variables are ""local"" - this is   helpful in accidental variables overwriting and keeping stuff localized * trap_add function is separated out to help in cases where we had   several traps handling the same signals.",1
Adds 'cncf.kubernetes' package back to backport provider packages. (#10659),1
Display conf as a JSON in the DagRun list view (#10644)Co-authored-by: Marco Aguiar <marco@DESKTOP-8IVSCHM.localdomain>,2
Unify error messages and complete type field in response (#10333)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add howto doc for Salesforce connection (#10482),1
Add example on airflow users create --help (#10662),1
"Remove requirements from the project. (#10668)The requirements are not needed any more. We replaced themwith a new, better ""constraints"" mechanism where constraintsare stored in a separate, orphaned branches in the repositoryand they are automatically maintained by the CI process.See more about our dependency management process here:https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pinned-constraint-files",2
Add placement_strategy option (#9444),1
Add Dataprep operators (#10304)Add DataprepGetJobGroupOperator and DataprepRunJobGroupOperatorfor Dataprep service.Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>,5
Add packages to function names in bash (#10670)Inspired by the Google Shell Guide where they mentionedseparating package names with :: I realized that this wasone of the missing pieces in the bash scripts of ours.While we already had packages (in libraries folders)it's been difficult to realise which function is where.With introducing packages - equal to the library file namewe are *almost* at a level of a structured language - andit's easier to find the functions if you are looking for them.Way easier in fact.Part of #10576,1
Add `log_id` field to log lines on ES handler (#10411)* Add `log_id` field to log lines on ES handler* Add `offset` field to log lines on ES handlerit will be set to the epoch timestamp in nanoseconds (this will just beused for ordering log lines when displayed in the webserver UI).* Update UPDATING.mdWith information regarding log_id and offset fields in JSON log lines written to stdout,2
Fix format of install commands (#10676),0
Improve getting started section (#10680),1
Remove airflow-pr tool  (#10675)* Remove airflow-pr tool* Add PyGithub back in* Remove gitpython,4
Update INTHEWILD.md (#10683),5
Remove redundant section from dev/README.md toc (#10689),2
[AIRFLOW-XXX] Add task execution process on Celery Execution diagram (#6961),1
docs: They added support for celltags to Jupyter Lab (#9141),1
Unify command names in CLI (#10669)* Unify command names in CLI,5
"Add on_kill support for the KubernetesPodOperator (#10666)This PR ensures that when a user kills a KubernetesPodOperator taskin the airflow UI, that the associated pod is also killed using theon_kill method.",1
"Revert recent breeze changes (#10651 & #10670) (#10694)* Revert ""Add packages to function names in bash (#10670)""This reverts commit cc551ba793344800d2d396c13d7fd0c8eed97352.* Revert ""Implement Google Shell Conventions for breeze script … (#10651)""This reverts commit 46c8d6714c981746cc114b8b1af5cb27aa0018e2.",4
Fix failing black test (#10697)* Fix failing black test,3
Fix missing dash in flag for statsd container (#10691)Co-authored-by: Kamil Olszewski <kamil.olszewski@polidea.com>,0
Chart: Flower deployment should use Flower image (#10701)Co-authored-by: Steven Miller <sjmiller609@gmail.com>,1
"Implement Google Shell Conventions for breeze script (#10695)* Implement Google Shell Conventions for breeze script … (#10651)Part of #10576First (and the biggest of the series of commits to introduceGoogle Shell Conventions in our bash scripts.This is about the biggest and the most complex breeze scriptso it is rather huge but it is difficult to split it intosmaller pieces.The rules implemented (from the conventions): * constants and exported variables are CAPITALIZED, where   local/temporary variables are lowercase * following the shell guide, once all the variables are set to their   final values (either from exported variables, calculation or --switches   ) I have a single function that makes all the variables read-only. That   helped to clean-up a lot of places where same functions was called   several times, or where variables were defined in a few places. Now the   behavior should be rather consistent and we should easily catch some   duplications * function headers (following the guide) explaining arguments,   variables expected, variables modified in the functions used. * setting the variables as read-only also helped to clean-up the ""ifs""   where we often had "":=}"" in variables and != """" or == """". Those are   replaced with `=}` and tests are replaced with `-n` and `-z` - also   following the shell guide (readonly helped to detect and clean all   such cases). This also should be much more robust in the future. * reorganized initialization of those constants and variables - simplified   a few places where initialization was overlapping. It should be much more   straightforward and clean now * a number of internal function breeze variables are ""local"" - this is   helpful in accidental variables overwriting and keeping stuff localized * trap_add function is separated out to help in cases where we had   several traps handling the same signals.(cherry picked from commit 46c8d6714c981746cc114b8b1af5cb27aa0018e2)(cherry picked from commit c822fd7b4bf2a9c5a9bb3c6e783cbea9dac37246)* fixup! Implement Google Shell Conventions for breeze script … (#10651)",0
Add packages to function names in bash (#10670) (#10696)Inspired by the Google Shell Guide where they mentionedseparating package names with :: I realized that this wasone of the missing pieces in the bash scripts of ours.While we already had packages (in libraries folders)it's been difficult to realise which function is where.With introducing packages - equal to the library file namewe are *almost* at a level of a structured language - andit's easier to find the functions if you are looking for them.Way easier in fact.Part of #10576(cherry picked from commit cc551ba793344800d2d396c13d7fd0c8eed97352)(cherry picked from commit 2bba276f0f06a5981bdd7e4f0e7e5ca2fe84f063),1
Update INTHEWILD.md (#10703),5
Change the name of Static Check without pylint (#10690),4
"Add jupytercmd and fix task failure when notify set as true in qubole operator (#10599)Add jupytercmd in Qubole Operator which fires a JupyterNotebookCommand to the jupyter notebooks running on user's QDS account. Along with this, we have fixed a minor bug that caused the tasks to fail with --notify is set in Qubole Operator.Co-authored-by: Aaditya Sharma <asharma@qubole.com>",1
Make Cloud Build system tests setup runnable (#10692)This change fixes error: open(quickstart.sh): Permission deniedthat was rised during git add.,1
[AIRFLOW-5948] Replace SimpleDag with SerializedDag (#7694),2
Update DAG Serialization docs (#10711)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Migrate speccy to spectral in OpenAPI linting. (#10351),5
"Don't commit when explicitly passed a session to TI.set_state (#10710)The `@provide_session` wrapper will already commit the transaction whenreturned, unless an explicit session is passed in -- removing thisparameter changes the behaviour to be:- If session explicitly passed in: don't commit (caller's  responsibility)- If no session passed in, `@provide_session` will commit for us already.",1
Make test_trigger_rule_dep tests re-runnable (#10712)If we run this test(TestTriggerRuleDep::test_get_states_count_upstream_ti specifically)more than once without clearing the DB in between it would fail due to aunique constraint violation.,0
"Ensure we heartbeat the DagFileProcessorManager regularly. (#10706)It could have wedged, (but the process still be alive) and we wouldnever notice.In this I use `time.monotonic` rather than a `datetime` object for tworeasons:1. We don't need the expense of a ""full"" date time object since all we    care about is the second diff between two points in time.2. It is ""more correct"" as `datetime.now()` would be inaccurate if the   system clock changes (NTP etc.)",4
Add Stacktrace when DagFileProcessorManager gets killed (#10681),1
Add Indeed to INTHEWILD.md (#10716),1
Unify command names in CLI (#10720)* Unify command names in CLI* fixup! Unify command names in CLI,0
Add generate_yaml command to easily test KubernetesExecutor before deploying pods (#10677)* Add generate_template command for kubernetes_executor* move import* fix test failure* Address @mik-laj comments* Address @mik-laj comments* Use current dir* add docs* fix test,3
"Relax requirement to allow latest version of flask-login (#9990)The current latest available version is 0.5.0 (https://pypi.org/project/Flask-Login/0.5.0/)Version 0.5.0 drops support for Python 2.6, 3.3, 3.4 and we don't use those versions in Airflow Master",1
Make grace_period_seconds option on K8sPodOperator (#10727)* Make grace_period_seconds option on K8sPodOperatorThis PR allows users to choose whether they want to gracefully killpods when they delete tasks in the UI or if they would like toimmediately kill them.* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
"Switches to better BATS asserts (#10718)BATS has additional libraries of asserts that are much morestraightforward and nicer to write tests for bash scriptsThere is no dockerfile from BATS that contains those, so wehad to build our own (but it follows the same structureas #9652 - where we keep our dev docker imagesources inside our repository and the generated docker imagesin ""apache/airflow:<tool>-CALVER-TOOLVER format.We have more BATS unit test to add - following #10576and this change will be of great help.",4
Add masterConfig parameter to MLEngineStartTrainingJobOperator (#10578)Co-authored-by: antonio-davide-cali <antonio.davide.cali@ikea.com>,5
Add securitySchemes in openapi spec (#10652)openapi-generator relies on this component to generate auth code insome of the clients.,1
Fix docs for generate-dag-yaml cli command (#10735),2
"Enable more checks for pydocstyle (#10741)Enable D106, D207 and D208D106Missing docstring in public nested classD207Docstring is under-indentedD208Docstring is over-indented",2
Update node installation cmd (#10744),5
Improve test coverage for test_common_schema.py (#10740)Adds test that an error is raised with specific message when unkown object type is passed,4
Fix typo in test_dag_run_schema.py (#10739),3
Improve test coverage for ConfObject in dag_run_schema (#10738)Adds test to verify that string can be passed to conf and ConfObject._deserialize works.,1
Add black to STATIC_CODE_CHECKS.rst (#10737)Add black to the table for Static Code Checkers,1
Remove duplicate entries from .mailmap (#10736),4
Add docs for how airflow manages packages and imports (#10303),2
Asynchronous execution of Dataproc jobs with a Sensor (#10673),5
"clean-logs script for Dockerfile: trim logs before sleep (#10685)If the pod restarts before the sleep time is over, the trim command will not run. I think it's better if we reorder the commands to execute the delete and then go to sleep. At the moment sleep is every 15 mins but people will just increase the EVERY line if they want longer sleep time and can encounter this bug.",0
Make scripts/ci/openapi Google Shell Guide compatible (#10747)Part of #10576,1
Switch to downloaded pgbouncer_exporter (#10759)Fixes #10753,0
Fix `breeze flags` command. (#10766)This was missed in #10670,0
Move role guide to access control (#10755),4
"Check all dockerfiles with hadolint (#10754)The hadolint check only checked the ""main dir"" Dockerfilebut we have more of them now. All of them are now checked.The following problems are fixed: * DL3000 Use absolute WORKDIR * DL4000 MAINTAINER is deprecated * DL4006 Set the SHELL option -o pipefail before RUN with a pipe in it. * SC2046 Quote this to prevent word splitting.The followiing problems are ignored: * DL3018 Pin versions in apk add. Instead of `apk add <package>` use `apk add   <package>=<version>`",1
"Add permission ""extra_links"" for Viewer role and above (#10719)This change adds 'can extra links on Airflow' to the Viewer role and above. Currently, only Admins can see extra links by default.",2
"The verbose functions will not exit immediately if not asked to (#10731)The docker(), helm(), kubectl() functions replace the real toolsto get verbose behaviour (we can print the exact command beingexecuted for those. But when 'set +e' was set before the commandwas called - indicating that error in those functions should beignored - this did not happen. The functions set 'set -e' justbefore returning the non-zero value, effectively exiting thescript right after. This caused first time experience to be notgood.The fix also fixes behaviour of stdout and stderr for thosefunctions - previously they were joined to be able to beprinted to OUTPUT_FILE but this lost the stderr/stdoutdistinction. Now both stdout and stderr are printed to theoutput file but they are also redirected to stdout/stderrrespectively, so that 2>/dev/null works as expected.While fixing it, it turned out that one of the remove_imagesmethods was not used any more - merged it with the breeze one.",7
Make ci/scripts/pre-commit Google Shell Guide compatible (#10748)Part of #10576,1
Simplify load connection in LocalFilesystemBackend (#10638),5
The scripts to run tests properly initialises constants (#10769)The constants were initialised after the readonly status was setfor the constants in the test script.This was mainly about default values for those consttants (but thishas already been handled by the _script_init.sh but more importantlythe INTEGRATIONS were not properly initialized that cause skipping ofsome integration tests.,3
Make static checks Google Shell Guide compatible (#10750)Part of #10576,1
Refactor DataprocCreateCluster operator to use simpler interface (#10403)DataprocCreateCluster requires now:- cluster config- cluster name- project idIn this way users don't have to pass project_id two times (in cluster definition and as parameter). The cluster object is built in create_cluster hook method,1
[AIRFLOW-10672] Refactor BigQueryToGCSOperator to use new method (#10773)Makes BigQueryToGCSOperator to use BigQueryHook.insert_job methodCommitter: Mateusz Kukieła <mateuszkukiela@gmail.com>,1
"Fixes pre-commit failing on build step (#10785)When rebuildig the image during commit, kill command failed tofind the spinner job to kill (this is just preventive measure)and failed the rebuild step in pre-commit.This is now fixed.",0
"fix task instance modal open performance issue (#10764)This reduces the task instance modal open wait time for large DAGs withextra links from minutes to 6 ms, an order of 10000 times speed up.",2
"Removes stable tests from quarantine (#10768)We've observed the tests for last couple of weeks and it seemsmost of the tests marked with ""quarantine"" marker are succeedingin a stable way (https://github.com/apache/airflow/issues/10118)The removed tests have success ratio of > 95% (20 runs withoutproblems) and this has been verified a week ago as well,so it seems they are rather stable.There are literally few that are either failing or causingthe Quarantined builds to hang. I manually reviewed themaster tests that failed for last few weeks and added thetests that are causing the build to hang.Seems that stability has improved - which might be casuedby some temporary problems when we marked the quarantined buildsor too ""generous"" way of marking test as quarantined, ormaybe improvement comes from the #10368 as the docker engineand machines used to run the builds in GitHub experience farless load (image builds are executed in separate builds) soit might be that resource usage is decreased. Another reasonmight be Github Actions stability improvements.Or simply those tests are more stable when run isolation.We might still add failing tests back as soon we see them behavein a flaky way.The remaining quarantined tests that need to be fixed: * test_local_run (often hangs the build) * test_retry_handling_job * test_clear_multiple_external_task_marker * test_should_force_kill_process * test_change_state_for_tis_without_dagrun * test_cli_webserver_backgroundWe also move some of those tests to ""heisentests"" categoryThose testst run fine in isolation but failthe builds when run with all other tests: * TestImpersonation testsWe might find that those heisentest can be fixed but fornow we are going to run them in isolation.Also - since those quarantined tests are failing more oftenthe ""num runs"" to track for those has been decreased to 10to keep track of 10 last runs only.",1
Deprecate using global as the default region in Google Dataproc operators and hooks (#10772)The region parameter is required for some of Google Dataproc operatorsand it should be provided by users to avoid creating data-intensive tasks in any default location.,5
Always return a list from S3Hook list methods (#10774),1
Move dev docker images to airflow registry (#9652)Part of #9401,1
Add unit test for AzureCosmosDocumentSensor (#10765),2
Extract missing gcs_to_local example DAG from gcs example (#10767)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,2
All files in providers package heve unit tests (#10799),3
"Check that all pre-commits are synchronized code<>docs (#10789)Until pre-commit implements export of all configuredchecks, we need to maintain the list manually updated.We check both - pre-commit list in breeze-complete anddescriptions in STATIC_CODE_CHECKS.rst",5
Add section for official source code (#10678)* Add section for official source code* Update README.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update README.mdCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update README.mdCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Fix typos* Fix capitalization* Update README.mdCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update README.mdCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Format fixup* Remove official docker images sectionCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,2
Refactor official source section to use bullets (#10801)* Refactor section to use bullets* Change structure to group the binary options,4
Make script/ci/images Google Shell Guide compatible (#10745)Part of #10576,1
Make script/ci/kubernetes Google Shell Guide Compatible (#10746)Part of #10576,1
"Fixed wrong ""-e"" on md5 file status check (#10803)The ""-e"" flag was not reset properly in the md5 status checkwhich could lead in some cases to removing output of flake check.",4
Make ci/backport_packages Google Shell guide compliant (#10733),1
Fix integration tests being accidentally excluded (#10807)The change from #10769 accidentally switched Integration testsinto far-longer run unit tests (we effectively run the teststwice and did not run integration tests.This fixes the problem by removing readonly status fromINTEGRATIONS and only setting it after the integrations areset.,1
Move Impersonation test back to quarantine (#10809)Seems that TestImpersonation is not stable even in isolationMoving it back to quarantine for now.,4
Don't include kubernetes_tests/ and backport_packages/ in our wheel (#10805)Since 1.10.x we have added some new top-level folders that were gettingincluded in the build wheel!,1
Add task logging handler to airflow info command (#10771),5
[AIRFLOW-3964][AIP-17] Consolidate and de-dup sensor tasks using Smart Sensor (#5499)Co-authored-by: Yingbo Wang <yingbo.wang@airbnb.com>,1
"Add pod_override setting for KubernetesExecutor (#10756)* Add podOverride setting for KubernetesExecutorUsers of the KubernetesExecutor will now have a ""podOverride""option in the executor_config. This option will allow users tomodify the pod launched by the KubernetesExecutor using a`kubernetes.client.models.V1Pod` class. This is the first stepin deprecating the tradition executor_config.* Fix k8s tests* fix docs",2
Update log exception to reflect rename of execute_helper (#10819)`SchedulerJob.execute_helper` was renamed to `SchedulerJob._run_scheduler_loop`,1
Upgrade black to 20.8b1 (#10818),5
Add connection caching to KubernetesHook (#10447)* Add connection caching to KubernetesHook* Fix recursion and remove redundant docstrings,2
Make airflow testing Google Shell Guide compatible (#10813)Part of #10576,3
Make scripts/ci/tools Google Shell Guide Compatible (#10811)Part of #10576,1
Add documentation for preparing database for Airflow (#10413),5
Make dockerfiles Google Shell Guide Compliant (#10734)Part of #10576,2
Fix doc errors introduced in #10413 (#10832)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
"Fix typos in the stackdriver howto document (#10834)Someone was overzealous in building the initial ignore list -- ""wil"" isnot a word :)And the spell checker does not catch repeated words.",5
Fix doc errors introduced in #10413 (#10833),0
Fix grammar in UPDATING.md (#10841)`changes` -> `changed`,4
Remove k8s dependency from serialization (#10831)Ensures that airflow can run without downloading theKubernetes python client,1
Fix typo in docs/howto/operator/amazon/aws/emr.rst (#10844)`Documetation` -> `Documentation`,2
Add missing assert call in test_dbapi_hook.py (#10842)`assert` call was missing so the statement didn't test or wouldn't fail if condition isn't true,0
Fix and remove some more typos from spelling_wordlist.txt (#10845),5
Add test for Health Endpoint when there is an exception (#10846),3
Add USC Graduate School to INTHEWILD.md (#10843),1
Move parse_once to quarantine (#10857),4
"Rename ""Beyond the Horizon"" section and refactor content (#10802)* Rename ""Beyond the Horizon"" section and refactor content* Rework copy",1
"Detect orphaned task instances by SchedulerJob id and heartbeat (#10729)Once HA mode for scheduler lands, we can no longer reset orphanedtask by looking at the tasks in (the memory of) the current executor.This changes it to keep track of which (Scheduler)Job queued/scheduled aTaskInstance (the new ""queued_by_job_id"" column stored againstTaskInstance table), and then we can use the existing heartbeatmechanism for jobs to notice when a TI should be reset.As part of this the existing implementation of`reset_state_for_orphaned_tasks` has been moved out of BaseJob in toBackfillJob -- as only this and SchedulerJob had these methods, and theSchedulerJob version now operates differently",1
"Fix and unquarantine TestDagFileProcessorAgent.test_parse_once (#10862)The SmartSensor PR introduces slightly different behaviour onlist_py_files happens when given a file path directly.Prior to that PR, if given a file path it would not include examples.After that PR was merged, it would return that path and the example dags(assuming they were enabled.)",0
Refactor content to a markdown table (#10863),4
Add on_kill method to DataprocSubmitJobOperator (#10847),5
"Add new lint check to now allow realtive imports (#10825)Relative and absolute imports are functionally equivalent, the onlypratical difference is that relative is shorter.But it is also less obvious what exactly is imported, and harder to findsuch imports with simple tools (such as grep).Thus we have decided that Airflow house style is to use absolute importsonly",2
Added Plexus as an Airflow provider (#10591),1
Update qubole_hook to not remove pool as an arg for qubole_operator (#10820),1
fix typo in firebase/example_filestore DAG (#10875),2
"Remove unused type comment that upsets mypy (#10877)Something slightly odd is happening here.In #10729 it passed the mypy tests with this in place, but now if I makea change to this file (or to scheduler_job.py, which imports DagRun) Iget an error _with_ this.",0
"Disables --warn-unused-ignore flag for mypy (#10880)There is a problem with MyPy's implementation of--warn-unused-ignore flag, that depending on it's incrementalor full run it will sometimes throw an ""unused ignore"" error(entirely randomly it seems). The problem is described(but only workarounded) inhttps://github.com/python/mypy/issues/2960.The workaround is to disable --warn-unused-ignore flag.There is little harm in having unused ignores and we canclean them up from time to time easily.",4
Modify helm chart to use pod_template_file (#10872)* Modify helm chart to use pod_template_fileSince we are deprecating most k8sexecutor argumentswe should use the pod_template_file when launching airflowusing the KubernetesExecutor* fix tests* one more nit* fix dag command* fix pylint,0
Add on_kill method to BigQueryInsertJobOperator (#10866)* Add on_kill method to BigQueryInsertJobOperator* BigQueryInsertJobOperator pylint disable=too-many-arguments,1
"Fix `breeze -i` error (#10887)It was erroring with ""unbound"" variable, expecting INTEGRATION to be apre-defined variable.",0
Flag --start-airflow for breeze (#10837),5
Bump prismjs from 1.20.0 to 1.21.0 in /airflow/www (#10234)Bumps [prismjs](https://github.com/PrismJS/prism) from 1.20.0 to 1.21.0.- [Release notes](https://github.com/PrismJS/prism/releases)- [Changelog](https://github.com/PrismJS/prism/blob/master/CHANGELOG.md)- [Commits](https://github.com/PrismJS/prism/compare/v1.20.0...v1.21.0)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
"Fix failing dependencies for FAB and Celery (#10828)Recent releases of FAB and Celery caused our installation tofail. Luckily we have protection so that regular PRs are notaffected, however we need to update the setup.py to excludethose dependencies that cause the problem.Those are:* vine - which is used by Celery Sensor (via kombu) - 5.0.0  version breaks celery-vine feature* Flask-OauthLib and flask-login - combination of the current  requirements caused a conflict by forcing flask login to  be 0.5.0 which is not compatible with Flask Application Builder",2
Removes snakebite kerberos dependency (#10865)Snakebite's kerberos support relied on a python-krbVwhich has been removed from PyPI. It did not workcompletely anyway due to snakebite not being officiallysupported in python3 (snakebite-py3 did not work withSSL which made Kerberos pretty much unusable.This commit removes the snakebite's kerberos supportfrom setup.py so that you still can install kerberosas extra for other uses.,1
"The entrypoints in Docker Image should be owned by Airflow (#10853)Since we are running the airflow image as airflow user, theentrypoint and clear-logs scripts should also be set as airflow.This had no impact if you actually run this as root user orwhen your group was root (which was recommended).",1
Pass conf to subdags (#9956),2
"Remove errors raised during initialiation of virtualenv (#10896)Our migration scripts are written in the way that if youare running them to reset the db and you have some files inexample_dags or dags, or some plugins in your plugins folder,you will get various kinds of errors.Those errors are printed to the user and even if they areultimately ignored by the SqlAlchemy migration process,they are confusing whether the database reset(and thus the whole initialize-virtualenv) were successfulor not.This change disables any DAGs that could be loaded during theDB reset (in case of running './breeze initialize-virtualenv'.Fixes #10894",5
Add pre-commit to sort INTHEWILD.md file automatically (#10851),2
"Fix separated strings in test_secrets_manager.py (#10900)""airflow.providers.amazon.aws.secrets.secrets_manager."" ""SecretsManagerBackend.get_conn_uri""to""airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend.get_conn_uri""",1
Fix syntax error in Dockerfile 'maintainer' Label (#10899),2
Fix typo in the word 'instance' (#10902)`instnace` -> `instance`,2
"Parameterize tests in hashicorp/hooks/test_vault.py (#10903)Some of the tests were parameterizable, so less line to maintain with the same level of testing",3
"Update the name of static check without pylint in CI.rst (#10909)We changed the name of the static check without pylint in https://github.com/apache/airflow/commit/a1032805bc2ac96a507660a813a3a304cf4a2f8c#diff-e9f950f17198d3d5e3122a44230a09b9, this commit updates it's name in the docs",2
Add missing closing bracket in CI.rst (#10908)A closing bracket was missing in the sentence.,1
"Make the usage of bash 'shift' consistent across Breeze (#10907)If an argument is not provided to `shift` it by default uses `shift 1`. The commands ""shift 1"" and ""shift"" (with no argument) do the same thing. This PR makes the usage consistent as there were occurences of both",1
Fix typos in BREEZE.rst (#10905)`lunch` -> `launch``disto` -> `distro`,2
Fix grammar in BREEZE.rst (#10904)`Other uses the Airflow Breeze environment` -> `Other uses of the Airflow Breeze environment`,1
Add Secrets backend for Microsoft Azure Key Vault (#10898),1
Fix parameter name collision in AutoMLBatchPredictOperator #10723 (#10869)Rename `params` to `prediction_params` to avoidclash with BaseOperator arguments,1
Fix typos in scripts/ci/docker-compose/local.yml (#10906)`an` -> `on` (grammatically that makes sense),1
fix bug where multiple volume mounts created (#10915),1
"Adds the maintain-heart-rate to quarantine. (#10922)The test occasionally fails, moving it to quarantine for now.",4
Make vrious scripts Google Shell Guide compatible (#10812)Part of #10576,1
Make Clients Google Shell guide compatible (#10810)Part of #10576,1
Make breeeze-complete Google Shell Guide compatible (#10708)Also added unit tests for breeze-completePart of #10576,3
"Remove unknown pytest.ini setting (#10923)This was added when we first migrated to pytest, but pytest is nowcomplaining that this setting is unknown.Since it doesn't do anything, lets remove it",4
"Mark task as failed when it fails sending in Celery (#10881)If a task failed hard on celery, _before_ being able to execute theairflow code the task would end up stuck in queued state. This changemakes it get retried.This was discovered in load testing the HA work (but unrelated to HAchanges), where I swamped the kube-dns pod, meaning the worker wassometimes unable to resolve the db name via DNS, so the state in the DBwas never updated",5
Add more type annotations to AWS hooks (#10671)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,1
Add Formatted Stacktrace for Spelling Error (#10919),0
Fix Failing static tests on Master (#10927),3
Fix chain methods for XComArg (#10827)__lshift__ and __rshift__ methods should return other not self.This PR fixes XComArg implementation  to support chain like this one:BaseOprator >> XComArg >> BaseOperatorRelated to: #10153,1
Fix grammar in Bug Report Template (#10936)`This questions` -> `These questions`,0
Stop running Doc tests with Spelling tests (#10938)Currently we run the following tests too when checking spell-check errors- check_guide_links_in_operator_descriptions()- check_class_links_in_operators_and_hooks_ref()- check_guide_links_in_operators_and_hooks_ref()- check_enforce_code_block()- check_exampleinclude_for_example_dags()- check_google_guides(),2
[AIRFLOW-10645] Add AWS Secrets Manager Hook (#10655),1
Allow to specify path to kubeconfig in KubernetesHook (#10453),1
Add Examples and documentation for pod_template_file feature (#10916)* Add examples and documentation for pod_mutation_hook* @mik-laj comments* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/executor/kubernetes.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* moved template files* fix spelling* spellcheckCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
Fix 'Tasks State' stats  UI Overlap (#10939)Resolves #10935,0
"Minor refactor of the login methods in tests.www.test_views (#10918)- Instead of supporting only an Admin user in the base test class, you can also use a normal User or Viewer- Only add users when they are being used so we can do a little less in the setup phase (minor speedup in TestDagACLView)",3
"Cache for kubernetes tests is updateable (#10945)The cache in Github Actions is immutable - once you create itit cannot be modified. That's why cache keys should containhash of all files that are used to create the cache.Kubernetes cache key did not contain it, and as a side effectthe cache from master kubernetes setup.py was used in the v1-10-testafter the breeze changes were cherry-picked.",4
"Add CeleryKubernetesExecutor (#10901)it consists of CeleryExecutor and KubernetesExecutor, which allows usersto route their tasks to either Kubernetes or Celery based on the queuedefined on a task",1
"Proposal: remove -serviceaccount suffix from KSA names in helm chart (#10892)* [WIP] remove -serviceaccount suffix in helm chartIt's quite annoying to have `-serviceaccount` in each service account name as this is a useless 15 characters that provides no additional information.""why is this so frustrating to you Jake?""GCP service accounts have 30 char name limit https://cloud.google.com/iam/docs/creating-managing-service-accounts#creatingFor manageability / clarity I'd like to keep KSA and GSA names exactly the same when using workload identity which maps KSA<>GSA 1:1 https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.",2
Fix the method defaults for _construct_volume (#10948),0
"Remove test dependency from TestApiKerberos (#10950)TestApiKerberos::test_trigger_dag previously was dependent that the `example_bash_operator` exist in the Database.If one of the other tests didn't write it to the DB or if one of the other tests cleared it from the DB, this test failed.",0
"Fixes retrieval of correct branch in non-master related builds (#10912)When we ported the new CI mechanism to v1-10-test it turned outthat we have to correct the retrieval of DEFAULT BRANCHand DEFAULT_CONSTRAINTS_BRANCH.Since we are building the images using the ""master"" scripts, we need tomake sure the branches are retrieved from _initialization.sh of theincoming PR, not from the one in the master branch.Additionally versions 2.7 and 3.5 builds have to be merged tomaster and excluded when the build is run targeting master branch.",1
Github repository can be overridden in command line by Breeze (#10943)During testing v1-10-test backport for Breeze the--github-repository flag did not work. It turned out thatthe lowercase variable was not re-set when the flag wasprovided by Breeze.This change causes the lowercasing to be run just before itis used to make sure that the GITHUB_REPOSITORY valueis used after it's been overwritten.,1
Fix Kubernetes Executor logs for long dag names (#10942)See #10292,2
Fix case of GitHub. (#10955)Changed `Github` to `GitHub`.,4
Fix ExternalTaskMarker serialized fields (#10924)Co-authored-by: Denis Evseev <xOnelinx@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
Add a default for DagModel.default_view (#10897)fixes https://github.com/apache/airflow/issues/10283,0
Fix more docs spellings (#10965)`desecending` -> `descending``Retrives` -> `Retrieves``programatic` -> `programmatic``subproces` -> `subprocess`,2
added environment configuration for using --start-airflow (#10971),1
Introduce TaskMixin (#10930)Both BaseOperator and XComArgs implement bit shift operatorsused to chain tasks in DAGs. By extracting this logic to newmixin we reduce code duplication and make it easier to implementit in the future. This change introduces also root property that allows usto reduce number of type checking.,1
Fix static error (tabs) introduced in #10971 (#10973),0
KubernetesPodOperator template fix (#10963)* Ensure that K8sPodOperator can pull namespace from pod_template_fileFixes a bug where users who run K8sPodOperator could not run becausethe operator was expecting a namespace parameter* add test* self.pod* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>* don't create pod until run* spellcheckCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Increase typing coverage for postgres provider (#10864),1
Fix empty asctime field in JSON formatted logs (#10515),2
Add example dag and system test for S3ToGCSOperator (#10951),1
"Allow CeleryExecutor to ""adopt"" an orphaned queued or running task (#10949)This can happen when a task is enqueued by one executor, and then thatscheduler dies/exits.The default fallback behaviour is unchanged -- that queued tasks arecleared and then and then later rescheduled.But for Celery, we can do better -- if we record the Celery-generatedtask_id, we can then re-create the AsyncResult objects for orphanedtasks at a later date.However, since Celery just reports all AsyncResult as ""PENDING"", even ifthey aren't tasks currently in the broker queue, we need to apply atimeout to ""unblock"" these tasks in case they never actually made it tothe Celery broker.This all means that we can adopt tasks that have been enqueued anotherCeleryExecutor if it dies, without having to clear the task and slowdown. This is especially useful as the task may have already startedrunning, and while clearing it would stop it, it's better if we don'thave to reset it!Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>",1
"Fix typo in the word ""committed"" (#10979)`commited` -> `committed`",2
"Task Instance Modal UX Enhancements (#10944)* Improve modal UX with logical form ordering, semantic form elements, visual hierachy tweaks* make modal header prefix dynamic if SUBDAG* Add heading as demarcation between action sections within modal* Update doc screenshot w/ added modal heading",1
Improve the Error message in Breeze for invalid params (#10980)Changed `Is` to `Passed`Before:```ERROR:  Allowed backend: [ sqlite mysql postgres ]. Is: 'dpostgres'.Switch to supported value with --backend flag.```After:```ERROR:  Allowed backend: [ sqlite mysql postgres ]. Passed: 'dpostgres'.Switch to supported value with --backend flag.```,1
"Remove redundant parentheses in serialized_objects.py (#10966)`elif isinstance(var, (Timezone)):` -> `elif isinstance(var, Timezone):`",4
Fix 'Upload documentation' step in CI (#10981),2
"Pins temporarily moto to 1.3.14 (#10986)As described in #10985, moto upgrade causes some AWS tests to fail.Until we fix it, we pin the version to 1.3.14.Part of #10985",0
Allows to build production images for 1.10.2 and 1.10.1 Airflow (#10983)Airflow below 1.10.2 required SLUGIFY_USES_TEXT_UNIDECODE envvariable to be set to yes.Our production Dockerfile and Breeze supports building imagesfor any version of airflow >= 1.10.1 but it failed on1.10.2 and 1.10.1 because this variable was not set.You can now set the variable when building image manuallyand Breeze does it automatically if image is 1.10.1 or 1.10.2Fixes #10974,0
The test_find_not_should_ignore_path is now in heisentests (#10989)It seems that the test_find_not_should_ignore_path test has somedependency on side-effects from other tests.See #10988 - we are moving this test to heisentests until wesolve the issue.,0
Unpin 'tornado' dep pulled in by flower (#10993)'tornado' version was pinned in https://github.com/apache/airflow/pull/4815The underlying issue is fixed for Py 3.5.2 so that is not a problem. Also since Airflow Master is already Py 3.6+ this does not apply to us.,0
"Simplify the K8sExecutor and K8sPodOperator (#10393)* Simplify Airflow on Kubernetes StoryRemoves thousands of lines of code that essentially ammount to usre-creating the Kubernetes API. Will offer a faster, simplerKubernetesExecutor for 2.0* Fix podgen tests* fix documentation* simplify validate function* @mik-laj comments* spellcheck* spellcheck* Update airflow/executors/kubernetes_executor.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
Add new teammate to Polidea (#11000),1
Fetching databricks host from connection if not supplied in extras. (#10762)* Fetching databricks host from connection if not supplied in extras.* Fixing formatting issue in databricks testCo-authored-by: joshi95 <shubham@playsimple.in>,3
Remove redundant curly brace from breeze echo message (#11012)Before:```❯ ./breeze --github-image-id 260274893GitHub image id: 260274893}```After:```❯ ./breeze --github-image-id 260274893GitHub image id: 260274893```,4
KubernetesJobWatcher no longer inherits from Process (#11017)multiprocessing.Process is set up in a very unfortunate mannerthat pretty much makes it impossible to test a class that inherits fromProcess or use any of its internal functions. For this reason we decidedto seperate the actual process based functionality into a class member,1
Replace PNG/text with SVG that includes name in proper typography (#11018),2
"[AIP-34] TaskGroup: A UI task grouping concept as an alternative to SubDagOperator (#10153)This commit introduces TaskGroup, which is a simple UI task grouping concept.- TaskGroups can be collapsed/expanded in Graph View when clicked- TaskGroups can be nested- TaskGroups can be put upstream/downstream of tasks or other TaskGroups with >> and << operators- Search box, hovering, focusing in Graph View treats TaskGroup properly. E.g. searching for tasks also highlights TaskGroup that contains matching task_id. When TaskGroup is expanded/collapsed, the affected TaskGroup is put in focus and moved to the centre of the graph.What this commit does not do:- This commit does not change or remove SubDagOperator. Although TaskGroup is intended as an alternative for SubDagOperator, deprecating SubDagOperator will need to be discussed/implemented in the future.- This PR only implemented TaskGroup handling in the Graph View. In places such as Tree View, it will look like as-if - TaskGroup does not exist and all tasks are in the same flat DAG.GitHub Issue: #8078AIP: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-34+TaskGroup%3A+A+UI+task+grouping+concept+as+an+alternative+to+SubDagOperator",5
Support extra_args in S3Hook and GCSToS3Operator (#11001),1
Remove Edit Button from DagModel View (#11026),2
Increase typing coverage JDBC provider (#11021),1
Add typing to amazon provider EMR (#10910),1
Fix typo in DagFileProcessorAgent._last_parsing_stat_received_at (#11022)`_last_parsing_stat_recieved_at` -> `_last_parsing_stat_received_at`,2
Fix logo issues due to generic scripting selector use (#11028)Resolves #11025,0
Get Airflow configs with sensitive data from AWS Systems Manager (#11023)Adds support to AWS SSM for feature added in https://github.com/apache/airflow/pull/9645,1
Refactor rebase copy (#11030),4
Add D204 pydocstyle check (#11031),2
Starting breeze will run an init script after the environment is setup (#11029)Added the possibility to run an init script,5
Replace JS package toggle w/ pure CSS solution (#11035),5
Only gather KinD logs if tests fail (#11058),0
"Separates out user documentation for production images. (#10998)We have now much better user-facing documentation.Only the parts interesting for users of the image areseparated out to the ""docs"" of Airflow.The README and IMAGES.rst contains links to thosedocs and internal details of the images respectively.Fixes #10997.",0
"All versions in CI yamls are not hard-coded any more (#10959)GitHub Actions allow to use `fromJson` method to read arraysor even more complex json objects into the CI workflow yaml files.This, connected with set::output commands, allows to read thelist of allowed versions as well as default ones from theenvironment variables configured in./scripts/ci/libraries/initialization.shThis means that we can have one plece in which versions areconfigured. We also need to do it in ""breeze-complete"" as this isa standalone script that should not source anything we addedBATS tests to verify if the versions in breeze-completecorrespond with those defined in the initialization.shAlso we do not limit tests any more in regular PRs now - we runall combinations of available versions. Our tests run quite abit faster now so we should be able to run more completematrixes. We can still exclude individual values of the matrixesif this is too much.MySQL 8 is disabled from breeze for now. I plan a separate followup PR where we will run MySQL 8 tests (they were not run so far)",1
Add Workflow to delete old artifacts (#11064),4
[Doc] Correct description for macro task_instance_key_str (#11062)Correction based on code https://github.com/apache/airflow/blob/master/airflow/models/taskinstance.py,2
"Revert ""KubernetesJobWatcher no longer inherits from Process (#11017)"" (#11065)This reverts commit 1539bd051cfbc41c1c7aa317fc7df82dab28f9f8.",4
Add JSON schema validation for Helm values (#10664)fixes #10634,0
Get Airflow configs with sensitive data from CloudSecretManagerBackend (#11024),5
Add some tasks using BashOperator in TaskGroup example dag (#11072)Previously all the tasks in airflow/example_dags/example_task_group.py were using DummyOperator which does not go to executor and is marked as success in Scheduler itself so it would be good to have some tasks that aren't dummy operator to properly test TaskGroup functionality,1
Replace Airflow Slack Invite old link to short link (#11071)Follow up to https://github.com/apache/airflow/pull/10034https://apache-airflow-slack.herokuapp.com/ to https://s.apache.org/airflow-slack/,2
Fix s.apache.org Slack link (#11078)Remove ending / from s.apache.org Slack link,2
Pandas behaviour for None changed in 1.1.2 (#11004)In Pandas version 1.1.2 experimental NAN value started to bereturned instead of None in a number of places. That broke our tests.Fixing the tests also requires the Pandas to be updated to be >=1.1.2,5
"Improves deletion of old artifacts. (#11079)We introduced deletion of the old artifacts as this wasthe suspected culprit of Kubernetes Job failures. It turned outeventually that those Kubernetes Job failures were caused bythe #11017 change, but it's good to do housekeeping of theartifacts anyway.The delete workflow action introduced in a hurry had two problems:* it runs for every fork if they sync master. This is a bit  too invasive* it fails continuously after 10 - 30 minutes every time  as we have too many old artifacts to delete (GitHub has  90 days retention policy so we have likely tens of  thousands of artifacts to delete)* it runs every hour and it causes occasional API rate limit  exhaustion (because we have too many artifacts to loop trough)This PR introduces filtering with the repo, changes the frequencyof deletion to be 4 times a day. Back of the envelope calculationtops 4/day at 2500 artifacts to delete at every run so we have low risk of reaching 5000 API calls/hr rate limit. and adds script that we arerunning manually to delete those excessive artifacts now. Eventuallywhen the number of artifacts goes down the regular job should deletemaybe a few hundreds of artifacts appearing within the 6 hours windowin normal circumstances and it should stop failing then.",0
"Requirements might get upgraded without setup.py change (#10784)I noticed that when there is no setup.py changes, the constraintsare not upgraded automatically. This is because of the dockercaching strategy used - it simply does not even know that theupgrade of pip should happen.I believe this is really good (from security and incremental updatesPOV to attempt to upgrade at every successfull merge (not thatthe upgrade will not be committed if any of the tests fail and thisis only happening on every merge to master or scheduled run.This way we will have more often but smaller constraint changes.Depends on #10828",4
Add D202 pydocstyle check (#11032),2
Add permissions for stable API (#10594)Related Github Issue: https://github.com/apache/airflow/issues/8112,0
"Make Skipmixin handle empty branch properly (#10751)closes: #10725Make sure SkipMixin.skip_all_except() handles empty branches like this properly. When ""task1"" is followed, ""join"" must not be skipped even though it is considered to be immediately downstream of ""branch"".",0
SkipMixin: Add missing session.commit() and test (#10421),3
Fix typo in STATIC_CODE_CHECKS.rst (#11094)`realtive` -> `relative`,2
"Avoid redundant SET conversion (#11091)* Avoid redundant SET conversionget_accessible_dag_ids() returns a SET, so no need to apply set() again* Add type annotation for get_accessible_dag_ids()",2
Fix for pydocstyle D202 (#11096)'issues' introduced in https://github.com/apache/airflow/pull/10594,0
Security upgrade lodash from 4.17.19 to 4.17.20 (#11095)Details: https://snyk.io/vuln/SNYK-JS-LODASH-590103,5
Introducing flags to skip example dags and default connections (#11099),2
Add template fields renderers for better UI rendering (#11061)This PR adds possibility to define template_fields_renderers for anoperator. In this way users will be able to provide informationwhat lexer should be used for rendering a particular field. This issuper useful for custom operator and gives more flexibility thanpredefined keywords.Co-authored-by: Kamil Olszewski <34898234+olchas@users.noreply.github.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Fix sort-in-the-wild pre-commit on Mac (#11103),0
Fix typo in README (#11106),2
Add Opensignal to INTHEWILD.md (#11105),1
"Revert ""Introducing flags to skip example dags and default connections (#11099)"" (#11110)This reverts commit 0edc3dd57953da5c66a4b45d49c1426cc0295f9e.",4
"Update initialize-database.rst (#11109)* Update initialize-database.rstRemove ambiguity in the language as only MySQL, Postgres and SQLite are supported backends.* Update docs/howto/initialize-database.rstCo-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>",5
Increasing type coverage FTP (#11107),3
"Adds timeout in CI/PROD waiting jobs (#11117)In very rare cases, the waiting job might not be cancelled whenthe ""Build Image"" job fails or gets cancelled on its own.In the ""Build Image"" workflow we have this step:- name: ""Canceling the CI Build source workflow in case of failure!""  if: cancelled() || failure()  uses: potiuk/cancel-workflow-runs@v2  with:    token: ${{ secrets.GITHUB_TOKEN }}    cancelMode: self    sourceRunId: ${{ github.event.workflow_run.id }}But when this step fails or gets cancelled on its own beforecancel is triggered, the ""wait for image"" steps couldrun for up to 6 hours.This change sets 50 minutes timeout for those jobs.Fixes #11114",0
Add Helm Chart linting (#11108),2
README Doc: Link to Airflow directory in ASF Directory (#11137)`https://downloads.apache.org` -> `https://downloads.apache.org/airflow` (links to precise dir),2
"Fix incorrect Usage of Optional[bool] (#11138)Optional[bool] = Union[None, bool]There were incorrect usages where the default was already set toa boolean value but still Optional was used",1
Fix FROM directive in docs/production-deployment.rst (#11139)`FROM:` -> `FROM`,2
Increasing type coverage for salesforce provide (#11135),1
Added support for encrypted private keys in SSHHook (#11097)* Added support for encrypted private keys in SSHHook* Fixed Styling issues and added unit testing* fixed last pylint styling issue by adding newline to the end of the file* re-fixed newline issue for pylint checks* fixed pep8 styling issues and black formatted files to pass static checks* added comma as per suggestion to fix static checkCo-authored-by: Nadim Younes <nyounes@kobo.com>,0
Fix error message when checking literalinclude in docs (#11140)Before:```literalinclude directive is is prohibited for example DAGs```After:```literalinclude directive is prohibited for example DAGs```,2
Upgrade to latest isort & pydocstyle (#11142)isort: from 5.4.2 to 5.5.3pydocstyle: from 5.0.2 to 5.1.1,2
"Do not silently allow the use of undefined variables in jinja2 templates (#11016)This can have *extremely* bad consequences. After this change, a jinja2template like the one below will cause the task instance to fail, if theDAG being executed is not a sub-DAG. This may also display an error onthe Rendered tab of the Task Instance page.task_instance.xcom_pull('z', key='return_value', dag_id=dag.parent_dag.dag_id)Prior to the change in this commit, the above template would pull thelatest value for task_id 'z', for the given execution_date, from *any DAG*.If your task_ids between DAGs are all unique, or if DAGs using the sametask_id always have different execution_date values, this will appear toact like dag_id=None.Our current theory is SQLAlchemy/Python doesn't behave as expected whencomparing `jinja2.Undefined` to `None`.",2
Move Backport Providers docs to our docsite (#11136),2
Fix user in helm chart pgbouncer deployment (#11143),2
Fixes celery deployments for Airflow 2.0 (#11129)The celery flower and worker commands have changed in Airflow 2.0.The Helm Chart supported only 1.10 version of those commands andthis PR fixes it by adding both variants of them.,1
"Fix gitSync user in the helm Chart (#11127)There was a problem with user in Git Sync mode of the Helm Chartin connection with the git sync image and official Airflowimage. Since we are using the official image, most of thecontainers are run with the ""50000"" user, but the git-sync imageused by the git sync user is 65533 so we have to set it asdefault. We also exposed that value as parameter, so thatanother image could be used here as well.",1
"Fix incorrect Usage of Optional[str] & Optional[int] (#11141)From https://docs.python.org/3/library/typing.html#typing.Optional```Optional[X] is equivalent to Union[X, None].```>Note that this is not the same concept as an optional argument, which is one that has a default. An optional argument with a default does not require the Optional qualifier on its type annotation just because it is optional.There were incorrect usages where the default was already set toa string or int value but still Optional was used",1
Remove link to Dag Model view given the redundancy with DAG Details view (#11082),2
Make sure pgbouncer-exporter docker image is linux/amd64 (#11148)Closes #11145,2
Update to latest version of pbgouncer-exporter (#11150)There was a problem with Mac version of pgbouncer exportercreated and released previously. This commit releases thelatest version making sure that Linux Go is used to buildthe pgbouncer binary.,1
Add new member to Polidea (#11153),1
"Massively speed up the query returned by TI.filter_for_tis (#11147)The previous query generated SQL like this:```WHERE (task_id = ? AND dag_id = ? AND execution_date = ?) OR (task_id = ? AND dag_id = ? AND execution_date = ?)```Which is fine for one or maybe even 100 TIs, but when testing DAGs atextreme size (over 21k tasks!) this query was taking for ever (162s onPostgres, 172s on MySQL 5.7)By changing this query to this```WHERE task_id IN (?,?) AND dag_id = ? AND execution_date = ?```the time is reduced to 1s! (1.03s on Postgres, 1.19s on MySQL)Even on 100 tis the reduction is large, but the overall time is notsignificant (0.01451s -> 0.00626s on Postgres).Times included SQLA query construction time (but not time for callingfilter_for_tis. So a like-for-like comparison), not just DB query time:```pythonipdb> start_filter_20k = time.monotonic(); result_filter_20k = session.query(TI).filter(tis_filter).all(); end_filter_20k = time.monotonic()ipdb> end_filter_20k - start_filter_20k172.30647455298458ipdb> in_filter = TI.dag_id == self.dag_id, TI.execution_date == self.execution_date, TI.task_id.in_([o.task_id for o in old_states.keys()]);ipdb> start_20k_custom = time.monotonic(); result_custom_20k = session.query(TI).filter(in_filter).all(); end_20k_custom = time.monotonic()ipdb> end_20k_custom - start_20k_custom1.1882996069907676```I have also removed the check that was ensuring everything was of thesame type (all TaskInstance or all TaskInstanceKey) as it felt needless- both types have the three required fields, so the ""duck-typing""approach at runtime (crash if doesn't have the required property)+mypychecks felt Good Enough.",5
Increase Type coverage for IMAP provider (#11154),1
Increasing type coverage for multiple provider (#11159),1
"Optionally disables PIP cache from GitHub during the build (#11173)This is first step of implementing the corporate-environmentfriendly way of building images, where in the corporateenvironment, this might not be possible to install the packagesusing the GitHub cache initially.Part of #11171",5
Update UPDATING.md (#11172),5
"New Breeze command start-airflow, it replaces the previous flag (#11157)",1
"Conditional MySQL Client installation (#11174)This is the second step of making the Production Docker Image morecorporate-environment friendly, by making MySQL client installationoptional. Instaling MySQL Client on Debian requires to reach outto oracle deb repositories which might not be approved by securityteams when you build the images. Also not everyone needs MySQLclient or might want to install their own MySQL client or MariaDBclient - from their own repositories.This change makes the installation step separated out toscript (with prod/dev installation option). The prod/dev separationis needed because MySQL needs to be installed with dev librariesin the ""Build"" segment of the image (requiring build essentialsetc.) but in ""Final"" segment of the image only runtime librariesare needed.Part of #11171Depends on #11173.",1
Add example DAG and system test for MySQLToGCSOperator (#10990),1
Increase type coverage for five different providers (#11170)* Increasing type coverage for five different providers* Added more strict type,1
"Adds Kubernetes Service Account for the webserver (#11131)Webserver did not have a Kubernetes Service Account defined andwhile we do not strictly need to use the service account foranything now, having the Service Account defined allows todefine various capabilities for the webserver.For example when you are in the GCP environment, you can mapthe Kubernetes service account into a GCP one, usingWorkload Identity without the need to define any secretsand performing additional authentication.Then you can have that GCP service account getthe permissions to write logs to GCS bucket. Similar mechanismsexist in AWS and it also opens up on-premises configuration.See more athttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identityCo-authored-by: Jacob Ferriero <jferriero@google.com>Co-authored-by: Jacob Ferriero <jferriero@google.com>",2
Allow overrides for pod_template_file (#11162)* Allow overrides for pod_template_fileA pod_template_file should be treated as a *template* not a steadfastrule.This PR ensures that users can override individual values set by thepod_template_file s.t. the same file can be used for multiple tasks.* fix podtemplatetest* fix name,0
Enables Kerberos sidecar support (#11130)Some of the users of Airflow are using Kerberos to authenticatetheir worker workflows. Airflow has a basic support for Kerberosfor some of the operators and it has support to refresh thetemporary Kerberos tokens via `airflow kerberos` command.This change adds support for the Kerberos side-car that connectsto the Kerberos Key Distribution Center and retrieves thetoken using Keytab that should be deployed as Kubernetes Secret.It uses shared volume to share the temporary token. The nicething about setting it up as a sidecar is that the Keytabis never shared with the workers - the secret is only mountedby the sidecar and the workers have only access to the temporarytoken.Depends on #11129,1
Make kill log in DagFileProcessorProcess more informative (#11124),5
"Show the location of the queries when the assert_queries_count fails. (#11186)Example output (I forced one of the existing tests to fail)```E   AssertionError: The expected number of db queries is 3. The current number is 2.EE   Recorded query locations:E   scheduler_job.py:_run_scheduler_loop>scheduler_job.py:_emit_pool_metrics>pool.py:slots_stats:94:1E   scheduler_job.py:_run_scheduler_loop>scheduler_job.py:_emit_pool_metrics>pool.py:slots_stats:101:1```This makes it a bit easier to see what the queries are, without havingto re-run with full query tracing and then analyze the logs.",2
Improve Google Transfer header in documentation index file  (#11166),2
Fix typos in Dockerfile.ci (#11187)Fixed some spellings,0
"Remove Unnecessary comprehension in 'any' builtin function (#11188)The inbuilt functions `any()` support short-circuiting (evaluation stops as soon as the overall return value of the function is known), but this behavior is lost if you use comprehension. This affects performance.",1
"Optionally tags image when building with Breeze (#11181)Breeze tags the image based on the default python version,branch, type of the image, but you might want to tag the imagein the same command especially in automated cases of buildingthe image via CI scripts or security teams that tag the imgebased on external factors (build time, person etc.).This is part of #11171 which makes the image easier to build incorporate environments.",1
in_container bats pre-commit hook and updated bats-tests hook (#11179),1
"Fixes image tag readonly failure (#11194)The image builds fine, but produces an unnecessary error message.Bug Introduced in c9a34d2ef9ccf6c18b379bbcb81b9381027eb803",0
"More customizable build process for Docker images (#11176)* Allows more customizations for image building.This is the third (and not last) part of making the Productionimage more corporate-environment friendly. It's been preparedfor the request of one of the big Airflow user (company) thathas rather strict security requirements when it comes topreparing and building images. They are committed tosynchronizing with the progress of Apache Airflow 2.0 developmentand making the image customizable so that they can build it usingonly sources controlled by them internally was one of the importantrequirements for them.This change adds the possibilty of customizing various steps inthe build process:* adding custom scripts to be run before installation of both  build image and runtime image. This allows for example to  add installing custom GPG keys, and adding custom sources.* customizing the way NodeJS and Yarn are installed in the  build image segment - as they might rely on their own way  of installation.* adding extra packages to be installed during both build and  dev segment build steps. This is crucial to achieve the same  size optimizations as the original image.* defining additional environment variables (for example  environment variables that indicate acceptance of the EULAs  in case of installing proprietary packages that require  EULA acceptance - both in the build image and runtime image  (again the goal is to keep the image optimized for size)The image build process remains the same when no customizationoptions are specified, but having those options increasesflexibility of the image build process in corporate environments.This is part of #11171.This change also fixes some of the issues opened and raised byother users of the Dockerfile.Fixes: #10730Fixes: #10555Fixes: #10856Input from those issues has been taken into account when thischange was designed so that the cases described in those issuescould be implemented. Example from one of the issue landed asan example way of building highly customized Airflow Imageusing those customization options.Depends on #11174* Update IMAGES.rstCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
[AIRFLOW-5545] Fixes recursion in DAG cycle testing (#6175)* Fixes an issue where cycle detection uses recursionand stack overflows after about 1000 tasks(cherry picked from commit 63f1a180a17729aa937af642cfbf4ddfeccd1b9f)* reduce test length* slightly more efficient* Update airflow/utils/dag_cycle_tester.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* slightly more efficient* actually works this timeCo-authored-by: Daniel Imberman <daniel@astronomer.io>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
"Add amazon glacier to GCS transfer operator (#10947)Add Amazon Glacier to GCS transfer operator, Glacier job operator and sensor.",1
Strict type coverage for Oracle and Yandex provider  (#11198)* type coverage for yandex provider* type coverage for oracle provider* import optimisation and mypy fix* import optimisation* static check fix,0
Strict type checking for SSH (#11216),5
Replace get accessible dag ids (#11027),2
Kubernetes executor can adopt tasks from other schedulers (#10996)* KubernetesExecutor can adopt tasks from other schedulers* simplify* recreate tables properly* fix pylintCo-authored-by: Daniel Imberman <daniel@astronomer.io>,0
"Optional import error tracebacks in web ui (#10663)This PR allows for partial import error tracebacks to be exposed on the UI, if enabled. This extra context can be very helpful for users without access to the parsing logs to determine why their DAGs are failing to import properly.",2
Strict type check for multiple providers (#11229),1
Fix typo in command in CI.rst (#11233),2
Add Python version to Breeze cmd (#11228),1
"Use more meaningfull message for DagBag timeouts (#11235)Instead of 'Timeout, PID: 1234' we can use something more meaningfulthat will help users understand the logs.",2
Prepare Backport release 2020.09.07 (#11238),5
"Airflow 2.0 UI Overhaul/Refresh (#11195)Resolves #10953.A refreshed UI for the 2.0 release. The existing ""theming"" is a bit long in the tooth and this PR attempts to give it a modern look and some freshness to compliment all of the new features under the hood.The majority of the changes to UI have been done through updates to the Bootstrap theme contained in bootstrap-theme.css. These are simply overrides to the default stylings that are packaged with Bootstrap.",5
Restore description for provider packages. (#11239)The change #10445 caused empty descriptions for all packages.This change restores it and also makes sure package creation workswhen there is no README.md,2
Small updates to provider preparation docs. (#11240),2
Fixed month in backport packages to October (#11242),0
Add task adoption to CeleryKubernetesExecutor (#11244)Routes task adoption based on queue name to CeleryExecutoror KubernetesExecutorCo-authored-by: Daniel Imberman <daniel@astronomer.io>,1
Remove redundant parentheses (#11248),4
Fix Broken Markdown links in Providers README TOC (#11249),1
Add option to bulk clear DAG Runs in Browse DAG Runs page (#11226)closes: #11076,1
Update yamllint & isort pre-commit hooks (#11252)yamllint: v1.24.2 -> v1.25.0isort: 5.5.3 -> 5.5.4,1
Ensure target_dedicated_nodes or enable_auto_scale is set in AzureBatchOperator (#11251),1
Add s3 key to template fields for s3/redshift transfer operators (#10890),1
"Add missing ""example"" tag on example DAG (#11253)`example_task_group` and `example_nested_branch_dag` didn't have the example tag while all the other ones do have it",2
Breeze: Fix issue with pulling an image via ID (#11255),0
Move test tools from tests.utils to tests.test_utils (#10889),3
Add Github Code Scanning (#11211)Github just released Github Code Scanning:https://github.blog/2020-09-30-code-scanning-is-now-available/,2
Add Hacktoberfest topic to the repo (#11258),1
Add operator link to access DAG triggered by TriggerDagRunOperator (#11254)This commit adds TriggerDagRunLink which allows users to accesseasily access in Web UI a DAG triggered by TriggerDagRunOperator,2
"The bats script for CI image is now placed in the docker folder (#11262)The script was previously placed in scripts/ci which causeda bit of a problem in 1-10-test branch where PRs were usingscripts/ci from the v1-10-test HEAD but they were missingthe ci script from the PR.The scripts ""ci"" are parts of the host scripts that arealways taken from master when the image is built, butall the other stuff should be taken from ""docker""folder - which will be taken from the PR.",2
"Limits CodeQL workflow to run only in the Apache Airflow repo (#11264)It has been raised quite a few times that workflow added in forkedrepositories might be pretty invasive for the forks - especiallywhen it comes to scheduled workflows as they might eat quotaor at least jobs for those organisations/people who forkrepositories.This is not strictly necessary because Recently GitHub recognized this as beinga problem and introduced new rules for scheduled workflows. But for people whoare already forked, it would be nice to not run those actions. It is enoughthat the CodeQL check is done when PR is opened to the ""apache/airflow""repository.Quote from the emails received by Github (no public URL explaining it yet):> Scheduled workflows will be disabled by default in forks of public repos and inpublic repos with no activity for 60 consecutive days.  We’re making twochanges to the usage policy for GitHub Actions. These changes will enableGitHub Actions to scale with the incredible adoption we’ve seen from the GitHubcommunity. Here’s a quick overview:> * Starting today, scheduled workflows will be disabled by default in new forks ofpublic repositories.> * Scheduled workflows will be disabled in public repos withno activity for 60 consecutive days.",1
Enable MySQL 8 CI jobs (#11247)closes https://github.com/apache/airflow/issues/11164,0
"Improve running and canceliling of the PR-triggered builds. (#11268)The PR builds are now better handled with regards to bothrunning (using merge-request) and canceling (with cancel notifications).First of all we are using merged commit from the PR, not the original commitfrom the PR.Secondly - the workflow run notifies the original PR with commentstating that the image is being built in a separate workflow -including the link to that workflow.Thirdly - when canceling duplicate PRs or PRs with failedjobs, the workflow will add a comment to the PR stating thereason why the PR is being cancelled.Last but not least, we also add cancel job for the CodeQL duplicatemessages. They run for ~ 12 miinutes so it makes perfect sense toalso cancel those CodeQL jobs for which someone pushed fixups in aquick succession.Fixes: #10471",0
Fix link to static checks in CONTRIBUTING.rst (#11271),2
fix job deletion (#11272),4
Allow labels in KubernetesPodOperator to be templated (#10796),1
"Access task type via the property, not dundervars (#11274)We don't currently create TIs form serialized dags, but we are about tostart -- at which point some of these cases would have just shown""SerializedBaseOperator"", rather than the _real_ class name.The other changes are just for ""consistency"" -- we should always get thetask type from this property, not via `__class__.__name__`.I haven't set up a pre-commit rule for this as using this dunderaccessor is used elsewhere on things that are not BaseOperatorinstances, and detecting that is hard to do in a pre-commit rule.",1
"When sending tasks to celery from a sub-process, reset signal handlers (#11278)Since these processes are spawned from SchedulerJob after it hasregistered it's signals, if any of them got signaled they would have thebehaviour of killing the ProcessorAgent process group! (MP has a defaultspawn of fork on Linux, so they inherit all previous state -- signals,and access to the `_process.pid` inside the ProcessorAgent instance)This behaviour is not what we want for these multiprocess.Pool processes.This _may_ be a source of the long-standing ""scheduler is alive but notscheduling any jobs. Maybe.",0
"Switched to Run Checks for Building Images. (#11276)Replaces the annoying comments with ""workflow_run"" linkswith Run Checks. Now we will be able to see the ""Build Image""checks in the ""Checks"" section including their status and directlink to the steps running the image builds as ""Details"" link.Unfortunately Github Actions do not handle well the links todetails - even if you provide details_url to link to the otherrun, the ""Build Image"" checks appear in the original workflow,that's why we had to introduce another link in the summary ofthe Build Image check that links to the actual workflow.",1
"Single/Multi-Namespace mode for helm chart (#11034)* Multi-Namespace mode for helm chartUsers should not REQUIRE a ClusterRole/ClusterRolebindingto run airflow via helm. This change will allow ""single"" and ""multi""namespace modes so users can add airflow to managed kubernetes clusters* add namespace to role* add rolebinding too* add docs* add values.schema.json change",4
Add LocalToAzureDataLakeStorageOperator (#10814),5
Add CeleryKubernetesExecutor to helm chart (#11288)Users of the CeleryKubernetesExecutor will require bothCelery and Kubernetes features to launch tasks.This PR will also serve as the basis for integration tests for thisexecutorCo-authored-by: Daniel Imberman <daniel@astronomer.io>,3
Strict type check for all hooks in amazon (#11250),1
Replaces depreated set-env with env file (#11292)Github Actions deprecated the set-env action due to moderate securityvulnerability they found.https://github.blog/changelog/2020-10-01-github-actions-deprecating-set-env-and-add-path-commands/This commit replaces set-env with env file as explained inhttps://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-commands-for-github-actions#environment-files,3
Breeze start-airflow command wasn't able to initialize the db in 1.10.x (#11207),5
"Add type annotations to ZendeskHook, update unit test (#10888)* Add type annotations to ZendeskHook__What__* Add correct type annotations to ZendeskHook and each method* Update one unit test to call an empty dictionary rather than aNoneType since the argument should be a dictionary__Why__* Building out type annotations is good for the code base* The query parameter is accessed with an index at one point, whichmeans that it cannot be a None type, but should rather be defaulted toan empty dictionary if not provided* Remove useless return",1
Add acl_policy parameter to GCSToS3Operator (#10804) (#10829),1
add releasing airflow docs to dev readme (#11245),2
"Prevent race condition in trying to collect result from DagFileProcessor (#11306)A rare race condition was noticed in the Scheduler HA PR where the""test_dags_with_system_exit"" test would occasionally fail with thefollowing symptoms:- The pipe was ""readable"" as returned by  `multiprocessing.connection.wait`- On reading it yielded an EOFError, meaning the other side had closed  the connection- But the process was still alive/runningThis previously would result in the Manager process dying with an error.This PR makes a few changes:- It ensures that the pipe is simplex, not duplex (we only ever send one  data) as this is simpler- We ensure that the ""other"" end of the pipe is correctly closed in both  parent and child processes. Without this the pipe would be kept open  (sometimes) until the child process had closed anyway.- When we get an EOFError on reading and the process is still alive, we  give it a few seconds to shut down cleanly, and then kill it.",4
Bump tenacity to 6.2 (#11313),5
Move latest_only_operator.py to latest_only.py (#11178) (#11304),3
"Adds --no-rbac-ui flag for Breeze airflow 1.10 installation (#11315)When installing airflow 1.10 via breeze we now enable rbacby default, but we can disable it with --no-rbac-ui flag.This is useful to test different variants of 1.10 when testingrelease candidataes in connection with the 'start-airflow'command.",5
Add remaining community guidelines to CONTRIBUTING.rst (#11312)We are cleaning up the docs from CWiki and this is what's left ofcommunity guidelines that were maintained there.Fixes #10181,0
Improve handling of job_id in BigQuery operators (#11287)Make autogenerated job_id more unique by using microseconds and hash of configuration. Replace dots in job_id.Closes: #11280,5
"Prints nicer message in case of git push errors (#11320)We started to get more often ""unknown blob"" kind of errors whenpushing the images to GitHub Registry. While this is clearly aGitHub issue, it's frequency of occurence and unclear messagemake it a good candidate to write additional message withinstructions to the users, especially that now they havean easy way to get to that information via status checks andlinks leading to the log file, when this problem happens duringimage building process.This way users will know that they should simply rebase oramend/force-push their change to fix it.",0
Add AzureFileShareToGCSOperator (#10991),2
"Automatically upgrade old default navbar color (#11322)As part of #11195 we re-styled the UI, changing a lot of the defaultcolours to make them look more modern. However for anyone upgrading andkeeping their airflow.cfg from 1.10 to 2.0 they would end up with thingslooking a bit ugly, as the old navbar color would be kept.This uses the existing config value upgrade feature to automaticallychange the old default colour in to the new default colour.",1
"Pin versions of ""untrusted"" 3rd-party GitHub Actions (#11319)According to https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/security-hardening-for-github-actions#using-third-party-actionsait's best practice not to use tags in case of untrusted3rd-party actions in order to avoid potential attacks.",1
Moves Commiter's guide to CONTRIBUTING.rst (#11314)I decided to move it to CONTRIBUTING.rst as is it is an importantdocumentation on what policies we have agreed to as community andalso it is a great resource for the contributor to learn what arethe committer's responsibilities.Fixes: #10179,0
Add environment variables documentation to cli-ref.rst. (#10970)Co-authored-by: Fai Hegberg <faihegberg@Fais-MacBook-Pro.local>,2
Update link for Announcement Page (#11337),2
Strict type check for azure hooks (#11342),1
Adds --install-wheels flag to breeze command line (#11317)If this flag is specified it will look for wheel packages placed in distfolder and it will install the wheels from there after installingAirflow. This is useful for testing backport packages as well as in thefuture for testing provider packages for 2.0.,1
Improve code quality of SLA mechanism in SchedulerJob (#11257),1
Improve Committer's guide docs (#11338),2
Add Azure Blob Storage to GCS transfer operator (#11321),1
Better message when Building Image fails or gets cancelled. (#11333),1
"Revert ""Adds --install-wheels flag to breeze command line (#11317)"" (#11348)This reverts commit de07d135ae1bda3f71dd83951bcfafc2b3ad9f89.",4
Fix command to run tmux with breeze in BREEZE.rst (#11340)`breeze --start-airflow` -> `breeze start-airflow`,1
Improve instructions to install Airflow Version (#11339)The instructions can be replaced by `./breeze start-airflow` command,1
"Reduce ""start-up"" time for tasks in LocalExecutor (#11327)Spawning a whole new python process and then re-loading all of Airflowis expensive. All though this time fades to insignificance for longrunning tasks, this delay gives a ""bad"" experience for new users whenthey are just trying out Airflow for the first time.For the LocalExecutor this cuts the ""queued time"" down from 1.5s to 0.1son average.",1
Bump cache version for kubernetes tests (#11355)Seems that the k8s cache for virtualenv got broken during therecent problems. This commits bumps the cache version to makeit afresh,1
Better diagnostics when there are problems with Kerberos (#11353),0
Fix to make y-axis of Tries chart visible (#10071)Co-authored-by: Venkatesh Selvaraj <venkateshselvaraj@pinterest.com>,2
Bugfix: Error in SSHOperator when command is None (#11361)closes https://github.com/apache/airflow/issues/10656,0
Allways use Airlfow db in FAB (#11364),5
"Use only-if-needed upgrade strategy for PRs (#11363)Currently, upgrading dependencies in setup.py still runs with previous versions of the package for the PR which fails.This will change to upgrade only the package that is required for the PRs",1
Fix DagBag bug when a dag has invalid schedule_interval (#11344),2
"Adding ElastiCache Hook for creating, describing and deleting replication groups (#8701)",4
Fix regression in DataflowTemplatedJobStartOperator (#11167),5
Strict type check for Microsoft  (#11359),5
"Reduce ""start-up"" time for tasks in CeleryExecutor (#11372)This is similar to #11327, but for Celery this time.The impact is not quite as pronounced here (for simple dags at least)but takes the average queued to start delay from 1.5s to 0.4s",2
"Set start_date, end_date & duration for tasks failing without DagRun (#11358)",2
Replace nuke with useful information on error page (#11346)This PR replaces nuke asciiart with text about reporting a bug.As we are no longer using asciiarts this PR removes it.,4
Users can specify sub-secrets and paths k8spodop (#11369)Allows users to specify items for specific key path projectionswhen using the airflow.kubernetes.secret.Secret class,1
Add capability of adding service account annotations to Helm Chart (#11387)We can now add annotations to the service accounts in a genericway. This allows for example to add Workflow Identitty in GKEenvironment but it is not limited to it.Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>Co-authored-by: Jacob Ferriero <jferriero@google.com>Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,1
Add pypirc initialization (#11386)This PR needs to be merged first in order to handle the #11385which requires .pypirc to be created before dockerfile gets build.This means that the script change needs to be merged to masterfirst in this PR.,7
"Fully support running more than one scheduler concurrently (#10956)* Fully support running more than one scheduler concurrently.This PR implements scheduler HA as proposed in AIP-15. The high leveldesign is as follows:- Move all scheduling decisions into SchedulerJob (requiring DAG  serialization in the scheduler)- Use row-level locks to ensure schedulers don't stomp on each other  (`SELECT ... FOR UPDATE`)- Use `SKIP LOCKED` for better performance when multiple schedulers are  running. (Mysql < 8 and MariaDB don't support this)- Scheduling decisions are not tied to the parsing speed, but can  operate just on the database*DagFileProcessorProcess*:Previously this component was responsible for more than just parsing theDAG files as it's name might imply. It also was responsible for creatingDagRuns, and also making scheduling decisions of TIs, sending them from""None"" to ""scheduled"" state.This commit changes it so that the DagFileProcessorProcess now willupdate the SerializedDAG row for this DAG, and make no schedulingdecisions itself.To make the scheduler's job easier (so that it can make as manydecisions as possible without having to load the possibly-largeSerializedDAG row) we store/update some columns on the DagModel table:- `next_dagrun`: The execution_date of the next dag run that should be created (or  None)- `next_dagrun_create_after`: The earliest point at which the next dag  run can be createdPre-computing these values (and updating them every time the DAG isparsed) reduce the overall load on the DB as many decisions can be takenby selecting just these two columns/the small DagModel row.In case of max_active_runs, or `@once` these columns will be set tonull, meaning ""don't create any dag runs""*SchedulerJob*The SchedulerJob used to only queue/send tasks to the executor afterthey were parsed, and returned from the DagFileProcessorProcess.This PR breaks the link between parsing and enqueuing of tasks, insteadof looking at DAGs as they are parsed, we now:-  store a new datetime column, `last_scheduling_decision` on DagRun  table, signifying when a scheduler last examined a DagRun- Each time around the loop the scheduler will get (and lock) the next  _n_ DagRuns via `DagRun.next_dagruns_to_examine`, prioritising DagRuns  which haven't been touched by a scheduler in the longest period- SimpleTaskInstance etc have been almost entirely removed now, as we  use the serialized versions* Move callbacks execution from Scheduler loop to DagProcessorProcess* Don’t run verify_integrity if the Serialized DAG hasn’t changeddag_run.verify_integrity is slow, and we don't want to call it every time, just when the dag structure changes (which we can know now thanks to DAG Serialization)* Add escape hatch to disable newly added ""SELECT ... FOR UPDATE"" queriesWe are worried that these extra uses of row-level locking will causeproblems on MySQL 5.x (most likely deadlocks) so we are providing usersan ""escape hatch"" to be able to make these queries non-locking -- thismeans that only a singe scheduler should be run, but being able to runone is better than having the scheduler crash.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
"Revert ""Revert ""Adds --install-wheels flag to breeze command line (#11317)"" (#11348)"" (#11356)This reverts commit f67e6cb805ebb88db1ca2c995de690dc21138b6b.",5
Replaced basestring with str in the Exasol hook (#11360),1
[airflow/providers/cncf/kubernetes] correct hook methods name (#11008),1
Fix airflow_local_settings.py showing up as directory (#10999)Fixes a bug where the airflow_local_settings.py mounts as a volumeif there is no value (this causes k8sExecutor pods to fail),0
Fix case of JavaScript. (#10957),0
Add tests for Custom cluster policy (#11381)The custom ClusterPolicyViolation has been added in #10282This one adds more comprehensive test to it.Co-authored-by: Jacob Ferriero <jferriero@google.com>,3
KubernetesPodOperator should retry log tailing in case of interruption (#11325)* KubernetesPodOperator can retry log tailing in case of interruption* fix failing test* change read_pod_logs method formatting* KubernetesPodOperator retry log tailing based on last read log timestamp* fix test_parse_log_line test  formatting* add docstring to parse_log_line method* fix kubernetes integration test,3
fix tests (#11368),3
"Constraints and PIP packages can be installed from local sources (#11382)* Constraints and PIP packages can be installed from local sourcesThis is the final part of implementing #11171 based on feedbackfrom enterprise customers we worked with. They want to havea capability of building the image using binary wheel packagesthat are locally available and the official Dockerfile. This meansthat besides the official APT sources the Dockerfile build shouldnot needd GitHub, nor any other external files pulled from outsideincluding PIP repository.This change also includes documentation on how to prepare set ofsuch binaries ready for inspection and review by security teamsin Enterprise environment. Such sets of ""known-working-binary-whl""files can then be separately committed, tracked and scrutinizedin an artifact repository of such an Enterprise.Fixes: #11171* Update docs/production-deployment.rst",2
"Push and schedule duplicates are not cancelled. (#11397)The push and schedule builds should not be cancelled even ifthey are duplicates. By seing which of the master mergesfailed, we have better visibility on which merge causeda problem and we can trace it's origin faster even if the buildswill take longer overall.Scheduled builds also serve it's purpose and they shouldbe always run to completion.",1
Remove redundant parentheses from Python files (#10967),2
Fixes automated upgrade to latest constraints. (#11399)Wrong if query in the GitHub action caused upgrade to latestconstraints did not work for a while.,1
"Fixes cancelling of too many workflows. (#11403)A problem was introduced in #11397 where a bit too many ""Build Image""jobs is being cancelled by subsequent Build Image run. For now itcancels all the Build Image jobs that are running :(.",1
Fix spelling (#11401),0
Fix spelling (#11404),0
"Workarounds ""unknown blob"" issue by introducing retries (#11411)We have started to experience ""unknown_blob"" errors intermittentlyrecently with GitHub Docker registry. We might eventually needto migrate to GCR (which eventually is going to replace theDocker Registry for GitHub:The ticket is opened to the Apache Infrastructure to enableaccess to the GCR and to make some statements about AccessRights management for GCR https://issues.apache.org/jira/projects/INFRA/issues/INFRA-20959Also a ticket to GitHub Support has been raised about ithttps://support.github.com/ticket/personal/0/861667 as wecannot delete our public images in Docker registry.But until this happens, the workaround might help usto handle the situations where we got intermittent errorswhile pushing to the registry. This seems to be a commonerror, when NGINX proxy is used to proxy Github Registry soit is likely that retrying will workaround the issue.",0
"Add capability of customising PyPI sources (#11385)* Add capability of customising PyPI sourcesThis change adds capability of customising installation of PyPImodules via custom .pypirc file. This might allow to installdependencies from in-house, vetted registry of PyPI",1
Moving the test to quarantine. (#11405)I've seen the test being flaky and failing intermittently several times.Moving it to quarantine for now.,4
Optionally set null marker in csv exports in BaseSQLToGCSOperator (#11409),1
Fixes SHA used for cancel-workflow-action (#11400)The SHA of cancel-workflow-action in #11397 was pointing to previous(3.1) version of the action. This PR fixes it to point to theright (3.2) version.,0
"Split tests to more sub-types (#11402)We seem to have a problem with running all tests at once - mostlikely due to some resource problems in our CI, therefore it makessense to split the tests into more batches. This is not yet fullimplementation of selective tests but it is going in this directionby splitting to Core/Providers/API/CLI tests. The full selectivetests approach will be implemented as part of #10507 issue.This split is possible thanks to #10422 which moved building imageto a separate workflow - this way each image is only built onceand it is uploaded to a shared registry, where it is quicklydownloaded from rather than built by all the jobs separately - thisway we can have many more jobs as there is very little per-joboverhead before the tests start runnning.",1
"Fix incorrect typing, remove hardcoded argument values and improve code in AzureContainerInstancesOperator (#11408)",1
Fix constraints generation script (#11412)Constraints generation script was broken by recent changesin naming of constraints URL variables and moving generationof the link to the DockerfileThis change restores the script's behaviour.,4
Fix spelling in CeleryExecutor (#11407),0
Add more info about dag_concurrency (#11300),2
Update MySQLToS3Operator's s3_bucket to template_fields (#10778),1
Change prefix of AwsDynamoDB hook module (#11209)* align import path of AwsDynamoDBHook in aws providersCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,1
Strict type check for google ads and cloud hooks (#11390),1
"Mutual SSL added in PGBouncer configuration in the Chart (#11384)Adds SSL configuration for PGBouncer in the Helm Chart. PGBounceris crucial to handle the big number of connections that airflowopens for the database, but often the database is outside of theKubernetes Cluster or generally the environment where Airflow isinstalled and PGBouncer needs to connect securely to such database.This PR adds capability of seting CA/Certificate and Private Keyin the PGBouncer configuration that allows for mTLS authentication(both client and server are authenticated) and secure connectioneven over public network.",1
"Merge Airflow and Backport Packages preparation instructions (#11310)This commit extracts common parts of Apache Airflow packagepreparation and Backport Packages preparation.Common parts were extracted as prerequisites, the release processhas been put in chronological order, some details about preparingbackport packages have been moved to a separate README.mdin the Backport Packages to not confuse release instructionswith tool instructions.",5
Fix syntax highlightling for concurrency in configurations doc (#11438)`concurrency` -> ``concurrency`` since it is rendered in rst,2
Fix typo in airflow/utils/dag_processing.py (#11445)`availlablle` -> `available`,2
"Selective tests - depends on files changed in the commit. (#11417)This is final step of implementing #10507 - selective tests.Depending on files changed by the incoming commit, only subset ofthe tests are exucted. The conditions below are evaluated in thesequence specified below as well:* In case of ""push"" and ""schedule"" type of events, all tests  are executed.* If no important files and folders changed - no tests are executed.  This is a typical case for doc-only changes.* If any of the environment files (Dockerfile/setup.py etc.) all  tests are executed.* If no ""core/other"" files are changed, only the relevant types  of tests are executed:  * API - if any of the API files/tests changed  * CLI - if any of the CLI files/tests changed  * WWW - if any of the WWW files/tests changed  * Providers - if any of the Providers files/tests changed* Integration Heisentests, Quarantined, Postgres and MySQL  runs are always run unless all tests are skipped like in  case of doc-only changes.* If ""Kubernetes"" related files/tests are changed, the  ""Kubernetes"" tests with Kind are run. Note that those tests  are run separately using Host environment and those tests  are stored in ""kubernetes_tests"" folder.* If some of the core/other files change, all tests are run. This  is calculated by substracting all the files count calculated  above from the total count of important files.Fixes: #10507",2
Fix correct Sphinx return type for DagFileProcessorProcess.result (#11444),2
Use augmented assignment (#11449)`tf_count += 1` instead of `tf_count = tf_count + 1`,1
Remove redundant None provided as default to dict.get() (#11448),1
Fix spelling (#11453),0
Refactor celery worker command (#11336)This commit does small refactor of the way we star celery worker.In this way it will be easier to migrate to Celery 5.0.,1
Move the test_process_dags_queries_count test to quarantine (#11455)The test (test_process_dags_queries_count)randomly produces bigger number of counts. Example here:https://github.com/apache/airflow/runs/1239572585#step:6:421,1
Google cloud operator strict type check (#11450)import optimisation,2
"Increase timeout for waiting for images (#11460)Now, when we have many more jobs to run, it might happen thatwhen a lot of PRs are submitted one-after-the-other there mightbe longer waiting time for building the image.There is only one waiting job per image type, so it does notcost a lot to wait a bit longer, in order to avoid cancellationafter 50 minutes of waiting.",1
Add more testing methods to dev/README.md (#11458),2
Adds missing schema for kerberos sidecar configuration (#11413)* Adds missing schema for kerberos sidecar configurationThe kerberos support added in #11130 did not have schema addedto the values.yml. This PR fixes it.Co-authored-by: Jacob Ferriero <jferriero@google.com>* Update chart/values.schema.jsonCo-authored-by: Jacob Ferriero <jferriero@google.com>,5
"Rename backport packages to provider packages (#11459)In preparation for adding provider packages to 2.0 line weare renaming backport packages to provider packages.We want to implement this in stages - first to rename thepackages, then split-out backport/2.0 providers as part ofthe #11421 issue.",0
Add option to enable TCP keepalive for communication with Kubernetes API (#11406)* Add option to enable TCP keepalive mechanism for communication with Kubernetes API* Add keepalive default options to default_airflow.cfg* Add reference to PR* Quote parameters names in configuration* Add problematic words to spelling_wordlist.txt,5
"Enables back duplicate cancelling on push/schedule (#11471)We disabled duplicate cancelling on push/schedule in #11397but then it causes a lot of extra strain in case several commitsare merged in quick succession. The master merges are alwaysfull builds and take a lot of time, but if we merge PRsquickly, the subsequent merge cancels the previous ones.This has the negative consequence that we might not know whobroke the master build, but this happens rarely enough to sufferthe pain at expense of much less strained queue in GitHub Actions.",7
Fix typo in docker-context-files/README.md (#11473)`th` -> `the`,2
added type hints for aws cloud formation (#11470),1
Mask Password in Log table when using the CLI (#11468),1
Mount volumes and volumemounts into scheduler and workers (#11426)* Mount arbitrary volumes and volumeMounts to scheduler and workerAllows users to mount volumes to scheduler and workers* tested,3
Bump FAB to 3.1 (#11475)FAB released a new version today - https://pypi.org/project/Flask-AppBuilder/3.1.0/ which removes the annoying missing font file format for glyphicons error,0
Allow multiple schedulers in helm chart (#11330)* Allow multiple schedulers in helm chart* schema* add docs* add to readmeCo-authored-by: Daniel Imberman <daniel@astronomer.io>,1
Fix Harcoded Airflow version (#11483)This test will fail or will need fixing whenever we release new Airflowversion,1
Add link on External Task Sensor to navigate to target dag (#11481)Co-authored-by: Kaz Ukigai <kukigai@apple.com>,2
"Spend less time waiting for LocalTaskJob's subprocss process to finish (#11373)* Spend less time waiting for LocalTaskJob's subprocss process to finishThis is about is about a 20% speed up for short running tasks!This change doesn't affect the ""duration"" reported in the TI table, butdoes affect the time before the slot is freeded up from the executor -which does affect overall task/dag throughput.(All these tests are with the same BashOperator tasks, just running `echo 1`.)**Before**```Task airflow.executors.celery_executor.execute_command[5e0bb50c-de6b-4c78-980d-f8d535bbd2aa] succeeded in 6.597011625010055s: NoneTask airflow.executors.celery_executor.execute_command[0a39ec21-2b69-414c-a11b-05466204bcb3] succeeded in 6.604327297012787s: None```**After**```Task airflow.executors.celery_executor.execute_command[57077539-e7ea-452c-af03-6393278a2c34] succeeded in 1.7728257849812508s: NoneTask airflow.executors.celery_executor.execute_command[9aa4a0c5-e310-49ba-a1aa-b0760adfce08] succeeded in 1.7124666879535653s: None```**After, including change from #11372**```Task airflow.executors.celery_executor.execute_command[35822fc6-932d-4a8a-b1d5-43a8b35c52a5] succeeded in 0.5421732050017454s: NoneTask airflow.executors.celery_executor.execute_command[2ba46c47-c868-4c3a-80f8-40adaf03b720] succeeded in 0.5469810889917426s: None```",4
Add endpoints for task instances (#9597),1
"Enable serialization by default (#11491)We actually need to make serialization the default, but this is aninterim measure for Airflow2.0.0.alpha1 reeaseSince many of the tests will fail with it enabled (they need fixing upto ensure DAGs are in the serializated table) as a hacky measure we haveset it back to false in the tests.",3
Add missing values entries to Parameters in chart/README.md (#11477),2
"Rename ""functional DAGs"" to ""Decorated Flows"" (#11497)Functional DAGs were so called because the DAG is ""made up of funcitons""but this AIP adds much more than just the task decorator change -- itadds nicer XCom use, and in many cases automatic dependencies betweentasks.""Functional"" also invokes ""functional programming"" which this isn't.",1
Prevent text-selection of scheduler interval when selecting DAG ID (#11503),2
Mark Smart Sensor as an early-access feature (#11499),5
Fix spelling for Airbnb (#11505),0
"Added support for provider packages for Airflow 2.0 (#11487)* Separate changes/readmes for backport and regular providersWe have now separate release notes for backport providerpackages and regular provider packages.They have different versioning - backport providerpackages with CALVER, regular provider packages withsemver.* Added support for provider packages for Airflow 2.0This change consists of the following changes:* adds provider package support for 2.0* adds generation of package readme and change notes* versions are for now hard-coded to 0.0.1 for first release* adds automated tests for installation of the packages* rename backport package readmes/changes to BACKPORT_** adds regulaar packge readmes/changes* updates documentation on generating the provider packaes* adds CI tests for the packages* maintains backport packages generation with --backports flagFixes #11421Fixes #11424",0
"Airflow tutorial to use Decorated Flows (#11308)Created a new Airflow tutorial to use Decorated Flows (a.k.a. functionalDAGs). Also created a DAG to perform the same operations without usingfunctional DAGs to be compatible with Airflow 1.10.x and to show thedifference.* Apply suggestions from code reviewIt makes sense to simplify the return variables being passed around without needlessly converting to JSON and then reconverting back.* Update tutorial_functional_etl_dag.pyFixed data passing between tasks to be more natural without converting to JSON and converting back to variables.* Updated dag options and task doc formatingBased on feedback on the PR, updated the DAG options (including schedule) and the fixed the task documentation to avoid indentation.* Added documentation file for functional dag tutorialAdded the tutorial documentation to the docs directory. Fixed linting errors in the example dags.Tweaked some doc references in the example dags for inclusion into the tutorial documentation.Added the example dags to example tests.* Removed multiple_outputs from task defnHad a multiple_outputs=True defined in the Extract task defn, which was unnecessary. - Removed based on feedback.Co-authored-by: Gerard Casas Saez <casassg@users.noreply.github.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Bump to Airflow 2.0.0a1 (#11507),5
Update CONTRIBUTING.rst (#11461)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Change Airflow version to 2.0.0a1 in Updating.md (#11508),5
Updated tutorial_decorated_flows.rst to add links (#11510)Added links to the Decorated Flows AIP and to the Decorated Flows section of the Concepts doc.,2
"Combine back multiple test types into single jobs (#11504)Seems that by splitting the tests into many small jobs has a badeffect - since we only have queue size = 180 for the whole Apacheorganisation, we are competing with other projects for the jobsand with the jobs being so short we got starved much more than ifwe had long jobs. Therefore we are re-combining the test types intosingle jobs per Python version/Database version and run all thetests sequentially on those machines.",3
Fix typo in scripts/ci/libraries/_initialization.sh (#11517)`initialized` -> `initialize`,5
"Fix example in UPDATING.md (#11518)`my_plguin.MyCustomExecutor` -> `my_plugin.MyCustomExecutor` as we mention in the sentence above that ""plugin wascalled `my_plugin` """,5
Fixes remaining test-type strategy problems (#11522)The test-type strategy matrix were not deleted entirely when combinedback tests in #11504,3
Add reset_dag_run option on dagrun_operator to clear existing dag run (#11484)* Add reset_dag_run option on dagrun_operator so that user can clear target dag run if exists.* Logging coding style changes.* Make pylint check pass.* Make pylint check pass.* Make pylint check pass on unit test file.* Make static check pass.* Use settings.STORE_SERIALIZED_DAGSCo-authored-by: Kaz Ukigai <kukigai@apple.com>,2
Minor improvements to dev/README.md (#11525),2
Remove flask-admin based Plugins (#11515),4
Upgrade to pymssql 2.1.5 for Py 3.8 support (#11523),1
Add protocol_version to conn_config for Cassandrahook (#11036),1
backport for add_xcom_sidecar (#11478)* backport for add_xcom_sidecar* activate function* add k8s filter* nit* fix to work,1
"Fix documentation errors (#11536)In trialing an upgrade of sphinx-autoapi to 1.5.1 (it has otherproblem with metaclasses right now, so we cant use it just yet) Inoticed these cases of warnings that would become errors if we upgraded.",0
Pymssql is maintained again (#11537)See https://github.com/pymssql/pymssql/commit/20457589b2878800cb40c1dd4f3045a4bb259675,5
More stable kubernetes port forwarding (#11538)Seems that port forwarding during kubernetes tests started to behaveerratically - seems that kubectl port forward sometimes might hangindefinitely rather than connect or fail.We change the strategy a bit to try to allocateincreasing port numbers in case something like that happens.,1
Resolve MSSQL DAG serialization bug by changing datatype for execution date (#11512)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
"Rename (confusing) dag.sub_dag to dag.partial_subset (#11542)There was a method on the DAG class called `sub_dag()` that had nothingto do with Sub-DAGs or the SubDagOperator - It instead created a new""partial"" dag that contained only a selected subset of tasks.To remove this confusion when seeing `dag.sub_dag()` used in code I haverenamed this function, and included a compat shim in case anyone isusing it outside of Airflow core.",1
Adds capability of installing wheel packages in CI image (#11527)The production image had the capability of installing images fromwheels (for security teams/air-gaped systems). This capabilitymight also be useful when building CI image espeically whenwe are installing separately core and providers packages andwe do not yet have provider packages available in PyPI.This is an intermediate step to implement #11490,1
Feature: Auto-refresh Graph view chart (#11534),2
Add better debug logging to K8sexec and K8sPodOp (#11502),2
"Visual tweaking of TI swatches, legend refactoring (#11550)Co-authored-by: James Timmins <james@astronomer.io>",4
"Create job for airflow migrations (#11533)Creating airflow migrations should run seperately from the user creationjob, as many users might not want to create users on deployment.",1
Auto-refresh default state (#11559),5
Prepend `DAG:` to dag permissions (#11189)This adds the prefix DAG: to newly created dag permissions. It supports checking permissions on both prefixed and un-prefixed DAG permission names.This will make it easier to identify permissions that related to granular dag access.This PR does not modify existing dag permission names to use the new prefixed naming scheme. That will come in a separate PR.Related to issue #10469,0
Add/implement Webpack plugin to minify CSS files on-compile (#11564),2
Clarify breeze docs --install-airflow-version/-reference (#11570)* Clarify breeze docs --install-airflow-version/-reference* Add to automated bash scripts,1
"The scripts fixing ownership and cleaning tmp use docker run (#11569)The scripts were using docker compose, but theycan be docker run commands. Also they are not needed to berun by breeze directly in CI image because I've added trapsto run the commands at the exit of all ""in_container"" scripts.",1
Updated template_fields_rendereds for PostgresOperator and SimpleHttpOperator (#11555)Co-authored-by: Michal Niemiec/IT/CREDITSAFE <Michal.Niemiec@creditsafe.com>,2
Prevent pop-over elements from being cut off by hidden overflow (#11574),5
Fix broken backtick usage in Timezone docs (#11575),2
Strict type checking for provider google cloud  (#11548),1
"Fixes dependencies to pre-release versions of apache-airflow (#11578)The dependencies in Alphas are currently >= 2.0.0 and should be>=2.0.0a0 in order to work with Alphas in cases which are not PEP440-compliant.According to https://www.python.org/dev/peps/pep-0440/, >= 2.0.0 shouldalso work with alpha/beta releases (a1/a2) but in some cases it does not(https://apache-airflow.slack.com/archives/C0146STM600/p1602774750041800)Changing to "">=2.0.0a0"" should help.Fixes #11577",0
"Guard against kubernetes not being installed (#11558)If the `kubernetes.client` import fails, then `airflow.kubernetes.pod_generator` also can't be imported, and there won't be attributes on `k8s` to use in `isinstance()` calls.Instead of setting `k8s` to `None`, use an explicit flag so later code can disable kubernetes-specific branches explicitly.Also, when de-serializing a Kubernetes pod with no kubernetes library installed is an error.",0
Utilize the state foreground color to ensure an accessible contrast ratio (#11579),5
Add DataflowStartFlexTemplateOperator (#8550),5
Add type annotations for AWS operators and hooks (#11434)Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,1
Pass SQLAlchemy engine options to FAB based UI (#11395)Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,4
Fix tooltip typo (#11593),2
"Clean up _trigger_dag function (#11584)- The dag_run argument is only there for test mocks, and only to access a static method. Removing this simplifies the function, reduces confusion.- Give optional arguments a default value, reduce indentation of arg list to PEP / Black standard.- Clean up tests for readability",3
Fix broken migration for Sqlite3 (#11573),0
"Replace methods on state with frozenset properties (#11576)Although these lists are short, there's no need to re-create them eachtime, and also no need for them to be a method.I have made them lowercase (`finished`, `running`) instead of uppercase(`FINISHED`, `RUNNING`) to distinguish them from the actual states.",1
"Allow loading plugins on Airflow start-up (#11596)https://github.com/apache/airflow/commit/0be7654fd2ab047c610f44d3c11467f67212a744 commit made an optimization where the plugin are lazy-loaded. However, there are use-cases where you would still want the plugins to be loaded on Airflow start-up.",1
UX Enhancement: Add button to clear search query from DAG search (#11583),2
Raises a warning for provide_context instead of killing the task (#11597)* raises a warning for provide_context instead of killing the task* Update airflow/operators/python.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* static checksCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Set doc_md when using task decorator and function has __doc__ (#11598),2
Cross Reference XCom in tutorial (#11600),5
Minor improvements to docs/tutorial.rst (#11604),2
Remove unnecessary use of comprehension in setup_provider_packages.py (#11605),1
Fix tcp keepalive parameters parsing (#11594),2
Fix rendering of code-block in operator/kubernetes.rst (#11606),1
Fix Task definition in docs (#11601),2
Minor doc improvements in blob_storage_to_gcs.rst (#11607)- Sentence completion- code-block rendering- Link to english docs for Azure instructions,2
Remove redundant code to serialized k8s.V1Pod (#11602),4
Replace old screenshots for managing Connections with new (#11608),1
bump werkzeug version (#11610)bump werkzeug version,5
"Behaviour to install all airflow providers added (#11529)In Airflow 2.0 we decided to split Airlow into separate providers.this means that when you prepare core airflow package, providersare not installed by default. This is not very convenient forlocal development though and for docker images built from sources,where you would like to install all providers by default.A new INSTALL_ALL_AIRFLOW_PROVIDERS environment variable controlsthis behaviour now. It is is set to ""true"", all packages includingprovider packages are installed. If missing or set to false, onlythe core provider package is installed.For Breeze, the default is set to ""true"", as for those cases youwant to install all providers in your environment. Similarly if youbuild the production image from sources. However when you buildimage using github tag or pip package, you should specifyappropriate extras to install the required provider packages.Note that if you install Airflow via 'pip install .' from sourcesin local virtualenv, provider packages are not going to beinstalled unless you set INSTALL_ALL_AIRFLOW_PROVIDERS to ""true"".Fixes #11489",0
Mention about .sh commands and templates in BashOperator docs (#11566),2
Add creating_job_id to DagRun table (#11396)This PR introduces creating_job_id column in DagRun table that links aDagRun to job that created it. Part of #11302Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
fix typo in 'Installing with Breeze' doc (#11611),2
Use permission constants (#11389)Use constants for permission resource and action names.,1
"Make DagRunType inherit from `str` too for easier use. (#11621)This approach is documented in https://docs.python.org/3.6/library/enum.html#others:```While IntEnum is part of the enum module, it would be very simple toimplement independently:class IntEnum(int, Enum):    pass```We just extend this to a str -- this means the SQLAlchemy has no troubleputting these in to queries, and `""scheduled"" == DagRunType.SCHEDULED`is true.This change makes it simpler to use `dagrun.run_type`.",2
"Add missing states in TaskGroup state (#11626)These states were not applied to TaskGroup in Graph View. This PR addsthem: removed, scheduled, running, shutdown and sensing",1
Expose flower and redis ports in breeze (#11624),5
Teardown of webserver tests is not picky about processes. (#11616)Fixes random failure when processes are still runningon teardown of some webserver tests. We simply ignor thatafter we send sigkill to those processes.Fixes #11615,0
"Name and optionally preserve data volumes in Breeze (#11628)So far breeze used in-container data for persisting it (mysql redis,postgres). This means that the data was kept as long, as long thecontainers were running. If you stopped Breeze via `stop` commandthe data was always deleted.This changes the behaviour - each of the Breeze containers hasa named volume where data is kept. Those volumes are also deletedby default when Breeze is stopped, but you can choose to preservethem by adding ``--preserve-volumes`` when you run ``stop`` or``restart`` command.Fixes: #11625",0
Strict type checking for provider Google (#11609),1
docs: Update Bigquery clustering docstrings (#11232),2
"Update order for pre-commits to fail fast (#11636)`shellcheck` is slow if a Bash Script is changed, hence it is moved down in the order.The following pre-commits are quick so moved them up the order:- pre-commit-descriptions- sort-in-the-wild- helm-lint",4
Fix typo in BREEZE.rst (#11637),2
Fix incorrect typing and move config args out of extra connection config to operator args (#11635),1
"Optimizes CI builds heavily with selective checks (#11541)* Images are not built if the change is not touching code or docs.* In case we have no need for CI images we run stripped-down  pre-commit checks which skip the long checks and only run for  changed files* If none of the CLI/Providers/Kubernetes/WWW files changed  the relevant tests are skipped, unless some of the core files  changed as well.* The selective checks logic is explained and documented.",2
Fix minor typos in tests (#11638)`cllient` -> `client``environement` -> `environment``naamespace` -> `namespace``alllow` -> `allow`,1
"Improves stability of K8S tests by caching binaries and repeats (#11634)* Improves stability of K8S tests by caching binaries and repeatsThe K8S tests on CI are controlled from the host, not frominside of the CI container image. Therefore it needs virtualenvto run the tests as well as some tools such as helm, kubectland kind. While those tools can bee downloaded and installedon demand, from time to time the download fails intermittently.This change introduces the following improvements:* the commands to download and setup kind, helm, kubectl are  repeated up to 4 times in case they fail* the ""bin"" directory where those binaries are downloaded is  cached between runs. Only the same combination of  versions of the tools are sharing the same cache.This way both cases - regular re-runs of the same jobs andupgrade of tools will be much more stable.",1
"Fix random kills during pre-commit image building (#11535)Seems like the trap with several steps and || true does not reallywork the way I wanted and when kill is run but the process isalready gone, we had error in the script.Looks like this approach with sub-process kill will do it.",0
Fixed an error introduced in selective checks (#11640)A few remnants of earlier version of the script caused occasionalerrors. Error introduced in #11541,0
"Fixes selective tests in case of missing merge commits (#11641)In case of very simple changes, there might be no merge commitsgenerated by GitHub. In such cases we should take the commit SHAinstead as the base of change calculation for selective tests.",3
"Revert ""Fixes selective tests in case of missing merge commits (#11641)"" (#11646)This reverts commit 4fcc71c2ffbf87585759f49ab7e426ccb9516f87.",4
"Revert ""Fixed an error introduced in selective checks (#11640)"" (#11647)This reverts commit 6fbb235f252ed7442447333c3ae7bd712c6643b6.",4
"Revert ""Optimizes CI builds heavily with selective checks (#11541)"" (#11648)This reverts commit 9237338f7553a3a6dc6d1ceccf05abcbd2bf9ba3.",4
"Dumps more logs in case of CI failure (#11614)We do not dump airflow logs on success any more, but we dump themand all the container logs in case of failure, so that we canbetter investigate cases like #11543 - that includes enablingfull deadlock information dumping in our mysql database.",5
"Remove usage of six (#11645)Since we support Py 3.6, there is no need of six library",1
fix cloudwatch and wasb taskhandler log path config (#11650),5
"Fixes MySQLToS3 float to int conversion (#10437)* fix: 🐛 Float to Int columns conversionThe `_fix_int_dytpes` method is applying the `astype` transformation tothe return of a `np.where` call. I added an extra step to the method inorder to apply this to the whole pd.Series. Note that Int64Dtype must beused as an instance, since Pandas will raise an Exception if a class isused.* test: Add dtype test for integers* style: Change line length",4
Use Python 3 Style super calls (#11644),1
"Sourcing the profile file should be sufficient to update the PATH, re-login is not required. (#11588)",1
Add ability to zoom into Graph view from selection in Tree View (#11553)Closed #11388Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>,1
Allow null schedule_interval in OpenAPI spec for DAGs (#11532),2
Add optional session argument to xcom_push. (#11485),1
Add Plugins View in web UI (#10770),1
"Fixes versioning for pre-release provider packages (#11586)When we prepare pre-release versions, they are not intended to beconverted to final release versions, so there is no need to replaceversion number for them artificially,For release candidates on the other hand, we should internally use the""final"" version because those packages might be simply renamed to thefinal ""production"" versions.Fixes #11585",0
Add service_account to Google ML Engine operator (#11619),1
Change to pass all extra connection paramaters to psycopg2 (#11019)Closes #10505Co-authored-by: priyankagovindaraju <Pallavika.05>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,2
Brings back GKEStartPodOperator to google provider. (#11664),1
"UX improvements of DAG tag filtering (#11661)* Improve/simplify UX/aesthetics of tag filter form* Keep persisting tag filter in cookie, update URL to always reflect tag filters",5
Improvement: Populate 'Configuration JSON' form with DAG default params json in the Trigger-DAG UI (#10839),2
Improvements for pod template file with git sync container (#11511)* Helm chart fixes in pod template- default pod_template image to `defaultAirflowRepository:defaultAirflowTag`- fix never-ending git-sync init containers- fix broken reference to volume* Fix helm chart test,3
Pod template file uses custom custom env variable (#11480),1
"Optimizes CI builds heavily with selective checks (#11656)* Images are not built if the change is not touching code or docs.* In case we have no need for CI images we run stripped-down  pre-commit checks which skip the long checks and only run for  changed files* If none of the CLI/Providers/Kubernetes/WWW files changed  the relevant tests are skipped, unless some of the core files  changed as well.* The selective checks logic is explained and documented.This is the second attempt at the problem with betterstrategy to get the list of files from the incoming PR.The strategy works now better in a number of cases:* when PR comes from the same repo* when PR comes from the pull_repo* when PR contains more than one commit* when PR is based on older master and GitHub creates  merge commit",7
Brings back fixup to CI optimisation (#11671)The fixup was lost during the rebase. This one restores it,0
"Introduced deterministic order in connection export (#11670)The tests for connection export failed when CLI tests arerun in isolation. The problem was with non-deterministicsequence of returned rows from connection export query.Rather than fixing the test to accept the non-deterministicsequence, it is better idea to return them always in theconnection_id order. This does not change functionality andis backwards compatible, but at the same time it gives stabilityin the export, which might be important if someone uses exportto determine for example if some connections were added/removed.",4
Bats tests should be much faster now for pre-commits. (#11662)For pre-commit run of the tests only the corresponding testsfor changed .sh files and changed .bats files should be run,1
Fix Start Date tooltip on DAGs page (#10637)Closes #10350,2
Switch PagerdutyHook from pypd to use pdpyras instead (#11151),1
"StreamLogWriter: Provide (no-op) close method. (#10884)Some contexts try to close their reference to the stderr stream at logging shutdown, this ensures these don't break.* Make pylint happyAn explicit `pass` is better here, but the docstring _is_ a statement.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",2
Removes separate cancel codeql workflow (#11672)The capability was added in the cancel-workflow-run action tocancel another workflow by name. We utilise it to merge cancelingof codeql workflow duplicates into the main build/cancel workflow,1
Add missing doc dependency in CI (#11678),2
"Security scans are also selective now (#11674)The security scans take a long time, especially for python code- it is about ~18 minutes now. This PR reduces strain on theGitHub actions by only running the scan in pull requestswhen any of python/javascript code changed respectively.",4
Add Kerberos Auth for PrestoHook (#10488),1
Fixing problem with missing output in pre-commits in some cases (#11684)Dumping logs from container should only be done in CI.Problem was introduced in #11614,0
Add missing template in pip package - python_virtualenv_script.jinja2 (#11677),1
"Improve example DAGs data by diversifying ""tags"" value (#11665)",5
Simplify import check in CLI (#11668)* Simplify import chekc in CLI* fixup! Simplify import chekc in CLI* fixup! fixup! Simplify import chekc in CLI,2
Add datumo.io to Airflow family (#11692),5
Fix example DAGs in pip packages (#11687),2
Add D200 pydocstyle check (#11688),2
Improve legibility with greater contrast ratio in footer and pagination (#11690),1
fix pod launcher rolebinding in helm chart (#11675)* Followup to #11034* Was not referencing the correct kind of resources if multiNamespaceMode = False,2
Extract Kubernetes command to separate file (#11669)* Move Kubernetes command to seperate file* fixup! Move Kubernetes command to seperate file,2
Fix documentation for PythonVirtualenvOperator (#11700)Fixed the op_args type description,0
Update TIs with a proper lock (#11683),5
"Revert ""Refactor celery worker command (#11336)"" (#11698)This reverts commit 02ce45cafec22a0a80257b2144d99ec8bb41c961.That refactored Clery worker to be compatible with 5.0. However thisintroduced some incompatibilities.Closes: #11622Closes: #11697",1
Make table header text nowrap (#11689),1
Consistent use images in Helm Chart (#11701),2
Better file extension for Helm template (#11702)* Better file extension for Helm template* fixup! Better file extension for Helm template,2
Enforce strict rules for yamllint (#11709),1
"Fix doc errors in google provider files. (#11713)These files aren't _currently_ rendered/parsed by autoapi, but I wasexploring making them parseable and ran in to some sphinx formattingerrors.The `Args:` change is because pydocstyle thinks that is a special word, butwe don't want it to be.",2
Fix case of GitHub (#11398),0
Check response status in slack webhook hook. (#11620),1
Remove unused value in Helm Chart - podMutation (#11703),2
"Also cancel duplicated Build Image runs for master pushes (#11655)We removed the ""skipEventType"" for CI jobs but we still hadthe skipping for Build Image and killing builds with failed jobs.This further optimises our job usage.",0
Fix case of JavaScript (#11718)Changes were:- `javascript` to `JavaScript`,4
Retry requests in case of error in Google ML Engine Hook (#11712),1
Add support for setting ciphers for SFTPHook (#11720),1
"Refactor ""loading"" state of DAGs view to remove visual jank (#11725)",4
Fix formatting errors introduced in #11720 (#11733),0
Add type hints to  aws provider (#11531)* Added type hints to aws provider* Update airflow/providers/amazon/aws/log/s3_task_handler.py* Fix expectation for submit_job* Fix documentationCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add Google Cloud Memorystore Memcached Operators (#10121)Co-authored-by: Tobiasz Kędzierski <tobiasz.kedzierski@polidea.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Fix static checks after merging #10121 (#11737),0
"Fixes ROVIDERS -> PROVIDERS typo in Dockerfile (#11738)There was a typo in the original file when review was made inthe #11529 but apparently this typo was still left in one placeand as the result, providers have not been installed in themaster Dockerfile.Fixes #11695",2
Ensure task logs go to the correct try number file (#11723)The run context (logging context) accesses task instance attributes via the log_filename_template configuration.Fixes #11717.,5
"Stop scheduler from thinking that upstream_failed tasks are running (#11730)This was messing up the ""max_active_runs"" calculation, and this fix is a""hack"" until we add a better approach of adding a queued state toDagRuns -- at which point we don't even have to do this calculation atall.",2
"Use unittest.mock instead of backported mock library (#11643)mock is now part of the Python standard library, available as unittest.mock in Python 3.3 onwards.",3
Improve Cloud Memorystore for Redis example (#11735),1
"Speed up `dag.clear()` when clearing lots of ExternalTaskSensor and  ExternalTaskMarker (#11184)This is an improvement to the UI response time when clearing dozens of DagRuns of large DAGs (thousands of tasks) containing many ExternalTaskSensor + ExternalTaskMarker pairs. In the current implementation, clearing tasks can get slow especially if the user chooses to clear with Future, Downstream and Recursive all selected.This PR speeds it up. There are two major improvements:Updating self._task_group in dag.sub_dag() is improved to not deep copy _task_group because it's a waste of time. Instead, do something like dag.task_dict, set it to None first and then copy explicitly.Pass the TaskInstance already visited down the recursive calls of dag.clear() as visited_external_tis. This speeds up the example in test_clear_overlapping_external_task_marker by almost five folds.For real large dags containing 500 tasks set up in a similar manner, the time it takes to clear 30 DagRun is cut from around 100s to less than 10s.",2
Docs: Typo in Smart sensor (#11746)``sensors_enabled`` config key was mistyped.,5
Fix Static Checks (#11749),0
"Enables splitting tests into smaller chunks (#11659)We've implemented the capability of running the tests in smallerchunks and selective running only some of those, but thiscapability have been disabled by mistake by default setting ofTEST_TYPE to ""All"" and not removing it when TEST_TYPES are setto the sets of tests that should be run.This should speed up many of our tests and also hopefullylower the chance of EXIT 137 errors.",0
Improve web server stopping (#11734),1
"Add reattach flag to ECSOperator (#10643)..so that whenever the Airflow server restarts, it does not leave rogue ECS Tasks. Instead the operator will seek for any running instance and attach to it.",1
"Ensure that manually creating a DAG run doesn't ""block"" the scheduler (#11732)It was possible to ""block"" the scheduler such that it would notschedule or queue tasks for a dag if you triggered a DAG run when theDAG was already at the max active runs.This approach works around the problem for now, but a better longer termfix for this would be to introduce a ""queued"" state for DagRuns, andthen when manually creating dag runs (or clearing) set it to queued, andonly have the scheduler set DagRuns to running, nothing else -- thiswould mean we wouldn't need to examine active runs in the TI part of thescheduler loop, only in DagRun creation part.Fixes #11582",0
"Fix FAB actions with for models with composite PKs (#11753)This fixes #11513 -- and has been submitted upstream to FAB ashttps://github.com/dpgaspar/Flask-AppBuilder/pull/1493, once that ismerged we can remove this override.",4
"Add test types displayed in CI (#11770)This is a follow up after #11659. Thanks to this one, it willbe immediately visible in CI which types of tests are run.This change has been lost during one of the rebases, so it isbrought back now.",4
fix: Override FAB styling (#11752),0
Add uSmart Securities to the INTHEWILD.md (#11757),1
The .tar.gz provider packages are installable now. (#11630)The packages lacked setup.py and they could not be installed.This change automatically generates setup.py for the packages andadds them to the packages.Fixes: #11546,0
Add reference link for KubernetesPodOperator in kubernetes.rst (#11782)This makes it easy to go to the class definition and find the arguments/params that can be passed to the Operator,1
"The .pypirc file is read from docker-context-files (#11779)If you used context from git repo, the .piprc file was missing andCOPY in Dockerfile is not conditional.This change copies the .pypirc conditionally from thedocker-context-files folder instead.Also it was needlessly copied in the main image where it is notneeded and it was even dangerous to do so.",2
"Improve the visual presentation of the variable import form (#11783)No functional changes, just visual enhancement of the variable importform. This simply refactors the markup slightly to better utilize some Bootstrapstyling and add an icon to the button.",1
"Improve interaction with Recent Tasks/DAG Runs circles by ignoring pointer-events (#11786)Resolves #11563. With the hover event triggering the tooltips, the tooltip was triggering a mouse-out event when the cursor was at the top of the circle (where the tooltip is presented). This was causing the fluttering behavior due to the looping off these events.",1
"Log task_instance execution duration as milliseconds (#10632)This is best achieved by passing a `timedelta()` to `Stats.timing()`, and leaveworrying about time units to that method.",4
Standardize the Airflow CLI help descriptions (#11790),5
[AIRFLOW-6585] Fixed Timestamp bug in RefreshKubeConfigLoader (#11219)Co-authored-by: Jan Brusch <jan.brusch@neuland-bfi.de>Co-authored-by: Marian Cepok <marian.cepok@neuland-bfi.de>,5
"Fix locking issue stopping tasks running with SequentialExecutor (#11797)Missing a commit means that the the row level lock was not releasedbefore `executor.heartbeat()` was called.  This was only a problem forthe SequentialExecutor, as all the other executors would continuerunning the scheduler code so the lock would be released shortly aftertasks are sent to the executor anyway. (Where as SequentialExecutordoesn't return control until tasks have run!)",1
Fix backwards compatibility with k8s executor_config resources (#11796),5
Add Project URLs for PyPI page (#11801)These links would show up at https://pypi.org/project/apache-airflow/,2
Update download url for Airflow Version (#11800)All the versions are available at https://archive.apache.org/dist/airflow/ and this link appears in https://pypi.org/project/apache-airflow/,2
Bump attrs to > 20.0 (#11799)fixes https://github.com/apache/airflow/issues/11756,0
"Remove loading DagBag multiple times in test_views.py:TestDecorators (#11804)Previously we were loading DagBag twice, once in `setUpClass` and once in `setUp` (which is called everytime a test in that class is run)",1
"Use Python 3 style super classes (#11806)example:```super().__init__(label, validators, **kwargs)```instead of```super(DateTimeWithTimezoneField, self).__init__(label, validators, **kwargs)```",5
Use PEP 380: Syntax for Delegating to a Subgenerator (#11805)Since Master is Py 3.6>= we can use PEP 380 which was introduced in Python 3.3.https://docs.python.org/3/whatsnew/3.3.html#pep-380,2
"Remove redundant ""UTF-8"" in Python 3 (#11808)```In [1]: ""result"".encode(""UTF-8"")Out[1]: b'result'In [2]: ""result"".encode()Out[2]: b'result'```",4
Update to latest isort & pre-commit-hooks (#11813),1
Remove redundant blank-line and parenthesis (#11811),4
"Replace non-empty sets with set literals (#11810)Example:```python{paused_dag_id for paused_dag_id, in paused_dag_ids}```Instead of```pythonset(paused_dag_id for paused_dag_id, in paused_dag_ids)```",2
Replace io.open with builtin open (#11807)From Python 3 `io.open` is an alias for the builtin `open()` function.Docs: https://docs.python.org/3/library/io.html#io.open,2
Use LocalExecutor by default with tmux + Breeze (#11791)* Use LocalExecutor by default with tmux + Breeze* Update run_tmux.sh* Update run_tmux.sh,1
Fix spelling and grammar (#11814),0
Remove redundant builtins imports (#11809)These imports are not needed in Python3,2
Add links to PyPI packages. (#11818)Same as in #11801 but in provider's packages.,1
Fix spelling (#11457),0
Fix spelling (#11821),0
Fixes flaky test test_should_response_200_with_reset_dag_run (#11817)Sometimes the rows were returned in reverse order.Fixes #11816,0
"Switch postgres from 10 to 13 (#11785)Seems that postgres is really stable when it comes to upgrades,so we take the assumption that if we test 9.6 and 13, and theywork, all the versions between will also work.This PR changes Postgres 10 to 13 in tests  and updates documentationwith all the versions in between.",2
Fix typo in scripts/in_container/entrypoint_ci.sh (#11824)* Fix typo in scripts/in_container/entrypoint_ci.sh* Update bats_tests.sh,3
Update download url for Airflow Providers Version (#11823)All the versions are available at https://archive.apache.org/dist/airflow/ and this link appears in https://pypi.org/project/apache-airflow/,2
Fix bug when marking tasks when DAG Serialization is enabled (#11803)This is because `current_task` is of SerializedBaseOperator type.,5
Removes duplicates from DISABLED_INTEGRATIONS variable (#11831)Presto DB is checked several times but it also means thatit is added several times to DISABLED_INTEGRATIONS in caseit is not enabled. This commit fixes it.,0
Fixes typos in production-deployment docs (#11833),2
Fix example DAGs tests + add sanity checks (#11840),1
Generated backport providers readmes/setup for 2020.10.29 (#11826),1
Dag Run endpoints returns count total results after filtering (#11832),1
Fix spellings (#11825),0
Fix the script that builds source for backports (#11846)First time preparing backports after converting scripts toalso support regular providers. Some small bugs were foundand fixed.,0
Fix commands in docs/usage-cli.rst(#11847)* Add uSmart Securities to the INTHEWILD.md* change airflow dag to airflow dags in 2.0,2
Few more small fixes found during preparation of the backports (#11848),0
Local Executor is used by default for MySQL/Postgres breeze (#11792),1
Google Memcached hooks - improve protobuf messages handling (#11743),1
Unauthenticated access with RBAC to URL has_dag_access results lose redirection (#11592)Co-authored-by: Michael Permana <mpermana@pinterest.com>Closes #11591,2
"The ""OpenAPI"" tests missed dependency on build-info step (#11849)The tests were not running for a few days because thedependency (needs) was missing - it was needed to get theselective checks work after the recent refactor in #11656.",4
"Use packaging.version, not semver module for version comparisons (#11854)Semver module doesn't like python version specifiers such as `0.0.2a1`-- since packaging module is already a dep from setup tools, and is whatthe python ecosystem uses to do version handling it makes sense to useit.",1
Prepare providers release 0.0.2a1 (#11855),1
Add @dag decorator (#10587),2
"Occasional docker-compose errors will be easier to diagnose (#11835)With this change we attempt to better diagnose some occasionalnetwork docker-compose issues that have beeen plaguing us afterwe solved or workarounded other CI-related issues. Sometimesthe docker compose jobs fail on checking if the container isup and running with either of the two errors: * 'forward host lookup failed: Unknown host` * 'DNS fwd/rev mismatch'Usually this happens in rabbitMQ and openldap containers.Both indicate a problem with DNS of the docker engine or maybesome remnants of the previous docker run that do not allow usto start those containers.This change introduces few improvements:* added --volume in `docker system prune` command which might  clean-up some anonymous volumes left by the containers between  runs* removed docker-compose down --remove-orphans --down command  after failure, as currently we are anyhow always doing it  few lines before (before the test). This change will cause  that our mechanism of logging container logs after failure  will likely give us more information about in case the root  cause is rabbitmq or openldap container failing to start* Increases number of tries to 5 in case of failed containers.",0
Cut 2.0.0alpha2 (#11860),5
Update instruction to only push a single tag while releasing Airflow (#11861),5
"Make Dag Serialization a hard requirement (#11335)Scheduler HA uses Serialized DAGs and hence it is a strictthe requirement for 2.0.It also has performance benefits for the Webserver and so shouldbe used by default anyway.Task execution on workers will continue to use the actual files for execution.Scheduler, Experimental API and Webserver will read the DAGs from DB using`DagBag(read_dags_from_db=True)`",5
Add FanDuel to the list of users in the wild.  (#11864),1
Pin werkzeug back to <1.0.0 (#11872)closes https://github.com/apache/airflow/issues/11871,0
Fix failing tests in tests/models/test_dag.py (#11868),3
Use correct name for PostgreSQL (#11869)`PostgresSQL` -> `PostgreSQL`,1
add wix & wixanswers (#11863),1
Retrieve PR labels from the associated PR (#11820)This change is needed to implement optimisation of running onlylimited version of the matrix tests for PRs that are not yetapproved by committers.,3
"Constraints generation runs regardless from test status (#11838)Constraints generation did not run when tests were failingbut this was wrong. In case of tests that fail due to constraintsupgrade we neeed to see the output of constraint generation, toknow which constraints failed. Only pushing the constraintshould be limited to the case where everything succeeded.This should make investigation of problems similar to #11837 mucheasier.",0
"Adds newly added fast CI jobs to ""fail-fast"" cancelling (#11830)We ""fail-fast"" the whole workflow run if we the queue is busy andwe notice that there is a failing ""fast"" job - i.e. job thatprobably failed rather fast, and you will have to rerun the buildanyway, so cancelling tests for the build that is going to bere-pushed again is a nice thing to do for others.There were two jobs added recently that did not match theregexp for job names.",1
Fixes the doc pattern in selective checks (#11834)The pattern contained $ which effectively stopped docs frombeing run on doc-only change :(,4
Upgrade FAB to 3.1.1 (#11884)We can also remove the FAB Actions fix with composite PKs (https://github.com/apache/airflow/pull/11753) since it is merged and release in FAB 3.1.1,7
"Revert ""fix: Override FAB styling (#11752)"" (#11888)This reverts commit 2df9d1c4d78d0add5efa18831725ddcc2a93d738.",1
Fix broken link in README.md (#11885)* Link to committers list was broken,2
Update start.rst (#11886),5
Fixes a problem with checked-out version of the selective check (#11891)The non-master version of selective check script hasbeen checked out by mistake in the build-info step.,5
Fixes backtick in the documentation (#11892),2
"Add providers, operators, and hooks readmes (#11829)* Add readmes with relevant information for Airflow 2.0 to providers, operators, and hooks directories* Add newline to end of files* Remove hanging EOF empty line",4
Fix typo in the word daemon (#11897)`deamon` -> `daemon`,2
Updating 2.0 docs (#11842)* Separate 2.0 upgrade steps into their own file* shell script* more changes,4
"Add readme for core sensors, standardize capitalization schema (#11898)",1
"Change Decorated Flow to TaskFlow API (#11895)* Change Decorated Flow to TaskFlow APIIn order to better describe what this new data transfer system does,we are changing the name of Decorated Flows to TaskFlow API* Update docs/concepts.rstCo-authored-by: Gerard Casas Saez <casassg@users.noreply.github.com>* Update docs/concepts.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update docs/tutorial_taskflow_api.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Gerard Casas Saez <casassg@users.noreply.github.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
"Unpin werkzeug & set default cookie_samesite to Lax (#11873)As suggested by https://flask.palletsprojects.com/en/1.1.x/config/#SESSION_COOKIE_SAMESITE , set `cookie_samesite` to `Lax`.This Werkzeug bug prevented using the latest version of Werkzeug (1.1.0): https://github.com/pallets/werkzeug/issues/1549 | https://github.com/pallets/werkzeug/pull/1550https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-CookieDetailed explanation: https://web.dev/samesite-cookies-explained/",2
Fix broken docs build (#11900),2
Add Python Helm testing framework (#11693)* Helm Python Testing* helm change* add back args,1
Add taskflow to accepted words (#11902),1
Add truncate table (before copy) option to S3ToRedshiftOperator (#9246)- add table arg to jinja template fields- change ui_colorCo-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,4
"Fix CI Step Name for Postgres (#11908)https://github.com/apache/airflow/commit/0d1ad6648ee63311614e043d5893cf36a0cd9aea updated the Step name by Mistake from `""Tests: ${{needs.build-info.outputs.testTypes}}""` to `""Tests: Helm""`",3
Constraints job depends on CI images (#11904)The constraints dependencies removed in #11838removed ci-images implicit dependency which is needed for theconstraints step to complete. This caused occasional failures inthe push /merges builds.,7
Fix Helm Chart Testing guide (#11909),3
Add note on GKE metadata server Workload Identity (#10728)* Add note on GKE metadata server Workload Identitycc: @mik-laj @potiuk* fixup! move detailed notes on ADC to separate section* fixup! remove trailing whitespace,4
Fix broken doc build on Master (#11915),2
Fixing re pattern and changing to use a single character class. (#11857),1
Add Ryan Hamilton to Committers list (#11923),1
Make taskinstances pid and duration nullable (#11906),1
"Simplify ""scheduled"" conditons to follow today's change in GA (#11876)GitHub Actions policy about runnig scheduled workflows in forkshas changed recently and as of October 27 2020 scheduled workflowsin forks will be disabled. Thanks to that change we can simplifysome of our conditions that disallowed running scheduled workflowsin forks.The change is active as of today (email from GitHub):> What will happen to scheduled workflows in forks I already have?> If you already have scheduled workflows in forks of public  repositories, they will be disabled on October 27, 2020. Repository  owners will receive a reminder email 7 days prior and can choose to  keep the workflows running at that time if they are needed.",1
Fix spelling problem introduced in #11923 (#11927),0
"Add docs about Scheduler HA, how to use it and DB requirements (#11467)",1
"Use resource permissions for Airflow view access (#11362)Migrate the Airflow view to use resource-backed permissions. This includes creation of a custom has_access decorator, deprecation of the has_dag_accessdecorator, a migration script to update existing roles to use the new permissions, and adding the new has_access decorator to the Airflow view class's methods.",1
"The PRs which are not approved run subset of tests (#11828)This PR is an implementation of optimisation - to only rundefault values for build matrix in case PR does not have""okay to test"" label.This ""okay to test"" label is set when the PR gets approvedbut it was not approved before, also then a comment is generatedurging the committer to rebase the PR to run full set of tests.Additionally a check is added (in-progress) that makes the PRnot yet ready to be merged. Only after re-running it it willbecome truly readty to be merged.",7
Quarantine test_exception_propagation (#11933)Issue created #11932,1
Add Umami Collective as Airflow user (#11935),1
"ci: Fix CodeQL Workflow for Javascript (#11941)We removed a bracket in bdd5e0b#diff-63bd641104d10e25f141d518a16b22a151d125e12701df2f9e79734b23b90188 due to which one of the conditions for CodeQL javascript has changed by mistake, so it is trying to run git checkout HEAD^2 for merged commits too instead of just running it on PRs.Example failed run on Master: https://github.com/apache/airflow/runs/1326054227",1
"Update flask_wtf version to work with werkzeug>=1.0 (#11939)Werkzeug 0.16 deprecated werkzeug.url_encode, and removed it in 1.0, sowe need the fixed version of flask_wtf",0
Use Github Discussions to asking User Questions (#11940),1
Add drain option when canceling Dataflow pipelines (#11374)* Add drain option when cancel Dataflow pipelines* fixup! Add drain option when cancel Dataflow pipelines* fixup! fixup! Add drain option when cancel Dataflow pipelines* fixup! fixup! fixup! Add drain option when cancel Dataflow pipelines,5
Standardize quotes in HTML files (#11724),2
Add Template Fields to RedshiftToS3Operator & S3ToRedshiftOperator (#11844),1
Add How-to guide for JDBC Operator (#11472),1
Remove archived link from README.md (#11945)closes https://github.com/apache/airflow/issues/11943,0
"Rename example JDBC dag (#11946)This was merged in #11472, but for some reason that didn't run tests sothe failure wasn't noticed",0
"Fix failing docs build on Master (#11951)This was merged in #11472, but for some reason that didn't run tests sothe failure wasn't noticed.a7ad204 fixed 1 error but the docs error was missed",0
Fix issue rendering k8s V1Pod (#11952)* Fix issue rendering k8s V1PodAdds return statement for json serialization of k8s.V1Pod thatpreviously caused errors in UI* fix task_instances,0
Fix broken master (isort fix) (#11954)Static checks are failing because of a Bad merge to Master.,7
"Fix oversized width of DAGs table with hide/reveal of ""links"" (#11866)* Conserve horizontal space by adding hide/reveal ""links"" in DAGs table* Reverse the order of links to reduce mouse distance for most popular",1
"Make mypy happy with airflow.executors.local_executor (#11944)This is an odd one -- in making a change in another file, this startederroring - I guess it gets confused about the fork, but I'm not surewhat changed to cause this to become a problem.",0
"Add missing space to log message in task completion logs (#11934)Previously it looked like this    INFO - Marking task as SUCCESS.dag_id=scenario1_case1_1_1, ...Now it looks like    INFO - Marking task as SUCCESS. dag_id=scenario1_case1_1_1, ...",2
"Cache CLI parser objects (#11957)Right now, when we are using the CLI parser in LocalTaskJob andexecutor, having to re-create the parser each time adds a noticeableoverhead for quick tasks -- by caching this we save 0.07s(I know that doesn't sound like much, but for an `echo true`BashOperator task that takes it from 0.25s to 0.19s - i.e. it cuts thetime down by about 20%!)",1
"Update flask-caching dep to version that works with Werkzeug 1 (#11955)Werkzeug 1.0 removed the werkzeug.contrib.caching module, so we need atleast 1.5.0 of flask-caching to deal with this -- seehttps://github.com/sh4nks/flask-caching/blob/v1.9.0/CHANGES#L97-L107",4
"Speed up task execution in Celery by pre-loading ""expensive"" modules (#11956)These modules add about 0.3s to _every_ task run time, which is nothingfor a long task, but significant if you want to run a lot of short tasksvery quickly.These modules were discovered by running with PYTHONPROFILEIMPORTTIME=1environment variable set and looking at what was imported and expensive.",2
Migration commands shouldn't print a bunch of data. (#11961),5
"Log instead of raise an Error for unregistered OperatorLinks (#11959)Currently, if someone uses OperatorLinks that are not registered,it will break the UI when someone clicks on that DAG.This commit will instead log an error in the Webserver logs so thatsomeone can still see the DAG in different Views (graph, tree, etc).",2
"Clean up command-line arguments (#11682)* Clean up command-line arguments- The `--do-pickle` argument is exclusively used by the scheduler, the `worker` command completely ignores it.- Remove the `ARG_DAG_ID_OPT` entry entirely. The scheduler doesn't take a `--dag-id` option, and while the `dags list-runs` and `dags list-jobs` commands *do* take such a switch, the help text here is incorrect, they simply need to use `ARG_DAG_ID`.* Put ARG_DAG_ID_OPT backIt differs from ARG_DAG_ID in that it is an optional command-line switch. I changed the help text to match its use: to select a specific DAG when listing runs or jobs. The dag is not actually triggered.",2
Move test_serve_logs to quarantine as it fails on mysql sometimes (#11963)Issue recorded at #11962,0
"Moves tests that should be always executed to 'always' directory (#11948)Some tests (testing the structure and importability ofexample) should be always run even if core part was not modified.That's why we move it to ""always"" directory.",4
Add autocommit property for snowflake connection (#10838),5
Fix: Responsive layout of DAGs (Home) view (#11958),2
Migrate from helm-unittest to python unittest (#11827)* Migrate from helm-unittest to python unittest* fixup! Migrate from helm-unittest to python unittest* fixup! fixup! Migrate from helm-unittest to python unittest,3
"Bump attrs and cattrs dependencies (#11969)`cattrs` now depends on `attrs >= 20.1.0`, because of `attr.resolve_types`.Source: https://github.com/Tinche/cattrs/blob/master/HISTORY.rst#110-2020-10-29closes https://github.com/apache/airflow/issues/11965",0
"Allow fractional seconds for timeout values (#11966)Python 3.3 added the `sigitimer()` function, which unlike `alarm`,allows fractional seconds to be specified (it still raises a SIGALRMwhen the timeout expires)",1
Pin `kubernetes` to a max version of 11.0.0. (#11974)12.0.0 introduces `TypeError: cannot serialize '_io.TextIOWrapper'object` when serializing V1Pod's in `executor_config`.,5
Add SalesforceToGcsOperator (#10760)Adds SalesforceToGcsOperator that allows users to transfer data fromSalesforce to GCS bucket.Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,1
Fixes broken ci.yaml workflow (#11981)The PR #11827 broke the ci.yaml workflow by not removing helm-testreference in tag-repo-nightly. This PR fixes it.,0
Move Project focus and Principles higher in the README (#11973),4
Change should_response to should_respond (#11978),4
Improve instalation command (#11971),1
Update doc images to reflect latest UI (#11984),3
"Implements canceling of future duplicate runs (but the latest) (#11980)* Implements canceling of future duplicate runs (but the latest)Previous version of the cancel-workflow-runs action implementedcanceling of only past duplicates, but when there are queuesinvolved, some future ""cancel-workflow-runs"" might be in a queuefor  long time. This change has the effect that cancel-workflow-runsfor duplicates will also allow future runs of the same branch/reposparing only the most recent run - no matter if the duplicateswere older than my own run.This should handle the case where we have queues blocking the""cancel-workflow-runs"" from running.* Update .github/workflows/build-images-workflow-run.ymlCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
Fix broken link in config.yml (#11986)Fixes broken link to k8s docs,2
Improve formatting in configurations docs (#11987),2
Added Bloomreach to the list of companies using Apache Airflow (#11995),1
"Update ""click"" to 7.x (#11999)The only breaking change in click between 6 and 7 is automatic naming ofcommands from functions with underscores in their name, but this doesn'tapply to ushttps://click.palletsprojects.com/en/7.x/upgrading/#upgrading-to-7-0",1
"Remove unused ""poke_exception_cache_ttl"" param from SmartSensorOperator (#11972)These four places are the only places this param appears in the codebase, and is likely left over from open sourcing the original feature.",2
"Disable XCom pickling by default (#11991)We have a note in UPDATING.md that we would change this default and even remove pickling for XComsince Airflow 1.9.0 (Jan 2, 2018).closes https://github.com/apache/airflow/issues/9606",0
Validate airflow chart values.yaml & values.schema.json (#11990)* Correct type for multiNamespaceMode chart value* Updated values.schema.json to reflect the latest change and to be stricter* Fixed current test* Added a test to validate the values file against the schema,2
Add Flower Authentication to Helm Chart (#11836),2
fix helm chart worker deployment without kerberos (#11681)Follow up to #11130 : we shouldn't mount the `kerberos-keytab` volumein the worker deployment if we are not usingkerberos in the first place.(the previous behavior is breaking the chart),2
All k8s object must comply with JSON Schema (#12003)* All k8s resources should have global labels* All k8s object must comply with JSON Schema,5
"fix helm scheduler deployment / scheduler logs (#11685)Based on the airflow image entrypoint, we should use airflow commands directly.The container exits otherwise.",1
"Switches to ""cancel-all-duplicates' mode of cancelling. (#12004)The cancel-workflow-runs action in version 4.1 got the capabilityof cancelling not only duplicates of own run (including future,queued duplicates) but also cancelling all duplicates from allrunning worklfows - regardless if they were triggered by my ownPR or some other PRs. This will be even more helpful with handlingthe queues and optimising our builds, because in case ANY ofthe build image workflows starts to run, it will cancel ALLduplicates immediately.",1
Adding SnowflakeOperator howto-documentation and example DAG (#11975)closes #11921,2
Fix K8S CI job name rendering (#12007),0
Adds documentation about the optimized PR workflow (#12006)We had a lot of problems recently about the queues in GithubActions. This documentations explains the motivation and approachwe have taken for optimizing our PR workflow.,1
"Remove hacktoberfest label (#12013)Since hacktoberfest is now over, let's remove the label",4
Adding Raisin to list of companies using Airflow (#12015),1
Code smell fixes for BackfillJob (#12005)- merge IFs when we can;- One of the IF condition check should reuse STATES_COUNT_AS_RUNNING which has been already defined;- Remove unused parameter for intermediate function _per_task_process(),1
Add contributor-targeted description of the PR workflow. (#12016)* Add contributor-targeted description of the PR workflow.Simpler/shorter description of the PR workflow targeted for newcontributors and contributors who did not follow the recent changesin the PR workflow.* Update CONTRIBUTING.rstCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>* Update CONTRIBUTING.rstCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,5
"Allow airflow.providers to be installed in multiple python folders (#10806)For example, this allows some providers to be installed in site packages(`/usr/local/python3.7/...`) and others to be installed in the user folder(`~/.local/lib/python3.7/...`) and both be importable.If we didn't have code in `airflow/__init__.py` this would be mucheasier to achieve (we simply delete the top level init file would beenough) - but sadly we can't take that route.From the docs of pkgutil: https://docs.python.org/3/library/pkgutil.html#module-pkgutil> This will add to the package’s __path__ all subdirectories of> directories on sys.path named after the package. This is useful if one> wants to distribute different parts of a single logical package as> multiple directories.Tested as follows:```$ pip install /wheels/apache_airflow-2.0.0.dev0-py3-none-any.whl$ ls -ald $(python -c 'import os; print(os.path.dirname(__import__(""airflow"").__file__))')/providersls: cannot access '/usr/local/lib/python3.7/site-packages/airflow/providers': No such file or directory$ pip install --constraint <(echo 'apache-airflow==2.0.0.dev0') apache-airflow-backport-providers-redis$ pip install --user --constraint <(echo 'apache-airflow==2.0.0.dev0') apache-airflow-backport-providers-imap$ python -c 'import airflow.providers.imap, airflow.providers.redis; print(airflow.providers.imap.__file__); print(airflow.providers.redis.__file__)'/root/.local/lib/python3.7/site-packages/airflow/providers/imap/__init__.py/usr/local/lib/python3.7/site-packages/airflow/providers/redis/__init__.py```",5
"Adds more aggressive cancelling of duplicate Build Image jobs (#12018)This change adds even more aggressive cancelling of duplicates of'Build Image' jobs. it's not an obvious task to know whichBuild Image jobs are duplicates, we are matching those duplicatesbased on specially crafted ""build-info"" job names. We addEvent, Branch, Repo to the job names and assume that tworuns with the same event + branch + repo are duplicates.It also disables self-preservation for this step becauseit is perfectly ok to cancel itself in case there is a newerin-progress Build Image job.Unfortunately even this will not work perfectly well. Those jobnames are resolved only for the jobs that are runnning rather thanthe queued ones, so in case we have several duplicates of thesame build image job in the queue, they will not be found/cancelled.The cancelling will only happen if both duplicates are alreadyrunning.It's good enough for now and we cannot do much more until thereis a missing feature added to GitHub API that allows to linkthe workflow_run with the run that triggered it. This issue hasbeen raised to GitHub Support and internal engineering tickethas been apparently opened to add this feature.More detailed status for the missing feature is kept at #11294",1
Require atleast 1 approving reviews for PRs (#12020)Looks like we can control this setting via .asf.yaml : https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features#git.asf.yamlfeatures-BranchProtection,5
"Refine request check in api_connextion Pool endpoints (#12019)If I miss both required properties in my request, I will only be warned one by one.That means I need to fix my error for twice, which is not necessary.This commit helps check all missing properties in one shot.",0
"Revise ""Project Focus"" copy (#12011)",5
"Fixes problem with non-iterable data returned by GH API (#12021)The action to cancel workflow switched from deprecatedmethod of retrieving jobs to a 'better' one but it causedsome unexpected failures as some of the job data is not iterable and failures in 'failedJobs"" matchingVersion 4.6 fixed the problem.",0
Fix typos (#12022),2
Fix canceling of CodeQL workflow (#12024)The previous update for 4.3 version of the action also brokeCodeQL cancelling. This PR fixes it.,0
"Turns failure of PR label when approved action into warning (#12017)Sometimes (quite often really) when PR gets approved, the PRgets merged rather quickly, without waiting for result of thisaction. Or a new PR gets pushed quickly. In those cases PR willnot be found. But this is usually not a problem then and ratherthan failing, we should simply print a warning and exit.",2
Added Axesor to INTHEWILD.md (#12026)* Added Axesor to INTHEWILD.md* Added Axesor (adac) to INTHEWILD.md,1
Added Avesta to the list of companies using Apache Airflow (#12027),1
Add Elai Data to INTHEWILD.md (#12029),5
Improve handling server errors in DataprocSubmitJobOperator (#11947)* Improve handling server errors in DataprocSubmitJobOperator* fixup! Improve handling server errors in DataprocSubmitJobOperator,5
Add homebrew/python/setproctitle issue to FAQ (#12025),0
Fix doc build error (#12034),0
Fix spelling build (#12036),0
Updates backport release process decription (#12032),5
Checks if all the libraries in setup.py are listed in installation.rst file (#12023),2
Fixes documentation-only selective checks (#12038)There was a problem that documentation-only checks triggeredselective checks without docs build (they resulted inbasic-checks-only and no images being built.This occured for example in #12025This PR fixes it by adding image-build and docs-build as twoseparate outputs.,2
Ignore the basepath when ignoring files via .airflowignore (#11993),2
Replace Docs GIF with updated UI screenshots (#12044)Resolves #11175,0
Install cattr on Python 3.7 - Fix docs build on RTD (#12045),2
adding quick description for singularity container operator (#12047)Signed-off-by: vsoch <vsochat@stanford.edu>,1
Fix incorrect .airflowignore behavior with multiple nested directories (#11994)* Add failing test* Fix failing test,3
Update faq.rst (#12041)* Update faq.rst* Update faq.rst,5
Adds updating note to ariflowignore fix (#12043)This is a follow-up after #11993 - the behaviour of airflowignorechanges significantly enough to require a warning.,2
Override FAB table views where table width extends beyond parent containers (#12048),2
"Retry Dagbag.sync_to_db to avoid Deadlocks (#12046)Previously we added Retry in DagFileProcessor.process_file toretry dagbag.sync_to_db. However, this meant that if anyone callsdagbag.sync_to_db separately then also need to manage retrying itby themselves. This caused failures in CI for MySQL.resolves https://github.com/apache/airflow/issues/11543",0
Docstring fix for S3DeleteBucketOperator (#12049),4
Vault with optional Variables or Connections (#11736),5
Remove unused JavaScript function (#12052),1
Testing XCom endpoint joins to avoid regression (#11859),3
"Uses DOCKER_TAG when building image in DockerHub (#12050)DockerHub uses `hooks/build` to build the image and it passesDOCKER_TAG variable when the script is called.This PR makes the DOCKER_TAG to provide the default valuei for tagthat is calculated from sources (taking the default branch andpython version). Since it is only set in the DockerHub build, itshould be safe.Fixes #11937",0
Add job name and progress logs to Cloud Storage Transfer Hook (#12014),1
"Perform ""mini scheduling run"" after task has finished (#11589)In order to further reduce intra-dag task scheduling lag we add anoptimization: when a task has just finished executing (success orfailure) we can look at the downstream tasks of just that task, and thenmake scheduling decisions for those tasks there -- we've already got thedag loaded, and we know they are likely actionable as we just finished.We should set tasks to scheduled if we can (but no further, i.e. not toqueued, as the scheduler has to make that decision with info about thePool usage etc.).Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",5
Update INTHEWILD.md (#12060),5
Add Nav to INTHEWILD (#12059),1
Log BigQuery job id in insert method of BigQueryHook (#12056),1
added american-family-insurance to INTHEWILD (#12062),1
Delete an environment-dependent value from CLI documentation (#12055),2
Adds a forgotten word in a README.md (#12066),2
"Replace deprecated PythonOperator module with the new one (#12064)Without this change, users will get a warning when using example dags too",2
Add Kubernetes cleanup-pods CLI command for Helm Chart (#11802)closes: https://github.com/apache/airflow/issues/11146,0
"If we build a new image, we should run more than basic checks (#12070)This lead to bases such as in #11699 where despite there being changes,and an image being build, the pre-commit tests were not being run.",1
Use PyUpgrade to use Python 3.6 features (#11447)Use features like `f-strings` instead of format across the code-base.More details: https://github.com/asottile/pyupgrade,1
Fixes import of BaseOperator in dinging (#12063)The import was wrongly importing BaseOperator from bash_operator.Now it correctly imports it from models.,2
Enable Black - Python Auto Formmatter (#9550),0
Add Badges for Black Code-Style & PyPI Downloads (#12076)Airflow gets around 513k downloads per month on PyPI and we just merged PR to enable black across our codebase,0
Document Pagerduty provider in installation.rst (#12054),1
Remove the ability to import operators and sensors from plugins (#12072)We have deprecated this in #12069 (for inclusion in 1.10.13) and thedocs http://airflow.apache.org/docs/stable/howto/custom-operator.htmlalready show how to do this without a plugin.Closes #9498,2
Fix typo in docker-context-files/README.md (#12078)`par` -> `part`,2
Fixes problem with building a PROD image (#12080)The change #12050 that aimed at automation of Docker imagesbuilding in DockerHub had an undesired effect of overriding theproduction image tag with the CI one.This is fixed by this PR.,0
Add template fields renderers to Biguery and Dataproc operators (#12067),1
"Avoid unnecessary IF checks when generate Duration & Landing Time views (#12075)The original code is looping in a space which could be smaller, meanwhile IF checks is not necessary.This change aims for:- bring MINOR performance improvement- cleaner code",4
"Use sys.exit() instead of exit() (#12084)The `exit` and `quit` functions are actually `site.Quitter` objects and are loaded, at interpreter start up, from site.py. However, if the interpreter is started with the `-S` flag, or a custom site.py is used then exit and quit may not be present. It is recommended to use `sys.exit()` which is built into the interpreter and is guaranteed to be present.",5
Fix proper SHA in check preventing accidentally merging PR (#12083)The SHA in check was not working for PRs from forks.,1
"Remove explicit casting to List when sorted() is applied (#12085)sorted() returns a sorted list.sorted(list(A)) is equivalent to sorted(A) no matter A is Tuple, List, or SetRef: https://docs.python.org/3/library/functions.html#sorted",2
Simplify string expressions (#12093),5
Add DataflowStartSQLQuery operator (#8553),1
Adding MySql howto-documentation and example DAG (#12077)closes https://github.com/apache/airflow/issues/11918,0
"Convert OpenAPI client generation tests to use selective checks (#12092)This test was bundled in with the existing needs-api tests, but thenperformed it's _own_ checks on if it should run. This changes that tohave selective_ci_checks.sh do this check.Additionally CI_SOURCE_REPO was often wrong -- at least for me as Idon't open PRs from ashb/airflow, and this lead to a confusing message:> https://github.com/ashb/airflow.git Branch my_branch does not existBut all we were using this for was to find the ""parent"" commit, butthere is any easier way we can do that: HEAD^1 with a fetch depth of 2to the checkout option.So I've removed calculating that and where it is used.If we need to bring it back we should use the output from the`potiuk/get-workflow-origin` action -- that gets the correct value",1
Correct failure message in sql_sensor.py. (#12057)Co-authored-by: Fai <faihegberg@gmail.com>,0
Add server side cursor support for postgres to GCS operator (#11793),1
Format all files (without excepions) by black (#12091),2
Improve reading SSL credentials file in GRPC Hook (#12094),1
Changed tutorial file to reflect name change to TaskFlow API (#12099)Changed the tutorial for decorated flows in the example dags directory to reflect the name change to TaskFlow API,4
"Update install_mysql.sh (#12101)After Debian 9 and according to the manual https://manpages.debian.org/stretch/apt/apt-key.8.en.html, after Debian 9  instead of using ""apt-key add"" a keyring should be placed directly in the /etc/apt/trusted.gpg.d/ directory with a descriptive name and either ""gpg"" or ""asc"" as file extension. Also added better redirection on the apt-key list command.",1
Small fixes in Google Cloud Secrets Manager guide (#12105),0
"Fix doc for ""hiding sensitive variables"" in Variable View (#12113)",2
Add Kubernetes files to selective checks (#12114)* Add Kubernetes files to selective checksThere are multiple kubernetes-related files that requirerunning the k8s integration tests. This PR adds those to therun_selective_tests* Update scripts/ci/selective_ci_checks.shCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update scripts/ci/selective_ci_checks.shCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update scripts/ci/selective_ci_checks.shCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update scripts/ci/selective_ci_checks.shCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update scripts/ci/selective_ci_checks.shCo-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
"Add SIGUSR2 handler to Scheduler to dump executor state (#12107)This provides a means to get a snapshot of the in-memory state of statea running scheduler, without having to turn on debug logging",2
Fix link to StackdriverTaskHandler reference docs (#12106),2
Refactor Elasticsearch provider to support 1.10.x (#11509),1
Remove redundant parenthesis (#12118),4
Add ability to specify pod_template_file in executor_config (#11784)* Add pod_template_override to executor_configUsers will be able to override the base pod_template_file on a per-taskbasis.* change docstring* fix doc* fix static checks* add description,1
Randomize pod name (#12117),5
Upgrade pygrep-hooks to 1.7.0 (#12124)https://github.com/pre-commit/pygrep-hooks/releases/tag/v1.7.0,1
Remove commented line (#12125)This line does not add any meaning and I think was left over in the PR,1
Clean-up Sphinx config (#12109),5
Replace conditional with builtin max (#12122)It is unnecessary to use an if statement to check the maximum of two values and then assign the value to a name. Just using the max built-in is straightforward and more readable.,1
Update INTHEWILD.md (#12129)Added Everis to INTHEWILD.md,1
airflow info fixed for python 3.8+ (#12132),0
Fix grammar in FAQ.rst (#12127),0
Simplify string expressions (#12123)Black has trouble formatting strings that are too long and produces unusual sring expressions.,5
"Remove the ability to add hooks to airflow.hooks namespace (#12108)Hooks do not need to live under ""airflow.hooks"" namespace for them towork -- so remove the ability to create them under there in plugins.Using them as normal python imports is good enough!We still allow them to be ""registered"" to support dynamically populatingthe connections list in the UI (which won't be done for 2.0)Closes #9507",1
Dataflow - add waiting for successful job cancel (#11501)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>,1
Unpin 'markdown' library (#12134),5
Work properly if some variables are not defined (#12135)Those variables are defined in GitHub environment so when theywere recently addded it was not obvious that they will fail whenrunning kubernetes tests locally.This PR fixes that.,0
Update to new helm stable repo (#12137)Switch out deprecated helm repo for new stable repo.- https://www.cncf.io/blog/2020/11/05/helm-chart-repository-deprecation-update/- https://helm.sh/docs/faq/#i-am-getting-a-warning-about-unable-to-get-an-update-from-the-stable-chart-repository,5
Add missing packages descriptions in docs/installation.rst (#12141)Co-authored-by: Szymon Nieradka <szymon[]nieradka.net>,2
"Retry Publishing Task to Celery Broker (#12140)If for some reason (network blip, redis is down) if AirflowTaskTimeout is raised (controlled by `[celery] operation_timeout`) when publishing Task to the broker, Airflow will be default atleast retry 3 times to publish the messages controlled by `[celery] task_publish_max_retries`.",1
"In AWS Secrets backend, a lookup is optional (#12143)",5
Fix default values for Helm Chart (#12153),2
Fixes undefined variables (#12155)There are few more variables that (if not defined) preventfrom using the CI image directly without breeze or theCI scripts.With this change you can run:`docker run -it apache/airflow:master-python3.6-ci`and enter the image without errors.,0
Add missing description of `celery.task_timeout_error` metric (#12152),0
"Fixes ""--force-clean-images"" flag in Breeze (#12156)The flag was broken - bad cache parameter value was passed.This PR fixes it.",0
fix spacing between table and pagination (#12160),0
Make doc_md field nullable and raise json for non-existing dag in dag detail endpoint (#12142),2
Fix docs build on RTD (#12161),2
"Sync FAB Permissions for all base views (#12162)If a user has set `[webserver] update_fab_perms = False` and runs `airflow sync-perm` command to sync all permissions, they will receive the following error:```webserver_1  | [2020-11-07 15:13:07,431] {decorators.py:113} WARNING - Access is Denied for: can_index on: Airflow```and if the user was created before and some perms were sync'd a user won't be able to find Security Menu & Configurations View",5
"Fix Celery Tests (#12166)Celery tests on Master are failing and then timing out on Postgres and MySQL.Stacktrace (link: https://github.com/apache/airflow/runs/1367123586#step:6:4860)```>           self.task_publish_retries.pop(key)E           KeyError: ('success', 'fake_simple_ti', datetime.datetime(2020, 11, 7, 8, 0, 13, 62424), 0)```This commit fixes the error introduced in https://github.com/apache/airflow/pull/12140",0
Move docs for max_db_retries option to core (#12167),5
Allow Connection Edit View to handle entries with NULL 'extra' (#12149),0
Proper title for XCom List View page (#12169),5
Added 1.10.x section to Upgrading to 2.0 doc (#12173)Added a section to describe the agreed supported policy forAirflow 1.10.x after the release of 2.0.,1
Fix ERROR - Object of type 'bytes' is not JSON serializable when using store_to_xcom_key parameter (#12172),2
azure key vault optional lookup (#12174),5
"Uses always the same Python base image as used for CI image (#12177)When new Python version is released (bugfixes), we rebuild the CI imageand replace it with the new one, however releasing of the pythonimage and CI image is often hours or even days apart (we onlyrelease the CI image when tests pass in master with the new pythonimage). We already use a better approach for Github - we simplypush the new python image to our registry together with the CIimage and the CI jobs are always pulling them from our registryknowing that the two - python and CI image are in sync.This PR introduces the same approach. We not only push CI imagebut also the corresponding Python image to our registry. This hasno ill effect - DockerHub handles it automatically and reusesthe layers of the image directly from the Python one so it ismerely a label that is stored in our registry that points to theexact Python image that was used by the last pushed CI image.",1
Remove redundant asserts in tests/www/test_views.py (#12176)Methods 'check_content_not_in_response'/'check_content_in_response' alreadytake care of status code check (by default asserts against 200)So no need to check status code explicitly if either of these two methods areused to check the response.,1
"Adds extra check while the selective checks are run (#12178)The selective checks are run in ""workflow_run"" becausethey need to be able to set label and make comments, howeverstatus of those checks are not displayed in GitHub and incases of small PRs the ""merge"" button might be green beforethe status complete.This PR adds additional check that is always completed afterthe ""worfklow_run"" finishes it's job. This will preventaccidental merges before the check completes.",7
Add authentication to AWS with Google credentials (#12079)* Add authentication to AWS with Google credentials* fixup! Add authentication to AWS with Google credentials* fixup! fixup! Add authentication to AWS with Google credentials* fixup! fixup! fixup! Add authentication to AWS with Google credentials,1
"Fix broken 'Blocked Highlight' feature in UI (#12183)* Fix broken 'Blocked Highlight' feature in UI main page* based on the latest UI design, changed how the highlighting is done.",4
Move metrics configuration to new section - metrics (#12165)* Move metrics configuration to new section* fixup! Move metrics configuration to new section* fixup! fixup! Move metrics configuration to new section* Apply suggestions from code reviewCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>* fixup! Apply suggestions from code reviewCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,0
Fixed path of the test_core.py file in docs (#12191)The test_core.py has been used as example in Breeze and it'slocation changed to tests/core folder. This PR fixes referencesto the changed location.,4
Add how-to Guide for Databricks operators (#12175),1
"Call scheduler ""book-keeping"" operations less frequently. (#12139)This change makes it so that certain operations in the scheduler arecalled on a regular interval, instead of only once at start up, or everytime around the loop:- adopt_or_reset_orphaned_tasks (detecting SchedulerJobs that died) was  previously only called on start up.- _clean_tis_without_dagrun was previously called every time around the  scheduling loop, but this isn't so needed to be done every time as  this is a relatively rare cleanup operation- _emit_pool_metrics doesn't need to be called _every_ time around the  loop, once every 5 seconds is enough.This uses the built in [""sched"" module][sched] to handle the ""timers"".[sched]: https://docs.python.org/3/library/sched.html",2
Provider packages are installed by default in production image (#12154)This is a fix to a problem introduced in #10806. The changeturned provider packages into namespace packages - which madethem ignored by find_packages function from setup tools - thusprodiuction image build automatically and used by Kubernetestests did not have the provider packages installed.This PR fixes it and adds future protection during CI tests ofproduction image to make sure that provider packages areactually installed.Fixes #12150,0
"Moves provider packages scripts to dev (#12082)The change #10806 made airflow works with implicit packageswhen ""airflow"" got imported. This is a good change, howeverit has some unforeseen consequences. The 'provider_packages'script copy all the providers code for backports in orderto refactor them to the empty ""airflow"" directory inprovider_packages folder. The #10806 change turned thatempty folder in 'airflow' package because it was in thesame directory as the provider_packages scripts.Moving the scripts to dev solves this problem.",0
KubernetesPodOperator: use randomized name to get the failure status (#12171)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
Provider's readmes generated for elasticsearch and google packages (#12194),1
Don't include provider datafiles in the apache-airflow sdist (#12196)We shouldn't include these in the main sdist now we've split providersto separate distributions.,1
Filter dags by owner (#11121)* Filter dags by owner* Seperate links for multiple owners* Minor style changeCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>,4
"Remove BaseDag and BaseDagBag classes (#12195)Since #7694 these haven't really be needed, but we hadn't removed themyet.No UPDATING.md note for this as I think it's extremely unlikely anyonewas using this directly -- it's very much an implementation detailrelating to DAG/SimpleDag.",2
Include data files in (backport) provider packages (#12200)This isn't the most stable long-term solution but this works for nowwhere we've only got 4 dists the need this,1
Workaround missing git commit in providers's check in CI (#12205)Temporary fix to unblock master PRs.There is a problem with comparing/fetching commits during testsand it should be investigated properly - this is temporary fixto workaround it.,1
Update provider READMEs for up-coming 1.0.0beta1 releases (#12206),1
Render k8s yaml for tasks via the Airflow UI (#11815)This function allows users of the k8s executor to get previewsof their tasks via the Airflow UI before they launchCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Fixes timeout in helm chart tests (#12209),3
Remove popd which is a remnant from past (#12211),4
"Fix permissions of mounted /tmp directory for Breeze (#12157)The ""tmp"" directory is mounted from the host (from tmp folderin the source airflow directory). This is needed to get someof our docker-in-docker tools (such as gcloud/aws/java) andget them working on demand. Thanks to that we do not haveto increase the size of CI image unnecessarily.Those tools were introduced and made to work in #9376However this causes some of the standard tools (such as apt-get)to not work inside the container unless the mounted /tmpfolder has write permission for groups/other.This PR fixes it.",0
"Extend the same keyword args callable support in PythonOperator to some other sensors/operators (#11922)This PR Standardises the callable signatures in PythonOperator, PythonSensor, ExternalTaskSensor, SimpleHttpOperator and HttpSensor.The callable facilities in PythonOperator have been refactored into airflow.utils.helper.make_kwargs_callable. And it's used in those other places to make them work the same way.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Make warnings more visible (#12204)This PR proposes to use custom showwarning function thatprovides users with better information about warnings usingrich library to highlight the warning.,2
"Point at pypi project pages for cross-dependency of provider packages (#12212)We mistakenly said ""backport"" which was clearly wrong, and also pointedat the source code for them, but the pypi project page is moreappropriate.",5
Remove redundant parenthesis (#12213),4
"Adds automated installation of dependent packages (#11526)When extras are specifying when airflow is installed, this one triggersinstallation of dependent packages. Each extra has a set of providerpackages that are needed by the extra and they will be installedautomatically if this extra is specified.For now we do not add any version specificatiion, until we agree theprocess in #11425 and then we should be able to implement anautomated way of getting information about cross-packageversion dependencies.Fixes: #11464",0
Release 2.0.0beta1 (#12215),5
Fix typo in docstrings (#12220)`meatadata` -> `metadata`,5
Beautify Output of setup-installation pre-commit (#12218),1
Add Compute Engine SSH hook (#9879),1
Add Markdown linting to pre-commit (#11465),1
Enable Markdownlint rule - MD032/blanks-around-lists (#12224),0
Fix typo (#12222),2
Simplify string expressions & Use f-string (#12216)* Simplify string expressions & Use f-stringThis is a follow-up clean-up work for the minor issues caused in the process of introducing Black* Fixup,0
[#12012]: Update INTHEWILD.md (#12235),5
CSS Changes to adjust content width as per screen size and responsive table with multiline td. (#12227),2
Enable Markdownlint rule - MD022/blanks-around-headings (#12225)https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md#md022---headings-should-be-surrounded-by-blank-lines,2
Add docs about security on GCP (#12187),2
Fix spelling in Python files (#12230),2
Enable markdownlint rule - MD031/blanks-around-fences (#12238),0
Fixes automated provider installation with extras (#12233)The #11526 was badly rebased just before beta1 relase and fewlines installing the providers were lost.This PR restores those lines.Fixes: #12231,0
Add back missing api_connextion/__init__.py file (#12240)A bad rebase in #12082 deleted this file by mistake.This missing file was also the cause of needing the documentationto exclude these filesFixes #12239,2
"Sign release files with an apache.org key by default (#12241)If you have more than a single private key in your GPG trust store, gpgwill use the first one, which for me is not right.This changes the script to by default use any key with `apache.org` inthe name. This is a patch I've been carrying locally for about 8releases now :D",1
"Remove Unnecessary comprehension (#12221)The inbuilt functions all() and any() in python also supportshort-circuiting (evaluation stops as soon as the overall return valueof the function is known), but this behavior is lost if you usecomprehension. This affects performance.",1
Update versions in UPDATING.md for 2.0.0b1 release (#12244),5
Added missing sendgrid readme (#12245)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,1
Release 2.0.0b2 (#12243),5
Fix spelling (#12250),0
"Fixes continuous image rebuilding with Breeze (#12256)There was a problem that even if we pulled the right imagefrom the Airflow repository, we have not tagged it properly.Also added protection for people who have not yet at all pulledthe Python image from airflow, to force pull for the first time.",1
Adds provider package documentation in installation.rst (#12203)Addresses initial version of #11880,5
Added Farfetch to the list of companies using Airflow (#12260),1
Fix spelling (#12253),0
Remove providers imports from core examples (#12252)Core example DAGs should not depend on any non-core dependencylike providers packages.closes: #12247Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,1
"Fix: Conditionally update button URL only when it is present (#12268)Resolves #12254A bug introduced in #11815. The function that updates the button URLs was failing when trying to update the ""K8s Pod Spec"" which is conditionally displayed (if k8s_or_k8scelery_executor). This fix adds a check to confirm the button exists before attempting.",5
Fix spelling (#12266),0
Fix pause/unpause toggle to display failed state when unsuccessful (#12267),0
"Docker context files should be available earlier (#12219)If you want to override constraints with local version,the docker-context-files should be earlier in the Dockerfile",2
Unify user session lifetime configuration (#11970)* Unify user session lifetime configuration* align with new linting rules* exit app when removed args are provided in conf* add one more test* extract stopping gunicorn to method* add docstring to stop_webserver method* use lazy formatting* exit webserver when removed options are provided* align with markdown lint* Move unify user session lifetime configuration section to master* add new line* remove quotes,4
"Don't treat warning message as rich formatting codes. (#12283)Before this commit:```  ...airflow/configuration.py:328 DeprecationWarning: The remote_logging option in  has been moved to the remote_logging option in  - the old setting has been used, but please update your config.```After this commit:```  ...airflow/configuration.py:328 DeprecationWarning: The remote_logging option in [core] has been moved to the remote_logging option in [logging] - the old setting has been used, but please update your config.```As this file is _always_ imported by anything in airflow, but warnings are quite rare I havealso delayed the import.",2
"Providers in extras are properly configured and verified (#12265)* Providers in extras are properly configured and verifiedThis fixes #12255 - where we published beta2 release with someextras pulling non-existing providers.The exact list of providers that had problems:Wrongly named extras/providers:* apache.presto: it was badly named -> renamed to 'presto'* spark (badly pointing to spark instead of apache.spark)* yandexcloud (the name remains there but we've also added 'yandex' extra to correspond 1-1 with 'yandex' providerExtras that were wrongly marked as having providers, where they hadnone:* dask* rabbitmq* sentry* statsd* tableau* virtualenv* Update scripts/ci/pre_commit/pre_commit_check_extras_have_providers.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update scripts/ci/pre_commit/pre_commit_check_extras_have_providers.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
"Added k9s as integrated tool to help with kubernetes testing (#12163)The K9s is fantastic tool that helps to debug a running k8sinstance. It is terminal-based windowed CLI that makes youseveral times more productive comparing to using kubectlcommands. We've integrated k9s (it is run as a docker containerand downloaded on demand). We've also separated out KUBECONFIGof the integrated kind cluster so that it does not mess withkubernetes configuration you might already have.Also - together with that the ""surrounding"" of the kubernetestests were simplified and improved so that the k9s integrationcan be utilized well. Instead of kubectl port forwarding (whichcaused multitude of problems) we are now utilizing kind'sportMapping feature + custom NodePort resource that mapsport 8080 to 30007 NodePort which in turn maps it to 8080port of the Webserver. This way we do not have to establishan external kubectl port forward which is prone to error andmanagement - everything is brought up when Airflow getsdeployed to the Kind Cluster and shuts down when the Kindcluster is stopped.Yet another problem fixed was killing of postgres by one of thekubernetes tests ('test_integration_run_dag_with_scheduler_failure').Instead of just killing the scheduler it killed all pods - includingthe Postgres one (it was named 'airflow-postgres.*'). That causedvarious problems, as the database could be left in a strange state.I changed the tests to do what it claimed was doing - so killing only thescheduler during the test. This seemed to improve the stabilityof tests immensely in my local setup.",1
"Don't wrap warrning messages when stderr is not a TTY (#12285)If stderr is not a TTY, rich was hard-wrapping warning messages at 80characters:```/home/ash/code/airflow/airflow/airflow/configuration.py:328 DeprecationWarning:The remote_logging option in [core] has been moved to the remote_logging optionin [logging] - the old setting has been used, but please update your config.```After```/home/ash/code/airflow/airflow/airflow/configuration.py:328 DeprecationWarning: The remote_logging option in [core] has been moved to the remote_logging option in [logging] - the old setting has been used, but please update your config.````rich.print()` doesn't take a `soft_wrap` option, so I had to create a`rich.console.Console` object -- and it seems best to cache those.",1
Fix indentation for affinities in helm chart (#12288)This PR fixes a bug in the helm chart where custom affinities inthe pod_template_file cause the yaml to fail due to invalid spacing,0
Fix test - TestImpersonation (#12274),3
Remove unneeded parentheses from Python files (#12270),2
Add an alias to improve git shortlog output (#12286)* Add an alias to improve git shortlog output* Add another,1
Detect partial examples DAGs for Google (#12277),2
"Make dag_id, task_id, and execution_date nullable in event log schema (#12287)",2
Handle naive datetimes in REST APIi (#12248),5
Use default view in TriggerDagRunLink (#11778),2
Update INTHEWILD.md (#12293),5
Add session_parameters option to snowflake_hook (#12071),1
Add reference for SubDagOperator (#12297)It would be better for users to have a link when they see SubDagOperator where they can read about it instead of just Class names,2
Replace remaining decorated DAGs reference (#12299)`decorated DAGs` -> `DAGs with Task Flow API`,2
Wait option for dagrun operator (#12126)* Add wait_for_completion option to dag run operator.* Add wait_for_completion option to dag run operator.* Change code format to pass sanity check.* Simplify the logic to check dag run state.* Move sleep in the beginning of loop and update pydoc.* Change elif to if on checking allowed_statesCo-authored-by: Kaz Ukigai <kukigai@apple.com>,1
"Python base image is shared between CI and PROD image (#12280)When you are building CI images locally you use the CIbase images from apache:airflow/python* now to maintainconsistency and avoid often rebuilds. But when you buildprod images, you would accidentaly override it with thepython base image available in python repo which might bedifferent (newer and not yet tested in CI). This PRchanges it to use the same base image which is nowtagged in Apache Airflow's dockerhub repository.",2
"Remove deprecated BashTaskRunner (#12295)This commit:- Remove support for BashTaskRunner, this task_runner was deprecated fromAirflow 1.10.3 (https://github.com/apache/airflow/blob/1.10.3/UPDATING.md#rename-of-bashtaskrunner-to-standardtaskrunner)- Support deprecated `hostname_callable` & `email_backedn` until 2.1 since it has not been deprecated in any relased Airflow versions",1
Remove deprecated Elasticsearch Configs (#12296)Since Airflow 1.10.4 we have removed `elasticsearch_` prefix from allconfig items under `[elasticsearch]` section. It is time we remove themfrom 2.0.https://github.com/apache/airflow/blob/1.10.4/UPDATING.md#changes-in-writing-logs-to-elasticsearch,5
Fix - TestSchedulerJobQueriesCount::test_process_dags_queries_count (#12273),3
"Fix prod image build (#12314)Running `setup.py` was failing with a KeyError: yandex, meaning #12265didn't quite fix it.It is unclear to at this point why CI passed on that PR.",4
Deploy was not working from Breeze (#12319)The get_cluster_name was called twice resulting in redonlyerror after rebasing/fixing CI failure in #12163.This PR is fxing it.,0
"Get all ""tags"" parameters not just one (#12324)",2
"Update automated PR labels (#12326)Based on today's Issue triage community call, I have updated the following labels:`k8s` -> `area:kubernetes``area:dev` -> `area:dev-tools``area:docs` -> `kind:documentation`https://docs.google.com/document/d/1Fx46SoOnNLiqZKtrC-tOHj3zFlZfQwWuR2LRFXJnWqw/edit?usp=sharing",2
Fixes the sending of an empty list to BigQuery `list_rows` (#12307)* Fixes an issue that was causing an empty list being sent to the BigQuery client `list_rows` method resulting in no schema being returned.* Added a test to check that providing an empty list for `selected_fields` results in `list_rows` being called wth `None`.,1
"Mount airflow.cfg to pod_template_file (#12311)* Mount airflow.cfg to pod_template_filek8sexecutor workers were launching without an airflow.cfg,this was preventing logs from being sent to distributed logging systems.* consistent naming",5
Fix and Unquarantine test_change_state_for_tis_without_dagrun (#12323)The test was simply wrong and failed since the new logic was added inhttps://github.com/apache/airflow/commit/c9a97baa86762b9ba37ef71432573b7949e47e2b,1
Add install/uninstall api to databricks hook (#12316)- adding install Databricks API to databricks hook(api/2.0/libraries/install)- adding uninstall Databricks API to databricks hook (2.0/libraries/uninstall),1
Docs installation improvements (#12304)* Improvements for installation docs,2
"Add extra error handling to S3 remote logging (#9908)If you have configured S3 logs, but there is a problem then this is neversurfaced to the UI (nor the webserver logs) making this very hard todebug.This PR exposes some of these errors to the user.Co-authored-by: Joao Ponte <jpe@plista.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
"For v1-10-test PRs and pushes, use target branch scripts for images (#12339)Previously, always master scripts were used to build imagesfor workflow_run, because workflow_run always runs from masterbranch. However that causes some surprising effects becuase thesripts from master had to support both master and 1.10.This change utilises a new feature in the ""get-workflow-origin""action - to get the target branch of PR and uses ci scripts from thattarget branch.This is perfectly secure, because both v1-10-test, v1-10-stableand future 2-0 branches can only be updated by committers,either by direct push or by merge.",7
Create DAG-level cluster policy (#12184)This commit adds new concept of dag_policy which is checkedonce for every DAG when creating DagBag. It also improvesdocumentation around cluster policies.closes: #12179Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,2
Enable protection for v1-10-stable branch (#12343),0
Update deprecated Apache Pinot Broker API (#12333)* Update depricated Apache Pinot Broker API* Fix typo on Pinot Hook,1
Improve presentation of DAG Docs (#12330)* Improve presentation of DAG docs* syntax fix,0
Fix/Enhancement: Disable forms and communicate to user when no DAG Runs (#12320)* Disable forms and communicate to user when no DAG runs yet* Refactor method name to not use negation in name* lint fix,0
Bugfix: REST API Variables update endpoint returns 204 No Content (#12321),5
Fix helm unit test for pod_template_file (#12345)Fixes bug in unittest that is causing master to fail.,0
Update & Fix 'Rotate Fernet Key' Doc (#12347)- Update the sample command: to be consistent with 2.0 CLI change- Fix typo,2
"Use the backend-configured model (#12336)Rather than import the backend Task model directly, use the class that the backend actually uses. This could have been customised, and there is no reason not to use this reference.",1
"K8s yaml templates not rendered by k8sexecutor (#12303)* K8s yaml templates not rendered by k8sexecutorThere is a bug in the yaml template rendering caused by the logic thatyaml templates are only generated when the current executor is thek8sexecutor. This is a problem as the templates are generated by thetask pod, which is itself running a LocalExecutor. Also generates a""base"" template if this taskInstance has not run yet.* fix tests* fix taskinstance test* fix taskinstance* fix pod generator tests* fix podgen* Update tests/kubernetes/test_pod_generator.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* @ashb commentCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",3
"Simplifies check whether the CI image should be rebuilt (#12181)Rather than counting changed layers in the image (which wasenigmatic, difficult and prone to some magic number) we rely nowon random file generated while building the image.We are using the docker image caching mechanism here. The randomfile will be regenerated only when the previous layer (which isabout installling Airflow dependencies for the first time) getsrebuild. And for us this is the indication, that the buildingthe image will take quite some time. This layer should berelatively static - even if setup.py changes the CI image isdesigned in the way that the first time installation of Airflowdependencies is not invalidated.This should lead to faster and less frequent rebuild for peopleusing Breeze and static checks.",1
Fix Sample CLI commands for upgrading to 2.0 (#12349),0
Add metric for scheduling delay between first run task & expected start time (#9544)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,1
Restructure documentation for releasing Airflow/Providers (#12350),1
Fix Static-check failure (#12356),0
"Make nav fully accessible y keyboard, fix mobile nav menus (#12351)",0
Manage Flask AppBuilder Tables using Alembic Migrations (#12352)closes https://github.com/apache/airflow/issues/9155The Migration is idempotent and allows both upgrade and downgrade.It also takes care of https://github.com/dpgaspar/Flask-AppBuilder/pull/1368i.e. increasing the length of ab_view_menu.name column from 100 to 250,1
Fix Description of Provider Docs (#12361)Apache Druid had description for Cassandra. Dingding had it for Datadog. And typo in Vertica,2
"Show all Providers in Docs (#12363)Before in https://airflow.readthedocs.io/en/latest/provider-packages-ref.html, the apache.spark and other nested providers were ignored```pythonIn [1]: from glob import glob   ...:In [2]: sorted(glob(f""airflow/providers/**/provider.yaml""))Out[2]:['airflow/providers/amazon/provider.yaml', 'airflow/providers/celery/provider.yaml', 'airflow/providers/cloudant/provider.yaml', 'airflow/providers/databricks/provider.yaml', 'airflow/providers/datadog/provider.yaml', 'airflow/providers/dingding/provider.yaml', 'airflow/providers/discord/provider.yaml', 'airflow/providers/docker/provider.yaml', 'airflow/providers/elasticsearch/provider.yaml', 'airflow/providers/exasol/provider.yaml', 'airflow/providers/facebook/provider.yaml', 'airflow/providers/ftp/provider.yaml', 'airflow/providers/google/provider.yaml', 'airflow/providers/grpc/provider.yaml', 'airflow/providers/hashicorp/provider.yaml', 'airflow/providers/http/provider.yaml', 'airflow/providers/imap/provider.yaml', 'airflow/providers/jdbc/provider.yaml', 'airflow/providers/jenkins/provider.yaml', 'airflow/providers/jira/provider.yaml', 'airflow/providers/mongo/provider.yaml', 'airflow/providers/mysql/provider.yaml', 'airflow/providers/odbc/provider.yaml', 'airflow/providers/openfaas/provider.yaml', 'airflow/providers/opsgenie/provider.yaml', 'airflow/providers/oracle/provider.yaml', 'airflow/providers/pagerduty/provider.yaml', 'airflow/providers/papermill/provider.yaml', 'airflow/providers/plexus/provider.yaml', 'airflow/providers/postgres/provider.yaml', 'airflow/providers/presto/provider.yaml', 'airflow/providers/qubole/provider.yaml', 'airflow/providers/redis/provider.yaml', 'airflow/providers/salesforce/provider.yaml', 'airflow/providers/samba/provider.yaml', 'airflow/providers/segment/provider.yaml', 'airflow/providers/sendgrid/provider.yaml', 'airflow/providers/sftp/provider.yaml', 'airflow/providers/singularity/provider.yaml', 'airflow/providers/slack/provider.yaml', 'airflow/providers/snowflake/provider.yaml', 'airflow/providers/sqlite/provider.yaml', 'airflow/providers/ssh/provider.yaml', 'airflow/providers/vertica/provider.yaml', 'airflow/providers/yandex/provider.yaml', 'airflow/providers/zendesk/provider.yaml']```Now:```pythonIn [4]: sorted(glob(f""airflow/providers/**/provider.yaml"", recursive=True))Out[4]:['airflow/providers/amazon/provider.yaml', 'airflow/providers/apache/cassandra/provider.yaml', 'airflow/providers/apache/druid/provider.yaml', 'airflow/providers/apache/hdfs/provider.yaml', 'airflow/providers/apache/hive/provider.yaml', 'airflow/providers/apache/kylin/provider.yaml', 'airflow/providers/apache/livy/provider.yaml', 'airflow/providers/apache/pig/provider.yaml', 'airflow/providers/apache/pinot/provider.yaml', 'airflow/providers/apache/spark/provider.yaml', 'airflow/providers/apache/sqoop/provider.yaml', 'airflow/providers/celery/provider.yaml', 'airflow/providers/cloudant/provider.yaml', 'airflow/providers/cncf/kubernetes/provider.yaml', 'airflow/providers/databricks/provider.yaml', 'airflow/providers/datadog/provider.yaml', 'airflow/providers/dingding/provider.yaml', 'airflow/providers/discord/provider.yaml', 'airflow/providers/docker/provider.yaml', 'airflow/providers/elasticsearch/provider.yaml', 'airflow/providers/exasol/provider.yaml', 'airflow/providers/facebook/provider.yaml', 'airflow/providers/ftp/provider.yaml', 'airflow/providers/google/provider.yaml', 'airflow/providers/grpc/provider.yaml', 'airflow/providers/hashicorp/provider.yaml', 'airflow/providers/http/provider.yaml', 'airflow/providers/imap/provider.yaml', 'airflow/providers/jdbc/provider.yaml', 'airflow/providers/jenkins/provider.yaml', 'airflow/providers/jira/provider.yaml', 'airflow/providers/microsoft/azure/provider.yaml', 'airflow/providers/microsoft/mssql/provider.yaml', 'airflow/providers/microsoft/winrm/provider.yaml', 'airflow/providers/mongo/provider.yaml', 'airflow/providers/mysql/provider.yaml', 'airflow/providers/odbc/provider.yaml', 'airflow/providers/openfaas/provider.yaml', 'airflow/providers/opsgenie/provider.yaml', 'airflow/providers/oracle/provider.yaml', 'airflow/providers/pagerduty/provider.yaml', 'airflow/providers/papermill/provider.yaml', 'airflow/providers/plexus/provider.yaml', 'airflow/providers/postgres/provider.yaml', 'airflow/providers/presto/provider.yaml', 'airflow/providers/qubole/provider.yaml', 'airflow/providers/redis/provider.yaml', 'airflow/providers/salesforce/provider.yaml', 'airflow/providers/samba/provider.yaml', 'airflow/providers/segment/provider.yaml', 'airflow/providers/sendgrid/provider.yaml', 'airflow/providers/sftp/provider.yaml', 'airflow/providers/singularity/provider.yaml', 'airflow/providers/slack/provider.yaml', 'airflow/providers/snowflake/provider.yaml', 'airflow/providers/sqlite/provider.yaml', 'airflow/providers/ssh/provider.yaml', 'airflow/providers/vertica/provider.yaml', 'airflow/providers/yandex/provider.yaml', 'airflow/providers/zendesk/provider.yaml']```",1
Fix case for PyPI in docs (#12364)`PyPi` -> `PyPI`,2
"Refactor root logger handling in task run (#12342)- Use a context manager to encapsulate task logging setup and teardown- Create a copy, not a reference, of the handlers list- Remove logging.shutdown(), it simply should not be calledCloses #12090",2
Add success/failed sets to State class (#12359)Co-authored-by: Ace Haidrey <ahaidrey@pinterest.com>,1
"Reorder Database Migrations (#12362)Becase `2c6edca13270` (Resource based permissions) & `849da589634d` (Prefix DAG permissions)were run before `92c57b58940d_add_fab_tables.py` and `03afc6b6f902_increase_length_of_fab_ab_view_menu_.py`,the FAB tables were already created because those migrations imported `from airflow.www.app import create_app`which calls the following lines that creates tables:https://github.com/dpgaspar/Flask-AppBuilder/blob/0e7f62418be0aea056302769ebac7d9bfa2bdd4e/flask_appbuilder/security/sqla/manager.py#L86-L97Previously:```INFO  [alembic.runtime.migration] Running upgrade bef4f3d11e8b -> 98271e7606e2, Add scheduling_decision to DagRun and DAGINFO  [alembic.runtime.migration] Running upgrade 98271e7606e2 -> 52d53670a240, fix_mssql_exec_date_rendered_task_instance_fields_for_MSSQLINFO  [alembic.runtime.migration] Running upgrade 52d53670a240 -> 849da589634d, Prefix DAG permissions.[2020-11-14 02:35:43,055] {manager.py:727} WARNING - No user yet created, use flask fab command to do it.[2020-11-14 02:35:46,790] {migration.py:515} INFO - Running upgrade 849da589634d -> 364159666cbd, Add creating_job_id to DagRun table[2020-11-14 02:35:46,794] {migration.py:515} INFO - Running upgrade 364159666cbd -> 2c6edca13270, Resource based permissions.[2020-11-14 02:35:46,795] {app.py:87} INFO - User session lifetime is set to 43200 minutes.[2020-11-14 02:35:46,806] {manager.py:727} WARNING - No user yet created, use flask fab command to do it.[2020-11-14 02:35:48,221] {migration.py:515} INFO - Running upgrade 2c6edca13270 -> 45ba3f1493b9, add-k8s-yaml-to-rendered-templates[2020-11-14 02:35:48,226] {migration.py:515} INFO - Running upgrade 45ba3f1493b9 -> 92c57b58940d, Create FAB Tables[2020-11-14 02:35:48,227] {migration.py:515} INFO - Running upgrade 92c57b58940d -> 03afc6b6f902, Increase length of FAB ab_view_menu.name column```Now:```INFO  [alembic.runtime.migration] Running upgrade bef4f3d11e8b -> 98271e7606e2, Add scheduling_decision to DagRun and DAGINFO  [alembic.runtime.migration] Running upgrade 98271e7606e2 -> 52d53670a240, fix_mssql_exec_date_rendered_task_instance_fields_for_MSSQLINFO  [alembic.runtime.migration] Running upgrade 52d53670a240 -> 364159666cbd, Add creating_job_id to DagRun tableINFO  [alembic.runtime.migration] Running upgrade 364159666cbd -> 45ba3f1493b9, add-k8s-yaml-to-rendered-templatesINFO  [alembic.runtime.migration] Running upgrade 45ba3f1493b9 -> 92c57b58940d, Create FAB TablesINFO  [alembic.runtime.migration] Running upgrade 92c57b58940d -> 03afc6b6f902, Increase length of FAB ab_view_menu.name columnINFO  [alembic.runtime.migration] Running upgrade 03afc6b6f902 -> 849da589634d, Prefix DAG permissions.[2020-11-14 02:57:18,886] {manager.py:727} WARNING - No user yet created, use flask fab command to do it.[2020-11-14 02:57:22,380] {migration.py:515} INFO - Running upgrade 849da589634d -> 2c6edca13270, Resource based permissions.```",1
Visually separate pre-commits which require CI image (#12367),1
Remove redundant method in KubernetesExecutor (#12317)The _inject_secrets method was invoked but it performed no action soit seems that we can remove it.,4
"Fix full_pod_spec for k8spodoperator (#12354)* Fix full_pod_spec for k8spodoperatorFixes a bug where the `full_pod_spec` argument is never factoredinto the kubernetespodoperator. The new order of operations is asfollows:1. Check to see if there is a pod_template_file and if so create the initial pod, else start with empty pod2. if there is a full_pod_spec , reconcile the pod_template_file pod and the full_pod_spec pod3.  reconcile with any of the argument overrides* add tests",3
Remove unused import (#12371),2
Improvements for operators and hooks ref docs (#12366),2
"Removes the cidfile before generation (#12372)If we do not remove the cidfile, the subsequent write to it doesnot change the content. The errors have been masked by thestderr redirection, so the error was invisible.",0
Fix RTD docs build (#12373),2
Reject 'connections add' CLI request if URI provided is invalid (#12370)The validity is decided by availability of both 'scheme' and 'netloc' in the parse result,1
"Check for TaskGroup in _PythonDecoratedOperator (#12312)Crucial feature of functions decorated by @task is to be ableto invoke them multiple times in single DAG. To do this we aregenerating custom task_id for each invocation. However, this didn'twork with TaskGroup as the task_id is already altered by adding group_idprefix. This PR fixes it.closes: #12309Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",0
All kubernetes tests use the same host python version (#12374)For Kubernetes tests all tests can be executed in the same pythonversion - default one - no matter which PYTHON_MAJOR_MINOR isused. This is because we are testing Airflow which is deployedvia production image. Thanks to that we can fix the python versionto be default and avoid any python version problems (this isespecially important for cherry-picking to 1.10 where we havepython 2.7 and 3.5.,2
Add DataflowJobStatusSensor and support non-blocking execution of jobs (#11726),1
Proper exit status for failed CLI requests (#12375)Some CLI commands simply print messages when the requests fail.The issue is the exit code for these commands are 0 while it should be non-zero.Pursuing very detailed status code may not make sense here.But we can at least ensure we give non-zero status by using raise SystemExit().More proper exist status ensures people can better make use of the CLI.(A few minor string expression issues are fixed here as well).,0
Fixes pull error on building tagged image (#12378)When building tagged image on DockerHub the build has beenfailing as it was trying to pull cached version of prod imagebut the tagged image should be built from scratch so cache shouldbe disabled.Fixes #12263,0
Add extra info when starting extra actions in Breeze (#12377),5
"Add info log message about duration taken to load plugins (#12308)Loading plugins, particularly from setuptools entry points can be slow,and since by default this happens per-task, it can slow down taskexecution unexpectedly.By having this log message users can know the source of the delay(The change to test_standard_task_runner was to remove logging-configside effects from that test)",3
Remove unneeded parentheses after Black formatting (#12380),4
Fix spelling in AWS docs (#12379),2
Add provide_file_and_upload to GCSHook (#12310)This commit adds provide_file_and_upload context managerwhich works similar to provide_file. Users using it canavoid boilerplate code of creating temporary file and thenuploading its content to GCS.,2
Use different deserialization method in XCom init_on_load (#12327)The init_on_load method used deserialize_value method whichin case of custom XCom backends may perform requests to externalservices (for example downloading file from buckets).This is problematic because wherever we query XCom the resuest would besend (for example when listing XCom in webui). This PR proposes implementingorm_deserialize_value which allows overriding this behavior. By defaultwe use BaseXCom.deserialize_value.closes: #12315,1
Handle outdated webserver session timeout gracefully. (#12332),5
"Remove inapplicable configuration section [ldap] (since 2.0.0) (#12386)[ldap] section in airflow.cfg is not applicable anymore in 2.0 and master,because the LDAP authentication (for webserver and API) is handled by FAB,and the configuration for this is handled by webserver_config.py file.",2
"Support creation of configmaps & secrets and extra env & envFrom configuration in Helm Chart (#12164)* Enable provisionning of extra secrets and configmaps in helm chartAdded 2 new values:*  extraSecrets*  extraConfigMapsThose values enable the provisionning of ConfigMapsand secrets directly from the airflow chart.Those objects could be used for storing airflow variablesor (secret) connections info for instance(the plan is to add support for extraEnv and extraEnvFrom later).Docs and tests updated accordingly.* Add support for extra env and envFrom items in helm chartAdded 2 new values:*  extraEnv*  extraEnvFromThose values will be added to the defintion ofairflow containers. They are expected to be string(they can be templated).Those new values won't be supported by ""legacy"" kubernetesexecutor configuration (you must use the pod template).Therefore, the value 'env' is also deprecated as it's kindof a duplicate for extraEnv.Docs and tests updated accordingly.",5
Update wrong commit hash in backport provider changes (#12390)The commit was rebased so hash changed. This restores the right one.,4
Add Dataflow sensors - job metrics (#12039),5
Fix typo in check_environment.sh (#12395)`Databsae` -> `Database`,5
Update asf.yaml INFRA link (#12398)Replace old link with new link,2
Properly mocks UUID objects (#12381)The uuid methods return UUID objects not strings and there isa certain expectation about those - like hex property for exampleWe had a few cases where those uuid method's return values havebeen mocked with strings and it caused a problem - for examplewith migration to latest sentry library which used this veryhex method from the uuid4() return value.,1
Add missing pre-commit definition - provider-yamls (#12393),1
Update Ash's github handle (#12403),0
Fix broken master due to ash -> ashb change (#12408),4
Remove CodeQL from PRS. (#12406)As discussed in https://lists.apache.org/thread.html/r18cc605bbdb6695c1d31e0706f1b033401f6fa6a19cd0584d7be6cc9%40%3Cdev.airflow.apache.org%3Eremoving CodeQL from PRs.,5
Adds mechanism for provider package discovery. (#12383)This is a simple mechanism that will allow us to dynamicallydiscover and register all provider packages in the Airflow core.Closes: #11422,1
Switching to Ubuntu 20.04 as Github Actions runner. (#12404)Ubuntu 20.04 will soon become the default runner for GA.See: https://github.com/actions/virtual-environments/issues/1816This PR tests if this is working fine.,1
Update Kaxil's Github handle (#12409),0
Add stack overflow link to Github Issues (#12407),0
Simplify using XComArg in jinja template string (#12405)This changes XComArg string representation from 'task_instance.pull(...)'to '{{ task_instance.xcom_pull(...) }}' so users can use XComArgs withf-string (and other) in simpler way. Instead of doingf'echo {{{{ {op.output} }}}}' they can simply do f'echo {op.output}'.,1
Clearer information for webserver_config.py (#12412)So easier for doc readers who never used FAB-based RBAC UI.,1
The messages about remote image check are only shown with -v (#12402)The messages might be confusing and should only be shown whenverbose is turned on.,5
Change log level for User's session to DEBUG (#12414)This line was logged too often -- too chatty,2
Don't display when None (#12415),5
Make K8sPodOperator backwards compatible (#12384)* Make the KubernetesPodOperator backwards compatibleThis PR significantly reduces the pain of upgrading to Airflow 2.0for users of the KubernetesPodOperator. Users will be allowed to    continue using the airflow.kubernetes custom classes* spellcheck* spelling* clean up unecessary files in 1.10* clean up unecessary files in 1.10* clean up unecessary files in 1.10,2
Fix issues with Gantt View (#12419)closes https://github.com/apache/airflow/issues/9813closes https://github.com/apache/airflow/issues/9633and does some cleanup,4
Fix spelling (#12421),0
Enable Markdownlint rule MD045/no alt text (#12423),0
Fix Entrypoint and _CMD config variables (#12411)closes https://github.com/apache/airflow/issues/8705Co-Authored-By: Noël Bardelot <11333203+NBardelot@users.noreply.github.com>,1
Fix docstrings for Kubernetes Backcompat module (#12422)This were missed in https://github.com/apache/airflow/pull/12384,2
Fix typo (#12424),2
Fix tests for missing example and system tests (#12425)`tranfers` -> `transfers`,3
Enable Markdownlint rule MD046/code-block-style (#12429),0
Raise correct Warning in kubernetes/backcompat/volume_mount.py (#12432)It was raising warning with message to use `V1Volume` instead of `V1VolumeMount`,1
JSON Response is returned for invalid API requests (#12305),5
Optimize json schema validation in providers_manager (#12420),1
Upgrade pyupgrade to v2.7.4 (#12434)https://github.com/asottile/pyupgrade/compare/v2.7.3...v2.7.4,5
Fix case of GitHub (#12433)Github -> GitHub,0
Move import at the top of the file (#12431)`from airflow.utils.state import State` is used in multiple functions.I suggested it in https://github.com/apache/airflow/pull/12384 but was missed,1
Enable Markdownlint rule MD014/commands-show-output (#12430)https://github.com/DavidAnson/markdownlint/blob/main/doc/Rules.md#md014---dollar-signs-used-before-commands-without-showing-output,2
PR to add 'files' to template-fields in EmailOperator class (#12428),1
Enable Markdownlint rule MD003/heading-style/header-style (#12427),0
Added `files` to templated fields of `EmailOperator` (#12435),1
Synchronization of supported K8S version (#12443),1
Add description field to connection (#10873)closes https://github.com/apache/airflow/issues/10840,0
Fix download method in GCSToBigQueryOperator (#12442)closes: #12439,1
Rename test_local_setting.py to test_settings.py (#12437),3
"Move setup properties out of setup.py in to setup.cfg (#12417)yI've moved all the ones that are ""static"" -- any form of dynamic orinterpolated values are left in setup.pyIf a value is passed as n kwrg to setup and in setup.cfg, the kwargwins out.The ./build/bin content only depends on the version of tools used(helm//kind/kubectl) and it does not depend on setup.py norsetup.cfg",5
Cope with '%' in password when waiting for migrations (#12440),4
Cleanup requirements in README a bit (#12446),1
Enable Markdownlint rule MD003/heading-style/header-style (#12427) (#12438)Co-authored-by: John Bampton <jbampton@users.noreply.github.com>,1
Fix bug in server timezone indicator (#12447),0
Update provider READMEs for 1.0.0b2 batch release (#12449),1
"Edit FAQ to reference Airflow 2 as a solution to latency problem, closes #12348 (#12450)",0
"Automatically apply ""area:UI"" labels to PRs (#12452)",5
"Fix backwards compatibility further (#12451)* Fix backwards compatibility furtherThis PR ensures that node_selector, affinity, and tolerations are allconverted into k8s API objects before they are sent to thepod_mutation_hook. this fixes an inconsistency that would force airflowengineers to consider both cases when writing their pod_mutation_hook* nit",1
Fix broken CI.yml (#12454)The PR #12417 broke CI.yaml accidentally. This PR fixes it.,0
Fix broken example_kubernetes DAG (#12455),2
Improve the layout of TI modal when browser at narrower widths (#12456),1
Update readmes for cncf.kube provider fixes (#12457),0
"Fix typoe in migrations: RESOURCE_DAGS to RESOURCE_DAG. (#12460)This was missed in CI because it only became a problem when run with existing DAG data in the DB, which CI doesn't have.",5
Fix failed KubernetesPodOperator tests (#12461)Fixes failure in KubernetesPodOperator tests cause bynodeSelector arguments,1
Bump version to 2.0.0b3 (#12462),5
Webserver: Further Sanitize values passed to origin param (#12459)Follow-up of https://github.com/apache/airflow/pull/10334,2
Fix case of GitHub in comment (#12474)github -> GitHub,0
Fix setup.py to install the right provider for mysql (#12476),1
Fix failing test on Py3.8 (#12481),3
Fix Python docstring parameter (#12483),2
Improve UI file naming/patterns (#12486)* Use friendlier terms for file naming* Correlate asset names to template names,1
GCP Secrets Optional Lookup (#12360),5
"Improve www.security.get_accessible_dags() and webserver performance (#12458)* Improve www.security.get_accessible_dags() and webserver performance- the performance of get_accessible_dags() is improved by returning as early as possible- the changes made in www.views.py are based on the fact that the check  on permissions.RESOURCE_DAG is already done in get_accessible_dags(),  which is invoked by get_accessible_dag_ids() then.* Fix-up. Incorporate the changes suggested by jhtimmins with minor changeCo-Authored-By: jhtimmins <jameshtimmins@gmail.com>",4
Remove unused/uncompiled JS file (#12490),2
Update tag color to be neutral (and match DAGs index view) (#12493),2
"Fix Kube tests (#12479)This is the same fix as in #12461, but we didn't notice it as the testsfailed after 50 failures.It also turns out that the k8s API doesn't take a V1NodeSelector and insteadjust takes a dict.Co-authored-by: Daniel Imberman <daniel.imberman@gmail.com>",0
ensure Moment date is valid before attempting to render it (#12492),5
Make kubernetes requirement optional for Example DAGs (#12494),2
Improvements for transfer operators references (#12482),1
Turn off foreign keys before altering table to prevent sqlite issue. (#12487)Closes #12488,0
"Fixes taskInstances API endpoint when start_date, end_date or state are None(null) (#12453)Fixes a bug when calling `/api/v1/dags/~/dagRuns/~/taskInstances/list` with dag_ids as parameter.The schema had defined `start_date`, `end_date` and `state` as non-nullable, but they are optional.",5
Reorder Migrations to make it 1.10.13 compatible (#12496)This commits makes Airflow 2.0 migrations compatible with 1.10.13 so users caneasily upgrade from 1.10.13 to 2.0,1
Unquarantine test_cli_webserver_background (#12501),3
Separate out documentation building per provider  (#12444)* POC* fixup! POC,0
Add missing file_token field to get dag details API endpoint (#12463),2
Fix the default value for VaultBackend's config_path (#12518)It is `config` not `configs`,5
Temporarily allow force-push on v1-10-stable (#12524),1
Add Energy Solutions to INTHEWILD.md (#12523),1
Enable v1-10-stable branch protection (#12525),0
"Fix S3ToSnowflakeOperator docstring (#12504)There's a parameter called s3_bucket in its docstring,but it doesn't exist actually. The stage parameter exists instead.",2
Fix git archive command in Release Management guide (#12526)There was a trailing back-tick which I found when cutting 1.10.13rc1,0
"Fix wait-for-migrations command in helm chart (#12522)If the migrations weren't yet applied this would fail with `NameError:name 'log' is not defined`. (I guess no one really noticed as thecontainer would restart, and try again.)",1
Fix build on RTD (#12529),0
Fix Python Docstring parameters (#12513),2
"Fixes unneeded docker-context-files added in CI (#12534)We do not need to add docker-context-files in CI before we runfirst ""cache"" PIP installation. Adding it might cause the effectthat the cache will always be invalidated in case someone hasa file added there before building and pushing the image.This PR fixes the problem by adding docker-context files laterin the Dockerfile and changing the constraints locationused in the ""cache"" step to always use the github constraints inthis case.Closes #12509",1
"Return nonzero exit codes on pool import errors. (#12095)The pool import command returns an exit code of zero in a few differenterror cases. This causes problems for scripts that invoke the command,since commands that actually failed will appear to have worked. Thispatch returns a nonzero code if the pool file doesn't exist, if the fileisn't valid json, or if any of the pools in the file is invalid.",2
update broken link to cli-and-env-variables-ref (#12540),2
Support installing providers with no dependencies via extras (#12497),1
Fix line breaks in CeleryKubernetesExecutor docs (#12538),2
Add example DAGs to provider docs (#12528),2
Move providers docs to separate package + Spell-check in a common job with docs-build (#12527),2
Fix sed command on MacOS (#12549),0
Deprecate Read the Docs (#12541),2
Add capability to specify gunicorn access log format (#10261),2
[AIRFLOW-5115] Bugfix for S3KeySensor failing to accept template_fields (#12389),0
Fix build on RTD (#12551),0
Troubleshooting moved to the installation page (#12533),4
"Spark-on-k8s sensor logs - properly pass defined namespace to pod log call (#11199)This is a follow up to #10023 - it seems that passing the defined namespace to the log call was missed in one of the rebases done during the PR.Without this fix, logging will fail when the k8s connection uses a different namespace than the one SparkApplication(s) are actually submitted to.",1
Fix use of `a` vs `an` (#12542)`actual` starts with a vowel while `heartbeat` not,1
Move doc around Manual Trigger Visual diff to Tree View page (#12565),2
"Housekeeping for www/security.py (#12516)## Housekeeping for www/security.py- correct type hint for dag_id (str rather than int)- Use DAG name without prefix ""DAG:"" in logging (line 644)- avoid unnecessary duplicated operation (line 653)## Clean-up the logic in update_admin_perm_view()Because RESOURCE_DAG_PREFIX is ""DAG:"" and RESOURCE_DAG is ""DAGs"",if we have view_menu_id.in_(pv_ids),we can be sure that view_menu_id != all_dag_view.id.By making this change, we have cleaner logic, and can avoid some talks to DB.",5
Add MTsolutions to companies using Airflow (#12566),1
Quarantine test_cli_webserver_background (#12570)We unquarantined test_cli_webserver_background inhttps://github.com/apache/airflow/pull/12501 but seems like thetest is still flaky: https://github.com/apache/airflow/runs/1443468804#step:6:2531,1
add Vestiaire Collective to INTHEWILD.md (#12572),1
"Doc Fix around Secret/Connection/Variable (#12571)Documentation fixes/improvements:- For Variables set by Environment Variable,   it was highlighted that it may not appear in the web UI.   But this was not highlighted for Connections set by Environment Variable.   This PR adds this note (in docs/howto/connection/index.rst).- Fix wrong docstring of airflow.secrets.base_secrets.BaseSecretsBackend.get_variable().- The Secret Backends don't properly mentioning Variables in the docstrings   (all the focus was put on Connections only). This PR addresses this.- Other a few minor changes.",4
Fix Dag Serialization crash caused by preset DagContext (#12530),2
"Use html urls instead of onclick for dags view links. (#12539)The dags view uses onclick events for dagrun and taskinstance links.This breaks url previews, copying urls, opening links in a new tab, etc.This patch uses svg anchors with href attributes instead of onclickevents so that these links behave like normal links.",2
Add check for duplicates in provider.yaml files (#12578),2
"Drop random.choice() in BaseHook.get_connection() (#12573)https://github.com/apache/airflow/pull/9067 made conn_id unique,and this is effective from 2.0.*.Due to this change, BaseHook.get_connections() will return a List of length 1, or raise Exception.In such a case, we should simply always get the only elementfrom the result from BaseHook.get_connections(),and drop random.choice() in BaseHook.get_connection(), which was only applicable for the earliersetting (multiple connections is allowed for single conn_id)",1
Improve code quality of ExternalTaskSensor (#12574),1
limit table of content at the main Airflow doc page (#12561),2
Dev documentation uses sphinx-airflow-theme (#12582),1
Hide ToC from the Apache Airflow doc main page (#12589)Hide the ToC from being displayed when thethe index.rst file in the Apache Airflow docs is rendered.This will improve user experience and prevent repetition of what hasalready been displayed on the sidebar.,1
"Add link to docs index to table of contents (#12594)Without this, it's not obvious how to get back to the main page",1
Don't set child tasks to schedulable in test runs (#12595)Fixes a bug where Airflow will attempt to set child tasks to schedulablefor test tasks when users run `airflow task test.` This causes an erroras Airflow runs a DB seek for a task that has not been recorded.,5
"Allow webserver to read pod logs directly (#12598)* Allow webserver to read pod logs directlyFor users who are testing the KubernetesExecutor, allows users to readpod logs via the Kubernetes API. Worth noting that these logs will onlybe accessible while the worker is running.* fix tests",3
Adds missing licence headers (#12593),1
Fix AWS DataSync tests failing (#11020)closes #10985,0
Update logging & doc for LocalFilesystem Secrets Backend (#12597)- Support towards YAML is added in PR https://github.com/apache/airflow/pull/9477  Most docs were updated for this. But a few docstrings and exception logging were missed,2
Remove foreign key constraint on SerializedDagModel's dag_runs field (#12586)Issue: https://github.com/apache/airflow/issues/12448,0
"Use AIRFLOW_CONSTRAINTS_LOCATION when passed during docker build (#12604)Previously, even though this was passed during docker build it wasignored. This commit fixes it",0
"Rename `[scheduler] max_threads` to `[scheduler] parsing_processes` (#12605)From Airflow 2.0, `max_threads` config under `[scheduler]` section has been renamed to `parsing_processes`.This is to align the name with the actual code where the Scheduler launches the number of processes defined by`[scheduler] parsing_processes` to Parse DAG files, calculates next DagRun date for each DAG,serialize them and store them in the DB.",5
"Revert ""Hide ToC from the Apache Airflow doc main page (#12589)"" (#12607)This reverts commit ce919912b7ead388c0a99f4254e551ae3385ff50.",4
Fix Connection.description migration for MySQL8 (#12596)Due to not executing MySQL8 tests Fixed in #12591 addeddescription for connection table was not compatible withMySQL8 with utf8mb4 character set.This change adds migration and fixes the previous migrationto make it compatible.,1
"Fixes inconsistent behaviour of utf8mb4 encoding on Mysql 5.7/8 (#12614)* Fix Connection.description migration for MySQL8Due to not executing MySQL8 tests Fixed in #12591 addeddescription for connection table was not compatible withMySQL8 with utf8mb4 character set.This change adds migration and fixes the previous migrationto make it compatible.* Fixes inconsistent setting of encoding on Mysql 5.7/8We missed that when we added supportfor differnet mysql versions in #7717 when we removed defaultcharacter set setting for the database server.This change forces the default on database server to beutf8mb4 - regardless if MySQL 5.7 or MySQL8 is used.Utf8mb4 is default for MySQL8 but latin1 is default fo MySQL 5.7.There was a suspected root cause of the problem:https://dev.mysql.com/doc/refman/8.0/en/charset-connection.htmlwhere mysql client falls back to the default collation ifthe client8 is used with 5.7 database, but this should beno problem if the default DB character set is forced to beutf8mb4This PR restores forcing the server-side encoding.",1
"Fixes tests that was not compatible with MySQL8 (#12615)In MySQL8 you cannot create table LIKE an INFORMATION_SCHEMAtable because they are not ""real"" tables and generic_transfertests failed on MySQL8 because of that.This PR changes the test to use connection table instead asa base for that test",3
Add docs about tagging and pushing constraints file (#12625)We push constraints files for each airflow release:```❯ git tag --list | grep constraintsconstraints-1.10.1constraints-1.10.10constraints-1.10.11constraints-1.10.13constraints-1.10.2constraints-1.10.3constraints-1.10.4constraints-1.10.5constraints-1.10.6constraints-1.10.7constraints-1.10.8constraints-1.10.9```,2
Sync Airflow 1.10.13 Updating.md with Master (#12624),5
Make AzureKeyVaultBackend backwards-compatible (#12626)This module was released in 1.10.13. This commit just makesit backwards-compatible,1
Fix session_lifetime_minutes config docs (#12628)- Update `version_added` to 1.10.13- Better format it using two back-ticks for code-block instead of italics,1
"Actually run against the version of the DB we select in the matrix. (#12591)Due to a bug in Breeze initialization code, we were always runningagainst Postgres 9.6 and MySQL 5.7, even when the matrix selectedsomething else.(We were overwriting the POSTGRES_VERSION and MYSQL_VERSION environmentvariables in initialization code)",5
fix db migration downgrade actions (#12608),5
"Ensure that the `prohibit_commit` guard only applies to _one_ session. (#12575)By listening on the engine's `commit` we were picking up _all_ sessioncommit calls, even from sessions other than the one passed to`prohibit_commit`, which was not intended.This changes it to listen to before_commit, which is session specific,rather than engine ""global"" and also adds tests which were lackingbefore hand.",3
"Adds possibility of forcing upgrade constraint by setting a label (#12635)You can now set a label on PR that will force upgrading to latestdependencies in your PR. If committer sets an""upgrade to latest dependencies"" label, it will cause the PRto upgrade all dependencies to latest versions of dependenciesmatching setup.py + setup.cfg configuration.",5
"Clean-up airflow/kubernetes/kube_config.py (#12627)- Remove the stale internal method `_get_security_context_val`  It was added in PR #5429, but to what I can see, it's not needed anymore.- Avoid hard-coding when we can (we already have `core_section` specified, so can avoid using ``'core'`)- Narrow down what we import from `airflow.settings`",1
Add Changelog for Airflow 1.10.13 (#12623),4
Improved breeze messages for initialize-local-virtualenv and static-check --help (#12640),5
Add Corsearch to in the wild (#12641),1
"Housekeeping: Remove 'dirty_ids' in www/views.py (#12645)This is a clean-up/housekeeping change.Usage of 'dirty_ids' is no longer applicable since PR #4378, back in Dec 2018https://github.com/apache/airflow/pull/4378",4
"Allows mounting local sources for github run-id images (#12650)The images that are build on github can be used to reproducethe test errors in CI - they should then be mounted withoutlocal sources. However in some cases when you are dealing withdependencies for example, it is useful to be able to mount thesources.This PR makes it possible.",1
Add DataflowJobMessagesSensor and DataflowAutoscalingEventsSensor (#12249),5
Replace foreign key constraints with foreign annotation (#12603)closes https://github.com/apache/airflow/issues/12448,0
"Add 1.10.13 to CI, Breeze and Docs (#12652)",2
Restructure the extras in setup.py and described them (#12548)Closes: #12544Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Typo Fix: Deprecated config force_log_out_after was not used (#12661)`force_logout_after` should be `force_log_out_after` in the codesection https://github.com/apache/airflow/blob/master/airflow/settings.py#L372-L381.As `force_log_out_after` is actually used and written inhttps://github.com/apache/airflow/blob/c5700a56bb3b9a5b872bda0fe0d3de82b0128bdf/UPDATING.md#unify-user-session-lifetime-configuration.,5
"Implement reading provider information from packages/sources (#12512)This PR implements discovering and readin provider information frompackages (using entry_points) and - if found - from localprovider yaml files for the built-in airflow providers,when they are found in the airflow.provider packages.The provider.yaml files - if found - take precedence over thepackage-provided ones.Add displaying provider information in CLICloses: #12470",5
Update setup.py to get non-conflicting set of dependencies (#12636)This change upgrades setup.py and setup.cfg to provide non-conflicting`pip check` valid set of constraints for CI image.Fixes #10854Co-authored-by: Tomek Urbaszek <turbaszek@apache.org>Co-authored-by: Tomek Urbaszek <turbaszek@apache.org>,0
Enable PIP check for both CI and PROD image (#12664)This PR enables PIP check after constraints have been updatedto be stable and 'pip check' compliant in #12636,5
Make migrations using kube_resource_version idempotent (#12670)closes https://github.com/apache/airflow/issues/12666,0
Fix packages errors summary for docs build (#12658),2
"Remove ""@"" references from constraints generattion (#12671)Likely fixes: #12665",0
Add example dag and system tests for azure wasb and fileshare (#12673),2
Move connection guides to provider documentation packages (#12653),2
Added `@apply_defaults` decorator. (#12620),1
"Temporarily disable PROD image check until Azure Blob is fixed (#12679)This PR disables temporarily PIP check result for productionimage, until the fix to switch Azure Blob to v12 is fixed.",0
Validate JSON schema files with JSON Schema (#12682),5
"Setup.cfg change triggers full build (#12684)Since we moved part of the setup.py specification tosetup.cfg, we should trigger full build when only that filechanges.",4
"The Pyarrow limitation in install_requires is not needed. (#12683)It was added to make snowflake happy, but it is not needed aspackage requirement in fact and google provider complains whenthe version of pyarrow is too low.Also when PyArrow limitation is removed, we have to limitthe importlib_resources library back.",2
Adds providers information to `airflow info` command (#12687),5
Fix typos and added missing descriptions in provider.yaml schema (#12690),1
Move production deployments tips to docs/production-deployment.rst (#12686),2
Use rich to render info and cheat-sheet command (#12689),5
"Replace pkg_resources with importlib.metadata to avoid VersionConflict errors (#12694)Using `pkg_resources.iter_entry_points` validates the versionconstraints, and if any fail it will throw an Exception for thatentrypoint.This sounds nice, but is a huge mis-feature.So instead of that, switch to using importlib.metadata (well, it'sbackport importlib_metadata) that just gives us the entrypoints - noother verification of requirements is performed.This has two advantages:1. providers and plugins load much more reliably.2. it's faster tooCloses #12692",1
"Ensure that tasks set to up_for_retry have an end date (#12675)If a task is ""manually"" set to up_for_retry (via the UI for instance) itmight not have an end date, and much of the logic about computingretries assumes that it does.Without this, manually setting a running task to up_for_retry will makethe make it impossible to view the TaskInstance details page (as ittries to print the is_premature property), and also the NotInRetryPeriodTIDep fails - both with an exception:> File ""airflow/models/taskinstance.py"", line 882, in next_retry_datetime>   return self.end_date + delay> TypeError: unsupported operand type(s) for +: 'NoneType' and 'datetime.timedelta'",5
"Don't use time.time() or timezone.utcnow() for duration calculations (#12353)`time.time() - start`, or `timezone.utcnow() - start_dttm` will workfine in 99% of cases, but it has one fatal flaw:They both operate on system time, and that can go backwards.While this might be surprising, it can happen -- usually due to clocksbeing adjusted.And while it is might seem rare, for long running processes it is morecommon than we might expect. Most of these durations are harmless to getwrong (just being logs) it is better to be safe than sorry.Also the `utcnow()` style I have replaced will be much lighter weight -creating a date time object is a comparatively expensive operation, andcomputing a diff between two even more so, _especially_ when compared tojust subtracting two floats.To make the ""common"" case easier of wanting to compute a duration for ablock, I have made `Stats.timer()` return an object that has a`duration` field.",1
"Refactor airflow plugins command (#12697)This commit refactors plugins command to make it moreuser-friendly, structured and easier to read.",1
Adds support for Connection/Hook discovery from providers (#12466)* Adds support for Hook discovery from providersThis PR extends providers discovery with the mechanismof retrieving mapping of connections from type to hook.Fixes #12456* fixup! Adds support for Hook discovery from providers* fixup! fixup! Adds support for Hook discovery from providers,1
Improve wording of selective checks comments (#12701),1
Remove now-incorrect warning about pools and multiple schedulers (#12709)Reverts #7643 now it's not true anymore after Scheduler HA has landed,4
"Refine the DB query logics in www.views.task_stats() (#12707)* Refine the DB query logics in www.views.task_stats()- given filter_dag_ids is either allowed_dag_ids, or intersection of allowed_dag_ids and selected_dag_ids,  no matter if selected_dag_ids is None or not, filter_dag_ids should ALWAYS be considered into the SQL query.  Currently, if selected_dag_ids is None, the query is actually getting the full result (then 'filter' at the end).  This means more (unnecessary) data travel between Airflow and DB.- When we join table A and B with A.id == B.id (default is INNER join), if we always confirm ALL A.id is in a specific list,  implicitly ALL ids in the result table are already guaranteed in this specific list as well.  This is why the two redundant .filter() chunks are removed.Minor performance improvement should be expected.Meanwhile, this change makes the code cleaner.",4
Move operator guides to provider documentation packages (#12681),2
"Remove deprecated dagbag metrics (#12695)These were deprecated in 1.10.6 via #6157, so we should remove thembefore 2.0 rolls around.",4
Deprecate BaseHook.get_connections method (#10135) (#10192)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Add Getir to in the wild! (#12719)* Add Getir to in the wild!* hotfix - Add Getir to in the wild,1
Move apache-airflow docs to subdirectory (#12715),2
Output of installing remaining packages is shown also on success (#12723)Previously the output of instaling remaining packges when testingprovider imports was only shown on error. However it is usefulto know what's going on even if it clutters the log.Note that this installation is only needed until we includeapache-beam in the installed packages on CI.Related to #12703This PR shows the output always .,2
Improve verification of images with PIP check (#12718)Verification of images with PIP is done in separate jobs andthey provide useful information to committers and contributorswhen the pip check fails.,0
Stronger language re: SQLite (#12727),5
Fix static checks - #12715 (#12729),0
"Allow switching xcom_pickling to JSON/Pickle (#12724)Without this commit, the Webserver throws an error whenenabling xcom_pickling in the airflow_config by setting `enable_xcom_pickling = True`(the default is `False`).Example error:```>           return pickle.loads(result.value)E           _pickle.UnpicklingError: invalid load key, '{'.airflow/models/xcom.py:250: UnpicklingError--------------------------------------------------```",0
Pins PIP to 20.2.4 in our Dockerfiles (#12738)Until we make sure that the new resolver in PIP 20.3 workswe should pin PIP to 20.2.4.This is hopefully a temporary measure.Part of #12737,1
User-friendly output of Breeze and CI scripts (#12735),1
"Fix chart jobs delete policy for improved idempotency (#12646)The chart has two jobs (migrate-database & create-user).These jobs are run post-install and post-upgrade and only deleted on success.So if for some reason (quick reinstall / upgrade), the job fails or is stuck then helmwill fail because the job already exists.This commit sets the `helm.sh/hook-delete-policy` to `before-hook-creation,hook-succeeded`so helm will always delete the jobs before creating them again.",1
Added Headout to INTHEWILD (#12734)Signed-off-by: Shivansh Saini <shivanshs9@gmail.com>,5
"Move config item 'worker_precheck' from section [core] to [celery] (#12746)* Move config item 'worker_precheck' from section [core] to [celery]This configuration is ONLY applicable for Celery Worker.So it should be in section [celery], rather than [core]* Add to deprecation/migration automatic list",1
Refactor list rendering in commands (#12704)This commit unifies the mechanism of rendering output of tabulardata. This gives users a possibility to eiter display a tabularrepresentation of data or render it as valid json or yaml payload.Closes: #12699Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
Don't let webserver run with dangerous config (#12747),5
Optimize subclasses of DummyOperator for Scheduling (#12745)Custom operators inheriting from DummyOperator will now instead of going to a scheduled state will go set straight to success if they don't have callbacks set. closes https://github.com/apache/airflow/issues/11393,0
Order broken DAG messages in UI (#12749),2
SlackWebhookHook use password instead of extra (#12674)closes: #12214,4
Allow using _CMD / _SECRET to set `[webserver] secret_key` config (#12742)`[webserver] secret_key` is also a secret like Fernet key. Allowingit to be set via _CMD or _SECRET allows users to use the external secret store for it.,1
Remove store_serialized_dags from config (#12754)The store_serialized_dags is no longer used in our code base asboth webserver and scheduler require serialization.,1
"Refactor and speed up ""DAG:"" prefix permissions migration (#12720)",0
"Fix typo in airflow/serialization/serialized_objects.py (#12767)""Cnn't load plugins"" -> ""Can not load plugins""",2
Change DEBUG color to green in coloured logger (#12784)In this way it's easier to see difference between debug and error.,0
Handle ParserError when dag is triggered with invalid execution_date (#12618),5
BugFix: Editing a DAG run or Task Instance on UI causes an Error (#12770)closes https://github.com/apache/airflow/issues/12489,0
Fix the exception that the port is empty when using db shell (#12740)* Fix the exception that the port is empty when using db shell,5
Prevent unused scrollbars from appearing in FF on Linux (#12795),1
Avoid log spam & have more meaningful log when pull image in DockerOperator (#12763)Fixing issue reported in https://github.com/apache/airflow/issues/12576This change actually also makes the log more meaningfulCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,2
Clean noqa labels wrongly handled by black linter (#12791),0
Add documentation on AIRFLOW__{SECTION}__{KEY}_SECRET config (#12797)closes #12774,5
Artifacts in Github Action have a short retention period (#12793),5
Convert state arguments to ExternalTaskSensor to list (#12794)Without this if you pass a tuple or a set etc things would break.,4
Improve error handling in cli and introduce consistency (#12764)This PR is a followup after #12375 and #12704 it improves handlingof some errors in cli commands to avoid show users to much tracebackand uses SystemExit consistently.,5
Add SMTP timeout and retry limit for SMTP email backend. (#12801),1
Add more json-schama checks + display all errors (#12805),0
"Get airflow version from importlib.metadata rather than hard-coding (#12786)One less thing to change, and one less pre-commit step needed :)",4
Add expandable groups in the docs build log (#12799),2
Move secret backends guides to provider docs (#12798),2
Fix paths to images in README.md (#12756),2
Add 'headers' to template_fields in HttpSensor (#12809)Co-authored-by: Zachary Climes <zclimes@zclimes-DCH2.com>closes: #12796,1
Dagrun object doesn't exist in the TriggerDagRunOperator (#12819)* Dagrun object doesn't exist in the TriggerDagRunOperatorfixes  https://github.com/apache/airflow/issues/12587Fixes issue where dag_run object is not populated if the dag_run alreadyexists and is reset* change to get_last_dag_run* Update airflow/operators/dagrun_operator.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,2
Updating documentation to specify sensitive config options (#12820),5
"Cleanup & improvements around scheduling (#12815)* Cleanup & improvement around scheduling- Remove unneeded code line- Remove stale docstring- Fix wrong docstring- Fix stale doc image link in docstring- avoid unnecessary loop in DagRun.schedule_tis()- Minor improvement on DAG.deactivate_stale_dags()  which is invoked inside SchedulerJob* Revert one change, because we plan to have a dedicated project-wise PR for this issue* One more fix: dagbag.read_dags_from_db = True in DagFileProcess.process_file() is not needed anymore",2
"Configuration.getsection copes with sections that only exist in user config (#12816)If you try to run `airflow config list` with an old config you upgradedfrom 1.8, it would fail for any sections that have been removed from thedefault cofing -- `ldap` for instance.This would also be a problem if the user makes a typo in a configsection, or is using the airflow config for storing their owninformation.While I was changing this code, I also removed the use of privatemethods/variable access in favour of public API",1
Add Telegram hook and operator (#11850)closes: #11845Adds:Telegram HookTelegram Operator,1
Fix for empty Graph View when task does not have a DAG during relationship setting (#12829)Closes #12757,1
"Add paused column to `dags list` sub-command (#12830)This can still show ""None"" if the dag is not yet in the metadata DB --showing either True or False there would give a false impression(especially False -- as if it doesn't exist in the DB it can't beunpaused yet!)",1
Add Qliro to INTHEWILD.md (#12833)Co-authored-by: Erik Thornblad <erik.thornblad@qliro.com>,1
Add `-o` as short form option for `--output` in CLI (#12831),1
Add support for extra links coming from the providers (#12472)Closes: #11431,1
Fix docstring for models.Variable.get() (#12828),1
Increments number of providers detected and stop failing (#12841)There seem to be a flaky number of providers returned byintegration test.For now exit will be disabled but we will observe the flakiness.,3
Add notes about PIP 20.3 breaking Airflow installation (#12840)Part of #12838,4
Add sensors section to describe different modes of sensors (#12803)closes: #10816,1
"Make `airflow --help` run five times quicker (#12836)Importing anything from airflow.models pulls in _a lot_ of airflow, sodelay imports until the functions are called, or make use of the`TYPE_CHECKING` to not actually import at runtime.**Before**: mean 2.58s (with a lot of variance)```airflow ❯ for i in 1 2 3; do time airflow --help >/dev/null; doneairflow --help > /dev/null  2.00s user 1.39s system 176% cpu 1.928 totalairflow --help > /dev/null  2.84s user 1.43s system 151% cpu 2.817 totalairflow --help > /dev/null  3.00s user 1.37s system 145% cpu 3.009 total```**After**: 0.526s```airflow --help > /dev/null  0.39s user 0.04s system 99% cpu 0.435 totalairflow --help > /dev/null  0.40s user 0.05s system 99% cpu 0.446 totalairflow --help > /dev/null  0.64s user 0.05s system 99% cpu 0.698 total```This also has an advantage in development where a syntax error doesn'tfail with a slightly confusing error message about ""unable to configurelogger 'task'"".",5
"Don't emit first_task_scheduling_delay metric for only-once dags (#12835)Dags with a schedule interval of None, or `@once` don't have a followingschedule, so we can't realistically calculate this metric.Additionally, this changes the emitted metric from seconds tomilliseconds -- all timers to statsd should be in milliseconds -- thisis what Statsd and apps that consume data from there expect. See #10629for more details.This will be a ""breaking"" change from 1.10.14, where the metric wasback-ported to, but was (incorrectly) emitting seconds.",4
Fix doc preview error in editor for google operators doc (#10962),2
"Updated UPGRADING TO 2.0  guide based on new releases (#12847)Updated to reference 1.10.14 instead of 1.10.13.Also added an additional step to install the upgrade check scripts,instead of expecting them to be part of the Airflow bridge release.",1
Fix Commands for Publishing constraints (#12800),0
"Build normal/backport providers based on correct list (#12848)This was using the backport list when trying to build the normal list,leading to papermill package not getting built.I have re-introduced the exit removed in #12841, and also done a littlebit of drive-by-tidying",4
Add conditional version retrieval from setup. (#12853)When airflow is not installed as package (for example for localdevelopment from sources) there is no package metadata.Many of our unit tests use the version field and they fail if theyare run within virtual environment where airflow is not installedas package (for example in IntelliJ this is default setting.This PR adds fall-back to read airflow version from setup incase it cannot be read from package metadata.,5
"Add missing crypto and s3 extras (#12850)We are missing two extras that were existingin Airflow 1.10 - crypto and s3. They are both deprecatedbecause s3 should be replaced with amazon and crypto isalready installed by default by Airflow as 'install_requires',but for backwards compatibility, we  add them to 2.0 as wellwith deprecation comment. Also yandexcloud has been removed(it was not present in 1.10 so no need to deprecate it)and new table with deprecated extras is added to the docs.",2
Quarantine test TestSchedulerJob.test_scheduler_task_start_date (#12860),5
Added reference to 2.0 CLI commands in 1.10.14 (#12864)Updated to include the reference to the Airflow 2.0 CLI commandsonow available in Airflow 1.10.14 to make it easier to upgrade.,1
"Mark required fields in Forms as required (#12856)We have a number of custom forms that have required fields that weren'texplicitly marked as required.This allowed you to submit the Connection form (for example) withnothing as the Conn Id, leading to an empty string being used as theconnection id. This marks that and all the other required fields asrequired.We also replace DataRequired with InputRequired. The previous one istested the truthyness of the value, rather than just that a value wassubmitted.",3
"Store per-task TIDeps in serialized blob (#12858)Without this change sensors in ""reschedule"" mode were being instantlyrescheduled because they didn't have the extra dep thatBaseSensorOperator added.To fix that we need to include deps in the serialization format (but tosave space only when they are not the default list). As of this PR rightnow, only built-in deps are allowed -- a custom dep will result in a DAGparse error.We can fix that for 2.0.x, as I think it is a _very_ uncommon thing todo.Fixes #12783",2
"Production images on CI are now built from packages (#12685)So far, the production images of Airflow were using sourceswhen they were built on CI. This PR changes that, to buildairflow + providers packages first and install themrather than use sources as installation mechanism.Part of #12261",1
Fix command for verifying signature (#12869),0
Change the format for sha512 sum for releases (#12867)closes https://github.com/apache/airflow/issues/12832This format is popular and supported by tools like Ansible.```❯ sha512sum ../airflow-dev/1.10.14rc3/apache-airflow-1.10.14rc3-bin.tar.gz953d3c04ee6fd2fa96e126750e642fc0872add96d180901440a91bd61c494a711b48836c634d93dcb181006935772556d5b4426671bf1a638f0a0698b51b119f  ../airflow-dev/1.10.14rc3/apache-airflow-1.10.14rc3-bin.tar.gz```vs```❯ gpg --print-md SHA512 ../airflow-dev/1.10.14rc3/apache-airflow-1.10.14rc3-bin.tar.gz../airflow-dev/1.10.14rc3/apache-airflow-1.10.14rc3-bin.tar.gz:953D3C04 EE6FD2FA 96E12675 0E642FC0 872ADD96 D1809014 40A91BD6 1C494A71 1B48836C 634D93DC B1810069 35772556 D5B44266 71BF1A63 8F0A0698 B51B119F```,1
Fix links to moved production-deployment file (#12854),2
Remove old option - git_password from sensitive_config_values (#12821),5
"Don't show ""Access deined"" message on login page (#12846)It's kind of obvious that you'd need to login to access something if presentedwith a login page.",2
Add start_date and end_date to DagRun View in UI (#12871)It would be useful to check start_date and end_date and searchDagRuns based and those dates.,5
"Show DAG serialization errors in the UI. (#12866)The previous behaviour led to ""bad"" data being written in the DB -- forexample:```json    ""dag"": {        ""tasks"": [            ""serialization_failed""        ],```(`tasks` should be a list of dictionaries. It clearly isn't.)Instead of doing this we throw an error, that is captured and showingusing the existing import_error mechanism for DAGs. This almostcertainly happens because a user has done ""something interesting"".",1
"Adds airflow as viable docker command in official image (#12878)The change is backwards-compatible. It still allows to pass airflowcommand without ""airflow"" as first parameter, but you can nowalso pass ""airflow"" and the rest of the parameters willbe treated as ""airflow"" command parameters.Documentation is updated to reflect the entrypoint behaviourincluding _CMD option in SQL connections.Part of #12762 and #12602Partially extracted from  #12766",4
Remove first_task_scheduling_delay from Updating.md (#12885)We backported the fix to 1.10.14rc4 so this is no longer a breaking change,4
Clarifies version args for installing 1.10 in Docker (#12875)This change clarifies that AIRFLOW_VERSION should be passedtogether with AIRFLOW_INSTALL_VERSION when the Docker imageis build.Fixes #8612,0
Update commands to verify singature and shasum for Providers (#12886)This commit ports changes from https://github.com/apache/airflow/pull/12869 & https://github.com/apache/airflow/pull/12867 to Providers doc,2
Adjust inner/nested radius to remove unwanted visual gap (#12887),4
"Refactor `@provide_session` to do less ""at runtime"" (#12868)This reworks the providcer_session decorator to pre-compute the argposition etc -- we don't need to do this every time the decorated functionis called.This may be a slight speed increase, but my primary driver for thischange is to make stepping through the decorator in a debugger easier.The change to lineage was to resolve an import cycle -- I have **no**idea why it suddenly started mattering. But it did.The change to subdag_operator is so that `@providcer_session` sees thereal arguments, not the arguments of the `@apply_defaults` decorator.",2
Improve support for special characters in DbApiHook.get_uri (#12775),5
Bugfix: Entrypoint Import Exception masked by attribute error (#12862)`entry_point.module_name` -- Entrypoint does not have a `module_name`attribute.This commit also makes importlib_metadata conditional as it is notneeded for Py 3.9,5
Make Migrations 1.10.14 Compatible (#12896),1
Kubernetes worker pod doesn't use docker container entrypoint (#12766)* Kubernetes worker pod doesn't use docker container entrypointFixes issue on openshift caused by KubernetesExecutor pods not runningvia the entrypoint script* fix* Update UPGRADING_TO_2.0.mdCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* fix UPDGRADING* @ashb commentsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
"Use consistent style for detecting current DB type in migrations (#12898)Ever other case uses this current style, so let's be consistent about it",1
Changes the type of source_code field in DagCode to MEDIUMTEXT (#12890)This change increases the maximum amount of code one can storein dag_code in MySQL. The limit for TEXT is 64KB where forMEDIUMTEXT is 16MB.Fixes #12776,0
"Fix plugin macros not being exposed through airflow.macros (#12788)In order to allow a plugin-provided macro to be used at templating time,it needs to be exposed through the airflow.macros module.* Add cleanup logic to test_registering_plugin_macrosThis test-case has side-effects in the sense that the symbol table ofthe airflow.macros module is altered when integrate_macros_plugins() isinvoked. This commit adds a finalizer to the test case that ensures thatthat module is being reloaded completely in order to prevent impact onother tests.* Integrate plugin-provided macros in subprocessesWhen Airflow is available in a virtual environment, and when thisenvironment runs at least Python 3, then plugin-provided macros shouldbe made available to the Python callable that is being executed in thisenvironment.* Document macros limitationPlugin-provided macros can not be used on Python 2 when usingPythonVirtualenvOperator any longer.",1
Minor changes in description of API docs (#12532),2
Move task handlers guides to provider docs (#12826),2
Fix Multiple Migrations Heads (#12901),0
"Add docs for new scheduler ""clean-up"" tunables. (#12899)",4
Fix broken MySQL Migration (#12904)We added a change https://github.com/apache/airflow/pull/12890 to fix type of `source_code` columnin `dag_code` table. But looks like MySQL does not like if we don't specify nullable field.,2
Fix failing test in TestSchedulerJob (#12906)This change was missed in https://github.com/apache/airflow/pull/12899,4
Move branch_operator.py to branch.py (#12900)Part of #11178 for the branch_operator,1
"Builds prod images on DockerHub from packages (#12908)This build combines building both CI and PROD image in onescript execution on DockerHub per python version.First the CI image is build and secondly, the image is usedto build all the packages from sources and then thosepackages are used to build the PROD image.Resulting image will be package image built from latest sources.Closes: #12261",3
Make sure perms list isn't None before looking within list (#12915),1
Make xcom_pull results order deterministic (#12905)closes https://github.com/apache/airflow/issues/11858,0
"Update comments in setup.py (#12903)Updates comments in setup.py according to the latest 2.0 agreementsabout semver and clarifies use for arrays defined in setup.py.We will refactor those after 2.0 further, but for now we keepthe current structure.",4
"Add support for dynamic connection form fields per provider (#12558)Connection form behaviour depends on the connection type. Since we'veseparated providers into separate packages, the connection form shouldbe extendable by each provider. This PR implements both:  * extra fields added by provider  * configurable behaviour per providerThis PR will be followed by separate documentation on how to write yourprovider.Also this change triggers (in tests only) the snowflake annoyancedescribed in #12881 so we had to xfail presto test where monkeypatchingof snowflake causes the test to fail.Part of #11429",0
"Adds predefined providers to install_requires. (#12916)* The 4 providers (http, ftp, sqlite, imap) are popularand they do not require any additionl dependencies so we decidedto include them by default in Airflow 2.0Co-authored-by: Jarek Potiuk <jarek.potiuk@polidea.com>* Update setup.pyCo-authored-by: Jarek Potiuk <jarek.potiuk@polidea.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Remove duplicate entry in Updating.md (#12919),5
ImportErrors are collapsed by default (#12811),2
Added documentation for Airflow Upgrade Check (#12872),2
Moved subdag_operator.py to subdag.py (#11307)Part of #11178,2
Add missing licences. (#12922)This PR adds missing licences before the RC release or addsappropriate exclusions.Closes: #11632,1
"Use ""auto"" instead of ""scroll"" to prevent visible (unused) scrollbars in some browsers (#12923)",1
Rename remaing modules to match AIP-21 (#12917)As discussed in AIP-21* Rename airflow.hooks.base_hook to airflow.hooks.base* Rename airflow.hooks.dbapi_hook to airflow.hooks.dbapi* Rename airflow.sensors.base_sensor_operator to airflow.sensors.base* Rename airflow.sensors.date_time_sensor to airflow.sensors.date_time* Rename airflow.sensors.time_delta_sensor to airflow.sensors.time_deltaCo-authored-by: Kaxil Naik <kaxilnaik@apache.org>,5
Increase timeout for SQLite tests to 80 minutes (#12926)We already have 80 minutes timeout for Postgres and MySQL tests.Currently some of Sqlite tests on PRs are timing out since theyare taking more than 60 mintues.Example: https://github.com/apache/airflow/runs/1518719040,1
Warning about unsafe migrations (#10254)Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>,2
Move dummy_operator.py to dummy.py (#11178) (#11293)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Disable experimental REST API by default (#12337),5
"Update doc to reflect the changes for DAG-Level access control (#12928)In 2.0 and master branch, there are permission & view name changes for DAG-level access control.- Permission: ""can_dag_read""/""can_dag_edit"" -> ""can_read""/""can_edit""- View: ""all_dags"" -> ""DAGs""These were missed to be reflected in the doc, and this PR addresses it.",1
Disable Pause/Unpause switch if user doesn't have edit access for DAG. (#12911),2
Simplify publishing of documentation (#12892)Close: #11423Close: #11152,2
Rename remaining Sensors to match AIP-21 (#12927)As discussed in AIP-21* Rename airflow.sensors.external_task_sensor to airflow.sensors.external_task* Rename airflow.sensors.sql_sensor to airflow.sensors.sql* Rename airflow.contrib.sensors.weekday_sensor to airflow.sensors.weekday,5
Fix typo in metrics doc (#12934),2
Add Clarification to pod_template_file exmaples (#12932)Addresses https://github.com/apache/airflow/issues/11686Clarifies why we have three files of pod_template_file examples,2
"Apply labels to Docker images in a single instruction (#12931)* Apply labels to Docker images in a single instructionWhile looking at the build logs for something else I noticed thisoddity at the end of the CI logs:```Tue, 08 Dec 2020 21:20:19 GMT Step 125/135 : LABEL org.apache.airflow.distro=""debian""...Tue, 08 Dec 2020 21:21:14 GMT Step 133/135 : LABEL org.apache.airflow.commitSha=${COMMIT_SHA}Tue, 08 Dec 2020 21:21:14 GMT  ---> Running in 1241a5f6cdb7Tue, 08 Dec 2020 21:21:21 GMT Removing intermediate container 1241a5f6cdb7```Applying all the labels took 1m2s! Hopefully applying these in a singlelayer/command should speed things up.A less extreme example still took 43s```Tue, 08 Dec 2020 20:44:40 GMT Step 125/135 : LABEL org.apache.airflow.distro=""debian""...Tue, 08 Dec 2020 20:45:18 GMT Step 133/135 : LABEL org.apache.airflow.commitSha=${COMMIT_SHA}Tue, 08 Dec 2020 20:45:18 GMT  ---> Running in dc601207dbcbTue, 08 Dec 2020 20:45:23 GMT Removing intermediate container dc601207dbcbTue, 08 Dec 2020 20:45:23 GMT  ---> 5aae5dd0f702```* Update Dockerfile",2
shorten name of hook re imports of provide_session and create_session (#12936),1
Fixed failed pylint in master (#12938),0
Fix typo in setup.cfg (#12935)```airflow.api.connextion.openaip=*.yaml```to```airflow.api_connexion.openapi=*.yaml```,5
Add explanation of the Presto Kerberos + SSL + snowflake problem (#12939),0
Enhanced TaskFlow API tutorial to use @dag decorator (#12937)Updated both the tutorial python file in the example_dags directoryand the tutorial documentation,2
"Fix typos/spelling in docs (#12942)When building the docs locally (not docker, cos I'm on linux and agrouch at times :grin:) I appear to be using a different backend toenchant to check spelling, and it noticed these two issues- spelling of yandex in extra-packages unrecognized- a smart quote instead of a backtick breaking a link",2
Adds documentation about custom providers. (#12921)Closes: #11429,1
Upgrading to Airflow 2.0 doc (#12930)This is based heavily on the UPGRADING_to_2.0.md file. The intent here isto make user facing and available as part of the Airflow documentation.This is still a work in progress.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Daniel Imberman <daniel.imberman@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Rename airflow.operators.dagrun_operator to airflow.operators.trigger_dagrun (#12933)Part of AIP-21Co-authored-by: Kishore Vancheeshwaran <24776049+kishvanchee@users.noreply.github.com>,1
Infer multiple outputs from dict annotations in TaskFlow API (#10349),5
Update theme to show hidden ToC in sidebar (#12949)Closes #12554,5
Fix return type in prev-date context variables (#12910),5
Clean up redirects.txt (#12948),5
Fix case of GitHub (#12950)Changed `Github` to `GitHub`,4
Update version to 2.0.0 (#12954),5
"Minor HTML fixes (#12477)- indent HTML code- add DOCTYPE- add lang=""en"" to the HTML start tag",1
Don't reference sphinx airflow theme via `@` URL in requirements. (#12957)PyPI rejects uploading a dist with an `@` in the requirements.,1
Remove unused pre-commit and Fix CI (#12964),0
Updates providers versions to 1.0.0 (#12955)Please enter the commit message for your changes. Lines starting,4
Update CI to run tests againt v2-0-test branch (#10891),3
Remove duplicate docs for check-hooks-apply pre-commit (#12973),1
Cleaned up formatting for Upgrading doc (#12977)Cleaned up some of the formatting in the document and also fixed a coupleof spelling errors.,0
"Allow all default roles to view Profile page + allow editing profile/resetting password for DB-ModelView. (#12971)This is a change discussed long time back in https://github.com/apache/airflow/pull/3889#issuecomment-507635839Essentially, the 7 permission-resource pairs are added for all users:- can_this_form_post on UserInfoEditView- can_this_form_get on UserInfoEditView- can_userinfo on UserDBModelView- userinfoedit on UserDBModelView- can_this_form_post on ResetMyPasswordView- can_this_form_get on ResetMyPasswordView- resetmypassword on UserDBModelViewIn addition, can_userinfo is added for all possible User ModelViews, so users can also view profile whenthe webserver is using different setting-ups.But they are ONLY allowed to edit profile and reset password when it's UserDBModelView",5
Fix command to filter package provider when building docs (#12984),2
Remove trailing back-tick from docs (#12986),2
Array declaration cannot be done inside function for DockerHub (#12967)For version of bash in DockerHub the arrays should be declaredoutside of functions to work.,1
Enhanced doc to cover ignoring rules and adding custom rules (#12974)Enhanced doc to cover ignoring rules and adding custom rulesEnhanced the upgrade-check documentation to cover how to ignore some of thepre=defined checks as well as how to add custom checks.,1
Purge deleted files from S3 bucket (#12947),2
"Update Dockerfile.ci (#12988)Fix permission issue in Azure DevOps when running the script install_mysql.sh, which prevents the build to succeed/bin/bash: ./scripts/docker/install_mysql.sh: Permission deniedThe command '/bin/bash -o pipefail -e -u -x -c ./scripts/docker/install_mysql.sh dev' returned a non-zero code: 126##[error]The command '/bin/bash -o pipefail -e -u -x -c ./scripts/docker/install_mysql.sh dev' returned a non-zero code: 126##[error]The process '/usr/bin/docker' failed with exit code 126",0
"Update Dockerfile (#12987)Fix permission issue in Azure DevOps when running the script install_mysql.sh, which prevents the build to succeed/bin/bash: ./scripts/docker/install_mysql.sh: Permission deniedThe command '/bin/bash -o pipefail -e -u -x -c ./scripts/docker/install_mysql.sh dev' returned a non-zero code: 126##[error]The command '/bin/bash -o pipefail -e -u -x -c ./scripts/docker/install_mysql.sh dev' returned a non-zero code: 126##[error]The process '/usr/bin/docker' failed with exit code 126",0
Download inventories only once (#12989),5
Promote new flags in ./docs/build_docs.py (#12991),2
"Add changes from 1.10.14 (#12993)This commit adds Changelog, Updating.md and replaces 1.10.13 to 1.10.14across the codebase",5
Less verbose output for docs build (#12994),2
Display progress for docs build (#13000),2
Trigger provider-yamls check on docs change (#12998),4
Fix failing master (#13001),0
Detect invalid package fiiters (#12996),5
Fix broken build of docs/ by removing unused import (#13007),2
Add asctime to spelling wordlist (#13010),1
Added database upgrade information to upgrade doc (#13005)Added information about db upgrade sample time to thedatabase upgrade section.,5
"Simplify release process for PyPI snapshots (#13020)Setuptools has a built in mechanism for adding a `suffix` to a version,so we don't have to edit a file and then remember to delete it!It's a little bit messy to get the suffix like this -- using sed tostrip out any leading digits or periods, but it's copy-pasteable thisway.",1
"Handle None values properly when CLI output is YAML/JSON format (#13024)str() should not be applied to None when we 'normalize' the data before we print it,otherwise it's not given in proper format in YAML/JSON format of the CLI output",5
Switching to standard --tag-build flag in setuptools in providers (#13021)Following #13020 provider packages are also switching to--tag-build flag.,1
"Adds customized_form_field_behaviours.schema.json to MANIFEST.in (#13031)The customized_form_field_behaviours.schema.json was present insetup.cfg but missing in MANIFEST.in. Result of it was thatwhile it was present in .whl package, it was missing from sdistpackage.Fixes: #13027",0
Bump ini from 1.3.5 to 1.3.8 in /airflow/www (#13030)Bumps [ini](https://github.com/isaacs/ini) from 1.3.5 to 1.3.8.- [Release notes](https://github.com/isaacs/ini/releases)- [Commits](https://github.com/isaacs/ini/compare/v1.3.5...v1.3.8)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Added information about currently supported Python versions (#13029)Added information to the Upgrading to 2.0 doc and the installation docto show the currently supported Python versions.,1
Add more links to navbar for production docs (#12953),2
"Changes release image preparation to use PyPI packages (#12990)* Changes release image preparation to use PyPI packagesSince we released all teh provider packages to PyPI now inRC version, we can now change the mechanism to prepare theproduction to use released packages in case of tagged builds.The ""branch"" production images are still prepared using theCI images and .whl packages built from sources, but therelease packages are built from officially released PyPIpackages.Also some corrections and updates were made to the release process:* the constraint tags when RC candidate is sent should contain  rcn suffix.* there was missing step about pushing the release tag once the  release is out* pushing tag to GitHub should be done after the PyPI packages  are uploaded, so that automated image building in DockerHub  can use those packages.* added a note that in case we will release some provider  packages that depend on the just released airflow version  they shoudl be released after airflow is in PyPI but before  the tag is pushed to GitHub (also to allow the image to be  build automatically from the released packages)Fixes: #12970* Update dev/README_RELEASE_AIRFLOW.mdCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update dev/README_RELEASE_AIRFLOW.mdCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",5
Update sqlalchemy_jsonfield to avoid pkgresources use (#13032)The previous version of sqlalchemy_jsonfield imported pkg_resourceswhich slowed down a lot of things. They have just released 1.0.0 withthat change.,4
Fix gpg verification command (#13035),0
"Install airflow and providers from dist and verifies them  (#13033)* Install airflow and providers from dist and verifies themThis check is there to prevent problems similar to those reportedin #13027 and fixed in #13031.Previously we always built airflow from wheels, only providers wereinstalled from sdist packages and tested. In this version bothairflow and providers are installed using the same package format(sdist or wheel).* Update scripts/in_container/entrypoint_ci.shCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Add Vidora to INTHEWILD.md (#13038),1
Fixes image building in DockerHub (#13039),2
🔒 Fix missing HTTPS on airflow site links (#13043),2
"Refactor plugins command output using AirflowConsole (#13036)This PR refactors the airflow plugins command to be compatible with'output' parameter which allows users to get output in form of table,json or yaml.",5
Add project_id to client inside BigQuery hook update_table method (#13018),5
Make AirflowJsonEncoder uses Flask's JSONEncoder as a base  (#13050)Flask before 2.0 (unreleased at time of writing) will prefer simplejson if it is installed.But unfortunately simplejson is not compatible with the stock JSONEncoder -- it alwayspasses an encoding argument. Changing the base class for our encoder to be what everFlask is using makes this more resilient.,1
Allows to install Airflow in Breeze from PIP with configurable extras (#13055)The extras configured by --extras Breeze switch are nowpassed to pip install command in case airfow is installed via--install-airflow-version or --install-airflow-reference switch.,4
Fix import from core to mysql provider in mysql example DAG (#13060),2
"Explicitly shutdown logging in tasks so concurrent.futures can be used (#13057)This fixes three problems:1. That remote logs weren't being uploaded due to the fork change2. That the S3 hook attempted to fetch credentials from the DB, but the   ORM had already been disposed.3. That even if forking was disabled, that S3 logs would fail due to use   of concurrent.futures. See https://bugs.python.org/issue33097",0
Bump version of sphinx-airflow-theme (#13054),5
Skip discovering snowflake provider in development mode (#13062)The snowflake provider when imported breaks other providersUntil https://github.com/apache/airflow/issues/12881 is fixedwe should skip discovering snowflake provider in development mode,1
Display version selector for production docs (#13051),2
Update stable version for published docs (#13052),2
Fetch inventories for third-party services only once (#13068)* Fetch inventories for third-party services only once* fixup! Fetch inventories for third-party services only once,0
Check for missing references to operator guides (#13059),1
Remove unneeded parentheses from Python file (#12968),2
Update INTHEWILD.md - Add Altafino (#13079),1
Fix failing pylint check on Master (#13078),0
Fix failing static check in master (#13082),0
Remove inapplicable arg 'output' for CLI pools import/export (#13071),2
Add question about PR to feature request template (#13087),1
Upgrading epoch to rebuild images after pyarrow upgrade (#13084)We've upgraded pyarrow release to 2.0.0 which seems to workwell with the upcoming pip 20.3.2 version. This PR is toautomatically rebuild our images to take the pyarrow constraintinto account.,1
Fix Code Coverage (#13092)Based on the suggesstions here:- https://community.codecov.io/t/reports-are-not-displayed-there-was-an-error-processing-coverage-reports/750- https://community.codecov.io/t/there-was-an-error-processing-coverage-reports-and-ci-not-detected/885/4it feels like the changes in this commit might fix the issue,0
Add script to generate integrations.json (#13073),5
Add link to PyPI Repository to provider docs (#13064),2
Add schema validation for config.yml (#13025)* Adding schema validation for config.yml* Update airflow/config_templates/config.yml.schema.jsonCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add identity pre-commit hook (#13089),1
"Move tests for airflow.utils.dates out of tests/core/ (#13088)These three functions were in test_core, but the separate test_datesfile is better suited.In addition I have removed the use of `assert_array_almost_equal` fromnumpy as pytest provides it's own version",1
Fix Data Catalog operators (#13096),1
Sort integrations.json by lowercase integration name (#13105),5
Rebase message before pulling now stands-out - it is yellow (#13104),5
Skip identity pre-commit in ci and always display it first (#13106),5
Annotate DagRun methods with return types (#11486),2
Update docs link in REST API spec (#13107),2
Add The Dyrt to INTHEWILD.md (#13098)* Add The Dyrt to INTHEWILD.md,1
Add Trade Republic to list of Airflow users (#13111)I changed the company. I am now working for Trade Republic.,1
Avoid confusion in doc for CeleryKubernetesExecutor (#13116)Make the doc around CeleryKubernetesExecutor clearer.,2
"For v2 target branches python 2.7 and 3.5 images are skipped (#13118)Previously we skipped building 2.7 and 3.5 for master branch butnow we have also v2-0-test, so it is better to skip theversions when branch is != v1-10-test (this is the onlyDEFAULT_BRANCH - even in v1-10-stable builds v1-10-test is usedas DEFAULT_BRANCH is v1-10-test.",3
Add guidelines for promoting Committers to PMC (#13090)This is based on the discussion on the mailing list and proposal from various PMC Members,1
"Cleans all DBs on ./breeze stop (#13119)When ./breeze stop is run, we run docker-compose down under thehood - by default with --volumes flag which also removes thevolumes. But the volumes were only defined when youselected the database.We want to clean up all the volumes on breeze stopin order to avoid surprizes when you switch the DB and findthe DB is there.Otherwise when you switch databases while they are runningstop will delete volumes for only the most recently useddatabase.The fix makes sure that all the dbvolumes are defined always so they are always all deleted on stop",4
Remove Kaxil's old record in INTHEWILD.md (#13123)I don't work at Data Reply anymore,5
Fix spelling (#13130),0
Update compatibility with google-cloud-os-login>=2.0.0 (#13126),2
Fix Google BigQueryHook method get_schema() (#13136)Co-authored-by: Manuel Bordes <manuel.bordes@gamesys.co.uk>,5
Add system tests for CloudKMSHook (#13122),1
Fix typo in provider name - Oracle (#13147),1
Fix typo in pip upgrade command :( (#13148),2
The default value in chart should be 2.0.0 (#13125),2
Fix spelling (#13135),0
Add Airflow 2.0.0 to requirements table (#13140),1
Fix Headings in CONTRIBUTING.rst (#13120),0
"Use generic information in UpdateMask component (#13146)The UpdateMask is used in connection, pools, variables and dag.So the docs should be more generic.",2
Bump datatables.net from 1.10.21 to 1.10.22 in /airflow/www (#13143)Bumps [datatables.net](https://github.com/DataTables/Dist-DataTables) from 1.10.21 to 1.10.22.- [Release notes](https://github.com/DataTables/Dist-DataTables/releases)- [Commits](https://github.com/DataTables/Dist-DataTables/compare/1.10.21...1.10.22)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Fix invalid provider name - Apache Kylin (#13157),1
Minor style fix in docs/extra-packages-ref (#13159),2
Add missing version information to recently added configs (#13161)These configs were added in 1.10.14 / 2.0.0,1
Remove duplicate pre-commit checks (#13167)- Sort INTHEWILD.md alphabetically- Lint Helm Chart,2
Sync Updating Guide with 2.0.0 (#13168)This commit sync Airflow 2.0 Updating Guide with Master i.e. removes alpha / beta mentions since it is now released,4
Replace deprecated dummy operator path in test_zip.zip (#13172)Replace deprecated path in `tests/dags/test_zip/test_zip.zip/test_zip.py`:```from airflow.operators.dummy_operator import DummyOperator```with```from airflow.operators.dummy import DummyOperator```,1
"Fix typos in TESTING.rst (#13169)Since TESTING.rst is not published on Apache Site, we don't run spell check on it and hence there were some typos introuduced without getting noticed.Time to fix them",0
Filter DagRuns with Task Instances in removed State while Scheduling (#13165)closes https://github.com/apache/airflow/issues/13151,0
"Stop sending Callback Requests if no callbacks are defined on DAG (#13163)If no on_*_callback are defined on DAG, Callbacks should not be registeredand sent to DAG Processor.This will reduce the KeyError mentioned in https://github.com/apache/airflow/issues/13047",0
"User werkzeug's own type conversion for request args (#13184)Werkzeug handles this exact behaviour for us, we don't need to do itourselves.https://github.com/pallets/werkzeug/blob/daeb2d0/src/werkzeug/datastructures.py#L259-L302",5
Display alternative tooltip when a Task has yet to run (no TI) (#13162),1
Use new logging options on values.yaml (#13173),2
Add documentation about webserver_config.py (#13155)Co-authored-by: Andre Amaral <aamaral@bionexo.com>,5
Update pylint to 2.6.0 (#13174),5
"Remove unused libraries - flask-swagger, funcsigs (#13178)",1
Fix brokend master (#13201),0
Fix parenthesis preventing Keda ScaledObject creation (#13183),1
Make function purpose clearer in example_kubernetes_executor example dag (#13216),2
Bump dompurify from 2.0.12 to 2.2.6 in /airflow/www (#13164)Bumps [dompurify](https://github.com/cure53/DOMPurify) from 2.0.12 to 2.2.6.- [Release notes](https://github.com/cure53/DOMPurify/releases)- [Commits](https://github.com/cure53/DOMPurify/compare/2.0.12...2.2.6)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Update chart readme to remove astronomer references (#13210),4
Add Mutt Data to INTHEWILD.md (#13227),5
Fix typos and minor simplification in TESTING.rst (#13194),3
Add regional support to dataproc workflow template operators (#12907)Workflow templates of GCP can be regional or global. In case ofregional the GCP API endpoint rpc url should match to the sameregion.In case of global templates needed to pass 'global' as region.It is not used for endpoint address but needed to as part oftemplate path.closes: #12804,1
Update minimum cattrs version (#13223)Fixes error on Python 3.9,0
"Fix QueuedLocalWorker crashing with EOFError (#13215)LocalExecutor uses a multiprocessing.Queue to distribute tasks to theinstances of QueuedLocalWorker. If for some reason LocalExecutor exits(e.g. because it encountered an unhandled exception), then each of theQueuedLocalWorker instances that it manages will also exit while tryingto read from the task queue.This obfuscates the root cause of the issue, i.e. that the LocalExecutorterminated. By catching EOFError, logging an error and exiting gracefullywe circumvent this issue.",0
Add note block to 2.x migration docs (#13094)closes: #13081,2
Support google-cloud-datacatalog>=1.0.0 (#13097),5
Update compatibility with google-cloud-kms>=2.0 (#13124),5
Support google-cloud-pubsub>=2.0.0 (#13127),1
specify constraint key type & drop auto fkey referred to users tables (#13239),1
"Reset PIP version after eager upgrade (#13251)PIP upgrades itself after eager update, and since we (for now)stick with the 20.2.4 version we want to reset PIP to thatversion after eager upgrade.",1
Flower should be enabled for CeleryKubernetesExecutor (#13248),0
Bump datatables JS to 1.10.23 (#13253)This was partially done in https://github.com/apache/airflow/pull/13143 but it kept datatables.net@1.10.21 in yarn.lock which causes the scanners to still detect vulnerability,1
skip entrypoint load if provider has already been loaded from local source (#13245),1
Add more operators to example DAGs for Cloud Tasks (#13235),2
Support google-cloud-redis>=2.0.0 (#13117),1
Fix link to Airflow master branch documentation (#13179),2
Describe which Python versions are supported. (#13259)* Describe what Python versions are supported.As the result of vote: https://s.apache.org/8epvx - we agreed tothe rules of supporting Python versions. This PR adds it toboth README and official documentation.* Update README.mdCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,2
Add Artelys to the list of Airflow users (#13261),1
"Allow PID file path to be relative when daemonize a process (scheduler, kerberos, etc) (#13232)Closes #13200.Currently, if the PID file path provided is relative,the process silently fails to launch.This fix ensures the PID path specified to bea normalized absolutized path eventually (expand with cwd),no matter the given value is relative or absolute.",0
Add airflow webserver URL into SLA miss email. (#13249),1
"Dispose connections when running tasks with os.fork & CeleryExecutor (#13265)Without this fix, when using CeleryExecutor and default config (i.e. `AIRFLOW__CORE__EXECUTE_TASKS_NEW_PYTHON_INTERPRETER=False`), tasks are run in fork and the pooled connections are shared to a forked process. This causes Celery tasks to hang infinitely (tasks will stay in queued state) with the following error:```[2020-12-22 18:49:39,085: WARNING/ForkPoolWorker-2] Failed to log action with (psycopg2.DatabaseError) error with status PGRES_TUPLES_OK and no message from the libpq```>It’s critical that when using a connection pool, and by extension when using an Engine created via create_engine(), that the pooled connections are not shared to a forked process.Sqlalchmey docs: https://docs.sqlalchemy.org/en/14/core/pooling.html#using-connection-pools-with-multiprocessing-or-os-fork",2
"UI: Add queued_by_job_id & external_executor_id Columns to TI View (#13266)While debugging one of issues with Celery, it would have helped if `queued_by_job_id` & `external_executor_id` were available to see in Webserver under Browse TaskInstances instead of running psql.",1
Disable suppress_logs_and_warning in cli when debugging (#13180)* Disable suppress_logs_and_warning in cli when debuggingIn some cases commands like 'dags list' can be used for debug purposes.The problem is that we are suppresing logs and warnings in some casesto make the output nice and clean. This commit disable this functionalityif logging level is set to DEBUG.* Add verbose flag* fixup! Add verbose flag,1
Default python version is used when building image (#13285)For image build the python version is passed viaPYTHON_MAJOR_MINOR_VERSION but there is a part of the build(preparing airflow package) that uses python installed on host.This is fine for Master/2.0 to use same version as the imagebut it should be unified (and in 1.10 when trying to build 2.7image it would fail).,0
Runs scheduled quarantine tests always (#13288)Fixes: #12909,0
Move kerberos auth backend tests (#13240),3
Compile assets automatically when needed at Breeze entry (#13292)We are storing md5 sum of all relevant files to know when we needto recompile the assets.Fixes #12262,0
"Correct commas on providers page, Q&A -> FAQ (#13294)This proposal corrects some comma placements in the providers documentation page. I also suggest renaming the Q&A section to FAQ, since that seems like it might be more fitting for describing the section. (This latter change is more an opinion so I don't mind if that's rejected 🙂 )",5
Rename second pylint pre-commit hook to distinguish it from first (#13303),1
Fix Flower network policy for CeleryKubernetesExecutor (#13301),1
add AzureDatalakeStorageDeleteOperator (#13206),5
Fix doci string in API entry. (#13300)Co-authored-by: aqua.wang <aqua.wang@ucloud.cn>,1
Add timeout option to gcs hook methods. (#13156),1
Add OracleToGCS Transfer (#13246),1
"Decode Remote Google Logs (#13115)* decode remote google logs before returningThe `Blob.download_as_string` function returns a byte which causethe log result to be displayed in a single line like:b""line1\nline2""instead ofline1line2added an isinstance check to make sure it doesn't break if itreturns string in some case and not others",4
"When CLI changes, we also re-run K8S tests (#13305)Since K8S tests use Airflow CLI (via Helm Chart) we shouldalso run the K8S tests when CLI changes.Fixes #12780",4
"Pass SchedulerJob.subdir to Dagbag (#13291)Because `SchedulerJob.subdir` was not used in Airflow 2.0, whenever SchedulerJob() would be initialized, it would serialize all the DAGs to the DB from settings.DAG_FOLDER.```root@b11b273fdffb:/opt/airflow# pytest tests/jobs/test_scheduler_job.py -k test_dag_file_processor_process_task_instances --durations=0Before: 9 passed, 120 deselected, 2 warnings in 22.11s =======================================================================================================After: 9 passed, 120 deselected, 2 warnings in 10.56s =======================================================================================================```",2
Pass image_pull_policy in KubernetesPodOperator correctly (#13289)* pass image_pull_policy to V1Containerimage_pull_policy is not being passed into the V1Container inKubernetesPodOperator. This commit fixes this.* add test for image_pull_policy not setimage_pull_policy should be IfNotPresent by default ifit's not set. The test ensure the correct value is passedto the V1Container object.,4
Validation of config is done at the very end (#13260)This prevents validation to be run before configuration isfully processed (for example before secret backends areinitialized)Fixes #13254,5
"Respect LogFormat when using ES logging with Json Format (#13310)This was a log standing bug / behaviour where Timestamps, log level,line number etc were not shown when using ElasticSearch Task Handler(Elasticsearch as remote logging) with json_format=True.",5
Installed providers are initialized in subshell (#13270)In DockerHub when non-release image was built the initializationwas leaving INSTALLED_PROVIDERS array as empty because thevalues were not appended to the existing array but the variablewas re-declared inside the function.This only happened when initialization was run as a subshell.,1
add system test for azure local to adls operator (#13190),1
Fix imagePullPolicy missing in tests (#13316)The #13289 fixed imagePullPolicy behaviour but broke master K8Stests. This PR attempts to fix it.,0
Updates IMAGES documentation to reflect Airflow 2.0 changes (#13312)Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,4
"Add pre-commit hook limiting hook name length (#13319)* When hook names are too long, pre-commit dispay becomes very ugly with many blank linesCo-authored-by: Daniel Standish <dstandish@techstyle.com>",1
"Rename PIP_VERSION to AIRFLOW_PIP_VERSION (#13320)Some older versions of PIP (including the one in dockerhub!) treatall env variables starting with PIP_ as a way to passoptions. Setting PIP_VERSION to 20.2.4 and exporting it causeserror ""ValueError: invalid truth value '20.2.4'"" because itdoes not have --version option and it treats it as --verbose¯\_(ツ)_/¯You can read more about it here:https://github.com/pypa/pip/issues/4528This PR renames the variable to avoid this side effect.",0
Fix typo in example (#13321)False should not be passed as a string,4
"Switch to Apache-owned GitHub actions (#13327)There was a change in Policy of ASF that only ""Made by GitHub""actions and actions residing in Apache-owned repositoriesare allowed to be used for ASF projects. This was inresponse to a security incident.More details:Policy:* https://infra.apache.org/github-actions-secrets.htmlDiscussion builds@apache.org:* https://lists.apache.org/thread.html/r435c45dfc28ec74e28314aa9db8a216a2b45ff7f27b15932035d3f65%40%3Cbuilds.apache.org%3EDiscussion users@infra.apache.org:* https://lists.apache.org/thread.html/r900f8f9a874006ed8121bdc901a0d1acccbb340882c1f94dad61a5e9%40%3Cusers.infra.apache.org%3E",5
Also add codecov action to apache airflow repo (#13328)Follow up after #13327,1
Vastly improves usability of CI logs (#13323)This change introduces improvements in the way logs are displayedin CI jobs and in amount of logs produced in general for CI jobsdue to much smarter cache usage.Logs in all CI jobs are now grouped in groups which are foldedby default when there is no error generated in such group. Similarsolution has been already used in docs job and it improvedboth readability and speed of loading of the logs in CI afterrecent improvements in Github UI (previously the speed of loadingthe logs was not improved by groups).Also cache usage has been reviewed and fixed in a number of placeswhich will result in much shorted setup times for static checksand kubernetes virtualenv but also far shorter logs generated bycache setup (we are using restore-keys feature that implementsincremental approach for cache building even if cache keys inGitHub Actions are immutable.,1
Prefer newer CLI syntax over legacy in helm chart (#13330)* saves approx 1 second and an error message when using >= 1.10.14Co-authored-by: Daniel Standish <dstandish@techstyle.com>,1
Fix the behavior for deactivate the authentication option and documenting the process to do it (#13191),2
Add missing sqlite provider for production image (#13332)The production image was missing sqlite provider (this failspip check),0
Support google-cloud-tasks>=2.0.0 (#13334),1
json-merge-patch becomes optional library and has looser restrictions (#13175),7
Re-enables verification of production image (#13329)The PROD image is now verified by several checks:* whether all expected providers are installed* whether pip-check shows no conflicts* whether imports are working for expected featuresPart of #13315,1
Print better error message when tests fail (#13339)The recently added log groupping hides error messages in casethere is an error in tests. You need to manually unfold last teststep which is somewhate hidden - it is followed by several'dump-container' logs.This change adds clear error message showing the exact loggroup that you need to unfold in case you want to look fora problem.,0
Add DataprocCreateWorkflowTemplateOperator (#13338)* Add DataprocCreateWorkflowTemplateOperator* fixup! Add DataprocCreateWorkflowTemplateOperator* fixup! fixup! Add DataprocCreateWorkflowTemplateOperator,5
"Revert ""Support google-cloud-tasks>=2.0.0 (#13334)"" (#13341)This reverts commit 1f712219fa8971d98bc486896603ce8109c42844.",4
"Adds missing LDAP ""extra"" dependencies to ldap provider. (#13308)It seems that for quite some time (1.10.4) the ""ldap"" extramissed python-ldap dependency.https://issues.apache.org/jira/browse/AIRFLOW-5261Also LDAP seems to be popular enough to be added as defaultextra in the production image.Fixes #13306",0
Refactored setup.py to better reflect changes in providers (#13314)This is a complete refactor of the setup.py providers/dependencies.It much better reflects the current setup where we have most ofthe extras 1-1 reflecting providers but also some extras that donot have their own providers.The pre-commits that were verifying setup versus documentationcan now be vastly simplified (no more need to parse thecomments so we can import setup.py variables directly ratherthan parse it via regexps. Also we can better categorize theextras - separate out (and verify) whether we correctlydescribed deprecated extras and to mark extras that installadditional providers as such.Fixes: #13309,0
Production image can also be upgraded to newer dependencies (#13345)Previously UPGRADE_TO_LATEST_CONSTRAINTS variable controlledwhether the CI image uses latest dependencies rather thanfixed constraints. This PR brings it also to PROD image.The name of the ARG is changed to UPGRADE_TO_NEWER_DEPENDENCIESas this corresponds better with the intention.,1
Add Fleek Fashion to the list of Airflow users (#13372)Add Fleek Fashion to INTHEWILD.md.,1
Fix Grammar in PIP warning (#13380)`might leads to errors` -> `might lead to errors`,0
Minor enhancements to Sensors docs (#13381)- Removed redundant comma- Used list-table so that modifications are easy- Added syntax highlighting for config code-block,5
Fix typo in Open API docs (#13374)`releaase` -> `release`,2
Bugfix: Sync Access Control defined in DAGs when running sync-perm (#13377)fixes https://github.com/apache/airflow/issues/13376,0
Fix broken link in PR Welcome message (#13386)https://github.com/apache/airflow/blob/master/docs/howto/custom-operator.rst no longer existsNew link: https://github.com/apache/airflow/blob/master/docs/apache-airflow/howto/custom-operator.rst,2
Add integration tests for Apache Pinot (#13195)* Add integration tests for Apache Pinot* fixup! Add integration tests for Apache Pinot* fixup! fixup! Add integration tests for Apache Pinot* fixup! fixup! fixup! Add integration tests for Apache Pinot* fixup! fixup! fixup! fixup! Add integration tests for Apache Pinot* Update setup.cfg,5
Fix Apache Airflow icon link in Helm Chart (#13387)Previous link (https://www.astronomer.io/static/airflowNewA.png) is broken.This commit uses link from official docs too instead of Astronomer.,2
Bump version to 2.1.0dev0 (#13382),5
Use 2.0.0 in Airflow docs & Breeze (#13379),2
"Disable persisting credentials in Github Action's checkout (#13389)This PR disables persisting credentials in Github Actions checkout.This is a result of discussion in builds@apache.orghttps://lists.apache.org/thread.html/r435c45dfc28ec74e28314aa9db8a216a2b45ff7f27b15932035d3f65%40%3Cbuilds.apache.org%3EIt turns out that contrary to the documentation actios (specificallycheckout action) can use GITHUB_TOKEN without specifying it asinput in the yaml file and the GitHub checkout actionleaves the repository with credentials stored locally thatenable pushing to Github Repository by any step in the samejob. This was thought to be forbidden initially (and thedocumentation clearly says that the action must have theGITHUB_TOKEN passed to it in .yaml workflow in order touse it). But apparently it behaves differently.This leaves open an attack vector where for exampleany PIP package installed in the following steps could pushany changes to GitHub Repository of Apache Airflow.Security incidents have been reported to both GitHub andApache Security team, but in the meantime we add configurationto remove credentials after checkout step.https://docs.github.com/en/free-pro-team@latest/actions/reference/authentication-in-a-workflow#using-the-github_token-in-a-workflow> Using the GITHUB_TOKEN in a workflow> To use the GITHUB_TOKEN secret, you *must* reference it in your workflow  file. Using a token might include passing the token as an input to an  action that requires it, or making authenticated GitHub API calls.",1
Improves documentation regarding providers and custom connections (#13375)Co-authored-by: Bijan <me+git@bijansoltani.com>,1
Fix mallformed table in production-deployment.rst (#13395),0
Simplify CeleryKubernetesExecutor tests (#13307)* Simplify CeleryKubernetesExecutor testsCo-authored-by: Daniel Standish <dstandish@techstyle.com>,3
Add Parquet data type to BaseSQLToGCSOperator (#13359),1
Update celery.rst to fix broken links. (#13400),2
Allow Tags on AWS Batch Job Submission (#13396)Co-authored-by: Derek Flionis <dflionis@tenable.com>,0
Update persists-credentials (#13401)Previous change to add persist-credentials #13389 wrongly addedpersists-credentials to python-setup rather than checkoutaction. Also one of the checkout actions used master rather thanv2 tag.,1
Limit old versions of pinotdb to force update on CI (#13402),5
Remove reference to scheduler run_duration param in docs (#13346)* links were old / dead* run_duration was removed from scheduler* clarify related notes on backward compat in helm chart valuesCo-authored-by: Daniel Standish <dstandish@techstyle.com>,2
Fixed broken aws test_batch_job tests introduced by #13396 (#13406),3
Add example DAG & how-to guide for sqlite (#13196),2
Support google-cloud-bigquery-datatransfer>=3.0.0 (#13337),5
"Warns politely, do not force run a long operation (#13313)* Warns politely, do not force run a long operation",1
Set minimum SQLite version supported. (#13412)* Set minimum SQLite version supported.Some users reported that some older versions of SQLite do notwork with Airflow 2.0. This happens for example with latestsqlite available by default on RHEL7 (sqlite version availablein fully updated system there is 7 (!) years old)Example of such issue: #13397.Not sure which 'minimum' version is supported butin the Breeze environment based on debian buster we have3.27.2 version in fully updated system. This shoudl be ourbaseline.* Update README.mdCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,2
"Adding documentation explaining ""strange"" URI required when using AWS… (#13355)",1
Enable interpretation of backslash escapes for colored message (#13418),0
Developers Quick Guide (#13417)rebased and updated new tmux image as per new changes.,4
Allow ./run_tmux.sh script to run standalone (#13420),1
Kuba openfaas sync call (#13356)* adds invoke_function() - synchronous call in addition to existing asynchronous call* adds invoke_function() - synchronous call in addition to existing asynchronous call* airflow faas upgrade* passing body consistently without json=body* removing return() statement that must be causing static checks to fail* removing return() statement that must be causing static checks to fail* resolving static check issues* resolving static check issues* resolving static check issues* resolving static check issues,0
Add last-commit example to static-check --help message. (#13411),1
Fix environment checking for Apache Pinot (#13419),0
Improves documentation regarding providers and custom connections 2 (#13410),1
"Change timeout s and disables reverse IP lookup for integrations (#13424)Seems that we are hitting more often one of the most favouritebugs by Ash: DNS. Quote: ""It's always DNS"".It looks like there is a race condition with docker composethat causes services that started fast enough (before DNS)to get a different reverse-DNS IP lookup (usually it isjust `<SERVICE>` but sometimes it is`<DOCKER_COMPOSE_APP>_<SERVICE>_1_<NETWORK>`).This produces misleading messages in log that mightmake analysis of such problems difficult, that's whywe chose to get rid of the reverse lookup and givebigger time for each service to check if it is ready.Netcat, unfortunately performs both forward and reverselookup when given a name - forward lookup to find theIP address and reverse lookup to write information to thelog about the host it connected to - and if it seesthat the original and reverse-looked-up names do not matcheven if it manages to connect, it retunrs an error:`DNS fwd/rev mismatch` - which is very misleading.This change performs the following:1) We lookup the host name in python via gethostbyname2) We set -n in netcat to disable ANY DNS use3) We feed netcat with the IP address4) We've standardized all waiting times to be up to 50 secondsThis way we should get rid of the DNS fwd/rev mismatch onceand for all.",1
Add verbose flag to ./build_docs.py (#13403),2
"Removes pip download when installing from local packages (#13422)This PR improves building production image from local packages,in preparation for moving provider requirements out of setup.cfg.Previously `pip download` step was executed in the CI scriptsin order to download all the packages that were needed. Howeverthis had two problems:1) PIP download was executed outside of Dockerfile in CI scripts   which means that any change to requirements there could not   be executed in 'workflow_run' event - because main branch version   of CI scripts is used there. We want to add extra requirements   when installing airflow so in order to be able to change   it, those requirements should be added in Dockerfile.   This will be done in the follow-up #13409 PR.2) Packages downloaded with PIP download have a ""file"" version   rather than regular == version when you run pip freeze/check.   This looks weird and while you can figure out the version   from file name, when you `pip install` them, they look   much more normal. The airflow package and provider package   will still get the ""file"" form but this is ok because we are   building those packages from sources and they are not yet   available in PyPI.Example:  adal==1.2.5  aiohttp==3.7.3  alembic==1.4.3  amqp==2.6.1  apache-airflow @ file:///docker-context-files/apache_airflow-2.1.0.dev0-py3-none-any.whl  apache-airflow-providers-amazon @ file:///docker-context-files/apache_airflow_providers_amazon-1.0.0-py3-none-any.whl  apache-airflow-providers-celery @ file:///docker-context-files/apache_airflow_providers_celery-1.0.0-py3-none-any.whl  ...With this PR, we do not `pip download` all packages, but insteadwe prepare airflow + providers packages as .whl files andinstall them from there (all the dependencies are installedfrom PyPI)",2
Add 'mongo_collection' to template_fields in MongoToS3Operator (#13361),1
Fixed failing pylint errors introduced in #13403 (#13429)This fixes a failing pylint error introduced in #13403. This erroralso trigger another pylint problem involved with c-extension,0
Generalize MLEngineStartTrainingJobOperator to custom images (#13318),5
Fix pylint issues - broken master (#13427),0
"Fix selective checks for changes outside of airflow .py files (#13430)When no airflow files change, selective tests only run basictests, but this is wrong, because many of .py files areoutside of the airflow folder.In this case we should enable image building because only thenfull set of static checks is executed.This bug caused for example #13403 to succeed even if it failedstatic checks after merge.",7
"Adds timeout to all curl commands (#13431)Curl has a sophisticated back-off mechanism when trying to connectand it causes sometimes that it hangs for a very long timewhen first few attempts to connect failed with a 'soft' error.Similarly, when curl starts transfer after connecting but theother party hanged, the client curl call might hang as well.This causes various problems for example sometimes waitig forimages in the ci build gets cancelled because curl commandto check for image fails - example:https://github.com/apache/airflow/pull/13413/checks?check_run_id=1635401914This change adds appropriate timeouts to all curl commands weuse in CI/manual operations. In many cases we implementedretry so the effect will be that those cases will stop happeningbut even in no-retry case, failing curl is better than hangs.",1
Add extras when installing prod image from packages (#13432)In the latest change #13422 change in the way product images areprepared removed extras from installed airflow - thus causedfailing production image verification check.This change restores extras when airflow is installed from packages,4
Improve style of code block on aws-ssm-parameter-store.rst (#13428),2
fixup! Adds timeout to all curl commands (#13431) (#13435),1
fixup! Fixed failing pylint errors introduced in #13403 (#13429) (#13437),0
Fix another pylint c-extension-no-member (#13438),0
Install airflow and providers together from context files (#13441)Airflow and provider packages need to be installed together tomake sure that constrainst are taken into account and that airflowdoes not get reinstalled from PyPI when eager upgrade runs.,1
Fix grammar in API docs (#13444)Fixes some minor grammatical issues,0
Replace deprecated decorator (#13443)`abc.abstractproperty` is deprecated since Python 3.3. Instead we should use `@property` with `abc.abstractmethod`.Docs: https://docs.python.org/3.6/library/abc.html#abc.abstractproperty,5
Fix few typos (#13450),2
Fix: Remove password if in LDAP or CUSTOM mode HiveServer2Hook (#11767)closes #11275,1
Streamline & simplify __eq__ methods in models Dag and BaseOperator (#13449)- Use getattr() instead of __dict__ as __dict__ doesn't return  correct values for properties.- Avoid unnecessary condition checks (the removed condition checks are covered by _comps),4
Add support for no-menu plugin views (#11742)- test: added test for no-menu view- doc: add info on no-menu plugin views- re-raise error if missing key is not name- treat empty string for name the same as missing keySigned-off-by: Shivansh Saini <shivanshs9@gmail.com>,5
"GitHub PROD image build is pushed to GitHub Registry. (#13442)One of the earlier changes removed unneccessary pulling of the'build' segment from GitHub Registry in `ci_wait_for_prod_image.sh'.It is not needed, because the K8S tests only use the final imageand pulling the build image is not necessary.This has the undesired effect that 'ci_wait_for_prod_image.sh' wasalso used at the step where master image pushes the prod imageto the repository as the 'latest' cache. In this case both - the'final' prod image and the 'build' segment should be pulled,because both are needed for PROD image build optimizations.",1
Fix insert_all method of BigQueryHook to support tables without schema (#13138)Fixes the insert_all method of BigQueryHook to support tables that does not have provided schema.Co-authored-by: Manuel Bordes <manuel.bordes@gamesys.co.uk>,5
Replace tests-only dependency - tzlocal (#13413),3
"Additional properties should be allowed in provider schema (#13440)The additional properties should be allowed in provider schema,otherwise future version of providers will not be compatible witholder versions of Airflow.Specifying 'additionalProperties' as allowed we are opening up toadding more properties to provider.yaml.This change fixes this is for now by removing extra fieldsadded since the Airlow 2.0.0 schema and verifying that the 2.0.0schema correctly validates such modified dictionary.In the future we might deprecate 2.0.0 and add >=2.0.1 limitationto the provider packages in which case we will be able to removethis modification of the provider_info dict.Also added additional test for provider packages whether theyinstall on Airflow 2.0.0. This tests might remain even after thedeprecation of 2.0.0 - we can just move it to 2.0.1. However thiswill give us much bigger confidence that the providers willcontinue work even for older versions of Airflow 2.0.We might have to modify that test and only include the providersthat are backwards-compatible, in case we have some providersthat depend on future Airflow versions. For now we assumeall providers should be installable from master on 2.0.0.",1
Add docker health check to integrations (#13446),2
Fix installation doc (#13462)The note should not be in the Bash code-block,2
Remove unused dependency - contextdecorator (#13455),1
Support google-cloud-datacatalog 3.0.0 (#13224),5
Log migrations info in consisten way (#13458)Resource based permissions migration changes logging handlersso each next migration is differently formatted when doingairflow db reset. This commit fixes this behavior.closes: #13214,0
"Rewrite handwritten argument parser in prepare_provider_packages.py (#13234)* Rewrite handwritten argument parser in prepare_provider_packages.py- Replaced sys.argv manipulation with argparse.- Replaced positional argument for PACKAGE with optional argument.Issue : 13069To be reviewed by : Kamil, Jarek.* Modified help text for prepare_provider_packages as suggested by Kamil.Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>* Moved each CLI subcommand in prepare_provider_packages.py to a separate function for modularity and code cleanup.Signed-off-by: Debodirno Chandra <debodirnochandra@gmail.com>",4
Remove redundant word (#13466)`for for` -> `for`,4
Upgrade pre-commit hooks (#13465)`doctoc` -> 1.4 to 2.0`pre-commit-hooks` -> 3.3 to 3.4,1
Fix typo in TaskGroup docstrings (#13475)`prerfix_group_id` -> `prefix_group_id`,0
Replace dictionary creation with dictionary literal (#13474)Instead of:```ctx = {}ctx['filename'] = filename```we can use:```ctx = {'filename': filename}```,2
Remove 'typing' dependency (#13472)We only needed `typing` for Python < 3.6 and Airflow 2.0 and Master are only Py 3.6+,4
"Revert ""Support google-cloud-datacatalog 3.0.0 (#13224)"" (#13482)This reverts commit feb84057d34b2f64e3b5dcbaae2d3b18f5f564e4.",4
Replace deprecated module and operator in example_tasks.py (#13473)- `from airflow.operators.bash_operator import BashOperator` to `from airflow.operators.bash import BashOperator`- `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`,2
Update supported Python version in LOCAL_VIRTUALENV.rst (#13468)We no longer support 3.5. We do support 3.7 and 3.8 in addition to 3.6,1
"Fix webserver ingress annotations (#12619)The indentation under `web.annotations` was wrong (6 leading spaceson first line, 4 on the rest) leading to    Error converting YAMl to JSON: yaml: line 32: did not find expected keywhen you run helm chart with value `ingres.web.annotations`",2
Jeremiah Lowin has resigned from the Airflow project (#13486),5
Add Vikram in the Airflow Committer's list (#13489),1
"Fix failing backport packages test (#13497)In #13473 - I updated the deprecated packages but looks like it broke backport packages:```    File ""/usr/local/lib/python3.6/site-packages/airflow/providers/google/cloud/example_dags/example_tasks.py"", line 32, in <module>      from airflow.models.baseoperator import chain  ImportError: cannot import name 'chain'```",2
"Check for minimum version of Sqlite (#13496)Some users testing Airlfow 2.0 with sqlite noticed that forold versions of sqlite, Airflow does not run tasks and fails with'sqlite3.OperationalError: near "","": syntax error' when runningtasks. More details about it in #13397.Bisecting had shown that minimum supported version of sqlite is3.15.0, therefore this PR adds checking if sqlite version ishigher than that and fails hard if it is not.Documentation has been updated with minimum requirements, someinconsisttencies have been removed, also the minimum requirementsfor stable 2.0 version were moved to installation.rst becausethe requirements were never explicitely stated in the user-facingdocumentation.",2
Chart: Add custom_airflow_environment to flower container (#12630),1
Add S3KeySizeSensor (#13049)S3KeySizeSensor allows checking the S3 object sizes or perform any other actions needed by users.,1
Add docs about mocking variables and connections (#13502),2
Add How-To guide for PostgresOperator (#13281)closes: #11917,1
Fix S3KeysUnchangedSensor so that template_fields work (#13490)- Add a test for rendering template_fields- Rename the instance variable 'bucket' to 'bucket_name'closes #13481,3
Add comprehensive tests for pod launcher role in helm chart (#13302)Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>,1
Fix Azure Data Explorer Operator (#13520)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Add DAG Description Doc to Trigger UI Page (#13365)closes #12550,2
Add docs about Flask CLI (#13500),2
upgrade mysql-connector-python to 8.0.22 (#13370),5
[AIRFLOW-3723] Add Gzip capability to mongo_to_S3 operator (#13187)Adding the gzip ability to the hook and then the mongo_to_S3 using itfrom the hook.,1
Update chart README with section on Airflow configuration (#13519),5
"When ""full tests needed"" label is present, run all tests (#13538)This label can be set manually, and the expectation is that it would runall tests, but in some cases for PRs that don't touch code, it wouldstill skip most of the matrix and run minimal static checks only.This changes the behaviour so that if the label is found it will run alltests no matter what files have been changed -- making the label namemore true :)",1
Add python-daemon limit for python 3.8+ (#13540)Related to #11456. Python daemon crashes with 'socket operation onnon-socket' for versions < 2.2.4.The issue is https://pagure.io/python-daemon/issue/34The fix is: https://pagure.io/python-daemon/c/b708912And it's been released in 2.2.4 version:https://pagure.io/python-daemon/blob/55ea71020486ea299ecbc03aec8e7f9dca2f5a85/f/ChangeLogThis PR adds limitation for python 3.8 to requirepython-daemon at least 2.2.4.,1
"Remove thrift as a core dependency (#13471)`thrift` is a dependency for Apache Hive and it is not required by Core Airflow:```airflow/providers/apache/hive/hooks/hive.py:489:        # This is for pickling to work despite the thrift hive client notairflow/providers/apache/hive/hooks/hive.py:500:        """"""Returns a Hive thrift client.""""""airflow/providers/apache/hive/hooks/hive.py:502:        from thrift.protocol import TBinaryProtocolairflow/providers/apache/hive/hooks/hive.py:503:        from thrift.transport import TSocket, TTransportairflow/providers/apache/hive/hooks/hive.py:531:            from thrift_sasl import TSaslClientTransportairflow/providers/apache/hive/sensors/hive_partition.py:41:    :param metastore_conn_id: reference to the metastore thrift serviceairflow/providers/apache/hive/sensors/metastore_partition.py:28:    queries generated by the Metastore thrift service when hittingairflow/providers/apache/hive/sensors/named_hive_partition.py:36:    :param metastore_conn_id: reference to the metastore thrift service```",2
Change the host in the example of using the API auth backend (#13548),1
Fix known-hosts volume name (#13457),0
Separate nodeSelector logic of chart (#13508),2
Replace deprecated module and operator in example_tasks.py (#13527)- `from airflow.utils.helpers import chain` to `from airflow.models.baseoperator import chain`This commit also adds Bowler refactor for backport packages,4
"More verbose and less frequent image poll (#13555)I noticed that in a number of cases the image poll in waiting forCI images takes a realy long time and appears to be hanging.In order to investigated the root cause of this this PR increasesthe verbosity of that step by adding ""Still waiting"" inevery loop as well as prints output of the potentially failedcurl command.",0
"Add NotFound response for DELETE methods in OpenAPI YAML (#13550)NotFound (404) is a valid possible responsefor 'Delete a Connection/Dag Run/Variable',but they were missed in the OpenAPI Doc YAML.",2
"The check for image is now more robust (#13556)The request to get manifest of the image in GitHub Packagesmight randomly return either schma 1 response:{     ""schemaVersion"": 1,     ""name"": ""apache/airflow/master-python3.6-ci"",     ""tag"": ""470317521"",     ""architecture"": ""amd64"",     ""fsLayers"": [        {Or schema 2 response:{     ""schemaVersion"": 2,     ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",     ""config"": {        ""mediaType"": ""application/vnd.docker.container.image.v1+json"",        ""size"": 56952,        ""digest"": ""sha256:5c80e2ab289647802affafc5c1efc879fe4f5b559cb7a2a1215868e84b1d6424""     },In order to check image more reliably we check the status codeof the curl response instead.",5
Optimises prod image preparation in CI (#13557)The prod images require CI images to build provider packages.The optmisation implemented makes the CI images built only onceand the prod images will just pull the images rather than buildthem again.,1
[AIRFLOW-7044] Host key can be specified via SSH connection extras. (#12944),5
Change render to render_template in plugins.rst (#13560)Changing render to render_template as BaseView object has no attribute 'render'.,4
"Optimize wait for prod images to be run after ci images (#13562)This change slightly optimizes waiting for prod images. Waitingfor images necessarily takes one slot from jobs queue, even ifthose jobs do nothing at all. This is unfortunate, but seemsthere is no easy way to do it differently because GitHubActions does not support yet cross-workflow dependencies.This PR optimizes waiting for images in the way that PROD imagesand CI images waiting happen sequentially rather than in parallell.PROD images are, as of #13557 prepared sequentially whichmeans that there is no point to wait for PROD images in parallelto CI images.",1
"Forces unistalling providers in editable mode. (#13439)We cannot skip installing providers, but this causesproblems when installing airflow in editable mode, because providersare in two places - in airflow sources and in provider packages.This change removes installed provider packages when airflowis installed in editable mode to mitigate the problem.This way, there is no need to use INSTALL_PROVIDERS_FROM_SOURCESvariable when installing in editable mode.We still need to keep INSTALL_PROVIDERS_FROM_SOURCES for cases whennon-editable mode is used. In this way one can easily install curentversion of provider packages locally with pip install and havethe latest sources of both airflow and providers installed.Also INSTALL_PROVIDERS_FROM_SOURCES is particularly useful if youdevelop a new provider and reinstall airflow - because otherwiseit will try to install the provider from a non-existing package.This is why all regular CI jobs and Breeze haveINSTALL_PROVIDERS_FROM_SOURCES set by default.",1
Little changes in How To Rebase a PR (#13564),4
Fix capitalisation of boolean in config (#13569),5
Make docs clear that Auth can't be disabled for Stable API (#13568),2
"Removes provider-imposed requirements from setup.cfg (#13409)This change removes the provider-imposed requirements from theairflow's setup.cfg to additional configuration in thebreeze/CI scripts. This does not change constraint apprachwhen installing airflow, the constraints to those versionsremain as they were, but airflow package does not have tohave the limits in 'install_requires' section which makesit much more ""standalone.We can add more requirements there as needed or removethem when provider's dependencies change.Also thanks to using --upgrade-to-newer-dependencies flag inBreeze, the instructions on what to do when there isa problem with conflicting dependencies are much simpler.You do not need any more to set the label in PRto test how upgrade to newer dependencies will look like,you can test it yourself locally.This is a final step of making airflow package fullyindependent from the provider's dependencies.",1
"Adds information about PIP being the only official install tool (#13565)Installation with remote constraint files is supported with PIPand we are using it in order to maintain constistent set ofrequirements, however we keep getting issues from people usingpoetry and pip-tools who try to install airflow without followingthe official way.This chapter explains why and sets PIP as the only officialinstallation tool supported.",1
Fix extraVolumeMounts of scheduler (#13509)Fix extraVolumeMounts position error if Values.dags.gitSync.enabled is True,2
Add Guidelines to become an Airflow Committer (#13236),1
"Use `yaml.full_load_all` instead of `yaml.load_all` (#13577)Calling `yaml.load_all()` without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.`yaml.full_load_all` instead use load_all() with Fullloader",1
"Fix depcrecated K8S api (#13575)Fix this K8S client deprecation warnings```/usr/local/lib/python3.6/site-packages/kubernetes/client/apis/__init__.py:12: DeprecationWarning: The package kubernetes.client.apis is renamed and deprecated, use kubernetes.client.api instead (please note that the trailing s was removed).      DeprecationWarning```",2
Fix image and add airflow config for cleanup pods (#13576),4
"Stop Log Spamming when `[core] lazy_load_plugins` is False (#13578)* Stop Log Spamming when `[core] lazy_load_plugins` is FalseCurrently when `[core] lazy_load_plugins` is False, it spams logs with the following line:```Loading 1 plugin(s) took 0.79 seconds```Example```root@a20fe6919413:/usr/local/airflow# pythonPython 3.7.9 (default, Dec 11 2020, 14:53:17)[GCC 8.3.0] on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> from airflow import version[2021-01-08 20:20:51,730] {plugins_manager.py:286} INFO - Loading 1 plugin(s) took 0.79 seconds```",5
Reduce the number of variables in Bash scripts (#13572),5
Remove archived links from docs & add link for AIPs (#13580)This commits removes the follow archived links- https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Links- https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Homeand adds link to AIPs,2
Fix link for Committers Requirement in README.md (#13581)The current link points to list of Committers instead of requirements,1
Fix Formatting and link for Installing Backport Providers (#13582)- It was a wrongly formatter URL- The link was broken,2
Minor fixes in upgrading-to-2.rst (#13583)This commit fixes the tense since Airflow 2.0 is already released.Example:- Airflow 2.0.0 will require Python 3.6++ Airflow 2.0.0 requires Python 3.6,1
Fix Link in Upgrading to 2.0 guide (#13584)The link used Markdown syntax instead of rst,1
Minor grammar fix in OpenAPI YAML (#13586),0
"Support external Redis in Helm Chart (#12010)* Fix chart network policies definion and complianceFollowup to #12003:* Some network policies & ingress are not valid  against the jsonschema (empty values mostly)* Some network policies conditionnal definitions  were incorrect* Support external redis instance in helm chartThe main objective here is to support the useof an external redis instance in the helm chart.The values 'data.brokerUrl' and'data.brokerUrlSecretName' are added andtemplates are updated.This support is added with no breaking changes(hopefully); only the redis.brokerURLSecretNamevalue is removed, but it wasn't actually used inthe chart.Extensive tests for the redis related part of thischart are also added (including runtime checks onthe values).Docs also updated.Closes #11705",5
"Eager upgrade works also in editable mode. (#13589)The recent change #13409 introduced common installation scriptbut one case turned out to be not working. Installing Airflowin editable mode with eager upgrade fails on building productionimage (the CI image effectively did not have airflow installedin editable mode, which it should).This PR fixes that.",0
Add classic installation scripts for additional tools (#13587),1
Warn about precedence of env var when getting variables (#13501),1
Fix code typo in logging-tasks.rst (#13594),2
Salesforce provider requires tableau (#13593)Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>,1
Deprecate `tableau` extra (#13595)There was no separate provider for `tableau` and it's dependencieshave been already incorporated in ``salesforce`` one thereforethis change deprecates it in favour of ``salesforce``.,1
Display message and docs link when no plugins are loaded (#13599),2
Support google-cloud-datacatalog>=3.0.0 (#13534),5
Support google-cloud-automl >=2.1.0 (#13505),5
Improve a little readability (#13605)Improve a little readability,1
add xcom push for ECSOperator (#12096)This pushes the last cloudwatch event to xcom when do_xcom_push is TrueCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
Updates 'wheel' version to latest released (#13608),3
"Run ""third party"" github actions from submodules instead (#13514)Rather than having to mirror all the repos we can instead usegit submodules to pull in the third party actions we want to use - withrecent(ish) changes in review for submodules on GitHub we still get thesame ""review/audit"" visibility for changes, but this way we don't haveto either ""pollute"" our repo with the actions code, nor do we have tomaintain a fork of the third party action.",4
Fix ./breeze exec command - don't force run tmux (#13607),1
Fix rendering of CI.rst in Github (#13613)It was treating the `(x)` as a numbered list.,0
Fix version in upgrading-to-2.rst (#13611),0
"Have proper default for webserver.expose_config in Helm Chart (#13596)webserver.expose_config should have consistent default value withthe normal Airflow default settings (False).This helps encourage safer setting up in Helm context, also avoidaccidental exposure of config (if users don't carefully check the configbefore install the chart)",2
Support Python 3.6 in generate-integrations-json.py (#13610),5
"Introduces separate runtime provider schema (#13488)The provider.yaml contains more information that required atruntime (specifically about documentation building). Thosefields are not needed at runtime and their presence is optional.Also the runtime check for provider information should be morerelexed and allow for future compatibility (withadditional properties set to false). This way we can add new,optional fields to provider.yaml without worrying about breakingfuture-compatibility of providers with future airflow versions.This changei restores 'additionalProperties': false in themain, development-focused provider.yaml schema and introducednew runtime schema that is used to verify the provider info whenproviders are discovered by airflow.This 'runtime' version should change very rarely as change toadd a new required property in it breaks compatibility ofproviders with already released versions of Airflow.We also trim-down the provider.yaml file when preparing providerpackages to only contain those fields that are required in theruntime schema.",1
Change the default celery worker_concurrency to 16 (#13612)This change was unintentional -- https://github.com/apache/airflow/pull/7205That PR just changed it to work with breeze. Since we had `16` as default in 1.10.xand to get better performance and keep in line with `dag_concurrency` and`max_active_runs_per_dag` -- I think `16` makes more sense.,1
Add submodules in added workflow job (#13631)The most recent submodule change for actions #13514 was done inparallel to Optimising worklfows in #13562 and the job added inthe #13562 still uses non-submodule version of check action.Also few checkout steps missed:'submodules: recursive' inputThis PR fixes that and all 3rd-party actions now are usedfrom submodule.,1
Audit Log records View should not contain link if dag_id is None (#13619)closes https://github.com/apache/airflow/issues/13602,0
Allow customization of probes path and host (#12634),1
Fixed changed sequence of checkout/action (#13633)The #13631 was also problematic because checkout/actionsequence was left from the original sequence (but it needsto be reverted to work).This change fixes the sequence,0
"Providers are not installed from PyPI when testing local files (#13632)When testing locally generated providers, we should skip installingthem from PyPI because some providers might be added which arenot yet in PyPI.There was a mistake in copy&pasted workflow which caused thatthe providers have been first installed from PyPI and thenuninstalled, but in case of a new providers added, thiscaused a failure as newly added providers failed to installfrom PyPI.",0
Make the tooltip to Pause / Unpause a DAG clearer (#13642)closes https://github.com/apache/airflow/issues/13624,0
"Fix the docs for Transfer Operators (#13641)Currently instead of ""S3 to GCS"" it says ""S3 to S3""",1
Correct XCOM pickle advisory in UPDATING.md (#13639),5
"Fixes problems with extras for custom connection types (#13640)The custom providers with custom connections can defineextra widgets and fields, however there were problems withthose custom fields in Aiflow 2.0.0:* When connection type was a subset of another connection  type (for example jdbc and jdbcx) widgets from the  'subset' connection type appeared also in the 'superset'  one due to prefix matching in javascript.* Each connection when saved received 'full set' of extra  fields from other connection types (with empty values).  This problem is likely present in Airflow 1.10 but due  to limited number of connections supported it had no  real implications besides slightly bigger dictionary  stored in 'extra' field.* The extra field values were not saved for custom connections.  Only the predefined connection types could save extras in  extras field.This PR fixes it by:* adding __ matching for javascript to match only full connection  types not prefixes* saving only the fields matching extra__<conn_type> when the  connection is saved* removing filtering on 'known' connection types (the above  filtering on `extra__` results in empty extra for  connections that do not have any extra field defined.Fixes #13597",0
Add system tests for Stackdriver operators (#13644),1
Update external docs URL for Segment (#13645),2
Fix invalid continue_token for cleanup list pods (#13563),4
Add S3ToFTPOperator (#11747)Co-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>Co-authored-by: Tobiasz Kędzierski <tobiaszkedzierski@gmail.com>,1
Docs: Fix heading for Mocking section in best-practices.rst (#13658)Currently 'Mocking variables and connections' section is on same levelas Best Practices (`h1` heading):http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/best-practices.htmlBecause of which this heading shows in TOC,3
Add how to use custom operators within plugins folder (#13186)Co-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,1
Install providers from sources is disabled for prod image build (#13657)One of the earlier changes caused production image to also installproviders from sources.This resulted in warnings printed when webserver started for allthe providers that did not have dependencies installed.This is now disabled and no warnings are printed.,2
Fixed up entrypoint regex (#13652),1
Losser restriction for colorlog (#13176),2
"Emergency fix to manifest problem in Github Package Registry (#13669)Seems that one of our cache images got permanently broken viamanifest v1 vs. v2 problem.This manifests (!) itself in the way that images pushed(in this case it is docker.pkg.github.com/apache/airflow/master-python3.6-ci)gets more-or-less permanent problems with pushing new versionsto it:* manifest invalid: Only schema version 2 is supported* unknown blobSuch image becomes corrupted then. When you try to pushsuch image you get this error:Error response from daemon: mediaType in manifest should be 'application/vnd.docker.distribution.manifest.v2+json' not ''The permanent solution to the problem would be to switch toGithub Container Registry but we need Infrastructure enablingthis on a global level:https://issues.apache.org/jira/projects/INFRA/issues/INFRA-20959For now the solution is to add prefixes to the images cachedin the GitHub Registry, hoping that those new images willbe more stable - until we will be able to switch to theGithub Container Registry",1
Structure and small content improvements in installation.rst (#13661)* Structure improvments in installation.rst file* fixup! Structure improvments in installation.rst file,2
Add missing Dag Tag for Example DAGs (#13665)`example_dag_decorator` and `tutorial_taskflow_api_etl` were missing`example` dag tag. All the other example DAGs had it.This makes it consistent.,1
"Update outdated docs in scheduler_job.py (#13663)As part of Airflow 2.0.0 and Scheduler HA, we updated the logicof what happens in DagFileProcessor and SchedulerJob.This PR updates the docstrings to match the code.",2
Support google-cloud-tasks>=2.0.0 (#13347),1
Better fix for broken GitHub Image (#13673)The fix in #13669 was not complete and the name chosen as prefixwas a bad choice (it is same as v2-0 branch name). This changeimplements suffix rather than prefix.,0
"Increase the default ``min_file_process_interval`` to decrease CPU Usage (#13664)With the previous default of `0`, the CPU Usage mostly stays around 100.As in Airflow 2.0.0, the scheduling decisions have been moved out fromDagFileProcessor to Scheduler, we can keep this number high.closes https://github.com/apache/airflow/issues/13637",0
Add recipes for installing a few common tools in Docker image (#13655),2
Fix some typos in celery_kubernetes.rst (#13606)fix some typos,2
"Run openapi-generator as ""current"" user, not root. (#13674)This tool will happily run without the user existing in the passwdfiles, and by running as this user we don't have a lot of files owned byroot left hanging around.Additionally, only reset the _spec_ file, not the entire repo",2
"BugFix: Dag-level Callback Requests were not run (#13651)In https://github.com/apache/airflow/pull/13163 - I attempted to only runCallback requests when they are defined on DAG. But I just found outthat while we were storing the task-level callbacks as string in SerializedJSON, we were not storing DAG level callbacks and hence it default to Nonewhen the Serialized DAG was deserialized which meant that the DAG callbackswere not run.This PR fixes it, we don't need to store DAG level callbacks as string, aswe don't display them in the Webserver and the actual contents are not used anywherein the Scheduler itself. Scheduler just checks if the callbacks are defined and sendsit to DagFileProcessorProcess to run with the actual DAG file. So instead of storingthe actual callback as string which would have resulted in larger JSON blob, I haveadded properties to determine whether a callback is defined or not.(`dag.has_on_success_callback` and `dag.has_on_failure_callback`)Note: SLA callbacks don't have issue, as we currently check that SLAs are defined onany tasks are not, if yes, we send it to DagFileProcessorProcess which then executesthe SLA callback defined on DAG.",2
Add Neo4j hook and operator (#13324)Close: #12873,1
Configurable API response (CORS) headers (#13620)* Allow setting of API response (CORS) headers via config* Fix RST syntax* Register function to only API instead of all views in app* Add missing/required property* Update spelling dictionary,5
"Add JSON linter to DAG Trigger UI (#13551)* Add JSON linter to Variable/DAG Trigger UIsAdding codemirror and jshint to lint the text input for add/edit a Variable and for config when triggering a DAG.variable_add whitespaceRemove JSON linter for add/edit VariablesVariable values can be either plain text or json which makes linting more complicated and not worth it for now.* Add JSON linter to DAG Trigger UIAdding codemirror and jshint to lint the text input for config when triggering a DAG.variable_add whitespaceAdd JSON linter to Variable/DAG Trigger UIsAdding codemirror and jshint to lint the text input for add/edit a Variable and for config when triggering a DAG.variable_add whitespaceRemove JSON linter for add/edit VariablesVariable values can be either plain text or json which makes linting more complicated and not worth it for now.update trigger dag conf testFixed failing test by adding `id=""json""` to the  expected html in the `test_trigger_dag_params_conf` test",3
Bugfix: Return XCom Value in the XCom Endpoint API (#13684)* Bugfix: Return XCom Value in the XCom Endpoint APIcloses https://github.com/apache/airflow/issues/13676,0
Support tables in DAG docs (#13533),2
Update docs to register Operator Extra Links (#13683)closes https://github.com/apache/airflow/issues/13659* Update docs/apache-airflow/howto/define_extra_link.rstCo-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,2
Add verify_ssl config for kubernetes (#13516),5
"Stop creating duplicate Dag File Processors (#13662)When a dag file is executed via Dag File Processors and multiple callbacks arecreated either via zombies or executor events, the dag file is added tothe _file_path_queue and the manager will launch a new process toprocess it, which it should not since the dag file is currently underprocessing. This will bypass the _parallelism eventually especially whenit takes a long time to process some dag files and since self._processorsis just a dict with file path as the key. So multiple processors with the same keycount as one and hence parallelism is bypassed.This address the same issue as https://github.com/apache/airflow/pull/11875but instead does not exclude file paths that are recently processed and thatrun at the limit (which is only used in tests) when Callbacks are sent by theAgent. This is by design as the execution of Callbacks is critical. This is donewith a caveat to avoid duplicate processor -- i.e. if a processor exists,the file path is removed from the queue. This means that the processor withthe file path to run callback will be still run when the file path is added again in thenext loopTests are added to check the same.closes https://github.com/apache/airflow/issues/13047 closes https://github.com/apache/airflow/pull/11875",0
Perform case-independent sorting in pre_commit_sort_in_the_wild.sh (#13699),5
Improve formatting on run-with-upstart.rst (#13698),1
Improvements for database setup docs (#13696),2
"Make Scheduler livenessProbe HA-compatible (#13705)To support running with more than a single scheduler pod we can nolonger rely on `most_recent_job` -- as that would simply select the mostrecent row to have beaten, which could be from a different pod.Close: #13677 #12098",1
Add Missing Statsd Metrics in Docs (#13708)closes https://github.com/apache/airflow/issues/10091,0
Add Missing Email configs in Configuration doc (#13709)closes https://github.com/apache/airflow/issues/13697,0
"[CI] Docs should be built when config.yml is changed (#13710)The ""Build Docs"" job was not running when the config.yml file changed, example: https://github.com/apache/airflow/pull/13709It should run it as Configuration Reference page (https://airflow.apache.org/docs/apache-airflow/2.0.0/configurations-ref.html) is built using that file",2
Switches to latest version of snowflake connector (#13654)This should allow us to release a new version of snowflakeprovider that is not interacting with other providers viamonkeypatching of SSL classes.Fixes #12881,0
Add dependency to azure-core (#13715)Snowflake has implicit azure-core>=1.10.0 dependency because ituses AzureSASCredential via azure storage-blob.This dependency will be moved to Azure soon when we mergethe #12188 and azure-storage-blob will be added as dependency there,1
Add connection arguments in S3ToSnowflakeOperator (#12564)* Add connection arguments in S3ToSnowflakeOperator* delete database* add database* indentCo-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,5
Add EUIGS - Admiral Group to INTHEWILD.md (#13689)Co-authored-by: Emilio García Orellana <emilio.garcia@euigs.com>,1
Add open id dependency (#13714)* Adds python3-openid requirementSeems that python3-openid dependency is not properly solved by toolslike poetry (it is properly resolved by pip). The result isthat old version of python3-openid is installed when poetry isused and errors when initdb is run.While we do not use poetry as an official installation mechanismthis happens frequently enought and it is easy enough to fixthat we can add this dependency to make it easier forpoetry users.Related to #13711 #13558 #13149* Update setup.cfg,5
Fix bug in GCSToS3Operator (#13718),1
Fix static check on Master (#13721)#13714 broke the master,0
[Doc] Replace module path to Class with just Class Name (#13719)Instead of `the airflow.executors.sequential_executor.SequentialExecutor`just have `SequentialExecutor with the link to the actual class.,2
"Improve doc for 1.10.x support (#13720)- Remove duplicate information- Use specific time (6 months) instead of ""limited time""",1
Fix Production Deployment doc (#13723)Extra identation was causing issues & there was a dockerhub link that needed auth and likely broken,2
updated Google DV360 Hook to fix SDF issue (#13703)Co-authored-by: Sara Hamilton <sarahamilton@google.com>,0
Update pod-template-file.kubernetes-helm-yaml (#13686),2
Use plain asserts in tests. (#12951),3
"Gets rid of all the docker cli tools in Breeze. (#13731)* Gets rid of all the docker cli tools in Breeze.The docker cli tools caused more trouble than benefits.Replaced the last two (azure and aws) with installation to""/files"" directory so that they survive breeze restart.Also added --reinstall to installation command for all toolsso that it is easier to reinstall them.* Update .pre-commit-config.yaml",5
"Fix backfill crash on task retry or reschedule (#13712)When a retry happens, task key needs to be recorded with try number + 1to avoid KeyError exception.",0
"Disables provider's manager warning for source-installed prod image. (#13729)When production image is built for development purpose, by defaultit installs all providers from sources, but not all dependenciesare installed for all providers. Many providers require moredependencies and when you try to import those packages viaprovider's manager, they fail to import and print warnings.Those warnings are now turned into debug messages, in caseAIRFLOW_INSTALLATION_METHOD=""."", which is set whenproduction image is built locally from sources. This is helpfulespecially when you use locally build production image torun K8S tests - otherwise the logs are flooded withwarnings.This problem does not happe in CI, because there by defaultproduction image is built from locally prepared packagesand it does not contain sources from providers that are notinstalled via packages.",1
Refactor DataprocOperators to support google-cloud-dataproc 2.0 (#13256),5
Update DAG Serialization docs (#13722)- Updated the figure to show Scheduler uses Serialized DAGs (also added in https://cwiki.apache.org/confluence/display/AIRFLOW/Drawio+Diagrams)- And updated the description,5
"Fix S3ToSnowflakeOperator to support uploading all files in the specified stage (#12505)* Fix S3ToSnowflakeOperator to support uploading all files in the specified stageCurrently, users have to specify each file to upload asthe ""s3_keys"" parameter when using S3ToSnowflakeOperator.But the `COPY INTO` statement, which S3ToSnowflakeOperatorleverages internally, allows omitting this parameterso that users can upload whole files in the specified stage.https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html#syntaxThis PR makes S3ToSnowflakeOperator's s3_keys parameter optionalso as to support this functionality.",1
"Fix broken link on resolving conflicts (#13748)link was broken because it had an ""."" at the end. Closes #13746",1
"Setting `max_tis_per_query` to 0 now correctly removes the limit (#13512)This config setting is documented as 0==unlimited, but in my HAscheduler work I rewrote the code that used this and mistakenly didn'tkeep this behaviour.This re-introduces the correct behaviour and also adds a test so that itis stays working in the future.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Add __repr__ for Executors (#13753)Before:```python>>> from airflow.executors.local_executor import LocalExecutor>>> LocalExecutor()<airflow.executors.local_executor.LocalExecutor object at 0x7f49b47f8d68>```After:```python>>> from airflow.executors.local_executor import LocalExecutor>>> LocalExecutor()LocalExecutor(parallelism=32)```,2
Use the correct link for Apache Airflow Dockerhub repo (#13752)https://hub.docker.com/repository/docker/apache/airflow requires auth while  https://hub.docker.com/r/apache/airflow does not,2
"Fix race conditions in task callback invocations (#10917)This race condition resulted in task success and failure callbacks beingcalled more than once. Here is the order of events that could lead tothis issue:* task started running within process 2* (process 1) local_task_job checked for task return code, returns None* (process 2) task exited with failure state, task state updated as failed in DB* (process 2) task failure callback invoked through taskinstance.handle_failure method* (process 1) local_task_job heartbeat noticed task state set to  failure, mistoken it as state bing updated externally, also invoked task  failure callbackTo avoid this race condition, we need to make sure task callbacks areonly invoked within a single process.",1
"Update installation notes to warn against common problems. (#13727)We have recently seen a number of issues created by users whotried to install airflow with poetry or pip-tools or who hadsuccesses with using the latest pip 20.3.3. This change aimsto update the 'note' content and make sure installationinstructions are consistent everywhere, so that new usersare warned against using anything else than PIP and that theyare aware about potential problems with 'pip 20.3' and waysto mitigate the problems.This responds to the needs of confused users such asone in https://github.com/apache/airflow/issues/13711#issuecomment-761694781",0
"Increase timeouts for tests (#13756)We are getting close to the previous timeouts for tests and sometests are crossing the 80 minutes.While we should speed it up in general, for now increasingtimeouts should do the job.",1
Dividing contributors guide into expert and beginner parts (#13742),5
AllowDiskUse parameter and docs in MongotoS3Operator (#12033)Co-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>Co-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,1
Adds automated user creation in production image (#13728)* Adds automated user creation in the production imageThis PR implements automated user creation for the production imagecontrolled by environment variables.This is a solution for anyone who would like to make a quick testof the production image and would like to:* init/upgrade the DB automatically* create a userThis is particularly useful for internal SQLite db initializationbut can also be used to initialize the user in docker-composeor similar cases where there is no equivalent of init containersthat are usually used to perform the initialization.Closes #860,5
"Remove chmod +x for installation script for docker build. (#13772)We've introduced chmod a+x for installation scripts in Dockerfiles.but this turned out to be a bad idea. This was to accomodatebuilding on Azure Deveops which has filesystem that does notkeep executable bit. But the side-effect of it that thelayer of the script is invalidated when the permission is changedto +x on linux. The problem is that the script has locally (oncheckout) different permissions depending on umask setting.Therefore changing permissions for the image to +a is not best.Instead we are running the scripts with bash directly, which doesnot require changing of executable bit.",4
Fix webserver exiting when gunicorn master crashes (#13518)* Correct the logic for webserver choosing number of workers to spawn (#13469)A key consequence of this fix is that webserver will properlyexit when gunicorn master dies and stops responding to signals.,0
Add description to hint if conn_type is missing (#13778)- add plaintext description to add/edit conn_type to make sure people remember to install necessary provider packages,1
Change log level from debug to info when spawning new gunicorn workers (#13780),1
Add a new argument for HttpSensor to accept a list of http status code to Continue Poking (#13499)closes: #13451,1
Fix SQL syntax to check duplicate connections (#13783)closes https://github.com/apache/airflow/issues/13679,0
Use DAG context manager in examples (#13297),2
Add acl_policy to S3CopyObjectOperator (#13773)closes https://github.com/apache/airflow/issues/13774Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
Add missing logos for integrations (#13717)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
BaseBranchOperator will push to xcom by default. (#13704) (#13763)This change will BaseBranchOperator to do xcom push of the branch it choose to follow.It will also add support to use the do_xcom_push parameter.The added change returns the result received by running choose_branch().Closes: #13704,1
Fix Deprecation for configuration.getsection (#13804),5
Add How To Guide for Dataflow (#13461),5
Improve environment variables in GCS system test (#13792),3
"Adds capability of switching to Github Container Registry (#13726)* Adds capability of switching to Github Container RegistryCurrently we are using GitHub Packages to cache images for thebuild. GitHub Packages are ""legacy"" storage of binary artifactsfor GitHub and as of September 2020 they introduced GithubContainer Registry as more stable, easier to manage replacementfor container storage. It includes complete self-managementof the images including permission management, public access,retention management and many more.More about it here:https://github.blog/2020-09-01-introducing-github-container-registry/Recently we started to experience unstable behaviour of theGithub Packages ('unknown blob' and manifest v1 vs. v2 whenpushing images to it. So together with ASF we proposed toenable Github Container Registry and it happened as ofJanuary 2020.More about it in https://issues.apache.org/jira/browse/INFRA-20959We are currently in the testing phase, especially when itcomes to management of permissions - the model of permissionmangement is not the same for Container Registry as it wasfor GitHub Packages (it was per-repository in GitHub Packages,but it is organization-wide in the Container Registry.This PR introduces an option to use GitHub Container Registryrather than GitHub Packages. It is implemented in both - CIlevel and Breeze level allowing to seamlessly switch betweenthose two solutions:In Breeze (which we use to test pushing/pulling the images)--github-registry option was added with `ghcr.io` (Github ContainerRegistry) or `docker.pkg.github.com` (GitHub Packages).In CI the same can be achieved by setting GITHUB_REGISTRY value(same values possible as for --github-registry Breeze parameter)* fixup! Adds capability of switching to Github Container Registry",1
Add params to the DAG details endpoint (#13790),2
"Fix error with quick-failing tasks in KubernetesPodOperator (#13621)* Fix error with quick-failing tasks in KubernetesPodOperatorAddresses an issue with the KubernetesPodOperator where tasks that diequickly are not patched with ""already_checked"" because they never makeit to the monitoring logic.* static fix",0
"Fixed image separator for Github Package registry image (#13825)The #13726 introduced possibility of using Github ContainerRegistry, but for bulding from Package Registry there was amistake - only visible after merging - that introducedfailed master (naming of build image contained / rather than -)This PR fixes it.",0
"Revert ""Fix error with quick-failing tasks in KubernetesPodOperator (#13621)"" (#13835)This reverts commit 94d3ed61d60b134d649a4e9785b2d9c2a88cff05.Co-authored-by: Daniel Imberman <daniel.imberman@gmail.com>",4
Improve environment variables in GCP Lifeciences system test (#13834),3
Improve environment variables in GCP Memorystore system test (#13833),3
Improve environment variables in GCP Datafusion system test (#13837)It will help to parametrize system tests,3
"Allows for more than one warning in deprecation message (#13836)Sometimes in our tests we get more than one deprecationwarnings. It is likely caused by transitive warningsfrom importing other external libraries.In order to get rid of those side effects, we are nowaccepting more than one warning and we expect that at leastone of the warnings will come from the file being tested",3
Update information about branching strategy vs. production images (#13813)Some users were not aware that we are not relasing images from`stable` branch. This change clarifies branching strategy usedand what they can expect from the reference image published inDockerHub.,2
Improve environment variables in GCP Dataflow system test (#13841)It will help to parametrize system tests,3
Fix GCP Secret Manager system test (#13848),3
Fix Google Spanner example dag (#13842)* Fix Google Spanner example dagSome tasks requires upstreamWithout upstream they want perform operations on resources which does not exist yet* fixup! Fix Google Spanner example dag,2
Switch to f-strings using flynt. (#13732),1
Upgrade azure blob to v12 (#12188),5
Quarantine test that often fails (same as alredy quarantined no pid) (#13845),0
Fix PyPI spelling (#13864),0
Improve environment variables in GCP Secret Manager test (#13844),3
Improve BREEZE.rst docs (#13869),2
Fix link to Apache Airflow docs in webserver (#13250),2
Fix link and section in CONTRIBUTORS_QUICK_START.rst (#13868),2
Fix spellings (#13867),0
"Replace `google_cloud_storage_conn_id` by `gcp_conn_id` when using `GCSHook` (#13851)google_cloud_storage_conn_id parameter has been deprecated by GCSHook, and should be replaced by gcp_conn_id parameter. google_cloud_storage_conn_id was still in use in many Operators.GCSHook renders a DeprecationWarning message everytime one of those operators usesgoogle_cloud_storage_conn_id. This PR avoid triggering DeprecationWarning when using GCSHook in the codebase.Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>",1
Fix TaskNotFound in log endpoint (#13872),2
Fix Kerberos envs for workers in Helm Chart (#13828),2
Added Aviva Plc to INTHEWILD.md (#13875),1
"AWS Glue Crawler Integration (#13072)This change integrates an AWS glue crawler operator, hook and sensor that can be used to trigger glue crawlers from Airflow.Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>",1
"Removes files from docker-context-files if not used (#13830)In case docker-context files are not used during build, theyshoudl be cleaned just before the build to make sure thatdocker context does not contain extra files here. Otherwisefiles left from previous runs might be in the context and causecache invalidation if you are building the images locally.",5
Add extra field to get_connnection REST endpoint (#13885),1
Add ExasolToS3Operator (#13847)Add ExasolToS3Operator to allow exporting data from Exasol databaseto S3 buckets.Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
"Revert ""Removes files from docker-context-files if not used (#13830)"" (#13889)This reverts commit 31b956c6c22476d109c45c99d8a325c5c1e0fd45.Temporarily as it breaks on MacOS",4
Don't add User role perms to custom roles. (#13856)closes: #9245 #13511,1
Fix wrong parameter for drawDagStatsForDag() in dags.html (#13884),2
Make Smart Sensors DB Migration idempotent (#13892),5
Clarifies differences between extras and provider packages (#13810),1
Upgrade slack_sdk to v3 (#13745)Co-authored-by: Kamil Breguła <kamil.bregula@polidea.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
"Fix race condition when using Dynamic DAGs (#13893)closes https://github.com/apache/airflow/issues/13504Currently, the DagFileProcessor parses the DAG files, writes it to the`dag` table and then writes DAGs to `serialized_dag` table.At the same time, the scheduler loop is constantly looking for the nextDAGs to process based on ``next_dagrun_create_after`` column of the DAGtable.It might happen that as soon as the DagFileProcessor writes DAG to `dag`table, the scheduling loop in the Scheduler picks up the DAG for processing.However, as the DagFileProcessor has not written to serialized DAG table yetthe scheduler will error with ""Serialized Dag not Found"" error.This would mainly happen when the DAGs are dynamic where the result of one DAG,creates multiple DAGs.This commit changes the order of writing DAG and Serialized DAG and hencebefore a DAG is written to `dag` table it will be written to `serialized_dag` table.",2
Fix to ensure 100vh min plays nicely w/ Linux+Chrome (#13857),0
"Improve the error when DAG does not exist when running dag pause command (#13900)When running `airflow dags unpause` with a DAG that does not exist, itcurrently shows this error```root@6f086ba87198:/opt/airflow# airflow dags unpause example_bash_operatoreddTraceback (most recent call last):  File ""/usr/local/bin/airflow"", line 33, in <module>    sys.exit(load_entry_point('apache-airflow', 'console_scripts', 'airflow')())  File ""/opt/airflow/airflow/__main__.py"", line 40, in main    args.func(args)  File ""/opt/airflow/airflow/cli/cli_parser.py"", line 48, in command    return func(*args, **kwargs)  File ""/opt/airflow/airflow/utils/cli.py"", line 92, in wrapper    return f(*args, **kwargs)  File ""/opt/airflow/airflow/cli/commands/dag_command.py"", line 160, in dag_unpause    set_is_paused(False, args)  File ""/opt/airflow/airflow/cli/commands/dag_command.py"", line 170, in set_is_paused    dag.set_is_paused(is_paused=is_paused)AttributeError: 'NoneType' object has no attribute 'set_is_paused'```This commit changes the error to show a helpful error:```root@6f086ba87198:/opt/airflow# airflow dags unpause example_bash_operatoreddDAG: example_bash_operatoredd does not exit in 'dag' table```",2
Fix typo in CLI error (#13913),0
Fix db shell for sqlite (#13907)closes: #12806,5
Add quick start for Airflow on Docker (#13660)Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
"Only compare updated time when Serialized DAG exists (#13899)closes https://github.com/apache/airflow/issues/13667The following error happens when Serialized DAGs exist in Webserver or Scheduler but it has just been removed from serialized_dag table,mainly due to the removal of DAG file.```Traceback (most recent call last):  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py"", line 1275, in _execute    self._run_scheduler_loop()  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py"", line 1377, in _run_scheduler_loop    num_queued_tis = self._do_scheduling(session)  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py"", line 1516, in _do_scheduling    self._schedule_dag_run(dag_run, active_runs_by_dag_id.get(dag_run.dag_id, set()), session)  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py"", line 1629, in _schedule_dag_run    dag = dag_run.dag = self.dagbag.get_dag(dag_run.dag_id, session=session)  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/utils/session.py"", line 62, in wrapper    return func(*args, **kwargs)  File ""/home/app/.pyenv/versions/3.8.1/envs/airflow-py381/lib/python3.8/site-packages/airflow/models/dagbag.py"", line 187, in get_dag    if sd_last_updated_datetime > self.dags_last_fetched[dag_id]```A simple fix is to just check if `sd_last_updated_datetime` is not `None` i.e. Serialized DAG for that dag_id is not None",2
Fix docker-compose command to initialize the environment (#13914),5
Add env variables to PubSub example dag (#13794)Add env variables to PubSub example dag to help parametrize system tests,3
Fix and improve GCP BigTable hook and system test (#13896)Improve environment variables in GCP BigTable system test.It will help to parametrize system tests.,3
Add deprecated config options to docs (#13883)closes: #12772,2
"Chart: Fix cannot list resource pods/log (#13630)When attempting to view the logs in kube I get```*** Trying to get logs (last 100 lines) from worker pod  ****** Unable to fetch logs from worker pod  ***(403)Reason: ForbiddenHTTP response headers: HTTPHeaderDict({'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 12 Jan 2021 09:03:18 GMT', 'Content-Length': '296'})HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:airflow:airflow-webserver\\"" cannot list resource \\""pods/log\\"" in API group \\""\\"" in the namespace \\""airflow\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'```",2
"Fix dag run type enum query for mysqldb driver (#13278)By default sqlalchemy pass query params as is to db dialect drivers forquery execution. This causes inconsistent behavior of query paramevaluation between different db drivers. For example, MySQLdb willconvert `DagRunType.SCHEDULED` to string `'DagRunType.SCHEDULED'`instead of string `'scheduled'`.see https://github.com/apache/airflow/pull/11621 for relevantdiscussions.",2
Add authentication to lineage endpoint for experimental API (#13870),1
"Update ``airflow_local_settings.py`` to fix an error message (#13927)Fix error message, no functional change: remote_base_log_folder is now in the logging section, not in core.",2
Update Mongodb inventory URL to fix docs build (#13939)Master is failing on docs build because of the following error:```Failed to fetch inventory: https://api.mongodb.com/python/current/objects.inv```This is because the URL is changed to https://pymongo.readthedocs.io/en/stable/objects.inv,2
"Updates Oracle.rst documentation (#13871)Resolves Issue #10186 (Move Tips & Tricks for Oracle shops should be moved to Airflow Docs)Fixes broken link, add UI connection documentation link, and connection tips.",2
Don't add Website.can_read access to default roles. (#13923)related: #13856,1
Dropbox uses Airflow (#13956),1
Fix link in INTHEWILD.md (#13958)Dropbox link fix,0
"Add Google Cloud Workflows Operators (#13366)Add Google Cloud Workflows Operators, system test, example and sensorCo-authored-by: Tobiasz Kędzierski <tobiasz.kedzierski@polidea.com>",3
Fix BQ hook system test (#13944),3
Improve GCS system test envs (#13946)The same bucket cannot be used as source and destination,1
Add information about all access methods to the environment (#13940),5
Use template strings for string concatenation in JS code in dags.html (#13957)- use template strings over '+'- fix bugs caused by '+' concatenation,1
Fix reading from zip package to default to text (#13962),0
"Fix invalid value error caused by long k8s pod name (#13299)K8S pod names follows DNS_SUBDOMAIN naming convention, which can bebroken down into one or more DNS_LABEL separated by `.`.While the max length of pod name (DNS_SUBDOMAIN) is 253, each labelcomponent (DNS_LABEL) of a the name cannot be longer than 63. Pod namesgenerated by k8s executor right now only contains one label, which meansthe total effective name length cannot be greater than 63.This patch concats uuid to pod_id using `.` to generate the pod anem,thus extending the max name length to 63 + len(uuid).Reference: https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/design/identifiers.mdRelevant discussion: https://github.com/kubernetes/kubernetes/issues/79351#issuecomment-505228196",0
Only allow passing JSON Serializable conf to TriggerDagRunOperator (#13964)closes https://github.com/apache/airflow/issues/13414,0
Add bucket_name to template fileds in S3 operators (#13973)Without that it's impossible to create buckets using for exampleexecution date. And that is quite common case.,5
Docs: Fix FAQ on scheduler latency (#13969),0
"Add test for Public role permissions. (#13965)In #13923, all permissions were removed from the Public role. This adds a test to ensure that the default public role doesn't have any permissions.related: #13923",3
"Revert ""Fix reading from zip package to default to text (#13962)"" (#13977)This reverts commit cc70227d9be25be5ce95c0dae4383c3883819d68.",4
Bugfix: Allow getting details of a DAG with null start_date (REST API) (#13959),5
Fix typo in ``NotPreviouslySkippedDep`` (#13933),2
Updated taskflow api doc to show dependency with sensor (#13968)* Updated taskflow api doc to show dependency with sensorUpdated the taskflow api tutorial document to show how to setup adependency to a python-based decorated task from a classicFileSensor task.,2
Fix DB Migration for SQLite to upgrade to 2.0 (#13921)closes https://github.com/apache/airflow/issues/13877,0
Added Nutanix to the list of companies using Apache Airflow (#13978),1
Added a FAQ section to the Upgrading to 2 doc (#13979)Added a FAQ question to the Upgrading to 2 doc and added an initialquestion and answer around needing providers to be installed beforeconnection types show up in the UI.,1
Bugfix: Manual DagRun trigger should not skip scheduled runs (#13963)closes https://github.com/apache/airflow/issues/13434,0
Fix decode error for ssh log (#13943),2
"Deprecate email credentials from environment variables. (#13601)Email backends fetch credentials from environment variables, but othercredentials are typically stored in connections. This patch deprecatesemail credentials from environment variables and checks connectionsfirst. We can drop the environment variable fallback in a futurerelease.",4
Fix error on DockerSwarmOperator with auto_remove True (#13532) (#13852)* Fix error on DockerSwarmOperator with auto_remove TrueThis change will solve an issue with DockerSwarmOperator tasks ended correctlywith auto_remove setted to True. The docker service is removed after a status check.If the status is'failed' and the flag is True the service is removed correctly.,4
Stop loading Extra Operator links in Scheduler (#13932)closes #13099,2
Add DANA Indonesia to the Companies uses Airflow (#13995)* Add DANA Indonesia to the Companies uses Airflow* Fix sorting order,0
Fix docs url in tests (#13992)https://github.com/apache/airflow/pull/13250 missed a case to updateURL in `tests/www/test_views.py`.,3
Fix ./breeze exec command error: /bin/bash: -c: option requires an argument (#13998),1
Remove failed DockerOperator tasks with auto_remove=True (#13532) (#13993)* Remove failed DockerOperator tasks with auto_remove=TrueRemoves exited containers if a task based on DockerOperator failswith StatusCode!=0 and auto_remove=True,4
Add aws ses email backend for use with EmailOperator. (#13986),1
Update wording in upgrading documentation (#13862)* Update wording in upgrading documentation* Update docs/apache-airflow/upgrading-to-2.rstCo-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>* Update docs/apache-airflow/upgrading-to-2.rstCo-authored-by: Vikram Koka <vikram@astronomer.io>Co-authored-by: Tomek Urbaszek <tomasz.urbaszek@polidea.com>Co-authored-by: Vikram Koka <vikram@astronomer.io>,2
Added missing return parameter in read function of FileTaskHandler (#14001)this issue ouccurs when invalid try_number value is passed in get logs apiFIXES: #13638,0
Fixed reading from zip package to default to text. (#13984)* Fixed reading from zip package to default to text.* Fixed open_maybe_zip unit test to account for io.TextIOWrapper.,3
Fix structure and typo in Updating.md (#14005),5
"Implement provider versioning tools (#13767)This change implements per-provider versioning tools. Version of theproviders is retrieved from provider.yaml file (top-level verion).Documentation is generated in the documentation folder rather thanin sources and embedded in provider's index. Backport providersremain as they were until we delete all the backport references inApril 2021 nd then the code can be simplified and thebackport functionality can be removed then.When generating multiple providers, only those that have versionthat has no corresponding `providers-<PROVIDER>/<VERSION>` aregenerated. Other providers are skipped with warnings.Old documentation is removed and new CHANGELOG.rst have beenprepared for all providers to accomodate to the new process(which is comming as a follow-up commit)Fixes: #13272, #13271, #13274, #13276, #13277, #13275, #13273",0
Add missing space between label and value (#14008),1
"Bugfix: Don't try to create a duplicate Dag Run in Scheduler (#13920)closes https://github.com/apache/airflow/issues/13685When the Scheduler is restarted or killed after creating Dag Run in `Scheduler._create_dag_runs` butbefore `Scheduler.self._update_dag_next_dagruns`, the Scheduler falls in a loop because it will not tryto create the Dag Run again in the Scheduler Loop. However, as the DagRun already exists it will failwith:```Traceback (most recent call last):  File ""/Users/kaxilnaik/opt/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context    cursor, statement, parameters, context  File ""/Users/kaxilnaik/opt/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute    cursor.execute(statement, parameters)psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint ""dag_run_dag_id_run_id_key""DETAIL:  Key (dag_id, run_id)=(scenario1_case2_02, scheduled__2021-01-25T00:00:00+00:00) already exists.```",1
Support google-cloud-monitoring>=2.0.0 (#13769),1
Make v1/config endpoint respect webserver expose_config setting (#14020),1
"Remove use of repeated constant in AirflowConfigParser (#14023)Small refactor that doesn't change any functionality, but does makefuture testing/refactoring easier.",3
Fix four bugs in StackdriverTaskHandler (#13784),0
Simplify Kerberos network setup (#13999),1
Fix typos in Upgrade Check Doc (#14035),2
Disable row level locking for Mariadb and MySQL <8 (#14031)closes #11899closes #13668This PR disable row-level locking for MySQL variants that do not support skip_locked and no_wait -- MySQL < 8 and MariaDB,5
"Bugfix: Fix permissions to triggering only specific DAGs  (#13922)From 1.10.x -> 2.0, the required permissions to trigger a dag have changed from DAG.can_edit to DAG.can_read + DagRun.can_create. Since the Viewer role has DAG.can_read by default, it isn't possible to give a Viewer trigger access to a single DAG without giving access to all DAGs.This fixes that discrepancy by making the trigger requirement DAG.can_edit + DagRun.can_create. Now, to trigger a DAG, a viewer will need to be given both permissions, as neither is with the Viewer role by default.This PR also hides the Trigger/Refresh buttons on the home page if the user doesn't have permission to perform those actions.closes: #13891related: #13891",1
Support google-cloud-logging` >=2.0.0 (#13801),2
Add retryer to SFTP hook connection (#13065),1
Utilize util method to yield versioned doc link (#14047),2
Fix critical CeleryKubernetesExecutor bug (#13247)closes https://github.com/apache/airflow/issues/13263,0
"Clean-up JS code in UI templates (#14019)- Use template literals instead of '+' for forming strings, when applicable- remove unused variables (gantt.html)- remove unused function arguments, when applicable",1
Add Apache Beam operators (#12814),1
Add WeekDayBranchOperator (#13997),1
"Prepare to release a new wave of providers. (#14013)For the regular providers, Vast majority is in `1.0.1` version and it isonly documentation update - but this way we will have a consistent setof documentation (including commit history) as well as when we releasein PyPI, the READMES will be much smaller and link to the documentation.We have two new providers (version 1.0.0):* neo4j* apache.beamThere are few providers with changes:Breaking changes (2.0.0)* google* slackFeature changes (1.1.0):* amazon* exasol* http* microsoft.azure* openfaas* sftp* snowflake* sshThere were also few providers with 'real' bugfixes (1.0.1):* apache.hive* cncf.kubernetes* docker* elasticsearch* exasol* mysql* openfaas* papermill* presto* sendgrid* sqliteThe ''backport packages"" documentation is prepared only for thoseproviders that had actual bugfix/features/breaking changes:```amazon apache.hive cncf.kubernetes docker elasticsearch exasol googlehttp microsoft.azure mysql openfaas papermill presto sendgrid sftpslack snowflake sqlite ssh```Only those will be generated with `2021.2.5` calver version.",2
"Make the role assigned to anonymous users customizable (#14042)Fixes the issue wherein regardless of what role anonymous users are assigned (via the `AUTH_ROLE_PUBLIC` env var), they can't see any DAGs.Current behavior causes:Anonymous users are handled as a special case by Airflow's DAG-related security methods (`.has_access()` and `.get_accessible_dags()`). Rather than checking the `AUTH_ROLE_PUBLIC` value to check for role permissions, the methods reject access to view or edit any DAGs.Changes in this PR:Rather than hardcoding permission rules inside the security methods, this change checks the `AUTH_ROLE_PUBLIC` value and gives anonymous users all permissions linked to the designated role. **This places security in the hands of the Airflow users. If the value is set to `Admin`, anonymous users will have full admin functionality.**This also changes how the `Public` role is created. Currently, the `Public` role is created automatically by Flask App Builder. This PR explicitly declares `Public` as a default role with no permissions in `security.py`. This change makes it easier to test.closes: #13340",3
"Retry critical methods in Scheduler loop in case of OperationalError (#14032)In the case of OperationalError (caused deadlocks, network blips), the scheduler will now retry those methods 3 times.closes #11899closes #13668",1
"Fix broken SLA Mechanism (#14056)closes https://github.com/apache/airflow/issues/14050We were not de-serializing `BaseOperator.sla` properly, hencewe were returning float instead of `timedelta` object.Example: 100.0 instead of timedelta(seconds=100)And because we had a check in _manage_sla in `SchedulerJob` and `DagFileProcessor`,we were skipping SLA.SchedulerJob:https://github.com/apache/airflow/blob/88bdcfa0df5bcb4c489486e05826544b428c8f43/airflow/jobs/scheduler_job.py#L1766-L1768DagFileProcessor:https://github.com/apache/airflow/blob/88bdcfa0df5bcb4c489486e05826544b428c8f43/airflow/jobs/scheduler_job.py#L395-L397",2
Remove unused 'context' variable in task_instance.py (#14049),1
Added json_render method to separate filtering from view (#14024),5
Fixes to release process after releasing 2nd wave of providers (#14059),1
Bugfix: Scheduler fails if task is removed at runtime (#14057)closes https://github.com/apache/airflow/issues/13464,0
"Correctly capture debug logs in plugin tests. (#14058)This fixes the test test_should_load_plugins_from_property, which is currently quarantined as a ""Heisentest"".Current behavior:The test currently fails because the records that it expects to find in the logger are not present.Cause:While the test sets the logger as ""DEBUG"", it doesn't specify which logger to update. Python loggers are namespaced (typically based on the current file's path), but this has to be defined explicitly. In the absence of a specified logger, any attempts to lookup will return the BaseLogger instance.The test is therefore updating the log level for the base logger, but when the test runs, the plugins_manager.py file defines a namespaced logger log = logging.getLogger(__name__) used throughout the file. Since a different logger is used, the original log level, in this case INFO, is used. INFO is a higher level than DEBUG, so the calls to log.debug() get filtered out, and when the test looks for log records it finds an empty list.Fix:Just specify which logger to update when modifying the log level in the test.",3
Add return to PythonVirtualenvOperator's execute method (#14061),1
"Update to Pytest 6.0 (#14065)And pytest 6 removed a class that the rerunfailures plugin was using, sowe have to upgrade that too.",1
"Remove permissions to read Configurations for User and Viewer roles (#14067)Only `Admin` or `Op` roles should have permissions to view Configurations.Previously, Users with `User` or `Viewer` role were able to get/view configurations usingthe REST API or in the Webserver. From Airflow 2.0.1, only users with `Admin` or `Op` role would be ableto get/view Configurations.",5
Fix DAGs mount path in Kubernetes worker pod when gitSync is enabled (#13826)* Update pod-template-file.kubernetes-helm-yaml* Fix ssh-key access issueThis change allows dags.gitSync.containerName to read ssh-key from file system.Similar to this https://github.com/varunvora/airflow/blob/ce0e6280d2ea39838e9f0617625cd07a757c3461/chart/templates/scheduler/scheduler-deployment.yaml#L92It solves https://github.com/apache/airflow/issues/13680 issue for private repositories.Co-authored-by: Denis Krivenko <36439732+dnskr@users.noreply.github.com>,1
Small docs readme update (#14062)* Add instruction for running docs locally* Fix RST syntax* Update docs/README.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Only show User's local timezone if it's not UTC (#13904),1
"Fix Kerberos network creation on older docker-compose (#14070)`attachable` is only a property of compose version 3.1 files, but we areon 2.2 still.This was failing on self-hosted runners with an error`networks.example.com value Additional properties are not allowed('attachable' was unexpected)`",1
Fix doc lint error on main (#14093),0
"Speed up building provider packages by ~4 times (#14084)These changes speeds up the build packages step from about 7mins30 to2min10 on my laptop, and the generate documentation from 3mins to1min30. On CI it will be even quickerThe changes are:- Don't do a git fetch for _every_ provider. Once is enough.- Don't shell out to black, but use the Python API. (Spawning a python  process is not free)- As a tidy up, don't print the info about version suffix once per  package, just once is enough as it doesn't change from package to  package",4
"Make helm chart commands compatible with 1.10.14 image (#13526)* Make chart command compatible with 1.10.14 image* [""airflow"", ""webserver""] does not work with the apache image* [""webserver""] works, as does ""bash"" ""-c"" etcCo-authored-by: Daniel Standish <dstandish@users.noreply.github.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Fixes missing reference in docs (#14079)closes: #14045,2
"Fix Commands to install Airflow in docker/install_airflow.sh (#14099)It was trying to run the following command and failing```pip install apache-airflow[amazon,google]2.0.0rc2```",0
Fix order of failed deps (#14036),0
Corrections in docs and tools after releasing provider RCs (#14082),1
Update Airflow Releasing Guide with 1.10.x and 2.0 commands (#14098)Some of the commands were only for 1.10.x,5
Improve GCSToSFTPOperator paths handling (#11284)* Improve GCSToSFTPOperator paths handling,1
"Restores flexible installation version, fixes manual tag build process. (#14107)Revert ""Fix Commands to install Airflow in docker/install_airflow.sh (#14099)""This reverts commit 68758b826076e93fadecf599108a4d304dd87ac7.Also fixes the docker build script that was the reason for originalattempt to fix it.",0
Improve Apache Beam operators - refactor operator - common Dataflow logic (#14094),2
"Allow users of the KPO to template environment variables (#14083)* Allow users of the KPO to template environment variablesAddresses https://github.com/apache/airflow/issues/13348Adds a ""templated_fields"" variable to k8s.V1EnvVar so jinja templatingwill work when passed by users``* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update airflow/providers/cncf/kubernetes/operators/kubernetes_pod.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* fix descriptionCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",0
"Add a decorator to retry functions with DB transactions (#14109)This commit adds a decorator: `@retry_db_transaction` to retryMethods and functions when there is an `OperationalError`. This unifiesthe retry configs in a single place and the adds logging.This makes the code clean and avoids repeatation. This decoratorshould not be used when using `@provide_session`.We already have `run_with_db_retries` to retry code-blocks with thesame default.`@retry_db_transaction` leverages `run_with_db_retries` (which uses tenacity)to retry.You can use this decorator both with and without passing args to it:`@retry_db_transaction` uses the default configs (example: 3 retries)If for whatever reason we want to change the number of retries fora certain function, we can use pass `retries` to decorator,exmaple: `@retry_db_transaction(retries=2)`The decorator also accepts kwargs where developers can passdifferent keyword arguments to `tenacity.Retrying` object.",1
Change link to keep ex-Polidea employees still INTHEWILD (#14113),2
Use MongoDB color for MongoToS3Operator (#14103)Setting UI color for MongoToS3Operator the MongoDB brand oneCo-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,5
Add statsd extraMappings option to helm chart (#14029),2
BashOperator to raise AirflowSkipException on exit code 127 (#13421),1
Remove left-over fields from required in provider_info schema. (#14119)Those fields were left-over from the full provider.yaml schema.Apparently they caused no problems but they should be removed.,4
Add statsd integration to breeze (#12708),1
Skip SLA check only if SLA is None (#14064)Users have to provide SLA as timedelta. If not provided it shouldbe a default None value.This change will result in run time errors if other type thantimedelta is passed as SLA. With this change spotting #14056would be easier.,4
Run KinD tests when cncf.kubernetes provider files are changed (#14122),4
"Template k8s.V1EnvVar without adding custom attributes to dict. (#14123)The previous attempt at this worked, but ended up breaking other testsas the `__eq__` method on this compared to dict.This customizes the render method to have the variable templated, butwithout needing to add an attribute to the V1EnvVar class.",1
"Add gdrive_to_gcs operator, drive sensor, additional functionality to drive hook, and supporting tests (#13982)",3
includes the STS token if STS credentials are used (#11227)as per AWS docs https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-authorization.html#copy-credentials,3
Remove duplicated volume mount for git sync in scheduler template (#12717)closes: #12716,4
Parametrized keda task concurrency in chart (#13571)* Rely on the config.celery.worker_concurrency valueto determine the number of task a keda worker can take(vs the previous 16 that was hardcoded in the query).* Updated documentation accordingly,2
Fix typos in concept docs (#14130),2
"Added support for DSS, ECDSA, and Ed25519 private keys in SSHHook (#12467)",1
Add allowed_jenkins_states to JenkinsJobTriggerOperator (#14131),1
Remove reinstalling azure-storage steps from CI / Breeze (#14102)Since https://github.com/apache/airflow/pull/12188 was merged Idon't think we need this steps.This step also caused the docker build step for 2.0.1rc2 to failCo-authored-by: Jarek Potiuk <jarek@potiuk.com>,0
Fix flower ingress annotations in chart (#13615)https://github.com/apache/airflow/pull/12010 introduceda small bug in the way annotations are handled for theflower ingress.A test have been added to prevent this from occuring againg.Also more conditionnals have been added to theflower-ingress.yaml file to make it compatible with schemavalidation.,5
#9803 fix bug in copy operation without wildcard  (#13919),0
"Run CI (Build Image and committer PRs) on self-hosted runner (#13730)* Run CI (Build Image and committer PRs) on self-hosted runnerThe queue for GitHub actions is starting to get painful.This PR changes it to run any build of main (push or scheduled), or a PRfrom a committer on the self-hosted runner.However since this setting is configured _in_ the repo, it could bechanged by a PR, so we can't rely on that; we also use a custom build ofthe self-hosted runner binary that enforces PRs are from contributors,otherwise it will fail the build* Don't free space on self-hosted runnersIt deletes the images we have pre-cached (defeating the point ofpre-loading them) and we manage freeing space via another process.The exception to this is the K8s tests, where we don't need the imageswe pre-cache, but we do want the space in our tmpfs.* Some jobs take longer on self-hosted runners* When running KinD tests on CI, use pass through the host docker creds to KindThis enabled self-hosted runners (which are logged in to a docker hubuser) to pull images without running in to pull limits.* Work around supurious blocking IO failuresThis seems to be an occasional problem on the build provider package jobwhich produces a lot of output all at once.The error appears as the job just failing with an exit code (sometimes 1, othertimes 120) but noerror.",0
Fix typo in Build Images workflow from self-hosted switch (#14150)The change to this file was untestable until it was merged to master :(,7
Refactor redundant doc url logic to use utility (#14080),1
Add Airflow 2.0.1 to Changelog and Updating.md (#14151)Changelog- https://github.com/apache/airflow/commit/38083fc353befcd012d07e65b29d0c9b9cd0ec7b- https://github.com/apache/airflow/commit/beb8af5ac6c438c29e2c186145115fb1334a3735and update the Updating.md,5
Doc fixes after releasing providers. (#14142),1
Replace 2.0.0 with 2.0.1 in docs (#14153),2
Add Paradigma to INTHEWILD.md (#14156)Paradigma is a technology consultancy company located in Spain that are using airflow in various projects,1
"Sync DB Migrations in Master with 2.0.1 (#14155)`449b4072c2da_increase_size_of_connection_extra_field_.py` does not exist in 2.0.1so we need to update down_migration```INFO  [alembic.runtime.migration] Running upgrade 2c6edca13270 -> 61ec73d9401f, Add description field to connectionINFO  [alembic.runtime.migration] Running upgrade 61ec73d9401f -> 64a7d6477aae, fix description field in connection to be textINFO  [alembic.runtime.migration] Running upgrade 64a7d6477aae -> e959f08ac86c, Change field in DagCode to MEDIUMTEXT for MySqlINFO  [alembic.runtime.migration] Running upgrade e959f08ac86c -> 82b7c48c147f, Remove can_read permission on config resource for User and Viewer role[2021-02-09 19:17:31,307] {migration.py:555} INFO - Running upgrade 82b7c48c147f -> 449b4072c2da, Increase size of connection.extra field to handle multiple RSA keys```",0
"Disable progress bar for PIP installation (#14126)When the image is prepared, PIP installation produces progressbars which are annoying - especially in the CI environment.This PR adds argument to control progress bar and sets it to ""off""for CI builds.",1
"Fix pod launcher role permissions to list events in chart (#13649)When using the KubernetesPodOperator withlog_events_on_failure=True, and the pod exits,the executor needs to access the events in the namespace.Before this resulted in an exception when using the chartas this it was not permitted by the pod-launcher role.",2
Document configuration for email backend credentials. (#14006),5
Fix the AttributeError with text field in TelegramOperator (#13990)closes: #13989,1
"Fix docs for Google Cloud Secret Manager backend (#14164)""airflow-tenant-primary"" is a bit confusing, and the examples don't match up in the configuration file example.fix ""connections_prefix"" and ""variables_prefix"" mixup in first unordered list in ""Managing Secrets"" sectionmake ""variable_prefix"" plural for consistency",0
"Add param to CloudDataTransferServiceOperator (#14118)When a one-time job is created with `CloudDataTransferServiceS3(GCS)ToGCSOperator`, the job remains on the GCP console even after the job is completed.This is a specification of the data transfer service, but I would like to add this parameter because there are normally cases where don't want to leave a one-time job.",1
"Speed up clear_task_instances by doing a single sql delete for TaskReschedule (#14048)Clearing large number of tasks takes a long time. Most of the time is spent at this line in clear_task_instances (more than 95% time). This slowness sometimes causes the webserver to timeout because the web_server_worker_timeout is hit.```        # Clear all reschedules related to the ti to clear        session.query(TR).filter(            TR.dag_id == ti.dag_id,            TR.task_id == ti.task_id,            TR.execution_date == ti.execution_date,            TR.try_number == ti.try_number,        ).delete()```This line was very slow because it's deleting TaskReschedule rows in a for loop one by one.This PR simply changes this code to delete TaskReschedule in a single sql query with a bunch of OR conditions. It's effectively doing the same, but now it's much faster. Some profiling showed great speed improvement (something like 40 to 50 times faster) compared to the first iteration. So the overall performance should now be 300 times faster than the original for loop deletion.",4
Fixes to dataproc operators and hook (#14086)Two quick fixes to Dataproc operators and hooks.Add more templated fields to the DataprocClusterDeleteOperatoras per #13454. There were a few other fields which could easily be templated so I added them as well.Don't use the global-dataproc.googleapis.com:443 URL when creating dataproc clients.This was partially done in #12907 but the other two client creation methods were not updated. Using the global-dataproc URL results in 404s when trying to create clusters in the global region.We don't need to specify the default endpoint as it is used by default in the dataproc client library:https://github.com/googleapis/python-dataproc/blob/6f27109faf03dd13f25294e57960f0d9e1a9fa27/google/cloud/dataproc_v1beta2/services/cluster_controller/client.py#L117,5
Tiny doc fixes after releasing backports (#14173),0
Log migrations info in consistent way (#14158)same as #13458 but for `82b7c48c147f_remove_can_read_permission_on_config_.py` migrationThis migration changes logging handlersso each next migration is differently formatted when doingairflow db reset. This commit fixes this behavior.,0
Add BigQueryUpdateTableOperator (#14149),5
Adds Estratégia Educacional to list of Airflow Users (#14180),1
"Add DAG Timeout in UI page ""DAG Details"" (#14165)",2
"Use `Lax` for `cookie_samesite` when empty string is passed (#14183)closes https://github.com/apache/airflow/issues/13971The value of `[webserver] cookie_samesite` was changed to `Lax` in >=2.0from `''` (empty string) in 1.10.x.This causes the following error for users migrating from 1.10.x to 2.0if the old airflow.cfg already exists.```Traceback (most recent call last):File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2447, in wsgi_appresponse = self.full_dispatch_request()File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1953, in full_dispatch_requestreturn self.finalize_request(rv)File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 1970, in finalize_requestresponse = self.process_response(response)File ""/usr/local/lib/python3.9/site-packages/flask/app.py"", line 2269, in process_responseself.session_interface.save_session(self, ctx.session, response)File ""/usr/local/lib/python3.9/site-packages/flask/sessions.py"", line 379, in save_sessionresponse.set_cookie(File ""/usr/local/lib/python3.9/site-packages/werkzeug/wrappers/base_response.py"", line 468, in set_cookiedump_cookie(File ""/usr/local/lib/python3.9/site-packages/werkzeug/http.py"", line 1217, in dump_cookieraise ValueError(""SameSite must be 'Strict', 'Lax', or 'None'."")ValueError: SameSite must be 'Strict', 'Lax', or 'None'.**```This commit takes care of it by using `Lax` when the value is empty string (``)",1
Use apache.spark provider without kubernetes (#14187)Catches Name Error on importing optional kubernetes package.,2
"Rework client-side script for connection form. (#14052)* Rework client-side script for connection form.Before this, fields on the connection form were dynamically changed byadding and removing the ""hide"" CSS class. If a provider were to add afield that requires validation (e.g. InputRequired), or if a differentfield type was used (e.g. NumberField), the entire form would beunsaveable, even if the currently selected connection type had nothingto do with that provider.This change takes a different approach; upon loading, the DOM elementsfor all extra fields are saved into a Map, then removed from the DOMtree. When another connection type is selected, these elements arerestored into the DOM.* Use template string literals in connection form.Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>",1
Update documents for using MySQL (#14174)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Allow AWS Operator RedshiftToS3Transfer To Run a Custom Query (#14177)Co-authored-by: Arati Nagmal <arati@stepfunction.ai>,1
"Add SubprocessHook for running commands from operators (#13423)This extracts the ""run this command"" logic from the BashOperator in to areusable hook.Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Remove accidental cuss (#14207),4
Update Tree View date ticks (#14141),5
Correct typo in GCSObjectsWtihPrefixExistenceSensor  (#14179)Rename GCSObjectsWtihPrefixExistenceSensor to GCSObjectsWithPrefixExistenceSensor,0
Add materialized view support for BigQuery (#14201),1
"Reorder doc/spelling build order and improve spelling error message for CI (#14196)Generic Sphinx errors that were unrelated to spelling were being raised as spelling errors leading to potential confusion. Reversing the order of the build (i.e. running the docs build before the spelling build) will catch such errors and more appropriately denote the issue as a build issue and not a spelling issue. Additionally, the error message for Sphinx errors unrelated to spelling has been updated to be more clear for such cases.closes: #14051",5
"Restructure COMMITTERS.rst (#14218)- Move ""Guidelines to become an Airflow Committer"" before ""Guidelines for promoting Committers to Airflow PMC""- Remove ""Becoming a Committer"" section which is same as ""Guidelines to become an Airflow Committer""",4
Add `exists_ok` flag to BigQueryCreateEmptyTable(Dataset)Operator (#14026)Co-authored-by: uma6 <>Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>,5
Add better description and guidance in case of sqlite version mismatch (#14209)Closes: #14208,1
Fixes regexp in entrypoint to include password-less entries (#14221)Fixes #14104,0
Correct a typo in upgrading-to-2.rst (#14225)Update `list_dag` to `list_dags`,2
Limits Sphinx to <3.5.0 (#14238)Sphinx 3.5.0 released on 14th of Feb introduced a problem in ourdoc builds.It is documented in https://github.com/sphinx-doc/sphinx/issues/8880Until this problem is solved we are limiting Sphinx.,0
CONTRIBUTING.rst minor link fix and spelling (#14237),0
CONTRIBUTING.rst Link fix from md to rst (#14235),0
Fix comparison dagTZ with localTZ (#14204),2
"Disables self-hosted runs for non-apache-owned repositories (#14239)When the user wants to test pull requests or - especilly - masterbuild in their own fork, they will not likely have self-hostedrunners configured and the builds will fail.This change uses self-hosted runners only if the build is run inthe context of the `apache/airflow` repository.",1
fixup! Disables self-hosted runs for non-apache-owned repositories (#14239) (#14242),1
CONTRIBUTING.rst: Clarifying setup step 2 concerning Docker (#14246),2
"docs: TESTING.rst: fix not loading image (#14247)[This image loads](https://github.com/apache/airflow/blob/master/images/testing/run-test.png) but [this](https://github.com/apache/airflow/blob/master/images/testing/run-tests.png) as referred to in the docs, not.",2
docs: NOTICE: Updated 2016-2019 to 2016-now (#14248),5
docs: CI.rst: Typos and Link Fix (#14250),0
Add GoogleDriveToLocalOperator (#14191)Add new operator to download file from Google Drive to local filesystem. Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,5
Add CODEOWNERS for automated PR review assignment (#14216)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Correct PostgreSQL password in doc example code (#14256)The example code block for PostgreSQL sets the password to the ``airflow_user`` instead of ``airflow_pass``.,4
Fix indentation in code block in Taskflow API doc (#14241),2
Js linting and inline migration for simple scripts (#14215)* Js linting and inline migration for simple scripts- Linting for all the relatively simple JS in our html templates- Replaced jQuery .click() and .ready() functions- changed all js files to snake_case instead of kebab-case for consistency* add function to get metadata* fix meta_head and clean up js* remove extra <head> tag* snake case task instances file,2
"Improved unit tests for open_maybe_zipped function. (#14114)Implemented ""real"" tests for open_maybe_zipped that test file contentreading without relying on mocks.",2
Add Airflow UI instance_name configuration option (#10162)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,5
MySQL hook respects conn_name_attr (#14240)The MySQL hook does not properly use the class attribute `conn_name_attr` inthe get_conn method; this adjusts that method to use the same approach as theget_uri method.,1
"Temporarily disable the check-actions step (#14271)We are now getting ""Error: Resource not accessible by integration""output when we try to run this, even from a triggered workflow shouldhave the write permissions needed in the token.To unblock CI I am temporarily disabling this step -- it is not required(though very nice) as it just creates links from the CI job back to thebuild image workflow",1
"Add more flexibility with FAB menu links (#13903)Airflow can be more flexible with the links plugins are allowed to add. Currently, you cannot add a top level link, a link with a label, or even without providing a category_icon (which isn't used anyways).This PR gives plugin authors the flexibility to add any link FAB supports.",1
"Revert ""Temporarily disable the check-actions step (#14271)"" (#14284)This reverts commit 8b5c4a742d42af2410e1fc999b2a295251aa0e6e.",4
"Fix race in test_celery_executor.py test_retry_on_error_sending_task test (#14273)The problem was that _sometimes_ the task would actually complete, andnot timeout like we wanted.This changes the test to _always_ raise a timeout error, instead ofrelying on any particular time offset.",1
Don't allow SlackHook.call method accept *args (#14289)Don't allow SlackHook.call accept *args as underlying Slack WebClientmethod doesn't accept *args anymore.,1
Fix get_context_data doctest import (#14288)This commit fixes the doctest replacing the old import - fromairflow.task.context - to the newer one.,1
"fix lossing duration < 1 secs in tree (#13537)truncat duration to 3dp when duration < 10Update airflow/www/views.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>fix import, retrigger ci",2
fix: Scheduler in helm chart cannot access DAG volume with git sync (#14203),2
"Fix misleading statement on sqlite (#14317)The statement ```By default, Airflow uses **SQLite**, which is *not* intended for development purposes only.```is confusing. If `postgres/mysql` are production worthy db backends, and `sqlite` as default db for `airflow` is for development purposes only, this statement is not correct. If I'm mistaken and `sqlite` is for both `production` and `development` purposes, please ignore this PR",5
Helm Chart: Add option to enable/disable flower (#14011),0
Fix render_template method name in plugin example (#14278)these methods exist:```flask_appbuilder.BaseView.render_templateflask_admin.BaseView.render```this one does not:```flask_appbuilder.BaseView.render```,0
Add tests for gitSync in helm chart (#14316)Added Missing tests for #14203,3
"Added Issue reporting and triage process document (#14223)* Added Issue reporting and triage process documentAdded Issue reporting and triage process document, including theupdated set of labels to be used for reporting and tracking issues.Based on PR feedback, moved the reference to the issue process doc from theairflow docs to Contributing doc. Also moved the .rst file to the right directoryand renamed it to match convention.",2
"Allow viewers to see all docs links (#14197)Currently, Viewers can only see the ""Documentation"" link in the docs menu. This PR allows Viewers (and above) to see all of the links.",2
Fix link in CONTRIBUTING.rst (#14325)Fixed a link error in the prior version of the document.,2
Scheduler should not fail when invalid executor_config is passed (#14323)closes #14182,4
Added Intellischool (#14330)Added Intellischool to the official list of Airflow users.,1
Fix various typos (#14335),2
Remove redundant parentheses from Python file (#14336),2
Docs: Corrected code example description (#14339)Changed the sentence:`Here is an example of creating and running a pipeline in Java with jar stored on GCS:`to `Here is an example of creating and running a pipeline in Java with jar stored on local file system:`,5
"Fixes failing test_views tests. (#14341)The get_safe_url previously apparently did not correctlyencode & into %38. I am not sure what was the reason,(python library fix ??)But in order to unblock master, this should be fixed.",0
"Attempts to stabilize and improve speed of static checks (#14332)This change attempts to stabilize pylint checks. Since we haverecently added self-hosted runners with multiple CPUS, seems thatre-enabling parallel mode for pylint makes perfect sense as we willfinally be able to use the parallelism and speed up static checkssignificantly.Previously the tests were run in single-processor mode in attemptto avoid random mistakes where different files were processed indifferent processes. This led to random pylint or mypy problemsand aither false-positives or false negatives before especiallywhen it came to circular dependencies. but since we are now pastheavy refactoring, this should be no problem for future changesand occasional false positive/negative is less disruptive thanlong checks.The attempt is made to apply sorting order in the files passedto pylint. This should provide more stability in the resultsof running the tests in PR and in master.We had some custom pylint plugins that prevented using of pylintparallelism. For now we are giving up on one of the plugins(no asserts use) and we rely on committer's review on that (wehave a rule in place to only use asserts in tests). The otherplugin was replaced by coming back to separation of ""main code""and ""test code"" and applying different rules to those - we havenow two different configuration files|: pylintrc andpylintrc-tests to control settings for those two different cases.Mypy and flake8 have been parallelized at the level of pre-commits.By implementing those changes we are likely to speed up thetests on self-hosted runners 6x-8x times.Also caching of pre-commit environments (including thepre-commit installation) is improved. Previously we only cached theenvironment created by the pre-commit, but with this change wealso cache the pre-commit installation in .local directory - thisshould save 1-2 minutes of the job.",4
Refactor GoogleDriveToGCSOperator to use common methods (#14276)Refactor GoogleDriveToGCSOperator to use common methods implementedin hooks used by this operator.,1
"Implements generation of separate constraints for core and providers (#14227)There are two types of constraints now:* default constraints that contain all depenedncies of airflow,  all the provider packages released at the time of the relese  of that version, as well as all transitive dependencies. Following  those constraints, you can be sure Airflow's installation is  repeatable* no-providers constraints - containing only the dependencies needed  for core airflow installation. This allows to install/upgrade  airflow without also forcing the provider's to be installed at  specific version of Airflow.This allows for flexible management of Airflow and Providerpackages separately. Documentation about it has been added.Also the provider 'extras' for apache airflow do not keep directdependencies to the packages needed by the provider. Thosedependencies are now transitive only - so 'provider' extras onlydepend on 'apache-airflow-provider-EXTRA' package and allthe dependencies are transitive. This will help in the futureto avoid conflicts when installing newer providers using extras.",1
Fix spelling (#14343),0
Separate resources parameter for kerberos sidecar (#14342),2
"By default PIP will install all packages in .local folder (#14125)In order to optimize the Docker image, we use the ~/.localfolder copied from build imge (this gives huge optimisationsregarding the docker image size). So far we instructed the usersto add --user flag manually when installing any packages when theyextend the images, however this has proven to be problematic asusers rarely read the whole documentation and simply try what theyknow.This PR attempts to fix it. `PIP_USER` variable is set to `true`in the final image, which means that the installation by defaultwill use ~/.local folder as target. This can be disabled byunsetting the variable or setting it to `false`.Also since pylint version has been released to 2.7.0, it fixesa few pylint versions so that we can update to the latest constraints.",3
Fix some tests failures after pylint fixes (#14350),0
Pprint default args and wrap (#14345),5
"Fix spelling in ""ignorable"" (#14348)",0
Split webserver config from configmap (#14353),5
"Remove testfixtures module that is only used once (#14318)This is only used in a single test, everywhere else we use Pytest orunittest's built in feature",3
"Fix caching of python images during builds (#14347)* Fix caching of python images during buildsAfter GHCR change, github image caching for python images wasbroken. GHCR requires each image that we push to it to betagged with ""org.opencontainers.image.source"" label to pointto the right project repository. Otherwise it will not appearin the repository images and we will not be able to managepermissions of the image.So we'v been extending the python base image with appropriatelabel, but it was not in all places necessary. As the result.the builds in CI were usign different images than the cacheimage in DockerHub, which in turn caused full image rebuildsALWAYS. By implementing this change we make sure tha the the""airflow-tainted"" python images - including the label.All images (when using breeze) are now built using suchairflow-tainted image which in turn should give5 up to 5-8minutes saving on building the images job per each job (CIand production)The change implements the following behaviours:1) When image is built on CI or locally without   UPGRADE_TO_NEWER_DEPENDENCIES flag and without   FORCE_PULL_IMAGES flag -t the latest image from the   GitHub Registry or DockerHub registry is used.2) When both FORCE_PULL_IMAGES and UPGRADE_TO_NEWER_DEPENDENCY   are set to something different than ""false"" - the latest Python   images are used - they are pulled first and tagged appropriately.This way - always when UPGRADE_TO_NEWER_DEPENDENCIES are set (alsowhen master pushes are done) - such builds will use latest versionof python base images published on DockerHub.* Update _push_pull_remove_images.sh",4
Remove duplicated clean_db function for ImportErrors (#14296)There were two functions -- clean_db_errors and clean_db_import_errorsthat did the same thing.,5
Fix permission error on non-POSIX filesystem (#13121)* Fix https://github.com/apache/airflow/issues/12669 - failure to change ownership of log file on azure,2
"Fixed URL typo for Intellischool (#14372)URL for Intellischool was mistyped... silly mistake, apologies to the repo mods!",2
"Easy switching between GitHub Container Registries (#14120)This change enables easy switching between GitHub Package Registryand GitHub Container Registry by simply adding GITHUB_REGISTRYsecret to be either `docker.package.github.com` or `ghcr.io`.This makes it easy to switch only by the Apache Airflow repositoryrun builds, as it requires preparation of images (to make thempublic and to add permissions to manage them) after they gotcreated for the first time. GitHub Package Registry worksout-of-the-box but it is less stable and considered a legacy,also it does not allow image retention.Documentation has been updated to reflect the reasoning of choosingthis solution as well as describing maintenance processes aroundimages (including adding new Python version)",1
Fix grammar in production-deployment.rst (#14386),0
Fix typo in docker.rst (#14389),2
"Documentation and example dag for CloudDLPDeidentifyContentOperator, GCSObjectExistenceSensor, GCSObjectsWithPrefixExistenceSensor (#14033)* Add documentation and example dag for: CloudDLPDeidentifyContentOperator, GCSObjectExistenceSensor, GCSObjectsWtihPrefixExistenceSensor* Moving gcs sensor docs and example dags to gcs operators docs/example dags* Add system tests for dlp and gcs* Adding further information on DLPDeidentifyContent operators* Pre-Commit tidyup: Renamed gcs/dlp system tests* Apply suggestions from code reviewCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>* reverting some changes following code review* removed redundant @pytest.mark.system(""google.cloud"")* removed operators with newly added examples from missing examples list (pytest fix)* updated all references to GCSObjectsWtihPrefixExistenceSensor (typo) to newly fixed: GCSObjectsWithPrefixExistenceSensor* fixing merge issue: including deprecated operator to be excluded from test suite of operatorsCo-authored-by: rachael-ds <rachael_ds@outlook.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
Add Ephraim to Committers List (#14397)https://lists.apache.org/thread.html/r4d6f50e0790c02a28da41066b97bf7b46ddda99bc2949de08647b2df%40%3Cdev.airflow.apache.org%3E,1
Add Ephraim as a Code Owner for the Stable REST API (#14401)Ephraim already does most of the API reviews so would be good to add him to it,1
Unique pod name (#14186)* Allow pod name override* Allow pod name override,1
"Add Cross reference for Accessing Task Context in TaskFlow API (#14405)Lot of users have been asking this on Slack, SO etc on how to dothat. While we have it in our doc, it isn't cross-referenced inTaskFlow API which causes that feature to be hidden.",1
Sort Committers via their names instead of usernames (#14403)Previously:GithubUsername (Name)Now:Name (GithubUsername),1
Avoid using threads in S3 remote logging uplod (#14414)This prevents `RuntimeError: cannot schedule new futures afterinterpreter shutdown`,1
correct email-config.rst path (#14408),5
"Add cross-reference for Context Dictionary (#14428)https://github.com/apache/airflow/issues/14396#issuecomment-785250252Many users don't know what they can use in the context dictionary,hopefully this will help.",1
Replace deprecated doc links to the correct one (#14429)https://airflow.apache.org/docs/stable/ has been moved to https://airflow.apache.org/docs/apache-airflow/stable/,2
"Pre-commit cache is tied to a specific python version (#14430)The Pre-commit cache did not work when there is a change inversion of host python version used. When python version changedfrom 3.6.12 to 3.6.13, the cache rendered useless.This change adds python versio to the cache key specification,including fallback value also including the python version, sothat even after changing pre-commit, the cache can be rebuiltfrom the last-good cache but for the same python version.",4
Pin moto to <2 (#14433)https://pypi.org/project/moto/#history -- moto 2.0.0 was released yesterday and is causing CI failures,0
"Upgrade to newer dependencies only set when setup changed for PR (#14437)Upgrade to newer dependencies is only set to something else thanfalse in two cases now:* when setup.py or setup.cfg change* when we are running push buildPreviously - mistakenly - it was also set when ""full_tests_needed""label was set on a PR.This caused for example the #19628 PR to fail beforemoto was limited to <2 in #14433.",0
Improve boring-cyborg file (#14442)- Remove duplicates (`tests/providers/microsoft/azure/**/*` already covers `tests/providers/microsoft/azure/*`- Update link of docs (e.g `docs/howto/operator/apache/*` -> `docs/apache-airflow-providers-microsoft-azure/**/*`)- Remove redundant non-existing paths,4
"Make TaskInstance.pool_slots not nullable with a default of 1 (#14406)closes https://github.com/apache/airflow/issues/13799Without it the migration from 1.10.14 to 2.0.0 can fail with following error for old TIs:```Traceback (most recent call last):  File ""/usr/local/lib/python3.6/dist-packages/airflow/jobs/scheduler_job.py"", line 1275, in _execute    self._run_scheduler_loop()  File ""/usr/local/lib/python3.6/dist-packages/airflow/jobs/scheduler_job.py"", line 1377, in _run_scheduler_loop    num_queued_tis = self._do_scheduling(session)  File ""/usr/local/lib/python3.6/dist-packages/airflow/jobs/scheduler_job.py"", line 1533, in _do_scheduling    num_queued_tis = self._critical_section_execute_task_instances(session=session)  File ""/usr/local/lib/python3.6/dist-packages/airflow/jobs/scheduler_job.py"", line 1132, in _critical_section_execute_task_instances    queued_tis = self._executable_task_instances_to_queued(max_tis, session=session)  File ""/usr/local/lib/python3.6/dist-packages/airflow/utils/session.py"", line 62, in wrapper    return func(*args, **kwargs)  File ""/usr/local/lib/python3.6/dist-packages/airflow/jobs/scheduler_job.py"", line 1034, in _executable_task_instances_to_queued    if task_instance.pool_slots > open_slots:TypeError: '>' not supported between instances of 'NoneType' and 'int'```Workaround was to run manually:```UPDATE task_instance SET pool_slots = 1 WHERE pool_slots IS NULL;```This commit makes adds a DB migration to change the value to 1 for records with NULL value. And makes the column NOT NULLABLE.This bug was caused by https://github.com/apache/airflow/pull/7160",1
BugFix: Fix remote log in azure storage blob displays in one line (#14313)* Fix wasb_task_handler.py for reading remote log on blog with one line* Fix wasb_task_handler.py for reading remote log on blog with one line* Fix wasb_task_handler.py for reading remote log on blog with one line* modify wasb readall() to content_as_text,2
Add PATH to basic_static_checks. (#14451)The #14332 change introduced --local flag for static checks andcaching of the pre-commit virtualenv but the necessary PATH changewas not added to `basic_static_checks.sh`. This PR fixes it.,0
Add Snowflake provider to boring cyborg automation (#14432),1
Further Improvements to boring cyborg file (#14444)- Missed some refactor of docs link,2
"Fix pylint pre-commit checks when only todo files are changed (#14453)Because we specified `--rcfile` as an argument to the script, the checkfor ""no files"" was failing and pylint was being run with no files,resulting in an error.",0
Add Tableau provider separate from Salesforce Provider (#14030)Closes #13614,1
"Fix logging error with task error when JSON logging is enabled (#14456)If the JSON logging mode for Elasticsearch logs is enabled, thehandle_failure function would fail, as it tried to treat the exceptionobject as the message and try to JSON serialize it (it expected astring) -- which fails with:```TypeError: Object of type type is not JSON serializable...  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1150, in handle_failure    self.log.exception(error)...  File ""/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py"", line 63, in emit    self.handler.emit(record)```",0
"Gracefully handle missing start_date and end_date for DagRun (#14452)closes: #14384This PR fixes two issues:1) A TypeError that would be raised from _emit_duration_stats_for_finished_state() when the scheduler transitions a DagRun from a running state into a success or failed state if the DagRun did not have a start_date or end_date set.2) An issue with the DagRunEditForm, which would clear the start_date and end_date for a DagRun, if the form was used to transition a DagRun from a failed state back into a running state (or any other state). In the event where the scheduler would determine the DagRun should've been in the failed or success state (e.g. because the task instances weren't cleared), then this would lead to a scheduler crash.",1
Fixes date command in breeze build-image to work on MacOS (#14458)The `date` command on MacOS does not have --rfc* flags so we haveto replace the flag with manually crafted rfc-compliant date.Fixes #14393,5
"Fix crash when user clicks on  ""Task Instance Details"" caused by start_date being None (#14416)This is to fix the following error that happens when a user clicks on 'Task Instance Details' for a TaskInstance that has previous TaskInstance not yet run. E.g.The previous TaskInstance has not yet run because its dependencies are not yet metThe previous TaskInstance has not yet run because scheduler is busy,the previous TaskInstance was marked success without running.This bug was caused by #12910. It affects Airflow 2.0.0 and 2.0.1.",1
Make airflow dags show command display TaskGroup (#14269)closes: #13053Make `airflow dags show` display TaskGroup.,2
Fix failing docs build on Master (#14465)https://github.com/apache/airflow/pull/14030 caused this issue,0
Unable to trigger backfill or manual jobs with Kubernetes executor. (#14160)closes: #13805,5
Rendering of IMAGES.rst was broken due to wrong header (#14471)This PR fixes it.,0
Speed up tests by moving app instantiation to class method (#14329)* blah.* Make fail on purpose.* Move app creation to class level.* Add space.* Remove blah.py.,4
Add plugins endpoint to the REST API (#14280)* add plugins endpoint* include access menu permission to plugin endpoint* apply suggestions from code review* fixup! apply suggestions from code review* refactor get plugins info* fixup! refactor get plugins info* fixup! fixup! refactor get plugins info* fixup! fixup! fixup! refactor get plugins info* added plugin in summary of changes* Remove menu access permission* remove menu_links and admin_views and add limit and offset to endpoint* fixup! remove menu_links and admin_views and add limit and offset to endpoint,1
"Don't create unittest.cfg when not running in unit test mode (#14420)Right now on airflow startup it would _always_ create a unittest.cfg,even if unit test mode was not enabled.This PR changes it so that this file is only created when unit testsmode is enabled (via the environment variable or in airflow.cfg).I have also refactored the mess of top-level code that was interspersedbetween functions to all be at one place -- at the end of the file.The bulk of the config loading code now lives in the `initialize_config`function.Adding lazy module attributes to airflow.configuration via the Pep562class caused some weird behaviour where AirflowConfigParser becameunpickleable because of some ""leaking"" closed-over scope. (Pickling thisclass is used almost exclusively by PythonVirtualEnvOperator). The fixhere is to add custom get/set state methods to not store more than weneed.",1
"BugFix: Serialize max_retry_delay as a timedelta (#14436)closes: #13086, #14212",1
Fix missing HTTPS on link (#14479),2
"Updates docs to include docker resource requirements for quickstart (#14464)When following along the quickstart using Docker's default resource allocationvalues, the webserver continuously crashes and it is not immediately obviousas to why. Since this might be someone's first exposure to running Airflow,letting them know that they should consider upping their resource allocationscould save new people a lot of aggravation. It makes sense - we default to usingCelery, but Airflow may be inadvertently blamed as the problem.Stacktrace:airflow-webserver_1  | [2021-02-25 17:21:54,330] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:21:54,803] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  |   ____________       _____________airflow-webserver_1  |  ____    |__( )_________  __/__  /________      __airflow-webserver_1  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /airflow-webserver_1  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /airflow-webserver_1  |  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/airflow-webserver_1  | [2021-02-25 17:21:54,874] {dagbag.py:448} INFO - Filling up the DagBag from /dev/nullairflow-webserver_1  | [2021-02-25 17:21:56,131] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:21:56,200] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [20] [INFO] Starting gunicorn 19.10.0airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [20] [INFO] Listening at: http://0.0.0.0:8080 (20)airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [20] [INFO] Using worker: syncairflow-webserver_1  | [2021-02-25 17:21:59 +0000] [31] [INFO] Booting worker with pid: 31airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [32] [INFO] Booting worker with pid: 32airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [32] [INFO] Booting worker with pid: 32airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [33] [INFO] Booting worker with pid: 33airflow-webserver_1  | [2021-02-25 17:21:59 +0000] [34] [INFO] Booting worker with pid: 34airflow-webserver_1  | [2021-02-25 17:22:02,860] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:22:02,860] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:22:02,860] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | [2021-02-25 17:22:02,860] {providers_manager.py:299} WARNING - Exception when importing 'airflow.providers.microsoft.azure.hooks.wasb.WasbHook' from 'apache-airflow-providers-microsoft-azure' package: No module named 'azure.storage.blob'airflow-webserver_1  | Running the Gunicorn Server with:airflow-webserver_1  | Workers: 4 syncairflow-webserver_1  | Host: 0.0.0.0:8080airflow-webserver_1  | Timeout: 120airflow-webserver_1  | Logfiles: - -airflow-webserver_1  | Access Logformat:airflow-webserver_1  | =================================================================airflow-webserver_1  | [2021-02-25 17:22:11,015] {webserver_command.py:255} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expectedairflow-oss_airflow-webserver_1 exited with code 137",1
"Fix statsd metrics not sending when using daemon mode (#14454)It seems that the daemonContext will close the socket of statsd.```    return self.statsd.incr(stat, count, rate)  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 35, in incr    self._send_stat(stat, '%s|c' % count, rate)  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 59, in _send_stat    self._after(self._prepare(stat, value, rate))  File ""/usr/local/lib/python3.8/site-packages/statsd/client/base.py"", line 74, in _after    self._send(data)  File ""/opt/airflow/airflow/stats.py"", line 40, in _send    self._sock.sendto(data.encode('ascii'), self._addr)OSError: [Errno 9] Bad file descriptor```",2
Fix spelling (#14472),0
"Use click for building prepare_provider_packages.py CLI (#14480)Click is already a dev dependency and handles a lot of the CLI we neednicely.One change I have made here is to only have the various options onlyapply to the subcommands that use them, i.e. `list-providers-packages--release-version 2020.10.10` would be an error now.This does mean that the command has to come before the options -- allthe automated uses have been updated.",5
Add Azure Data Factory hook (#11015)fixes #10995,0
Removes DigitalOcean from INTHEWILD.md (#14488)Airflow is no longer used at DigitalOcean.,1
Log all breeze output to a file automatically (#14470),2
Adds --dry-run-docker flag to just print the docker commands (#14468)Whenever docker commands should be used the --dry-run-dockerflag will print the command rather than execute it.Closes: #14460,2
"Refactor SQL/BigQuery/Qubole/Druid Check operators (#12677)closes: #10271related: #9844 #14184This PR refactor SQL/BigQuery Check operators to reduce duplicated code:create BaseSQLOperator: it standardizes how some of the generic SQL operators retrieve DB hook with the .get_db_hook() methodAdd a database kwarg *CheckOperators for a consistent interfacecreate _BigQueryDbHookMixin to standardize the .get_db_hook() method for BigQuerycreate _QuboleCheckOperatorMixin to remove duplicate codereplace <class-name>.template_fields with _get_template_fields in __getattribute__ to avoid hard coding class name, and reduce duplicate coderemove and deprecate DruidCheckOperator the same functionality can be achieved by SQLCheckOperator - the deprecation method is the same for PrestoCheckOperatorMisc:Fix docstringsUpdate deprecated Operator name and import pathRemove unnecessary if statements check parameters in SQLBranchOperator",1
"Fix broken docs build on Master (#14496)Currently docs build are broken, example: https://github.com/apache/airflow/runs/1991279427because of the following error:```  sphinx.errors.SphinxWarning: failed to reach any of the inventories with the following issues:  intersphinx inventory '/opt/airflow/docs/_inventory_cache/apache-airflow-providers-tableau/objects.inv' not fetchable due to <class 'FileNotFoundError'>: [Errno 2] No such file or directory: '/opt/airflow/docs/_inventory_cache/apache-airflow-providers-tableau/objects.inv'```This PR fixes it and takes care of the case when inventory is not present, which happens when. a new provider is added. Once the docs are built the objects.env file is uploaded successfully.",2
"Update docs about tableau and salesforce provider (#14495)Since #14030 tableau is no longer deprecated, we can remove this doc entry",1
Corrects order of argument in docstring in GCSHook.download method (#14497),1
Adding support to put extra arguments for Glue Job. (#14027),1
Allow your own Docker production image to be verified by bash script (#14224)Co-authored-by: Kamil Bregula <kamilbregula@Kamils-MacBook-Pro.local>,2
Add docs about supported logging levels (#14507)* Add docs about supported logging levels* fixup! Add docs about supported logging levelsCo-authored-by: Kamil Bregula <kamilbregula@Kamils-MacBook-Pro.local>,2
"Replace Stale Bot with Stale Github Action (#14494)Looks like Stable Bot is not working anymore for us, so let's replace it with Github Action that runs everyday at 00:00",1
Fix spellings (#14483),0
Fix breeze redirect on macOS (#14506),0
Prepare to release the next wave of providers: (#14487)* Prepare to release the next wave of providers:Regular providers:* amazon          - 1.2.0* apache.beam     - 1.0.1* apache.druid    - 1.1.0* apache.hive     - 1.0.2* apache.spark    - 1.0.2* cncf.kubernetes - 1.0.2* dingding        - 1.0.2* docker          - 1.0.2* elasticsearch   - 1.0.2* exasol          - 1.1.0* google          - 2.1.0* http            - 1.1.1* jenkins         - 1.1.0* microsoft.azure - 1.2.0* mysql           - 1.0.2* neo4j           - 1.0.1* openfaas        - 1.1.1* papermill       - 1.0.2* presto          - 1.0.2* qubole          - 1.0.2* salesforce      - 2.0.0* sendgrid        - 1.0.2* sftp            - 1.1.1* slack           - 3.0.0* snowflake       - 1.1.1* sqlite          - 1.0.2* ssh             - 1.2.0* tableau         - 1.0.0* telegram        - 1.0.2Backport providers (all in version 2021.3.3):amazon apache.beam apache.druid apache.hive apache.spark cncf.kubernetesdingding docker elasticsearch exasol google http jenkins microsoft.azuremysql neo4j openfaas papermill presto qubole salesforce sendgrid sftpslack snowflake sqlite ssh tableau telegram* fixup! Prepare to release the next wave of providers:,1
Removes the step to upload artifact with documentation (#14510)It takes insane 9:30 on the self-hosted runner and likely incursa lot of cost on pushing the docs (and it's value is limited),2
Update hadolint from v1.18.0 to v1.22.1 (#14509)* Update hadolint from v1.22.1 to v1.18.0* fixup! Update hadolint from v1.22.1 to v1.18.0* fixup! fixup! Update hadolint from v1.22.1 to v1.18.0Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,5
Production image can be run as root (#14226)* Production image can be run as root* fixup! Production image can be run as root* fixup! fixup! Production image can be run as rootCo-authored-by: Kamil Bregula <kamilbregula@Kamils-MacBook-Pro.local>Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,1
Enable LDAP auth in docker-compose.yaml (#14516)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,2
chore: fix case of GitHub (#14525),0
Add health-check for celery worker (#14522)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,1
Change HTTP to HTTPS in links (#14527),2
Make airflow info to work with pipes (#14528)After this change output from AirflowConsole can be piped in bashwithout loosing some information thanks to fixed width of output.Closes: #14518Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Add pre-commit check to sort and remove duplicates from the spelling wordlist (#13170),4
"Fix asset recompilation message (#14532)The asset recompilation message did not work well in case of thetests - where we did not mount local sources. It was alwaysshowign the instructions to recompile the assets.This is now fixed and the ""OK"" message is green.",0
Add CLI check for scheduler (#14519)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,1
Implemented S3 Bucket Tagging (#14402),5
Use airflow db check command in entrypoint_prod.sh (#14530)* Use airflow db check in entrypoint_prod.sh* fixup! Use airflow db check in entrypoint_prod.shCo-authored-by: Kamil Breguła <kamilbregula@apache.org>,1
"Add Google leveldb hook and operator (#13109) (#14105)* Add Google leveldb hook (#13109)* Add write_batch, options for DB creation(comparator and other) plus fixes (#13109)* Fix some static checks, add docs (#13109)* Apply suggestions from code reviewCo-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>* Fix some otger static checks and docs (#13109)* Fix tests and some build-docs checks (#13109)* Apply suggestions from code reviewCo-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>* Fix build-docs checks (#13109)* Fix build-docs checks (#13109)* fixup! Fix build-docs checks (#13109)* Rewrite example dag as in google package (#13109)* Add extra options in operator, fix docstrings (#13109)* Update airflow/providers/google/leveldb/operators/leveldb.pyCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>* Add system testing and docstrings (#13109)* Fix comparator place in spelling wordlist(#13109)Co-authored-by: RosterIn <48057736+RosterIn@users.noreply.github.com>Co-authored-by: Kamil Bregula <kamilbregula@Kamils-MacBook-Pro.local>Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",1
"Add docs about Celery monitoring (#14533)Part of: #11161To have a full description of the monitoring of all Airflow components, I add information about HTTP and CLI checks for Celery. Thanks to this, we will not have to search for information one by one, but everything will be in one place.Documentation for Celery does not describe the inspect ping command, but hopefully, this will be added soon.",1
BugFix: TypeError in monitor_pod (#14513)If the log read is interrupted before any logs are produced then`last_log_time` will not be set and the line`delta = pendulum.now() - last_log_time` will fail with```TypeError: unsupported operand type(s) for -: 'DateTime' and 'NoneType'```This commit fix this issue by only updating `read_logs_since_sec` if`last_log_time` has been set.,1
Resolve issue related to HiveCliHook kill (#14542)Closes: #13629Fixed issue in Hive.py file,2
WinRM Operator: Fix stout decoding issue & add option to run command (#13153)Fix #13132 and add the ability to run command as powershell script.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Add new committers (#14544)https://lists.apache.org/thread.html/r33d43764cfb4a3a5f8e463c543229de3f13ee86a9713e7263ef34d39%40%3Cdev.airflow.apache.org%3E,1
Minor doc fixes (#14547),0
clean up gantt js (#14545)Remove most inline js for the Gantt chart.Linting all Gantt jsUse more conventional file name,2
Disable health checks for ad-hoc containers (#14536)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,2
Add more tips about health checks (#14537)* Add more tips about health checks* fixup! Add more tips about health checks* Apply suggestions from code reviewCo-authored-by: Kamil Breguła <kamilbregula@apache.org>,1
Simplify configuration/legibility of Webpack entries (#14551),5
Add CollectionInfo in all Collections that have total_entries (#14366)* missing total_entries in ConnectionCollection* use CollectionInfo for all API collection endpoints* remove CollectionInfo from 200 responses* typo :hankey:* typo again :hankey:,2
Add privileged option in DockerOperator (#14157)This option is useful when you need to give extended privileges to a container.,1
remove inline tree js (#14552),4
"Add plain format output to cli tables (#14546)Add plain format output to cli tables so users can use standard linux utilities like awk, xargs etc.closes: #14517",1
Print right version in airflow info command (#14560),5
adding textnow to users (#14568),1
"BugFix: fix DAG doc display (especially for TaskFlow DAGs) (#14564)Because of how TaskFlow DAGs are constructed, their __doc__ linesmay start with spaces. This fails markdown.markdown(), and thedoc in Markdown format cannot be transformed into HTML properly,and further fails the doc display in the UI.This commit fixes this by always doing left strip for each line for the doc md.",2
Remove WARNINGs from BeamHook (#14554),1
Fix AzureDataFactoryHook failing to instantiate its connection (#14565)closes #14557,0
Bugfix: Plugins endpoint was unauthenticated (#14570)The plugins endpoint missed auth check,0
"Add note in Updating.md about FAB datamodel change (#14478)Use of CustomSQLAInterface instead of SQLAInterface to create custom data model.From Airflow 2.0, if you want to define your own Flask App Buildermodels you need to use CustomSQLAInterface instead of SQLAInterface.For Non-RBAC replace:`from flask_appbuilder.models.sqla.interface import SQLAInterface``datamodel = SQLAInterface(your_data_model)`with RBAC (in 1.10):`from airflow.www_rbac.utils import CustomSQLAInterface``datamodel = CustomSQLAInterface(your_data_model)`and in 2.0:`from airflow.www.utils import CustomSQLAInterface``datamodel = CustomSQLAInterface(your_data_model)`",5
Updated provider's releasing instructions after 27.02.2021 release (#14514)* Updated provider's releasing instructions after 27.02.2021 release* Update dev/README_RELEASE_PROVIDER_PACKAGES.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Replace Graph View Screenshot to show Auto-refresh (#14571),2
"Suppress LOG/WARNING for a few tasks CLI for better CLI experience (#14567)For 'state'/'list'/'render', more likely userswould like to rely on the CLI output for some works,and the Python LOG/WARNING prints cause inconvenience.",1
Fix flower port in Running Airflow in Docker document (#14575),2
"docs: pod_mutation_hook sample adjusted to new API (#14576)The current documentation presents example of pod_mutation_hook that uses old API. The change presents same functionality, but using new API (direct Kubernetes objects)",1
Bugfix: Fix wrong output of tags and owners in dag detail API endpoint (#14490)* fix wrong output of tags and owners in dag detail endpoint* fixup! fix wrong output of tags and owners in dag detail endpoint* fixup! fixup! fix wrong output of tags and owners in dag detail endpoint,2
Replace deprecated postgresql chart location with bitnami/postgresql (#13928),2
Fix grammar/typo in concepts.rst (#14582),2
Doc: Fix typo in Jira Providers link (#14585),2
fix ui bugs in tree view (#14566)- add duration to the dagrun tooltip if it doesn't exist- better calculate white space to put between the tree diagram and task instance list,1
Fix some small typos also TESTING.rst (#14594)- Small typos in Jdbc provider documentation;- Find in the Concepts page an example using the old xcom_push arg and changed it;- Some changes in TESTING.rst that I found when trying the quick start.,1
Fixes failing test_views tests (#14599)This reverts commit 49952e79b04da932242ebf3981883e591b467994.Not sure what happended. We made that change because of cPythonvulnerability (https://github.com/python/cpython/pull/24297/files)in #14341I am not sure what happened here but this should fix the Master.,0
"Restore correct terminal with to interactive breeze usage (#14579)The change to redirect all breeze output to a file had the effect ofmaking stdin no longer a TTY, which meant that the docker container hada fixed with of 80 columns.The fix is to replace the use of `tee` with `scripts`, which does whatwe want -- captures stdout+stderr, but makes the running program believethat it is still attached to a terminal (if it ever was).",1
"Add link to 2.0 Blog post in Changelog (#14602)We currently don't have anything mentioned about 2.0.0 in the changelog, this commit should help users get to our blog post highlighting 2.0 features.",2
docs: Capitalise & minor fixes (#14283) (#14534),0
Remove redundant step in CodeQL GitHub Actions step (#14600)From https://github.com/apache/airflow/runs/2031451066#step:4:9Warning: 1 issue was detected with this workflow: git checkout HEAD^2 is no longer necessary.Please remove this step as Code Scanning recommends analyzing the merge commit for best results.I also double-check it with https://github.com/github/codeql-action,7
Bump Redoc to resolve vulnerability in sub-dependency (#14608),0
"Fix docstrings for Kubernetes code (#14605)We don't use ""@param"" type od docstring. This PR fixes it",0
Fix bug allowing task instances to survive when dagrun_timeout is exceeded (#14321)closes: #12912related: #13407,2
"Use libyaml C library when available. (#14577)This makes loading local providers 1/3 quicker -- from 2s down from 3son my local SSD.The `airflow.utils.yaml` module can be used in place of the normal yamlmodule, with the bonus that `safe_load` will use libyaml where availableinstead of always using the pure python version.This shaves 3 minutes off the ""WWW"" tests - down to 8 minutes from11 minutes.I have not used this module in tests/docs code etc, as I don't want toforce importing `airflow` (and everything in currently brings in) in tothose contexts.",2
Remove duplicate dependecies (#14611),4
Bump version to match node dependency (#14624),5
chore: remove duplicate words (#14615),4
"S3DataSource is not required (#14220)A channel's datasource can be a file system or S3. The code shouldn't assume all datasourcesare S3 datasources and try to verify the URI.Documentation showing the filed ""S3DataSource"" is not required: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DataSource.html",5
"Fix breeze redirection on linux/Ubuntu 20.04 (#14626)The version of `script` on ubunutu 20.04 (and likely others) has doesn'thave the `--log-out` cli arg, but that is the default mode for the""un-named"" arg anyway.",2
"Fix health spelled incorrectly (#14407)In the healthcheck endpoint, several occurrences of ""health"" are spelled as ""heath"". This corrects the misspelling.",0
Default to Celery Task model when backend model does not exist (#14612)closes https://github.com/apache/airflow/issues/14586We add this feature in https://github.com/apache/airflow/pull/12336but looks like for some users this attribute does not exist.I am not sure if they are using a different Celery DB Backend or notbut even Celery > 5 contains this attribute(https://github.com/celery/celery/blob/v5.0.5/celery/backends/database/__init__.py#L66)and even Celery 4 but this commits use the Celery Task model when an attributeerror occurs,0
Update filepaths in boring-cyborg.yml (#14628)- Remove entries for non-existing files- Add more files for kubernetes labels,2
Update pre-commit hooks (#14627)- `pyupgrade`: `v2.7.4` to `v2.10.0`- `pygrep-hooks`: `v1.7.0` to `v1.8.0`- `yamllint`: `v1.25.0` to `v1.26.0`,1
"Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581)- Fix functionality  last_scheduler_run was missed in the process of  migrating from sync_to_db/bulk_sync_to_db to bulk_write_to_db.  This issue will fail DAG.deactivate_stale_dags() method,  and blocks users from checking the last schedule time of each DAG in DB- Change name last_scheduler_run to last_parsed_time,  to better reflect what it does now.  Migration script is added, and codebase is updated- To ensure the migration scripts can work,  we have to limit the columns needed in create_dag_specific_permissions(),  so migration 2c6edca13270 can work with the renamed column.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Bugfix: DruidOperator fails to submit ingestion tasks (#14418)closes: #14417,0
Fix various links in README.md (#14630)- Committer's guide was broken- Some links pointed to old url- Added reference to scheduler docs,2
Fix broken link in LOCAL_VIRTUALENV.rst (#14634)The commits fixes broken link,2
Remove unused usage of logging module (#14632)Unless I am missing something the logging module was imported in the following files but was not used:- airflow/api_connexion/endpoints/dag_source_endpoint.py- airflow/api_connexion/endpoints/version_endpoint.py,2
Fix broken static check on Master  (#14633)Looks like #14627 broke the Master. All tests did not run on that PR (We did not account for the file change in selective checks).,4
Replace Python 2.7 with 3.6 in IMAGES.rst (#14636)`apache/airflow:v2-0-test-python2.7-ci` does not exists and we do not support Python 2.7 from Airflow 3.6+,1
Fix type hints in OpsgenieAlertOperator (#14637)`actions` and `tags` are list of strings. The docstrings contain the correct type but for some reason TypeHints had it wrong.Docs: https://docs.opsgenie.com/docs/alert-api#section-create-alertTests also contain correct example: https://github.com/apache/airflow/blob/779ecc41847c4e476a12fbc1c607c08a00e90205/tests/providers/opsgenie/operators/test_opsgenie_alert.py#L50-L51,3
"Use built-in `cached_property` on Python 3.8 where possible (#14606)Functionality is the same, this just removes one dep for Py 3.8+",4
"Adds new Airbyte provider (#14492)This commit add hook, operators and sensors to interact with Airbyte external service.",1
Create a new documentation package for Helm Chart (#14643),2
Fix grammar and remove duplicate words (#14647)* chore: fix grammar and remove duplicate words,4
Fix link to Helm chart docs (#14652)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,2
Update docs about baking DAGs in docker image (#14648)Co-authored-by: Kamil Breguła <kamilbregula@apache.org>,2
Cache Hook when initializing `CloudFormationCreateStackSensor` (#14638)- `CloudFormationCreateStackSensor` created `AWSCloudFormationHook` in its init method- This commit delays creation of hook until it is needed,1
Created initial guide for HDFS operators  (#11212)closes https://github.com/apache/airflow/issues/8197Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
"Adds verbose mode to prepare_provider_packages scripts/breeze cmds. (#14508)* Adds verbose mode to prepare_provider_packages scripts/breeze cmds.Also clarified a little unclear change resulting from #14480 whichdid not carry an explanation why we want to return exit code 64.Explanation is added back and explicit check for False is used. It isnot clear how result callback function behaves when comand returnsno value (it could easily be called with result = None). This isnot clear in the click documentation, so expicit check for Falsevalue better reflects the intention here.Breeze commands and documentation is also updated so that bothinvocation via breeze (which assures coommon import check) andmanual execution for debugging can take advantage of it.* Update dev/provider_packages/README.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Update dev/provider_packages/README_BACKPORT_PACKAGES.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
Update Flask-AppBuilder dependency to allow 3.2 (and all 3.x series) (#14665),1
Prepare ad-hoc release of the four previously excluded providers (#14655)Documentation update for the four previously excluded providers thatgot extra fixes/bumping to the latest version of the libraries.* apache.beam* apache.druid* microsoft.azure* snowflake,3
Update instructions to download Apache RAT for license verification (#14653)The jar file for Apache RAT is inside the `apache-rat-0.13-bin.tar.gz` instead of sources.,2
BugFix: Fix taskInstance API call fails if a task is removed from running DAG (#14381)Closes: #14331,2
Fix set task instance form test (#14501)* Fix set task instance form test* Return include_future non-string test,3
Add documentation on load_examples with official Docker and helm (#14646)cloeses: #14491,2
Fix minor issues in 'Concepts' doc (#14679),2
Fix documentation for provider's release (#14654),1
"Elasticsearch Provider: Fix logs downloading for tasks (#14686)Without this, Webserver fails with:```[2021-03-09 18:55:19,640] {base.py:122} INFO - POST http://aa.aa:9200/_count [status:200 request:0.142s][2021-03-09 18:55:19 +0000] [64] [ERROR] Error handling requestTraceback (most recent call last):  File ""/usr/local/lib/python3.7/site-packages/gunicorn/workers/sync.py"", line 181, in handle_request    for item in respiter:  File ""/usr/local/lib/python3.7/site-packages/werkzeug/wsgi.py"", line 506, in __next__    return self._next()  File ""/usr/local/lib/python3.7/site-packages/werkzeug/wrappers/base_response.py"", line 45, in _iter_encoded    for item in iterable:  File ""/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py"", line 84, in read_log_stream    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)  File ""/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py"", line 58, in read_log_chunks    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)  File ""/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py"", line 217, in read    log, metadata = self._read(task_instance, try_number_element, metadata)  File ""/usr/local/lib/python3.7/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py"", line 186, in _read    and offset >= metadata['max_offset']TypeError: '>=' not supported between instances of 'str' and 'int'```",1
"Fix eager upgrade typo (#14694)After recent snowflake provider upgrade (#14655), a typo was introducedin the urllib that prevented it to upgrade constraints automatically.This PR restores the correct name of the urllib library.",2
Prevent mixed case env vars from crashing processes like worker (#14380)* Handle misformed env vars without crashing* Include traceback in celery_executor error log* Add test for airflow/configuration.py::as_dict,5
"Add support for worker persistence with KEDA v2.0.0 in helm chart (#13209)* Upgrade KEDA to v2.0.0 and add support for worker persistenceKEDA 2.0.0 introduces support for StatefulSets, so we can now remove the constraint thatworker persistence must be disabled.Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>",1
Bump elliptic from 6.5.3 to 6.5.4 in /airflow/www (#14668)Bumps [elliptic](https://github.com/indutny/elliptic) from 6.5.3 to 6.5.4.- [Release notes](https://github.com/indutny/elliptic/releases)- [Commits](https://github.com/indutny/elliptic/compare/v6.5.3...v6.5.4)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
"Add new datetime branch operator (#11964)closes: #11929This PR includes a new datetime branching operator: the current date and time, as given by datetime.datetime.now is compared against target datetime attributes, like year or hour, to decide which task id branch to take.",5
"Separate out tests to cater of changes in Python 3.8.8 (#14698)https://github.com/python/cpython/pull/24297 change was included inPython 3.8.8 to fix a vulnerability (bpo-42967)Depending on which Base Python Image is run in our CI, two of the testscan fail or succeed.Our Previous two attempts:- https://github.com/apache/airflow/commit/061cd236deb22567e4de36af11025f028d787989#- https://github.com/apache/airflow/commit/49952e79b04da932242ebf3981883e591b467994We might for a while get different base python version depending on the changes of a PR (whether or not it includes a change to dockerfiler).a) when you have PR which do not have changes in the Dockerfile, they will use the older python version as base (for example Python 3.8.7)b) when you have PR that touches the Dockerfile and have setup.py changes in master, it should pull Python 3.8.8 first.",4
"Fix tests for all urllib versions with only '&' as separator (#14710)Turns out #14698 did not fix the issue as Master failed again. Afterdigging a bit more I found that the CVE was fixed in allPython versions: 3.6.13, 3.7.10 & 3.8.8The solution in this PR/commit checks the `parse_qsl` behavior withfollowing tests:```❯ docker run -it python:3.8-slim bashroot@41120dfd035e:/# pythonPython 3.8.8 (default, Feb 19 2021, 18:07:06)>>> from urllib.parse import parse_qsl>>> parse_qsl("";a=b"")[(';a', 'b')]>>>``````❯ docker run -it python:3.8.7-slim bashroot@68e527725610:/# pythonPython 3.8.7 (default, Feb  9 2021, 08:21:15)>>> from urllib.parse import parse_qsl>>> parse_qsl("";a=b"")[('a', 'b')]>>>```",2
"New UI GitHub integrations (#14702)* Add initial codeowners for new UI directory* Add inititial ""area:UI"" labeling automation",5
Add job labels to bigquery check operators. (#14685),1
"Remove duplicated WORKDIR in CI Dockerfile (#14697)Having it doesn't cause any harm, but it was already set five commandsfurther up to the same value",1
"Don't use author_association for self-hosted vs public runner decision. (#14718)Using this has two draw-backs for us.1. MEMBER applies to _anyone in the org_, not just members/commiters to   this repo2. The value of this setting depends upon the user's ""visiblity"" in the   org. I.e. if they hide their membership of the org, the   author_association will show up as ""CONTRIBUTOR"" instead.Both of these combined mean we should instead use an alternative list.We can't use a secret as the `secrets.` context is not available in the runs-onstanza, so we have to have a hard-coded list in the workflow file :( This is assecure as the runner still checks the author against it's own list.",1
Fix typo in DAG Serialization doc (#14722),2
"Initialize the new UI project (#14691)* init new neutrino app* init chakra, linting, static & testing* add first documentation* include link to our project board* remove unneeded code* fix documentation and entry file",2
"Remove broken and undocumented ""demo mode"" feature (#14601)",2
"Fix tests in tests/www/test_views.py (#14719)One of tests fixed in (#14710) had an usused variable - `expected_url`,copy/paste failure. This commit fixes it and adds a condition too toonly replace url if it contains a semi-colon",1
"Reduce duplication in pre_commit_check_order_setup.py script (#14731)Now that we are using Py3.6+ we can rely on dictionary key order to befixed (it was always fixed in 3.6, just not explicitly documented assuch from 3.7) -- as a result we can load the source, rather than try toparse it with regexes",1
Remove un-needed/left over environment variables in ci.yml (#14732)AIRFLOW_COMMITERS was left over from a previous PR and should have neverbeen included.EVENT_NAME and TARGET_COMMIT_SHA aren't needed as GitHub already provideus GITHUB_EVENT_NAME and GITHUB_SHA with the same values.,1
"Wrapping create-user-job by double quote for Helm Chart (#14723)When UI default user password includes a special character like `$`, it occurs error in helm.",0
"Fixed runs-on for non-apache repository (#14737)The change #14718 by mistake left the 'self-hosted"" runs-on in case ofpush or schedule. This caused failures on non-apache repositories.",0
"Add Documentation for Project Guidelines (#14674)This document is a replacement of our Confluence WIKI for theguidelines agreed on the mailing list.We can expand on this doc as we agree around Providers release,some of the guidelines for core release, Helm chart etc.",2
"Webserver: Allow Filtering TaskInstances by queued_dttm (#14708)We allow filtering TaskInstance in the Webserver by queued_dttmsimilar to start_date and end_date.This helps in debugging issues quicker, then getting access to DB.",5
"Webserver: Sanitize string passed to origin param (#14738)Follow-up of #12459 & #10334Since https://github.com/python/cpython/pull/24297/files (bpo-42967)also removed ';' as query argument separator, we remove query argumentswith semicolons.",4
"Add note on execution_delta to ExternalTaskSensor docs (#14741)I spent quite a while figuring out how to wait for external tasks running at a different time in the same day, so I've added a note to help others.",1
"Speed up www and api_connexion tests (#14684)This was accomplished by removing slow and unnecessary initialization steps from test setups, as well as mocking slow login methods.This was accomplished by removing slow and unnecessary initialization stepsfrom test setups, as well as mocking slow login methods.**Slow Initialization**Whenever create_app is called, multiple sub-modules are initialized, includingpermission roles, the old api, new api, logging, error handling, etc. Most ofthese are not relevant for many of the tests. This PR removes unnecessaryinitialization.**Slow login**Login functionality currently depends on hash functions to hash passwords. Bymocking the password hashing functionality in we can sidestep this slowdown.**Approaches analyzed but not used**- Adding indexes to FAB permissions tables. FAB queries roles and permissions  on unindexed name columns. Surprisingly, adding indexes didn't result in a  meaningful test speedup.- Caching api_connexion rendering. the `init_api_connexion` method is the slowest  initialization method, but is required for all `api_connexion` methods. It  looks like the Jinja rendering is the slowest part. I investigated storing  the rendered API in an intermediate state in a separate file, but this wasn't  feasible. If we want more speedups in the future, caching the api_connexion  app, much like we cache the current flask app, is likely the biggest win.",2
Note that the DB must be using UTF-8 (#14742)Without it non-ASCII characters in serialized dag will cause an error.,0
Migrate dags.html javascript (#14692)* migrate dags.html javascript* migrate dags.html javascript,2
Remember expanded task groups in localStorage (#14661)* Save expanded and focused state of TaskGroups in localStorage* Restore focus on refresh* Address style issues according to comments,0
"Refactor Taskflow decorator for extensibility (#14709)* Refactor Taskflow decorator for extensibilityThe Taskflow API is a very clean way to turn python functions intoAirflow tasks, but it is currently limited to only running in thevirtualenv of the parent worker. We eventually want to offer the abilityto use decorators that tie to docker images, kubernetes pods, etc.This commit seperates out the python specific code and creates adecorators directory, which will allow us to both build in more ""core""decorators as well as decorators in providers.* remove unecessary file* fix backcompat* add comment* remove circular import* fix imports* @ashb fixes* @ashb fixes* handle backcompat* Update airflow/decorators/__init__.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/decorators/__init__.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* simplify API* fix docs* Add docs* Update airflow/decorators/python.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* arbitrary change to restart CICo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",4
"Fix attributes for AzureDataFactory hook (#14704)* Add 'conn_type', 'conn_name_attr', 'default_conn_name' and 'hook_name';* Change expected_number_of_hooks to 62.Resolves: #14669",0
Small fixes in provider preparation docs (#14689),2
Add confirming getopt and gstat #14750 (#14751),1
`./breeze stop` is not necessary for new comers #14752 (#14753),1
"Remove Heisentest category and quarantine test_backfill_depends_on_past (#14756)The whole Backfill class was in Heisentest but only one of those testsis problematic nowi: test_backfill_depends_on_past. Therfore it makessense to remove the class from heisentests and move thedepends_on_past to quarantine.It turned out that this is the last ""Heisentest"" and with theisolation we have now coming in parallel tests, it turns out thatHeisentests are not really good way thinking about the tests - runningthem in isolation does not often help, it only makes it more difficultto flag the tests as flaky.The quarantine test_backfill_depends_on_past ihas been captured inthe #14755 issue - and hopefully we will make an effort tode-quarantine some of those tests soon.",3
"Rename DateTimeBranchOperator to BranchDateTimeOperator (#14720)* Update names and paths to be consistent with other Branch operatorsThis included renaming the operator to match the '^Branch.*' pattern, like other operators do, as well as updating file and import paths to remove deprecated files (for example, changing the deprecated dummy_operator to dummy).* Add missing reference to operator guide",1
"Fixes force-pulling base python images (#14736)Sometimes base python image patchlevel might case failure of tests.This happens for example with test_views.py tests fixed in #14719where CVE fix in all python versions caused our tests to fail.There was an error in our scripts - when --force-pull-imageswere used, the base python version was not updated to thelatest version even if there was a newer one and it caused ourimages to bounce few times between two latest patchlevelswhen they were manually refreshed.This change fixes it so that the base python image is always usedwhena) FORCE_PULL_IMAGES is true orb) UPGRADE_TO_NEWER_DEPENDENCIES is != falseThis will cause python upgrade in two cases: - when images are rebuilt with --force-pull-images locally - when images are upgraded with newer dependencies on master",1
Excludes .git-modules from rat-check (#14759),5
Prepare for releasing Elasticsearch Provider 1.0.3 (#14748)https://github.com/apache/airflow/pull/14686 fixes an issues with ES logging that should be released asap. Since we just released the providers it makes sense to release this separately.,1
"Update documentation for broken package releases (#14734)This is a documentation generated for provider packagesthat have been broken in releae 2020.10.29. As explained inthe #14673, some whl packages were broken in 2020.10.29 releaseand we need to regenerate those packages even if they were notupdated.",5
Add elasticsearch to the fixes of backport providers (#14763)We have a new elasticsearch fix and it should be added to the releaseof backport providers. This change updates the documentation for theelasticserch release.,2
Add Helm Chart logo to docs index (#14762),2
Refactor info command to use AirflowConsole (#14757)This change unifies way how we render info output. This solvesfew problems:- users can use output flag- because of that users can use plain output which can be usefulwhen working with docker,2
"Add minimum version of pylint (#14775)We are using some pylint rules that were added in pylint 2.7.0,for example `consider-using-generator`. When older version ofpylint is used, it fails with this error:* E0012: Bad option value 'consider-using-generator' (bad-option-value)Adding lower-limit forces the pylint to be upgraded even ifsomeone's image or installation already had pylint installed.",1
Add script to verify that all artefacts are in svn (#14777)* Add script to verify all is in svnThis change adds simple tool to verify that all expected filesare present in airflow svn when doing release. Also in caseof providers/backport releases it generates simple dockerfilethat can be used to verify installation.,1
Fixes limits on Arrow for plexus test (#14781)Arrow must be <1.0.0 for plexus to work,1
Better diagnostics for image waiting (#14779)Sometimes (very rarely) some 'wait for image' pulling stepsloop forever (while other steps from parallell jobs pulling thesame image have no problems).Example here:Failed step here:* https://github.com/apache/airflow/runs/2106723280?check_suite_focus=true#step:5:349Another similar step in parallel job had no problems with retrieving thesame image earlier:* https://github.com/apache/airflow/runs/2106723269?check_suite_focus=true#step:5:119Both images pulled the same image:docker.pkg.github.com/apache/airflow/master-python3.6-ci-v2:651461418This change adds diagnostics information that might provide moreinformation in case this happens again so that we can understandwhat is going on and mitigate the issue.,0
"Only rebuilds base python image when upgrading to newer deps (#14783)The base python image is only updated when manually triggered andin case of checking for upgraded dependencies in master build.While automated upgrade to latest Python image is good forsecurity, it can cause a number of problems when run automaticallyin the CI:* cache invalidation - thus longer builds* sudden test failuresThis happened in the past already quite a number of times so itis time to switch to a bit different mode. Python images will onlybe automatically upgraded in those cases:1) When Master CI build is run in scheduled nightly build - to check   that tests still pass for latest version of the image2) When manually refreshed with --force-pull-base-python-image3) When DockerHub official images (from tags) are built.The procedure to refresh the images manually in our CI has beenadded to the documentation.",2
Fixes case where output log is missing for image waiting (#14784),2
Drop support for SequentialExecutor in Helm Chart (#14766)* Drop support for SequentialExecutor in Helm Chart* fixup! Drop support for SequentialExecutor in Helm Chart* fixup! fixup! Drop support for SequentialExecutor in Helm Chart* fixup! fixup! fixup! Drop support for SequentialExecutor in Helm Chart,2
Fixes recent scripting breeze fix to work also with zsh (#14787)The BASH variable introduced in #14579 is not set when the main shell iszsh on MacOS (which is the default) this PR changes it so that theoutput of `command -v bash` is used instead.Fixes #14754,0
Add documentation for SQLite upgrade on AmazonLinux2 and CentOS (#14351),2
"Further speed up Connexion API tests with pytest session fixtures (#14746)Creating the Flask API and Connexion take a significant amount of timeto create, and each of these test modules creates the Flask app with thesame set up ""initializers"".To make this work we had to switch away from `unittest.TestCase`, aspytest fixtures won't work with TestCase subclasses.The `configured_app` fixture is defined at the module level, otherwiseeach _subclass_ would have it's ""own"" module scope fixture.This takes the test time for api_connexion/endpoints down to sub-1 minutefor me.",3
"Add GCS timespan transform operator (#13996)This change adds a new GCS transform operator. It is based on the existing transform operator with the addition that it will transform multiple files that match the prefix and that were updated within a time-span. The time-span is implicitly defined: it is the time between the current execution timestamp of the DAG instance (time-span start) and the next execution timestamp of the DAG (time-span end).The use-case is some entity generates files at irregular intervals and an undefined number. The operator will pick up all files that were updated since it executed last. Typically the transform script will iterate over the files, open them, extract some information, collate them into one or more files and upload them to GCS. These result files can then be loaded into BigQuery or processed further or served via a webserver.",2
"Prepare to switch master branch for main. (#14688)There are many more references to ""master"" (even in our own repo) thanthis, but this commit is the first step: to that process.It makes CI run on the main branch (once it exists), re-words a fewcases where we can to easily not refer to master anymore.This doesn't yet re-name the `constraints-master` or `master-*` images -that will be done in a future PR.(We don't be able to entirely eliminate ""master"" from our repo as werefer to a lot of other GitHub repos that we can't change.)",4
Add FTPToS3Operator (#13707)Co-authored-by: javier.lopez <javier.lopez@promocionesfarma.com>,1
Add Guide to release Apache Airflow Upgrade Check (#14690)This commit adds guide to release Apache Airflow Upgrade Check,1
Pin SQLAlchemy to <1.4 due to breakage of sqlalchemy-utils (#14812)The 1.4 releae of SQLAlchemy breaks sqlalchemy-utils.This change pins it to < 1.4Fixes #14811,0
"Rearange API auth tests to the correct place (#14808)- tests/api/auth/bakcned/test_basic_auth was almost exclusively testing  the Connexion API -- moved in to tests/api_connexion/ folder.  (To make this work the conftest for connexion has been moved up a  folder out of .../endpoints/ )- tests/test_utils/test_remote_user_api_auth_backend.py is testing test  code itself, and so isn't needed- Created/copied Basic auth test to  tests/api_connexion/test_basic_auth.py -- this ensures the Connexion  API handles basic auth correctly too.",0
Fix KubernetesExecutor issue with deleted pending pods (#14810)This change treats a pending KubernetesExecutor worker pod deletionas a failure. This allows them to follow the configured retry rulesfor the task as one would expect.,1
Support extraContainers configuration in Helm Chart (#13735)closes https://github.com/apache/airflow/issues/13211,0
"Fix dag endpoint tests not being collected nor ran (#14826)Currently, the dag endpoint tests are not being collected nor ran becauseof the `__init__` constructor under TestDagEndpoint.This PR fixes this, making sure the tests are collected and ran",3
Make pytest collection warnings errors (#14832)In a previous PR I introduced a mistake that made some test classes notbe collected (and thus were never run) -- this ensures that we don'thave this mistake again.,1
Add readonly REST API endpoint for roles and permissions (#14664)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
"Speed up tests/api/ from 20s down to 6s (#14833)Not a big over-all speed up, but also not a big change",4
"Suggest using $http_host instead of $host (#14814)If the reverse proxy is not running in port 80, using $host won't forward the port in the HTTP request to airflow and it won't build the correct redirect URL.E.g.I have nginx and airflow running in docker in the default ports. My mapping for nginx x is 7003:80.The request http://myserver:7003/myorg/airflow/ will redirect to http://myserver/myorg/airflow/admin/ instead of http://myserver:7003/myorg/airflow/admin/.",2
Add dynamic fields to snowflake connection (#14724)Adds form fields and custom form behavior for the Snowflake connection so it is more obvious to new users what fields need to be filled out. Also the doc_strings for the hook are updated to reflect the params along with helpful information using sphinx directives. I have categorized the fields below to explain the change.,4
relax boto3 requirment (#14824),5
"When `breeze stop` is called all integrations are enabled (#14825)Sometimes when an integration got broken, it could be brokenpermanently due to left-over volume. This was because whenbreeze stop was called the integration were not enabled andsome volumes were not deleted.As result, breeze sometimes could not start with integrations, due tomisterious 'unhealthy' condition of one of the integrations.This change enables all integrations automatically whenbreeze stop is called so that all volumes are removed.",4
Pre commit new UI (#14836),1
Create a documentation package for Docker image (#14765)* Create a documentation package for Docker image* fixup! Create a documentation package for Docker image* fixup! fixup! Create a documentation package for Docker image* fixup! fixup! fixup! Create a documentation package for Docker image* Apply suggestions from code reviewCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
"Fix `sync-perm` to work correctly when update_fab_perms = False (#14847)If Airflow is configured with update_fab_perms config setting to False,then the Op, User and Viewer roles are created _before_ the permissionsobjects are written to the database, meaning that these roles did notcorrectly get assigned all the permissions we asked for (the missingpermissions are just silently not created.)Because of the ""migrate to resource permission"" migration this problemis not ""disasterous"" as all most of the Permissions et al. we use arecreated by a migration.This changes it so that the permissions are always created/synced beforewe look at the roles.(Re-running sync-perm wouldn't fix this, as although the second timearound the Permissions will exist in the DB, we see that Op role alreadyhas permissions and don't make any changes, assuming that the siteoperators made such changes.)",4
Undo skip snowflake integration tests (#14844)Snowflake doesn't monkey patch any more so there is no need to skip the tests any longer.,3
Extend HTTP extra_options to LivyHook and operator (#14816)The LivyHook used by the LivyOperator has extra_options in its main run_method but there's no way to use them from the actual operator itself.,1
"Update the docs to release Providers (#14842)While releasing the ES Provider 1.0.3, I updated the docsto release providers.One section was duplicate and there was other minor fixes",0
"Revert ""Create a documentation package for Docker image (#14765)"" (#14867)This reverts commit 03d3c7db931df34a724a3f254bf13197a9063576.",5
Add files to generate Airflow's Python SDK (#14739),2
Fix grammar (#14662),0
"Speed up TestFlaskCli test (#14865)Creating a new process and reloading all of the app is slow, so insteaduse the built-in `runpy` module to run the `flask` module in the sameinterpreter, taking advantage of the already-cached app.This one test was taking about 20s, now down to 3s (still slow, but muchmuch faster)",3
"Fix running child tasks in a subdag after clearing a successful subdag (#14776)After successfully running a SUBDAG, clearing it(including downstream+recursive) doesn't trigger the inner tasks.Instead, the subdag is marked successful and the inner tasks allstay cleared and aren't re-run.The above problem is because the DagRun state of the subdags are not updatedafter clearing. This PR solves it by updating the DagRun state of all DAGsincluding subdags when include_subdags is True",2
Update license check to include TypeScript file extensions (#14868),2
[AIRFLOW-6076] fix dag.cli() KeyError (#13647),0
Add Changelog & Updating.md for 1.10.15 (#14870)This commit adds Changelog & Updating.md for 1.10.15,5
Replaces 1.10.14 with 1.10.15 where needed (#14866),5
Fixes some of the flaky tests in test_scheduler_job (#14792)The Parallel tests from #14531 created a good opportunity toreproduce some of the race conditions that cause some of thescheduler job test to be flaky.This change is an attempt to fix three of the flaky teststhere by removing side effects between tests. The previousimplementation did not take into account that scheduler jobprocesses might still be running when the test finishes andthe tests could have unintended side effects - especiallywhen they were run on a busy machine.This PR adds mechanism that stops all runningschedulerJob processes in tearDown before cleaningthe database.Fixes: #14778Fixes: #14773Fixes: #14772Fixes: #14771Fixes: #11571Fixes: #12861Fixes: #11676Fixes: #11454Fixes: #11442Fixes: #11441,0
Add Airflow 2.0.1 to ``breeze-complete`` and BREEZE.rst (#14876)2.0.1 was missing from the breeze-complete list and the docs,2
Fixes unbound variable on MacOS (#14877)Without it I get:```$ ./breeze./breeze: line 28: @: unbound variable```,1
Adds option to set extraVolumeMounts for git-sync container (#14837),1
Add readonly REST API endpoints for users (#14735)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Fix error when running tasks with Sentry integration enabled. (#13929)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,0
"Adds missing variable for force pull base image variable (#14901)The variable's default value was not set, thus failing pre-commitscripts in case image was not built recently.",0
"Simplify cleaning string passed to origin param (#14738) (#14905)Looks like ""trying to be smart approach"" in https://github.com/apache/airflow/pull/14738does not work on old Python versions. The ""smart"" part being if semicolon exists in URLonly those specific query argument were removed. While this solves the issue for Py 3.6.13 it didn't fix for 3.6.12 (although it minimzed it).Python 3.6.12:```python>>> parse_qsl(""r=3;a=b"")[('r', '3'), ('a', 'b')]```Python 3.6.13:```python>>> parse_qsl(""r=3;a=b"")[('r', '3;a=b')]```This commit simplifies it and check if the url contains `;`, it just redirects to`/home`.",5
"Turn provider's import warnings into debug logs (#14903)When only some providers are installed, cross-dependencies betweenthe providers might cause import erors. Those import errors arerepeated in webserver as it is reloaded every 30 seconds.Since ""only some providers"" case is valid, it should not generatewarrnings if it is an ImportError. Those warnings are nowturned into debug messages.Fixes: #14286",0
"Remove Backport Providers (#14886)We are removing support for Backport Providers now.The last release was sent yesterday- as planned, on 17 March 2021 - thelast release of the Backport Providers.As agreed before, and documented here:https://github.com/apache/airflow/blob/master/dev/PROJECT_GUIDELINES.md#support-for-backport-providers> Backport providers within 1.10.x, will be supported for critical fixesfor three months (March 17, 2021) from Airflow 2.0.0 release date (Dec17, 2020).For the future reference, if anyone would like to build backportproviders with cherry-picking any fixes, the branch to start from is`legacy-backport-cutoff-point`. The documentation and tools to build thebackports are there, but there will be no more community releases forbackports.Good Bye Backport Providers.",1
"Running tests in parallel (#14531)This is by far the biggest improvements of the test execution timewe can get now when we are using self-hosted runners.This change drives down the time of executing all tests onself-hosted runners from ~ 50 minutes to ~ 13 minutes due to heavyparallelisation we can implement for different test types and thefact that our machines for self-hosted runners are far morecapable - they have more CPU, more memory and the fact thatwe are using tmpfs for everything.This change will also drive the cost of our self-hosted runnersdown. Since we have auto-scaling infrastructure we will simply needthe machines to run tests for far shorter time. Since the numberof test jobs we run on those self hosted runners is substantial(10 jobs), we are going to save ~ 6 build hours per one PR/mergedcommit!This also allows the developers to use the power of theirdevelopment machines - when you use`./scripts/ci/testing/ci_run_airflow_testing.sh` the scriptdetects how many CPU cores are available and it will run asmany parallel test types as many cores you have.Also in case of Integration tests - they require more memory to runall the integrations, so in case there is less than ~ 32 GB of RAMavailable to Docker, the integration tests are run sequentiallyat the end. This drives stability up for machines with lower memory.On one personal PC (64GB RAM, 8 CPUS/16 cores, fast SSD) the fulltest suite execution went down from 30 minutes to 5 minutes.There is a continuous progress information printed every 10 seconds wheneither parallel or sequential tests are run, and the full output isshown at the end - failed tests are marked in red groups, and succesful aremarked in green groups. This makes it easier to see and analyse errors.",0
"Add ability to specify api group and version for Spark operators (#14898)closes: #14897There were add two parameters for SparkKubernetesOperator and SparkKubernetesSensor. I've placed them in the end and provided default values, so there will be backward compatibility.Also added description and tests.",3
"Adds resource check when running Breeze (#14908)When Breeze is run, it requires some resources in the dockerengine, otherwise it will produce strange errors.This PR adds resource check when running breeze - it will printhuman-friendly size of CPU/Memory/Disk available for dockerengine and red error (still allowing Docker to run) when theresources are not enough.Fixes: #14899",0
Fix broken link in dev/README_RELEASE_PROVIDER_PACKAGES.md (#14916)`PROVIDER_PACKAGES.md` was renamed to `PROVIDER_PACKAGE_DETAILS.md`,1
Bump UI packages to latest releases (#14902),3
"Sort lists, sets and tuples in Serialized DAGs (#14909)Currently we check if the dag changed or not via dag_hash.The problem is since the insertion order is not guaranteed, it producesa different hash and hence results in a DB write unncessarily.This commit fixes it.",0
Multiple minor doc fixes (#14917),0
Create a documentation package for Docker image (#14846),2
fix deprecated import path of tests (#14923),3
Update docstrings to adhere to sphinx standards (#14918),2
"Fix typo in doc docker-stack (#14928)Fix a typo on doc , the _AIRFLOW_WWW_USER_PASSWORD_CMD is repeated two times",4
Add exclusions for new node_modules directory (#14935),1
Add documentation on database connection URI (#14124)* Add connection string docs to config.yml* Replace connection string docs with link to documentation* Fix plural typo* Match default_airflow.cfg to config.yml,5
Speed up TestDagRunsEndpoint by using pytest fixtures (#14875),0
Make script_args templated in AwsGlueJobOperator (#14925),1
Acquire lock on db for the time of migration (#10151)We have a situation when both web server and k8s airflow-initdb canstart migrations at the same time.Alembic in no way supports concurrent migrations.The solution is based on https://github.com/sqlalchemy/alembic/issues/633Change-Id: I5b894c947ec2e56efab622357e160e7c300b7b99Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>Co-authored-by: Cloud Composer Team <no-reply@google.com>Co-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>,1
GCS to BigQuery Transfer Operator with Labels and Description parameter (#14881)This adds the following optional parameters to the GCS to BQ transfer operator:labelsdescriptionThese can be set by users to update labels and/or description information in the destination BigQuery table.,5
"Running tests in parallel (#14915)This is by far the biggest improvements of the test execution timewe can get now when we are using self-hosted runners.This change drives down the time of executing all tests onself-hosted runners from ~ 50 minutes to ~ 13 minutes due to heavyparallelisation we can implement for different test types and thefact that our machines for self-hosted runners are far morecapable - they have more CPU, more memory and the fact thatwe are using tmpfs for everything.This change will also drive the cost of our self-hosted runnersdown. Since we have auto-scaling infrastructure we will simply needthe machines to run tests for far shorter time. Since the numberof test jobs we run on those self hosted runners is substantial(10 jobs), we are going to save ~ 6 build hours per one PR/mergedcommit!This also allows the developers to use the power of theirdevelopment machines - when you use`./scripts/ci/testing/ci_run_airflow_testing.sh` the scriptdetects how many CPU cores are available and it will run asmany parallel test types as many cores you have.Also in case of Integration tests - they require more memory to runall the integrations, so in case there is less than ~ 32 GB of RAMavailable to Docker, the integration tests are run sequentiallyat the end. This drives stability up for machines with lower memory.On one personal PC (64GB RAM, 8 CPUS/16 cores, fast SSD) the fulltest suite execution went down from 30 minutes to 5 minutes.There is a continuous progress information printed every 10 seconds wheneither parallel or sequential tests are run, and the full output isshown at the end - failed tests are marked in red groups, and succesful aremarked in green groups. This makes it easier to see and analyse errors.",0
"Add label area:providers to boring cyborg (#14941)Boring Cyborg now only allows labeling using a basic path. When someone changes a big provider (Apache, Google, AWS) the boring cyborg is going to apply two labels: area:providers and provider:Google for example, which is not a problem IMHO.",0
"Optimizes image verification steps. (#14780)So far we had matrix of builds that verified images - eachimage was verified by separate matrix-based job and thoseverifications were run after all images were alredy available.This step optimizes it. Those steps are run in the same jobas ""waiting for image"", also they run in parallel which willmake them a bit faster.This verification is fast and it can be run on any machinein parallel without any problems.",0
"Compare string values, not if strings are the same object (#14942)I found this when investigating why the delete_worker_pods_on_failure flag wasn't working. The feature has sufficient test coverage, but doesn't fail simply because the strings have the same id when running in the test suite, which is exactly what happens in practice.flake8/pylint also don't seem to raise their respective failures unless one side it literally a literal string, even though typing is applied 🤷‍♂️.I fixed 2 other occurrences I found while I was at it.",0
"Fixes default group of Airflow user. (#14944)The production image did not have root group set as default forthe airflow user. This was not a big problem unless you extendedthe image - in which case you had to change the group manuallywhen copying the images in order to keep the image OpenShiftcompatible (i.e. runnable with any user and root group).This PR fixes it by changing default group of airflow userto root, which also works when you extend the image.```Connected.airflow@53f70b1e3675:/opt/airflow$ lsdags  logsairflow@53f70b1e3675:/opt/airflow$ cd dags/airflow@53f70b1e3675:/opt/airflow/dags$ ls -ltotal 4-rw-r--r-- 1 airflow root 1648 Mar 22 23:16 test_dag.pyairflow@53f70b1e3675:/opt/airflow/dags$```",3
"Much easier to use and better documented Docker image (#14911)Previously you had to specify AIRFLOW_VERSION_REFERENCE andAIRFLOW_CONSTRAINTS_REFERENCE to point to the right versionof Airflow. Now those values are auto-detected if not specified(but you can still override them)This change allowed to simplify and restructure the Dockerfiledocumentation - following the recent change in separating outthe docker-stack, production image building documentation hasbeen improved to reflect those simplifications. It should bemuch easier to grasp by the novice users now - very cleardistinction and separation is made between the two types ofbuilding your own images - customizing or extending - and itis now much easier to follow examples and find out how tobuild your own image. The criteria on which approach tochoose were put first and forefront.Examples have been reviewed, fixed and put in a logicalsequence. From the most basic ones to the most advanced,with clear indication where the basic aproach ends and wherethe ""power-user"" one starts. The examples were also separatedout to separate files and included from there - also theexample Docker images and build commands are executableand tested automatically in CI, so they are guaranteedto work.Finally The build arguments were split into sections - from mostbasic to most advanced and each section links to appropriateexample section, showing how to use those parameters.Fixes: #14848Fixes: #14255",0
Update python openapi gen script to generate code in a submodule (#14932),5
chore: Refactor code quality issues (#14920)Signed-off-by: ankitdobhal <dobhal.ankit@protonmail.com>* Remove duplicate elements during set declarationSigned-off-by: ankitdobhal <dobhal.ankit@protonmail.com>* Remove redundant 'None' defaultSigned-off-by: ankitdobhal <dobhal.ankit@protonmail.com>,4
"Docs: Clarify behavior of delete_worker_pods_on_failure (#14958)Clarify that the `delete_worker_pods_on_failure` flag only applies to worker failures, not task failures as well.",0
Google Dataflow Hook to handle no Job Type (#14914)Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,0
hostAliases support for workers in helm chart (#14681)This PR adds workers.hostAliases parameter in helm chart in order to use kubernetes HostAliases in worker pods.,1
doc: Fix typo in `secrets_manager.py` docstring (#14943),2
"Change 1.10.14 to 1.10.15 in README.md (#14971)Now that 1.10.15 is out, time to update README too",5
"Increase default `worker_refresh_interval` to `6000` seconds (#14970)The default value for `[webserver] worker_refresh_interval` was `30` seconds forAirflow <=2.0.1. However, since Airflow 2.0 DAG Serialization is a hard requirementand the Webserver used the serialized DAGs, there is no need to kill an existingworker and create a new one as frequently as `30` seconds.",1
"Revert ""Pre commit new UI (#14836)"" (#14984)This reverts commit e395fcd247b8aa14dbff2ee979c1a0a17c42adf4.",5
"Adds initial router, routes, and placeholder views  (#14927)* Adds initial router, routes, and placeholder views* fix router tests- fix linting with `skipLibCheck`- fix motion warning on test with `resolutions` in `package.json`* remove resolutions in package.json* Upgrade to latest Chakra release* Use username instead of id in route* Add new UI node files to rebuild check* Add linting dependenciesCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>",1
"Fix failing doc build (#14986)- For some reason pymongo's stable inventory fetch is redirecting, I can reproduce it locally too if I try to access https://pymongo.readthedocs.io/en/stable/objects.inv from by browser.",2
"Fix import cycle in cluster policy examples (#14973)If you import anything directly from `airflow` in cluster policy file, an import cycle will prevent the policy from getting loaded.This PR fixes the import in the example and adds a small note warning about this.closes: #14945",2
Fix typo in Helm chart tests (#14979)Fix typo Allways to Always on image policy,2
"Add basic authentication to new UI (#14988)* Adds initial router, routes, and placeholder views* fix router tests- fix linting with `skipLibCheck`- fix motion warning on test with `resolutions` in `package.json`* add login with basic auth* add simple header/logout + readme for env* remove unnecessary library* add licenses* more strict allow origin* remove resolutionCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>",4
"Fixes problem with two different files mdsumed with the same name (#14998)* Fixes problem with two different files mdsumed with the same nameWhen we check whether we should rebuild image, we check if themd5sum of some important files changed - which would triggerquestion whether to rebuild the image or not (because ofchanged dependencies which need to be installed). Thishappens for example when package.json or yarn.lock changes.Previously, all the important files had distinct names, sowe stored the md5 hashes of those files with just filenames +.md5sumbut they were flattened to a single directory. Unfortunately,as of #14927 (merged with failing build) we had two package.jsonand two yarn.locks and it caused overwriting of md5hash of oneby the other. This triggered unnecessary rebuilding of the imagein CI part which resulted in failure (because of Apache Beamdependency problem).This PR fixes it by adding parent directory to the name ofthe md5sum file (so we have www-package.json and ui-package.json)now. Those important files change very rarely so this incidentshould not happen again but we added some comments preventingit.* Update scripts/ci/libraries/_initialization.shCo-authored-by: Felix Uellendall <feluelle@users.noreply.github.com>",1
"Fixes broken asset compilation in Docker images (#14995)The change #14911 had a bug - when PYTHON_MINOR_MAJOR_VERSIONwas removed from the imge args, the replacement `python -m site`expression missed `/airflow/` suffix. Unfortunately it was notflagged as an error because the recompiling script silentlyskipped recompilation step in such case.This change:* fixes the error* removes the silent-skipping if check (the recompilation will  fail in case it is wrongly set)* adds check at image verification whether dist/manifest.json is  present.Fixes: #14991",0
Update grammar & typos in dag-serialization.rst (#14992)Fixing minor stylistic mistakes and some typos,2
"Speed up webserver start when there are many DAGs (#14993)This fixes a short circuit in `create_dag_specific_permissions` toavoid needlessly querying permissions for every single DAG, and changes`get_all_permissions` to run 1 query instead of many.With ~5k DAGs, these changes speed up `create_dag_specific_permissions`by more than 65 seconds each call (on my machine), and since that methodis called twice before the webserver actually responds to requests, thiseffectively speeds up the webserver startup by over 2 minutes.",2
"Remove GH Action sections that are always empty (#15006)These sections only produce output when verbose output is enabled,otherwise. In all other times they just have extra sections to look at.This also removes the log lines about redirected output or settings thatcan be tweaked when run under GitHub Actions -- it's not relevant there!",1
"UI layout containers + navigation (#15007)* Adds layout containers to render navigation- Adds additional placeholder routes/views* Remove ""containers"" directory, move files in context w/in views* Add app container to Login view* Convert static list to use data array* Move/separate Icons and SVGs into separate components* Remove unintention file addition* Update test contex* Make the env var more universal",1
Skips provider package builds and provider tests for non-master (#14996)This PR skips building Provider packages for branches differentthan master. Provider packages are always released from masterbut never from any other branch so there is no point in runningthe package building and tests there,3
UI basic api (#15015),5
"Fix support for long dag_id and task_id in KubernetesExecutor (#14703)The key used to remove a task from executor.running is reconstitutedfrom pod annotations, so make sure the full dag_id and task_id are inthe annotations.",2
Merge contextlib.suppress() calls (#15029),7
"Check if we need to upgrade deps before rebuilding everything (#15033)Due to the order of checkes in selective_ci_checks.sh, we were nevereagerly upgrading deps on a Pull Reuqest, because we exited beforegetting to `check_if_setup_files_changed`.Previoulsy the output was this```Get changed filesCheck if everything should be run  Changed files matching the ^.github/workflows/|^Dockerfile|^scripts|^setup.py|^setup.cfg pattern:  Dockerfile  Dockerfile.ci  setup.py  Important environment files changed. Running everything  ...  image-build=true  upgrade-to-newer-dependencies=false```And then it exited. By simply changing the order we set the right flag_first_ and then exit.",1
Bump mysqlclient to support the 1.4.x and 2.x series (#14978),1
Add missing comma in docs for KubernetesExecutor (#15035),2
Fix Sphinx Issues with Docstrings (#14968)This PR fix some typos and issues were the doctrings do not adhere to sphinx standards in the following modules:- AWSDataSyncOperator- S3KeySizeSensor- DatadogHook- DatadogSensor- ComputeEngineSSHHook- CloudDataCatalogLookupEntryOperator- GoogleDisplayVideo360UploadLineItemsOperator- GoogleDisplayVideo360CreateSDFDownloadTaskOperator- GoogleDisplayVideo360SDFtoGCSOperator- AzureBatchOperator- AzureCosmosDocumentSensor- SingularityOperator- SlackAPIOperator- DataprocCreateMapReduceJobOperator- DataprocCreateSparkJobOperator- DataprocCreatePysparkJobOperator,5
"Don't import mysql exceptions from ""private"" module (#15039)This module was removed/renamed in mysqlclient 2.0 -- this new nameworks for both",1
"UI scaffold views, routes, and layout containers for Runs and Tasks (#15041)",1
Added retry to ECS Operator (#14263)* Added retry to ECS Operator* ...* Remove airflow/www/yarn-error.log* Update decorator to not accept any params* ...* ...* ...* lint* Add predicate argument in retry decorator* Add wraps and fixed test* ...* Remove unnecessary retry_if_permissible_error and fix lint errors* Static check fixes* Fix TestECSOperator.test_execute_with_failures,3
Add more strict Helm Chart schema checks for image pullPolicy & dags accessMode (#15040)`values.schema.json` can be used to help check the structure & the values in `values.yaml`. - In this PR I extend the `values.schema.json` to ensure we have more strict checks on the values users add in `values.yaml`.- Tests are added.,1
"Improve docstrings for various modules (#15047)Update docstring to be consistent with source code params for SQLIntervalCheckOperator and fixes sphinx issue with GoogleAdsListAccountsOperator, among other docstrings.",2
"Upgrades moto to newer version (~=2.0) (#15051)According to https://github.com/spulec/moto/issues/3535#issuecomment-8087069391.3.17 version of moto with a fix to be compatible with mock> 4.0.3 isnot going to be released because of breaking changes. Therefore we needto migrate to newer version of moto.At the same time we can get rid of the old botocore limitation, whichwas added apparently to handle some test errors. We are relying fullyon what boto3 depends on.Upgrading dependencies also discovered that mysql tests need tobe fixed because upgraded version of dependencies cause some testfailure (those turned out to be badly written tests).",3
improve react-query testing (#15043),3
"Adds dill exclusion to Dockerfiles to accomodate upcoming beam fix (#15048)* Upgrades moto to newer version (~=2.0)According to https://github.com/spulec/moto/issues/3535#issuecomment-8087069391.3.17 version of moto with a fix to be compatible with mock> 4.0.3 isnot going to be released because of breaking changes. Therefore we needto migrate to newer version of moto.At the same time we can get rid of the old botocore limitation, whichwas added apparently to handle some test errors. We are relying fullyon what boto3 depends on.Upgrading dependencies also discovered that mysql tests need tobe fixed because upgraded version of dependencies cause some testfailure (those turned out to be badly written tests).* Adds dill exclusion to Dockerfiles to accomodate upcoming beam fixWith the upcoming apache-beam change where mock library will beremoved from install dependencies, we will be able to remove`apache-beam` exclusion in our CI scripts. This will be a finalstep of cleaning dependencies so that we have a trulygolden set of constraints that will allow to install airflowand all community managed providers (we managed to fix all thosedependency issues for all packages but apache-beam).The fix https://github.com/apache/beam/pull/14328 when mergedand Apache Beam is released will allow us to migrate to the newversion and get rid of the CI exclusion for beam.Closes: #14994",1
Override project in dataprocSubmitJobOperator (#14981),5
The PYTHON_MAJOR_MINOR build arg has been deprecated (#15054)The python version is now auto-detected and the formerbuild-arg is deprecated.Time to remove it.,4
"More proper default value for namespace in K8S cleanup-pods CLI (#15060)Currently the default value for namespace is always 'default'.However, `conf.get('kubernetes', 'namespace')` may be a more properdefault value for namespace in this case",5
Fix autocommit calls for mysql-connector-python (#14869)The mysqlclient and mysql-connector-python clients use different methods for getting and setting the autocommit mode.  This code adds checks to get/set_autocommit() to determine if `conn.autocommit` should be used as a property (for mysql-connector-python) or as a method (for mysqlclient)Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>,1
"Remove extra/needless deprecation warnings from airflow.contrib module (#15065)If you have `from airflow.contrib.operators.emr_add_steps_operatorimport EmrAddStepsOperator` line in your DAG file, you get threewarnings for this one line```/home/ash/airflow/dags/foo.py:3 DeprecationWarning: This module is deprecated./home/ash/airflow/dags/foo.py:3 DeprecationWarning: This package is deprecated. Please use `airflow.operators` or `airflow.providers.*.operators`./home/ash/airflow/dags/foo.py:3 DeprecationWarning: This module is deprecated. Please use `airflow.providers.amazon.aws.operators.emr_add_steps`.```All but the last is not helpful.",1
Scheduler: Remove TIs from starved pools from the critical path. (#14476)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,4
"Loosen cassandra-driver requirement to allow latest version (#15022)For py 3.6-3.8 we don't need the CASS_ settings in the Dockerfiles butas Datastax haven't yet published a Python 3.9 wheel, I have kept thesettings for now -- they are ignored when installing a wheel, but willbe used if the python version (i.e. 3.9) doesn't have a published wheel.",1
"Faster default role syncing during webserver start (#15017)This makes a handful of bigger queries instead of many queries whensyncing the default Airflow roles. On my machine with 5k DAGs, this ledto a reduction of 1 second in startup time (bonus, makes tests fastertoo).",3
turn off autocomplete for connection forms (#15073)* turn off autocomplete for connection forms* switch autocomplete to the whole form* add comment,1
Prevent clickable bad links on disabled pagination (#15074),2
"Add different modes to sort dag files for parsing (#15046)This commit adds the feature to allow users to set one of the following modes, the scheduler will list and sort the dag files to decide the parsing order.:- `modified_time`: Sort by modified time of the files. This is useful on large scale to parse the recently modified DAGs first.- `random_seeded_by_host`: Sort randomly across multiple Schedulers but with same order on the same host. This is useful when running with Scheduler in HA mode where each scheduler can parse different DAG files.- `alphabetical`: Sort by filename",2
Pin flynt to fix failing PRs (#15076)A new version of flynt was released 0.63 on 27 March 2021 - https://pypi.org/project/flynt/#history which caused failures on PR due to eager updates. This PR will pin the flynt version used,1
The --force-pull-images is restored in breeze (#15063)It's been accidentally removed during rebase.,4
"Fix typo in task_runner for ``AirflowConfigException`` (#15067)Key name ""executor"" has changed to ""task_runner"" since this value does not belong to the executor, but the task_runner.This PR fixes the typo in the exception message.closes: #14933",2
"Add query mutations to new UI (#15068)* UI: add save and trigger dag mutations* testing, names and table header* use pipeline in url* linting* use humps.decamelize and duration var* missing toast durations",1
Add test to guard how DAG/Operator params work together (#15075),1
Removes references to directory that no longer exists (#15083),4
Pass queue to BaseExecutor.execute_async like in airflow 1.10 (#14861)Any schedulers depending on the queue functionality that haven't overridden`trigger_tasks` method will see queue functionality break when upgrading to 2.0,4
"Make skip_exit_code configurable in BashOperator (#14963)Exit code 127 is used when a command is not found and we don't want toskip those tasks. Exit code 99 was arbitrarily chosen, however, mostimportantly it isn't used as a standard exit code:https://tldp.org/LDP/abs/html/exitcodes.htmlThis also allows users to provide their own `skip_exit_code` if theywant to use a different exit code than the default 99.Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>",1
Assign TS type to fix linting (#15090),0
"Remove 'conf' from search_columns in DagRun View (#15099)i.e. to not support filtering by 'conf' column in DagRun View.This cannot be supported because FAB uses ILIKE under the hood,which is not supported by 'bytea' type in Postgres or 'BLOB' in SQLite.Closes issue #14374",0
"Fixed deprecated code example in Concepts doc (#15098)This document used the now-deprecated import for ""task"";this updates it to come from `airflow.decorators`so it won't raise a DeprecationWarning if copied and used.",1
fix broken link in experimental API deprecation headers (#13547),2
Allow celery workers without gossip or mingle modes (#13880)Arguments: --without_mingle and --without_gossip,1
Added Pretius to the list of companies using Apache Airflow (#14367),1
Fix file permission issue when running git-sync in the KubernetesExecutor mode. (#12441),1
"Parallelize build of documentation. (#15062)This is far more complex than it should be because ofautoapi problems with parallel execution. Unfortunately autoapidoes not cope well when several autoapis are run in parallel onthe same code - even if they are run in separate processes andfor different packages. Autoapi uses common _doctree and _apidirectories generated in the source code and they overrideeach other if two or more of them run in parallel.The solution in this PR is mostly applicable for CI environment.In this case we have docker images that have been already builtusing current sources so we can safely run separate dockercontainers without mapping the sources and run generationof documentation separtely and independently in each container.This seems to work really well, speeding up docs generation2x in public GitHub runners and 8x in self-hosted runners.Public runners:* 27m -> 15mSelf-hosted runners:* 27m -> < 8m",1
Add timeout to test jobs to prevent hanging docker containers (#15078)Some of the test jobs are hanging - either becasue of someweird race conditions in docker or because the test hangs (happensfor quarantined tests). This change add maximum timeout we letthe test suite execute to 25 minutes.,3
"add timezone context in new ui (#15096)* add a timezone provider for new ui* /providers directory, tests and linting* remove config check",5
Doc: Update kubernetes.rst to clarify Kubernetes Connection (#14954),5
Move celery.default_queue to operators.default_queue to re-use between executors  (#14699)The default_queue config option resides in the celery section.We already have an operators config section which would be a better place to specify a default.This also allows other executors to re-use the default queue functionality without having to set celery specific config.closes: #14696,5
Add workers extraVolumes to Kubernetes pod template for Helm Chart (#14743),2
"Re-introduce dagrun.schedule_delay metric (#15105)This was mistakenly removed in the HA scheduler refactor work.It is now added back, and has tests this time so we will notice if itbreaks in future.By using freezegun we can assert the _exact_ of the metric emitted tomake sure it also has the correct value without introducing intiming-based flakiness.",1
Fix deprecated import of `@task` in example DAG (#15111),2
"Allow pathlib.Path in DagBag and various util fns (#15110)We do a lot of path manipulation in this test file, and it's easier tounderstand by using pathlib without all the nested `os.path.*` calls.This change adds ""support"" for passing Path objects to DagBag andutil functions.",1
"Fix doc link permission name (#14972)Converts the docs_link permission resource name to Documentation.This is an extension of #14946, which standardized default FAB permissions.",2
Fixed all remaining code usage of old task import (#15118)This finishes the job of removing all imports of task from`operators.python` and replaces them with the new import from`decorators`.,2
Add REST API query sort and order to some endpoints (#14895),1
"Run UI tests selectively when UI files have changed (#15009)If we only change the new React UI files, we don't want to have to runthe whole python test suite!",3
"Avoid scheduler/parser manager deadlock by using non-blocking IO (#15112)There have been long standing issues where the scheduler would ""stopresponding"" that we haven't been able to track down.Someone was able to catch the scheduler in this state in 2.0.1 andinspect it with py-spy (thanks, MatthewRBruce!)The stack traces (slightly shortened) were:```Process 6: /usr/local/bin/python /usr/local/bin/airflow schedulerPython v3.8.7 (/usr/local/bin/python3.8)Thread 0x7FF5C09C8740 (active): ""MainThread""    _send (multiprocessing/connection.py:368)    _send_bytes (multiprocessing/connection.py:411)    send (multiprocessing/connection.py:206)    send_callback_to_execute (airflow/utils/dag_processing.py:283)    _send_dag_callbacks_to_processor (airflow/jobs/scheduler_job.py:1795)    _schedule_dag_run (airflow/jobs/scheduler_job.py:1762)Process 77: airflow scheduler -- DagFileProcessorManagerPython v3.8.7 (/usr/local/bin/python3.8)Thread 0x7FF5C09C8740 (active): ""MainThread""    _send (multiprocessing/connection.py:368)    _send_bytes (multiprocessing/connection.py:405)    send (multiprocessing/connection.py:206)    _run_parsing_loop (airflow/utils/dag_processing.py:698)    start (airflow/utils/dag_processing.py:596)```What this shows is that both processes are stuck trying to send data toeach other, but neither can proceed as both buffers are full, but sinceboth are trying to send, neither side is going to read and make morespace in the buffer. A classic deadlock!The fix for this is two fold:1) Enable non-blocking IO on the DagFileProcessorManager side.   The only thing the Manager sends back up the pipe is (now, as of 2.0)   the DagParsingStat object, and the scheduler will happily continue   without receiving these, so in the case of a blocking error, it is   simply better to ignore the error, continue the loop and try sending   one again later.2) Reduce the size of DagParsingStat   In the case of a large number of dag files we included the path for   each and every one (in full) in _each_ parsing stat. Not only did the   scheduler do nothing with this field, meaning it was larger than it   needed to be, by making it such a large object, it increases the   likely hood of hitting this send-buffer-full deadlock case!",1
"Remove duplicate call to sync_metadata inside DagFileProcessorManager (#15121)`_process_message` already calls sync_metadata if it's a DagParsingStat,so there's no need to call it again.  This isn't expensive, but no pointdoing itI've also added a runtime check to ensure this function is only used insync mode, which makes the purpose/logic used in this function clearer.",1
Little clarification in Aws connection docs (#14290),2
update remaining old import paths of operators (#15127),1
"Document Airflow's versioning and release policy for users (#14132)This document aims to show users what they can expect and rely upon fromAirflow versions.It is far from complete and we will build it up over time, but creatingthe document from scratch is the hard part!",2
"Fix bug in airflow.stats timing that broke dogstatsd mode (#15132)The fix for this was very easy -- just a `timer` -> `timed` typo.However it turns out that the tests for airflow.stats were insufficientand didn't catch this, so I have extended the tests in two ways:1. Test all the other stat methods than just incr (guage, timer, timing,   decr)2. Use ""auto-specing"" feature of Mock to ensure that we can't make up   methods to call on a mock object.   > Autospeccing is based on the existing spec feature of mock.   > It limits the api of mocks to the api of an original object (the   > spec), but it is recursive (implemented lazily) so that attributes of   > mocks only have the same api as the attributes of the spec. In   > addition mocked functions / methods have the same call signature as   > the original so they raise a TypeError if they are called   > incorrectly.",0
"Don't run UI tests when python files have changed (#15138)The intent of the `set_outputs_run_all_tests` function wasn'timmediately clear to me, so I mistakenly set `needs_ui_tests true` inthere, which resulted in running the React UI test jobs in cases wherewe only changed python files -- a waste of a job!",2
Make app creation session- or class-wide when possible to make tests faster (#14878)* Module-wide app fixture for test_endpoints* Hoist app fixture across experiemental API tests* Merge app usages in API connexion tests* Hoist app creation in cli tests to conftest* Share app instance across methods to save time* Move test out of class to avoid needless setup* Share app across Google OpenID tests* Share app instance with plugin tests* Convert Google Open ID tests to use fixtures,0
Taskgroup decorator (#15034)* Added taskgroup decoratorFixed Typo.Added test case of multicall taskgroupRemoved reference from docsRemoved printExample include in docsUpdate airflow/utils/task_group.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* change taskgroup to task_group* Update airflow/example_dags/example_task_group_decorator.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update docs/spelling_wordlist.txtCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* fix imports* fix spelling* @ashb comments* get tests to pass* Update airflow/utils/task_group.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/utils/task_group.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* @ashb comments* fix static* nit* fix tests* remove inspect stuff* Update airflow/decorators/task_group.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update tests/utils/test_task_group.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* put back arg checkCo-authored-by: Vivek Bhojawala <vbhojawala@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,3
Send region_name into parant class of AwsGlueJobHook (#14251),1
"Fix password masking in CLI action_logging (#15143)Currently as long as argument '-p' if present, code tries to mask it.However, '-p' may mean something else (not password), like a boolean flag. Such cases may result in exception",4
Better handling of docker command (#15080)Not all docker commands are replaced with functions now.Earlier wer replaced all docker commands with a function to be ableto capture docker commands used and display it with -v for breeze.This has proven to be harmful asthis is an unexpected behaviour for a docker command.This change introduces docker_v command which outputs the commandwhen needed.,2
Mark the test_scheduler_task_start_date as quarantined (#15086)Details captured in #15085,5
Fixes failing docs upload on master (#15148),2
fix year in CONTRIBUTING.rst (#15166),0
"A bunch of template_fields_renderers additions (#15130)See #11177. Mostly SQL fields and Python options. Two notable execeptions:* `WinRMOperator.command` is marked as a new renderer `powershell`.* A new `jinja` renderer is implemented. The Pygment lexer is called `DjangoLexer`, but [Pygments suggets rendering Jinja2 with it](https://pygments.org/docs/lexers/#pygments.lexers.templates.DjangoLexer).Fix #14543.",2
AWS: Do not log info when SSM & SecretsManager secret not found (#15120),5
Minor Helm Chart doc enhancements (#15124)- Better formatting for `tip`- Correct listing,1
Store inventories in GitHub Action cache (#15109)fixes #14989.,0
Add core-operators label to boring cyborg automation (#15167),1
Fix documentation error in `git_sync_template.yaml` (#13197)When reading the documentation (https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html?highlight=pod_override#pod-template-file) I noticed that there are errors in `volumes` and `volumeMounts` sections.,0
"Deactivate trigger, refresh, and delete controls on dag detail view. (#14144)Deactivate the trigger, refresh, and delete DAG buttons on the DAG details pages if the current user doesn't have the necessary permissions to perform those actions. This was added to the DAGs list page in 2.0.1, and is now being added to the detail page as well (not including it before was an oversight).",1
Add Guidelines about releasing Providers (#15168)Based on the discussion in https://lists.apache.org/thread.html/rc72dbb2ac6773c1cb1dd573c60fcce191943b5f56dbfb2c98f87f461%40%3Cdev.airflow.apache.org%3E we have agreed to the three guidelines that I have added in this commit:- Batch & Ad-hoc Releases- Frequency- Skipping release when doc-only changes,4
Close issues that are pending response from the issue author (#15170)Based on the meeting notes in https://docs.google.com/document/d/1Fx46SoOnNLiqZKtrC-tOHj3zFlZfQwWuR2LRFXJnWqw/ we had decided that we will automate closing the issues if an issue does not receive response from the issue author in 30 days (+7 days after stale).,0
Restore base lineage backend (#14146)This adds back the base lineage backend which can be extended to send lineage metadata to any custom backend.closes: #14106Co-authored-by: Joao Ponte <jpe@plista.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>,5
"Increase timeout for building the docs (#15157)Sometimes when docs are building in parallel, it takes longerthan 4 minutes to build a big package and the job fails withtimeout.This change increases the individual package build timeout tobe longer (8 minutes instead of 4)",1
"Merges prepare/test provider packages into two jobs (#15152)The 'wheel' package installation tests all optionscomprehensively - including preparing documentationand installing on Airflow 2.0.The 'sdist' package installation takes longer (becausethe packages are converted to wheels on-the-fly bypip), so only basic installation is tested (the restis the same as in case of wheel packages)",3
Finish quarantine for test_should_force_kill_process (#15081)Changing the test to check actual PID of the process to kill,3
Add documentation create/update community providers (#15061),1
"Adds Blinker dependency which is missing after recent changes (#15182)This PR fixes a problem introduced by #14144This is a very weird and unforeseen issue. The change introduced anew import from flask `before_render_template` and this causedflask to require `blinker` dependency, even if it was notspecified before as 'required' by flask. We have not seen itbefore, because changes to this part of the code do not triggerK8S tests, however subsequent PRs started to fail becausethe setup.py did not have `blinker` as dependency.However in CI image `blinker` was installed because it isneeded by sentry. So the problem was only detectable in theproduction image.This is an ultimate proof that our test harness is really good incatchig this kind of errors.The root cause for it is described inhttps://stackoverflow.com/questions/38491075/flask-testing-signals-not-supported-errorFlask support for signals is optional and it does not blinker asdependency, but importing some parts of flask triggers the needfor signals.",2
Minor fixes to Stale Bot (#15184)- Update the Stale Issue message- Rename the workflow to `Close stale PRs & Issues` from `Close stale PRs`,0
"Bump K8S versions to latest supported ones. (#15156)K8S has a one-year support policy. This PR updates theK8S versions we use to test to the latest available in threesupported versions of K8S as of now: 1.20, 1.19. 18.The 1.16 and 1.17 versions are not supported any more as of today.https://en.wikipedia.org/wiki/KubernetesThis change also bumps kind to latest version (we use kind forK8S testing) and fixes configuration to match this version.",5
Fix Providers doc (#15185)`pip pip install -e /path/to/my-package` -> `pip install -e /path/to/my-package`,2
Replace new url for Stable Airflow Docs (#15169)`https://airflow.apache.org/docs/stable/` -> `https://airflow.apache.org/docs/apache-airflow/stable`,2
Pin pandas-gbq to <0.15.0 (#15114),5
Fix mistake and typos in doc/docstrings (#15180)- Fix an apparent mistake in doc relating to catchup- Fix typo pickable (should be picklable),2
Fixes problem when Pull Request is `weird` - has null head_repo (#15189)Fixes: #15188,0
Removes unused CI feature of printing output on error (#15190)Fixes: #13924,0
"Bugfix: Task docs are not shown in the Task Instance Detail View (#15191)closes https://github.com/apache/airflow/issues/15178closes https://github.com/apache/airflow/issues/13761This feature was added in 2015 in https://github.com/apache/airflow/pull/74 and it was expected to set `doc_md` (or `doc_rst` and other `doc_*`) via `task.doc_md` instead of passing via arg. However, this did not work with DAG Serialization as we only allowed a selected args to be stored in Serialized version of DAG.",2
Fix string concatenation using `f-strings` (#15200)Some of the strings had implicit concatentations. This commit fixes it.,0
Add CUD REST API endpoints for Roles (#14840),1
"Bugfix: resources in `executor_config` breaks Graph View in UI (#15199)closes https://github.com/apache/airflow/issues/14327When using `KubernetesExecutor` and the task as follows:```pythonPythonOperator(    task_id=f""sync_{table_name}"",    python_callable=sync_table,    provide_context=True,    op_kwargs={""table_name"": table_name},    executor_config={""KubernetesExecutor"": {""request_cpu"": ""1""}},    retries=5,    dag=dag,)```it breaks the UI as settings resources in such a way is only therefor backwards compatibility.This commits fixes it.",0
Update import path and fix typo in `dag-run.rst` (#15201)1. fix typo parametrized ->  parameterized2. update `from airflow.operators.bash_operator import BashOperator` -> `from airflow.operators.bash import BashOperator`,1
Bugfix: Fix overriding `pod_template_file` in KubernetesExecutor (#15197)This feature was added in https://github.com/apache/airflow/pull/11784 butit was broken as it got `pod_template_override` from `executor_config`instead of `pod_template_file`.closes #14199,2
"Import connections from a file (#15177)* Add connections import CLI command* Add tests for CLI connections import* Add connections import overwrite testWhen a connections file contains collisions with existing connections,skip them and print a message to stdout indicating that the connectionwas not imported.* Resolve lint errors",0
Merges quarantined tests into single job (#15153),3
"Updates 3.6 limits for latest versions of a few libraries (#15209)This PR sets Pythong 3.6 specific limits for some of the packagesthat recently dropped support for Python 3.6 binary packagesreleased via PyPI. Even if those packages did not drop thePython 3.6 support entirely, it gets more and more difficult toget those packages installed (both locally and in the Docker image)because the require the packages to be compiled and they oftenrequire a number of external dependencies to do so.This makes it difficult to automatically upgrade dependencies,because such upgrade fails for Python 3.6 images if we attemptto do so.This PR limits several of those dependencies (dask/pandas/numpy)to not use the lates major releases for those packages but limitsthem to the latest released versions.Also comment/clarification was added to recently (#15114) added limitfor `pandas-gbq`. This limit has been added because of brokenimport for bigquery provider, but the comment about it was missingso the comment is added now.",1
"Add support for modifying celery worker deployment strategy (#15213)This commit modifies the worker template to allow passing a non-default deployment update strategy to worker deployments, in particular celery workers. The values have been set to allow 100% maxSurge and 50% maxUnavailable allowing new deployments of celery workers to launch a full set of replicas before the old set goes away. Allowing the new workers to pick up work as quickly as possible, rather than the current default which is 1 at a time.`maxSurge` is the number of pods that will be scheduled beyond the replica count during a rolling deploy. You can specify specific values or percentages. For example if you set the `maxSurge` to 100% and had 4 replicas, when a rolling deployment started it would launch 4 new pods and then scale down the old ones as the new ones came online.",1
"Separate Kubernetes pod_launcher from core airflow (#15165)* Separate pod_launcher from core airflowCurrently, the KubernetesPodOperator uses the pod_launcher class in airflow core. This means that if we need to fix a bug in the KubernetesPodOperator such as #15137 then the new cncf.kubernetes package will require an Airflow upgrade. Since we hope to release providers in a much faster cadence than Airflow core releases, we should separate this dependency.* fix podlauncher* remove warnings from pod_launcher* fix tests* add deprecated class* fix* fix import* one more nit* fix docs* fix docs again",2
Update .github/CODEOWNERS (#15215)* Remove miklaj from google providers in CODEOWNERS* Update .github/CODEOWNERSCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* Update .github/CODEOWNERS* Update .github/CODEOWNERSCo-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
Less docker magic in docs building (#15176),2
Fix test bug introduced by #15165 (#15221)https://github.com/apache/airflow/pull/15165 introduced a bug in thekubernetes_executor tests. This fixes that bug by changing pytest mock,3
"Constraints are now parallelized and merged in single job (#15211)Originally, the constraints were generated in separate jobs and uploaded asartifacts and then joined be a separate push job. Thanks to parallelprocessing, we can now do that all in a single job, with both cost andtime savings.",7
Fix celery executor bug trying to call len on map (#14883)Co-authored-by: RNHTTR <ryan@wiftapp.com>,1
Build priority packages in separate processes pool (#15214),5
Fix semantic mistake in ISSUE_TRIAGE_PROCESS.rst (#15224),0
Fix deprecated warning hvac auth (#15216)Fixes the following Hashicorp Vault (hvac) approle auth deprecation warning:```DeprecationWarning: Call to deprecated function 'auth_approle'. This method will be removed in version '0.12.0' Please use the 'login' method on the 'hvac.api.auth_methods.approle' class moving forward.```,4
Use context manager to manage pools (#15220),1
"API: Raise `AlreadyExists` exception when the execution_date is same (#15174)This issue occurs when the execution_date is same as previous dag run, raise a AlreadyExists exception with 409 status code instead of 500 errorFIXES: #15150",0
Task Instance model: allow pool names >50 chars (#15203)* Task Instance model: allow pool names >50 chars* Add nullability in migration,1
"Adds 'Trino' provider (with lower memory footprint for tests) (#15187)While checking the test status of various CI tests we came toconclusion that Presto integration took a lot of memory (~1GB)and was the main source of failures during integration tests,especially with MySQL8. The attempt to fine-tune the memoryused turned out in the discovery, that Presto DB stoppedpublishing their Docker image (prestosql/presto) - apparentlyafter the aftermath of splitting-off Trino from Presto.Th split-off was already discussed in #14281 and it was plannedto add support for Trino (which is the more community-drivenfork of the Presto - Presto remained at Facebook Governance,where Trino is an effort continued by the original creators.You can read more about it in the announcement:https://trino.io/blog/2020/12/27/announcing-trino.html. WhilePresto continues their way under The Linux Foundation, Trinolives its own live and keeps on maintaining all artifacts andlibraries (including the image). That allowed us to updateour tests and decrease the memory footprint by around 400MB.This commit:* adds the new Trino provider* removes `presto` integration and replaces it with `trino`* the `trino` integartion image is built with 400MB less memory  requirementes and published as `apache/airflow:trino-*`* moves the integration tests from Presto to TrinoFixes: #14281",0
Fixed #14270: Add error message in OOM situations (#15207)In the case where a child process is reaped early (before we get to it)the presumption in the code is that it is due to an OOM error and we setthe return code -9. This adds an error message alongside that returncode to make it more obvious.,1
Add new Committers to docs (#15235)Announcement Link: https://lists.apache.org/thread.html/rcc95b5e04b14d971567369626eb72411140a31404094582b2769c992%40%3Cdev.airflow.apache.org%3E,2
Updated documentation for provider packages before April release (#15236),1
Fix missing on_load trigger for folder-based plugins (#15208),0
Chart: Update the docs to create Kind cluster (#15237)The old description was outdated. We run with this Helm chart in CI with Kubernetes 1.16+,2
Retry pod launching on 409 ApiExceptions (#15137),1
Refreshed provider's upcoming release with k8s retries (#15239),1
"Revert ""Fixes failing docs upload on master (#15148)"" (#15240)This reverts commit 83d702c345f8f4ce16d32268f4f83ee508fea676.",4
Clear tasks by task ids in REST API (#14500),5
"Fixes pushing constraints (#15243)Afer merging the constraints, the 'recursive' mode was not addedto checkout resulting with non-checked out github push action.This commit fixes it and adds color to diff output in committo better show differences when pushing.",1
Support all terminus task states in Docker Swarm Operator (#14960)* Support all terminus task states in Docker Swarm OperatorDocker reference for task states - https://docs.docker.com/engine/swarm/how-swarm-mode-works/swarm-task-states/* Add Docker Swarm operator test cases for non-complete status,3
Docs: Fix name of setting to use internal IP (#15251)The setting described in the docs is incorrect.Introduced in b1c8c5e,2
"Remove user_id from API schema (#15117)The role schema doesn't seem to contain an ID at all (only the role name), so I just removed the user ID.Part of  #15059.",1
"Add logs to show last modified in SFTP, FTP and Filesystem sensor (#15134)",5
Run kubernetes tests in parallel (#15222),3
Submodules are needed to update constrains (#15242),5
use jquery ready instead of vanilla js (#15258),1
"Refactor/Cleanup Presentation of Graph Task and Path Highlighting (#15257)This is a collection of related updates to improve the overall user experience of the Graph view.I tested these updates across varying DAG shapes/sizes including Task Groups. This testing also covered the hovering/clicking of the task status legend and the search filter functionality.### General- Simplifies JavaScript by removing the redundant inline styling for highlighting tasks and paths between them. This is accomplished by pairing a `data-highlight` attribute with CSS keyed off of it.- I tried not to go overboard (but couldn't help myself in a few places) on the syntax cleanup as that will be handled in the migration to an external JavaScript file for #14115.- Changes the ""edge"" (lines between tasks) curve to be smooth instead of angled straight lines. Single, smooth lines are easier for eyes to follow the path of than multiple jointed lines. This was accomplished by utilizing the `d3-shape` library.- Slightly increases the vertical node separation (`nodeSep`) to make the graphs feel a little less crowded. This also reduces the amount that the task nodes overlap with the edges.- Fixed an unreported bug when you hover or click on ""no_status"" in the status legend at the top, the correlating tasks would not highlight. - Updated the Graph UI screenshot in the docs to reflect the changes in this PR.### Task and Path highlighting- Removes the varying stroke (border) weights of tasks when highlighting/hovering. This interaction isn't necessary given the non-highlighted tasks fade out. It (subjectively) makes for a smoother transition for only the stroke color to change instead of the weight as well.- Improves the styling of the ""path highlighting"". Instead of styling the downstream, hovered, and upstream tasks with 3 different border colors, I've given them all the same ""highlighted"" color and have also given the edges that connect them a similar styling. This simplifies the pattern and visually highlights the actual path between the tasks. The upstream and downstream directions can still easily be deciphered by the edge arrows.### Fixes tooltip jank- Prevents the jumping of the tooltip position that occurs when moving your cursor within the task node- Positions the tooltip slightly higher so it no longer overlaps with the task node's border- Adds a very slight opacity to the tooltip background since it can cover up relevant paths between the hovered task node",1
"Display explicit error in case UID has no actual username (#15212)Fixes #9963 : Don't require a current usernamePreviously, we used getpass.getuser() with no fallback, which errors outif there is no username specified for the current UID (which happens alot more in environments like Docker & Kubernetes). This updates mostcalls to use our own copy which has a fallback to return the UID as astring if there is no username.",1
Remove ``test_deserialization_across_process`` from quarantine (#15264)I have not seen `TestStringifiedDAGs.test_deserialization_across_process` failing from a long time.Based on https://github.com/apache/airflow/issues/10118 -- this is the only test we should un-quarantine for now as I have seen the other tests failing here and there,0
BugFix: CLI 'kubernetes cleanup-pods' should only clean up Airflow-created Pods (#15204)closes: #15193Currently condition if the pod is created by Airflow is not considered. This commit fixes this.We decide if the Pod is created by Airflow via checking if it has all the labels added in PodGenerator.construct_pod() or KubernetesPodOperator.create_labels_for_pod().,1
Docs: Remove extra single quote from example connection (#15265)closes https://github.com/apache/airflow/issues/15260,0
Add a note in set-config.rst on using Secrets Backend (#15274)Clarifying documentation that when using configuration options that are connections (sql_alchemy_conn for example) then they should still be defined as config options in secrets backend and not connection options.,5
Fix typo in a docstring (#15276)This PR fixes a typo in a docstring to GCSListObjectsOperator class.,1
allow hiding of all edges when highlighting states (#15281),1
"Better compatibility/diagnostics for arbitrary UID in docker image (#15162)The PROD image of airflow is OpenShift compatible and it can berun with either 'airflow' user (UID=50000) or with any otheruser with (GID=0).This change adds umask 0002 to make sure that whenever the imageis extended and new directories get created, the directories aregroup-writeable for GID=0. This is added in the defaultentrypoint.The entrypoint will fail if it is not run as airflow user or ifother, arbitrary user is used with GID != 0.Fixes: #15107",0
"Add PythonVirtualenvDecorator to Taskflow API (#14761)To improve the usability of the TaskFlow API, we will add the ability todefine virtualenv environments so users can run tasks withenvironments that do not match that of the Airflow system",5
Moves test_scheduler_keeps_scheduling_pool_full to quarantine (#15256)Recorded in: #15255,3
"Add test to check Valid Affinity, Tolerations & Node Selector for Cleanup Job (#15278)This commit adds tests to check that affinity, tolerations andNode Selectors can be applied on cleanup Cron Job",4
"Synchronize the commiter list (#15292)The list of committers was not fully synchronized with the`/runners/apache/airflow/configOverlay`The name of the parameter was not updated either in the ci.yml.The list of committers is now synchronized, as well as the nameof parameter fixed.",0
"Remove datepicker for task instance detail view (#15284)Closes #15261 by removing the datetimepicker and replacing it with a static heading.The datetimepicker was broken. It is simpler to remove it rather than fix. Also, I don't think it was an effective UX to navigate between task instances even if it were functional.",1
"Add support for labelling DAG edges (#15142)This adds support for putting human-readable labels on edges in the DAGbetween Tasks, as well as making the underlying framework for thatgeneric enough that future metadata could be added if desired.",1
Remove duplicate test utils (#15300)* Remove duplicate test utils* fixup! Remove duplicate test utils,3
"Styling of edge labels when task highlighting (#15298)Part of the resolution of #15140 (paired with #15142)""Edge labels"" are an existing concept in the library that powers the Graph view. This feature will be employed in Airflow the Airflow Graph view with #15142. This PR ensures that the labels are displayed properly when interacting with the Task/path/status highlighting features of the Graph. Primarily, it fades out the labels when not relevant.",5
"Chart: Add tests for tolerations, affinity & node-selector (#15297)This commits adds tests and improve tests coverage for the Helm chartby testing that tolerations, affinity and node-selector can be modifiedfor the following components:- Flower- Pgbouncer- Scheduler- Statsd- Webserver- Worker",1
Entrypoint support in docker operator (#14642)* include entrypoint in DockerOperator* update DockerOperator test with entrypoint* de-duplicate container removal logic* rename get_command method,1
Rename nteract-scrapbook to scrapbook (#15290)* Rename nteract-scrapbook to scrapbook* fixup! Rename nteract-scrapbook to scrapbook* Remove version pin given it's minimal versionCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Migrate graph js (#15307)* move graph js to its own file* remove some eslint-disable* keep node update,5
Migrate task instance log (ti_log) js (#15309)* migrate ti_log js* fix wrap and toggle buttons,0
"Pass environment variables to process with yarn kill command (#15304)Pass environment variables to process with yarn kill commandWhen I manual stop DAG's job, for example, mark it as ""failed"", in Yarn cluster the same job is still running.This error occurs because empty environment is passed to subprocess.Popen and hadoop bin directory doesn't exist in PATH (ERROR - [Errno 2] No such file or directory: 'yarn': 'yarn').Closes #15280",2
"Chart: Allow setting an existing secret for PgBouncer config (#15296)Previously, if a user wanted to supply the username and password to the `users.txt` secret for use by pgbouncer, they had to be set directly in the `values.yaml` file. This change allows users to create this secret out of band (with the `pgbouncer.ini`) and avoid supplying secrets directly.",5
Add picture and examples for Edge Labels (#15310)This builds on the main feature landing in 19b74fd,1
"Fix exception caused by missing keys in the ElasticSearch Record (#15163)Optional LogRecord attributes cannot be added to log_format due to format exception.This happened because ElasticSearch removes keys with null values from the record.Configuration to reproduce. Optional attribute `exc_text` added to `log_format` and `json_fields`:```[logging]remote_logging = Truelog_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s - %%(exc_text)s[elasticsearch]json_format = Truejson_fields = asctime, filename, lineno, levelname, message, exc_text```",2
"Chart: Add tests to check labels, kind and annotations (#15313)This commits adds more unit tests to test:- labels are added to all pods- annotations are added to pods of Scheduler, Worker & Webserver deployment- kind of scheduler and worker deployment as it can be `StatefulSet` or `Deployment`  based on if persistence is enabled or not",0
"Fix url generation for TriggerDagRunOperatorLink (#14990)Fixes: #14675Instead of building the relative url manually, we can leverage flask's url generation to account for differing airflow base URL and HTML base URL.",0
"Ensure executors end method is called (#14085)closes: #11982 SchedulerJob doesn't actually call the executors `end` method when the scheduler receives SIGINT/SIGTERM or when there is an exception (e.g. database connectivity issues).If you are using KuberenetesExecutor, this results in the scheduler not actually exiting.In contrast, if you are using LocalExecutor, this results in the scheduler not dealing with the tasks it is currently running. By ensuring the executors `end` method is called, LocalExecutor will now wait around until the tasks complete or a second SIGINT/SIGTERM/exception. I believe we need to document that behavior change somewhere, I just haven't looked where yet.",4
"Avoids error on pushing PROD image as cache (#15321)After stabilizing the builds on CI, the master builds startedto finally get green - except pushing to prod image cache whichcontinuous to fail as it missed python image to push.This PR fixes it.",0
"Fixes doc for SQSSensor (#15323)As far as I understand, Docstrings for `SQSSensor` seemes to include a mistake.The key for XCom should be 'messages', not 'message'.https://github.com/apache/airflow/blob/0f327788b5b0887c463cb83dd8f732245da96577/airflow/providers/amazon/aws/sensors/sqs.py#L91",1
Add links to new modules for deprecated modules (#15316)closes: #14394Previous PR #15299 had altered an existing by accident. This is the new one that is its own commit.,1
"Fix DAG last run link (#15327)Camelcase used instead of snake case for dag_id, resulting in ""?dag_id=undefined"" link in column ""Last run"" on the DAG list",2
Adds description field in variable (#12413) (#15194)* add description field in variable (#12413)* Change description file to TextCo-authored-by: itroulli* Rebase migration on master* remove length parameter from description column for compatibility with Postgres* update migration revision* delete past migration revisionCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>,4
Remove python2 related handlings and dependencies (#15301)* Remove python2 related handlings and dependencies* fixup! Remove python2 related handlings and dependencies* Keep PythonVirtualenvOperator tests for python2* Keep __init__.py with comments,5
Chart: Allow disabling `git-sync` for Webserver (#15314)closes https://github.com/apache/airflow/issues/11704,0
"Fix `sendgrid` -> `google`. (#15334)Fixes minor typo in Airflow documentation. The documentation talks about `sendgrid`, when the section is about Google Cloud Platform's Secrets Manager.",2
Standardize default fab perms (#14946)* Add back changes.* Add custom view class tests.* Cover missing clear permission.* Add some of the mappings.* Add the rest of the mappings.* Fix permission names.* Fix permission names.* Use standard names for new users endpoints.* Document user access.* Remove unused tests.* Make roles tests pass by cleaning test roles from test_views.py.* Remove old permission names.* Update role tests.* Reorder permissions.* Remove RESOURCE_ROLE_MODEL_VIEW.* Remove db merge.,7
Add Configurable LivenessProbe Values to Scheduler (#15333)- Add configurable livenessProbe values to scheduler component in helm chart- Increase default values to avoid livenessProbe failure- Closes: #15259,0
"Fix Helm GitSync dag volume mount from pod-template-file (#15331)This fixes an issue where the workers cannot find the dag that is meant to run.With git-sync enabled, the airflow_dag location is being set to where the dags are being cloned to, ex `$AIRFLOWHOME/dags/repo/dags`, but the subpath is remapping it to `$AIRFLOWHOME/dags`, thus the workers are not able to find the actually dag. Also related: https://github.com/apache/airflow/blob/master/chart/templates/_helpers.yaml#L333Removing the subpath resolves this issue. I locally ran the helm unit tests, and it succeeded.",3
"Add dynamic connection fields to Azure Connection (#15159)This PR adds custom connection form widgets and behaviors to the Azure Connections defined in the following hooks:  - AzureBaseHook- AzureContainerInstanceHook-  AzureDataExplorerHook- AzureCosmosDBHook- AzureBatchHook- AzureDataFactoryHook- AzureDataLakeHook- AzureContainerRegistryHook- WasbHookThis PR also adds a new connection 'azure_container_registry' for the AzureContainerRegistryHook. The form for both the Azure connection, and the Azure Container Instance connection are identical.screenshots:AzureContainerInstanceHook![Screenshot_2021-04-01 Add Connection - Airflow(1)](https://user-images.githubusercontent.com/63181127/113424206-ba51be80-939d-11eb-94f0-493fffcd9728.png)AzureDataExplorerHook![azure-data-explorer](https://user-images.githubusercontent.com/63181127/113774431-485adb80-96f5-11eb-9288-97c75b77f502.png)AzureCosmosDBHook![image](https://user-images.githubusercontent.com/63181127/113774462-53157080-96f5-11eb-9e21-71e63659de83.png)AzureBatchHook![azure-batch](https://user-images.githubusercontent.com/63181127/113774499-60caf600-96f5-11eb-82d6-b05fa57b25da.png)AzureDataFactoryHook![azure-data-factory](https://user-images.githubusercontent.com/63181127/113774580-76d8b680-96f5-11eb-802d-abd2c4784f84.png)AzureDataLakeHook![azure-data-lake](https://user-images.githubusercontent.com/63181127/113774715-a12a7400-96f5-11eb-890a-2e5fe5621492.png)AzureContainerRegistryHook![image](https://user-images.githubusercontent.com/63181127/113774632-8657ff80-96f5-11eb-878d-5293045b967b.png)WasbHook![Azure-blob](https://user-images.githubusercontent.com/63181127/113774745-aedff980-96f5-11eb-8cd2-4c64dbb3eaf7.png)",5
"Change default of `[kubernetes] enable_tcp_keepalive` to `True` (#15338)We've seen instances of connection resets happening, particularly inAzure, that are remedied by enabling tcp_keepalive. Enabling it bydefault should be safe and sane regardless of where we are running.",1
Updates provider release process (#15339)Updates the provider release process with approach whereFinal PyPI upload are the same as SVN uploads.This way we avoid re-building packages again when we release PyPIversion and the files which get uploaded to PyPI are the same thatare stored in SVN.,1
Add support for arbitrary json in conn uri format (#15100)Currently in airflow web UI and the CLI you can store arbitrary (e.g. nested) json in the `extra` field.  But the URI format can only handle primitive key-value pairs.  This PR provides support for arbitrary json in the URI format.  Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
S3Hook.load_file should accept Path object in addition to str (#15232)Co-authored-by: Daniel Standish <dstandish@users.noreply.github.com>,1
"Do not forward cluster-identifier to psycopg2 (#15360)`cluster-identifier` is only used by botocore to fetch Redshift credentials.   If passed to psycopg2, it will produce error `psycopg2.ProgrammingError: invalid dsn: invalid connection option ""cluster-identifier""`.Co-authored-by: Jordan Zhang <jorzhang@justin.tv>",0
Do not remove 'full-tests-needed' when approval missing (#15175),3
"Import Connection lazily in hooks to avoid cycles (#15361)The current implementation imports Connection on import time, whichcauses a circular import when a model class needs to reference a hookclass.By applying this fix, the airflow.hooks package is completely decoupledwith airflow.models on import time, so model code can reference hooks.Hooks, on the other hand, generally don't reference model classes.Fix #15325.",0
Make James Timmins a code owner for everything permissions related (#15366),1
"Require `name` with KubernetesPodOperator (#15373)We will require a name when using KPO without pod_template_file orfull_pod_spec. Previously it would use name literallywithout being randomized, which is likely to confuse users duringruntime. This emits an exception during parsing instead.This also adds test coverage around pod_template_file andfull_pod_spec.closes: #15326",2
Undeprecate private_key option in SFTPHook (#15348)Remove the deprecation warning for the private_key option in SFTPHook. There are valid use cases for storing a private key in the connection object (e.g. storing them in connections backed by an external secrets backend).,1
Remove unused packages (#15383),1
"Update CODEOWNERS file for Documentation (#15387)It is difficult for me to review PRs for all Providers, mostof the PRs with Providers touch /docs/apache-airflow-provider-*so I get tagged -- hence I am limiting the docs area I can efficientlyreview",2
"Revert ""Fix Helm GitSync dag volume mount from pod-template-file (#15331)"" (#15390)This reverts commit e4c0689535f1353c9e647773c06bedf8cd22b239.",4
fix docstring typos (#15392),2
"Add documentation for the HTTP connection (#15379)Add documentation for the http connection so it is more clear to a new user how to set them up. This PR also links the new documentation to operators, hooks, and sensors that use a http connection.",1
"Bugfix: ``TypeError`` when Serializing & sorting iterables (#15395)This bug got introduced in #14909. Removed sorting from list and tuple as list & tuples preserve order unlike set.The following DAG errors with: `TypeError: '<' not supported between instances of 'dict' and 'dict'````pythonfrom airflow import modelsfrom airflow.operators.dummy import DummyOperatorfrom datetime import datetime, timedeltaparams = {    ""staging_schema"": [{""key:"":""foo"",""value"":""bar""},                       {""key:"":""this"",""value"":""that""}]}with models.DAG(dag_id='test-dag',                start_date=datetime(2019, 2, 14),                schedule_interval='30 13 * * *',                catchup=False,                max_active_runs=1,                params=params                ) as dag:    my_task = DummyOperator(        task_id='task1'    )```Full Error:```  File ""/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py"", line 210, in <dictcomp>    return cls._encode({str(k): cls._serialize(v) for k, v in var.items()}, type_=DAT.DICT)  File ""/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py"", line 212, in _serialize    return sorted(cls._serialize(v) for v in var)TypeError: '<' not supported between instances of 'dict' and 'dict'During handling of the above exception, another exception occurred:...```This is because `sorted()` does not work with dict as it can't compare. Removed sorting from list & tuples which fixes it.It also fails when we have set with multiple types.",1
Don't try to push the python build image when building on release branches (#15394)They use the same python image as master (as already mentioned in thecomments in ci_prepare_prod_image_on_ci.sh) so we don't want to tryand push the python image when we aren't building the main branch.,1
"Adds a test for the description field in variable  (#15400)Related: #15194In continuation of PR #15194, I've added a test for the description field of the variable.",3
Share app instance between Kerberos tests (#15141),3
Add changelog for what will become 2.0.2 (#15380),4
Update azure connection documentation (#15352)Update the documentation on connecting to azure services. This PR creates a page in docs for each azure connection that explains how to setup the connection. This PR also connects the conn_id params in hooks and operators docstrings to connection documentation as shown below.,2
Persist tags params in pagination (#15411),2
Add documentation for Databricks connection (#15410)Co-authored-by: Kenten Danas <kentendanas@Kentens-MacBook-Pro.local>,5
"Add Traceback in LogRecord in ``JSONFormatter`` (#15414)Currently traceback is not included when ``JSONFormatter`` is used.(`[logging] json_format = True`) . However, the default Handlerincludes the Stacktrace. To currently include the trace we need toadd `json_fields = asctime, filename, lineno, levelname, message, exc_text`.This is a bigger problem when using Elasticsearch Logging with:```ini[elasticsearch]write_stdout = Truejson_format = Truejson_fields = asctime, filename, lineno, levelname, message, exc_text[logging]log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s - %(exc_text)s```Running the following DAG with the above config won't show trace:```pythonfrom airflow import DAGfrom airflow.operators.python import PythonOperatorfrom airflow.utils.dates import days_agowith DAG(    dag_id='example_error',    schedule_interval=None,    start_date=days_ago(2),) as dag:    def raise_error(**kwargs):        raise Exception(""I am an exception from task logs"")    task_1 = PythonOperator(        task_id='task_1',        python_callable=raise_error,    )```Before:```[2021-04-17 00:11:00,152] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: example_python_operator.print_the_context 2021-04-17T00:10:57.110189+00:00 [queued]>......[2021-04-17 00:11:00,298] {taskinstance.py:1482} ERROR - Task failed with exception[2021-04-17 00:11:00,300] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=example_python_operator, task_id=print_the_context, execution_date=20210417T001057, start_date=20210417T001100, end_date=20210417T001100[2021-04-17 00:11:00,325] {local_task_job.py:146} INFO - Task exited with return code 1```After:```[2021-04-17 00:11:00,152] {taskinstance.py:877} INFO - Dependencies all met for <TaskInstance: example_python_operator.print_the_context 2021-04-17T00:10:57.110189+00:00 [queued]>......[2021-04-17 00:11:00,298] {taskinstance.py:1482} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1138, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1311, in _prepare_and_execute_task_with_callbacks    result = self._execute_task(context, task_copy)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1341, in _execute_task    result = task_copy.execute(context=context)  File ""/usr/local/lib/python3.7/site-packages/airflow/operators/python.py"", line 117, in execute    return_value = self.execute_callable()  File ""/usr/local/lib/python3.7/site-packages/airflow/operators/python.py"", line 128, in execute_callable    return self.python_callable(*self.op_args, **self.op_kwargs)  File ""/usr/local/airflow/dags/eg-2.py"", line 25, in print_context    raise Exception(""I am an exception from task logs"")Exception: I am an exception from task logs[2021-04-17 00:11:00,300] {taskinstance.py:1532} INFO - Marking task as FAILED. dag_id=example_python_operator, task_id=print_the_context, execution_date=20210417T001057, start_date=20210417T001100, end_date=20210417T001100[2021-04-17 00:11:00,325] {local_task_job.py:146} INFO - Task exited with return code 1```",5
"[Airflow-15245] - passing custom image family name to the DataProcClusterCreateoperator (#15250)* [airflow-15245] - custom_image_family added as a parameter to DataprocCreateClusterOperatorSigned-off-by: ashish <ashishpatel0720@gmail.com>* [airflow-15245] - test added to check both custom_image and custom_image_family must not be passedSigned-off-by: ashish <ashishpatel0720@gmail.com>* [airflow-#15245] - typo fixed in documentationSigned-off-by: ashish <ashishpatel0720@gmail.com>* [Airflow-15245] - comments updated, more info provided.* [Airflow-15245] - sanity check added for image_version and custom_image_family.* Update airflow/providers/google/cloud/operators/dataproc.pyCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>* Apply suggestions from code reviewCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>* [Airflow-15245] - added a test case to verify the generated cluster config is as expected with custom_image_family and single_node.* Remove print() from test caseCo-authored-by: Ashish Patel <Ashish.Patel@walmartlabs.com>Co-authored-by: Xinbin Huang <bin.huangxb@gmail.com>",3
"Better ""dependency already registered"" warning message for tasks #14613 (#14860)",2
"Sync DAG specific permissions when parsing (#15311)This POC allows the DAG specific permissions to be created/updated during DAG parsing, instead of during webserver start or cli `sync-perm`.With a large number of DAGs, walking through them all to do DAG specific permissions isn't exactly fast and they can only change during the scheduler parsing anyways. Overall more efficient as we don't need to check every DAG as well, we only need to check a given DAG when it changes.This also fixed a bug where the default webserver DAG specific syncing didn't handle `access_control`.Closes #8609",0
Fix contributing page broken link (#15430),2
"Clarify installation of new packages in docker-compose env (#15433)The problem with installing new packages in the Docker-compose environment is repeated very often in discussions on Slack, so I would like to update this tutorial to make this task easier.See: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1618838584433200",1
"Clarifies installation/runtime options for CI/PROD images. (#15320)After PROD images were added, some of the flags had two meaningsThese behaved differently in PROD image and CI image and were thesource of confusion especially when start-airflow command was used.For PROD image, the image can be customized during image building,and packages could be installed from .whl or .sdist packagesavailable in `docker-context-files`. Thisis used at CI and dockerhub building time to produce image builtpackages that were prepared using local sources.The CI image is always built from local sources but airflow canbe removed and re-installed at runtime from pypi.Both airflow and provider packages can be installedfrom .whl or .sdist packages available in dist folder. This isused in CI to test current provider packages with olderAirflow released (2.0.0) and to test provider packages locally.After the change we have two sets of flags/variables:PROD image (building image):* install-airflow-version, install-airflow-reference,  install-from-docker-context-filesCI image (runtime):* use-airflow-version, use-packages-from-distThat should avoid confusion and failures of commands such as`start-airflow` that is used to test provider packages andairflow itself.",1
"Prevent creating flask sessions on REST API requests (#15295)* Prevent creating flask sessions on REST API requestsCurrently, flask sessions are created on API requests. This PR prevents itby setting a value in flask global object g and customizing cookie sessioncreation* fixup! Prevent creating flask sessions on REST API requests* Move session interface modification logic to the security manager for easy debugging and customization* Update tests/api_connexion/test_security.pyCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>* Add comment for better understanding* Remove login_from_api* Use blueprint to detect url* Move the DefaultSessionInterface to init_session and rename itCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
"Fix incorrect slots stats when TI ``pool_slots > 1`` (#15426)Fixes the incorrect number of queued and running slots, and therefore, open slots, when there exist task instances that occupy > 1 pool_slots. This was causing the scheduler to over-commit to a given pool, and a subsequent state where no further tasks can be scheduled because slots cannot be freed.closes: #15399",1
"Bugfix: Invalid name when trimmed `pod_id` ends with hyphen in ``KubernetesPodOperator`` (#15443)When a pod name is more than `MAX_LABEL_LEN` (63 characters), it is trimmed to 63 charshttps://github.com/apache/airflow/blob/8711f90ab820ed420ef317b931e933a2062c891f/airflow/kubernetes/pod_generator.py#L470-L472and we add a safe UUID to the `pod_id` joined by a dot `.`. However the regexfor Kubernetes name does not allow `-` followed by a `.`.Valid Regex:```^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$```This commit strips any hypens at the end of the trimmed `pod_id`.",1
Fixed shellcheck error with static checks (#15450)Shellcheck released today broke static checks. This PR fixes itand pins shellcheck to specific version to avoid it in the future.,0
Further fix trimmed `pod_id` for `KubernetesPodOperator` (#15445)Missed a case in (#15443) where `.` can be followed by another `.`.,1
Fixes publishing instructions to include docker-stack (#15452),2
The scheduled Quarantined build is removed (#15436)This build is not really needed any more gathering statsabout quarantined builds was not very successful experiment.,4
Fixes wrongly specified path for leveldb hook (#15453)Fixes #15451,0
Fix used_group_ids in partial_subset (#13700) (#15308)closes: #13700This should address the root cause by removing any task_ids that are not in the subset DAG from the `used_group_ids`,1
"Add worker_pod_pending_timeout support (#15263)Instead of allowing pending worker pods to be stuck in pending forever,we define a timeout after which point they will be deleted and marked asfailed. This allows the retry mechanism to be applied to these tasks aswell.closes: #15218",1
Fixed default XCom deserialization. (#14827)This is a fix for migrating with the default enable_xcom_pickling (before & after).I.e. pickle-d (old) and JSON (new) data both existing in database.,5
"Change 2.0.1 to 2.0.2 in docs (#15459)Since 2.0.2 was released yesterday, our guides and Breeze should pointto that.",2
Fix docstring of SqlSensor (#15466)Fix docstring of SqlSensor by removing `:` from `type:`.,4
Fix typo in DataprocCreateClusterOperator (#15462)Fix typo in DataprocCreateClusterOperator `param` was accidentally written as `parm`,2
"Fixes constraint generation for pypi providers (#15470)* Fixes constraint generation for pypi providersThe constraints generated from PyPI version of providers, missedcore requirements of Airflow, therefore the constraints were notconsistent with setup.py core requirements.Fixes: #15463",0
"Fix deprecated provider aliases (#15465)Deprecated provider aliases (e.g. kubernetes -> cncf.kubernetes) shouldinstall the provider package (e.g.  apache-airflow-provider-cncf-kubernetes)by default, not the requirements for the provider package. This behaviorwas accidentally broken.",1
Breeze should load local tmux configuration in 'breeze start-airflow'  (#15454),5
Fix the operator name - LocalFileSystemToGCSOperator (#15478),5
"Automatically replace current Airflow version in docs (#15484)There are a number of places where we want the current Airflow versionto appear in the docs, and sphinx has this build in, `|version|`.But sadly that only works for ""inline text"", it doesn't work in codeblocks or inline code. This PR also adds two custom plugins that makethis work inspired byhttps://github.com/adamtheturtle/sphinx-substitution-extensions (butentirely re-written as that module Just Didn't Work)",1
"Bugfix: Fix rendering of ``object_name`` in ``GCSToLocalFilesystemOperator`` (#15487)https://github.com/apache/airflow/pull/14918 made coms consistency changeswhere the template_fields was changed for ``GCSToLocalFilesystemOperator``.While the change was correct since``object`` param was deprecated, theinstance attribute wasn't updated hence you see this error:```AttributeError: 'GCSToLocalFilesystemOperator' object has no attribute 'object_name'```",5
"Chart: Fix passing upgrading strategies to Celery Worker (#15477)We create a Statefulset if persistence is enabled and Deploymentotherwise. Upgrading strategies are different for both those typesof resources.Statefulst needs `updateStrategy`, deployment needs `strategy`.Without this fix, using Helm Chart with CeleryExecutor failed with```helm install airflow -n airflow --set executor=CeleryExecutor --set images.airflow.repository=k8s-dags --set images.airflow.tag=1.0.2 .Error: unable to build kubernetes objects from release manifest: error validating """": error validating data: ValidationError(StatefulSet.spec): unknown field ""strategy"" in io.k8s.api.apps.v1.StatefulSetSpec```",1
Update Docstrings of Modules with Missing Params (#15391)Added params and descriptions to docstrings to improve documentation of the following modules - MySqlHook - RedshiftHook - CloudSQLHook - BigQueryHook - OracleHook - MongoHook - HdfsSensor - ZendeskHook - ElasticsearchHook,1
Cleanup KubernetsPodOpertor tests (#15475),3
"Skip DAG perm sync during parsing if possible (#15464)For DAGs without `access_control` that already have their PermissionViewrecords, we can skip syncing their permissions during parsing. This cutsdown on database queries and is faster (~2 seconds, mostly import time).",2
Fix typo in Breeze message on OSX (#15498)`ou` -> `you`,2
Chart: Change default Airflow version to 2.0.2 (#15497)2.0.2 is more stable than 2.0.0 so this commit changes the defaultAirflow version to 2.0.2,4
"Speeds up Docker build process by combining RUN and ENV commands (#15438)The Dockerfile is more ""packed"" and certain ARG/ENVs are in separateparts of it but we save minutes in certain scenarios when the imagesare built (especially when they are built in parallell, thedifference might be significant)This change also removes some of the old, already unused CASS_DRIVERARGS and ENVS. They are not needed any more as cassandra drivers donot require CPYTHON compilation any more.",1
"Auto refresh on Tree View (#15474)* initial working example* keep transitions when navigating branches* replace missing transitions* add shared get_tree_data function* Update airflow/www/views.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/www/views.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* move dagId to dag.html* make, class method, typing and linting* resolve Ryan's comments* keep space for loading dots* clean up locals* check that dataInstance and row are valid* Update airflow/www/views.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",5
Fixes failing tests for helm chart after changing default version (#15505)After changing default Airflow version in #15497 the test startedfailing. This change fixes it and makes the test works no matterthe version.,1
Update README.rst (#15506),5
"Execute ``on_failure_callback`` when SIGTERM is received (#15172)Currently, on_failure_callback is only called when a task finishesexecuting not while executing. When a pod is deleted, a SIGTERM issent to the task and the task is stopped immediately. The task isstill running when it was killed and therefore on_failure_callbackis not called.This PR makes sure that when a pod is marked for deletion and thetask is killed, if the task has on_failure_callback, the callbackis called.Closes: #14422",0
"Fix labels on the pod created by ``KubernetsPodOperator`` (#15492)When using `pod_template_file` or `full_pod_spec`, Pod identifying labelswere not applied to the POD which meant `reattach_on_restart` did notwork for them.```python{""dag_id"": ""dag"",""kubernetes_pod_operator"": ""True"",""task_id"": ""task"",""try_number"": ""1"",""airflow_version"": 2.0.2,""execution_date"": ""2016-01-01T0100000100"",}```This commit fixes that and makes the labels consistent whether usersuse `pod_template_file`, `full_pod_spec` or just pass params toKubernetesPodOperator.closes https://github.com/apache/airflow/issues/13918",0
"Use pull_request.user, not actor to determine PR user (#15504)In most cases these are the same -- the one exception is when(re)opening an issue, in which case the actor is going to be someonewith commit rights to a repo, and we don't want the mere act ofre-opening to cause a PR to run on self-hosted infrastructure as thatwould be surprising (and potentially unsafe)",5
Add code style note: no asserts (#15512)Some time ago we agreed to no use plain assert outside tests.This was voted but is missing from our 'coding style' guide.,3
"Fix timeout when using XCom with KubernetesPodOperator (#15388)* Fix timeout when using XCom with KubernetesPodOperatorCurrently, the xcom sidecar container for the KubernetesPodOperator willsleep for 30 seconds before checking if the xcom has completed. This isfar too long of a wait, as a 1 second wait will ensure that the processis not consistently blocked.* get rid of unused variable* remove unecessary poddefaults* update docs* Add UPDATING* more description* fix tests* fix tests* Finallyfound it",3
"Removes unnecessary AzureContainerInstance connection type (#15514)The AzureContainerInstanceHook was derived from AzureBaseHookand it did not add any new connection fields. Insteadit duplicated the tennantId and subscriptionId extr behaviour,but it never worked, because it duplicated the fieldsfrom the AzureBaseHook - and caused a lot of warnings whenAzureProvider was added.This change sets the default connection type for theAzureContainerInstanceHook to be azure_default and theconnection type to be 'azure'.Those defaults are much more 'sane'. The existing connectionswill continue to work, only when you open them in the UI, theywill display the extras directly rather than in dedicated fieldsuntil the user changes the connection type to ""azure"" which willfix the display.So no disruptions, just temporary UI/Editing glitch. No more warningsprinted when Azure provider is added.",1
"Add Connection Documentation to more Providers (#15408)* add/update connections docs for ftp, sftp, ssh and snowflake.* fix errors* add to spelling list* fix ftp conn id",0
Update IMAGES.rst (#15522)Fix broken link to docs/docker-stack/index.html,2
Fix AthenaSensor calling AthenaHook incorrectly (#15427),1
Updated manual PROD image push and verification steps (#15449),5
"Fix Task Adoption in ``KubernetesExecutor`` (#14795)Ensure that we use ti.queued_by_job_id when searching for pods. The queued_by_job_id is used byadopt_launched_task when updating the labels. Without this, after restarting the schedulera third time, the scheduler does not find the pods as it is still searching for the id ofthe original scheduler (ti.external_executor_id)Co-Authored-By: samwedge <19414047+samwedge@users.noreply.github.com>Co-Authored-By: philip-hope <64643984+philip-hope@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
Change KPO node_selectors warning to proper deprecationwarning (#15507)Changes the warning KPO raises when `node_selectors` is used into a `DeprecationWarning` and also simplifies that code path.,2
Finish refactor of DAG resource name helper (#15511)This finishes moving `prefixed_dag_id` into `airflow.security.permissions.resource_name_for_dag`.,2
"Fix `logging.exception` redundancy (#14823)Having both `logging.exception` and any other `logging` methods side by side seems unnecessary, and may lead to misread.- Per [doc](https://docs.python.org/3/library/logging.html#logging.Logger.exception), the first argument of `logging.exception` is a string-like `msg`.- `logging.exception(msg)` ≡ `logging.error(msg, exc_info=True)`- `exc_info=True` will add `str(exception)` at the end of trace anyway.",1
"Use Pip 21.* to install airflow officially (#15513)* Use Pip 21.* to install airflow officiallyThe PIP 20.2.4 was so far the only officially supported installationmechanism for Airflow as there were some problems with conflictingdependencies (which were ignored by previous versio of PIP).This change attempts to solve this by removing a [gcp] extrafrom `apache-beam` which turns out to be the major source ofthe problem - as it contains requirements to the old version ofgoogle client libraries (but apparently only used for tests).The ""apache-beam"" provider migh however need the [gcp] extrafor other components so in order to not break the backwardscompatibility, another approach is used.Instead of adding [gcp] as extra in the apache-beam extra,the apache.beam provider's [google] extra is extended with'apache-beam[gcp]' additional requirement so that whenever theprovider is installed, the apache-beam with [gcp] extra is installedas well.* Update airflow/providers/apache/beam/CHANGELOG.rstCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Update airflow/providers/apache/beam/CHANGELOG.rstCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Update airflow/providers/google/CHANGELOG.rstCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Update airflow/providers/google/CHANGELOG.rstCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",4
Make Airflow code Pylint 2.8 compatible (#15534),1
Fixes wrong limit for dask for python>3.7 (should be <3.7) (#15545),0
Update INTHEWILD.md (#15550)Add Menhir into the wild!,1
Add Connection Documentation for Providers (#15499)This PR documents connections for the following providers- MySQL- Slack- Tableau- Neo4j,1
Add Connection Documentation for Popular Providers (#15393)This PR adds and updates documentation for connecting to popular providers. It also adds links to this documentation in the doc strings of modules that use each connection. Documentation for the following connections is improved or updated:- IMAP- Docker- Postres- Kubernetes- Mongo- Spark,2
"Add description about ``secret_key`` when Webserver > 1 (#15546)when running more than 1 intances of websever, make sure all of them use the same ``secret_key`` otherwise one of them will error with ""CSRF session token is missing"".closes https://github.com/apache/airflow/issues/15532",0
"Fix kube client on mac with keepalive enabled (#15551)`socket` on mac doesn't support `TCP_KEEPIDLE`, so when keepalive isenabled we should check before trying to set it so the defaults work onmacs. We will be defensive with the other `TCP_KEEP*`s as well.https://github.com/websocket-client/websocket-client/blob/719fc9d2e50e16292b4f09ca86385540456a55a7/websocket/_socket.py#L37-L44",1
"Adds interactivity when generating provider documentation. (#15518)The release manager, when reviewing providers to release mightmake interactive decisions what to do:1) mark certain provider as 'doc-only' change2) decide whethere to generate documentation for the providerIn case the provider change is marked as 'doc-only' the next timewhen providers are checked the doc-only change is not seen as'change' and the provider is automatically skipped.This saves time when preparing subsequent releases of providersas all the ""doc-only changes"" from the previous release do nothave to be re-reviewed (unless there are some new changes).",4
"Upgrade hadolint to version 2.3.0 (#15573)Newer versions of hadolint hint about more Docker problems:* consecutive RUN operation* invalid labelsThis PR fixes all the problems reported in our dockerfilesby the latest hadolint and refreshes all our images used in CIand chart so that corrected label names are included (one ofthe errors in all our dockerfiles turned out to be camel-caseand - in label keys, which is not valid according toDocker label key specification.Fixes: #15544",0
Fix doc typo (#15584),2
Update pre-commit checks (#15583),5
"`func.sum` may returns `Decimal`  that break rest APIs (#15585)`sqlalchemy.func.sum` has a known *""issue""* that it **may** returns `Decimal` value (_in my case MySQL 5.7_).This will cause problem when calling [rest APIs](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#tag/Pool). E.g.```httpGET /airflow/api/v1/pools?limit=100...TypeError: Object of type 'Decimal' is not JSON serializable```",5
"Serve logs with Scheduler when using Local or Sequential Executor (#15557)Currently, the `serve_logs` endpoint only exists on Celery workers. Thismeans if someone launches Airflow with the `LocalExecutor` and wants tograb the logs from the scheduler, there is no way to move that to thewebserver if it is on a different pod/machine.This commit makes the scheduler automatically serves logs when using`LocalExecutor` or `SequentialExecutor`. However, it means forAirflow <= 2.0.2, the Helm Chart won't serve logs.closes https://github.com/apache/airflow/pull/15070closes https://github.com/apache/airflow/issues/13331closes https://github.com/apache/airflow/issues/15071closes https://github.com/apache/airflow/issues/14222",0
add an option to trigger a dag w/o changing conf (#15591),5
"An initial rework of the ""Concepts"" docs (#15444)This takes the old, single-page Concepts document and turns it into a newConcepts section, rewrites some of the text to have consistent terminology(upstream/downstream, Task vs Operator, etc.) and tries to make it generallymore accessible and high-level. It also moves a couple of other pages in thatdirectly fit with the existing content (Scheduler and Smart Sensors).",4
"Change helm chart logging level back to default (INFO) (#15597)The chart shouldn't default the logging level to DEBUG. Instead, we willelect DEBUG when we are running with breeze and let normal releases usethe Airflow default of INFO.",5
Adds log persistence to helm chart (#15595)This adds the option to use a PVC (one provisioned by the chart or anexisting claim) to persist Airflow logs. This provides an executoragnostic way to persist all of the Airflow logs.,2
Remove odbc dependency in microsoft.mssql provider (#15594),1
Fix KEDA Autoscaler ``connectionFromEnv`` (#15561),0
"Fix parallelism after KubeExecutor pod adoption (#15555)* Fix parallelism after KubeExecutor pod adoptionThis fixes a bug where adopted pods didn't count as being run by theexecutor for the purposes of honoring parallelism, nor for metrics beingexported. Now adopted tasks will be reflected like a task the executoractually started.* Fix circular import",2
"Improves production image preparation scripts. (#15577)This change improves the process of image preparation in DockerHuband manual version of it, in case the DockerHub automation doesnot work. It introduces the following changes:* The ""nightly-master"" builds were failing because they tried  to prepare packages without the ""dev"" suffix (such packages  are skipped now in case package with the same version has  already been released). The ""dev"" suffix forces the packages  to be build.* VERBOSE_COMMAND variable is removed to get more readable output  of the script.* Image verification is now part of the process. The images are  automatically tested after they are built and the scripts  will not push the images if the images do not pass the  verification.* Documentation is updated for both RC and final image preparation  (Previous update did not update the RC image preparation)* Documentation is added to explain how to manually refresh the  images in DockerHub in case the nightly builds are not running  for a long time.",1
"Chart: Change default executor to ``CeleryExecutor`` (#15603)``CeleryExecutor`` is currently the most stable of all, so it should be the default when using the Helm Chart.",2
Use latest version of `git-sync` image (#15606)This commit changes `git-sync` container image from `3.1.6` to `3.3.0` (the latest version)https://github.com/kubernetes/git-sync/releases,3
"Expose snowflake query_id in snowflake hook and operator, support multiple statements in sql string (#15533)",1
"Remove the limit on Gunicorn dependency (#15611)It seems that the < 20.0 limit for gunicorn was added at some pointin time without actual reason. We are already using gunicorn in1.10 line of Airflow, so it should not be a problem to bump theversion of gunicorn, especially that the 19. line is somewhatdeprecated already.This change came after the discussion n #15570",4
Add test for the create user job in helm chart (#15614)This change adds test for the create user job and ensures changing of the uid is detectedCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,4
Fix changing the parent dag state on subdag clear (#15562)Closes: https://github.com/apache/airflow/issues/15374This pull request follows https://github.com/apache/airflow/pull/14776. Clearing a subdag with Downstream+Recursive does not automatically set the state of the parent dag so that the downstream parent tasks can execute.,2
"Support MySQL db in the helm chart (#15616)While we generally suggest using Postgres, we should support using anon-chart-provisioned mysql database as well.closes: #15558",5
Helm RBAC Best Practices (#14152)This PR builds off of and supersedes @jaydesl's work on his [PR](https://github.com/apache/airflow/pull/11769) to move forward with properly following [helm's rbac best practices](https://helm.sh/docs/chart_best_practices/rbac/). This PR updates every potential pod that can be deployed to include the option to either create or use an existing service account. This is the first step towards supporting environments where users have the [PodSecurityPolicy](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy) admission controller enabled without forcing such users to provide any additional permissions to the default service account in the namespace this is deployed to.closes: https://github.com/apache/airflow/issues/11755related: https://github.com/apache/airflow/issues/13643 Co-authored-by: jaydesl <jay.deslauriers@gmail.com>Co-authored-by: Ian Stanton <ian@astronomer.io>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
"Refactor building of results backend connection secret (#15618)Minor refactor of how the results backend connection secret is built. This moves logic out of an already long line, like it is already done in the metadata connection secret.",5
Prepares provider release after PIP 21 compatibility (#15576)This PR updates changelog and bumps version of providers to bereleased after we reached PIP 21 compatibility. It is necessaryfor two reasons:1) We need it in order to get constraints for PyPI released   providers to be updated automatically (PIP 21 conflicts master   airflow with few released providers2) We want to release Airflow 2.0.3 which will be PIP 21 installable   with those providers.,1
Update support policy for Python and Kubernetes (#15612)Following discussion here:https://lists.apache.org/thread.html/r04f228319206ab920088c89f1001482671ca06b3b6b5cc12477c76ab%40%3Cdev.airflow.apache.org%3Ewe update the policy of supporting Python/K8S versions.,1
Automated tagging of default images in DockerHub (#15625)The image tagging now is fully automated within the builddockerhub script including :<VERSION> and :latest tags.,3
Kubernetes Test Cleanup (#15626)Some of the tests were redundant in `kubernetes_tests/test_kubernetes_pod_operator_backcompat.py` which didn't test anything related to deprecated classes and were already covered by either `kubernetes_tests/test_kubernetes_pod_operator.py` or `tests/providers/cncf/kubernetes/operators/test_kubernetes_pod.py`,3
Docs: Replace 'airflow' to 'apache-airflow' to install extra (#15628)Some of the docs advised to use 'airflow[azure]' whereas it should beapache-airflow[azure],1
"Add docker-context-files detection and cleanup flag. (#15593)When building images for production we are using docker-context-fileswhere we build packages to install. However if those context filesare not cleaned up, they unnecessary increase size and time neededto build image and they invalidate the COPY . layer of the image.This PR checks if docker-context-files folder contains just readmewhen Breeze build-image command is run (for cases whereimages are not built from docker-context-files). Inversely italso checks that there are some files in case the image isbuilt with --install-from-docker-context-files switch.This PR also ads a --cleanup-docker-context-files switch toclean-up the folder automatically. The error mesages also helpthe user instructing the user what to do.",1
PostgresHook: deepcopy connection to avoid mutating connection obj (#15412)Co-authored-by: Jordan Zhang <jorzhang@justin.tv>,1
Better description of UID/GID behaviour in image and quickstart (#15592)* Better description of UID/GID behaviour in image and quickstartFollowing the discussion inhttps://github.com/apache/airflow/discussions/15579seems that the AIRFLOW_UID/GID parameters were not clearlyexplained in the Docker Quick-start guide and some users couldfind it confusing.This PR attempts to clarify it.* fixup! Better description of UID/GID behaviour in image and quickstart,1
Fix add quotes to extraConfigMaps/extraSecrets (#15633)* add quotes to extraConfigMapsAdd quotes to extraConfigMaps* Update values.yamladd quotes,1
Worker ServiceAccount is needed for KubernetesExecutor (#15642)KubernetesExecutor is also in the list of executors who needs a workerServiceAccount.,1
Bump ssri from 6.0.1 to 6.0.2 in /airflow/www (#15437)Bumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.- [Release notes](https://github.com/npm/ssri/releases)- [Changelog](https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md)- [Commits](https://github.com/npm/ssri/compare/v6.0.1...v6.0.2)Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Feature qubole hook support headers (#15615)* add support for include_headers in QuboleHook get_results,1
Chart: Add unit tests for limits and resources (#15621),3
When one_success mark task as failed if no success (#15467),0
Fix breeze k8s tests on mac (#15643),3
Rename example bucket names to use INVALID BUCKET NAME by default (#15651),1
Allow chart to work with custom cluster domains (#15640)K8s clusters can use a different cluster domain than the default`cluster.local`. This change will make the chart compatible with thoseclusters as well.,2
Implement BigQuery Table Schema Update Operator (#15367)Co-authored-by: Jens Larsson <jens.larsson@c02cv73mml85.lan>,1
"HttpHook: Use request factory and respect defaults (#14701)Use Request's session.request factory for HTTP request initiation, this will useenvironment variables and sensible defaults for requests.Also use verify option only if it is provided to run method, as requests libraryalready defaults to True.Our organization uses firewalls and custom SSL certificates to communicatebetween systems, this can be achieved via `CURL_CA_BUNDLE` and`REQUESTS_CA_BUNDLE` environment variables.  Requests library takes both intoaccount and uses them as default value for verify option when sending request toremote system.Current implementation is setting verify to True, which overwrites defaults andas results requests can not be made due to SSL verification issues. This PR isfixing the problem.",0
"Use the Stable REST API for Kubernetes executor integration tests (#15644)Currently, we use the experimental REST API to run the Kubernetes executor integration tests.This PR changes this to use the stable REST API for these tests",3
"Add delimiter argument to WasbHook delete_file method (#15637)This change adds a delimiter to the delete_file method of Wasbhook. This way, users will be able to locate the files they want to delete using a delimiter.Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",1
Fix chart DAG ``git-sync`` and persistence with ``KubernetesExecutor ``(#15657)This prevents a gitsync contianer from being created in theKubernetesExecutor worker if DAG persistence is enabled as the DAG willalready be on the volume. This also only mounts the DAGs volume once inthe worker.,1
"Rename old ""Experimental"" API to deprecated in the docs. (#15653)If you aren't familiar with the history of the APIs, then from lookingat the titles it looks like the Experimental API might be the ""next""one.We don't want people to think that!",2
"Improve Test Coverage for Kubernetes Executor (#15617)According to the current stats of codecov.io assessment of the Airflow code base, the test coverage forthe kubernetes_executor.py module is about 63%. This metric definitelyneeds to be improved upon.This PR addresses the unit test coverage for the delete_pod method in the AirflowKubernetesScheduler class in the kubernetes_executor.py module. And when merged will help to improve the test coverage metric.fixes part of #15523",0
Fix log persistence with KubernetesExecutor (#15659)KubernetesExecutor workers also need the log volume mounted.,2
Fix dags table overflow (#15660)Adds a false bottom to the table element so we can have overlap on the final row.Fixes #15656,0
Fix building assets on ``breeze start-airflow`` (#15663)Error because `webpack` is not install because `yarn install --frozen-lockfile` is not run:```root@f5fc5cfc9a43:/opt/airflow# cd /opt/airflow/airflow/www/; yarn devyarn run v1.22.5$ NODE_ENV=dev webpack --watch --colors --progress --debug --output-pathinfo --devtool eval-cheap-source-map --mode development/bin/sh: 1: webpack: not founderror Command failed with exit code 127.info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.root@f5fc5cfc9a43:/opt/airflow/airflow/www#```This commits adds `yarn install --frozen-lockfile` to the command which fixes it.This was missed in https://github.com/apache/airflow/pull/13313/files,2
Add --executor option to breeze kind-cluster deploy command (#15661)This change will enable us easily deploy airflow to kubernetes clusterand test it using different executors.Example usage:   ./breeze kind-cluster --executor CeleryExecutor deploy,1
"More verbose logs when running `airflow check_migrations` (#15662)Currently it just shows ""TimeoutError: There are still unapplied migrations after 60 seconds. "" which is not that helpful.This commit adds more info to show the difference in migrations in DB and source code running `check_migrations`:Now:```TimeoutError: There are still unapplied migrations after 60 seconds. Migration Head(s) in DB: {'e165e7455d70'} | Migration Head(s) in Source Code: {'a13f7613ad25'}```closes https://github.com/apache/airflow/issues/15650",0
"Mask passwords and sensitive info in task logs and UI (#15599)This masks sensitive values in logs for Connections and Variables.It behaves as follows:- Connection passwords are always masked, where-ever they appear.  This means, if a connection has a password of `a`, then _every_ `a` in  log messages would get replaced with `***`- ""Sensitive"" keys from extra_dejson are also masked. Sensitive is  defined by the ""existing"" mechanism that the UI used, based upon the  name of the key.- ""Sensitive"" Variables are also masked.",1
"Allows overriding versioned dirs when publishing documentation (#15624)We are allowign doc-only-changes when releasing providers,therefore we might want to regenerate documentation for latestversion of the provider packages when there are doc-only changes.The new --override-versioned flag enables that.",0
"When installing providers from the context folder, use `pip --find-links` (#15673)Without this change it is impossible for one of the providers to dependupon the ""dev""/current version of Airflow -- pip instead would try andgo out to PyPI to find the version (which almost certainly wont exist,as it hasn't been released yet)",1
"Fix on_failure_callback when task receive SIGKILL (#15537)This PR fixes a case where a task would not call the on_failure_callbackwhen there's a case of OOM. The issue was that task pid was being setat the wrong place and the local task job heartbeat was not checking thecorrect pid of the process runner and task.Now, instead of setting the task pid in check_and_change_state_before_execution,it's now set correctly at the _run_raw_task method",1
More test coverage on pgbouncer in the helm chart (#15684)Simply more test coverage on pgbouncer in the helm chart.,2
Clean up the pre-commit config file (#15681),2
"Update KubeExecutor pod templates to allow access to IAM permissions (#15669)If AWS's Identity-based IAM policies are in use on the cluster theytoken file will be mounted in to the pod (via the service account) and,prior to this change, will be owned by root.Specifying `fsGroup` makes the file group-readable by the `airflow`user.We already specify this in our helm chart, so this change is just foranyone looking at the docs.",2
Add docs to the markdownlint and yamllint config files (#15682),2
Add note on changes to configuration options (#15696),5
"Emit error on duplicated DAG ID (#15302)This will be shown in logs on initialization, and flashed in UI on laterscheduled refreshes.closes #15248",5
"Support jinja2 native Python types (#14603)Docs: https://jinja.palletsprojects.com/en/2.11.x/nativetypes/```python>>> from jinja2 import nativetypes>>> ne = nativetypes.NativeEnvironment()>>> import pendulum>>> ne.from_string('{{ x }}').render(x=pendulum.now())<Pendulum [2021-03-04T15:33:17.073343+00:00]>>>> ne.from_string('{{ x }}').render(x=pendulum.now().isoformat())'2021-03-04T15:33:29.516540+00:00'>>> ne.from_string('{{ x }}').render(x=""2012-10-10"")'2012-10-10'```Current:```python>>> environment.Environment().from_string('{{ [""w"",""x""] }}').render()""['w', 'x']""```Proposed:```python>>> nativetypes.NativeEnvironment().from_string('{{ [""w"",""x""] }}').render()['w', 'x']```* Add a flag for rendering to native python objectsThe problem with replacing `NativeEnvironment` as `Environment` only isthat `NativeEnvironment` does not raise an error on Undefined templates:https://github.com/pallets/jinja/blob/2.11.3/src/jinja2/nativetypes.py#L70-L94",0
"Chart: Update Webserver update strategy based on Airflow Version (#15627)This commit adds the following things:- Add ""airflowVersion"" flag that will allow use to add some componentsthat are just available or work with certain Airflow version.Example: pod_template_file is available for Airflow >= 1.10.12- Update logic for selecting pre/post Airflow 2.0 CLI commands basedon that flag- Updates stragtegy of Airflow Webserver based on that flag as thewebserver in Airflow >= 2 does not need access to DAG files, hencewe don't need to recreate but can have a ""true"" rollingUpdate- Allow overriding webserver udpate strategy",1
Clean up the pre-commit config (#15703),5
add oracle  connection link (#15632),2
"Run helm chart tests in parallel (#15706)* Allow helm chart tests to run in parallelThe helm chart tests are pretty slow when run sequentially. Modifyingthem so they can be run in parallel saves a lot of time, from 10 minutesto 3 minutes on my machine with 8 cores.The only test that needed modification was `test_pod_template_file.py`,as it temporarily moves a file into the templates directorywhich was causing other tests to fail as they weren't expecting anyobjects from that temporary file. This is resolved by giving thepod_template_file test an isolated chart directory it can modify.`helm dep update` also doesn't work when it is called in parallel, sothe fixture responsible for running it now ensures we only run it one ata time.* Enable parallelism for helm unit tests in CICo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
"Revert ""Clean up the pre-commit config (#15703)"" (#15707)This reverts commit 803850ad22e5995c7fffc5389a29e48d9ca0bb9a.",4
Fixes syntax of terraform script. (#15710)Latest pygments version does not render properly thesyntax where `{}` is used (terraform expects this to beseparated by EOL.,1
Add extra links for google dataproc (#10343),5
Add rest API to query for providers (#13394)* Add API to query for providers (#12468)* Improve tests speed (#12468),3
Update breeze (#15717)Fixing typo in bash script,2
Add Connection Documentation for the Hive Provider (#15704),1
Refactor tests/www/test_views.py (#15666)* Move out www connection tests* Move out www variable tests* Move out www plugin tests* Move out www pool tests* Move out mount point tests* Move out www configuration tests* Move out www redoc testsAlso merge some side-effect-less tests into one file so we don't get toomany tiny files.* Move out task instance list tests* Make sure base view init is in app context* Move out www helper function tests* Move out www dagrun tests* Move out www extra link tests* Move out www trigger DAG tests* Move out www rendered field tests* Always load example DAGs before running view tests* Properly handle test sessions so new tests work* Remove 'checker' in favor of plain functions,1
"Get rid of Airflow 1.10 in Breeze (#15712)Gets rid of Airflow 1.10 in Breeze and script/configuration.We were still using occasionally the master version of Breeze torun 1.10 version of Airflow, but this madness should end now whenwe are approaching 2.1 release. All changes in Breeze were so farported to 1.10 but this is about the time to finish it.",5
"Create cross-DAG dependencies view (#13199)This adds a new view for displaying dependencies between DAGs. It's based on theDAG Dependencies plugin (https://github.com/ms32035/airflow-dag-dependencies)and has been updated to work with Airflow 2.0.Since quite a bit of code is common with DAG graph view, that has beenexternalized to a new module.Unlike the external plugin this is uses the DAG serializater to storedependencies at parse time, meaning it doesn't need to load all DAGs.Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",2
"Auto-apply apply_default decorator (#15667)* Auto-apply apply_default decorator* Automatically patch warnings stacklevel inside Operator constructorsThis makes two changes to make the apply_defaults transparent towarnings:1. It removes `__call__` from the Metaclass (which added one frame)2. It ""patches"" warnings.warn _in the function scope_ to automatically   adjust the stacklevelThe reason for this is that now that apply_defaults is called""magically"" from the Metaclass we're adding extra frames to thecallstack. By locally patching warnings like this it means that usercode doesn't have to be aware of what we've done.(I wish Python had a native way of handling this case as it crops up allthe time. Perhaps I should submit a PEP for it with a better design)* Don't test provider packges against 2.0 anymoreSince we have changed the dep to need Airflow 2.1, testing against 2.0no longer makes any sense.And since 2.1 is not yet out, we have disabled this specific step fornowCo-authored-by: Kamil Breguła <kamil.bregula@polidea.com>",1
Reduce false-positives when detecting SQLite usage (#15716),5
"Create a DAG Calendar View (#15423)IIntroduce a DAG Calendar View to provide visibility over the full state of thedag by displaying the aggregated dag runs' states in a calendar.This makes it possible to monitor the state of thousands of dag runs in asingle view that is concise and easy to understand. It is particularly usefulto monitor the state of large backfills:Each day is displayed with a color according to the dag runs' states for thatday:- If at least one dag run has failed for a day, the day will be displayed as  ""failed"".- If all dag runs have succeeded, the day will be shown as ""succeeded"".- If there are still running dag runs (and no failed dag run) for that day, the  day will be shown as ""running"".A tooltip, for each day, gives the exact number of dag runs in each state for the day.Clicking on a day redirects to the tree view for that day to show the taskinstances for that day.Co-authored-by: Benoit Hanotte <benoit@wayve.ai>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>",1
Attempt to upgrade to newer Node version to build UI. (#15718)We've started to receive deprecation warnings for Node 10 andthis PR attempts to upgrade to recommended Node 14.Fixes: #15713,0
"New UI: Add Timezone select (#15674)* timezone dropdown componentsCopied the UX from the current UI on how to change timezone preferences.Working on a Chakra UI wrapper for React-Select.* clean up select ts* connect dropdown to timezone context* test dropdown* fix tests* update labels, select name & icons* 3rd party timezones, better dag rendering* check if timezone exists in a group* close menu on timezone selection",2
Add short description to BaseSQLToGCSOperator docstring (#15728),2
Fix sql_to_gcs docstring lint error (#15730),0
Remove duplicate key from Python dictionary (#15735),4
"Run Airflow package preparation in docker in breeze/CI (#15723)Moved building airflow package to within the container similarlyas we do with provider packages. This has the following advantages:* common environment used to build airflow* protection against leaking SECRET_* variables in CI in case third  party packages are installed* specify --version-suffixes and renaming the packages according  to destination (SVN/PyPI) automatically* no need to have node installed in CI runner* no need to have node installed in DockerHub* no need to install PIP/Python3 in DockerHub runner (currently  Python2 is still default and it fails the build there)* always deleting egg-info and build before the build* cleaning up egg-info and build after the buildAlso following the way providers are released, the documentationis updated to change publishing Airflow using previously votedand renamed packages - the very same packages that were committedto SVN. This way anyone will be able to manually verify that thepackages in SVN are the same as those published in SVN and thereis no need to rebuild the packages when releasing them.",1
"Skip failing for undefined variables in MacOS (#15744)Failing with undefined variables is a good technique to avoidtypos in Bash, but for MacOS this is problematic as bashused by default on MacOS fails with undefined variablewhen there is an empty array passed - which is often needed,for example when you pass ""${@}"" arguments.This PR disables undefined variable check for MacOS.",4
"Enforce READ COMMITTED isolation when using mysql (#15714)* Enforce READ COMMITTED isolation when using mysql* Fixing up tests, removing indentation* Fixing test",3
[Oracle] Add port to DSN (#15589),1
"Use ""apache/airflow"" repository as source of constraints (#15746)In case build is done in master of a fork repository, constraintsshould be taken from the 'apache/airflow' not from the originalrepository. The fork might simply have outdated constraintsbranches if they were forked some time ago.",5
"extra docker-py update to resolve docker op issues (#15731)Due to changes in the docker api in api version 1.41, the docker pythonclient needs an update to properly handle the filter param imagejsonendpoint.  Without this fix, the `DockerOperator` will not pull the imageunless `force_pull` is set to True.Fixes #13905",0
Add missing docstring params (#15741),2
Remove redundant character escape from regex (#15740),4
"Small changes on ""DAGs and Tasks documentation"" (#14853)",2
Fix argument ordering and type of bucket and object (#15738),0
Fix spelling (#15699)Fix spelling of directory and PNG file name,2
"Update `SimpleHttpOperator` to take auth object (#15605)A `requests.auth.AuthBase` object is not passed through from the`SimpleHttpOperator` to the underlying `HttpHook`, thus if you want touse the `SimpleHttpOperator` but have a custom auth_type, you mustinherit from it and override the execute method.  Update the operatorto take this parameter.",2
Mark test_mark_success_on_success_callback as quarantined (#15756)Captured the issue in #15754,0
MongoToS3Operator failed when running with a single query (not aggregate pipeline) (#15680)* Remove allowDiskUse argument from mongo hook find* Update airflow/providers/amazon/aws/transfers/mongo_to_s3.pyCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>* Update mongo_query docstring typeCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>,2
Remove unused dependency (#15762),1
Update croniter to 1.0.x series (#15769),5
"Clean up unnecessary Airflow config in helm chart (#15729)This removes Airflow config from the chart that isn't necessary.The configs are either already the default, don't need to changed from thedefault, or aren't real Airflow configs anyways.",5
Add integration test for other executors in kubernetes cluster (#15695)This change adds tests to run some example dags with other executors.Steps to run tests with for example CeleryExecutor```./breeze kind-cluster start./breeze kind-cluster --executor CeleryExecutor deploy./breeze kind-cluster test```,3
Feature qubole hook support headers (#15683)* Added support for including headers in qubole results* Added missing issue number* Fixed pylint errors and warnings* fixing formatting that caused error with statics checks* Apperently qubole is using strings as true false* Adding Unit Tests for include_headers flag* Fixing static-check pylint errors* Added better unit tests for Quboles Hook and Operator* fixed typo in pylint comment* Fixing failed static checks and lint errors* fixed some more lint issues,0
"Unset PIP_USER in prod entrypoint. (#15774)In Airflow image we are setting PIP_USER variable to true, in order toinstall all the packages by default with the ``--user`` flag. Howeverthis is a problem if a virtualenv is created later which happens inPythonVirtualenvOperator. We are unsetting this variable here, so thatit is not set when PIP is run by Airflow later on.Fixes: #15768",0
"Add resolution to force dependencies to use patched version of lodash (#15777)Resolves [a known vulnerability](https://github.com/advisories/GHSA-35jh-r3h4-6jhm) in lodash. Lodash is an indirect dependency and not all of the the direct dependencies have been patched yet. This resolution forces the currently utilized `4.17.15`, `4.17.19`, and `4.17.20` versions to use the safe `4.17.21` patch.",1
Fixed type annotations in DAG decorator (#15778)Co-authored-by: Pyaive Oleg <pyaive.oleg@huawei.com>,2
"Fix OdbcHook handling of port (#15772)Even if port was set in the connection object, still it was not added to connection string.  This commit fixes that.",0
Return output of last task from task_group wrapper. (#15779),5
Bump stylelint to remove vulnerable sub-dependency (#15784),4
Update Flask App Builder limit to recently released 3.3 (#15792),5
Pin `itsdangerous` to < 2 (#15804)Looks like new version of `itsdangerous` has broken some logging configs: https://itsdangerous.palletsprojects.com/en/2.0.x/changes/#version-2-0-0,4
"Chart: Allow setting annotations on Airflow pods & `Configmap` (#15238)This PR adds a new field (`airflowConfigAnnotations`) that allows users to add `annotations` to the main `configmap.yaml` file. I ended up setting up a new testing file as I didn't find a file where this specifically fit, but if it should be moved elsewhere let me know.closes https://github.com/apache/airflow/issues/13643",0
"Expand date provider for new UI (#15725)* timezone dropdown componentsCopied the UX from the current UI on how to change timezone preferences.Working on a Chakra UI wrapper for React-Select.* clean up select ts* connect dropdown to timezone context* test dropdown* fix tests* update labels, select name & icons* 3rd party timezones, better dag rendering* check if timezone exists in a group* close menu on timezone selection* add a dateformat provider- create a provider for the format string to give to dayjs- initial example use-case is toggling between a 12 and 24 hour clock* merge timezone and dateformat providers* rename timezone provider in test* create dateFormat() wrapper around dayjs()",5
"Chart: Use `v2-0-stable` branch for `git-sync` (#15809)Since the default Airflow versions we are using is 2.0.2, we should use DAGs from `v2-0-stable` for `git-sync`",2
Chart: Rename `dashboard` to `Webserver` in Installation Notes (#15808)Before:```Your release is named airflow.You can now access your dashboard(s) by executing the following command(s) and visiting the corresponding port at localhost in your browser:Airflow dashboard:        kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflowFlower dashboard:         kubectl port-forward svc/airflow-flower 5555:5555 --namespace airflow```After:```Your release is named airflow.You can now access your service(s) by executing the following command(s) and visiting the corresponding port at localhost in your browser:Airflow Webserver:        kubectl port-forward svc/airflow-webserver 8080:8080 --namespace airflowFlower dashboard:         kubectl port-forward svc/airflow-flower 5555:5555 --namespace airflow```,2
Update Openapi generator version (#15816),5
"Chart: Avoid `git-sync` sidecar on Websever when Airflow>=2.0.0 (#15814)For `apache-airflow>=2.0.0`, DAG Serialization is enabled by defaultand we don't need to have a sidecar on Websserver.Previously this was done using `gitSync.excludeWebserver`. Howeverwith https://github.com/apache/airflow/pull/15627 - we now have`airflowVerson` so we can just do a comparison of the version.",1
"Allow S3ToSnowflakeOperator to omit schema (#15817)Fix #12001.As a drive-by cleanup, I also rewrote the text-building logic a bit to remove superfulous spaces and blank lines in the query string.",4
"Refactor tests/www/test_views.py (Part 2/2) (#15719)Following up #15666This converts the rest of `tests/www/test_views.py` into `tests/www/views/` and use pytest fixtures to manage them.(Run time numbers are different from #15666 since I’m working on another machine.)```(master) $ ./breeze tests -- tests/www/test_views.py tests/www/views/========== 246 passed, 2 skipped, 10 warnings in 170.95s (0:02:50) ==========(tests-www-refactor) $ ./breeze tests -- tests/www/views/========== 252 passed, 2 skipped, 10 warnings in 164.77s (0:02:44) ==========```Not much difference since the tests were already sharing resources well. But the refactoring is still very worthwhile IMOby virtue of breaking done the three convoluted monster classes in `test_views.py`.The test count increased slightly because I split a few tests in `test_views.py` into smaller ones with `parametrize`. They were doing too much in one tests and did not correctly manage test setup very well.There are still some possible performance boosts left. The `app` fixture in `tests/ww/views/` is currently initialised per-module because some tests modify `app.dag_bag`. But not all modules do, and those that don’t can share an `app` instance instead. But that would make the test setup more complicated (if we incorrectly re-use `app` when we shouldn’t the tests could become stateful and incorrect) so I opted to not do it.After this I will work on increasing test coverage of `airflow/www/views.py` as suggested in #15525.",3
Relax required version for `python-telegram-bot` (#15776),1
"Remove unused constant in serialization code (#15824)This was added back in by a PR that added links to Google DataProcoperators, but this is not used anymore, instead the code looks at theprovider registry for links.",2
"Ensure that task preceeding a PythonVirtualenvOperator doesn't fail (#15822)The addition in 2.0.0 of the ""mini scheduler run"" at the end of a taskwould cause any task preceeding a PythonVirtualenvOperator to fail withan exception of `cannot pickle 'module' object`.",0
"Improve feedback message after installing or upgrading airflow with helm chart (#15820)With the aid of Helm chart's `templates/NOTES.txt` file, some nifty messages and/or instructions can be printed out to STDOUT for users after they run either the `helm install` or `helm upgrade` command.This PR aims to improve the user experience when using Helm chart to install Airflow (or upgrade Airflow) in a kubernetes cluster. It contains handy ""next steps"" and essential information about a fresh Airflow installation or upgrade.closes #15315",5
Add download links to provider documentation (#15739)This PR adds links in the provider documentation tothe official Apache Airflow Downloadsite including checksum and signature links.,2
Migrate dag js (#15823)* initial pass at migrating dag js- blocked on changes to graphjs* remove duplicate head_meta* extra linting in dag.js* resolve subdag and log redirect* wrap log redirect with if statementshow_external_log_redirect may be undefined. Wrapping <meta> tag in the if statement that was in javascript previously,2
"Build chart parameter docs from `values.schema.json` (#15827)This automatically builds the chart parameter docs page from the`values.schema.json`. It also tracks and enforces that the defaults in`values.yaml` and `values.schema.json` match.The parameter docs page now has different sections, which are set in theschema file under `docs_schema`. That key is required for top levelproperties in the schema file, and optional otherwise as the parents`docs_schema` is used as a default.We also now have a schema for our `values.schema.json` in`values_schema.schema.json` which enforces the items we need to buildthe docs are set appropriately: `docs_schema`, `default`, and`description`.Closes #15771",2
Rename docs fields in chart values schema (#15828)We will use the `x-` prefix to signify these are custom non-standardfields in our JSON schema.,5
Save pod name to xcom for KubernetesPodOperator (#15755)* Save pod name to xcom for KubernetesPodOperator* fix kubernetes test,3
Improve test coverage of task_command.py FIXES: #15524 (#15760),0
Keep confirmDeleteDag inline html (#15836)Tests and delete functionality rely on `confirmDeleteDag` to be inside the html. It is simpler just to have it inline instead of refactoring all dependent tests to make it work in an external js file.,2
CloudwatchTaskHandler reads timestamp from Cloudwatch events (#15173)* format Cloudwatch timestamp using default template and utc timezone,1
Make DAG dependency detector configurable (#15829),5
Fixes building pre-2.1 Airflow images from 2.1 Dockerfiles. (#15802)Airflow in version pre 2.1 is not compatible with PIP 21. If youtry to build image for Airflow 2.0* using master Dockerfile(soon released in 2.1) Airflow will not build properly.This change will automatically fall-back to PIP 20.2.4in case Dockerfile is built with Airflow 2.0 or below.Fixes: #15790,0
Move `airflowLocalSettings` to be a top level chart param (#15838)Moving it to be a top level chart param makes sense as it is used inmost of the Airflow components.,1
Explicitly point Jinja2 documentation to 2.11.x (#15847),2
Support extra env vars on gitsync containers (#15842)Related: #15786,1
Add back-compat shim for BranchDateTimeOperator rename (#15849),5
"Revert ""Add back-compat shim for BranchDateTimeOperator rename (#15849)"" (#15853)This reverts commit 3f0a90429575286f03ba28b0e29da71a3aba8098.Backcompat is not needed as the original feature hasn't been released yet.",4
Fix modal import in graph.js (#15852)`call_modal` no longer existed. Switching to import `callModal` instead.,2
Fix docstring formatting on ``SlackHook`` (#15840),1
Set `airflowPodAnnotations` on Flower and `KuberneteExecutor` workers (#15859),1
Remove unused licenses: `python-nvd3` & `python-slugify` (#15860)This was added when we vendorized nvd3 and slugify: https://github.com/apache/airflow/commit/e36bdef0b34c16def20ecbb8248950070eb5fa33but we forgot to remove it in https://github.com/apache/airflow/pull/9136,4
"Check chart labels on all objects (#15686)We weren't checking labels on all of the objects our chart creates.Instead of making it fatal, I've added a warning so new objects that areforgotten are more visible.",1
Remove unused licenses from LICENSE (#15863)Missed it in #15860 + we can remove rich too -- I advised wrongly that we might have to add license -- but we only need to add licsense if we include/vendor it in source code,1
"Make Helm Chart ready for release (#15864)- Add `INSTALL`, `LICENSE` and `NOTICE` files as requried by ASF.- Updated Docs to point to Soon To Be Published Docs",2
Add support for listing user objects. (#15862)Fixes the issue that prevents an admin user from listing users on non DB-backed authentication views.,5
"Default `resultBackendConnection` to `metadataConnection` (#15861)Instead of requiring anyone using an external db and CeleryExecutor toset their database details twice, let the default for`resultBackendConnection` be the values from `metadataConnection`.Anyone who wants to use a separate backend for results still can.",1
"Add more metadata to `Chart.yaml` (#15866)- Adds more metadata like `appVersion`, `home`, `maintainers`, `sources` etc- Remove `tests` from `helm package`",3
Update Instructions to Install Chart from source (#15868)The INSTALL file should contain details to run chart from source not other repo. This PR fixes it for Helm Chart Release.,2
Fix static check (#15869)I forgot to remove license files from `setup.cfg` in #15863,5
Move common pitfall documentation to Airflow docs (#15183)Reviewed Common Pitfall page and migrated to Airflow documentation. I also added more common pitfalls.closes: #10180,1
Fix pause/unpause Dag on dag view (#15865)This PR adds a `pausedUrl` via `<meta>` tag that is readable by js instead of a jinja template. This was causing a bug where a user couldn't pause/unpause a dag from the dag page.,2
Dev: Update command to install ``setuptools-build-subpackage`` (#15872)https://github.com/ashb/setuptools-build-subpackage has now been published to PyPI: https://pypi.org/project/setuptools-build-subpackage/,1
Fix missing mkdir for hadoop dockerfile (#15871)closes: #15365,2
"New production guide for helm chart docs (#15867)New production guide for the helm chart. A starting point, will definitely need some fleshing out.Related: #14303",5
Update KylinHook docstring (#15602)Add short description to KylinHook.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
"Relax the version constraint of rich (#15531)`rich` is used in cli to print tables to the terminal, as well as in settings.py to print deprecation warnings. Neither uses require the version constraint.fixes #15529",0
Fix `task_instance_mutation_hook` when importing airflow.models.dagrun (#15851)If a dag imported `airflow.models.dagrun` it would cause task_instance_mutation_hook from the site local settings to not be picked up.,1
"Docs: Fix examples values in docs (#15878)Before:```extraEnv: ""- name: AIRFLOW__CORE__LOAD_EXAMPLES\n   value: True""```After:```extraEnvFrom:  secretRef:    name: '{{ .Release.Name }}-airflow-connections'extraEnvFrom:  configMapRef:    name: '{{ .Release.Name }}-airflow-variables'```",5
"Revert ""Docs: Fix examples values in docs (#15878)"" (#15880)This reverts commit b90bb6ed3a788ab28692a0d18adee4fbbe89ca44.",4
Better multiline string formatting for chart docs (#15881)This will properly format multiline strings in the helm chart parameterdocs.,2
Add v2.1 changelog  (#15811),4
Fix docs spelling errors (#15884),0
Update installation page (#15737)The installation page is updated with:* updated Python/Kubernetes support policies* added download information to installation page,5
"Get rid of requests as core dependency (#15781)This change gets rid of requests as core dependency. We have tochange requests to become an optional dependency because it(so far) pulls in chardet as dependency and chardet isLGPL, which is not allowed to be mandatory dependency byASF policies.More info here:https://issues.apache.org/jira/browse/LEGAL-572The changes:* connexion is vendored-in (and requests usage is replaced with httpx)* Http Provider is turned into optional provider (not preinstalled)* Few places where requests were used in core and in cloud_sql provider  which did not cause compatibility problem, it was replaced by httpx.* new extra added for deprecated experimental API (which is disabled  by default and optional)* tests are fixed (using pytest-httpx fixture package)* The providers: http, airbyte, apache.livy, opsgenie, slack (all depend  on http) now explicitely depend on `requirements`.",1
Detect 🚧 as a WIP PR (#15893)Emoji are Cool. I'm down with the kids see?,5
"Fix spacing in ``AwsBatchWaitersHook`` docstring (#15839)Sphinx requires the short description to be separated by a newline, otherwise it considers all lines as the short description. See [the rendered docs](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/hooks/batch_waiters/index.html) for the issue - the one liner description is currently ""A utility to manage waiters for AWS batch services **Examples:**"".",0
"Fix 500 error from updateTaskInstancesState API endpoint when `dry_run` not passed (#15889)The `default` schema parameter is for converting back to JSON,`missing` is the one that takes effect when converting to Python dict.",5
"Replace DockerOperator's 'volumes' arg for 'mounts' (#15843)* Replace DockerOperator's 'volumes' arg for 'mounts'All existing 'volumes' usages are also migrated to use 'mounts' instead.This also includes a fix to DockerSwarmOperator's inability to mountthings into the container; the new argument is also passed to Swam, sousers can mount by setting 'mounts' on DockerSwarmOprerator as well.",2
Use 'authentication backend' instead of executor in the section about authentication backends. (#15894),1
"Add optional result handler to database hooks (#15581)This allows retrieving result rows, or other post-query accessto the cursor object.Co-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
"Faster helm unit tests (#15874)The helm unit tests don't use a database, so skipping that setup saves~10 seconds on that CI step.",1
Enforce formatting on helm chart json schema files (#15895),2
Add Asana Provider (#14521),1
"Add Airflow Standalone command (#15826)Runs all parts of an airflow deployment under one main process, providing an easier level of entry than Breeze and a very handy tool for local development.Handles the following:* Runs all database migrations/db init steps* Creates an admin user if one is not present (with a randomised password)* Runs the webserver* Runs the scheduler* Overrides the executor to be `LocalExecutor` or `SequentialExecutor` depending on the database in useIt will also be able to run the `triggerer` process from AIP-40 with a three-line change once that PR lands.This diff also updates the quickstart guide to use this rather than the separate commands, since it makes the set of commands significantly shorter.",1
Add missing word to release docs. (#15898),2
"fix modal actions (#15896)Modal events are triggering twice. once, with an executionDate, and again without. Now the submit function will check that an executionDate exists before doing an action",5
Fix task search function in Graph view (#15901),1
"Also check chart schema when the schema itself changes (#15902)This changes our schema pre-commit hooks to also run when the schemaitself changes, not just when the file itself changes.",4
Use different executors for Helm Chart tests in CI (#15791)closes https://github.com/apache/airflow/issues/14301,0
"Ensure that secrets are masked no matter what logging config is in use (#15899)And rather than only applying the filters for the current known remotebackends, I have applied the filter to the task handler ""globally"" sothat all task logs are filtered, even for custom remote back ends.",2
Set unique name on ``pre-commit`` hook and update schema (#15903)This renames a hook to better represent what it's doing and alsoupdates the schema to the earliest k8s version we test against.It also moves to a schema from an active fork instead of theabandoned original repo.,4
Update example `KubernetesExecutor` `git-sync` pod template file (#15904)This fixes the example to have a functional gitsync.Closes: #11789,1
Fix colon spacing in ``AzureDataExplorerHook`` docstring (#15841),2
"Add missing License details for ``Connexion`` (#15906)This was missed in https://github.com/apache/airflow/pull/15781Since ""connexion"" is Apache licensed, this is ""not a blocker"" for 2.1.0 as mentioned in https://www.apache.org/legal/release-policy.html#license-file>When a package bundles code under several licenses, the LICENSE file MUST contain details of all these licenses. For each component which is not Apache licensed, details of the component MUST be appended to the LICENSE file. The component license itself MUST either be appended or else stored elsewhere in the package with a pointer to it from the LICENSE file, e.g. if the license is long.As ""connextion"" is Apache 2 Licensed, this _might_ be OK.",2
"Remove Version suffix for SVN while releasing (#15905)Reason explained in https://lists.apache.org/thread.html/rca430c836e5286b6d848831bdbc4f37fe5d6620ee9757e93b401495a%40%3Cdev.airflow.apache.org%3EWhile working on the Helm Chart release, I was verifying what we were doing for ""apache-airflow/python dists"" over the weekend, which is ""wrong"".We should be renaming the files as the SHA512 check fails on ""Release"" repo: https://dist.apache.org/repos/dist/release/airflow/2.0.2/For example, check out 2.0.2 release on Airflow:Since the SHA512 were generated with the original filename (with rc in it), it fails now in filename part:```shell❯ for i in *.sha512do    echo ""Checking $i""; shasum -a 512 `basename $i .sha512 ` | diff - $idoneChecking apache-airflow-2.0.2-bin.tar.gz.sha5121c1< 4281b3ff5d5b483c74970f8128d7ad8ba699081086fd098e10b12f8b52a7d0f92a205d7ea334c29e813ac06af7a26de416294fd18c3a1a949388a4824955ce2e  apache-airflow-2.0.2-bin.tar.gz---> 4281b3ff5d5b483c74970f8128d7ad8ba699081086fd098e10b12f8b52a7d0f92a205d7ea334c29e813ac06af7a26de416294fd18c3a1a949388a4824955ce2e  apache-airflow-2.0.2rc1-bin.tar.gzChecking apache-airflow-2.0.2-source.tar.gz.sha5121c1< ca783369f9044796bc575bf18b986ac86998b007d01f8ff2a8c9635454d05f39fb09ce010d62249cf91badc83fd5b38c04f2b39e32830ccef70f601c5829dcb7  apache-airflow-2.0.2-source.tar.gz---> ca783369f9044796bc575bf18b986ac86998b007d01f8ff2a8c9635454d05f39fb09ce010d62249cf91badc83fd5b38c04f2b39e32830ccef70f601c5829dcb7  apache-airflow-2.0.2rc1-source.tar.gzChecking apache_airflow-2.0.2-py3-none-any.whl.sha5121c1< 779563fd88256980ff8a994a9796d7fd18e579853c33d61e1603b084f4d150e83b3209bf1a9cd438c4dd08240b1ee48b139690ee208f80478b5b2465b7183e50  apache_airflow-2.0.2-py3-none-any.whl---> 779563fd88256980ff8a994a9796d7fd18e579853c33d61e1603b084f4d150e83b3209bf1a9cd438c4dd08240b1ee48b139690ee208f80478b5b2465b7183e50  apache_airflow-2.0.2rc1-py3-none-any.whl```I was also checking how other projects did it, Apache Spark for instance, they also just have the ""rc"" name in the directoryand that is all: https://dist.apache.org/repos/dist/dev/spark/v2.4.8-rc3-bin/ so it is easy to""just move"" from ""dev"" to ""release"" without changing anything.For Airflow releases since we already do it in a directory that is named ""VERSION-rcX"" (example: `2.1.0rc1` in https://dist.apache.org/repos/dist/dev/airflow/2.1.0rc1/)we don't need to add rcX in the filename of the artifact.This helps us just moving files without renaming and changing filenames in SHA512 and is released exactly with what was voted on.",2
Add proper rat-excludes and missing licence (#15908),1
Prep for 2.1.0 release (#15913),5
Add support cli delete user by email (#15873),1
Update version to next dev version (#15916)Now that 2.1 has branched off we need to update the version,5
"Sandbox templates (#15912)Templates _shouldn't_ ever be taken from untrusted user input (and it's hard todo so without just edit the dag file, at which point you can run whateverpython code you like _anyway_), but this is a reasonable safety measure just incase someone does something ""clever"".",1
fix dag dependency search (#15924)Use `.filter()` instead of `.forEach()` when highlighting nodes in dag dependency view,2
Add `template_fields` to `S3ToSnowflake` operator (#15926),1
Move plyvel to google provider extra (#15812)Plyvel does not build on macOS without levelDB installed in system.  Its better to make it an optional install.,1
Remove the `set -x` in mypy check producing verbose output (#15932),1
Add usage for --package-filter option (#15918),1
Improve Helm Chart Git-Sync documentation (#15937)Mounting DAGs from a private Github repo using Git-Sync sidecar isquite complex because of the several steps required and the manyother moving parts.The PR aims to ameliorate some of these pain points so that users canhave a smoother experience when mounting their DAGs from private reposon Github.,2
"Updates branches and branch documentation after 2.1.0rc1 (#15528)* Updates branches and branch documentation after 2.1.0rc1This PR updates branches and corresponding documentation andtools after 2.1.0rc1 release.It describes what needs to be done when new release branch is created,and provides tools that allow to do most of the worksemi-automatically.Wherever possible and easy, the 2-0 references were replaced with 2-*and where it was more difficult, TODOS were left.The `dev/retag_docker_images.py` script will also be usefulwhen we get to renaming the `master` tag to `main` tag.",1
Correctly implement autocomplete early return (#15940),5
Fix broken link (#15911),2
add version life cycle table (#15936)* add version life cycle table* add to airflow docs* fixes,0
"Chart: Add warning about missing ``knownHosts`` (#15950)Also document knownHosts in production guide, and refactor privategitsync dags docs to use `extraSecrets` instead of a separate secret.",1
"Mount DAGs read only when using ``gitsync`` (#15953)We will mount the DAGs as read only when we are using gitsync, but notwhen we are only using persistence. This also moves the whole mount definitioninto helpers instead of just the mount path, making it easier to modifythe DAGs mount everywhere it is used in the future.",1
Update Helm Chart docs for 1.0.0 release (#15957)Updates repo name and chart name and some minor errors,0
Add Badge for Artifact Hub (#15958)We just published our Helm Chart on Artifact Hub: https://artifacthub.io/packages/helm/apache-airflow/airflowThis commit adds a nice badge for Readme,1
Fix typo in production guide for HelmChart (#15966),2
"Fail tasks in scheduler when executor reports they failed (#15929)When a task fails in executor while still queued in scheduler, the executor reportsthis failure but scheduler doesn't change the task state resulting in the taskbeing queued until the scheduler is restarted. This commit fixes it by ensuringthat when a task is reported to have failed in the executor, the task is failedin scheduler",0
Add memory usage warning in quick-start documentation (#15967)Users of docker quickstart on MacOS often complain that Airlfowdoes not start.Examples: #15961 #15927This PR adds warning about it in the docs and providesinstruction on how to check and change the limits.Fixes: #15941,0
"Produce less output from breeze when VERBOSE_COMMANDS is set (#15970)The prepare_usage command was being called and setting variables toproduce the full usage text for all commands at start up -- and whilethis isn't a problem for speed, it makes the output when running with `bash-x`/`VERBOSE_COMMANDS` a **lot** longer.This change delays creating the usage variables and formatted versionsuntil they are needed.Net result when running this command:```bash -c 'exec 5>breeze-trace.log; export BASH_XTRACEFD=5; VERBOSE_COMMANDS=true VERBOSE=true ./breeze -n shell -- -c ""true""'```Before 4500 lines, 204K of ""trace"" logs```❯ wc -lc breeze-trace.log  4528 204088 breeze-trace.log```After is 1/4 as much output:```❯ wc -lc breeze-trace-with-change.log  1402  68167 breeze-trace-with-change.log```",4
"Bug Fix Pod-Template Affinity Ignored due to empty Affinity K8S Object (#15787)**Issue**:KubernetesPodOperator pod-template files with pod affinities are ignored, even if no affinities are passed to the KubernetesPodOperator object. **Cause** During the pod-initialization an empty k8s.Affinity object is created if no affinities are supplied. This will later prevent the pod-template affinities to be used, because during the pod_reconciliation the empty k8s.Affinity object takes precedence. All other attributes such as`self.k8s_resources = convert_resources(resources) if resources else {}``self.image_pull_secrets = convert_image_pull_secrets(image_pull_secrets) if image_pull_secrets else []`handle it properly.",0
"Removes unnecessary function call (#15956)No need to make this call, as if no perms are passed`sync_resource_permissions` short circuits anyways.",4
"Chart: Remove ``git-sync``: ``root`` and ``dest`` params (#15955)We can simplify gitsync by not exposing these to the end user, as theydon't provide any useful behaviors.",1
Add `KubernetesPodOperat` `pod-template-file` jinja template support (#15942)This PR adds jinja template support for KubernetesPodOperatorpod-template-file.fixes #15892,2
Use api version only in GoogleAdsHook not operators (#15266)Add parameter api_version to both GoogleAdsToGcsOperator & GoogleAdsListAccountsOperator.Remove repeated line self.gcp_conn_id = gcp_conn_id.Change the default api_version to v5 since v3 is deprecated.Add api_version to GoogleAdsHook's docstring.,2
Add more tests for the kubernetes executor (#15992)This change adds tests to improve coverage for the kubernetes executorPart of #15523,3
`loadBalancerIP` and `annotations` for both Flower and Webserver (#15972)This adds `loadBalancerIP` to both flower and webserver Service and adds`annotations` to the flower Service.Closes: #12751,1
Chart: ``gitsync`` Clean Up for ``KubernetesExecutor``  (#15925)The gitsync ssh key was being mounted into the KubernetesExecutor workerwhich we don't need or want. This also does some more minor gitsyncrelated cleanup.Closes: #15900,4
Fix failing spelling check on Master (#15998)For some reason https://github.com/apache/airflow/pull/15972 was green -- Build docs was skipped for some reason (https://github.com/apache/airflow/runs/2634819101)which caused failing Master,0
Bump pyupgrade v2.13.0 to v2.18.1 (#15991)* Bump pyupgrade v2.13.0 to v2.18.1* fixup! Bump pyupgrade v2.13.0 to v2.18.1,0
"Add Docs for ``default_pool`` slots (#15997)Until now this was undocumented, however it was used in:https://github.com/apache/airflow/blob/aa4713e43f92d3e4c68c3ad00e2d44caaf29aafe/airflow/utils/db.py#L75This was done in 1.10.4 in https://github.com/apache/airflow/commit/2c99ec624bd66e9fa38e9f0087d46ef4d7f05aeccommit. This commit also updates the name from `non_pooled_task_slot_count` to `default_pool_task_slot_count` asthis is what it actually does.",5
Add proper link for wheel packages in docs. (#15999)Co-authored-by: jarek <jarek@penguin>,2
Improvements for Docker Image docs (#14843)* Improvments for Docker Image docs* fixup! Improvments for Docker Image docs* fixup! fixup! Improvments for Docker Image docs,2
Fix flower serviceAccount created without flower enable (#16011),0
Brings back testing providers against 2.1.0 (#16006)We temporarily disabled testing of providers against Airflow 2.1.0until it gets released. New providers are going to be 2.1+compatible only so we stopped testing them against 2.0.0.This PR restores the tests.,3
Handle special characters in password sfor Helm Chart (#16004),2
Add Helm Chart to apache-airflow docs (#16014),2
Chart: Fix Elasticsearch secret created without Elasticsearch enabled (#16015)* Fix elasticsearch secret created without elastticsearch enabled* Update chart/templates/check-values.yamlCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* fixup! Update chart/templates/check-values.yamlCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
"Always return a response in TI's ``action_clear`` view (#15980)A flask view should always return a response object, not None. This makes `TaskInstanceModelView.action_view` works like all other action views in the same class, redirecting to the previous page even after an exception (and show the exception as a flash).This does *not* get #15775 resolved, but should make the error more obvious to the user, instead of masking it with another error.",0
Ability to test connections from UI or API (#15795),3
"Removes arrow higher limits for plexus provider (#16026)Plexus provider needlessly limits arrow to <1.0.0. This was addedin #14781 to fix failing tests, but Plexus uses arrow in a verylimited way and it turned out that just checking for None valuebefore running arrow.get(), fixes the problem (and we can upgradeto latest version of arrow as well).",3
Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018)Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl>Obtain tree_data object endpoint from meta.closes: #16017,5
Make scripts/ci/libraries Google Shell Guide Compliant (#15973)* Make scripts/ci/libraries Google Shell Guide CompliantPart of #10576,1
Remove TaskInstance.log_filepath attribute (#15217)* Remove TaskInstance.log_filepath attribute* fixup! Remove TaskInstance.log_filepath attribute* Update UPDATING.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
"Unprotect ""main"" branch (#16046)Although we want this branch protected eventually, we need to unproctectit for now so that we can delete it, and thus allow a GitHub org adminto _rename_ the default branch to master -- they can't do this if thebranch already exists.",1
"Don't die when masking `log.exception` when there is no exception (#16047)It is possible that `exc_info` can be set, but contain no exception.We shouldn't fail in this case, even if the output doesn't make sense asshown by the test (the `NoneType: None` line is the exception beinglogged.)",2
Introduce compat shim airflow.compat.functools (#15969)This module shims 'cached_property' and 'cache' so modules don't need toall do their own ad-hoc try-except ImportError.,2
"Streamline Build Images workflow using new GitHub Actions features (#15944)Use `pull_request_target` event for building images, and `concurrency` toautomatically cancel old jobs for PRs.This means that:- GitHub will automatically cancel old jobs for us, so we don't have to  handle that ourselves (removes most of the use of the  cancel-workflow-action)- GitHub displays these checks directly on the PR, but it is still run  in the context of our repo, meaning it has access write to our  repo/access to secrets etc.- Since it shows up directly on the PR checks, we don't need to create the  check in the ""CI"" workflow to show the status of the Image Build.- We also don't need to post the comment saying _why_ it failed, as the  Build Image status will show up directly there- Since `pull_request_target` has information about the PR in the  `github.event` context, we don't need the complex mechanism to find  the ""other"" PR, we can do a fairly simple API request and filter by  the commit SHA to find and cancel to CI workflow job. (This removes  the final use of the cancel-workflow-action)One change I had to make here what tag we use for Docker images we buildand push up. Previously we used the ""source run ID"" (i.e. the id of theCI run) but with pull_request_target we don't have that anymore. Wecould use the same API mechanism we do to cancel to find the target job,but the only requirement here is for an ID that both jobs know -- theSHA of the PR branch fills that needExtra side benefits of this:- The sidebar of commits in main branch aren't ""polluted"" with Build  Images for PRs like they were previously.",1
"Ensure that we don't try to mask empty string in logs (#16057)Although `Connection.password` being empty was guarded against, thereare other possible cases (such as an extra field) that wasn't guardedagainst, which ended up with this in the logs:    WARNING - ***-***-***-*** ***L***o***g***g***i***n***g*** ***e***r***r***o***r*** ***-***-***-***Oops!",2
"Enforce js linting for current ui in pre-commit (#15858)* first linting pass* fix errors in tree.js* add separate www lint for js* remove all lint errorsfix all linting errors, primarily in `graph.js`Still warnings, but those don't prevent eslint from passing* narrow linting scope to just www/static/js/",4
Chart: Add both airflow and extra annotations to jobs (#16058)Our create user and database migrations jobs were missing the airflowannotations and didnt support adding extra annotations.,1
Add collapsible import errors (#16072)* Make DAG Import errors collapsableShow only the first line of a DAG Import Error that will expand when clicked on* make each error its own alert box,0
Support ``strategy``/``updateStrategy`` on scheduler (#16069)Allow the schedulers strategy (for Deployment) orupdateStrategy (for StatefulSet) to be set via helm parameters.,2
"set max tree width to 1200px (#16067)the totalwidth of the tree view will depend on the window size like before, but max out at 1200px",1
Remove ``dags.gitSync.excludeWebserver`` from chart ``values.schema.json`` (#16070)This was mistakenly added back to the schema and can safely be removed.,4
"Add Connections, Variables and Env Vars to Helm Chart docs (#16059)",2
"Chart: Template ``airflowLocalSettings`` and ``webserver.webserverConfig`` (#16074)By templating `airflowLocalSettings` and `webserver.webserverConfig`, weallow users to more easily create reusable values. For example:```airflowLocalSettings: |  def pod_mutation_hook(pod):      pod.spec.containers[0].env.append({          ""name"": ""HELM_RELEASE"",          ""value"": ""{{ .Release.Name }}"",      })```",1
Improve compatibility with mssql (#9973)This PR adds full support for MsSQL! YAY!,1
"Parse recently modified files even if just parsed (#16075)This commit adds an optimization where the recently modified files(detected by mtime) will be parsed even though it has not reached`min_file_process_interval`.This way you can increase `[scheduler] min_file_process_interval` toa higher value like `600` or so when you have large number of files toavoid unnecessary reparsing if files haven't changed, while still makingsure that modified files are taken care of.",2
Modify return value check in python virtualenv jinja template (#16049),5
Configurable resources for git-sync sidecar (#16080),5
pass wait_for_done parameter down to _DataflowJobsController (#15541),5
Add close/open indicator for import dag errors (#16073)* Add close/open indicator for import dag errorsAdd the same icon used to open all Dag import errors to expand/collapse each error* Update airflow/www/static/css/flash.cssCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>,5
"A bunch of web UI backend tests (#15993)* Add test coverage to task instance clear view* Add tests for all TaskInstanceModelView viewsThe failure case for action_clear is not currently working. Bugfix PRfor that is pending, we'll remove the xfail marker when it goes in.* Test all DagRunModelView action views* Remove needless str-bytes coersion codeIn Python 3.6+, json.loads() can take bytes directly, so we don't needto decode ourselves.* Add test for VariableModelView.action_varexport* Add test for VariableModelView.action_muldelete* Test PoolModelView.action_muldelete",4
Fix bigquery type error when export format is parquet (#16027),0
Chart: Add ``extraInitContainers`` to scheduler/webserver/workers (#16098)Allow users to specify custom init containers on the core airflowcomponents.,5
Chart: Fix ``PgBouncer`` exporter sidecar (#16099)An extra colon crept in and was breaking the pgbouncer exporter sidecar,4
"Restores apply_defaults import in base_sensor_operator (#16040)The GCSToLocalFilesystemOperator in Google Provider <=3.0.0 had wrongimport for apply_defaults. It used`from airflow.sensors.base_sensor_operator import apply_defaults`instead of`from airflow.utils.decorators import apply_defaults`When we removed apply_defaults in #15667, the base_sensor_operatorimport was removed as well which made the GCSToLocalFilestystemOperatorstops working in 2.1.0Fixes: #16035",0
Check synctatic correctness for code-snippets (#16005)* Check syntactic correctness for code-snippets* fixup! Check syntactic correctness for code-snippets* fixup! fixup! Check syntactic correctness for code-snippets* fixup! fixup! fixup! Check syntactic correctness for code-snippets,0
Properly remove user for test_create_user (#15981),3
fix: restore parameters support when sql passed to SnowflakeHook as str (#16102),1
Remove the `not-allow-trailing-slash` rule on S3_hook (#15609),1
add mssql health check (#16103),1
"Fill the ""job_id"" field for `airflow task run` without `--local`/`--raw` for KubeExecutor (#16108)",1
Fix apply defaults for task decorator (#16085),0
"Fixes problem where conf variable was used before initialization (#16088)There was a problem that when we initialized configuration, we've runvalidate() which - among others - checkd if the connection is an `sqlite`but when the SQLAlchemy connection was not configured via variable butvia secret manager, it has fallen back to secret_backend, which shouldbe configured via conf and initialized.The problem is that the ""conf"" object is not yet created, becausethe ""validate()"" method has not finished yet and""initialize_configuration"" has not yet returned.This led to snake eating its own tail.This PR defers the validate() method to after secret backends havebeen initialized. The effect of it is that secret backends mightbe initialized with configuration that is not valid, but there areno real negative consequences of this.Fixes: #16079Fixes: #15685starting",0
Updated table component (#15805)* use react-table for table data* add skeleton loader to table* refactor loading data* UI pagination* server-side pagination* componentize custom react table* fix placeholder switch* add test that errors are rendered* Update airflow/ui/src/interfaces/react-table-config.d.tsremove extraneous commentCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>* Update airflow/ui/src/components/Table.tsxCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>* update sort icons and pagination displayCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>,5
Fix hooks extended from http hook (#16109),1
Add ``.gitattributes`` for ignoring tests files in ``git archive`` (#16122)I did this manually while releasing the Helm Chart.,2
Update Chart version to ``1.1.0-rc1`` (#16124)Let's change the Chart version to differentiate from currentlyreleased version.,2
Fix Typo: ``RedctableItem`` -> ``RedactableItem``  (#16119),2
"Chart: Add extra ini config to ``pgbouncer`` (#16120)This allows users to add extra ini config to pgbouncer, for example toadjust how frequently pgbouncer stats are output or reserve_pool size.",5
"Add Helm Chart Release Guide (#16121)This commits adds ""manual"" instructions to release Helm Chart.This will be automated by a script over coming days.",2
Add  mssql db check before starting tests (#16134)* do a db check for mssql before starting tests* add health check integration* fix health check,0
"Chart: Allow ``webserver.base_url`` to be templated (#16126)As `config`'s documentation states values are passed through `tpl`,one would expect `config.webserver.base_url` to also support templating.",1
Pins docutils to <0.17 until breaking behaviour is fixed (#16133)Sphinx RTD theme 0.5.2. introduced limitation to docutils to account forsome docutils markup change:https://github.com/readthedocs/sphinx_rtd_theme/issues/1112,4
"Removes unnecessary packages from setup_requires (#16139)This change removes unnecessary dependencies from setup_requires:* docutils are not needed in setup requires and actually  having them here caused harmful upgrade even if docutils  are limited to <0.17 elsewhere* setup_tools should not be needed in setup_requires (by  the time setup_requires are parsed, they should be already  installed* bowler is not needed any more in setup.py (we got rid of it  when we got rid of backport packages.Also docutils<0.17 limitation is moved to install_requiresbecause docutils was already a transitive dependency ofairflow without extras and it could be upgraded even if thereis a limitation in extra.",2
Move images needed only during CI to `airflow-ci` DockerHub (#16116)We have now separate `apache/airflow-ci` DockerHub repo and wemove all our images needed only during CI there.The images from the main `apache/airflow` remaining are:* airflow tagged and latest tagged production images* images neded by the Helm Chart,2
"Chart: Adds support for custom command and args (#16153)Some images may not want to use the same command/args as the communityimage, so expose them as parameters.",2
remove retry for now (#16150),1
Added Kayzen to INTHEWILD.md (#16154)Added Kayzen to the list of companies using Apache Airflow,1
"Fix Celery executor getting stuck randomly because of reset_signals in multiprocessing (#15989)Fixes #15938multiprocessing.Pool is known to often become stuck. It causes celery_executor to hang randomly. This happens at least on Debian, Ubuntu using Python 3.8.7 and Python 3.8.10. The issue is reproducible by running test_send_tasks_to_celery_hang in this PR several times (with db backend set to something other than sqlite because sqlite disables some parallelization)The issue goes away once switched to concurrent.futures.ProcessPoolExecutor. In python 3.6 and earlier, ProcessPoolExecutor has no initializer argument. Fortunately, it's not needed because reset_signal is no longer needed because the signal handler now checks if the current process is the parent.",0
"Fix dag.clear() to set multiple dags to running when necessary (#15382)closes: #14260related: #9824When clearing task across dags using ExternalTaskMarker the dag state of the external DagRun is not set to active. So cleared tasks in the external dag will not automatically start if the DagRun is a Failed or Succeeded state.#9824 tried to fix a similar issue for subdag. But it did not fix ExternalTaskMarker. This PR fixes both.Two changes are made to fix the issue:Make clear_task_instances set DagRuns' state to dag_run_state for all the affected DagRuns.The filter for DagRun in clear_task_instances is fixed too. Previously, it made an assumption that execution_dates for all the dag_ids are the same, which is not always correct.test_external_task_marker_clear_activate is added to make sure the fix does the right thing.",0
Marking success/failed automatically clears failed downstream tasks  (#13037)closes: #12485,0
"Treat `AirflowSensorTimeout` as immediate failure without retrying (#12058)## Expected behaviourFor a sensor like this, the intention of the DAG author is usually to fail the sensor if it's still not done after ten minutes. However, if the sensor fails prematurely due to other unexpected reasons (such as network outage), retry at most twice.```pythonsensor = PythonSensor(    task_id='sensor',    python_callable=python_callable,    timeout=60 * 10,    retries=2,    mode=""reschedule"",)```## Actual behaviourThe actual current behaviour of Airflow is to retry when the sensor times out. So the effective timeout of the sensor becomes 60 * 10 * (retries + 1) = 30min. This often causes confusion. It also makes it impossible to achieve the expected behaviour no matter how the author configures the sensor.## FixThis PR fixes this issue. `AirflowSensorTimeout` is now treated as immediate failure. This achieves the expected behaviour. The sensor will fail if timeout is reached. If someone really wants the previous behaviour, he can always increase the timeout. I.e instead of failing and retrying every ten minutes three times, just set the timeout to 30min.",1
Add updated-name wrappers for built-in FAB methods. (#16077),5
"Fix failing static check (#16162)This check was failing on master, example: https://github.com/apache/airflow/runs/2702121768#step:10:160",1
"Mark `test_send_tas_to_celery_hang` as quarantined (#16169)The test_send_tasks_to_celery_hang hangs on self-hosted runners moreoften than not.It's been introduced in #15989 and while the test does not usually hangon regular GitHub runners, or in case of running it locally (I could notmake it fail), it does hang almost always when run on self-hostedrunners.Marking it as quarantined for now.Issue #16168 created to keep track of it.",1
Cattrs 1.7.0 released by the end of May 2021 break lineage usage (#16173)See https://github.com/apache/airflow/issues/16172For now we limit the cattrs to < 1.7.0,0
Fix: Unnecessary downloads in ``GCSToLocalFilesystemOperator`` (#16171)Fixes #15005 GCSToLocalFilesystemOperator unnecessarily downloads objects when it checks object size.Co-authored-by: Pavel Kachalov <pavel_kachalov@epam.com>,5
Chart Docs: Separate section for Guides (#16175)Similar to https://airflow.apache.org/docs/apache-airflow-providers-google/stable/index.htmlwe should have separate section on Guides,2
"Bump ``pre-commit`` hooks (#16174)Updates `pyupgrade`, `black` and `pydocstyle`",2
Fix: GCS To BigQuery source_object (#16160)* Fix: GCS To BigQuery source_object #16008Fix GCS To BigQuery source_object to accept both str and list* convert source_objects to list if not listconverting source_objects to list instead of modifying the logic part* add tests,3
Wait for successful airflow-init service completion (#16180),5
CI: Remove ``sleep`` from Static Check Step (#16178)I think this was added as a DEBUG step which was forgotten to removein https://github.com/apache/airflow/pull/15944,4
Fixes failing static checks after recent pre-commit upgrade (#16183),0
Fix loading CI images from new `airflow-ci` location (#16187),1
"Uses bind volume instead of docker volume for MSSQL docker in tmpfs (#16159)Seems that MSSQL is not able to use data volume when it is mountedfrom tmpfs filesystem. See https://github.com/microsoft/mssql-docker/issues/13In such case, instead of mounting docker-created volume we mounta volume mounted from home directory of the user which is unlikelyto be a tmpfs volume.",1
Replace deprecated ``dag.sub_dag`` with ``dag.partial_subset`` (#16179)This is follow up of https://github.com/apache/airflow/pull/11542to all the missed places in the codebase.,2
Fix docs for ``dag_concurrency`` (#16177)The docs were incorrect. Bug introduced in https://github.com/apache/airflow/pull/15183/unfortunately.,0
Update the Python client version (#16191),5
Fix typo. (#16192),2
Rename the main branch of the Airflow repo to be `main` (#16149),5
"Format more dates with timezone (#16129)* extra datetime formattingSome dates were not being formatted based on the user-selected timezone.This will check for the dag, task, and dag details pages.Also, on the task page, we can check for http strings and turn them into links* use updateAllDateTimes with classNames of places to formatadd a classname to areas we want to format with the timezone and will be updated whenever the timezone select changes value* prefix selector classes with js-* add js- tag to ti-attr selector class",1
Fixed tests failing after switching to `main` branch (#16197),0
call resource based fab methods. (#16190),5
Adding extra requirements for build and runtime of the PROD image. (#16170)This PR adds capability of adding extra requirements to PROD image:1) During the build by placing requirements.txt in the   ``docker-context-files`` folder2) During execution of the container - by passing   _PIP_ADDITIONAL_REQUIREMENTS variableThe second case is only useful durint quick test/development andshould not be used in production.Also updated documentation to contain all development/testvariables for docker compose and clarifying that the optionsstarting with _ are ment to be only used for quick testing.,3
Fix S3 Select payload join (#16189),0
Update local variable names. (#16212)* Update local variable names.* Fix reverted method names.,4
"Chart: Adds labels to Kubernetes worker pods (#16203)We want to set labels on the KubernetesExecutor worker pods as well asone would expect those pods to show up when, say, filtering running podsby release name.",1
Fix typo in docker-stack documentation (#16221),2
Add transparency for unsupported connection type (#16220),1
"Bug Pod Template File Values Ignored (#16095)These seem to be the remaining values which are forcefully ignored when set by a pod-template, due to their default values in the constructor. Furthermore I do not see the value of a ""test_image_pull_policy_not_set"" test or feature, since k8s already provides a default value for image_pull_policy.",1
"Chart: Only mount DAGs in webserver when required (#16229)Regardless of gitsync or persistence, 2.0+ does not need the DAGs in thewebserver.",2
Doc: Add page containing list of Database Migrations (#16181)closes https://github.com/apache/airflow/issues/11989,0
Small improvements for README.md files (#16244),2
Update .github/boring-cyborg.yml (#16259),5
Add test_connection method to Airbyte hook (#16236),1
Remove unused internal function left over form Scheduler HA work (#16269)As of AIP-15 this function is not called anymore and should have beendeleted then.,4
"Ignore airflow/_vendor for building python API docs (#16270)On 3.6 it's slower than needed, but on Py 3.8 it causes a doc builderror (on a type comment of all things)",0
Fix broken CI (#16265)* Fix broken CI* Changes by pre-commit,4
"Add pre-commit hook for Boring Cyborg (#16260)I noticed that the cyborg configuration contains frequently unused patterns that result from moving files. To spot these problems before merging, I add a pre-commit hook that checks to see if each pattern has at least one match.",1
Use f-string in custom operator docs (#16250)Use f-string formatting to support good practices,1
Chart: Update the default Airflow Version to ``2.1.0`` (#16273)Since 2.1.0 is released we should make it default,1
"Doc: Use correct version in Chart docs (#16277)If a version is not specified it uses ""devel"" when building production docs",2
Fix Helm git sync secrets typo (#16278),2
Add Clicksign to INTHEWILD.md (#16272),1
Docs: Fix url for ``Elasticsearch`` (#16275)`https://https//www.elastic.co/elasticsearch` -> `https://www.elastic.co/elasticsearch`,0
Update target branch in `boring-cyborg` for `checkUpToDate` (#16274)If we don't update the `targetBranch` it uses `master` by default,1
Run Kubernetes integration tests when Helm Chart is changed (#16276)Currently we were only running K8S integration tests when Kubernetes files were changed.Example: https://github.com/apache/airflow/pull/16273 . This PR fixes it,0
Don't run migration for adhoc command (#16255),1
Improve argument handling in entrypoint_prod.sh (#16258),1
feat: Helm chart adding minReplicaCount to the Keda worker-kedaautoscaler.yaml (#16262)Co-authored-by: andormarkus <andormarkus@marc-o-polo.com>,1
Add support for extra parameters to samba client (#16115)* Add support for extra parameters to samba client* Add description of extra fields* Reformat extra field descriptions* Add spelling for 'NetBIOS'* Fix imports* Format as code to avoid spellcheck errors* Add word to spellcheck* Fix order,0
"Chart: Fix updating from ``KubernetesExecutor`` to ``CeleryExecutor`` (#16242)We will create these secrets (if necessary) _even if_ we aren'tcurrently using CeleryExecutor or CeleryKubernetesExecutor. As we arerelying on the ""pre-install"" hack to prevent changing randomlygenerated passwords, updating the executor later doesn't give us theopportunity to deploy them when we need them. We will always deploythem defensively to make the executor update path actually work.",1
"Don't fail to log if we can't redact something (#16118)Rather than dying with an exception, catch it and warn about that,asking users to report it to us.Additionally handle the specific case where a file handle/IO object islogged -- we definitely don't want to iterate over that!",5
Fixed wrong value of store-dag-code default (#16093)Fixes: #16090,0
Updated documentation for June 2021 provider release (#16294),1
Docs: Fix creating a connection docs (#16312)Minor fix for Creating a Connection from the CLI documentation,2
fix wasb remote logging when blob already exists (#16280)Co-authored-by: Felipe Lolas <felipe.lolas@bci.cl>,2
Fix a couple typos in the docs (#16321)Co-authored-by: Razzi Abuissa <rabuissa@wikimedia.org>,2
Fix Dag Details start date bug (#16206)* Only show Start Date when catchup=True* add catchup field to details* add started field for catchup=false dags,2
Use updated _get_all_non_dag_permissions method. (#16317),2
"Bump pylint CI step to 60m timeout (#16335)The pylint step has been pretty consistenly timing out for me lately, sobump the timeout higher. We can always reduce it later once things arerunning more smoothly.",1
Docs: Fix ``flask-ouathlib`` to ``flask-oauthlib`` in Upgrading docs (#16320),2
Chart: Update the ``appVersion`` to 2.1.0 in ``Chart.yaml`` (#16337)Missed it in https://github.com/apache/airflow/pull/16273,2
Chart docs: Fix ``extrasecrets`` example (#16305)Found a couple places where we have incorrect examples of `extraSecrets`.,0
Use DAG_ACTIONS constant. (#16232),2
"Chart: Always deploy a ``gitsync`` init container (#16339)We will always deploy a gitsync init container anywhere we deploy agitsync sidecar, that way we ensure the main container has the DAGs whenit starts.",2
"Chart: Support job level annotations; fix jobs scheduling config (#16331)Add job level annotations as some tooling needs to be able to add them.Also fix jobs to use their own nodeSelector, affinity, andtolerations.closes #16291",5
"Chart: ``podAntiAffinity`` for scheduler, webserver, and workers (#16315)Set default podAntiAffinity for the components that can scale, as inmost cases you'd want them spread across available nodes.",5
Add Wisr to INTHEWILD.md (#16360),1
"Run mini scheduler in LocalTaskJob during task exit (#16289)Currently, the chances of tasks being killed by the LocalTaskJob heartbeat is high.This is because, after marking a task successful/failed in Taskinstance.py and mini scheduler is enabled,we start running the mini scheduler. Whenever the mini scheduling takes time and meet the next job heartbeat,the heartbeat detects that this task has succeeded with no return code because LocalTaskJob.handle_task_exitwas not called after the task succeeded. Hence, the heartbeat thinks that this task was externally marked failed/successful.This change resolves this by moving the mini scheduler to LocalTaskJob at the handle_task_exit method ensuringthat the task will no longer be killed by the next heartbeat",0
Make task ID on legend have enough width and width of line chart to be 100%.  (#15915)* Make task ID on legend have enough width and width of line chart to be 100%.* Make task ID on legend have enough width and width of line chart to be 100%.* Fix pylint errors.,0
Depreciate private_key_pass in SFTPHook conn extra and rename to private_key_passphrase (#14028),4
Make REST API List DAGs endpoint consistent with UI/CLI behaviour (#16318)Co-authored-by: jpyen <>,2
Docs: Change 10 minutes to 100 minutes in ``worker_refresh_interval`` (#16369)6000 seconds = 100 minutes not 10 minutesI forgot one zero when writing that down :),1
Swap out calls to find_permission_view_menu for get_permission wrapper. (#16377),1
Fix broken static checks from #15915 (#16378),0
Refactor: `SKIPPED` shouldn't be logged again as `SUCCESS`. (#14822)* `SKIPPED` shouldn't be logged again as `SUCCESS`.* `_safe_date` duplicates with `_date_or_empty`.* Borrowed advantage from `_safe_date`.,5
Fix normalize-url vulnerability (#16375)Update two packages that used a highly vulnerable version of normalize-urlSee https://github.com/facebook/create-react-app/issues/11054,0
"Fix TI success/failure links (#16233)fixes issue #15234.As of now, TI success & failure endpoints are POST only and behave differently as per the ""confirmed"" flag. They either render a confirmation page or updates the TI states on the basis of that flag, something which is not a great design.Also, as these endpoints are POST only, they throw a 404 error when someone clicks on the link received via email.To fix the issue, extracting the rendering functionalities into a diff endpoint ""/confirm"" & keeping these endpoints as pure POST endpoints.",5
"Fix CLI connections import and migrate logic from secrets to Connection model (#15425)* Add field 'extra' to Connection init* Fix connections import CLIIn connections_import, each connection was deserialized and stored into aConnection model instance rather than a dictionary, so an erroneous call to thedictionary methods .items() resulted in an AttributeError. With this fix,connection information is loaded from dictionaries directly into theConnection constructor and committed to the DB.* Apply suggestions from code review* Use load_connections_dict in connections importCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",2
"Calendar UI improvements (#16226)- change calendar borders from black to grey to match rest of the app better- remove unneeded title- remove ""View"" from all dag views to save space and consistency",2
Correctly set `dag.fileloc` when using the `@dag` decorator (#16384)Previously this was always showing up as `airflow/models/dag.py`!,2
Removes quotes for _PIP_ADDITIONAL_REQUIREMENTS (#16382)Fixes #16363,0
Remove deprecated import form within test zip file. (#16390)This was causing the following warning in the DagBag tests```/opt/airflow/tests/models/../dags/test_zip_invalid_cron.zip/test_invalid_cron.py:21: DeprecationWarning: This module is deprecated. Please use `airflow.operators.dummy`.```,1
Look for the correct flash message on failure (#16385),0
Use resource and action names. (#16380),1
Add `passphrase` and `private_key` to default sensitive fileld names (#16392),2
"Remove old test dag that is out of place (#16391)This dag hasn't been used in tests since about 2017, and is the lastfile in the top level `dags/` folder -- it's high time we removed it",4
"Support remote logging in elasticsearch with filebeat 7 (#14625)Filebeat 7 renamed some fields (offset->log.offset and host->host.name),so allow the field names Airflow uses to be configured.Airflow isn't directly involved with getting the logs _to_elasticsearch, so we should allow easy configuration to accomodatewhatever tools are used in that process.",1
Update permission migrations to use new naming scheme. (#16400),1
"Quarantine a flaky test (#16402)test_process_kill_calls_on_failure_callback seems to be flaky in the CIsuite and occasionally fails without reason. Until it's fixed, it isquarantined with other flaky tests.",3
"Don't show stale Serialized DAGs if they are deleted in DB (#16368)If `DagBag.get_dag()` is called currently, it will return the DAGeven if the DAG does not exist in `serialized_dag` table.This PR changes that logic to remove the dag from local cache toowhen `DagBag.get_dag()` is called. This happens after`min_serialized_dag_fetch_secs`.",2
"Don't run tests for deprecated method on DAG class (#16397)And since the deprecated method uses a decorator, we need stacklevel ofthree for the warning to show up in the right place.",2
"Fixes AzureFileShare connection extras (#16388)* Fixes AzureFileShare connection extrasThe Azure File Share connection has not been creted in #15159 and itcaused an unexpected side effect as the default Azure Connectionpassed service_options dictionary to FileServicewith key that was unexpected.This change fixes two things:1) adds AzureFileShare connection that has separate conn_type   and handles the extra_options specific for FileService Hook   available in the Airflow UI.2) handles the ""deprecated"" way of passing keys without UI prefix   but raises a deprecation warning when such key is passed or   when the Wasb connection is used with an empty extras rather   than Azure File Share.Fixes #16254* fixup! Fixes AzureFileShare connection extras* fixup! fixup! Fixes AzureFileShare connection extras",2
Sanitize end of line character when loading token from a file (vault) (#16407)This commit addresses https://github.com/apache/airflow/issues/16406,0
Add BigQueryToMsSqlOperator (#15422),1
postgres_hook_aws_conn_id (#16100),1
Add support of capacity provider strategy for ECSOperator (#15848),1
fix: AwsGlueJobOperator change order of args for load_file (#16216),2
Validate retries value on init for better errors (#16415),0
Clean Markdown with dedent to respect indents (#16414),4
Update copy command for s3 to redshift (#16241),5
"OdbcHook returns None. Related to #15016 issue. (#15510)* OdbcHook returns None. Related to #15016 issue.This PR is related to #15016 issue.OdbcHook returns None for non-boolean-like string values in connect_kwargs dict arg, however connect_kwarg values should remain as is in this case.According to the discussion on #15016, we agreed to remove the clean_bool function, as it is not needed actually for processing boolean values in JSON .",5
Remove assignment that assigns a variable to itself (#16413),4
fix: ensure datetime-related values fully compatible with MySQL and BigQuery (#15026),5
"Fix deprecation warnings location in google provider (#16403)These warnings were being issued from the wrong location, making themhard for any users who hit them to fix```tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/dagbag.py:317: DeprecationWarning: This operator is deprecated. Please use BigQueryUpdateDatasetOperator.    loader.exec_module(new_module)tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_roundtrip_provider_example_dagstests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/baseoperator.py:181: DeprecationWarning: `destination_bucket` is deprecated please use `bucket_name`    result = func(self, *args, **kwargs)tests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_roundtrip_provider_example_dagstests/serialization/test_dag_serialization.py::TestStringifiedDAGs::test_serialization  /opt/airflow/airflow/models/baseoperator.py:181: DeprecationWarning: `destination_object` is deprecated please use `object_name`    result = func(self, *args, **kwargs)```",1
Add rules describing SemVer approach for various Airflow packages. (#16422),1
"Fix issue with parsing error logs in the KPO (#15638)This fixes an issue where logs that do not have timestamps cause theKubernetesPodOperator to crash. Basically error logs created by airflowdo not have timestamps, which was causing an unhandled exception thatwould kill the task. This PR handles that exception and ensurescontinued task processing",0
Queue tasks with higher priority and earlier execution_date first. (#15210)Co-authored-by: Ginevra Gaudioso <ggaudioso@vectra.ai>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
"Make SparkSqlHook use Connection (#15794)* Make SparkSqlHook use ConnectionThis allows a SparkSqlHook to be created without a backing Connection,and if a backing Connection *is* found, use it to provide the defaultarguments not explicitly passed into the hook.* Properly clean connections for Spark tests* Expected Connection values in SparkSqlHook testsNow that SparkSqlHook defaults to read values from Connection if aconfig is not explicitly provided, we need to tweak the tests to reflectthis expectation.",3
add num_runs query param for tree refresh (#16437)- add `num_runs` as a meta field to add to the tree refresh request,1
generate go client with latest openapi generator template (#16411)* generate go client with latest openapi generator template* bump client version,3
"Adding `only_active` parameter to /dags endpoint (#14306)I noticed that the `/dags` endpoint returns information on all entries in the DAG table, which is often many more DAGs than are activeand likely includes DAGs which have been removed from Airflow. This PR adds a boolean `only_active` parameter to the `/dags` endpoint which will then only return active DAGs. I also noticed that this endpoint was hitting a deprecated codepath by dumping a `DAG` object to the DAGDetailSchema, thus hitting calling `DAG.is_paused()` I have updated the schema to call the correct function (`DAG.get_is_paused`) since I'm assuming the deprecated functions may be removed some day.",4
Fix spellcheck failure (#16445),0
yarn audit (#16440)Bump a lot of npm modules in packages.json to resolve all severe and moderate vulnerabilities found when using `yarn audit`,1
Disable Pylint member check for ``tests/decorators/test_python.py`` (#16443)Some PRs are failing this check:https://github.com/apache/airflow/pull/16408/checks?check_run_id=2823934948#step:9:54https://github.com/apache/airflow/pull/16393/checks?check_run_id=2813095540#step:9:54However those PRs have not changed that file.,2
Added ability for Snowflake to attribute usage to Airflow by adding an application parameter (#16420),2
Add ElasticSearch Connection Doc (#16436),2
"More documentation update for June providers release (#16405)The provider changelogs were already merged but they could not bereleased due to unavailability of signing key in remote locationThe documentation has once more been updated, including latestmerges - documentation-only changes were removed frombugs/feetures/breaking changes lists.Few other improvements:* Pre-commit was added to make sure that the documentation  in provider's index.rst files includes the latest changelog.* Index.rst now contain includes of the CHANGELOG.rst rather  than copy of the CHANGELOG.rst* The `prepare-provider-package` breeze command has --non-interactive  flag now* generated provider package README.rst contain changelog so you can  see the changelog directly in PyPI* ""Suggest change on this page"" link in documentation is fixed.",0
"Remove class references in changelogs (#16454)PyPI does not handle class references in changelogs.Since we want changelogs to be part of PyPI readmes,we need to remove those.",4
Fix formatting and missing import (#16455),2
"Fix external elasticsearch logs link (#16357)During the 2.0 upgrade, the external log link when using elasticsearchremote logs was broken. This fixes it, including it only being shown if`[elasticsearch] frontend` is set.",1
"Fix templated default/example values in config ref docs (#16442)We should show the actual default/example value in the configurationreference docs, not the templated values.e.g. `{dag_id}` like you get in a generated airflow.cfg, not `{{dag_id}}like is stored in the airflow.cfg template.",5
Add missing tests for snowflake changes (#16463)Fixes problem introduced in #16420,0
"Handle missing/null serialized DAG dependencies (#16393)When a serialized DAG is missing a ""dag_dependencies"" field (possiblewhen upgrading), PostgreSQL would return NULL when accessing the fieldwith a JSON function. This value would fail subsequent code, so we needsome logic to handle it.Fix #16356",0
"Correctly handle None returns from Query.scalar() (#16345)This is possible when the query does not return a row, according toSQLAlchemy documentation. We can handle them to provide better errors inunexpected situations.Toward #8171, fix #16328.",0
Synchronizes updated changelog after buggfix release (#16464),0
"Switch to built-in data structures in SecretsMasker (#16424)Using Iterable in SecretsMasker might cause undesireableside effect in case the object passed as log parameteris an iterable object and actually iterating it is not idempotent.For example in case of botocore, it passes StreamingBodyobject to log and this object is Iterable. However it can beiterated only once. Masking causes the object to be iteratedduring logging and results in empty body when actual resultsare retrieved later.This change only iterates list type of objects and recurrentlyredacts only dicts/strs/tuples/sets/lists which should neverproduce any side effects as all those objects do not have sideeffects when they are accessed.Fixes: #16148",0
"Remove support jinja templated log_id in elasticsearch (#16465)* Remove support jinja templated log_id in elasticsearchThis simplifies the handling of log_id in elasticsearch remote logging.Support for a jinja templated log_id was never explicitly documented, andwhere it was included in examples it was actually broken.If someone fixed the broken jinja examples and used it, differing formats forexecution_date may be problematic and updating documents in Elasticsearchmay be required.* Fix changelog* fixup! Fix changelogCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",4
Chart: Allow configuration of pod resources in helm chart (#16425)Currently it's possible to configure some of the pod resources via the helm chart values like for example the scheduler pod main container but it's not possible for the `scheduler-log-groomer` container. In deployments with `ResourceQuotas` it's desirable to be able to control the `limits` of each container specifically to avoid pods reserve way too much cpu and memory  and eat up the quota unnecessarily. Even though the current helm chart allows to add a `LimitRange` that defines the default `limits` that is not enough. This introduces the ability to provide a specific resource blocks for * `pod_template_file`: for the pods started by the `KubernetesPodOperator` itself . ([pod_template_file doc](https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html#pod-template-file))* pgbouncer's `metrics-exporter`* scheduler's  `scheduler-log-groomer`* webserver's initContainer `wait-for-airflow-migrations`* worker's `worker-log-groomer`,2
"Update link to match what is in pre-commit (#16408)[The k8s schema repository that has been used for chart pytest has gone stale with no updates in 14 months](https://github.com/instrumenta/kubernetes-json-schema). There are no new updates beyond 1.18.1, and [PRs for updates are not being merged](https://github.com/instrumenta/kubernetes-json-schema/pulls). Airflow is using a more active fork [in pre-commit](https://github.com/apache/airflow/blob/main/.pre-commit-config.yaml#L571), so this change uses [that updated fork](https://github.com/yannh/kubernetes-json-schema) in chart pytests too. This updated fork's latest schema is 1.21.1, and has had changes within the last month.",4
fix: change graph focus to top of view instead of center (#16484),4
"Fix Elasticsearch external log link with ``json_format`` (#16467)When using json_format with elasticsearch remote logging theexecution date is sanitized, so we need to use the samesanitized value when building the log_id for external links.",2
Fix description on scheduler.livenessprobe.periodSeconds (#16486)Simply fix the description of scheduler.livenessprobe.periodSeconds so it is right in the params reference docs.,2
Backfill: Don't create a DagRun if no tasks match task regex (#16461)Backfill should not create a DagRun in case there is no any task that matches the regex.closes: #16460,2
"Rename DAG concurrency settings for easier understanding (#16267)``dag_conccurency`` ->  ``max_active_tasks_per_dag``Some of Airflow's concurrency settings have been a source of confusion for a lot of users (including me), for example:- https://stackoverflow.com/questions/56370720/how-to-control-the-parallelism-or-concurrency-of-an-airflow-installation- https://stackoverflow.com/questions/38200666/airflow-parallelismThis commit is an attempt to make the settings easier to understand",1
Make job name check optional in SageMakerTrainingOperator (#16327)closes: #16299In this commit I make it possible to avoid listing existing training jobs by adding a `check_if_job_exists` parameter to the SageMakerTrainingOperator.,5
Support non-https elasticsearch external links (#16489)Also provide an example for `frontend` as its not immediately apparenthow to build a short(ish) deep link.,2
Fix typo in elasticearch frontend docs (#16490),2
"Fix unsuccessful KubernetesPod final_state call when `is_delete_operator_pod=True` (#15490)If a Kubernetes Pod ends in a state other than `SUCCESS` and `is_delete_operator_pod` is True, then use the `final_state` from the previous `create_new_pod_for_operator` call since the pod is already deleted and the current state can't be re-read.closes: https://github.com/apache/airflow/issues/15456",0
Checks-out missing commits on selective checks (#16470)Recent improvement of the build images to use pull_request_targetremoved by accident fetching of the PR commits which makes itimpossible to determine which files has changed in PR in thePR build image workflow.This commit brings it back.,1
Use resource and action names. (#16410),1
Update `airflow tasks *` commands to lookup TaskInstances from DagRun Table (#16030)This change allows to lookup TaskInstances using DagRun.run_id in task commandsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Fix S3ToFTPOperator (#13796),1
Convert port value to a number before calling test connection (#16497),3
"We don't need to build against Python 2.7 or 3.5 anymore (#16433)Airflow 1.10 has reached end of life on June 17th 2021, so we can tidyup our build scripts and not have to build these versions anymore",2
Docs: update ``ci.yml`` link in TESTING.rst (#16496)Update link in TESTING.rst guide. Hopefully this is the CI.yml file being referred to.,2
Add means to Duplicate connections from UI (#15574)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Avoid recursing too deep when redacting logs (#16491)Fix #16473,0
Fix broken build image build after removing Py2.7/3.5 builds (#16509),4
"Docs: Fix API verb from ``POST`` to ``PATCH`` (#16511)Updating the dag, i.e. pausing/unpausing is ``PATCH``.",1
Exclude Yarn.lock to be built into python wheel (#16494),5
"Bugfix: Allow clearing tasks with just ``dag_id`` and empty ``subdir`` (#16513)Currently if we run the following command:```airflow tasks clear example_bash_operator --subdir """"```we get the following error:```Traceback (most recent call last):  File ""/usr/local/bin/airflow"", line 33, in <module>    sys.exit(load_entry_point('apache-airflow', 'console_scripts', 'airflow')())  File ""/opt/airflow/airflow/__main__.py"", line 40, in main    args.func(args)  File ""/opt/airflow/airflow/cli/cli_parser.py"", line 48, in command    return func(*args, **kwargs)  File ""/opt/airflow/airflow/utils/cli.py"", line 91, in wrapper    return f(*args, **kwargs)  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 454, in task_clear    if args.dag_id and not args.subdir and not args.dag_regex and not args.task_regex:  File ""/opt/airflow/airflow/models/dag.py"", line 1413, in clear_dags    for dag in dags:TypeError: 'DAG' object is not iterable```This is becase `DAG.clear_dags` expects an iterable.",2
Fix DAG run state not updated while DAG is paused (#16343)The state of a DAG run does not update while the DAG is paused.The tasks continue to run if the DAG run was kicked off beforethe DAG was paused and eventually finish and are marked correctly.The DAG run state does not get updated and stays in Running state until the DAG is unpaused.This change fixes it by running a check on task exit to update state(if possible) of the DagRun if the task was able to finish the DagRun while the DAG is pausedCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
"Revert ""Exclude Yarn.lock to be built into python wheel (#16494)"" (#16518)This reverts commit a76a67cb8bb3dc55b1b0ffd05084919924b5b8d3.",4
Fix package version in ``LICENSE`` file (#16514)Some of the versions were outdated and we no longer use `Bootstrap Toggle` as we removed it in https://github.com/apache/airflow/pull/11035,4
Chart: Support ``extraContainers`` and ``extraVolumes`` in flower (#16515)This allows for deploying sidecars in the flower pod.,1
Update Watchtower version to 1.0.6 (#16469)Currently a pre 1.0 version of Watchtower is being used. There have beenmany bug fixes and improvements since this version.,1
"Don't fail the build if we cant run `apt clean` (#16510)I just had a build fail with this:```E: Could not get lock /var/cache/apt/archives/lock. It is held byprocess 53088 (unattended-upgr)E: Unable to lock directory /var/cache/apt/archives/```That is not a fatal error, and we should just continue",0
"Allow null value for operator field in task_instance schema(REST API) (#16516)This change makes it possible to get ""old"" task instances from 1.10.x",1
Add docs index to README.md (#16495),2
Add AWS DMS replication task operators (#15850),1
Prepares documentation for rc2 release of Providers (#16501)* adds clear information that the provider is for 2.1+* adds explicit dependency to apache-airflow>=2.1.0 in dependency list* adds capability of specifying additional dependencies* different providers now can depend on different Airflow version* removed pre-commit check and provider info update for  provider-schema 2.0.0 compatibility  (not needed any more after >= 2.1.0 is used as Airflow >2.0.1  allows additional properties in provider_info)* Update changelog documentation for all providersCo-authored-by: jarek <jarek@penguini>,5
Update docs on setting up SMTP (#16523)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,1
Fix bug in mark TI success api (#16524)Mistakenly checking for the wrong args in TI success API. Introduced in PR https://github.com/apache/airflow/pull/16233.,0
"Stop showing output of a parallel job once it has finished. (#16528)If you are running this manually/locally you get a lot of ""extra"" outputwhich makes it harder to see what jobs are still actually running.",1
Updating the DAG docstring to include render_template_as_native_obj (#16534),2
Fetch Helm Chart inventory from remote cache (#16535),2
Add ``extra_headers`` argument to ``LivyHook`` and ``LivyOperator`` (#16512)Adds an `extra_headers` option to `LivyHook` and `LivyOperator`. This allows passing extra header options to requests before sending a request to Livy. This is required for instance when CSRF protection is enabled (i.e. on HDinsights): https://github.com/MicrosoftDocs/azure-docs/issues/10457.,2
Add undocumented parameters to ``LivyOperator`` and ``LivyHook`` docstrings (#16542),2
Added new pipeline example for the tutorial docs (#16084)Closes https://github.com/apache/airflow/issues/11208,0
Prepare for Python 3.9 support (#16536)This is the first step to add Python 3.9 support to Airflow.Hive should be excluded in this version because it requires sasllibrary which for now does not support Python 3.9.Until the https://github.com/dropbox/PyHive/issues/380 is solvedwe will exclude hive provider. This will be the next stepto add full support and exclusion but we need to merge it firstin order to be able to build image from `main`.Dockerfiles have been updated to remove some obsolete limits andtest dask executor was disabled conditionally in casedistributed framework cannot be imported.,2
Improve hints for reproducing tests (#16545)The hints to reproducible run of test are improved:* colors added* they are much shorter and only most useful option is shown,1
update example_jenkins_job_trigger.py (#16532)fix initialization of Request which used by jenkins.Jenkins.jenkins_open in DAG demo,2
Cleans up pre-commits a little (#16547)Few cleanups of pre-commits:* moving non-image-dependent pre-commits before image building* add setuptools as requirement for provider's dependency pre-commit* force python 3.6 for all image-dependent pre-commits,1
Adds automated generation of provider issue to track test progress (#16419)Allows to automatically generate draft of the issue which can beused to track progress of testing released providers.,1
Add selective permissions for GitHub Tokens (#16546)As of end of April we can set selective permissions for GitHubtokens https://github.blog/changelog/2021-04-20-github-actions-control-permissions-for-github_token/This allows us to use just a very small subset for workflows of ourswhich is good idea for limiting vector of attacks for supply-chainattacks.For example that would render recent codecov hacking completelyuseless even if someone grabs and uses the token immediately.,1
"added explanation for virtualenv vs docker (#16549)added a paragraph on top, and made ""Installing airflow in the local virtual environment"" its own section to show that setting up the venv and setting up docker are two different methods, making this more beginner-friendly",1
Make custom JSON encoder support Decimal (#16383),1
update README.md with 1.10 in EOL state (#16556),2
"Tree View UI for larger DAGs & more consistent spacing in Tree View (#16522)* Made squareX independent of screen width* Removed now unnecessary variable innerWidth.Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Schneider, Thilo <t.schneider3@fraport.de>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>",4
Clean up MSSQL data directories at the end of each CI run (#16503)Co-authored-by: Aneesh Joseph <admin@coderplus.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,1
"Switches to manually building docker images (#16570)According tohttps://www.docker.com/blog/changes-to-docker-hub-autobuilds/Docker is going to disable autobuilds for free tiers. We might be exemptfrom that via ASF, but docker autobuilds never worked well for us formultitude of reasons.This PR turns manually preparing the image into obligatory, manual stepwhen releasing Airflow.Part of #16555",1
"Use safe get with AWS DMS describe replication tasks (#16540)AWS DMS boto3 `describe_replication_tasks` omits `Marker` property inthe response if no more tasks is available, it also omits`ReplicationTasks` property if no tasks is found.",5
Add test connection method to http hook (#16568),1
Update Year in Providers NOTICE file and fix branch name (#16576)This PR/commit updates the year for the license to 2021 from 2020 and fixes the branch name to main instead of master,0
Remove limitation for elasticsearch library (#16553)* Remove limitation for elasticsearch libraryElasticsearch <7.6.0 does not work with Python 3.9 (importerrors on deprecated base64 functionality that have been removedin Python 3.9) see:ihttps://bugzilla.redhat.com/show_bug.cgi?id=1894188This PR bumps elasticsearch library version to latest available(7.13.1 as of this writing) in order to get it Python 3.9compatible.,1
Fix Orphaned tasks stuck in CeleryExecutor as running (#16550),1
Fix tasks in an infinite slots pool were never scheduled (#15247)Infinite pools: Make their `total_slots` be `inf` instead of `-1`,5
Redact conn secrets in webserver logs (#16579),2
Docs: Fix Taskflow API docs (#16574)Add missing `def` statement and `EmailOperator` import.,2
"Exclude ``yarn.lock`` from built Python wheel file (#16577)Same as https://github.com/apache/airflow/pull/16494 - However that PR had to be reverted in https://github.com/apache/airflow/pull/16518 as it failed building of PROD image, this PR/commit will fix it.PROBLEM: Currently the airflow wheel is built with the yarn.lock which is not actually used by the airflow itself. Having this file in the docker image causes the clair and trivy scanners to failFIX: The fix is to exclude the yarn.lock by specifying it in the manifest.in",0
"Fix pylint error in tests/ (#16585)I'm not sure why this changed/started failing, as we haven't touched this part of the file recently.",2
Ensure that `dag_run.conf` is a dict (#15057)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
"Switch to GitHub Container Registry by default (#16586)Yesterday GitHub moved Github Container Registry toGeneral Availability status. We are prepared to switch and testedit before, so this PR attempts to switch to it.",3
"Add back-compat layer to clear_task_instances (#16582)It is unlikely that anyone is using this function directly, but it iseasy for us to maintain compatibility, so we should",1
Remove Shell scripts under ``airflow/www`` from releases (#16588)We don't need the following files:- `airflow/www/compile_assets.sh`- `airflow/www/ask_for_recompile_assets_if_needed.sh`So we exclude them from sdist and wheel,1
"Always install sphinx_airflow_theme from pypi (#16594)We don't need a way to specify _in_ setup.py that this should beinstalled from a GitHub release -- it's never needed by users, and ifyou are developing the theme you can install the custom versionyourself.(The variable name is confusing too -- it wasn't pulling from git, butfrom a published release on GitHub.)Removing this just means one less thing to update.",5
Don't crash attempting to mask secrets in dict with non-string keys (#16601),5
Switch back temporarily to deprecated package registry (#16603),1
"Fix label_when_reviewed_workflow_run permissions (#16596)* Fix label_when_reviewed_workflow_run permissionsThis workflow run is currently failing with:```Run ./.github/actions/checks-action  with:    token: ***    name: Selective build check    status: in_progress    sha: 2cf8c7f268c1db73d840f029aa5180941519c492    details_url: https://github.com/apache/airflow/actions/runs/960898933    output: {""summary"": ""Checking selective status of the build in [the run](https://github.com/apache/airflow/actions/runs/960898933) ""}Error: Resource not accessible by integration```so I _think_ this will help* Update .github/workflows/label_when_reviewed_workflow_run.ymlCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",5
"Drop support for Helm 2 (#16575)Helm 2 is EOL, so bump our chart to the v2 apiVersionhttps://helm.sh/blog/helm-v2-deprecation-timeline/",2
TaskGroup add default_args (#16557)* TaskGroup add default_args* test case && pylint* TaskGroup default_args docs* Update docs/apache-airflow/concepts/dags.rstCo-authored-by: Xinbin Huang <bin.huangxb@gmail.com>Co-authored-by: Xinbin Huang <bin.huangxb@gmail.com>,2
Fix file name to verify release packages (#16605)typo: `check.files.py` -> `check_files.py`,2
"Chart: Support for overriding webserver and flower service ports (#16572)This allows services to expose sidecars with webservers in them, or even to only expose a sidecar (say enforcing inbound traffic to go through a proxy).If we are okay with the general approach, both NetworkPolicy and Ingress need similar changes so there is broad support for these types of sidecars.Closes #16039",1
UPDATING.md for changes included in 2.1.1 (#16615),4
Fix ``AttributeError``: ``datetime.timezone`` object has no attribute ``name`` (#16599)closes: #16551Previous implementation tried to force / coerce the provided timezone (from the dag's `start_date`) into a `pendulum.tz.timezone.*` that only worked if the provided timezone was already a pendulum's timezone and it specifically failed when with `datetime.timezone.utc` as timezone.,5
Add schema as DbApiHook instance attribute (#16521),5
Update Boto3 API calls in ECSOperator (#16050),1
fix(smart_sensor): Unbound variable errors (#14774)Signed-off-by: Shivansh Saini <shivanshs9@gmail.com>Closes #14770,5
"Set Process title for Worker when using ``LocalExecutor`` (#16623)This has annoyed me for a long time. When using  ``LocalExecutor``, it was difficult to see which process is a worker as it just showed up as below -- which had same title as parent scheduler process. This PR/commit adds a title for idle workers and when a task is running it has the ""command"" that is running in the title, similar to our supervising processBefore:```root       124  0.0  0.0   6676  4636 pts/1    Ss   Jun23   0:00  \_ -bashroot      1449  0.8  2.6 988356 326312 pts/1   Sl+  Jun23   0:16  |   \_ /usr/local/bin/python /usr/local/bin/airflow webserverroot      1584  0.0  0.4 121068 56864 pts/1    S+   Jun23   0:01  |       \_ gunicorn: master [airflow-webserver]root      1587  0.6  2.5 986144 318712 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1588  0.6  2.5 984776 317672 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1589  0.6  2.5 985688 318148 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1590  0.6  2.5 985200 317776 pts/1   Sl+  Jun23   0:11  |           \_ [ready] gunicorn: worker [airflow-webserver]root       128  0.0  0.0   6676  4552 pts/2    Ss   Jun23   0:00  \_ -bashroot     13933 31.0  0.9 466596 117656 pts/2   S+   00:22   0:01      \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13941  0.0  0.7 466340 97988 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13942  3.2  0.8 1392072 100136 pts/2  Sl+  00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13950  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13952  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13955  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13958  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13962  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13966  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13969  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13975  0.0  0.8 466340 98404 pts/2    S+   00:22   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13979  6.5  0.8 466596 99956 pts/2    S    00:22   0:00          \_ airflow scheduler -- DagFileProcessorManager```After (with no running tasks - idle workers):```root       124  0.0  0.0   6676  4636 pts/1    Ss   Jun23   0:00  \_ -bashroot      1449  0.8  2.6 988356 326312 pts/1   Sl+  Jun23   0:16  |   \_ /usr/local/bin/python /usr/local/bin/airflow webserverroot      1584  0.0  0.4 121068 56864 pts/1    S+   Jun23   0:01  |       \_ gunicorn: master [airflow-webserver]root      1587  0.6  2.5 985752 318184 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1588  0.6  2.5 984776 317672 pts/1   Sl+  Jun23   0:11  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1589  0.6  2.5 985688 318148 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1590  0.6  2.5 985200 317776 pts/1   Sl+  Jun23   0:11  |           \_ [ready] gunicorn: worker [airflow-webserver]root       128  0.0  0.0   6676  4552 pts/2    Ss   Jun23   0:00  \_ -bashroot     13237 25.7  0.9 466596 117692 pts/2   S+   00:20   0:02      \_ airflow worker -- LocalExecutorroot     13245  0.1  0.7 466340 97804 pts/2    S+   00:20   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13246  2.1  0.8 1318340 100104 pts/2  Sl+  00:20   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     13254  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13256  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13259  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13263  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13267  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13271  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13274  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13276  0.0  0.8 466340 98396 pts/2    S+   00:20   0:00          \_ airflow worker -- LocalExecutorroot     13282  4.1  0.8 466596 99952 pts/2    S    00:20   0:00          \_ airflow scheduler -- DagFileProcessorManager```After (with running tasks):```root@a7c8aa590704:/opt/airflow# ps auxfUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot      6434  0.0  0.0   6652  4584 pts/3    Ss   00:01   0:00 /bin/bashroot     19250  0.0  0.0   9556  3064 pts/3    R+   00:39   0:00  \_ ps auxfroot         1  0.0  0.0   2148   720 ?        Ss   Jun23   0:00 /usr/bin/dumb-init -- /entrypointroot         7  0.0  0.0   6656  4400 pts/0    Ss   Jun23   0:00 /bin/bashroot       121  0.0  0.0   8220  3228 pts/0    S+   Jun23   0:00  \_ tmuxroot       101  0.0  0.0  15856  4272 ?        Ss   Jun23   0:00 /usr/sbin/sshdroot       123  0.0  0.0  10176  5148 ?        Ss   Jun23   0:00 tmuxroot       124  0.0  0.0   6676  4636 pts/1    Ss   Jun23   0:00  \_ -bashroot      1449  0.6  2.6 988356 326312 pts/1   Sl+  Jun23   0:20  |   \_ /usr/local/bin/python /usr/local/bin/airflow webserverroot      1584  0.0  0.4 121068 56864 pts/1    S+   Jun23   0:01  |       \_ gunicorn: master [airflow-webserver]root      1587  0.4  2.5 986144 318712 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1588  0.4  2.5 984776 317672 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1589  0.4  2.5 985848 318600 pts/1   Sl+  Jun23   0:13  |           \_ [ready] gunicorn: worker [airflow-webserver]root      1590  0.4  2.5 985628 318424 pts/1   Sl+  Jun23   0:12  |           \_ [ready] gunicorn: worker [airflow-webserver]root       128  0.0  0.0   6676  4552 pts/2    Ss   Jun23   0:00  \_ -bashroot     19030 17.9  0.9 467108 118628 pts/2   S+   00:38   0:02      \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     19038  0.0  0.7 466084 97776 pts/2    S+   00:38   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     19039  1.4  0.8 1318084 99804 pts/2   Sl+  00:38   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     19047  0.0  0.8 466084 98692 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', '2021-06-24T00:39:06.539715+00:00', '--local', 'root     19240 25.3  0.8 470820 104400 pts/2   S+   00:39   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', '2021-06-24T00:39:06.539715+00:00', '--local', '--poroot     19246  0.0  0.8 470956 103980 pts/2   S    00:39   0:00          |       \_ airflow task runner: example_bash_operator runme_2 2021-06-24T00:39:06.539715+00:00 91root     19049  0.1  0.8 466084 98696 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', '2021-06-24T00:39:06.539715+00:00', '--local', 'root     19241 26.0  0.8 470824 104408 pts/2   S+   00:39   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', '2021-06-24T00:39:06.539715+00:00', '--local', '--poroot     19248  0.0  0.8 470824 103720 pts/2   S    00:39   0:00          |       \_ airflow task runner: example_bash_operator runme_1 2021-06-24T00:39:06.539715+00:00 93root     19052  0.1  0.8 466084 98760 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-06-24T00:39:06.539715+00:00', '--local', 'root     19244 26.0  0.8 470824 104404 pts/2   S+   00:39   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-06-24T00:39:06.539715+00:00', '--local', '--poroot     19245  0.0  0.8 471212 104032 pts/2   S    00:39   0:00          |       \_ airflow task runner: example_bash_operator runme_0 2021-06-24T00:39:06.539715+00:00 90root     19056  0.1  0.8 466084 98760 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', '2021-06-24T00:39:06.539715+00:00', '--loroot     19243 24.6  0.8 470824 104400 pts/2   S+   00:39   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', '2021-06-24T00:39:06.539715+00:00', '--local'root     19247  0.0  0.8 470956 103712 pts/2   S    00:39   0:00          |       \_ airflow task runner: example_bash_operator this_will_skip 2021-06-24T00:39:06.539715+00:00 92root     19057  0.1  0.8 466084 98760 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2021-06-24T00:39:06.539715+00:00', '--locroot     19242 26.6  0.8 470824 104404 pts/2   R+   00:39   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2021-06-24T00:39:06.539715+00:00', '--local',root     19249  0.0  0.8 470824 101976 pts/2   S    00:39   0:00          |       \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2021-06-24T00:39:06.539715+00:00', '--locroot     19062  0.0  0.8 466084 98300 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutorroot     19066  0.0  0.8 466084 98300 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutorroot     19069  0.0  0.8 466084 98300 pts/2    S+   00:38   0:00          \_ airflow worker -- LocalExecutorroot     19075  2.7  0.8 466596 100144 pts/2   S    00:38   0:00          \_ airflow scheduler -- DagFileProcessorManager```Once the worker is done executing a task, the worker is renamed back to `airflow worker -- LocalExecutor`",1
Add support for non-RSA type key for SFTP hook (#16314),1
Reduce log messages for happy path (#16626),2
"Refactor `dag.clear` method (#16086)There were a number of ""internal"" parameters that were to do withgetting the TIs which should not have been exposed via the public API.Additionally this method and `get_task_instances` shared a lot ofsimilar code (albeit the later was simpler) -- they now both use ainternal method to do the actual querying.",1
Chart: Add more clear docs for setting `pod_template_file.yaml` (#16632)The values.yaml didn't have any docs on it.,2
"AWS DataSync cancel task on exception (#11011) (#16589)Small improvements to DataSync operator. Most notable is the ability of the operator to cancel an in progress task execution, eg if the Airflow task times out or is killed. This avoids a zombie issue when the AWS DataSync service can have a zombie task running even if Airflow's task has failed. Also made some small changes to polling values. DataSync is a batch-based uploading service, it takes several minutes to operate so I changed the polling intervals from 5 seconds to 30 seconds and adjusted max_iterations to what I think is a more reasonable default.closes: #11011",4
"Remove SQLAlchemy <1.4 constraint (#16630)This was added due to flask-sqlalchemy and sqlalchemy-utils not declaringthe upper bounds. They have since released sqlalchemy 1.4-compatibleversions, so we can remove that hack.Note that this does *not* actually make us run on sqlalchemy 1.4 sinceflask-appbuilder still has a <1.4 pin. But that's for flask-appbuilderto worry about -- code in Airflow is compatible, so we can remove theconstraint now, and get sqlalchemy 1.4 as soon as flask-appbuilderallows us to.",1
commiting dagPickle session when the airflow tasks run --ship-dag --interactive command is executed FIXES: 15748 (#15890),0
Rename test_cycle to check_cycle (#16617),3
"Fix multiple issues in Microsoft AzureContainerInstancesOperator (#15634)Fix multiple issues in Microsoft AzureContainerInstancesOperator,most importantly run sleep during main loop while executing.",1
Bump Jinja2 upper-bound from 2.12.0 to 4.0.0 (#16595),5
"Move DagFileProcessor and DagFileProcessorProcess out of scheduler_job.py (#16581)This change moves DagFileProcessor and DagFileProcessorProcess out of scheduler_job.py.Also, dag_processing.py was moved out of airflow/utils.",4
Allow ssh connection to breeze container for remote development (#16621) (#16639),1
"Set process title for ``serve-logs`` and ``LocalExecutor`` (#16644)Follow up of https://github.com/apache/airflow/pull/16623.This PR/commits adds title to serve-logs command and multiprocessingmanager for LocalExecutor.The serve-logs process is on celery worker when using CeleryExecutor butfor LocalExecutor, it is a separate process in Scheduler.**Before**:```root       124  0.0  0.0   6676  4636 pts/1    Ss   Jun23   0:00  \_ -bashroot     25299 25.3  2.6 988372 326344 pts/1   Sl+  01:30   0:09  |   \_ /usr/local/bin/python /usr/local/bin/airflow webserverroot     25510  3.6  0.4 121068 57152 pts/1    S+   01:31   0:00  |       \_ gunicorn: master [airflow-webserver]root     25555 35.7  2.5 983584 316564 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25556 35.7  2.5 983840 316684 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25557 35.5  2.5 983840 316548 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25558 37.2  2.5 984920 317700 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root       128  0.0  0.0   6676  4552 pts/2    Ss   Jun23   0:00  \_ -bashroot     25090  5.8  0.9 467508 118808 pts/2   S+   01:30   0:03      \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     25098  0.0  0.7 466080 97800 pts/2    S+   01:30   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     25099  0.4  0.8 1391812 99788 pts/2   Sl+  01:30   0:00          \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     25107  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25109  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25114  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25117  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25120  0.0  0.8 466080 98552 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25125  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', '2021-06-24T01:31:30.507415+00:00', '--loroot     26139  0.0  0.8 468988 102204 pts/2   S+   01:31   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', '2021-06-24T01:31:30.507415+00:00', '--local'root     25128  0.0  0.7 466080 98076 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25132  0.0  0.7 466080 98076 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25137  1.1  0.8 466592 100016 pts/2   S    01:30   0:00          \_ airflow scheduler -- DagFileProcessorManagerroot@a7c8aa590704:/opt/airflow# ps aux```**After**:```root       124  0.0  0.0   6676  4636 pts/1    Ss   Jun23   0:00  \_ -bashroot     25299 25.3  2.6 988372 326344 pts/1   Sl+  01:30   0:09  |   \_ /usr/local/bin/python /usr/local/bin/airflow webserverroot     25510  3.6  0.4 121068 57152 pts/1    S+   01:31   0:00  |       \_ gunicorn: master [airflow-webserver]root     25555 35.7  2.5 983584 316564 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25556 35.7  2.5 983840 316684 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25557 35.5  2.5 983840 316548 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root     25558 37.2  2.5 984920 317700 pts/1   Sl+  01:31   0:08  |           \_ [ready] gunicorn: worker [airflow-webserver]root       128  0.0  0.0   6676  4552 pts/2    Ss   Jun23   0:00  \_ -bashroot     25090  5.8  0.9 467508 118808 pts/2   S+   01:30   0:03      \_ /usr/local/bin/python /usr/local/bin/airflow schedulerroot     25098  0.0  0.7 466080 97800 pts/2    S+   01:30   0:00          \_ airflow serve-logsroot     25099  0.4  0.8 1391812 99788 pts/2   Sl+  01:30   0:00          \_ airflow executor -- LocalExecutorroot     25107  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25109  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25114  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25117  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25120  0.0  0.8 466080 98552 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25125  0.0  0.8 466080 98548 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', '2021-06-24T01:31:30.507415+00:00', '--loroot     26139  0.0  0.8 468988 102204 pts/2   S+   01:31   0:00          |   \_ airflow task supervisor: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', '2021-06-24T01:31:30.507415+00:00', '--local'root     25128  0.0  0.7 466080 98076 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25132  0.0  0.7 466080 98076 pts/2    S+   01:30   0:00          \_ airflow worker -- LocalExecutorroot     25137  1.1  0.8 466592 100016 pts/2   S    01:30   0:00          \_ airflow scheduler -- DagFileProcessorManagerroot@a7c8aa590704:/opt/airflow# ps aux```",2
Add support for managed identity in WASB hook (#16628)* Add support for managed identity in WASB hook* Log info that we're using managed identity credential* Must use managed identity credential here if previous branch is engaged* Add comment that managed identity will be attempted if no other authentication is provided,1
Update airflow/hooks/dbapi.py (#16629),5
Fix failing pylint checks (#16656),0
Add Python 3.9 support (#15515)This includes several things:* added per-provider support for python version. Each provider  can now declare python versions it does not support* excluded ldap core extra from Python 3.9.* skip relevant tests in Python 3.9,3
Add type annotations to setup.py (#16658),1
"Fix permissions for CodeQL workflows (#16660)After limiting permissions, our CodeQL workflow started failing.This is because it needs some extra permissions as explained inthe https://github.com/github/codeql-action/issues/464This PR adds the required permissions.",1
Docs: Fix rendering of ``PYTHONPATH`` values (#16664),0
"Remove duplicated/overlapping tests around render_k8s_pod_yaml (#16642)When making another change here, I noticed that we were basicallytesting the same thing twice in test_taskinstance andtest_renderedtifields, which does no one any good.I have updated the tests to use mocking to avoid duplication, andexercised a few more of the branches in the functions under test",3
Add Changelog updates for 2.1.1 (#16665)* Add Changelog updates for 2.1.1* Fix spelling errors.,0
"Chart: Apply worker's node assigning settings to Pod Template File (#16663)Since we treat Kubernetes Task Pod as worker when using KubernetesExecutor or CeleryKubernetesExecutor, we should allow overriding `.Values.nodeSelector` by `.Values.workers.nodeSelector` for Pod template file too.This is consistent with how we assign `serviceAccountName`, `VolumeMounts` etc",2
Add new committers: ``Jed`` and ``TP`` (#16671)Announcement Email: https://lists.apache.org/thread.html/rae56495ae62fc28cfe1a88e9d28043d78fdbdb611e8a8437bb044ae4%40%3Cdev.airflow.apache.org%3E,5
"Remove upstart from docs (#16672)No modern dist ships upstart anymore (everyone has migrated to systemd,or never used upstart) so these docs have no value anymore.",2
This change adds test for changing different executors in helm chart upgrade (#16394)Co-authored-by: EphraimBuddy <splendidzigy24@gmail.com>,2
Fix spelling issues introduced when adding Tzu (#16673),1
Chart: Fix overriding node assigning settings on Worker Deployment (#16670)`.Values.nodeSelector` should be over-ridable by `.Values.workers.nodeSelector` similar to pod_template_file which I fixed in #16663,0
Update release documentation for elasticsearch (#16662)After adding Python 3.9 we need to release an out-of-bandPython 3.9 compliant elasticsearch release. The only differencefor the provider are changed dependencies to make them workfor Python 3.9. Without it we will not be able to generateconstraints when we release next airflow version.,1
Docs: Added new pipeline example for the tutorial docs (#16548)Added hooks and changed whats next position as per request from #16084,4
Bump ``sphinxcontrib-spelling`` and minor improvements (#16675)- Bump `sphinxcontrib-spelling` from `5.2.1` to `7.2.1`- Excludes `project.rst` and `changelog.rst` from spell-check for `apache-airflow` package so that we don't need to add Committer's Name everytime.- Removes committers name and ``'airfl%'`` from `docs/spelling_wordlist.txt` as it isn't needed. It should be a code-block not an actual word.,5
Bump Airflow version to 2.1.0 in docs (#16677)Some of the docs used 2.0.2 instead of latest one,3
Rearrange ``README.md`` to make it easy for first-time users (#16679)For first time users our README.md is a bit on heavier side in terms of details.This PR/commits re-arranges the section so that the section first-time usersor potential users/lurkers would care about are at the top.,1
"Fix ""Invalid JSON configuration, must be a dict"" (#16648)Wrong variable name used",1
Refactor: added type annotation (#16668),1
Fix TI success confirm page (#16650),5
Add Wise to INTHEWILD.md (#16683),1
AWS Hook - allow IDP HTTP retry (#12639) (#16612),1
Add preparation of images as part of RC preparation process (#16674),1
Fix calculating duration in tree view (#16695)Make sure moment doesn't default the end_date to now and show the wrong duration,0
"prefer consistent casing (#16693)Because SLAs are otherwise capitalized in this docstring, it makes sense to apply that casing consistently.",1
Refactor usage of unneeded function call (#16653)`ts` was created for `timezone.utcnow()` `dag_processor` but looks like it was not used for actual comparison.,1
Removes pylint from our toolchain (#16682)We've agreed during the voting process that Pylint supportshould be disabled: https://lists.apache.org/thread.html/r9e2cc385db8737ec0874ad09872081bd083593ee29e8303e58d21efb%40%3Cdev.airflow.apache.org%3EThis PR:* removes all # pylint comments* removes pylint pre-commits and related scripts/files* removes CI jobs running pylint checks* removes documentation about pylint* removes unnecessary #noga (adds pre-commit for that)* fixes some remaining pydocstyle errors after removing #noqa's,4
Separate out coverage files for different executors (#16689)* Separate out coverage files for different executorsThe coverage files and virtualenvs were not separated outfor different executors - running them in parallell could causethe files override each other and mysterious failures ofthe K8S tests.This PR separates out the coverage files.* Update TESTING.rst,3
Updating task dependencies (#16624),5
bump dnspython (#16698),5
"AIP-39: Handle DAG scheduling with timetables (#15397)This creates a new subpackage airflow.timetables, and implementstimetable constructs that provides DAG scheduling logic. The timetableclasses are used to refactor schedule inference logic out of the DAGclass, and existing functions related to scheduling are refactored touse timetables (and deprecated).Usages of the deprecated DAG functions in Airflow's code base aremodified to either use the timetable, or infer the information by othermeans. For example, usages of previous_schedule() (what was a DAG lastscheduled to run before this run?) are refactored to query the databasewhen the previous scheduled run actually happened, instead of using theschedule interval (cron or timedelta) in infer the information. This isbecause an AIP-39 timetable does not necessarily run on a periodic-ishschedule, and we cannot reliably infer when the previous run happened.",1
Add conn to jinja template context (#16686)Closes #14597,1
"Note that AWS and Astronomer.io have provided funding for the CI machines (#16702)GCP have too, but we aren't currently using it.",1
"Remove duplicated try, there is already a try in create_session (#16701)",1
Fix direct use of cached_property module. (#16710)On Python 3.8 and 3.9 we use the built in functools decorator for this.,1
"Adjust sizes of CI sponsor logos to look more similar (#16707)They have different aspect ratios and different ""percived"" sizes, sosetting height didn't really work -- these manually chosen widths look""about equal to me"".",1
Change default airflow version in Dockerfile (#16714),2
Add ``jedcunningham`` to ``INTHEWILD`` (#16723),1
"Fix ``CeleryKubernetesExecutor`` (#16700)closes https://github.com/apache/airflow/issues/16326Currently when running celery tasks when running with ``CeleryKubernetesExecutor``,we see the following error. This error occurs as the ``BaseJob`` (via ``LocalTaskJob``) tries to needlesslyinstantiate a `KubernetesExecutor` which in turn tries to create a multiprocessing process/Managerwhich fails.```[2021-06-29 00:23:45,301: ERROR/ForkPoolWorker-16] Failed to execute task daemonic processes are not allowed to have children.Traceback (most recent call last):  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/executors/celery_executor.py"", line 116, in _execute_in_fork    args.func(args)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py"", line 48, in command    return func(*args, **kwargs)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py"", line 91, in wrapper    return f(*args, **kwargs)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py"", line 237, in task_run    _run_task_by_selected_method(args, dag, ti)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py"", line 64, in _run_task_by_selected_method    _run_task_by_local_task_job(args, ti)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py"", line 117, in _run_task_by_local_task_job    pool=args.pool,  File ""<string>"", line 4, in __init__  File ""/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/state.py"", line 433, in _initialize_instance    manager.dispatch.init_failure(self, args, kwargs)  File ""/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__    with_traceback=exc_tb,  File ""/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_    raise exception  File ""/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/state.py"", line 430, in _initialize_instance    return manager.original_init(*mixed[1:], **kwargs)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/local_task_job.py"", line 76, in __init__    super().__init__(*args, **kwargs)  File ""<string>"", line 6, in __init__  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py"", line 97, in __init__    self.executor = executor or ExecutorLoader.get_default_executor()  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/executors/executor_loader.py"", line 62, in get_default_executor    cls._default_executor = cls.load_executor(executor_name)  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/executors/executor_loader.py"", line 79, in load_executor    return cls.__load_celery_kubernetes_executor()  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/executors/executor_loader.py"", line 116, in __load_celery_kubernetes_executor    kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/executors/kubernetes_executor.py"", line 421, in __init__    self._manager = multiprocessing.Manager()  File ""/usr/local/lib/python3.6/multiprocessing/context.py"", line 56, in Manager    m.start()  File ""/usr/local/lib/python3.6/multiprocessing/managers.py"", line 513, in start    self._process.start()  File ""/usr/local/lib/python3.6/multiprocessing/process.py"", line 103, in start    'daemonic processes are not allowed to have children'AssertionError: daemonic processes are not allowed to have children```We don't need to instantiate an executor when running ``LocalTaskJob`` as executor isn't used in it.",1
Minor link correction in breeze docs (#16724),2
Adding missing word to welcome message (#16726),1
Chart: fix labels on cleanup serviceaccount (#16722),4
Chart: refactor webserver and flower networkpolicy (#16619)This adds support for overriding ports on the webserver and flowernetworkpolicies. This allows sidecars with webservers in them tofunction when networkpolicy is enabled.This also renamed the existing parameter used to define `from` in the networkpolicies ingress.,1
"Fix unchecked indexing in _build_metrics (#16744)I am not sure if this can happend in regular uses, but when running testcases `sys.argv` can be that args passed to the pytest. When this is thecase it is definently possible for argv to only contain a singleelement.",3
Logging and returning info about query execution SnowflakeHook (#15736),1
fix: instance name env var (#16749),0
Fix timing out tests for public GitHub Runners. (#16750)This PR:* upgrades kind to latest version with security fixes* increases timeouts to account for low-resource GitHub RunnersFixes: #16736,0
"ensure task is skipped if missing sla (#16719)* ensure task is skipped if missing slaThis protects against missing SLAs.For instance, if a task does not have an SLA then it's possible this will result in an error like so:TypeError: unsupported operand type(s) for +: 'DateTime' and 'NoneType'  File ""airflow/jobs/scheduler_job.py"", line 565, in execute_callbacks    self.manage_slas(dagbag.dags.get(request.dag_id))  File ""airflow/utils/session.py"", line 70, in wrapper    return func(*args, session=session, **kwargs)  File ""airflow/jobs/scheduler_job.py"", line 433, in manage_slas    if following_schedule + task.sla < timezone.utcnow():Here we simply skip tasks which do not have SLAs and avoid raising the exception in subsequent logic.",2
Breeze should work with new docker-compose fallback (#16743)The new Docker Desktop beta brings new docker v2 implementationwith docker-compose being a docker command. It also provides fallbackto docker-compose command but adding --log-level messes upthe alias it uses. The --log-level INFO command was superfluousand we can get rid of it.,1
"Only allow webserver to request from the worker log server (#16754)Logs _shouldn't_ contain any sensitive info, but they often do bymistake. As an extra level of protection we shouldn't allow anythingother than the webserver to access the logs.(We can't change the bind IP form 0.0.0.0 as for it to be useful itneeds to be accessed from different hosts -- i.e. the webserver willalmost always be on a different node)",1
"Correctly load openssh-gerenated private keys in SSHHook (#16756)When Paramiko loads an openssh-generated RSA private key it wouldhappily ""parse"" it as valid a DSS key, only to fail at first use.This commit fixes the problem in two ways:1. It re-orders the list to move DSA to the last format to be tried   (which is now not widely used)2. Attempts to ""use"" the key by signing some data, causing it to be   checked early.",1
Remove remaining Pylint disables (#16760)Follow up of https://github.com/apache/airflow/pull/16689 to clean remaining pylint disables,5
Add more CODEOWNERS (#16761)- This PR/commit ads TP to `airflow/timetables/` dir and Jed to Helm Charts,2
Validate type of `priority_weight` during parsing (#16765)closes https://github.com/apache/airflow/issues/16762Without this the scheduler crashes as validation does not happen at DAG Parsing time.,2
Have UI and POST /task_instances_state API endpoint have same behaviour (#16539)Keep the behavior of `post_set_task_instances_state` in the api the same as that of Mark Success/Failed in the UI after #13037.Marking Success/Failed in the UI also clears downstream tasks that are in failed/upstream_failed state. This PR makes the corresponding feature in the api `post_set_task_instances_state` do the same. See comments from @ashb [here](https://github.com/apache/airflow/pull/13037#pullrequestreview-673022834).,1
"Switches to ghcr.io container registry (#16775)After fixing permission problems, we can now switch to ghcr.io",0
"Remove redundant logging in SFTP Hook (#16704)There is no equivalent logging in the store method – and arguably, a user of this hook whowants this sort of operation logging would want more information here such as bytes transferred, transfer rate, etc.",5
"Mask value if the key is ``token`` (#16474)Some connections (including the databricks connection) use the key 'token' in the 'extra' field (this has always been the case). Including it here so that these sensitive tokens are also masked by default.The prior implementation just masked all of the 'extra' json: ""XXXXXXXX"" if conn.extra_dejson else None https://github.com/apache/airflow/blob/88199eefccb4c805f8d6527bab5bf600b397c35e/airflow/hooks/base.py#L78",1
"Fix slow (cleared) tasks being be adopted by Celery worker. (#16718)Celery executor is currently adopting anything that has ever run before and has been cleared since then.**Example of the issue:**We have a DAG that runs over 150 sensor tasks and 50 ETL tasks while having a concurrency of 3 and max_active_runs of 16. This setup is required because we want to divide the resources and we don't want this DAG to take up all the resources. What will happen is that many tasks will be in scheduled for a bit as it can't queue them due to the concurrency of 3. However, because of the current implementations, if these tasks ever run before, they would get adopted by the schedulers executor instance and become stuck forever [without this PR](https://github.com/apache/airflow/pull/16550). However, they should have never been adopted in the first place.**Contents of the PR**:1. Tasks that are in scheduled should never have arrived at an executor. Hence, we remove the task state scheduled from the option to be adopted.2. Given this task instance `external_executor_id`  is quite important in deciding whether it is adopted, we will also reset this when we reset the state of the TaskInstance.",1
Update default image as ``2.1.1`` for Helm Chart (#16785)Change Helm chart default version to `2.1.1`,2
Update ``click`` to 8.x (#16779)Don't see any breaking changes here: https://click.palletsprojects.com/en/8.0.x/upgrading/,4
"Fix release date for 2.1.1 (#16788)Airflow 2.1.1 was released a bit later than we expected, it was released on 2021-07-02",5
Added select_query to the templated fields in RedshiftToS3Operator (#16767)Co-authored-by: Weiping He <weiping.he@cirium.com>,1
SSHHook: Using correct hostname for host_key when using non-default ssh port (#15964),1
Removes coverage from kubernetes tests (#16794)The coverage generated by parallel runs of K8S tests cause oftenfailures of tests because temporary .coverage files were generatedin airflow sources. However the coverage of those tests was actuallywrong - it did not check the coverage of Airflow code (it isrunning inside K8S in scheduler/workers/webserver pods).This is quite a bit complex task captured in #16793 but for now weshould simply disable the coverage for those tests.,3
Remove legacy GitHub Packages (#16776)This PR removes the legacy GitHub Packages support:* removes checking for images in Packages/Registry* removes output informing about the registry* hard-codes registry to ghcr.io* Updaes documentation describing the registries,2
Require approval for merging to ``v2-*-stable`` branches (#16819)Similar to v1-10-stable we should need 1 vote for v2-1-stable most importantly as we will be release 2.1.2 and 2.1.3 in coming days and months,2
Update TaskGroup typing (#16811)* Update TaskGroup typing* Update task_group.py,5
Remove AbstractDagFileProcessorProcess from dag processing (#16816)This change removes AbstractDagFileProcessorProcess from dag processing,2
"BugFix: Correctly handle custom `deps` and `task_group` during DAG Serialization (#16734)We check if the dag changed or not via dag_hash, so we need to correctly handle deps and task_group during DAG serialization to ensure that the generation of dag_hash is stable.closes https://github.com/apache/airflow/issues/16690",0
Chart: Allow using krb5.conf with ``CeleryExecutor`` (#16822),5
"Add 'queued' state to DagRun (#16401)This change adds queued state to DagRun. Newly created DagRunsstart in the queued state, are then moved to the running state aftersatisfying the DAG's max_active_runs. If the Dag doesn't havemax_active_runs, the DagRuns are moved to running state immediatelyClearing a DagRun sets the state to queued stateCloses: #9975, #16366",1
Add Aneesh Joseph as Airflow Committer (#16835)https://lists.apache.org/thread.html/r2995b1c1614aa1f9b0b8b5c1ee27dbb285016b203ae5111147b2d488%40%3Cdev.airflow.apache.org%3E,5
Chart: warn when using default logging with KubernetesExecutor (#16784),2
Add changelog updates for 2.1.2 (#16838),5
"When a task instance fails with exception, log it (#16805)The previous .exception() call looks at sys.exc_info() for the activeexception, but since the failure handler is not in a Python exceptionhandling context, it fails to actually log the exception. This isamended by passing in the exception instance explicitly, which is avalid argument type according to logging's documentation.",2
"Change whimsy url since Airflow is a TLP (#16841)Since Airflow is a Top-level project and graduated from incubation, instead of PPMC the url we should use is different.",1
Clarify the role of commits history (#16840),5
Update AWS Base hook to use refreshable credentials (#16770) (#16771),1
"Add State types for tasks and DAGs (#15285)This adds TaskState and DagState enum types that contain all possible states, makes all other core state constants derive their values from them, and adds a couple of initial type hints that use the new enums (with the plan being that we can add signficantly more later).closes: #9387",1
"Don't check execution_date in refresh_from_db (#16809)The native sqlalchemy DateTime type does not compare well when timezonesdon't match. This can happen if the current execution_date on a DagRuninstance is not in UTC (the db entry is always in UTC).Since DagRun has a unique constraint on (dag_id, run_id), these twoshould be able to return one unique result, and the executrion_datecolumn should not be needed anyway. Let's just remove that filter toprevent all the datetime comparison trouble.",5
Fix impersonation issue with LocalTaskJob (#16852)Running a task with run_as_user fails because PIDs are not matchedcorrectly.This change fixes it by matching the parent process ID (the `sudo`process) of the task instance to the current process ID of the task_runnerprocess when we use impersonationCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Chart: Better comment and example for `podTemplate` (#16859)Add better comment and an example for podTemplate.,1
Add 'queued' to DagRunState (#16854)This change adds 'queued' to DagRunState and improved typing for DagRun stateCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Standardise dataproc location param to region (#16034)* Standardise dataproc location param to regionStandardises DataProc hook & operators `location` parameter to `region` in linewith underlying google DataProc Python client library.* Adding back `location` parameter for backward compability* Fix test* Update airflow/providers/google/CHANGELOG.rstCo-authored-by: Jarek Potiuk <jarek@potiuk.com>,4
Adding: Snowflake Role in snowflake provider hook (#16735)* Adding:1. 'extra__snowflake__role' to get_connection_form_widgets() to enable snowflake role capture.2. 'extra__snowflake__role' to get_ui_field_behaviour() to placeholders to return snowflake role to the UI.3. Updated _get_conn_params() to capture snowflake role from 'extra__snowflake__role'.Co-authored-by: saurasingh <saurabhsingh@dal.ca>,2
Improve graph view refresh (#16696)* Improve graph view refresh- only refresh if state has actually changed- stop refresh if all states are final- swap out `.attr()` to `.prop()` for handling `checked` see https://stackoverflow.com/questions/5874652/prop-vs-attr* check only on final states instead of pending,4
"Enable ""Rebase and merge"" button on PRs. (#16882)The default will still be Squash and Merge, but there are some times(mostly when working on release branches, but a few other cases havecropped up) where we want to accept a PR but keep the commits.Rather than making committers need to use the CLI and having to knowthat you can approve a PR on GitHub (but don't merge it there) and thenmanually push, lets just let people do it in GitHub directly.",7
Updating Airbyte example DAG to use XComArgs (#16867),1
Prevent Redis access directly from host (#16885)* Prevent Redis access directly from host -  alter ports to expose,5
Fix minor typo in configuration.py (#16832),5
Add a calendar field to choose the execution date of the DAG when triggering it (#16141)* Enable choosing the execution date on the Trigger Dag UI* Fix issue with invalid conf* Remove redundant check for the default exec date* Handle condition where execution date is invalid* Add error message for failed to parse execution_date* Switch to exception log levelCo-authored-by: Joao Ponte <jpe@plista.com>,2
Fixing typos in GlacierToGCSOperator documentation (#16899),2
Update changelog with Python 3.9 support.,1
Added template_fields_renderers for MySQL Operator (#16914),1
Fix Minor Bugs in Apache Sqoop Hook and Operator (#16350),1
Allow disable SSL for TableauHook (#16365)* Fixed Tableau connection with ssl mode.* Restored site_id field in tableau hook.* Updated test for provider Tableau Hook to adapt them to the new changes.* Added the name of the parameter in function call for better code readability.* Implemented the possibility to add verify and cert parameters in extra option in Tableau Hook.* Covered connection test with SSL parameter.* Updated documentation for Tableau connection.* Added conversion from string to bool for verify parameter.* Updated Tableau hook test.* Precommit modifications.* Fixed bug in test.* Fixed Tableau docs.Co-authored-by: Michele <mzanchi@tenaris.com>,2
Remove not needed log line (#16925)This got added by mistake in https://github.com/apache/airflow/pull/16289/files#diff-62f7d8a52fefdb8e05d4f040c6d3459b4a56fe46976c24f68843dbaeb5a98487R1354,5
Fix Airflow releasing guide (#16924)Fix Airflow releasing guide with some minor issues,0
"Avoid verification of images multiple times (#16928)The CI and PROD images are verified in CI build, and it takesa minute or so, however they were verified multiple times.Some time ago verification was added to be run then ""wait for images""job was run, but the same script has been run in every test. This isnot needed because this is the exact same image downloaded fromGitHub registry (identified by commit hash), so the extraverificatoins are not needed. This will speed up the builds byfew percents.",1
"Remove cache for kubernetes tests (#16927)Different python versions are used for different tests for k8sso we should not attempt to cache the venv for tests, otherwisethey will randomly fail.",0
"Fixed task instance retrieval in XCom view (#16923)The SQL logical ""and"" was not implemented correctly and hence the filter was returning wrong results",0
"Docs: Better description for `pod_template_file` (#16861)In Airflow 2+, `pod_template_file` is the only way to configure workers,so we can remove the ""other fields"" language.",4
Fix wrong template_fields_renderers for AWS operators (#16820),1
"BugFix: Using `json` string in template_field causes issue with K8s Operators (#16930)closes https://github.com/apache/airflow/issues/16922Because we simply check if fields in template_field ends with `template_ext`,this was causing issues if the str ends with json, in which case Airflow wouldtry to search for file instead of using the string",1
Docs: Suggest use of Env vars instead of Airflow Vars in best practises doc (#16926),2
Fix UPDATING.md (#16933),5
Refactor: Remove processor_factory from DAG processing (#16659)This change removes processor_factory that was passed around a lotbetween different classes and creates the processor at the point of needCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Change the logic of None comparison in model_list template (#16893),2
"Switch Breeze/CI to ghcr.io excusively (#16780)Breeze used traditionally DockerHub to pull images, becausethey were public and GitHub Packages were not. With GitHub ContainerRegisry however, we can switch fully to using GitHub ContainerRegistry also for Breeze.Thanks to moving to Github Container Registry we can removea lot of code responsible for maintaining different namingand different versions of the images in DockerHub andGitHub Container Registry. Also it streamlines and simplifiesthe process of refreshing the images when new python versionsare released - the CI push builds will check if the new Pythonimage is released in DockerHub and it will rebuild the baseimage automatically if needed (and push it as cache)The CI documentation (including sequence diagrams) has beenrefreshed to reflect those changes (and other changes done inthe meantime). The flows are now simplified as DockerHub islargely moved out of the picture.The only remaining DockerHub Images now are:* images used during CI for integrations (airflow-ci)* officially released Production Airflow images (airflow)The integration images will be moved to GitHub Container Registryin a subsequent PR and the only images remaining in DockerHubwill be the officially released Production Airflow images.Part of #16555",2
"Adds warning about using dynamic installation of packages (#16935)While we are supporting installing packages dynamically in ourhelm chart and docker compose while testing, this method isinherently insecure in production environments (it opens up foran attack where removing dependency of a dependency migh bringthe Airflow deployment down).Added explanation about it and explicit warning against this.",2
"Pulls latest images to build images in ""Build Image"" flow (#16948)The recent switching to GHCR.io revealed a problem with misconfiguredGITHUB_REGISTRY_PULL_IMAGE_TAG variable for PROD images.It was trying to pull build image with COMMIT_SHA before it wasbuilt (This was previously hidden by fallback of pulling imagefrom DockerHub and one of the reasons of slower builds of PROD imagesThis PR should fix it.",0
AIRFLOW-5529 Add Apache Drill provider. (#16884),1
Fixed parsing issue of _docker.env file for docker-compose v2 (#16950),2
Add recursive flag to glob in filesystem sensor (#16894)This PR aims to fix #16725 by adding the `recursive` flag to `glob` in the filesystem sensor.closes: #16725,5
Add Talkdesk as a user of airflow 😁 (#16947),1
"Fixes passing variables via docker --env-file command (#16959)The #16950 aimed to fix an incompatibility introduced bybeta version of docker-compose v2 (which is automatically pushedto MacOS users now).The issue is documented inhttps://github.com/docker/compose-cli/issues/1917Unfortunately it has an undesired side-effect that the same filecannot be used to specify list of variables for docker command(the variables come empty).Until the problem is solved, we need to keep two copies of thosevariable files. Not ideal, but hopefully the issue will be solvedsoon and we can go back to original env file in docker-compose v2.",2
"Remove duplicate line in ``UPDATING.md`` (#16955)This line is duplicated, the other contains title and description too.",5
Fixes typo in the name of file for Breeze docker compose env (#16971)There was a typo in #16959,2
"Add CRST - The Transportation Solution, Inc to INTHEWILD.md (#16946)",1
Fixed to check number key from jenkins response (#16963)Co-authored-by: 남상준/데이터솔루션팀 <sangjun.nam@musinsa.com>,0
Update alias for field_mask in Google Memmcache (#16975)The July 12 2021 release of google-memcache library removedfield_mask alias from the library which broke our typecheckingand made google provider unimportable. This PR fixes the importto use the actual import.,2
"Switch back http provider after requests removes LGPL dependency (#16974)Following merging the https://github.com/psf/requests/pull/5797and requests 2.26.0 release without LGPL chardet dependency,we can now bring back http as pre-installed provider as it doesnot bring chardet automatically any more.",1
Prevent running `airflow db init\upgrade` migrations and setup in parallel. (#16311),1
"Move CI-integration images to ghcr.io (#16797)This is the final step of moving the images used for CI integrationto `ghcr.io` from DockerHub. With Publicly available imageswith self-management provided by GitHub, we can finally move tokeep the images ""properly"" - i.e. each image is separate andtag is only image version.Part of #16555",4
Removes duplicated lines from generate constraints instructions (#16984),4
Update chain() and cross_downstream() to support XComArgs (#16732)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Extended template_fields_renderers for MySQL provider (#16987),1
"Errors out instead of trying to workaround buggy docker-compose v2 (#16989)Docker-Compose v2 Beta has an error in processing environmentvariable file which prevents Breeze from running. Until it isfixed, we are going to print an error, explain how to disableit and exit - because the workaround introduces more problemsthan it solves (passing environment variables to containeris broken partially)Also see https://github.com/docker/compose-cli/issues/1917",2
"Updating command to run all tests on the last commit (#16997)The previous command read `./breeze static-check all -- --ref-from HEAD^ --ref-to HEAD`, however the `options` used should be `--from-ref` and `--to-ref`.",1
Fix extras name in ``UPDATING.md`` (#16998)The description didn't match the command. Atlas instead of Azure,5
"Fix release guide when copying artifacts (#17001)When copying artifacts from dev svn repo to release repo:Before:```❯ for f in ${AIRFLOW_DEV_SVN}/$RC/*; doecho ""${$(basename $f)/rc?/}""doneapache-airflow-2.1.2-sou.tar.gzapache-airflow-2.1.2-sou.tar.gz.ascapache-airflow-2.1.2-sou.tar.gz.sha512apache-airflow-2.1.2.tar.gzapache-airflow-2.1.2.tar.gz.ascapache-airflow-2.1.2.tar.gz.sha512apache_airflow-2.1.2-py3-none-any.whlapache_airflow-2.1.2-py3-none-any.whl.ascapache_airflow-2.1.2-py3-none-any.whl.sha512```After:```❯ for f in ${AIRFLOW_DEV_SVN}/$RC/*; doecho ""${$(basename $f)/}""doneapache-airflow-2.1.2-source.tar.gzapache-airflow-2.1.2-source.tar.gz.ascapache-airflow-2.1.2-source.tar.gz.sha512apache-airflow-2.1.2.tar.gzapache-airflow-2.1.2.tar.gz.ascapache-airflow-2.1.2.tar.gz.sha512apache_airflow-2.1.2-py3-none-any.whlapache_airflow-2.1.2-py3-none-any.whl.ascapache_airflow-2.1.2-py3-none-any.whl.sha512```",0
Bump pre-commit hooks (#17000)Just upgrades to latest minor versions of pre-commit hooks,1
Remove extra forward slash in stackdriver address. (#16990)This PR removes the extra forward slash in the stackdriver address in the README.,1
Fix static checks after pre-commit upgrades (#17006)Fixes changes after: https://github.com/apache/airflow/pull/17000,4
Move docs about masking to a new page (#17007)* Move docs about masking to a new page* Update docs/apache-airflow/security/secrets/mask-sensitive-values.rstCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
"Allow attaching to previously launched task in ECSOperator (#16685)This PR adds a parameter reattach_prev_task to ECSOperator.**Before:**Until now we could use 'reattach' which was reattaching a running ECS Task (if there was one running) of the same 'family' instead of creating a new one. The problem was that if we had workflows using the same ECS Task Definition in several tasks, it didn't know which one to reattach and we could only use `concurrency=1` in some pipelines for example (when we launch the same ECS task in parallel from Airflow with different configurations).**Now:**Now with reattach_prev_task instead, when we launch a new ECS task, it will store temporarily the ECS Task ARN in XCOM. If there is an issue during the run (typically connection problem between Airflow and ECS for long-running tasks or Airflow worker restarting which was then still running those tasks in the background without Airflow being aware of it):- self._start_task will store the ECS task ARN in XCOM (in a 'fake' task_id equal to f""{self.task_id}_task_arn""- in the next execution, it will check if this task ARN is still running and if so it will reattach it to the operator, otherwise it will create a new one- when the operator runs succesfully it will delete the XCOM valueI didn't change the logic of 'reattach' to do that directly because I didn't know if it had been designed for other use cases**Update 2021-07-01:**After discussing with @darwinyip  I made the change to 'reattach' directly instead of creating a new flag",1
"Adds option to disable mounting temporary folder in DockerOperator (#16932)* Adds option to disable mounting temporary folder in DockerOperatorThe DockerOperator by default mounts temporary folder to insidethe container in order to allow to store files bigger thandefault size of disk for the container, however this did not workwhen remote Docker engine or Docker-In-Docker solution was used.This worked before the #15843 change, because the /tmp hasbeen ignored, however when we change to ""Mounts"", the ""/tmp""mount fails when using remote docker engine.This PR adds parameter that allows to disable this temporarydirectory mounting (and adds a note that it can be replacedwith mounting existing volumes). Also it prints a warningif the directory cannot be mounted and attempts to re-runsuch failed attempt without mounting the temporarydirectory which brings back backwards-compatible behaviourfor remote engines and docker-in-docker.Fixes: #16803Fixes: #16806",0
Chart: Update the default Airflow version to ``2.1.2`` (#17013)Updates default Airflow version to 2.1.2 as it has been released.,5
Update Airflow version in ``README.md`` (#17009)This commit/PR also updates the release date in CHANGELOG.txt,5
Drop support for Airflow 1.10 in entrypoint_prod.sh and improve MSSQL compatibility (#17011),1
Deprecate Tableau personal token authentication (#16916)* Deprecate Tableau personal token authentication* Fix spelling in documentation* Fix styling issue in TableauHook,1
Prepare documentation for July release of providers. (#17015),1
Update Airflow version in docker stack documentation to 2.1.2 (#17017),2
Fix bug and small improvements in scripts/tools/list-integrations.py (#17004),1
Control Triage Role users via `.asf.yaml` (#17028)We can now add/remove users from Airflow Traige Role group. Docs: https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features#Git.asf.yamlfeatures-AssigningexternalcollaboratorswiththetriageroleonGitHubFor now I have added everyone in https://github.com/orgs/apache/teams/airflow-triage/members and removed some who have already become committers,4
Dev: Bump stale action to v4 (#17025)https://github.com/actions/stale/releases/tag/v4.0.0 -- Looking forward for colored logs,2
"Fixes ""development"" and ""rc"" cross dependencies between providers (#17023)In case we have additional dependencies between providers releasedat the same time (for example we need to release sftp and sshpackages now where sftp package depends on release of sshat the same time) we have to add suffix to the version of theadditional_dependency.PIP does not take into account unfortunately that developmentdependencies should likely be considered as fulfilling therequirement of >=. For example if you have:sftp depends on ssh>=2.1.0 and you release ssh 2.1.0.dev0 atthe same time the ssh>=2.1.0 condition is not fulfilled.Same case will be with rc1. Therefore we need to add the suffix in suchcross-provider dependencies to be able to install them in CIand in rc candidates.In the future we might ask PIP to change behaviour in such case.",4
Fixed wrongly escaped characters in amazon's changelog (#17020)* Fixed documenation generation for July providers.* fixed a problem with documentation generation problem with  html-escaped characters* regenerated the documentation to include the changelog changes.,4
Fix minor issues in `UPDATING.md` (#17026)- Duplicate line `#### `airflow.providers.http.operators.http.SimpleHttpOperator``- Fix install command,0
"Chart: Update description for Helm chart to include 'official' (#17040)Including the word ""official"" should differentiate our chart against others```❯ helm search hub airflowURL                                               CHART VERSIONAPP VERSIONDESCRIPTIONhttps://artifacthub.io/packages/helm/airflow-he...8.4.1        2.1.1      the community Apache Airflow Helm Chart - used ...https://artifacthub.io/packages/helm/apache-air...1.0.0        2.0.2      Helm chart to deploy Apache Airflow, a platform...https://artifacthub.io/packages/helm/bitnami-ak...10.2.4       2.1.1      Apache Airflow is a platform to programmaticall...https://artifacthub.io/packages/helm/bitnami/ai...10.2.5       2.1.2      Apache Airflow is a platform to programmaticall...https://artifacthub.io/packages/helm/larribas/a...1.0.1        1.10.7     [Airflow](https://airflow.apache.org/) + Kubern...https://artifacthub.io/packages/helm/douban/hel...0.2.1        0.2.0      A Helm chart for Douban Helpdesk```",2
Updated clean-logs.sh (#16978),4
Chart: Add instructions to run with Example DAGs (#17043)For quick testing it is easier to show users in Quick Start on how they can run Helm chart with Example DAGs,2
Chart: Remove hard-coded namespace in port-forward instructions (#17042)The port-forward command had a hard-coded namespace.This fixes it and improves formatting - from inline to code-block,1
"Update CONTRIBUTING.rst and CONTRIBUTORS_QUICK_START.rst files with jq installation instructions (#17060)* Add jq installation info as a prerequisite for setting up breeze* Add macOS example for jq installation using HomebrewThe ""Configure Your Environment"" section was missing the examples oninstalling jq on macOS. Earlier version of the document only had theexample for the Ubuntu OS.",2
Update .mailmap (#17059),5
Fixes detection of version 2 of docker-compose (#17062)Docker compose 2 added `v` in front of the version :(,1
[FIX] Docker provider - retry docker in docker (#17061),2
Update INTHEWILD.md (#17058),5
Update INTHEWILD.md (Adding Skai.io) (#17067)* Update INTHEWILD.md* Update INTHEWILD.md,5
Added docs & doc ref's for AWS transfer operators between SFTP & S3 (#16964),1
"Converts the specification of branch for pushes to be flexible (#17065)We already have flexible configuration of branches in CI workflowbut we missed them in build-images.yml - as a result direct pushesto v2-1-test branch are not building the images.This PR brings flexible branch specification to build-imagesworkflow as well so that we will not have to update iteven when we release 2.2, 3.0 etc. branches.",5
Switch test_scheduler_job.py from unittest to pytest style (#17053)This is prep work to let us use pytest fixtures more extensively inthese tests.,3
fix: dataprocpysparkjob project_id as self.project_id (#17075)set project_id as self.project_id from self.hook.project_id,1
Prepares documentation for RC2 release of Docker Provider (#17066),1
Prevent running `airflow db init\upgrade` migrations and setup in parallel. (#17078),1
Improve executor validation in CLI (#17071)* Improve executor validation in CLI* fixup! Improve executor validation in CLI* fixup! fixup! Improve executor validation in CLI,5
Support secret backends/airflow.cfg for celery broker in entrypoint_prod.sh (#17069)* Support secret backends in entrypoint_prod.sh* Update entrypoint_prod.sh,1
removing try-catch block (#17081),1
Fixed template_fields_renderers for Amazon provider (#17087),1
"Fixes UI assets compilation from PROD image built from sources (#17086)The #16577 change removed yarn.lock from installed packagesand it removed the possibility of preparing assets after thepackage is installed - so far that was the way it was done inthe PROD image built from sources. The asset compilationwas supposed to work after the change but it was notperformed in this case.The change fixes it by:* detecting properly if the PROD image is built from sources  (INSTALLATION_METHOD)* compiling the assets from sources, not from package* installing airflow from sources AFTER assets were compiledFixes #16939",0
AirbyteHook - Consider incomplete status (#16965),1
Add insert_args for support transfer replace (#15825),1
#16976 Add json.dumps() for templated fields objects: 'dict' and 'list' (#17082),5
Translate non-ascii characters (#17057),5
Parse template parameters field for MySQL operator (#17080),1
"Chore: Some code cleanup in `airflow/utils/db.py` (#17090)As part of https://github.com/apache/airflow/pull/17078, a separate `filldb` function was created which isn't needed as it isn't used anywhere outof `initdb`",5
Adds oauth libraries to PROD docker image (#17093)The flask OAuth requires oauth libraries and this case isfrequent enough to be added to the reference image.Fixes: #16944,0
Add more typing to airflow.utils.helpers (#15582)* Add more typing to airflow.utils.helpers* Apply suggestions from code reviewCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Update airflow/utils/helpers.py* Update airflow/utils/helpers.pyCo-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Xiaodong DENG <xd.deng.r@gmail.com>,5
Fix static checks after merging #17093 (#17096),0
Fix error in Druid connection attribute retrieval (#17095),0
Adds more explanatory message when SecretsMasker is not configured (#17101)The secrets masker added in 2.1.0 introduced requirement thatat least one SecretsMasker needs to be configured per task.However this introduced problems for several users who migratedfrom Airflow 1.10 and had their custom logging configurationdone without first copying the base airflow configuration.The message about missing SecretsMasker was pretty cryptic for theusers. This PR changes the message to be much more descriptiveand pointing the user to the right place in documentationexplaining how advanced logging configuration should be done.,5
Enable using custom pod launcher in Kubernetes Pod Operator (#16945)* :tada: Initial commit.* :recycle: Refactoring code.* :bulb: Documenting source code.* :bulb: Documenting source code.* :recycle: Refactoring code.* :bug: Fixing a bug.,0
Ensure a DAG is acyclic when running DAG.cli() (#17105)Also contains minor fixes to dag_cycle_tester to correct typeinformation and docstring description to match reality.,2
Add Pytest fixture to create dag and dagrun and use it on local task job tests (#16889)This change adds pytest fixture to create dag and dagrun then use it on local task job testsCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,3
Fixing typo for render_template_as_native_obj (#17114),2
Switch test_backfill_job.py from unittest to pytest style (#17112)Prep work to use pytest fixtures in these tests,3
Correct the :mod: documentation for s3_to_redshift_operator (#17115),1
"Chart: Update postgres subchart to 10.5.3 (#17041)We were on 6.3.12 and the current latest version is 10.5.3.We have dropped support for Helm 2 already so Helm 3 users won't be affected. Secondly this postgres should only used for development, not production.",1
Fix bool conversion Verify parameter in Tableau Hook (#17125),1
Refactored waiting function for Tableau Jobs (#17034)* Implemented new function for get job status and waiting in Tableau Hook and updated Sensor and Operator.* Updated test for Tableau.* Mods from pre-commit scripts.* Fixed docs Tableau code.* Generalized waiting_for_succeeded in wait_for_state in Tableau Hook.* Updated test for new functionality in Tableau Hook.* Restored original format of docs Tableau scripts.* Changed docs for wait_for_state and get_job_status in Tableau Hook.* Modified test in paremetrizated test and added CANCELED case for test_get_job_status.* Changes from pre-commit scripts.* Added case CANCELED for test_wait_for_state in Tableau Hook tests.Co-authored-by: Michele Zanchi <mzanchi@tenaris.com>,3
"Add missing changelog entry for 2.1.2 (#17136)This change was missing from changelog, potentially because of bad commit message ""yarn audit"" (https://github.com/apache/airflow/commit/9b0b0c68f6db940b16037be8dd7c1884ce6f1a87)",5
Adding SalesforceToS3Operator to Amazon Provider (#17094),1
Made S3ToRedshiftOperator transaction safe (#17117),1
ECSOperator / pass context to self.xcom_pull as it was missing (when using reattach) (#17141),1
Simplify `default_args` in Kubernetes example DAGs (#16870),2
"Warn on Webserver when using ``SQLite`` or ``SequentialExecutor`` (#17133)Context: https://lists.apache.org/thread.html/r07e83f2462f08c4a52f6428b771a07e067ce9272b6256dca470a87fc%40%3Cdev.airflow.apache.org%3ENote: Red flashes are for errors, we shouldn't use that.",1
"Doc: Recommend using same configs on all Airflow components (#17146)Some users seem to be using different configs for different components. This doc makes it clear on what we recommend.Context: https://github.com/apache/airflow/pull/16754#issuecomment-884419057>I'm new to Airflow and I haven't seen an explicit part within the docs that says ""all configuration must be shared between airflow components"". In fact, all configuration keys are prefixed with some naming scheme which led me to think that some are common, some are for webserver etc... I personally don't like to have common configuration because changing a single setting requires a change in all components (hence restart) but I get that this is intentional for Airflow. I think this can be better communicated throughout the docs.",2
"Chart: Create a random secret for Webserver's flask secret key (#17142)After https://github.com/apache/airflow/pull/16754 -- it is important that both Webserver and Worker have the same config value for `[webserver] secret_key` or else you will see the following error:```*** Fetching from: https://worker.worker-svc.default.svc.cluster.local:8793/log/<dag>/<task>/2021-07-15T11:51:59.190528+00:00/1.log*** Failed to fetch log file from worker. 403 Client Error: FORBIDDEN for url: https://worker.worker-svc.default.svc.cluster.local:8793/log/<dag>/<task>/2021-07-15T11:51:59.190528+00:00/1.logFor more information check: https://httpstatuses.com/403```This happens because Airflow generates a random value for them if value isn't provided, which causes a random string generated on webserver and worker. Hence they don't match, resulting in the error.This PR creates a K8s Secret object and creates a key for that setting and pass it as Env Var similar to what we do with Fernet Key.",4
Chart Docs: Add single-line description for ``multiNamespaceMode`` (#17147)Adds description for `multiNamespaceMode`,1
"Chart: changelog for 1.1.0, add UPDATING to docs (#17149)",2
Use dag_maker fixture in test_backfill_job.py (#17118)This change uses the dag_maker fixture in testsfixup! Use dag_maker fixture in test_backfill_job.pyfixup! fixup! Use dag_maker fixture in test_backfill_job.py,3
"Doc: Add hyperlinks to Github PRs for Chart Changelog (#17167)This uses same logic as the core ""apache-airflow"" docs to add Hyperlinks",2
Chart docs: note uid write permissions for existing pvc (#17170),2
Chart: Release 1.1.0 of Helm Chart (#17171)Release 1.1.0 of Helm Chart,2
Update Helm Chart release guide (#17173)This PR/commit update the Helm chart release guide with some minor corrections:- Fixes sed command- Adds step to remove old artifacts from dev repo,4
"Avoid logging in to GitHub Container Registry when not in CI (#17169)* Avoid logging in to GitHub Container Registry when not in CIWhen GITHUB_TOKEN was set in environment, attempt to login tohttps://ghcr.io/ was made. But GITHUB_TOKEN is commonly used toauthenticate and if you happened to not have access there theattempt failed.This PR only attempts to login when the`AIRFLOW_LOGIN_TO_GITHUB_REGISTRY` variable is set to `true`and sets the variable in CI.",1
Chart docs: better note for logs existing pvc permissions (#17177),2
Fix bug that log can't be shown when task runs failed (#16768)The log can't be shown normally when the task runs failed. Users can only get useless logs as follows. #13692<pre>*** Log file does not exist: /home/airflow/airflow/logs/dag_id/task_id/2021-06-28T00:00:00+08:00/28.log*** Fetching from: http://:8793/log/dag_id/task_id/2021-06-28T00:00:00+08:00/28.log*** Failed to fetch log file from worker. Unsupported URL protocol </pre>The root cause is that scheduler will overwrite the hostname info into the task_instance table in DB by using blank str in the progress of `_execute_task_callbacks` when tasks into failed.  Webserver can't get the right host of the task from task_instance because the hostname info of  task_instance table is lost in the progress.Co-authored-by: huozhanfeng <huozhanfeng@vipkid.cn>,5
Core: Enable the use of __init_subclass__ in subclasses of BaseOperator (#17027)This fixes a regression in 2.1 where subclasses of BaseOperator could nolonger use `__init_subclass__` to allow class instantiation timecustomization.Related BPO: https://bugs.python.org/issue29581Fixes: https://github.com/apache/airflow/issues/17014,0
"Fix: ``TaskInstance`` does not show ``queued_by_job_id`` & ``external_executor_id`` (#17179)**Problem discovery:**I was debugging a bug with the `external_executor_id` Airflow after which this UI bug caught my eye and I got annoyed by it. I figured to fix this one first so my other testing can go a bit smoother :)**Description of the problem:**Currently there is a BUG inside the Task Instance details (/task) view.It loads the TaskInstance by calling `TI(task, execution_date)` and then uses `refresh_from_db()` to refresh many fields that are no filled in yet.However, the assumption is made in that case that it refreshes all values, which it does not.`external_executor_id` and `queued_by_job_id` are not updated at all and `executor_config` is only instantiated by the original `TI(task, execution_date)` call but also not updated in `refresh_from_db()`.This also shows in the UI where these values are always showing None, while the TaskInstance view shows you these values are not None.**The changes in the PR:**1. Changes to the `update_from_db()` method to include the missing three values.2. A new test that checks we are really updating ALL values in `update_from_db()`3. Removal of an incorrect comment as we do need the `execution_date` for that view.",5
Fix typo in ``GoogleCloudStorageToBigQueryOperator`` (#17159)This PR includes a couple of fixes in the deprecation message of the `GoogleCloudStorageToBigQueryOperator` module and the `GCSToBigQueryOperator` test. closes: #17155,3
"Client-side filter dag dependencies (#16253)Problem: Many dags have no dependencies, adding a lot of noise to the Dag Dependencies viewSolution: Default to only showing dags with a dependency graph",2
API endpoint to create new user (#16609)* API endpoint to create new user* Implement PATCH to modify a user* Implement DELETE user endpoint* exist() doesn't seem to work on MSSQL,1
"Webserver: Unpause DAG on manual trigger (#16569)Problem: sometimes we forget to unpause a dag when we manually trigger a run and get frustrated when it doesn't runSolution: By default, unpause a paused dag when manually triggering. It will be an extra field in ""Trigger w/ config"" that a user can opt out of",1
[CASSANDRA-16814] Fix cassandra to gcs type inconsistency. (#17183),0
Remove turbaszek from CODEOWNERS (#17189)Due to limited time capacity I would like to reduce number of reviews I get so I can realy help.,1
Updating Apache example DAGs to use XComArgs (#16869),1
Create virtualenv via python call (#17156),1
"Only allows supported field types to be used in custom connections (#17194)* Only allows supported field types to be used in custom connectionsOnly four field types are supported in Connection Forms:String, Password, Integer, Boolean.Previously when custom connections tried to use other fieldtype, ConnectionForm behaved in a very strange way - theconnection form reloaded quickly hiding the actual errorand no error was printed making it next to impossible to figureout the root cause of the problem.With this change, non-supported field types generate Warningand the Connections that use them are not added to the list ofsupported connections.Fixes: #17193",0
Add dunnhumby to INTHEWILD.md (#17204)Co-authored-by: Deepak Kumar <deepak.kumar1@dunnhumby.com>,1
Adding custom Salesforce connection type + SalesforceToS3Operator updates (#17162),5
"Do not seek error file when it is closed (#17187)We do not check if error file is closed before we seek it, which causes exceptions.Sometimes, this error file does not exist e.g when the task state is changed externally.This change fixes it by returning None when the file is closed so that custom text can be used for error.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
Adding EdgeModifier support for chain() (#17099),1
Include exit code in AirflowException str when BashOperator fails. (#17151),0
Improve documentation and examples in example_asana.py (#15959),2
Fix GCStoGCS operator with replace diabled and existing destination object (#16991),1
Google Ads Hook: Support newer versions of the google-ads library (#17160),1
Remove locks for upgrades in mssql (#17213)Closes: #17088,4
Fixes statich check failures (#17218)Fixes static check failures after #17160 was merged with somefailing status of main.,0
"Fixes several failing tests after broken main (#17222)Several problems slipped through after recent broken main eventend they were merged with failing tests. This PR fixes it as wellas brings correct approach for ""initialize-local-virtualenv""to get constraints from sources rather than PyPI constraints,which makes it easier to synchronize with latest versions ofdepenendencies.",3
Quarantine TestSchedulerJob.test_scheduler_verify_pool_full (#17225)The test fails occasionally as noted in #17224,0
Quarantine test_verify_integrity_if_dag_changed (#17227)The test fails occasionally as noted in #17226,0
"Do not fail-fast kubernetes tests (#17228)When any of the k8s tests fails, all others were cancelled.This is not a good idea when we have transient errors because thefailure might be intermitted and we might want to merge changeeven if one of the K8S tests fail.",0
Remove/refactor default_args pattern for Microsoft example DAGs (#16873),2
US Bank added as Airflow User  (#17215),1
Adds mssql version parameter to Breeze (#17234),2
fix string encoding when using xcom / json (#13536)Co-authored-by: Alessio Montuoro <alessio@montuoro.info>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
Updating miscellaneous Google example DAGs to use XComArgs (#16876),1
Updating Google Cloud example DAGs to use XComArgs (#16875),1
Updating Jenkins example DAGs to use XComArgs (#16874),1
Updating Docker example DAGs to use XComArgs (#16871),1
Remove/refactor default_args pattern for miscellaneous providers (#16872),1
Updating Amazon-AWS example DAGs to use XComArgs (#16868),1
"Be verbose about failure to import airflow_local_settings (#17195)* Be verbose about failure to import airflow_local_settingsCurrently, if the module exists, but has errors (for example syntaxerror, or transitive import of module that does not exist),the airflow scheduler will not show any error.A case of `airflow_local_settings.py` importing a modulethat does not exist, will also throw `ModuleNotFoundError`,but it should not be silently ignored.",0
Prepares docs for Rc2 release of July providers (#17116),1
"Update spark_kubernetes.py (#17237)** new PR in correct branch as response to https://github.com/apache/airflow/pull/16956 **Perhttps://github.com/kubernetes-client/python/blob/master/kubernetes/docs/CustomObjectsApi.md#create_namespaced_custom_objectthe body passed to create_custom_object must be a JSON string, not a filepath.  Templating will take care of the latter, but JSON string is also a valid entry",1
"Chart: Bump version to ``1.2.0-rc1`` (#17245)Since we have released 1.1.0 of the Chart, we will start working towards 1.2.0-rc1",1
Doc: Update Helm Chart 1.1.0 Release Date (#17244)We released it on 26th July 2021 instead of 25th,5
Fix Helm chart release guide (#17242)Before:```❯ for f in ../../../airflow-dev/helm-chart/$RC/*; do svn cp $f ${$(basename $f)/rc?/}; doneA         airflow-1.1.0.tgzA         airflow-1.1.0.tgz.ascA         airflow-1.1.0.tgz.provA         airflow-1.1.0.tgz.sha512A         airflow-chart-1.1.0-sou.tar.gzA         airflow-chart-1.1.0-sou.tar.gz.ascA         airflow-chart-1.1.0-sou.tar.gz.sha512A         index.yaml```After:```❯ for f in ../../../airflow-dev/helm-chart/$RC/*; do svn cp $f ${$(basename $f)/}; doneA         airflow-1.1.0.tgzA         airflow-1.1.0.tgz.ascA         airflow-1.1.0.tgz.provA         airflow-1.1.0.tgz.sha512A         airflow-chart-1.1.0-source.tar.gzA         airflow-chart-1.1.0-source.tar.gz.ascA         airflow-chart-1.1.0-source.tar.gz.sha512```,2
Improve postgres provider logging (#17214)* Remove duplicated logging of sql* Add logging for copy expert sql,2
Dont use TaskInstance in CeleryExecutor.trigger_tasks (#16248)* Dont use TaskInstance in CeleryExecutor.trigger_tasks* fixup! Merge branch 'main' into celery-multiprocessing-2,7
Remove support for Airflow 1.10 cmds in entrypoint_prod.sh (#17248),1
Fix static checks (#17256)We merged #17237 with too long a line.,7
"Prepares release for Salesforce provider (#17272)By mistake, Salesforce provider has been marked as doc-only where ithad new connection type added. This PR updates Salesforce docs preparingit for separate release.",2
Chart: Add instructions to Update Helm Repo before upgrade (#17282)This adds some docs to make this more visible. Also updates the step to remove old artifacts after a day or keep current + last artifact in release svn repo.,4
"Fix task retries when they receive sigkill and have retries and properly handle sigterm (#16301)Currently, tasks are not retried when they receive SIGKILL or SIGTERM even if the task has retry. This change fixes itand added test for both SIGTERM and SIGKILL so we don't experience regressionAlso, SIGTERM sets the task as failed and raises AirflowException which heartbeat sometimes see as externally set to failand not call failure_callbacks. This commit also fixes this by calling handle_task_exit when a task gets SIGTERMCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
"Fix running tasks with default_impersonation config (#17229)When default_impersonation is set in the configuration, airflow fails to run task due to PID mismatch between the recorded PID and the current PID This change fixes it by checking if task_runner.run_as_user is True and use the same way we check when ti.run_as_user is true to check the PID",1
"Stop attempting to pull base python image when pulling commit hash (#17231)When we publish latest images, we also publish Python base imagesfor them, so that we know where the base images are taken from.This is not happening when we build ""per-build"" images - we onlypublish the resulting images rather than base python images becausewe do not need the base python images. This change was implementedafter ghcr.io move and it was not reflected in --github-idswitch handling, so ./breeze command with --github-id specified,failed trying to pull base python image with the same ID.This PR makes sure that we only pull base python image when webuild/pull latest images, not the per-build ones.",3
"Fix references link in Providers docs (#17286)fixes issue mentioned in https://apache-airflow.slack.com/archives/C0146STM600/p1627475111055600Corresponding Airflow site change: https://github.com/apache/airflow-site/pull/458If the docs are published 'for production' (i.e. to publish on stable doc site) then we should use ""stable"" for Airflow version instead of ""latest""",3
Fix typo in webserver.rst (#17288)Porting https://github.com/apache/airflow-site/pull/454 to Airflow docs. Added @pumpkiny9120 as co-authorCo-authored-by: Yanan Valencia <pumpkiny9120@gmail.com>,1
"Show serialization exceptions in DAG parsing log (#17277)Make sure that any exceptions that happen when writing serialized DAGsto the db get written to the DAG parsing log, instead of only being addedto `import_errors` for consumption via the UI.",2
Update warning about MariaDB and multiple schedulers (#17287),5
Quarantine TestSchedulerJob.test_retry_still_in_executor (#17292)The test fails occasionally as noted in #17291,0
"Do not use constraints when preparing venv for k8s tests on CI (#17290)When k8s virtualenv is prepared to run k8s tests we are usingconstraints, this however might lead to a problem when we increaseminimum version of an affected dependency and it conflicts withthe constraints stored in main.Therefore in case we run tests in CI (which is indicated byspecific pull tag that we use) we do not use constraints forinstalling the kubernetes venv. It should be fine, as we arepretty much running this only as a vehicle to run tests.",3
"Uses current sources when running k8s tests (#17289)There was a bug in our CI - Kubernetes tests were executed runninglates ghcr.io image, rather than the image built from sources ofthe current PR.This PR fixes it by correctly using the PR commit as tag of theimage used as base image for kubernetes tests.",3
"Updates to FlaskAppBuilder 3.3.2+ (#17208)There are some clarifications about using the authenticationvia FlaskAppBuilder - the change implements minimum version of theFAB to 3.3.2 and clarifies the dependencies used in FAB 3 seriesto be only authlib rather than flask-oauth.Fixes: #16944 (this is the second, proper fix this time).",0
Doc: Strip unnecessary arguments from MariaDB JIRA URL (#17296)This was included in https://github.com/apache/airflow/pull/17287 but we can strip other args,5
"Doc: Fix a broken link in an ssh-related warning message (#17294)Warning Message:```    #####################################################    #  WARNING: You should set dags.gitSync.knownHosts  #    #####################################################    You are using ssh authentication for your gitsync repo, however you currently have SSH known_hosts verification disabled,    making you susceptible to man-in-the-middle attacks!    Information on how to set knownHosts can be found here:    https://airflow.apache.org/docs/helm-chart/latest/production-guide.html#knownhosts```broken: https://airflow.apache.org/docs/helm-chart/latest/production-guide.html#knownhostsfixed: https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#knownhostsCo-authored-by: Matt Rixman <MatrixManAtYrService@users.noreply.github.com>",1
"Fix breeze kind-cluster deploy failing with ECONREFUSED (#17293)Currently, kind-cluster deploy fails occasionally due to yarn install when compilingassets. This PR fixes it by using the recommended option --network-concurrency=1 whenrunning yarn install",1
"More optimized lazy-loading of provider information (#17304)With this change we truly lazy-load hooks and external_links onlywhen we need them. Previously they were loaded when any of theproperties of ProvidersManager was used, but with this changein some scenarios where only extra links are used or when weonly need list of providers, but we do not need details onwhich custom hooks are needed, there will be muchfaster initialization. This is mainly for some CLI commands(for example `airlfow providers list` is much faster now), butalso in some scenarios where for example .get_conn() is neverused in Tasks, tasks might also never need to import/load the hooksand they might perform faster, with smaller memory footprint.",1
deprecate dummy trigger rule infavor of always (#17144),5
Fix docs link for using SQLite as Metadata DB (#17308)Identified the issue in: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1627558105383900The page should have been https://airflow.apache.org/docs/apache-airflow/2.1.2/howto/set-up-database.html#setting-up-a-sqlite-database and not https://airflow.apache.org/docs/apache-airflow/2.1.2/howto/set-up-database.rst#setting-up-a-sqlite-database,5
"Grammar and clarity pass on documentation (#17318)Minor grammar edits, fixes to broken links, and rewording for clarification.There are a few changes that others may disagree with me about:- Changed ""outwith"" to ""instead of""- All non-code references I found to ""time-zone"" or ""timezone"" changed to ""time zone""- It seems like top level pages are supposed to have capitalized words other than articles and prepositions, but two pages were not following this convention. I have changed them to conform to the others.- I found a sentence in the health checks section extremely confusing. I took my best attempt to restate it clearly, but I'm not sure I understood it well enough to restate it correctly.",5
"Fix race condition with dagrun callbacks (#16741)Instead of immediately sending callbacks to be processed, wait untilafter we commit so the dagrun.end_date is guaranteed to be there whenthe callback runs.",1
Remove DAG refresh buttons (#17263)Now that the DAG parser syncs DAG specific permissions there reallyisn't a need to manually refresh DAGs via the UI.,2
docs: fix inconsistencies in configuration docs (#17317),2
Fix CLI 'kubernetes cleanup-pods' which fails on invalid label key (#17298)Fix for #16013 - CLI 'kubernetes cleanup-pods' fails on invalid label key,0
GCP Secret Manager error handling for missing credentials (#17264),0
Fix typo in build_images (#17327),2
Fix `airflow celery stop` to accept the pid file. (#17278),2
Fix isort (#17330),0
Fix failing celery test (#17337)This change fixes failing test due to mocking,3
"Fix failing test on MSSQL (#17334)This change fixes the failing test on MSSQL by disposing and creating a new connectionso as to use the new connection to make queriesAlso, some other flaky tests were resolved",0
Added 2RP Net name in INTHEWILD file (#17345),2
Added print statements for clarity in provider yaml checks (#17322),1
Handle connection parameters added to Extra and custom fields (#17269),1
Fix link (#17351),2
Fix typo in AUTOMATICALLY GENERATED marker (#17335),2
"Introduce RESTARTING state (#16681)closes: #16680This PR makes sure that when a user clears a running task, the task does not fail. Instead it is killed and retried gracefully.This is done by introducing a new State called RESTARTING. As the name suggests, a TaskInstance is set to this state when it's cleared while running. Most of the places handles RESTARTING the same way SHUTDOWN is handled, except in TaskInstance.is_eligible_to_retry, where it is always be treated as eligible for retry.",1
Update best-practices.rst (#17357),5
"Add Accenture to the INTHEWILD file (#17358)Accenture is a global consulting firm that helps client with projectsnot limited to digital transformation. Thus, also has people working onAirflow to build data pipelines for its clients.",5
"Moves SchedulerJob initialization to within daemon context (#17157)In Scheduler, the SchedulerJob was instantiated before demon context wasactivated. SchedulerJob is a database ORM object from SQL Alchemy and itopens the connection to Postgres:When you activate daemon context, what happens under the hood is forkingthe process, and while some of the opened sockets were passed to theforks (stdin and stderr but also the opened log file handle), theestablished socket for DB connection was not passed.As the result, when scheduler was started with --daemonize flagthe error `SSL SYSCALL error: Socket operation on non-socket` wasraised.The PR moves SchedulerJob initialization to within the contextwhich makes the connection to Postgres initialized after theprocess has been forked and daemonized.Fixes: #17120",0
error early if virtualenv is missing (#15788),0
Switch to 'smbprotocol' library (#17273),5
"Add Vodafone to the list of organisations using Airflow (#17359)Vodafone uses Airflow to orchestrate data pipelines in the on-premises.This commit adds Vodafone to the INTHEWILD.md file, as one of theorganisations that use Airflow",1
Quarantine test_mark_success_on_success_callback (#17364)The test is quarantined and recorded in #17363,3
Fixes #16972 - Slugify role session name in AWS base hook (#17210),1
Update hasicorp-vault.rst to describe use of config (#17313)This page shows use of Hashicorp Vault to use for Connections and Variables. But we can also use it for storing some of the attributes from Airflow configuration file. There is no concrete example for the latter use-case in the documentation.,2
Provide information about IRSA on EKS (#17283),5
Update instructions to signify commands as breeze environment commands (#17366)The earlier instructions indirectly indicate that the commands that havebeen modified by this commit will be executed outside the breezeenvironment. Update the instruction to be more clear and also indicatethat these will be executed as airflow commands within the breeze CIenvironment.,5
Fixed SlackAPIFileOperator to upload file and file content (#17247),2
Fix timestamp test (#17365),3
"Improve `dag_maker` fixture (#17324)This PR improves the dag_maker fixture to enable creation of dagrun, dag and dag_model separatelyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
make platform version as independent parameter of ECSOperator (#17281),1
Adds compile_assets to INSTALL (#17377),1
Add autoscaling subcluster support and remove defaults (#17033),4
Fix messed-up changelog in 3 providers (#17380)Some last minute chnages cause 3 providers to haveslightly messed-up changelogs.,4
Enable specifying dictionary paths in `template_fields_renderers` (#17321)Added the handling of paths in `template_fields_renderers` which enables information contained in dictionaries to be unpacked and rendered appropriately.,5
Enhancement to bash scripts (#17098),5
Example DAG-related updates for Apache Drill (#17384),5
refactor: fixed type annotation for 'sql' param in PostgresOperator (#17331),1
Suggest to use secrets backend for variable when it contains sensitive data (#17319),5
Improve AWS SQS Sensor (#16880) (#16904),1
New job update (#17386)Co-authored-by: Bas Harenslak <bas@astronomer.io>,5
Fix quarantined/flaky tests in test_local_task_job.py (#17385)This PR attempts to fix some flaky/quarantined tests in test_local_task_job.pyby removing assert not process.is_alive() in the tests and making sure process.joinis called with timeout,5
Remove /dagrun/create and disable edit form generated by F.A.B (#17376),2
Regression on pid reset to allow task start after heartbeat (#17333)Regression on PID reset to allow task start after heartbeatCo-authored-by: Nicolas MEHRAEIN <nicolas.mehraein@adevinta.com>,1
More touches on dag_maker fixture to create DagModel automatically (#17391)This change improves the dag_maker fixture to create DagModel on exit,2
refactor: fixed type annotation for 'sql' in MySqlOperator (#17388),1
doc: fixed docstring for sql param in Neo4jOperator (#17407),1
"Proper warning message when recorded PID is different from current PID (#17411)Currently, when the recorded PID is different from the current PID, inthe case of run_as_user, the warning is not clear because ti.pid is usedas the recorded PID instead of parent process of ti.pid. In this case,users would see that the PIDs are the same but there was a warning thatthey are not the sameThis change fixes it.",0
"Support DAGS folder being in different location on scheduler and runners (#16860)There has been some vestigial support for this concept in Airflow for awhile (all the CLI command already turn the literal `DAGS_FOLDER` in tothe real value of the DAGS folder when loading dags), but sometimearound 1.10.1-1.10.3 it got fully broken and the scheduler only everpassed full paths to DAG files.This PR brings back this behaviour",2
Improve diagnostics message when users have secret_key misconfigured (#17410)* Improve diagnostics message when users have secret_key misconfiguredRecently fixed log open-access vulnerability have causedquite a lot of questions and issues from the affected users whodid not have webserver/secret_key configured for their workers(effectively leading to random value for those keys for workers)This PR explicitly explains the possible reason for the problem andencourages the user to configure their webserver's secret_keyin both - workers and webserver.Related to: #17251 and a number of similar slack discussions.,1
"Add timeout when asking whether to rebuild image (#17412)This PR adds timeout to answer the question, whether to rebuildimage when `breeze` is invoked or when pre-commit is run.This reflects the typical use cases where rebuild is mostly notneeded, only in case of some tests which require new dependenciesto be included.User has still 4 seconds to answer Y and have the images rebuiltand just the presence of the question will be enough to get theuser trigger it from time to time.",1
Optimize context sent for docker build (#17415)The `provider_packages` folder is not needed during docker buildand it often contains copied sources when building the providerpackages. Also _doctree folder is prepared during documentationbuilding. This makes `docker build` wait for a few seconds whenthose directories have a lot of files.Excluding those decreases docker build overhead significantly.,2
"Do not pull CI image for ownership fixing on first, fresh breeze run (#17419)When you run Breeze on fresh machine, this script pulled the CIimage before any operation. It is not harmful in most cases butit unnecessarily delays the first real image check and rebuild,where fixing ownership is not really needed (as we've never runBreeze before).",1
Increases timeout for helm chart builds (#17417),2
"Improve image building documentation for new users (#17409)* Improve image building documentation for new usersThis PR improves documentation for building images of airflow,specifically targetting users who do not have big experience withbuilding the images. It shows examples on how custom image buildingcan be easily used to upgrade provider packages as well as howimage building can be easily integrated in quick-start usingdocker-compose.",2
"Optimizes structure of the Dockerfiles and use latest tools (#17418)* Remove CONTINUE_ON_PIP_CHECK_FAILURE parameterThis parameter was useful when upgrading new dependencies,however it is going to be replaced with better approach in theupcoming image convention change.* Optimizes structure of the Dockerfiles and use latest toolsThis PR optimizes the structure of Dockerfile by moving someexpensive operations before the COPY sources so thatrebuilding image when only few sources change is much faster.At the same time, we upgrade PIP and HELM chart used to latestversions and clean-up some parameter inconsistencies.",2
Fix failing static checks in main (#17424),0
Update emr.rst (#17420),5
Fixes some static check errors (#17432)Disabling pre-commit even for a while and usin GitHub toedit .rst files can have disastrous consequences for the static checks.This one fixes my own mistakes from 3 PRs (!) after being far toohasty in adding/merging them.Will do better next time.,1
Updating INTHEWILD.md (PEXA) (#17427),5
Fix Google Cloud Operators docs (#17440),2
Quarantine test_process_sigterm_works_with_retries and test_task_sigkill_works_with_retries in TestLocalTaskJob (#17441)These tests are flaky and fail sometimes,0
"Switches to ""/"" convention in ghcr.io images (#17356)We are using ghcr.io as image cache for our CI builds and Breezeand it seems ghcr.io is being ""rebuilt"" while running.We had been using ""airflow-.."" image convention before,bacause multiple nesting levels of images were not supported,however we experienced errors recently with pushing 2.1 images(https://issues.apache.org/jira/browse/INFRA-22124) and duringinvestigation it turned out, that it is possible now to use ""/""in the name of the image, and while it still does not introducemultiple nesting levels and folder structure, the UI of GitHubtreats it like that and if you have image which starts wiht""airflow/"", the airflow prefix is stripped out and you can alsohave even more ""/"" in then name to introduce further hierarchy.Since we have to change image naming convention due to (stillunresolved) bug with no permission to push the v2-1-test imagewe've decided to change naming convention for all our cacheimages to follow this - now available - ""/"" connvention to makeit better structured and easier to manage/understand.Some more optimisations are implemented - Python, prod-build andci-manifest images are only pushed when ""latest"" image is prepared.They are not needed for the COMMIT builds because we only needfinal images for those builds. This simplified the code quitea bit.The push of cache image in CI is done in one job for bothCI and PROD images and the image is rebuilt again withlatest constraints, to account for the latest constraintsbut to make sure that UPGRADE_TO_NEWER_DEPENDENCIESis not set during the build (which invalidates the cachefor next non-upgrade builds)Backwards-compatibility was implemented to allow PRs that havenot been upgraded to continue building after this one is merged,also a workaround has been implemented to make this changeto work even if it is not merged yet to main.This ""legacy"" mode will be removed in ~week when everybody rebaseon top of main.Documentation is updated reflecting those changes.",4
Handle and log exceptions raised during task callback (#17347)Add missing exception handling in success/retry/failure callbacks,0
"Fix type annotations in OracleOperator,  JdbcOperator, SqliteOperator (#17406)",1
"Adding JWT, IP filtering, and direct session login support for SalesforceHook (#17399)Adding other auth type inputs to SalesforceHook",1
Add missing permissions to varimport (#17468),2
Add ephraimbuddy to INTHEWILD (#17472)Adding myself to INTHEWILD,1
"KEDA task count query should ignore k8s queue (#17433)CeleryKubernetesExecutor lets us use both celery and kubernetes executors.KEDA lets us scale down to zero when there are no celery tasks running.If we have no celery tasks running, and we run a k8s task, then KEDA willlaunch a worker even though there are still no celery tasks.  We can preventthis from happening by ignoring the kubernetes queue in the KEDA query.",1
Disable Helm tests when branch is not main (#17457)We are preparing Helm chart from main branch only and we neverrun it from airflow version branches (similarly as providers)This change disables Helm Chart tests in case default branchis different than main.,3
Add Datatonic to INTHEWILD (#17483),5
Remove trigger-dag-run with configuration from quarantine (#16818)This test caused missing failure that manifested in 2.1.1 inthe #16810,0
"Make schema in DBApiHook private (#17423)There was a change in #16521 that introduced schema field inDBApiHook, but unfortunately using it in provider Hooks derivingfrom DBApiHook is backwards incompatible for Airflow 2.1 and below.This caused Postgres 2.1.0 release backwards incompatibility andfailures for Airflow 2.1.0.Since the change is small and most of DBApi-derived hooks alreadyset the schema field on their own, the best approach is tomake the schema field private for the DBApiHook and make a changein Postgres Hook to store the schema in the same way as all otheroperators.Fixes: #17422",0
[AIRFLOW-17200] Add Alibaba Cloud OSS support (#17201),1
"Attempt to reduce flakiness of PythonVirtualeEnv test_airflow_context (#17486)We have a global limit (60 seconds) for individual test execution,however 'test_airflow_context' of the Python Virtualenv test might take longer incase they are run in parallel - because they are using dill serializationincluding a lot of serializable data from the context of the task.We give the test 120 seconds to complete now.",3
"Install providers from sources in prod image only on main (#17458)When we build production image during test in main, we installproviders from current sources, to make sure that all the testsincluding Helm Chart/Kubernetes tests are using latest sources forproviders.However, when we build the prod image during v* branches, wewant to build the production image using latest released providersinstead, because this will be the way it will be built shortly whenwe release it. We do not run providers test not helm chart tests inthis branch so it is more important to build the image in the way itwill be built for releases - we run verification then and installdependencies in the very same way it will be done during release.",1
"Ask for provider versions in bug reports (#17480)We get a lot of bug reports for providers, and it's rare that theversions being used are in the initial report. Let's ask for them.",5
Update INTHEWILD.md (#17491)Update in the wild with our organization.,5
Add Microsoft PSRP provider (#17361),1
add deprecation notice for SubDagOperator (#17488),2
Use `dag_maker` fixture in models/test_taskinstance.py (#17425)The change applies dag_maker fixture in test_taskinstance.py,3
Improve breeze resource check (#17492)The resource check in breeze was slow (3 docker commands insteadof one) and it used an extra image which needed to be downloaded.The new check uses already available airflow CI image and itperforms all check in one docker command - thus is a lot fasterand it also checks the image at the same time.,2
Use `dag_maker` in tests/core/test_core.py (#17462)This PR applies dag_maker to tests in test_core.py module,3
New generic tableau operator: TableauOperator  (#16915),1
"Better diagnostics and self-healing of docker-compose (#17484)There are several ways people might get the quick-startdocker-compose running messed up (especially on linux):1) they do not run initialization steps and run docker-compose-up2) they do not run docker-compose-init firstAlso on MacOS/Windows default memory/disk settings are notenough to run Airflow via docker-compose and people are reporting""Airflow not working"" where they simply do not allocate enoughresources.Finally the docker compose does not support all versions of airflowand various problems might occur when you use thisdocker compose with old version of airflow.This change adds the following improvements:* automated check of minimum version of airflow supported* mkdir -p in the directories creation in instructions* automated checking if AIRFLOW_UID has been set (and printing  error and instruction link in case it is not)* prints warning about too-low memory, cpu, disk allocation  and instruction link where to read about it* automated fixing of ownership of the directories created in  case they were not created initially and ended up owned by  root user",1
Rearrange Dag parsing tests out of test_scheduler_job.py (#17504)Dag parsing code is not entirely isolated in to airflow/dag_processing/so it makes sense to move the tests to match -- nothing intest_scheduler_job should be dealing directly with DAG files anymore.,2
Use pytest.param in pytest.mark.parametrized (#17505),3
Remove superfulous chars from resource warning (#17503),2
"Add back missing permissions to UserModelView controls. (#17431)Currently, on user model views except from UserDBModelView, view controls don't show up on /users/list. This fixes that issue by adding the missing views back to all user model views.Additionally, Edit User on the different Show User views all redirect to the logged in user's profile views. This fixes that issue by removing the Edit User view from Show User.closes: #16202",1
Use `dag_maker` fixture in test_scheduler_job.py (#17265)This change adds dag_maker fixture to test_scheduler_job.py. Thiswill help us create dagruns each time we create a dag,2
Add Match.com to list of companies using Apache Airflow (#17518)Added Match.com to list of companies using Apache Airflow,1
"Hard-remove parallell locks at cleanup (#17514)Some stale parallel locks might be a reason for hanging testsThis is an attempt to hard-remove all the .parallel remnantdirectories in hope that it will avoid some of the hangs.Unfortunately in our approach we try to reuse runners betweenruns which might cause some remnants from the previous runs toimpact subsequent runs. When we move to GCP we will try to spinoff a new runner for every job to make sure the real ""true cleanstate"" is used when running a job.",1
Remove hard-coded container name for trino (#17525)This had no negative effect because we only run trino duringintegration tests but it could actually prevent two integrationtests run in parallel on the same machine if we choose otherwiseand it will look better in logs:```NAMEairflow-integration-mysql_pinot_1airflow-integration-mysql_statsd-exporter_1airflow-integration-mysql_redis_1airflow-integration-mysql_mysql_1airflow-integration-mysql_cassandra_1airflow-integration-mysql_grafana_1airflow-integration-mysql_openldap_1trinoairflow-integration-mysql_kdc-server-example-com_1airflow-integration-mysql_mongo_1airflow-integration-mysql_rabbitmq_1airflow-integration-mysql_airflow_run_908822c9c10f```,2
"Decreases cassandra memory requirement during Integration tests (#17524)Seems that Cassandra was the main reason why a lot of memorywas used during our integration tests. This has been overlookedbefore but seems that cassandra will always take as much as 25%of available memory by default and it meant for the GitHub publicrunners it was ~2 GB. Also for the self-hosted runners, even ifthey had a lot of memory, cassandra could eat up big chunk of thememory.With the settings added in PR Cassandra will take ~750 MB of memorytotal.",1
Add XCom.clear so it's hookable in custom XCom backend (#17405)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Doc: Add FAQ to speed up parsing with tons of dag files (#17519)This feature was added in https://github.com/apache/airflow/pull/16075. This PR adds it to docs to avoid situations like https://github.com/apache/airflow/issues/17437closes https://github.com/apache/airflow/issues/17437,0
Update pre-commit hooks (#17530)black 21.6b0 -> 21.7b0.pyupgrade v2.21.0 -> v2.23.3.yamllint v1.26.1 -> v1.26.2.isort: 5.9.2 -> 5.9.3.,1
"Fixed broken json_client (#17529)The json_client depends on httpx and thus the response doesn't containthe attribute 'ok'. In consequence, the missing attribute has been changedto is_error.closes: #17527Co-authored-by: Szymon Wojciechowski <szymon.wojciechowski@here.com>",0
"Ignores exception raised during closing SSH connection (#17528)Sometimes, when the connection gets closed after shutdown, a racecondition causes ""bad socket"" error to be thrown. Actually thereis nothing wrong that can happen when closing a closed connectionso we should ignore any exceptions from close command here.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",0
Minor README fixes (#17532)- Markdown was not formatted correctly in the in-line code-block- Just adds single quotes around pip install as the square bracket errors out on some terminals,0
Fixing ParamValidationError when executing load_file in Glue hooks/operators (#16012)* Fixing ParamValidationError when executing load_file in Glue hooks/operatorsCo-authored-by: Rahul Raina <raina_rahul@singaporeair.com.sg>,1
"docs(celery): reworded, add actual multiple queues example (#17541)I was reading the documentation to learn how to define multiple queues, but the example is incomplete, so I had to experiment on the CLI to see what works:This failed: `airflow celery worker -q spark, park`  This works: `airflow celery worker -q spark,quark`This PR updates the documentation to clarify the command line syntax.",2
"Add date format filters to Jinja environment (#17451)On its own this doesn't add all that much, but this is preparatorywork to be combined with the new data_interval_start template contextvariables we are adding, without having to add the ds/ts/no-dash etcpermutations of all of them.",1
"Skip PV's lost+found dir when cleaning logs (k8s) (#17547)When using an RWX PV for sharing logs between pods, some storage providers create a lost+found directory. This directory is only accessible by root; therefore, the log-groomer container will fail with the execution of the find command and enter a CrashLoopBackOff state and fail to do its work.To avoid this error, this change skips the lost+found directory.",4
Fix spelling in ``CeleryExecutor`` docs (#17553),2
"Don't cache Google Secret Manager client (#17539)Caching the Google Secret Manager client doesn't work in all cases, socreate it fresh each time.",1
"AIP-40: Add Deferrable ""Async"" Tasks (#15389)This is the implementation of AIP-40, Deferrable ""Async"" Tasks (https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177050929).The main changes are:- A new concept of a Trigger is introduced, as a small piece of asyncio code that can fire off events  - There is a BaseTrigger and some time-related triggers under a new `airflow.triggers` package  - There is a new Trigger database model and associated `trigger` table  - `Async` versions of the various date/time sensors have been added which defer rather than poke.- There is a new persistent process (Job) called `triggerer`  - It only runs on Python 3.7+  - It handles polling the database for which triggers need running, running them, and re-scheduling task instances whose triggers have fired events  - If a trigger throws an exception or exits without firing an event, it logs why and marks dependent task instances as failed  - It monitors the asyncio event loop with a watchdog task and alerts the user if anything is overrunning (i.e. not using `await`) and blocking the loop.  - It is designed to run in parallel with itself in a highly-available manner, and also has built-in consistent-hash based partitioning (sharding) support- Task Instances have a new `deferred` state which indicates they are waiting on a trigger to run  - The trigger they are waiting for is stored in a new `trigger_id` column, and a failure timeout is in a `trigger_timeout` column  - The scheduler takes care of timing out task instances into the `failed` state  - Deferral is triggered by raising the `TaskDeferred` exception, or calling `self.defer` on the TaskInstance which does the same thing.  - A `next_method` and `next_kwargs` column are added to specify what a task instance/operator's execution entry point should be if it's not the default of `execute()`. They are currently only used by deferral, but have been written to be independent in case they are useful elsewhere.- Two new dependencies are added  - `jump-consistent-hash` is a small MIT licensed library that implements a fast, consistent hash algorithm  - `pytest-asyncio` is an Apache 2 licensed library that enables async tests to be written easilyChanges that are deliberately not in here and will be in a future PR for them specifically: - UI warning when the triggerer is not running and you have deferred task instances - Updating Breeze to include `triggerer` in what it runs - Updating the Docker Compose files to include `triggerer` - Updating the Helm Chart to include `triggerer` - Some way of detecting/preventing DB access within triggers",5
Chart: Allow podTemplate to be templated (#17560),1
Use dag_maker fixture in test_processor.py (#17506)This change applies dag_maker fixture in test_process.pyfixup! Use dag_maker fixture in test_processor.pyfixup! fixup! Use dag_maker fixture in test_processor.py,3
Fix Invalid log order in ElasticsearchTaskHandler (#17551),0
Path correction in docs for airflow core (#17567)Airflow core constraint path was wrong.,0
AIP-39: DagRun.data_interval_start|end (#16352)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Quarantine test_mark_success_no_kill test (#17580)This test is flaky.Logging it in: #17579,2
Add Mongo projections to hook and transfer (#17379),1
"Chart: Support ``extraContainers`` in k8s workers (#17562)This allows extraContainers to be provided for k8s workers, however onemust keep in mind they are responsible for signaling any sidecars toexit.",1
Doc: Fix docstrings for ``MongoToS3Operator`` (#17588)The doc was missing closing parentheses and had markdown formatted in-line code-block instead of rst one,2
"Make `pandas` an optional core dependency (#17575)We only use `pandas` in `DbApiHook.get_pandas_df`. Not all users use it, pluswhile `pandas` now supports many pre-compiled packages it still can take forever whereit needs to be compiled.So for first-time users this can be a turn off. If pandas is already installed thiswill work fine, but if not users have an option to run `pip install apache-airflow[pandas]`closes #12500",1
"Fixed long delays in the Temporal trigger (#17564)This trigger was never actually working for delays over two hours, as itwas comparing a Pendulum.Period to a datetime.datetime, in a way thatalways returned True.This commit fixes the behaviour. This unfortunately seems un-testable,as it's not possible to interrupt asyncio's sleep() while it'sin-flight.",3
Document overriding ``XCom.clear`` for data lifecycle management (#17589)This was added in https://github.com/apache/airflow/pull/17405 but was not documented,2
"fix: filter condition of TaskInstance does not work #17535 (#17548)Make sure that after clicking the All Instances button in the Task Instance panel, the results will be filtered by dag_id and task_idcloses https://github.com/apache/airflow/issues/17535",0
"Rescue if a DagRun's DAG was removed from db (#17544)Fix #17442.The exception happens when a DAG is removed from the database (via web UI or something else), but there are still unfinished runs associated to it. This catches the scenario and use the existing fallback setting `max_active_runs` to zero.",1
"Run more than just basic checks on ``.pre-commit-config.yaml`` change (#17590)https://github.com/apache/airflow/pull/17530 only run basic checks and skipped mypy, black etc which can cause failing main branch.",0
"Simplify 404 page (#17501)Currently, for some reason, the 404 page renders an animation of a bunch of circles. This is complicated to render and doesn't seem necessary. Instead, we should just show basic html/css.Closes #10549",2
Bump `mysql-connector-python` to latest version (#17596)mysql-connector-python 8.0.26 is available and there is no reason ti limit the minor and patch versions for it.,3
"Remove  redundant ``numpy`` dependency (#17594)Missed removing ``numpy`` from `setup.cfg` in https://github.com/apache/airflow/pull/17575. It was only added in setup.cfg in https://github.com/apache/airflow/pull/15209/files#diff-380c6a8ebbbce17d55d50ef17d3cf906numpy already has `python_requires` metadata: https://github.com/numpy/numpy/blob/v1.20.3/setup.py#L473so we don't need to set `numpy<1.20;python_version<""3.7""`",1
Remove redundant note on HTTP Provider (#17595)Since https://github.com/apache/airflow/pull/16974 we have switched back to including HTTP provider and this comment now is not correct.,1
Add more ``project_urls`` for PyPI (#17598)This adds more of our urls for display on PyPI,1
"Use built-in ``cached_property`` on Python 3.8 for Asana provider (#17597)Functionality is the same, this just removes one dep for Py 3.8+",4
Remove upper-limit on ``tenacity`` (#17593)The latest version of tenacity (8.0.1) looks compatible with Airflow.,3
"Improve validation of Group id (#17578)When Group id of task group is used to prefix task id, it shouldfollow the same limitation that task_id has, plus it should nothave '.'. The '.' is used to separate groups in task idso it should not be allowed in the group id.If this is not checked at Task Group creation time, users willget messages about invalid task id during deserializationand it's not entirely obvoius where the error came fromand it crashes the scheduler..Also this validation will be performed at parsing time, ratherthan at deserialization time and the DAG will not even getserialized, so it will not crash the scheduler.Fixes: #17568",0
"Fix MySQL database character set instruction (#17603)The currently instructed character set `utf8mb4 COLLATE utf8mb4_unicode_ci;` does not work on mysql 8. When I do: `airflow db init` the following error occurs:`sqlalchemy.exc.OperationalError: (MySQLdb._exceptions.OperationalError) (1071, 'Specified key was too long; max key length is 3072 bytes')`Changing to this character set: `utf8 COLLATE utf8_general_ci;` solved the problem",0
"Simplify bug report template (#17561)* Simplify bug report templateBy looking at several tens of issues recently I think I got a bitbetter understanding on what people are actually reporting andwhich information is really important - and I think the reporttemplate might be vastly shortened, leaving only the importantparts:* removed some comments that were repetitve and a bit too polite  (strenghtening the necessity of filling all details* remove kernel. It's useless and default uname -a produced  some superfluous artifacts (Github recognising that as issues)* comment stressing that Airflow version/OS is mandatory* deployment is now a separate entry detailing all options",1
Fix ``triggerer`` query where limit is not supported in some MySQL version (#17601)This PR fixes the triggerrer query where limit is not supported in some DB versions and also fixed the issue where total_hours was used on a timedelta.,1
Chart: Fix elasticsearch-secret template port default function (#17428),1
"Add cherry-pick notes to the release README (#17610)This adds some more detail to the release README about the process ofselecting what is in a release, including the fact that the releasethe manager has some discretion about what to include based on severity.",1
Update documentation regarding Python 3.9 support (#17611)https://github.com/apache/airflow#requirements,1
Add new LocalFilesystemToS3Operator under Amazon provider (#17168) (#17382),1
"Fix redacting secrets in context exceptions. (#17618)* Fix redacting secrets in context exceptions.Secret masking did not work in implicit andexplicit context exceptions (seehttps://www.python.org/dev/peps/pep-3134/)When there was a `try/except/raise` sequence,or `raise ... from` exception - the originalexceptions were not redacted.Related: #17604",1
Use gunicorn to serve logs generated by worker (#17591),1
Move worker_log_server_port option to the logging section (#17621),2
"Switch Alibaba OSS tests to us-east-1 (#17616)The tests were using cn-hengzou before which - due to thelimitations in cross-border communication with China behavederratically and caused a number of transient errors.Since GitHub Actions Public runners and our Self-hosted runnersrun in US-east (Azure/AWS), switching to us-east-1 should addressthe stability of tests.Ideally we should switch to mocking, but that might be doneseparately.",3
Fix spelling of Airflow (#17624),0
Improve cross-links to operators and hooks references (#17622)* Improve cross-links to operators and hooks references* fixup! Improve cross-links to operators and hooks references* Update docs/apache-airflow/concepts/operators.rst,2
"Add Triggerer warning banner in UI (#17565)This adds a ""triggerer is not running"" banner in the Web UI, similar to the one that shows when the scheduler is not running.As well as checking for a TriggererJob, though, it also checks to make sure there are pending triggers - this ensures Airflow users who do not use Deferred Operators are not affected.",1
"Force emulation on ARM platforms for Docker (#17627)The ARM platform (mostly Apple M1) does not yet have full supportin Airflow/Breeze. We will soon start building multi-platformimages, but until this happens, we should force AMD platform,so that M1 MacOS will use emulation of Intel platform.",1
Enhanced configure_environment.sh declared readonly varaible (#17619)Co-authored-by: Shraman Basyal <shraman@shramans-mbp.lan>,5
"Do not delete running DAG from the UI (#17630)When the DAG appear again in the UI and we rerun it, say we have catchup set to True,those running task instances that were not deleted would be rerun and an external state changeof the task instances would be detected by the LocalTaskJob thereby sending SIGTERM to the task runnerThis change resolves this by making sure that DAGs are not deleted when the task instances are stillrunning",1
Use new Sphinx Autodoc mock import path (#17634),2
Open relative extra links in place (#17477),2
"Add root to tree refresh url (#17633)We were not passing the root to the `/tree_data` api call. Therefore, filtering upstream of a task would be reset during auto-refresh even though root was still defined.",1
Fixed SlackAPIFileOperator to upload file and file content. (#17400)* [14168] * [14168] Fixed Lint errors* [14168] Updated documentation for Slack API File Operator* [14168] Fixed indendation in SlackAPIFileOperator docs* Update airflow/providers/slack/operators/slack.pyFixed indentation.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Fixed docs comment in Slack.py* Fixed flake8* Fixed bug with SlackAPIFileOperator with uploading file.* Added example DAG link for slack* Fix doc error* Fixed example DAG for slack and the comments* Updated slack operator tests* Fixed slack file tests* Fixed tests* Fixed PR review comments* Fixed file variable in example_slack.pyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
"Make output from users cli command more consistent (#17642)The users cli command wasn't consistent with how it represented the userbeing acted upon. Sometimes you'd see the username, other times thefirst/last name, and yet other times the passed --username (which could beNone if you used --email). Now we will be consistent by always using username.This also simplifies some code paths and improves test coverage.",3
"Adds secrets backend/logging/auth information to provider yaml (#17625)This is preparatory work to automatically generate summary ofsecret backends, logging handlers, API auth backendsfor all providers.It adds logging/secrets-backends/auth-backends sections in theprovider.yaml and refactors the code a little to extract commoninitialization routines to a decorator as well as common sanitycheck that imports classes/modules and checks if communityproviders are using correct package prefix.The provider manager is also simplified by removing theunnecessary `_add` methods.",1
Fix wrong query on running tis (#17631)Fix wrong query on my PR about deleting running dags #17630Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Only run 2.2.0 providers commands for Airflow >= 2.2.0 (#17647)We test providers also for older versions of Airflow and not allolder version of airflow have the same set of provider commandsavailable.With this change the new provider commands are only tested for Airflowversions that supports them.,1
"Forces rebuilding the image for cache pushing (#17635)Fixes bug in pushing latest image to cache on ""push/schedule"".When the build is successful and passes all tests vi either`push' or 'schedule' events, we attempt to rebuild the imagewith latest constraints just pushed and push it as a freshcache for Github Registry. This keeps the time to build imagesmall without manually refreshing the cache, it also automaticallychecks if there is a new ""python"" base image available so thatwe can use it in the new cache.There was a bug that the image has not been FORCE_PULLED andrebuilt in this case - just latest images were used.This had so far no negative effects because due to testinstability, latest main images pretty much never succeeded inall tests, so the images in `main` were refreshed manuallyperiodically anyway. However for v2-1-test the scope of testsrun is far smaller now (no Helm tests, no Provider tests)and they succeed mostly when they should.Also PROD image was built without "".dev0"" suffix whichalso failed.This PR fixes it so that the images are built properly and pushed.",0
"Speed up tests that use BackfillJob (#17648)Calling `heartbeat` was putting in a sleep in which isn'tnecessary/useful in tests, where we want it to run as quick as possible.The sleep has been kept in ""normal"" mode as otherwise the status output(`[backfill progress] | finished run %s of %s |` etc.) will beessentially spammed, rather than only being printed every few seconds.This makes the tests/jobs/ run in about 20s (vs 120s without thechange.)",4
Chart: use serviceaccount template for log reader rolebinding (#17645),2
Add Changelog updates for 2.1.3 (#17644)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
"Update pre-commit checks-flynt to 0.66 (#17672)Additionally, we now download flynt configurations from the official repository, which allows us to automatically download updates using the pre-commit autoupdate command",5
Fix link to generating constraints in BREEZE.rst (#17670),2
"Dev: Remove duplicate step to push Docker Image (#17674)We have the same step few lines below ""## Prepare production Docker Image""",2
Update docs on syncing forks (#17675)* Update docs on syncing forkscloses https://github.com/apache/airflow/issues/17665,0
"Have the dag_maker fixture (optionally) give SerializedDAGs (#17577)All but one test in test_scheduler_job.py wants to operate on serializeddags, so it makes sense to have this be done in the dag_maker for us, tomake each test ""smaller"".",3
Avoid endless redirect loop when user has no roles (#17613),1
Remove the use of multiprocessing in TestLocalTaskJob and Improve Tests (#17581)This PR removes the use of multiprocessing in TestLocalTaskJob and improvesthe test to be more reliableCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,3
Docs: Make ``DAG.is_active`` read-only in API (#17667)Add readOnly=True property on DAG.is_activecloses: #17639,2
"Renames main workflow to `Tests` (#17650)This is a long-overdue change for CI workflows. Since we are buildingimages in a separate workflow, the `CI Builds` name of the workflowwas - first of all misleading, and secondly - too long. The workflownames displayed in the GitHub UI contains the workflow name as prefixso having as short as possible name is an advantage.The `Tests` names seems to be appropriate because this is in factwhat we do in this workflow.The change updates the name of workflow as well as documentationthat referred to it and fixes a few inconsistencies found innames of the `Build Image` -> `Build Images` workflow.The sequence diagrams showing the CI workflow have been alsoregenerated with the new name (thanks to mermaid it was super-easy)",1
Remove legacy image convention (#17692)The image convention has been changed recently and we kept it fora while to allow PRs to run without rebasing. More than a weekhappened since and we can remove the legacy option now.Follow up after #17356,4
Chart: fix running with uid 0 (#17688),1
Fix sqlite hook - insert and replace functions (#17695),1
Replace execution_date with run_id in airflow tasks run command (#16666)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,1
hdfs provider: allow SSL webhdfs connections (#17637),1
Add steps for building release package without breeze. (#17702)* Add steps for building release package without breeze.* Update dev/README_RELEASE_AIRFLOW.mdCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,5
docs(impersonation): update note so avoid misintrepretation (#17701),5
Add ``Scribd`` to INTHEWILD.md (#17685)* Update INTHEWILD.mdAdd Scribd* Update INTHEWILD.md* use single-bracket style - makes more sense that way,1
docs(dagowner): describe dag owner more carefully (#17699)Describe dag `owner` more carefullyCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>,2
Minor doc formatting fix (#17704)Just a minor doc formatting fix to remove space,4
Clearly document no breaking change for 2.1.2 and 2.1.3 (#17706)Similar to https://github.com/apache/airflow/pull/6240,4
"Sync DB Migrations with Airflow 2.1.3 (#17703)Since https://github.com/apache/airflow/pull/16401 was backported to Airflow 2.1.3, the order of DB migrations needs to be changed in the `main` branch as the other PRs with DB migrations weren't ported and would be released in 2.2.This PR sync the migrations to allow ugprade from 2.1.3 to 2.2",1
Adding support for multiple task-ids in the external task sensor (#17339)* Adding support for multiple task-ids in the external task sensor* Fixing flake8 errors* Fixing import errors,0
Add `v2-*-stable` to codecov branches (#17707)Remove `v1-*` branches and adds `v2-*` branches,1
"Add a note about XCOMs push condition for K8sPodOperator docs (#17690)My team was thinking like XCOMs can be used for both cases no matter how tasks are completed you just have to write to the file `/airflow/xcom/return.json`.  Which is not an accurate understanding. I do not aware of how it is done for other operators, at least I think this note can warn users about this restriction.",1
Add optional SQL parameters in ``RedshiftToS3Operator`` (#17640)Adding optional parameters to render the SQL unload query with. Inspired by https://github.com/apache/airflow/blob/main/airflow/providers/postgres/operators/postgres.py,1
Doc: Replace deprecated param from docstrings (#17709)Follow-up of https://github.com/apache/airflow/pull/16267 . Docstrings should show new values instead of deprecated. (`max_active_tasks` instead of concurency),1
"Implemented Basic EKS Integration (#16571)* Implemented Basic EKS Integration* Remove explicit region defaultingcr https://code.amazon.com/reviews/CR-52973030* Refactor the token generation and remove the AWS CLI dependency.* move kubeconfig generator into `hooks/eks.py`* EKS List hooks return all results* Use a tempfile to store kubeconfig data* Move kube config into Hook class as a contextmanager* Removed random traits in tests* Rework the eks.rst doc file* Implemented Jinja templates for operators- Added jinja template fields- Refactored fields to snake_case since they are now exposed- Removed a couple of straggling pylint instructions; pylint is no longer used* conn_id refactor- Refactored using IDE magic: - any field named `self.conn_id` is now `self.aws_conn_id` - any param named `conn_id` is now `aws_conn_id` - any constant named `CONN_ID` is now `DEFAULT_CONN_ID`- Sensors were missing template fields, added those.* Remove try/log blocks from hooks* Implemented EKS system tests* Remove List and Describe Operators and supporting code.* Fixed `nextToken` final result bug* Remove some nesting and some unnecessary logging before raising an exception* Use the force* Improved docs and samples* Additional jinja templating* Corrected some misused Optionals.* Doc formatting fix* Corrected logo* Corrected logo - background and size",2
Add error check for config_file parameter in GKEStartPodOperator (#17700),1
Use `dag_maker` fixture in some test files under tests/models (#17556)This PR uses dag_maker fixtures in tests under tests/modelsCo-authored-by: Ash Berlin-Taylor <ash@apache.org>,3
Doc: Fix replacing Airflow version for Docker stack (#17711)currently it shows up as:```apache/airflow:|version| - the versioned Airflow image with default Python version (3.6 currently)```Example: http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/docker-stack/index.html or even https://airflow.apache.org/docs/docker-stack/index.htmlThis commit fixes it,0
Doc: Update Upgrade to 2 docs with Airflow 1.10.x EOL dates (#17710)This PR improves the Upgrading to 2 docs and add dates when 1.10.x reached EOL and clarifies few things like Python 3.9 support etc.,1
"Add support for configs, secrets, networks and replicas for DockerSwarmOperator (#17474)",2
Improve description of Python compatibility approach (#17721)The entry was terribly out-dated. We did not have manyproblems with different python versions backwards compatibilty fora looong time. Seems that the new SenVer approach and releasecontrol and cadence work pretty well comparing to 3.5/3.6 times(BTW. good job Python developers!) so I updated the entry toreflect the `recommended` approach we should take.,1
Fix instructions of release to get proper path in shasum file (#17716),2
"Import Hooks lazily individually in providers manager (#17682)This change implements lazy loading of individual hooks for providersmanager. First the hooks list is discovered by the manager whenhooks are accessed, but the hooks are not immediatelyimported - the hooks initially keep just a callable that willbe used to retrieve the hook when first accessed.Besides listing details of all hooks, all Hooks are only imported when wewant to retrieve the list of available field behaviours and widgets(which only happens in webserver and should happen anyway whenever oneof those are needed because they are all collectively used in theconnection view).In the case when hooks are accessed in tasks(connection.get_hook()) only the individual Hooks areimported when accessed.The chand deprecates 'hook-class-names' json-schema and replaces itwith 'connection-types' because we need to know connection-typefor each HookClass name declaratively so that we can utilseit in connection.get_hook() mehtod (otherwise we do not knowwhich Hooks conrrespond to which connection type without importingthem.The change is backwards compatible. It adds deprecationwarnings in case providers use the 'hook-class-names' propertyonly and log warnings in case it provides both `hook-class-names`and `connection-types` but there are inconsistencies betweenthose.Part of this change is also to fix some inconsistencies foundwhen all hooks were added to connection-types arrays, whichmake potetntially backwards-incompatible changes to Google Providerwhere some hooks were useing `google_cloud_default` name fordefault_connection_type, but they were in fact using different,specialized Hook.",1
"Do not let create_dagrun overwrite explicit run_id (#17728)Previous DAG.create_dagrun() has an weird behavior that when *all* ofrun_id, execution_date, and run_type are provided, the function wouldignore the run_id argument and overwrite it by auto-generating a run_idwith DagRun.generate_run_id(). This fix the logic to respect theexplicit run_id value.I don't think any of the ""Airflow proper"" code would be affected bythis, but the dag_maker fixture used in the test suite needs to betweaked a bit to continue working.",1
"Rename ``task_concurrency`` to ``max_active_tis_per_dag`` (#17708)Follow-up of https://github.com/apache/airflow/pull/16267Renames `task_concurrency` to `max_active_tis_per_dag`Some of Airflow's concurrency settings have been a source of confusion for a lot of users (including me), for example:https://stackoverflow.com/questions/56370720/how-to-control-the-parallelism-or-concurrency-of-an-airflow-installationhttps://stackoverflow.com/questions/38200666/airflow-parallelismThis PR is an attempt to make the settings easier to understand",1
"Extend init_containers defined in pod_override (#17537)Needs to extend also the init_containers not just override (e.g. git sync and similar init_containers will stay)With that change if you define a pod_override and specifies the ""init_containers"" attribute it will extend the base init_containers with the new ones instead of overwriting them. That is to keep the dag git sync init_containers or other pre-defined init_containers based on the template or the configuration.",5
Added table to view providers in Airflow ui under admin tab (#15385)Added page in Airflow UI for provider package info,5
"Make sure ""podcution-readiness` of docker-compose is well explained (#17731)Many Airflow users are using the docker-compose `quick-start` asproduction-ready solution despite notes in the compose file.This PR adds better warning and a link that we can give thsoeusers so tha they can understand that in order to use docker-composethey need to become really docker-compose experts if they want torun their own docker-compose production-ready installation and thatthey should not expect that this docker-compose will be good fortheir needs (and that they will have support from community basedon the fact tha community publishes the docker-compose)",2
"Remove ``[core] store_dag_code`` & use DB to get Dag Code (#16342)While DAG Serialization is a strict requirement since Airflow 2, we allowed users to controlwhere the Webserver looked for when showing the **Code View**.If `[core] store_dag_code` was set to `True`, the Scheduler stored the code in the DAG file in theDB (in `dag_code` table) as plain string. And the webserver just read it from the same table.If the value was set to `False`, the webserver read it from the DAG file.While this setting made sense for Airflow < 2, it caused some confusion to some users where they thoughtthis setting controlled DAG Serialization.From Airflow 2.2, Airflow will only look for DB when a user clicks on **Code View** for a DAG.",2
Fix clean-logs after improper skipping of lost+found (#17739)The fisxin #17547 was not working properly because -prune and-delete in find cannot be combined. The -prune is still neededto solve the problem of non-readable `lost+found` directory thatfailed find but it should be run in connection to xargs thatshould delete the found files.Fixes #17733,2
Fix comment in Helm Chart for worker ``logGroomerSidecar`` (#17742)It said `scheduler` instead of `worker`,1
Rename ``none_failed_or_skipped`` by ``none_failed_min_one_success`` trigger rule (#17683)closes #17012Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,0
"Add logical_date to OpenAPI DAGRun schema (#17122)The idea is to make *both* logical_date and execution_date getserialized when a DAGRun is returned, but prefer logical_datefrom the user input (and fall back to execution_date when only itis provided).",1
"Remove Marshmallow schema warnings (#17753)Marshmallow 3.0 (pulled in by FAB) issues warnings on the old way wewere doing this, and it's possible the `min=n` validation was neverworking :)",1
Separate infer_data_interval for data interval timetables (#17755)CronDataIntervalTimetable and DeltaDataIntervalTimetable needdifferent infer_data_interval implementations because the 'align'method aligns the time *forward*. CronDataIntervalTimetable needs tocall get_prev one more time than DeltaDataIntervalTimetable to get thecorrect interval.,1
Chart: Add loadBalancerSourceRanges in webserver and flower services (#17666),5
Add links to provider's documentation (#17736)The provider package names in the UI are now linked to thedocumentation of the provider (in the exact version provider isinstalled in!).Follow up after #15385,1
Bump `pip` version to `21.2.4` (#17746)Updates `pip` version from `21.2.2` to `21.2.4`,5
"Ensure ``DateTimeTrigger`` receives a datetime object (#17747)While using the following example DAG, the task failed with `moment` does not have `tzinfo` attribute. This happened because a string was passed from `DateTimeSensor` to `DateTimeTrigger`. This PR ensures that a datetime object is passed and fixed logic in `DateTimeSensor` too so that `self.target_time` is always a datetime object.```pythonfrom datetime import timedeltafrom airflow import DAGfrom airflow.sensors.date_time import DateTimeSensorAsyncfrom airflow.utils import dates, timezonewith DAG(    dag_id='example_date_time_async_operator',    schedule_interval='0 0 * * *',    start_date=dates.days_ago(2),    dagrun_timeout=timedelta(minutes=60),    tags=['example', 'example2', 'async'],) as dag:    DateTimeSensorAsync(task_id=""test"", target_time=timezone.datetime(2021, 8, 19, 23, 15, 0))```",5
"[Airflow 13779] use provided parameters in the wait_for_pipeline_state hook (#17137)I removed wait_for_pipeline_state from start_pipeline hook. By this call, I think we have a bug in this operator, for example when we have pipeline which starting more than 300 seconds, so it have a starting status, we get the error because this pipepline is not in correct state after 300 seconds. Even when we pass our parameters sucess_states and pipeline_timeout we get this error in this case, so I think when I pass both parameters the logic should use them not default. Why we have 300 second and these SUCCESS_STATES + [PipelineStates.RUNNING], because we had these values in the wait_for_pipeline_state call which I removed from hook, I think we should replace this 300 second and use default value from __init__ method (this is a open question I think)",5
Fetching and logging livy session logs for LivyOperrator (#17393),2
Add triggerer to `docker-compose.yaml` file (#17745)Adds triggerer component added in #15389 (AIP-40) to the docker-compose.yaml file for quick start,2
Fix failing main spellcheck build (#17761),0
"Chart docs: Format ``loadBalancerSourceRanges`` using code-block (#17763)Since we use rst for docs, and to be consistent with what we have elsewhere",2
"Fix using XCom with ``KubernetesPodOperator`` (#17760)This commit effectively revers https://github.com/apache/airflow/pull/15942 because of the issues it is causing. Until we find better solution we should revert this changeError:```[2021-07-26 20:23:54,109] {taskinstance.py:1108} INFO - Executing <Task(KubernetesPodOperator): write-xcom> on 2021-07-26T20:23:27.058907+00:00[2021-07-26 20:23:54,113] {standard_task_runner.py:52} INFO - Started process 11 to run task[2021-07-26 20:23:54,297] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'k8_pod_operator_xcom', 'write-xcom', '2021-07-26T20:23:27.058907+00:00', '--job-id', '1757', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/k8s_xcom_example.py', '--cfg-path', '/tmp/tmp0q94pkhs', '--error-file', '/tmp/tmpoz9qqp2l'][2021-07-26 20:23:54,298] {standard_task_runner.py:77} INFO - Job 1757: Subtask write-xcom[2021-07-26 20:23:54,511] {logging_mixin.py:104} INFO - Running <TaskInstance: k8_pod_operator_xcom.write-xcom 2021-07-26T20:23:27.058907+00:00 [running]> on host k8podoperatorxcomwritexcom.21384021df914227ad4e4b3a34313710[2021-07-26 20:23:54,713] {taskinstance.py:1502} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1158, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1295, in _prepare_and_execute_task_with_callbacks    self.render_templates(context=context)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1796, in render_templates    self.task.render_template_fields(context)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 999, in render_template_fields    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())  File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 1012, in _do_render_template_fields    rendered_content = self.render_template(content, context, jinja_env, seen_oids)  File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 1063, in render_template    return [self.render_template(element, context, jinja_env) for element in content]  File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 1063, in <listcomp>    return [self.render_template(element, context, jinja_env) for element in content]  File ""/usr/local/lib/python3.7/site-packages/airflow/models/baseoperator.py"", line 1047, in render_template    return jinja_env.get_template(content).render(**context)  File ""/usr/local/lib/python3.7/site-packages/jinja2/environment.py"", line 883, in get_template    return self._load_template(name, self.make_globals(globals))  File ""/usr/local/lib/python3.7/site-packages/jinja2/environment.py"", line 857, in _load_template    template = self.loader.load(self, name, globals)  File ""/usr/local/lib/python3.7/site-packages/jinja2/loaders.py"", line 115, in load    source, filename, uptodate = self.get_source(environment, name)  File ""/usr/local/lib/python3.7/site-packages/jinja2/loaders.py"", line 197, in get_source    raise TemplateNotFound(template)jinja2.exceptions.TemplateNotFound: mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json[2021-07-26 20:23:54,797] {taskinstance.py:1552} INFO - Marking task as FAILED. dag_id=k8_pod_operator_xcom, task_id=write-xcom, execution_date=20210726T202327, start_date=20210726T202353, end_date=20210726T202354[2021-07-26 20:23:54,936] {local_task_job.py:153} INFO - Task exited with return code 1```Dag:```from airflow import DAGfrom airflow.operators.bash import BashOperatorfrom airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (    KubernetesPodOperator,)from airflow.utils.dates import days_agofrom airflow.configuration import confnamespace = conf.get(""kubernetes"", ""NAMESPACE"")# This will detect the default namespace locally and read the# environment namespace when deployed to Astronomer.if namespace == ""default"":    config_file = ""/usr/local/airflow/include/.kube/config""    in_cluster = Falseelse:    in_cluster = True    config_file = Nonedefault_args = {    ""owner"": ""airflow"",}with DAG(    dag_id=""k8_pod_operator_xcom"",    default_args=default_args,    schedule_interval=None,    start_date=days_ago(2),    tags=[""k8""],) as dag:    write_xcom = KubernetesPodOperator(        namespace=namespace,        in_cluster=in_cluster,        config_file=config_file,        image=""ubuntu"",        cmds=[            ""sh"",            ""-c"",            ""mkdir -p /airflow/xcom/;echo '[1,2,3,4]' > /airflow/xcom/return.json"",        ],        name=""write-xcom"",        do_xcom_push=True,        is_delete_operator_pod=True,        task_id=""write-xcom"",        get_logs=True,    )    pod_task_xcom_result = BashOperator(        bash_command=""echo \""{{ task_instance.xcom_pull('write-xcom')[0] }}\"""",        task_id=""pod_task_xcom_result"",    )    write_xcom >> pod_task_xcom_result```closes https://github.com/apache/airflow/issues/17186",0
"make the providers operators/hooks reference much more usable (#17768)The providers operators/hooks reference contained only top-level list ofgroups of providers, which make them less-usable than they could be asthe users did not see at this page links to particular operators/hooks,it was not really visible what is ""available"" (discoverability) andthe more detailed ""Service"" and ""Transfer"" pages are not reallyreadable enough to give ""at a glance"" overview what is available.This change improves that, removes the repeated multiple times""operators and hooks"" which was kind of annoying, and increasesthe TOC-level to 3 giving a nice overview of all available andexposed operator and hooks.",1
"Improve pre-commit documentation (#17770)The Pre-commit documentation did not show immediately how to installpre-commit and did not explain what the pre-commits are, divingstraight into list of pre-commits. Also it did not explain quicklythat even after installing pre-commits you can easily skip eitherall pre-commits or some of them (even permanently). That could leadto people removing pre-commits when they found it not working asthey did not even know that they can use it selectively.Hopefully this will make locally installed pre-commit a bit morepopular among our contributors.",1
"Fix ``TimeSensorAsync`` (#17748)When using the following example dag, it currently fails with `You cannot pass naive datetimes` error.This happens because `TimeSensorAsync` passes a `datetime.time` object while `DateTimeTrigger` expectsa `datetime.datetime` object. This PR fixes that.Example DAG:```pythonfrom datetime import timedeltafrom airflow import DAGfrom airflow.sensors.time_sensor import TimeSensorAsyncfrom airflow.utils import dates, timezonewith DAG(    dag_id='example_date_time_async_operator',    schedule_interval='0 0 * * *',    start_date=dates.days_ago(2),    dagrun_timeout=timedelta(minutes=60),    tags=['example', 'example2', 'async'],) as dag:    TimeSensorAsync(task_id=""test-2"", target_time=timezone.time(22, 43, 0))```",1
"Automatically use utf8mb3_general_ci collation for mysql (#17729)The index size is too big in case utf8mb4 is used as encodingfor MySQL database. We already had `sql_engine_collation_for_ids`configuration to allow the id fields to use different collation,but the user had to set it up manually in case of a failure tocreate a db and it was not obvious, not discoverable and ratherclumsy.Since this is really only a problem with MySQL the easy solutionis to force this parameter to utf8mb3_general_ci for all mysqldatabases. It has no negative consequences, really as allrelevant IDs are ASCII anyway.Related: #17603",5
Remove deprecated metrics from metrics.rst (#17772),4
"Enable Connection creation from Vault parameters (#15013)Currently using the Vault secrets backends requires that users storethe secrets in connection URI format:https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#connection-uri-formatUnfortunately the connection URI format is not capable of expressingall values of the Connection class. In particular the Connectionclass allows for arbitrary string values for the  `extra` parameter,while the URI format requires that this parameter be unnested JSONso that it can serialize into query parameters.```>>> Connection(conn_id='id', conn_type='http', extra='foobar').get_uri()[2021-03-25 13:31:07,535] {connection.py:337} ERROR - Expecting value: line 1 column 1 (char 0)Traceback (most recent call last):  File ""/Users/da.lum/code/python/airflow/airflow/models/connection.py"", line 335, in extra_dejson    obj = json.loads(self.extra)  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/__init__.py"", line 348, in loads    return _default_decoder.decode(s)  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/decoder.py"", line 337, in decode    obj, end = self.raw_decode(s, idx=_w(s, 0).end())  File ""/nix/store/8kzdflq0v06fq0mh9m2fd73gnyqp57xr-python3-3.7.3/lib/python3.7/json/decoder.py"", line 355, in raw_decode    raise JSONDecodeError(""Expecting value"", s, err.value) from Nonejson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)[2021-03-25 13:31:07,535] {connection.py:338} ERROR - Failed parsing the json for conn_id id'http://'```As shown, the `extra` data is missing from the return value `http://`.Although there is an error logged, this does not help users who werepreviously able to store other data.",5
Ensure that airflow modules are not imported too early in conftest (#17779)If someone adds `import airflow` or any submodule in to conftest it willcause airflow.settings and airflow.configuration to be imported andinitialized before we set the test env vars -- this leads to _some_tests failing in bizarre ways.,0
"Improve dag/task concurrency check (#17786)Currently, tasks can be run even if the dagrun is queued. Task instances of queued dagrunsshould only be run when the dagrun is in running state. This PR makes sure tis of queued dagrunsare not run thereby properly checking task concurrency.Also, we check max_active_runs when parsing dag which is no longer needed since dagrunsare created in queued state and the scheduler controls when to change the queued dagrunsto running considering the max_active_runs.This PR removes the checking of max_active_runs in the dag too.",2
Fix dag_processing.last_duration metric random holes (#17769)* Fix dag_processing.last_duration metric random holes* Fix test* Fix mssql+sqlite test* move dag_processing.last_duration timing to _collect_results_from_processor,2
Add Triggerer to Helm Chart (#17743)Adds triggerer component added in #15389 (AIP-40) to the Helm Chart,2
Fix failing Helm Chart docs test (#17789)Fixes: https://github.com/apache/airflow/runs/3402579247#step:7:718,1
"Move Model collation args tests to correct folder (#17791)The tests for this got added to test_base.py, which is the right filename, but inside tests/sensors/, which isn't right :)Created a new tests/models/test_base.py for this",3
"Add warning about https configuration in SimpleHttpOperator (#17783)For historical reasons, configuring ``https`` via SimpleHttpOperatoris well, complex.This PR adds warning which informs the users about it as wellprovides explanation why it is like that and gives some helpfulexamples, so that people do not have to look for answers inStackOverflow questions or GitHub issues or JIRAs (as they didso far - for example #17780 and inhttps://issues.apache.org/jira/browse/AIRFLOW-2910 orhttps://stackoverflow.com/questions/51630344and many other questions.",0
Chart: Update the default Airflow version to ``2.1.3`` (#17794)Since 2.1.3 is out we should use that as the default Airflow version.,1
Update ``README.md`` to point to Airflow 2.1.3 (#17793),2
feat: Add Loadsmart in the list of companies using it (#17792),1
Update description about the new ``connection-types`` provider meta-dataThe ``hook-class-names`` provider's meta-data property has been deprecated andis now replaced by ``connection-types`` property. This documents thechange.,4
"Improve discoverability of Provider packages' functionalityThe documentation of provider packages was rather disconnectedfrom the apache-airlfow documentation. It was hard to find theways how the apache airflow's core extensions are implemented bythe community managed providers - you needed to know what you werelooking for, and you could not find links to the summary of thecore-functionality extended by providers when you were looking atthe functionality (like logging/secret backends/connections/auth)This PR inroduces much more comprehensive cross-linking betweenthe airflow core functionalithy and the community-managed providersthat are providing extensions to the core functionality.",1
"Move instriuctions of constraint/image refreshing to devWhen we have a prolonged issue with flaky tests or Github runnersinstabilities, our automated constraint and image refresh mightnot work, so we might need to manually refresh the constraintsand images. Documentation about that was in CONTRIBUTING.rstbut it is more appriate to keep it in ``dev`` as it only appliesto committers.Also during testing the parallell refresh without delays an errorwas discovered  which prevented parallell check of random imagehash during the build. This has been fixed and parallellimage cache building should work flawlessly now.",1
"Remove airflow dependency from http providerThe http provider has been temporarily moved out of preinstalledproviders (because of licensing issues). Those issues have nowbeen removed and http provider went back to be preinstalled,however it still had the apache-airflow>=2.1 as dependency.This PR removes the dependency.Fixes: #17795",0
Fix missing whitespace in ``apply_default`` deprecation message (#17799),0
"Prepare release for Kubernetes Provider (#17798)https://github.com/apache/airflow/issues/17186 -- This has made the XCom functionality not work with KubernetesPodOperator, this has been fixed by https://github.com/apache/airflow/pull/17760 -- so we should get this out sooner rather than later as recently released Airflow 2.1.3 will pull in latest kubernetes provider when we run `pip install -U apache-airflow[cncf.kubernetes]`",1
"Add pre/post execution hooks (#17576)Adds overrideable pre-/ post- execution hooks. With this change you can override pre-/post- hooks at the time of DAG creation, without the need of creating your own derived operators. This means that you can - for example - skip /fail any task by raising appropriate exception in a method passed as the pre- execution hook based on some criteria (for example you can make a number of tasks always skipped in a development environment). You can also plug-in post-execution behaviour this way that will be always executed at the same worker as the task run, sequentially to the task (as opposed to callbacks, which can be executed elsewhere and asynchronously)",1
Fix: Mysql 5.7 id utf8mb3 (#14535),0
Use one interpreter for Airflow and gunicorn (#17805),1
"Fixes unbound variable during remote image check in Breeze (#17807)The error would occur when comparing the hashes. It would notresult in big problem, simply unnecessary docker pull wouldbe run always when you rebuild the image, making it slightlyslower (as it would always have to pull the most recentlayers that you already rebuilt locally with your localsource changes).```Checking if the remote image needs to be pulledccc1270fce717b3e6918169c6de4d741262072ad78cc423fdd3e20f56134eb02ccc1270fce717b3e6918169c6de4d741262072ad78cc423fdd3e20f56134eb02/home/jarek/code/airflow/scripts/ci/libraries/_build_images.sh: line 329: remote_image_build_cache_file: unbound variableYour image and the dockerhub have different or missing build cache hashes.Local hash: 'eb64c3e26ab3b1e3c4cb38e691483d9f5f6508e5e71a06fed9b200c2d596'. Remote hash: ''.```",2
API documentation spelling fix (#17811) `paasword` to `password`,4
fix EXTRA_LOGGER_NAMES param and related docs (#17808),2
"Improves documentation about modules management (#17757)* Improves documentation about modules managementThis PR is result of responding to a number of questions and issuesAirflow users had on Slack, Stack Overflow and GitHub issues.While the document was really helpful it did not address some ofthe questions people had and it did not have enough examples, it alsodid not warn people to avoid some of the common mistakes theycould make (such as using relative imports).The sequence of the document has also been slightly improved.It was not clear what the document was reallyh about as it hadseveral independent chapters without logical sequence. Now thedocument starts with intro and explaining the options you have,explaining in general how python loads packages and modules andthen going deper into the usual ways users will add code toAirflow, ending with description on how to prepare your custompackage (which logically follows the frequency of the ways usersare adding code to Airflow).",1
"Show all import_errors from zip files (#17759)Instead of showing a single import error from a zip file,show them all.",2
Fixed numeric list (#17813),0
"Show import errors in DAG views (#17818)Currently, import errors are not shown on DAG views(gragh, tree etc).This PR addresses it",1
Chart: configurable number of retention days for log groomers (#17764),2
Increase width for Run column (#17817),1
Update INTHEWILD.md (#17832)Add Tapsi to the list of Airflow users,1
"Only show import errors for DAGs a user can access (#17835)For new DAGs (ones that have not previously parsed successfully), importerrors will only be shown to users who can read all DAGs.Closes: #17684",2
Fix typos in docs & ``bug_report`` template (#17809),0
"(docs): update README.md (#17806)* (docs): update README.md- correct pronoun agreement- reduce verbiage- increase readability- correct capitalization- make capitalization cohesive- add punctuation- make `md` section spacing cohesive- make bolding of bullet items cohesive (i.e., **Note:** versus **Note**:)",1
Improve graph view load time for dags with open groups (#17821)* Only draw once during initial graph setupThe previous behavior could cause significat slowness for when loadingthe graph view for large dags with many task groups.* Improve name and fix camelCased* Fix indent* PR suggestions remove args,4
"Fix broken MSSQL test (#17797)This broken test was causing the next test to use the db to fail. Also,by not ignoring exceptions here we let the failure be exposed whereits broken, not in the next test that happens to run.",1
"Avoid redirect loop for users with no permissions (#17838)Like we recently did for users with no roles, also handle it gracefullywhen users have no permissions instead of letting them get stuck in aredirect loop.This also changes the approach to rendering the template as a 403 forthe originally requested URI instead of redirecting to a separate endpoint.Closes: #16587",4
Fix provider.yaml errors due to exit(0) in test (#17858)There were a few errors in provider structure resulting fromaccidental exit(0) in provider's tests. Luckily none of the changeshave been released yet.,4
Update to Celery 5 (#17397)Celery 4 is no longer supported as of 2021-08-01:https://docs.celeryproject.org/en/stable/history/whatsnew-5.0.html#long-term-support-policyCloses: #11301,2
"Sane detection of the host/port in entrypoint prod (#17847)The previous regexp parsing was well, not perfect closely following theancient Chinese proverb ""If you have problem, introduce regexp - youwill have two problems"".This PR replaces regexp matching with python urlsplit method.Fixes: #17828",0
"Fix log links on graph TI modal (#17862)The graph view should show the ""Download Log"" and ""View Logs in {remotelogging system}"", like is done on the tree view.",5
"Allow omission of `initial_node_count` if `node_pools` is specified (#17820)* Add error check for config_file parameter* Move error check to init* fix static checks* Apply suggestions from code review* Apply suggestions from code review* Modify validation to allow node_pools* WIP: pass test cases 0, 1, 3* WIP: pass test case 4* WIP: pass test case 2* WIP - pass test cases 5 and 6, remove trailing comma* WIP - pass test case 7* WIP pass test case 8* cleanup* fix static checkCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",0
Gcp ai hyperparameter tuning (#17790)* Add hyperparameters to MLEngineStartTrainingJobOperator* Fixed pre-commit errors* Added passing hyperparameters to MLEngineStartTrainingJobOperator in example_mlengine.py* Added passing hyperparameters to Google ML CreateTrainingJob operator,1
Move setting of project ID after activating service account (#17866)Co-authored-by: Dmytro Khimich <khimich@google.com>,1
Add Brent to Committers list (#17873)https://lists.apache.org/thread.html/r5ed62c56a96a90fda48b64f95d1cab94f24a6b576d9345f695d303e4%40%3Cdev.airflow.apache.org%3E,1
Allow google-auth < 3 and google-api-core < 3 (#17671),1
Convert issue templates into forms (#17855)Following the discussion athttps://lists.apache.org/x/thread.html/r36d084d83cf7a66698c84185558e1bbe971c59533d4ac5d4994b0aca@%3Cdev.airflow.apache.org%3EThe issue templates are now Forms.,0
"Secrets backend failover (#16404)Currently Airflow does not check the default secrets backends (env and metastore db) if there is any sort of connection related error to an Alternative Backend, causing related tasks to fail. The change proposed here allows it to fail over to checking the default backends when this happens.Additionally GCP Secrets Manager causes the Airflow Webserver to crash at startup if credentials for the backend cannot be found. This behavior seems to be unique to GCP Secret Manager and this PR addresses that for parity in behavior regarding missing credentials across all backends.closes: #14592",1
"Add an Amazon EMR on EKS provider package (#16766)* Add an Amazon EMR on EKS provider package - Adds an operator, sensor, and hook for running Spark jobs on EMR on EKS as well as docs and an example DAG",2
ExasolHook get_pandas_df does not return pandas dataframe but None (#17850)closes #17135ExasolHook get_pandas_df does not return pandas dataframe but None,5
Add Brent to ``CODEOWNERS`` (#17877)Brent has been championing UI since some time now.,1
Add back deleted comment (#17884)Fixes comment deleted in #17304,4
fix wrong documents around upgrade-check.rst (#17903),2
replace  to contributing quick start guide (#17908),5
"Remove all deprecation warnings in providers (#17900)There were a number of deprecation warnings when just importingcertain providers or their examples. Some of them came from stillusing 1.10 classes, some from using still `apply_defaults` andsome from badly implemented fallback mechanism to supportbackwards compatibility.This has all been fixed and our documentation generation stepfor providers in CI will also fail in case there are some newDeprecationWarnings generated, which means that they will haveto be fixed before merging.While checking that we found that BigQuery table_resource newapproach had some inconsistencies when passing new `table_resource`parameters. Some deprecated parameters should be None by default,otherwise you'd have to explicitly set them to None when`table_resource` is set but when it is not set, the backwardscompatible behaviour is maintained - the parameters get the same.default values set.Several deprecation warnings had wrong stack-level - this has beenfixed as well.The ""legitimate"" warnings are now filtered out and they are onlyfiltered out for the appropriate modules we know they are generatedfrom or in case the warnings result from direct import of thedeprecated module.",2
[AIRFLOW-9300] Add DatafusionPipelineStateSensor and aync option to the CloudDataFusionStartPipelineOperator (#17787),5
"Fixed button size in ""Actions"" group. (#17902)",0
Fix missing Data Fusion sensor integration (#17914)We recently fixed provider.yaml validation and PR #17787added a new Data Fusion integration without flagging this as errorbecause it was based on earlier version of the change withoutthe fix to static checks.This PR adds the missing integration,1
Add doc warning about connections added via envvars (#17915)Closes #17852,1
"BugFix: ``TimeSensorAsync`` returns a naive datetime (#17875)My fix in https://github.com/apache/airflow/pull/17748 was only partially correct and I missed one part. `TimeSensorAsync` passed a naive datetime which failed when passed to `DateTimeTrigger`. This PR fixes it and adds test to avoid regression.Error:```[2021-08-27 23:31:11,508] {taskinstance.py:1657} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/opt/airflow/airflow/models/taskinstance.py"", line 1296, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File ""/opt/airflow/airflow/models/taskinstance.py"", line 1415, in _prepare_and_execute_task_with_callbacks    result = self._execute_task(context, task_copy)  File ""/opt/airflow/airflow/models/taskinstance.py"", line 1471, in _execute_task    result = execute_callable(context=context)  File ""/opt/airflow/airflow/sensors/time_sensor.py"", line 60, in execute    trigger=DateTimeTrigger(moment=self.target_datetime),  File ""/opt/airflow/airflow/triggers/temporal.py"", line 40, in __init__    raise ValueError(""You cannot pass naive datetimes"")ValueError: You cannot pass naive datetimes```Example DAG:```pythonfrom datetime import timedelta, timefrom airflow import DAGfrom airflow.sensors.time_sensor import TimeSensorAsyncfrom airflow.utils import dates, timezonewith DAG(    dag_id='example_date_time_async_operator',    schedule_interval='0 0 * * *',    start_date=dates.days_ago(2),    dagrun_timeout=timedelta(minutes=60),    tags=['example', 'example2', 'async'],) as dag:    TimeSensorAsync(task_id=""test-2"", target_time=time(0, 38, 0))```",1
Fix ``DagRunState`` enum query for ``MySQLdb`` driver (#17886)same as https://github.com/apache/airflow/pull/13278 but for `DagRunState` introduced in https://github.com/apache/airflow/pull/16854closes https://github.com/apache/airflow/issues/17879,0
Fix broken XCOM in EKSPodOperator (#17918),1
Fix `TestSecurity.test_current_user_has_permissions` (#17916)This test wasn't working on python > 3.7.,1
Add issue form template for Helm Chat (#17917)With so many people reviewing nobody noticed that we forgot to addHelm Chart issue form :)This rectifies the mistake.,0
"Fix saving last choice of backend and other flags between breeze runs (#17913)Breeze used to have the ability of storing the last choice ofbackend and few other flags in the "".BUILD"" directory. Whilethey were saved, one of the refactors changing the sequenceof initialization made the default value to override the oneprovided by the .BUILD directory.This PR changes the sequence of initialization slightly:1) First we make sure that the .BUILD and few other temp dirs   are created2) Then we read variables stored there (if they are stored)3) Then we follow through to the next steps of initialization and   set the default values for parameters that do not have the   values set yet",1
HiveHook fix get_pandas_df() failure when it tries to read an empty table (#17777),0
Add August 2021 Provider's documentation (#17890),2
Allow setting specific cwd for BashOperator (#17751)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Separate Managed Services for Airflow (#17926)The information which service Airflow is installed at is much moreuseful than just knowing that it is a Managed Service.Also added a comment to direct users of Managed Services to theregular support of those services first.,1
Allow custom timetable as a DAG argument (#17414),2
"Update error message to guide the user into self-help mostly (#17929)The error report generated, when there is a crash on webserver,directed the user to open an issue in Apache Airflow without anyextra explanation or suggesting other actions. This is not a goodidea because it might lead people to thinking that they canjust follow the link, open issue and it will be solved, However,more often than not such issue can be caused by misconfiguration,networking or other actions that the user should - in many cases -be able to fix or workaround on their own, with a little effortof gathering logs/information and searching for relevant problems.This PR changes the message as folows:* explains that user should gather more information (the link  and bug report does not contain any information)* search for similar problem (we provide links to the places which  can be searched and suggest using regular search engine as well* direct the managed services users to open the issue using  relevant channels* only as the last resort, opening an issue in Airflow GitHub,  providing sufficient information.",5
"Always upgrade to newer dependencies in main (#17939)When we want to run the ""push"" or ""schedule"" build, we wantto always ""upgrade to newer dependencies"" because we want toattempt to generate latest constraints.There was a mistake that caused failures like celeryincompatibilities because successfull main build pushed downgradedconstraints because the released image did not contain latestcelery libraries and it has not been rebuilt with latestconstraints yet.",3
Example xcom update (#17749),5
Fix docs about login for hdfs connections (#17936),2
Fix grammar in `traceback.html` (#17942)`those` -> `these`,0
"Fix instantiating Vault Secret Backend during configuration (#17935)When Secrets Backend are instantiated during configuration, notall Airlfow packages are yet imported, because they need SecretBackends. We have a weird cyclical relation between models,configuration and settins which forces us to be extra carefularound configuration, settings and backends.In this case top-level import of Connections by the Vault SecretBackend triggered cyclic import problem (importing airflow modelsrequire configuration to be fully loaded and initialized) but thenit could not be initialized because models needed to be importedfirst.The fix is to move Connections to be locally imported.",2
Update docker.rst (#17882)This didn't work on my 2019 Macbook 16 inch until I ran the Linux section.Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Adds Github Oauth example with team based authorization (#17896),1
Fixing bug which restricted the visibility of ImportErrors (#17924),2
Add possibility to run DAGs from system tests and see DAGs logs (#17868),2
Add Next Run to UI (#17732)* add next run to home page* add nextrun to dag pages* keep date check consistent* fix test_views* Include data interval values in /last_dagruns view* Use dag.next_dagrun_create_after* Fix timezone formattingUse `<time>` to use our existing `datetime_utils` to format and handle timezone changes* Update next and last run tooltips* wrap meta tags in if statementCo-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
"Refactor BranchDayOfWeekOperator, DayOfWeekSensor (#17940)* Refactor BranchDayOfWeekOperator, DayOfWeekSensor.1. Extract shared code to utils.2. Allow any iterable as week_day.",1
Queue support for DaskExecutor using Dask Worker Resources (#16829),1
"Improve MySqlToHiveOperator tests (#17958)These tests were actually just testing the hook again (by checking thecommand executed) but were asserting nothing about the CSV passed to thehook.This change makes the operator tests check the logic in the operator andno longer tests the hook code again.`test_mysql_to_hive_verify_loaded_values` was removed as since weremoved Java/a real hive CLI from our tests this has only been testingour mock, not the real code.",3
"Only show Pause/Unpause tooltip on hover (#17957)After clicking on the Pause/Unpause toggle, the element remained in focus and therefore the toggle wouldn't go away. After a change event we will also trigger a blur event to remove the focus so the tooltip will only appear on hover.Fixes: #16500",0
Add robots.txt and X-Robots-Tag header (#17946)Co-authored-by: thejens <jens.larsson@tink.com>,5
"Fix passing Jinja templates in ``DateTimeSensor`` (#17959)While fixing ``DateTimeSensorAsync`` in https://github.com/apache/airflow/pull/17747 -- I broke ``DateTimeSensor``.As `target_time` is a template_field for `DateTimeSensor`, Jinja tries to render it which does not work if the input is a datetime object or if someone passes just a template field like ``{{ execution_date }} `` it throws an error:```    DateTimeSensor(task_id=""foo"",                   target_time=""{{ execution_time }}""    )    '        Traceback (most recent call last):          File ""/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py"", line 131, in _parse            dt = parser.parse(          File ""/usr/local/lib/python3.9/site-packages/dateutil/parser/_parser.py"", line 1368, in parse            return DEFAULTPARSER.parse(timestr, **kwargs)          File ""/usr/local/lib/python3.9/site-packages/dateutil/parser/_parser.py"", line 643, in parse            raise ParserError(""Unknown string format: %s"", timestr)        dateutil.parser._parser.ParserError: Unknown string format: {{ execution_time }}        During handling of the above exception, another exception occurred:        Traceback (most recent call last):          File ""<string>"", line 3, in <module>          File ""/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 186, in apply_defaults            result = func(self, *args, **kwargs)          File ""/usr/local/lib/python3.9/site-packages/airflow/sensors/date_time.py"", line 66, in __init__            self.target_time = timezone.parse(target_time)          File ""/usr/local/lib/python3.9/site-packages/airflow/utils/timezone.py"", line 175, in parse            return pendulum.parse(string, tz=timezone or TIMEZONE, strict=False)  # type: ignore          File ""/usr/local/lib/python3.9/site-packages/pendulum/parser.py"", line 29, in parse            return _parse(text, **options)          File ""/usr/local/lib/python3.9/site-packages/pendulum/parser.py"", line 45, in _parse            parsed = base_parse(text, **options)          File ""/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py"", line 74, in parse            return _normalize(_parse(text, **_options), **_options)          File ""/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py"", line 135, in _parse            raise ParserError(""Invalid date string: {}"".format(text))        pendulum.parsing.exceptions.ParserError: Invalid date string: {{ execution_time }}```This PR fixes it by reverting change in `DateTimeSensor` and parses the string to datetime in `DateTimeSensorAsync.execute`",5
Invalidate Vault cached prop when not authenticated (#17387),5
New google operator: SQLToGoogleSheetsOperator (#17887),1
Add support for kinit options [-f|-F] and [-a|-A] (#17816)kinit can now emit non forwardable ticket and ticket without originate IP.,5
Fix Clear task instances endpoint resets all DAG runs bug (#17961),0
"Making spelling of ""TaskFlow"" consistent in docs (#17968)",2
Update max_tis_per_query to better render on the webpage (#17971),1
deduplicate running jobs on BigQueryInsertJobOperator (#17496),1
Add missing menu access for dag dependencies and configurations pages (#17450),5
Chart: Make cleanup cronjob cmd/args configurable (#17970),5
Delete unnecessary parameters in EKSPodOperator (#17960),1
Remove default_args pattern + added get_current_context() use for Core Airflow example DAGs (#16866)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,2
"Fix constraint generation properly (#17964)The #17939 did not fix the problem finally. It turned out thatone more change was needed - since we now always upgrade to latestdependencies in `push` and `schedule` type of build we do not needto check for the variable UPGRADE_TO_NEWER_DEPENDENCIES (whichwas not set in ""Build Image"" step.This fixes it, but also changes the constraint generation to addcomments in the generated constraint files, describing how andwhy the files are generated.",2
"Fix max_active_runs not allowing moving of queued dagruns to running (#17945)Currently, if you set max_active_runs for a dag and that dag has many queued dagrunswith execution dates older than another dag's queued dagruns, airflow will not movethe newer queued dagruns to running with the effect that only one dagruns would be in running at any timeThis PR fixes this by updating the DagRun.last_scheduling_decision whenever a decision of schedulingwas made",2
Add DAG run endpoint for marking a dagrun success or failed(#17839)Co-authored-by: bbenshalom <bbenshalom@outbrain.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>,0
Fix blank dag dependencies view (#17990)* Fix blank dag dependencies view* calculate graph if node and edges are empty,2
Serialize the template_ext attribute to show it in UI (#17985)Co-authored-by: Bas Harenslak <bas@astronomer.io>,5
Fix grammar in local.rst (#18001),0
"Hide variable import form if user lacks permission (#18000)This hides the variable import form if the user does not have the ""cancreate on variable"" permission.",1
"Reduce number of lines in ""monitoring job"" parallel group (#17995)Although this group is collapsed when the job is finished it it is largeit can cause the logs view in GitHub to bog down the browser, so this PRdoes a few things:- Stops showing disk information for ""overlay"" FS types -- i.e. docker  containers as they share the same disk usage as the docker root- Removes a few ""duplicated"" messages to further reduce output- Sleep for 15s instead of 10 between poll attempts",4
"Make EMR cluster visible to all users note (#17557)If someone tries the examples given here they will probably not see the EMR clusters in the EMR Management Console, so add a note to explain how to add the VisibleToAllUsers option to make the clusters visible in the console.",1
"Applied permissions to self._error_file (#15947)* Applied permissions to self._error_fileUsed os.chmod with 0o0777 umask to avoid Permission Denied errors when accessing the temporary error file.* Changed os.chmod to os.chownInstead of using os.chmod and applying permissions to the error file, we are setting the file owner according to the variable self.run_as_user. This would avoid setting hardcoded permissions for the log file while maintaining the correct permissions to the user responsible for the DAG to manipulate the log file.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",2
Require timetable class be registered via plugin (#17989),1
"Summarize test failures at the end of CI run (#18008)This prints the names of the failed tests, and their error/failuremessages which makes it easier to see the errors, rather than having toexpand all the cases, scroll down to find the error many times.",0
Logs task launch exception in StandardTaskRunner (#17967)Adds an error log message to task runner when task launch fails with error code 1Co-authored-by: Yash Dodeja <ydodeja365@gmail.com>,0
Implement API endpoint for DAG deletion (#17980)Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,4
Add Spark to the EMR cluster for the job flow examples (#17563)The example step fails if Spark is not included in the applications in the cluster.,0
Fix failed tests from run_as_user+error_file changes (#18013),4
Allow specifying multiple URLs via the CORS config option (#17941),5
Fix unexpected bug in exiting hook context manager (#18014),1
Adding missing init file in example_dags directory (#18019),2
Fix spelling error causing tests to fail on main (#18021)I merged the PR that added this too eagerly,1
AwsBaseHook make `client_type` & `resource_type` optional params for `get_client_type` & `get_resource_type` (#17987)* AwsBaseHook make client_type & resource_type optional params for get_client_type & get_resource_type,1
fix(CloudSqlProxyRunner): don't query connections from Airflow DB (#18006)Instead of directly querying connections from Airflow DB we use`get_connection()` which also support external secrets backends.Fixes: #18003,0
Update release notes for 3 extra providers released (#18018)We are releasing out-of-bands providers now:* Hashicorp - due to bug found in previous version* Celery - due to change in Celery dependency* Microsoft PSRP - due to bug found and fixed after relesing first  version,0
"Adds capability of Warnings for incompatible community providers (#18020)When we release providers, we do not know if some future versionof Airflow will be incompatible with them, so we cannot add hardlimits there. We have constraints that contain the ""latest""providers at the moment of release but if someone has an oldversions of providers installed and just upgrades Airflow, theincompatible versions of providers might be still installed.From now on Airflow will print warnings in case such incompatibleprovider is detected.",1
Adds example showing the ES_hook (#17944)* Adds example showing the ES_hookadds comment* reformats code* Black formatting* airflow.operators.python_operator -> airflow.operators.python* Updates docs to reference example DAGs,2
"Fix building docs in `main` builds (#18035)The builds run from `main` branch perform eager upgrade ofconstraints. For some reason (the root cause is still not known)importing google provider docs fails for autoapi. The importfails with ""TypeError(""unsupported operand type(s) for +:'SSL_VERIFY_PEER' and 'SSL_VERIFY_FAIL_IF_NO_PEER_CERT'"",))""This problem is gone however when generate google docs for thesecond time - apparently the first pass generates cache inthe inventory that unblocks the real import.This is similar error to those that we already handle bymaking them eligible for rebuilding. Adding the type oferror to the list of eligible errors fixes the problem, andlikely when the new sphinx iventory for google providerwill be pushed, it will disappear completely.",1
"Add Python2 to installed packages (#18037)As of August 2021, the buster-slim python images, no longercontain python2 packages. We still support running Python2 viaPythonVirtualenvOperator and our tests started to fail whenwe run the tests in `main` - those tests always pull and buildthe images using latest-available buster-slim images.Our system to prevent PR failures in this case has proven to beuseful - the main tests failed to succeed so the base imageswe have are still using previous buster-slim images which stillcontain Python 2.This PR adds python2 to installed packages - on both CI imagesand PROD images. For CI images it is needed to pass tests, forPROD images, it is needed for backwards-compatibility.",3
"Opmitise LoggingMixin.log (#17280)The log instance is now instantiated per class, instead of per instance,saving duplicated getLogger() calls. Those getLogger() calls wouldreturn the same logger instance anyway, so the functionality iseffectively the same; only the function call overhead os saved.The self.log property is also converted to a cached_property, so the'is None' check would only happen once per instance, instead of everytime the property is accessed.",5
"Improve the description of how to handle dynamic task generation (#17963)The Top-Level best practices were a little misleading. Theysuggested that no code should be written at the top-level DAG otherthan just creating operators, but the story is a little more nuanced.Better explanation is give and also examples on how you can dealwith the situation when you need to generate your data based onsome meta-data. From Slack discussion it seems that it is notobvious at all what are the best ways to handle that so twoalternatives were presented with generating a meta-data fileand generating an importable python code containing the meta-data.During that change, I noticed also, that config sections andconfig variables were not sorted - which made it very difficult tosearch for them in the index. All the config variables are nowsorted so the references to the righ sections/variables make muchmore sense now.",1
Fix providers tests in main branch with eager upgrades (#18040)The SQS and DataCatalog were failing tests in main branch becausesome recent release of dependencies broke them:1) SQS moto 2.2.6 broke SQS tests - the queue url in the 2.2.6+   version has to start with http:// or https://2) DataCatalog part of Google Provider incorrectly imported   types and broke tests (used beta instad of datacatalog path),5
Suppress stat stderr output when we have fallback (#18044),5
"Revert ""Opmitise LoggingMixin.log (#17280)"" (#18047)This reverts commit fe7efcaeb22f9053aa1cfdec01a50dc943583a8e.",4
"Fix building documentation broken by upgrade of dnspython (#18046)The automated upgrade of dependencies in main broken building ofAirflow documentation in main build.After a lot of experimentation, It has been narrowed downto upgrade of dnspython from 1.16.0 to 2.+ which was broughtby upgrading eventlet to 0.32.0.This PR limits the dnspython library to < 2.0.0. An issuehas been opened:https://github.com/rthalley/dnspython/issues/681",0
"Change TaskInstance and TaskReschedule PK from execution_date to run_id (#17719)Since TaskReschedule had an existing FK to TaskInstance we had to movechange both of these at the same time.This puts an explicit FK constraint between TaskInstance and DagRun,meaning that we can remove a lot of ""find TIs without DagRun"" code inthe scheduler too, as that is no longer a possible situation.Since there is now an explicit foreign key between TaskInstance andDagRun, we can remove a lot of the ""cleanup"" code in the scheduler thatwas dealing with this.This change was made as part of AIP-39Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",4
"Optimize imports of Providers Manager (#18052)During the investigation of the problem #18046, it turned out that weneedlessly import wtform when initializing ProvidersManager.This PR optimizes the imports so that Wtforms and relatedfield types are only imported when the hook class details areretrieved by Providers Manager - which is happening only whenthe hook is used.",1
"Do not fail KubernetesPodOperator tasks if log reading fails (#17649)In very long running airflow tasks using KubernetesPodOperator,especially when airflow is running in a different k8s cluster than wherethe pod is started with, we see sporadic, but reasonably frequentfailures like this, after 5-13 hours of runtime:[2021-08-16 04:00:25,871] {pod_launcher.py:198} INFO - Event: foo-bar.xyz had an event of type Running[2021-08-16 04:00:25,893] {pod_launcher.py:149} INFO - 210816.0400+0000 app-specific-logs......... (~few log lines ever few minutes from the app)...[2021-08-16 17:20:29,585] {pod_launcher.py:149} INFO - 210816.1720+0000 app-specific-logs....[2021-08-16 17:27:36,105] {taskinstance.py:1501} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 436, in _error_catcher    yield  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 763, in read_chunked    self._update_chunk_length()  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 693, in _update_chunk_length    line = self._fp.fp.readline()  File ""/usr/local/lib/python3.7/socket.py"", line 589, in readinto    return self._sock.recv_into(b)  File ""/usr/local/lib/python3.7/ssl.py"", line 1071, in recv_into    return self.read(nbytes, buffer)  File ""/usr/local/lib/python3.7/ssl.py"", line 929, in read    return self._sslobj.read(len, buffer)TimeoutError: [Errno 110] Connection timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1157, in _run_raw_task    self._prepare_and_execute_task_with_callbacks(context, task)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1331, in _prepare_and_execute_task_with_callbacks    result = self._execute_task(context, task_copy)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/models/taskinstance.py"", line 1361, in _execute_task    result = task_copy.execute(context=context)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 366, in execute    final_state, remote_pod, result = self.create_new_pod_for_operator(labels, launcher)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 520, in create_new_pod_for_operator    final_state, remote_pod, result = launcher.monitor_pod(pod=self.pod, get_logs=self.get_logs)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/airflow/providers/cncf/kubernetes/utils/pod_launcher.py"", line 147, in monitor_pod    for line in logs:  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 807, in __iter__    for chunk in self.stream(decode_content=True):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 571, in stream    for line in self.read_chunked(amt, decode_content=decode_content):  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 792, in read_chunked    self._original_response.close()  File ""/usr/local/lib/python3.7/contextlib.py"", line 130, in __exit__    self.gen.throw(type, value, traceback)  File ""/opt/pysetup/.venv/lib/python3.7/site-packages/urllib3/response.py"", line 454, in _error_catcher    raise ProtocolError(""Connection broken: %r"" % e, e)urllib3.exceptions.ProtocolError: (""Connection broken: TimeoutError(110, 'Connection timed out')"", TimeoutError(110, 'Connection timed out'))Most likely because the task is not emitting a lot of logs, or simply dueto sporadic network slowdown between clusters.So, if this fails, do not fail whole operator and terminate the task,until the call to `self.base_container_is_running` function also fails orreturns false.",0
"Build CI images for the merge result of a PR, not the tip of the PR (#18060)The change to use pull_request_target had the unintended side-effect ofmeaning that the images were built using the tip of the PR, instead ofthe merge of main and the PR branch.This restores that behaviour (and should mean that fewer PRs will needrebasing to fix their builds when master gets broken)",1
"Revert ""Build CI images for the merge result of a PR, not the tip of the PR (#18060)"" (#18063)This reverts commit 1bfb5722a8917cbf770922a26dc784ea97aacf33. (temporarily until we add gh to our AMI)",1
Fix Neo4jHook to get the query response (#18007),1
"Add missing ""be"" in documentation (#18059)New phrase: ""The operators will be instantiated."" Add missing ""be""",1
Added Jagex to the list of companies using Apache Airflow (#18058)Co-authored-by: anum_sheraz <anum.sheraz@jagex.com>,1
"Change id collation for MySQL to case-sensitive (#18072)For quite some time we recommended MySQL collation to beutf8mb3_general_ci in order to avoid too-large-index size. Turnsout that this collation is .... case-insensitive (that's whereci stands for) this causes problems in case of renamingtags (!) where only the case differs (Test -> test) as thosetags are considered equal (!). It would also cause problems ifthere were several DAGs with ids differing by case only.Moreoever ... there is no ""cs"" (case sensitive) collation forutf8 for MySQL as this is apparently a hard problem:https://stackoverflow.com/questions/4558707/case-sensitive-collation-in-mysqlThe solution in this PR is to change collation to utf8mb3_bin -it messes up with ORDER BY, but this is not a big problem for IDkind of values.Fixes: #17897",0
Adding CI check to verify __init__.py file exists in providers' example_dags dirs (#18076),2
"Fix KubernetesPodOperator reattach when not deleting pods (#18070)This change:- ignores any pods already marked as complete instead of trying to  reattach to them.- properly marks all 'finished' unsuccessful pods that won't  be deleted.Combined, these allow `is_delete_operator_pod=False` and`reattach_on_restart=True` to function together properly during retries.",1
Chart: fix webserver secret key update (#18079),5
"Update non-working example in documentation (#18067)Chain function cannot accept a list, but the example provided supplies with a list. Supplied example did not create chain dependencies as expected Replaced non-working example with working example, but does not use the chain function. I do not think you are able to use chain function dynamically.",1
"Reapply ""Build CI images for the merge result of a PR, not the tip of the PR (#18060)"" (#18086)This reverts commit 0dba2e0d644ab0bd2512144231b56463218a3b74.Revert ""Revert ""Build CI images for the merge result of a PR, not the tip of the PR (#18060)"" (#18063)"" (#18086)",7
Allow filtering DAGS by tags in the REST API (#18090)This PR adds a change to allow filtering DAGs by tags,2
sftp_to_s3 stream file option (#17609),2
"Fixes incorrect parameter passed to views (#18083) (#18085)The task_stats, last_dagruns, blocked etc expect dag_ids not dagIds.This caused the endpoint to return all dags the user had access to bydefaultcloses: #18083",1
"Limit colorlog version (6.x is incompatible) (#18099)The ""color"" method seems to have been removed.",4
Chart: Fix minor Triggerer issues (#18105)- We don't need `checksum/result-backend-secret` annotation for Triggerer- Fixes syntax error in Healthcheck- Fixes typo in filename for tests,3
"Adds LoggingMixins to BaseTrigger (#18106)This is a simple change to add logging to BaseTrigger, making it consistent with other classes like BaseOperator.",1
"Advises the kernel to not cache log files generated by Airflow (#18054)* Advises the kernel to not cache log files generated by AirflowExtends the standard python logging.FileHandler with advise to theKernel to not cache the file in PageCache when it is written. Whilethere is nothing wrong with such cache (it will be cleaned when memoryis needed), it causes ever-growing memory usage when scheduler isrunning as it keeps on writing new log files and the files are notrotated later on. This might lead to confusion for our users, who aremonitoring memory usage of Scheduler - without realising that it isharmless and expected in this case.Adding the advice to Kernel might help with not generating the cachememory growth in the first place.Closes: #14924",1
"Fix DagRun execution order from queued to running not being properly followed (#18061)We made a fix that resolved max_active_runs not allowing other dagruns to move torunning state, see #17945 and introduced a bug that dagruns were not following theexecution_date order when moving to running state.This PR fixes it by adding a 'max_active_runs` column in dagmodel. Also an extra testnot connected with this change was added because I was able to trigger the bug whileworking on this",1
Rename FileToWasbOperator to LocalFilesystemToWasbOperator (#18109),5
Limit the number of queued dagruns created by the Scheduler (#18065)There's no limit to the amount of queued dagruns to create currentlyand it has become a concern with issues raised against it. See #18023 and #17979Co-authored-by: Sam Wheating <samwheating@gmail.com>,0
Update wasb.rst (#18117)typo. should be WASB not WASP,2
"Check for missing DagRun rows for ""downstream"" tables before migrating DB (#17030)The migrations and code changes to switch these models over to userun_id instead of execution_date will follow in future PRs.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",5
"Fix constraints generation scripts. (#18094)We are now generatnung constraints with better description, andwe include information about DEFAULT_BRANCH (main/v2-1-test etc.)The scripts to generate the constraints need to get teh variablepassed to docker.Also names of generated files were wrong. The constraints didnot update the right constraint files.",2
added TUD to INTHEWILD.md (#18123),1
"Add ""packaging"" to core requirements (#18122)",1
"Change XCom class methods to accept run_id argument (#18084)Longer term (2.3 most likely) we will want to change the column on XComto have run_id instead of execution_date, but for now this changes theinterface to allow passing a run_id to XCom in place of anexecution_date.",5
fix misspelling (#18121),0
Make next_dagrun_info take a data interval (#18088),5
Fix deprecation error message rather than silencing it (#18126),0
Fix Sentry handler from LocalTaskJob causing error (#18119)The `enrich_errors` method assumes the first argument to the functionits patch is a TaskInstance when infact it can also be a LocalTaskJob.This is now handled by extracting the task_instance from theLocalTaskJobCloses #18118,4
Update version added fields in airflow/config_templates/config.yml (#18128),5
Fix ``BigQuery`` data extraction in ``BigQueryToMySqlOperator`` (#18073)closes: #17198,1
"Test coverage on the autocomplete view (#15943)* Test autocomplete status filtering* Convert DagModelView into a base viewNothing in the view ever used any of the model view features, so weshould convert it to a base implementation. This also ""improves"" thecoverage since the dead code are not covered.",3
"Simplify s3 ``unify_bucket_name_and_key`` (#17325)This `if key_name ...` part was doing nothing.`key_name` in previous code was always a one of two thing.Never empty, None, etc. (the typing annotation was wrong)So` if key_name` always evaluated to `True`.It is possible to move the key_name logic to be only undermissing bucket_name case, it would be slightly faster,but then missing argument case would be maybe harderto debug.",0
"Aws secrets manager backend (#17448)With this proposed modification, you can use AWS Secrets Manager using keys and values and have some kind of freedom to choose different words for each key to make the get_conn work.",1
"Adding ``TaskGroup`` support in ``BaseOperator.chain()`` (#17456)Related to: #17083, #16635",1
"Add Snowflake operators based on SQL Checks  (#17741)Add three new Snowflake operators based on SQL ChecksThe SnowflakeCheckOperator, SnowflakeValueCheckOperator, andSnowflakeIntervalCheckOperators are added as subclasses of their respectiveSQL Operators. These additions follow the conventions set in the BigQueryOperatorssubclassing from the same SQL_CheckOperators.closes: #17694",1
ECSOperator returns last logs when ECS task fails (#17209)closes: https://github.com/apache/airflow/issues/17038This PR changes the message in the AirflowException when the ECS task launched by ECSOperator is stopped. **Before:**The message when it failed was:`This task is not in success state {<huge JSON from AWS containing all the ECS task details>}`**Now:**The message is:```This task is not in success state - last logs from Cloudwatch:<last_logs_from_cloudwatch>```which makes it much more useful to understand what failed in the underlying code directly from the alert.The number of logs can be customized with the parameter `number_logs_exception`.,2
Add a note about no back-compat guarantees for experimental features (#18139),1
"Mark passing pre/post execute callbacks to operators as experimental. (#18140)My primary concern here is that by being able to arbitrarily ""change""what an operator does will greatly increase the ""accidental complexity""of both Airflow (for us as developers) and of the DAG itself (for ourusers).By marking this features as experimental we reserve the right to deleteit at any point (for instance once we add better methods of doing whatthe OP wanted with these hooks.)",1
[Airflow 16364] Add conn_timeout and cmd_timeout params to SSHOperator; add conn_timeout param to SSHHook (#17236),1
"Change from dynamic date to fixed date in examples (#18071)If you have a task that runs every week with days_ago(1), then at the time it would kick off the start_date will be yesterday morning but the execution_date will be a week ago. Therefore the dag will not be started as start_date > execution_date.This changes the example dags to use static date because of the above",1
Add missing __init__.py files for some test packages (#18142)Lacl of the __init__.py caused failures in some specificcases - especially when new providers have been added.This PR adds missing ``__init__.py`` files and modifies pre-commitcheck which was only implemented for main files andexample_dags. It checks if those files are present and adds them ifmissing.,1
Fix quarentine tests affected by AIP-39 (#18141),3
Doc: Minor wording tweaks (#18148)I find it easier to read this way.,2
Fix typo in StandardTaskRunning log message (#18149),2
"Fix bad repository name in pre-commit config (#18151)Having a trailing slash on the `flynt` entry makes some versions of Git (in my case, the one under Ubuntu 20.04) unwilling to clone it. Removing the trailing slash to match the other entries fixes the problem.",0
"Fixes warm shutdown for celery worker. (#18068)The way how dumb-init propagated the signal by defaultmade celery worker not to handle termination well.Default behaviour of dumb-init is to propagate signals to theprocess group rather than to the single child it uses. This isprotective behaviour, in case a user runs 'bash -c' commandwithout 'exec' - in this case signals should be sent not onlyto the bash but also to the process(es) it creates, otherwisebash exits without propagating the signal and you need secondsignal to kill all processes.However some airflow processes (in particular airflow celery worker)behave in a responsible way and handles the signals appropriately- when the first signal is received, it will switch to offlinemode and let all workers terminate (until grace period expiresresulting in Warm Shutdown.Therefore we can disable the protection of dumb-init and let itpropagate the signal to only the single child it spawns in theHelm Chart. Documentation of the image was also updated to includeexplanation of signal propagation. For explicitness theDUMB_INIT_SETSID variable has been set to 1 in the image as well.Fixes #18066",0
Fixing Vault AppRole authentication with CONN_URI (#18064),0
"Fixed log view for deferred tasks (#18154)Deferred tasks were not showing up in the log view while they weredeferred, as they decrement the try_number on the way into deferralstatus much like rescheduling sensors. This applies the same fix thatrescheduling sensors have to make their log appear.",2
Reorder migrations to be compatible with 2.1.4 (#18153)This commits makes Airflow 2.2 migrations compatible with 2.1.4 so users caneasily upgrade,1
Add C2FO to ``INTHEWILD.md`` (#18157),1
"Make auto refresh interval configurable (#18107)closes: #18069We accidentally DDOS'd our own webserver yesterday 😄 More details in the issue on how exactly this happened, but the gist is that auto-refresh can be a significant strain if there are many active tasks whose statuses must be polled. Auto-refresh is a wonderful feature but we wanted to be able to lengthen the interval to protect against this.On main, the interval is hard-coded to 3 seconds. I'm proposing we add a new webserver config variable that will allow this interval to be customized.",1
Fix Airflow version for `[logging] worker_log_server_port` (#18158)This will be released in 2.2.0 not 2.3.0,2
Use parameters instead of params (#18143),2
Add script that validate version fields in config.yaml (#18130)* Add script that validate version fields in config.yaml* fixup! Add script that validate version fields in config.yaml* fixup! fixup! Add script that validate version fields in config.yaml,5
"Remove limits for dnspython (#18162)I believe, while the root cause of the problem is still not solved,the optimisation in providers's manager imports implementedin #18052 might not trigger the SSL error when examples arehighlighted. The only way the SSL error was triggered was insideimport done by sphinx and currently that import should notimport wtforms (and email/dns transitively).",2
Allow users to submit issues for 2.2.0beta1 (#18165)This will allow users to test 2.2.0beta1 and we can track them in a better way,1
"Improves quick-start docker-compose warnings and documentation (#18164)The recently updated docker-compose had a bit broken behaviourfor non-Linux users. It expected the .env file to be createdalways, but the instructions to create them were not workingon Windows. This fixes the problem by turning the errorinto warning, and directing the users to the right instructionsper operating system.Also the recent ``DUMB_INIT_SESS_ID`` was added for worker toallow to handle signals properly also in our quick-startdocker-compose.",2
Fix spelling mistake in documentation (#18167),2
Apply parent dag permissions to subdags. (#18160)This fixes a bug where users can't access a subdag even when they have access to the parent dag.closes: #17652related: #7752,2
"Allow publishing Docker images with more pre-release versions (#18170)Without it, it currently fails with the following:```Building and pushing 2.2.0b1 Airflow PROD image for 3.9ERROR: Bad value for install-airflow-version: '2.2.0b1'. Only numerical versions allowed for PROD image here'!ERROR: The previous step completed with error. Please take a look at output above###########################################################################################                   EXITING WITH STATUS CODE 1###########################################################################################Finished the script build_dockerhub.shElapsed time spent in the script: 0 secondsExit code 1```",2
"Separate Installing from sources section and add more details (#18171)This PR separate installing Airflow from sources section and also fixes links for binary source, it had `-bin` suffix which we don't use anymore. And I have added section on verifying integrity. And add more details with examples",1
Remove redundant single quote from Breeze build image script (#18173)Before:```ERROR: Bad value for install-airflow-version: '2.2.0b1'. Only numerical versions allowed for PROD image here'!```After:```ERROR: Bad value for install-airflow-version: '2.2.0b1'. Only numerical versions allowed for PROD image here !```,1
"Set task state to failed when pod is DELETED while running (#18095)There is a bug in the Kubernetes Job Watcher that occurs when a node with a running worker pod is removed from the cluster. If the worker pod doesn't complete before the node is removed, it is orphaned and forced deleted by the garbage collector. This is communicated by the API with a status='Running' but an event with type='DELETED'Because in the if statement the Job Watcher doesn't check the event type, the last information we get from the pod is that is it running. The running scheduler never gets any information about the pod and shows it as stuck in a queued state. This situation is fixed when the scheduler/executor restarts and this function is run.",1
Fix usage of ``range(len())`` to ``enumerate`` (#18174)https://www.reddit.com/r/learnpython/comments/4af432/who_is_teaching_beginners_to_use_rangeleniterable/ has good reasons :)Also https://www.reddit.com/r/learnpython/comments/nn0il2/rangelens_vs_enumerate/ has more details and we had pylint disables at few places which are fixed,0
Update various pre-commits (#18176)```black: 21.7b0 -> 21.8b0blacken-docs: v1.10.0 -> v1.11.0pyupgrade: v2.23.3 -> v2.25.1yamllint: v1.26.2 -> v1.26.3flynt: 0.66 -> 0.69```,2
Add note about params on trigger DAG page (#18166),2
Fix typo in task fail migration (#18180)`faskfail` -> `task fail`,0
Remove Brent from Collaborators (#18182)Brent is already a committer so we don't this entry here. It was needed only when he was not.,1
Fix broken link in ``dev/REFRESHING_CI_CACHE.md`` (#18181)The link was broken,2
Doc: Use ``closer.lua`` script for downloading sources (#18179)- Follows first point of https://infra.apache.org/release-download-pages.html#download-page to use `https://www.apache.org/dyn/closer.lua/PROJECT/VERSION/SOURCE-RELEASE` for mirrors- Fixes bug as the current version substitution does not work for Hyperlinks (open PR: https://github.com/adamtheturtle/sphinx-substitution-extensions/issues/178),0
Fix minor issues in Airflow release guide (#18177)Fixed wrong links and wrong artefact name,0
Deprecate default pod name in EKSPodOperator (#18036),1
sets encoding to utf-8 by default while reading task logs (#17965),2
"Automatically create section when migrating config (#16814)Previously, if a config is migrated to a new section, the migration codewould crash with NoSectionError if the user does not add that section toairflow.cfg after upgrading Airflow. This patch automatically creates anempty section when that happens to avoid Airflow from crashing.",1
"Add official download page for providers (#18187)In order to fullfill requirements of announce@apache.org we needto have separate download page with instructions on how to downloadand verify the packages - links should point to mirroring systemused by Apache.This PR adds the capability, and since we already have couple ofdoc .rst files in each provider, the PR also adds the verificationif the expected files are there in the documentation.",2
Fix typo in decorator test (#18191)unknow --> unknown,3
"Adding Variable.update method and improving detection of variable key collisions (#18159)* Update Variable.set() method to take a description argument.* Update Variable.setdefault() method to take a description argument.* Adding a Variable.update() method which will throw a KeyError if the Variable doesn't exist, and an AttributeError if it   doesn't exist in the Database (since a non-metastore Variable can't be modified)* Improved logging around key collisions between different variable backends.*  Updated documentation to warn users about key collisions between variable backends.* If a user has a duplicated key in the metastore and an extra secrets backend, then updates to the Variable will update the   value in the metastore, but reads will read the value in the additional backend.",1
Doc: Improve installing from sources (#18194)Based on https://github.com/apache/airflow/pull/18187/#discussion_r706861500,1
Added upsert method on S3ToRedshift operator (#18027),1
Rename LocalToAzureDataLakeStorageOperator to LocalFilesystemToADLSOperator (#18168),5
Fix error when create external table using table resource (#17998),1
Swap dag import error dropdown icons (#18207)- Open/Close icons were backwards. This swaps them to be consistent with the expand/contract the whole dag error banner,0
Migrate Google Cloud Build from Discovery API to Python SDK (#18184),5
Add heartbeat to TriggererJob (#18129),1
BugFix: Wipe ``next_kwargs`` and ``next_method`` on task failure (#18210)This allows them to not persist between retries. Fixes #18146.,0
Upgrade ``importlib-resources`` version  (#18209)Co-authored-by: Braden McKallagat <braden.mckallagat@gmail.com>Closes #18155,2
Omit ``airflow._vendor`` package in coverage report (#18221),3
Fix failing main due to #18209  (#18215)* Use mock in test_hooks mock import spec* comment on mockCo-authored-by: Braden McKallagat <braden.mckallagat@gmail.com>,2
"Chart: Allow running and waiting for DB Migrations using default image (#18218)This keeps the same behaviour but allows setting `useDefaultImageForMigration` to `true` which will then use the default image (set by `.Values.defaultAirflowRepository` and `.Values.defaultAirflowTag`) to run and wait for DB migration.This safeguards issues with dependency and other user codes like badly written dags, etc",2
"Doc change: XCOM / Taskflow (#18212)Added a description and code sample of going FROM normal Airflow Operators TO Taskflow Operators, passing in XCOMs",4
Add some basic metrics to the Triggerer (#18214),1
Advanced Params using json-schema (#17100),5
Remove loading dots even when last run data is empty (#18230)Loading dots are displayed on the last run column while we get the last run data for each dag.But the dots are only removed if there actually is data. But if you haven't had any dag runs yet it would always appear to be loading. Instead the loading dots should be removed even when the response is empty.,4
Return explicit error on user-add for duplicated email (#18224)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Don't check for `__init__.py` under pycache folders. (#18238)Otherwise we end up with errors like this from pre-commit:```No __init__.py file was found in the following provider directories:/home/ash/code/airflow/airflow/airflow/providers/airbyte/__pycache__/home/ash/code/airflow/airflow/airflow/providers/airbyte/hooks/__pycache__/home/ash/code/airflow/airflow/airflow/providers/airbyte/operators/__pycache__```,1
Fix example dag of PostgresOperator (#18236)* Fix example dag of PostgresOperator,1
Revert Changes to ``importlib-resources`` (#18250)Revert changes in #18209 and #18215 as it is causing issues and I merged that PR having missed that test was failing,0
Make sure create_user arguments are keyword-ed (#18248),1
"Improves installing from sources pages for all components (#18251)* Shorter menu sections for installation page* Added ""installing from sources"" for Helm Chart* Added Providers summary page for all provider packages* Added scripts to verify PyPI packages with gpg/sha",1
"Reduce lengths of the name, username and email fields for this test (#18263)",3
Adding missing `replace` param in docstring (#18241),2
Fix deleting of zipped Dags in Serialized Dag Table (#18243)The file locations of DAGs in zipped folders are not correctly listed when removing deleted dags fromserialized dag table thus the delete query for deleting deleted dags from serialized DAGs is deletingdags in zipped folders. Likewise DagCode.remove_deleted_codeThis PR fixes it by listing all the file paths as stored in SDM so that the delete query will work properlyCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Fix external_executor_id not being set for manually run jobs (#17207),1
Sort adopted tasks in _check_for_stalled_adopted_tasks method (#18208)This PR adds sorting in adopted_tasks_timeout to ensure we correctlyclear stalled adopted tasks,1
"Fix DB session handling in XCom.set. (#18240)Since the function has the `@provide_session` decorator it should not becommitting the session (the decorator handles that if it's not passed asession) and nor should it be calling expunge_all -- that detaches allobjects from the session which is just not needed (or right) behaviourform setting an XCom value.By using the `session` fixture we get the transaction automaticallyrolled back, so we don't need any setup/teardown methods",1
"Fix dag_run FK check in pre-db upgrade (#18266)The check was broken as it was using the models, which now have`execution_date` as an associationproxy. This fixes the problem by usingthe introspected tables to query instead of the models.",1
Add ``triggerer`` to ``./breeze start-airflow`` command (#18259)This adds ``airflow triggerer`` command to ``./breeze start-airflow``,1
"show next run if not none (#18273)- Instead of `none` which can sometimes end up as `invalid date` We will only show the next run if it exists and otherwise leave the place blank, which is a cleaner UI anyway.",4
"remove all reference to date in dropdown tests (#18271)Sometimes there can be a race condition between the date calculated by the test and the one made by the test renderer.To prevent this possibility, the test is no rewritten to test the same logic but only checking the timezone tags instead of the full date.",5
Add metrics docs for triggerer metrics (#18254)Follow-on from https://github.com/apache/airflow/pull/18214,2
"Use try/except when closing temporary file in task_runner (#18269)Occasionally we get FileNotFoundError when calling close on the named temporary file,the reason I believe is that the file has been removed by the subprocess before we callclose on it.When NamedTemporary file is instantiated  with delete=True, calling close on the file deletes the fileThis PR adds a try/except when calling the close method to capture the error",0
Improved log handling for zombie tasks (#18277),2
"Silence warnings in tests from using SubDagOperator (#18275)And in order to track these down I had to correct the stacklevel -- +1for the provide_session, and +1 for the BaseOperator metaclass.",1
Fix web view rendering errors without a DAG run (#18244)* Make sure task view can be rendered without a ti* Ensure rendered view works without DAG run* Fix table when ti is not available in task view* Tests for task view without DAG run,1
Fix provider test acessing importlib-resources (#18228),2
"Make `XCom.get_one` return full, not abbreviated values (#18274)If you used this class method directly (such as in a custom operatorlink) then the value would _always_ be subject to the`orm_deserialize_value` which would likely give the wrong result oncustom XCom backends.This wasn't a problem for anyone using `ti.xcom_pull` as it handled thisdirectly.",0
Fixed wasb hook attempting to create container when getting a blob client (#18287)Co-authored-by: ignas.kizelevicius <ignas.kizelevicius@beyondanalysis.net>,1
Chart: Use stable API versions where available (#17211),1
"Chart: Fix applying labels on Triggerer (#18299)Currently it errors with:```{""result"":{""message"":""YAML parse error on airflow/charts/airflow/templates/triggerer/triggerer-deployment.yaml: error converting YAML to JSON: yaml: line 30: mapping values are not allowed in this context""},""deployment"":{}}```This PR fixes that and adds unit tests around it.",3
Fix Chart doc build (#18302)Error on main:``` WARNING: Title underline too short.```,2
Run Docs build when updating `chart/UPDATING.rst` (#18303)Run Docs build when updating `chart/UPDATING.rst`,5
Add information about differencies of deps in eager-upgrade (#18293)Whenever tests fail in case of eager-upgrade we print now the diffof the dependencies used in the build versus stable set of constraintsThis helps in cases when new dependency released cause test failures- this way you can immediately see which dependencies were upgradedand what might be the possible cause. Also additional informationis printed in order to guide the PR owners to understand wherethe test failures could came from.,0
Added Bentego to INTHEWILD.md (#18301)Added Bentego to INTHEWILD.md,1
"Pass exception to ``run_finished_callback`` for Debug Executor (#17983)When running the Debug Executor, the context inside the `on_failure_callback` method doesn't have the `exception` object.This is because the `exception` is not passed to `run_finished_callback` with the Debug Executor",0
Fix spelling on chart docs (#18308),2
Build docs when chart changelog or values schema changes (#18307),4
Add 2.2.0b1 changelog (#18205),4
"Fix mini scheduler not respecting wait_for_downstream (#18310)When wait_for_downstream is set on a task, mini scheduler doesn't respect itand goes ahead to schedule unrunnable task instances.This PR fixes it by checking the dependency in mini schedulerCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
"Refactor installation pages (#18282)This PR splits ad improves the installation-related documentationfor Airflow. The ""installation"" page had become overloadedwiht everything-but-the-kitchen-sink and it became ratherdifficult to navigate and link to relevant sections.Also there was not a single page where one could have an overviewon different installation methods possible, cases wheneach instalation works best as well as understanding whatis involved in following each installation method in terms ofmaintenance, and expectations that users should have when itcomes to what Apache Airflow Community provides.The PR leaves the installation page as basically a summary ofall installation methods with all above explained and linksto detailed pages explaining prerequisites, dependencies,database setup and supported versions.",1
Chart: Ability to access http k8s via multiple hostnames (#18257)Add support for accessing the airflow ui and the flower ui (ifapplicable) from multiple hostnames when install via helm chart onto akubernetes cluster.closes: #18216.,2
Chart: less fragile webserver deployment tests (#18332)Simply make the log persistence tests be less fragile by not caringabout volume and volume mount order.,3
"Fix mini scheduler not respecting ``wait_for_downstream`` dep (#18338)When ``wait_for_downstream`` is set on a task, mini scheduler doesn't respect itand goes ahead to schedule unrunnable task instances.This PR fixes it by including all direct downstream tasks so that ``wait_for_downstream`` check works correctly.Resolves https://github.com/apache/airflow/pull/18310#discussion_r711101901Closes: #18229",0
Move DB call out of ``DatabricksHook.__init__`` (#18339)Because of that using `DatabricksHook` in DAG files causes tons of issues as DB calls are made everytime files is parsed.,2
"Graceful scheduler shutdown on error (#18092)closes: https://github.com/apache/airflow/issues/18096This solves a potential rare occurrence of a process deadlock when using logs serving.If the scheduler, for any reason, were to encounter an unrecoverable error (such as a loss of connectivity to the database, or anything forcing the process to exit), the serve_logs subprocess would not be properly terminated.Thus, the process hangs instead of gracefully shutting down, thus requiring external restart.By ensuring the `.terminate()` function is _always_ called in case of a failure, the parent process can properly shut itself down.",0
Fix kinesis test (#18337),3
"Added more automation on latest version (#18335)Some of the versions stored in docs have to be stored asverbatim, rather than as |version| placeholder - they have to referto latest released airflow version.This PR automates that:When a new version of airflow is set in setup.py, the Dockerfilesand relevant other documentation files are using that version.However for tests, the version in Dockerfile is replaced withlastest version that has been released - just to makesure that the Dockerfiles are still valid and they can buildwith the latest released version.",3
Fix breeze failing breeze on missing docker config (#18330),5
"Dag bulk_sync_to_db dag_tag only remove not exists (#8231)* Dag bulk_sync_to_db dag_tag only remove not existsFor now we remove all record in dag_tag, but actuallywe only need to delete tag not exists in dag fileanymore",2
Initial commit (#18203),5
"Remove 2017-latest from supported MSSQL versions (#18345)Seems that the 2017-latest version of MSSQL has some problems withlocking and continuously fails in `main`. Actually we do not haveto support 2017 version, it's perfectly ok to support only2019 version as we had not made any promise yet to our users.",1
"Remove check for at least one schema in GCSToBigquery (#18150)For the case when updating an existing table or insert data to a particular partition, no schema is needed.Autodetect doesn't always work, e.g. cannot distinguish partition correctly. Other options requires forking the schema to airflow.",1
"Explain sentry default environment variable for subprocess hook (#18346)* Explain sentry default environment variable for subprocess hookSentry by default monkey-patches standard library POpen functionto capture and pass current process' environment to subprocesswith `SUBPROCESS_` prefix. This break SubprocessHook testswhen sentry tests are run before SubprocessHook tests, and alsoit modifies SubprocessHook behaviour (and promise) in productionenvironment as well.This PR:* adds documentation to both sentry documentation and the  SubprocessHook documentation explaining the interaction between  the two* Adds documentation explaining how to disable this default  Sentry behaviour* disables default integrations in the Sentry tests to avoid  side-effectsFixes: #18268",0
Add IAM Role Credentials to S3ToRedshiftTransfer and RedshiftToS3Transfer (#18156),1
ECSOperator realtime logging (#17626),2
fix get_connections deprecation warning in webhdfs hook (#18331),1
add sla_miss_callback section to the documentation (#18305),2
Improve coverage for airflow.security.kerberos module (#18258)* Improve coverage for airflow.security.kerberos module* fixup! Improve coverage for airflow.security.kerberos module* fixup! fixup! Improve coverage for airflow.security.kerberos module,3
"Deactivating DAGs which have been removed from files (#17121)<!--Thank you for contributing! Please make sure that your code changesare covered with tests. And in case of new features or big changesremember to adjust the documentation.Feel free to ping committers for the review!In case of existing issue, reference it using one of the following:closes: #ISSUErelated: #ISSUEHow to write a good git commit message:http://chris.beams.io/posts/git-commit/-->Closes: https://github.com/apache/airflow/issues/11901Closes: https://github.com/apache/airflow/issues/17516Ensuring that the active DAGs in the DB are all actually present in their corresponding python files by reconciling the DB state and with the contents of a given DAG file on every parse operation. This _should_ prevent a lot of issues encountered when writing multiple DAGs per-file, renaming DAGs or dynamically generating DAGs based on a config file read at parse-time.This will cause a few other functional changes:1) DAGs will disappear from the UI if an ImportError is introduced in the underlying file.2) DAGs will no longer be re-marked as active if they are inactive / missing but the file is present.3) DAGs could be incorrectly marked inactive if there's an unexpected parsing error (I guess this is a tradeoff between false-poitives and false-negatives)#### Validation:I have validated these changes in a local breeze environment with the following DAG:```pythonfrom airflow.models import DAGfrom airflow import utilsfrom airflow.operators.python import PythonOperatorNUM_DAGS=1def message():    print('Hello, world.')for i in range(NUM_DAGS):    with DAG(f'dag-{i}', schedule_interval=None, start_date=utils.dates.days_ago(1)) as dag:        task = PythonOperator(            task_id='task',            python_callable=message        )        globals()[f""dag_{i}""] = dag```By changing the value of `NUM_DAGS` I can quickly change the number of DAG objects present in this file. Before this change, decreasing the value of `NUM_DAGS` would leave a bunch of stale DAGs in the UI. These could be triggered but would then fail as the executor was not able to load the specified task from the file.After implementing this change, stale DAGs disappear from the UI shortly after decreasing the value of `NUM_DAGS`.#### Questions:1. Is there a good reason for Airflow to mark inactive DAGs as active if the file still exists? I looked through the [original PR which introduced this](https://github.com/apache/airflow/pull/5743/files) but couldn't find an explanation.2. How significant is the performance hit incurred by updating the DAG table on every parse operation? 3. Will this change introduce any other functional changes that I haven't mentioned above?#### Follow up:If this PR is merged, we should also add some documentation which discusses dynamically generated DAGs, and explains how to generate multiple DAGs in a loop as shown in the example above.",2
Inclusive Language (#18349),5
"Do not display resource statistics during parallell jobs (#18344)Those stats are useful to analyse problems but they areverbose and misleading for ""normal"" runs. The statsafter this PR will only be displayed in `push` events orin case committers manually set ""debug-ci-resources"" labelon PR",0
Fix broken static check on main (#18351)This got broken in https://github.com/apache/airflow/pull/18349,0
Chart: Update the default Airflow version to ``2.1.4`` (#18354)Since 2.1.4 is out we should use that as the default Airflow version.,1
"Doc: Fix broken ``asc`` download link (#18350)There was a typo, hence the download link was broken",2
Fix links in releasing guide (#18352)This fixes minor issues in releasing guide,0
Update version added field in config after 2.1.4 release (#18355),5
"Add Changelog and other 2.1.4 details (#18353)Updates `CHANGELOG`, `UPDATING.md`, `README.md` and adds `2.1.4` in Bug report template",0
"Creating ADF pipeline run operator, sensor + ADF custom conn fields (#17885)",1
Improve coverage for airflow.cli package (#18220),3
"Set default branch in codecov.yml (#18361)The codecov uses master as default branch, See: https://docs.codecov.com/docs/codecov-yaml#master-copyHopefully this will solve the codecov stats missing problem.",0
Add support for templated fields in PapermillOperator (#18357),1
"Fix random deadlocks in MSSQL database (#18362)Default isolation level for the MSSQL database is READ_COMMITTEDbut its implementation in MSSQL might cause random deadlocks forselect queries - because by default MSSQL uses shared locksfor READ_COMMITTED transaction isolation that might causedeadlocks with the UPDATE statements which create exclusivelocks.This change switches the MSSQL DB used during tests to haveREAD_COMMITTED_SNAPSHOTS enabled and it adds check at the Airflowstartup to make sure that if the MSSQL database is used, theREAD_COMMITTED_SNAPSHOTS is set and documentation is added todescribe it.It also allows to revert removal of 2017 version of mssql whichwas done in the faith that removal of 2017 will remove theflaky random test failures.Also Fixes: #17018",0
Show Triggers table in Webserver (#17876)This PR adds a view for Triggers table and adds permissions for that view.,1
"Don't permanently add zip DAGs to ``sys.path`` (#18384)Only leave the zip on `sys.path` while parsing that zip file, as weshouldn't assume parsing only happens in separate processes. Forexample, `resetdb` is used to run migrations, some of which parsethe DAGs.",2
"Fix stuck ""queued"" tasks in KubernetesExecutor (#18152)There are a set of circumstances where TaskInstances can get ""stuck"" in the QUEUED state when they are running under KubernetesExecutor, where they claim to have a pod scheduled (and so are queued) but do not actually have one, and so sit there forever.It appears this happens occasionally with reschedule sensors and now more often with deferrable tasks, when the task instance defers/reschedules and then resumes before the old pod has vanished. It would also, I believe, happen when the Executor hard-exits with items still in its internal queues.There was a pre-existing method in there to clean up stuck queued tasks, but it only ran once, on executor start. I have modified it to be safe to run periodically (by teaching it not to touch things that the executor looked at recently), and then made it run every so often (60 seconds by default).This is not a perfect fix - the only real fix would be to have far more detailed state tracking as part of TaskInstance or another table, and re-architect the KubernetesExecutor. However, this should reduce the number of times this happens very signficantly, so it should do for now.",0
"Add a Docker Taskflow decorator (#15330)Add the ability to run @task.docker on a python function and turn it into a DockerOperator that can run that python function remotely.```@task.docker(    image=""quay.io/bitnami/python:3.8.8"",    force_pull=True,    docker_url=""unix://var/run/docker.sock"",    network_mode=""bridge"",    api_version='auto',)def f():    import random    return [random.random() for i in range(10000000)]```One notable aspect of this architecture is that we had to build it to make as few assumptions about user setups as possible. We could not share a volume between the worker and the container as this would break if the user runs the airflow worker on a docker container. We could not assume that users would have any specialized system libraries on their images (this implementation only requires python 3 and bash).To work with these requirements, we use base64 encoding to store a jinja generated python file and inputs (which are generated using the same functions used by the PythonVirtualEnvOperator). Once the container starts, it uses these environment variables to deserialize the strings, run the function, and store the result in a file located at /tmp/script.out.Once the function completes, we create a sleep loop until the DockerOperator retrieves the result via docker's get_archive API. This result can then be deserialized using pickle and sent to Airflow's XCom library in the same fashion as a python or python_virtualenv result.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
Added example JSON for airflow pools import (#18376)Co-authored-by: Bas Harenslak <bas@astronomer.io>,2
Fix BigQuery system test (#18373),3
"Display alert messages on dashboard from local settings (#18284)Allow alert messages to be displayed on the UI dashboard from`airflow_local_settings`. This feature can be used to announce things toall users, warn about setup limitations/issues, and probably otherthings I haven't considered.The message, category, and role(s) the flash message should be shown tocan be configured.Here is an example:In `airflow_local_settings`:```from airflow.www.utils import UIAlertDASHBOARD_FLASH_MESSAGES = [    UIAlert(""hello"", category=""error""),    UIAlert(""world"", roles=[""Viewer"", ""Admin""]),    UIAlert('Visit <a href=""https://airflow.apache.org"">airflow.apache.org</a>'), html=True),    UIAlert(""Only for users"", roles=[""Users""]),]```View as an Admin user:![Screen Shot 2021-09-15 at 5 28 49 PM](https://user-images.githubusercontent.com/66968678/133526478-9aa12dde-5c31-4be9-a9ae-4e31141630d4.png)View as a User user:![Screen Shot 2021-09-15 at 5 35 42 PM](https://user-images.githubusercontent.com/66968678/133526629-2567e977-87ac-4a23-a28a-94ae7a5d7d30.png)related: #18098",1
"Add PGBouncer recommendation in ""setup-database' doc. (#18399)We were recommending using PGBouncer for all Postgres installationfor quite some time at least verbally but also in the Helm Chartdocumentation. However we missed such recommendation in thegeneral Postgres area of 'Setting Up the database` doc.This PR adds a note that we can refer to when explainingproblems with connections and stability to the users whouse Postgres without PGBouncer proxy (which is known to helpin such cases)",1
"Production-level support for MSSQL (#18382)MSSQL has been somewhat experimental in the `main` branch, but aswe near releasing for 2.2.0 version, the image should supportthe mssql at the level as it supports other databases.This PR adds proper support for both PROD and CI images.",1
"Stop using docker manifest to check for image presence (#17883)We need to set the ""experimental"" flag in CI in order to use`docker manifest` command to check for presence of the imagesin ghcr.io. In order to use them we need to enable experimentalfeatures via ~/.docker/config.json.Sometimes, very rarely, we had the case that the config filegot broken and the problem turned out to be that we triedto do this experimental replacement in parallel by severalrunning ""wait image"" commands (:facepalm: here for myself)that were apparenlty overriding the same config.jsonat the same time in non-atomic way, which (very rarely)led to corrupted file.However for quite some time we pulled the image immediatelyafter it was available, in order to verify the image,so rather than checking if the imageis there via manifest, we can simply pull the imageand effect will be the same - if it fails, the image is notthere, if it has been pulled - we can immediately verifyit. We do not need experimental flag at all for thatso no messing around with .docker/config.json is neededat all.",5
Always draw borders if task instance state is null or undefined (#18033)* Always draw borders if task instance state is null* Add external trigger information* Include undefined states* Disable mouseover effect for manual runs* Define common properties in one place,1
Add docs for AIP 39: Timetables (#17552),2
Add common parameter to example URI for MSSQL (#18404),2
Labels on job templates (#18403),5
Add ODBC extra for the production image (#18407)The ODBC extra has been missing from #18382. This PR adds themissing extra and verifies if pyodbc is importable in the PRODimage.,2
Update boto3 to <1.19 (#18389),5
"Properly handle ti state difference between executor and scheduler (#17819)When a task fails to start, the executor fails it and its state inscheduler is queued while its state in executor is failed. Currentlywe fail this task without retries to avoid getting stuck.This PR changes this to only fail the task if the callback cannot beexecuted. This ensures the task does not get stuckcloses: #16625Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",1
"Make Kubernetes job description fit on one log line (#18377)Currently, when the Kubernetes executor creates a pod it prints a dictionary description of the pod across many lines (can easily be 20+ lines). This is fine if you're reading the log in a stream in a text file, but throws off log search tools like Kibana. A better practice would be to print the whole pod description on a single line. It is quite easy to prettify a dictionary if one wants to see it back in a more human-friendly form with the newlines.This update simply forces the log from this command into a single line.",2
"Explain scheduler fine-tuning better (#18356)* Explain scheduler fine-tuning betterA lot of users have an expectations that Airflow Scheduler will`just work` and deliver the `optimal performance` for them withoutrealising that in case of such comples systems as Airflow is youoften have to decide what you should optimise for or accept sometrade-offs or increase hardware capacity if you are not willing tomake those trade-offs.Also it's not clear where the responsibilityis - should it `just work` or should the user be responsible forunderstanding and fine tuning their system (both approaches arepossible, there are some complex systmes which utilise a lot ofautomation/AI etc. to fine tune and optmise their behaviour butAirflow expects from the users to know a bit more on how thescheduling works and Airflow maintainers deliver a lot ofknobs that can be turned to fine tune the system and to maketrade-off decisions. This was not explicitely stated in ourdocumentation and users could have different expectations aboutit (and they often had judging from issues they raised).This PR adds a ""fine-tuning"" chapter that aims to set theexpectations of the users at the right level - it explains whatAirflow provides, but also what is the user's responsibility - todecide what they are optimising, to see where their bottlenecksare and to decide if they need to change the configuration orincrease hardware capacity (or make appropriate trade-offs).It also brings more of the fine-tuning parameters to the`tuneables` section of scheduler, based on some of the recentquestions asked by the users - seems that having a specificoverview of all performance-impacting parameters is a good idea,and we only had a very limited subset of those.Some user prefer `watch` rather than read that's why this PRalso adds the link to the recording of talk from theAirlfow Summit 2021 where Ash describes - in a very conciseand easy to grasp way - all the whys and hows of the scheduler.If you understand why and how the scheduler does what it does,fine-tuning decisions are much easier.* fixup! Explain scheduler fine-tuning better",1
Require can_edit on DAG privileges to modify TaskInstances and DagRuns (#16634),2
Update 2.2.0 changelog for b2 (#18417),4
Fix ``docker-stack`` docs build (#18419),2
Add Opus Interactive to INTHEWILD (#18423),1
"Add driver parameter for MsSQL URL for production image in docs (#18426)The production image uses ODBC Driver to connect to mssql, and youneed to specify the driver explicitely to make it works.",1
Chart Doc: Delete extra space in adding connections doc (#18424)Extra space in secret keys was causing an error while parsing values.yaml,0
add extraContainers for migrateDatabaseJob (#18379),5
Fix image retagging when pulling the images (#18433)The change #17883 missed re-tagging images after downloadingwhich caused unnecessary image rebuilding for PRs as well as usingwrong image for doc building - previous version of docs were usedwhen building docs!,2
Switch to latest version of PGBouncer-Exporter (#18429)The latest version of pgbouncer exporter have been updated inhttps://github.com/apache/airflow-pgbouncer-exporter/pull/3 andpushed. This PR switches to this exporter version.,5
Custom timetable must return aware datetimes (#18420),5
"Add cascade to DagRun/TaskInstance relationship (#18434)* Add cascade to DagRun/TaskInstance relationshipWe currently have issue where deleting dagruns causes a dependency error in Sqlalchemy becausethe session doesn't know what to do with the related taskinstances.This PR adds cascade so that when a dagrun is marked for deletion, the related taskinstancesare also deleted",4
Properly fix dagrun update state endpoint (#18370)The dagrun update state endpoint was recently added but is not workingas expected. This PR fixes it to work exactly like the UI mark dagrun state API,2
Fix task instance url in webserver utils (#18418)execution_date was omitted when generating the task instance url.This PR fixes it,0
Protect agains images not being pulled properly (#18435)Recently we had a problem that our CI got broken becausepulled images were not tagged properly after #17883 missed imagetagging. This has been fixed in #18433 but the problem is that thismight happen in the future and mignt not get noticed on time.This PR prevents from similar situations happnening. Whenever wetry to run doc building or tests we set --pull policy to neverfor both docker and docker compose which should simply fail ifthe images were not pulled and tagged properly rather thanfail over to pulling latest `main` image.,3
Limit azure-storage-blob due to breaking changes in 12.9.0 (#18443),4
"Doc: Fix remote name in Airflow publishing guide (#18451)Avoid errors such as below:```❯ git checkout constraints-${VERSION_CONSTRAINT_BRANCH}hint: If you meant to check out a remote tracking branch on, e.g. 'origin',hint: you can do so by fully qualifying the name with the --track option:hint:hint:     git checkout --track origin/<name>hint:hint: If you'd like to always have checkouts of an ambiguous <name> preferhint: one remote, e.g. the 'origin' remote, consider settinghint: checkout.defaultRemote=origin in your config.fatal: 'constraints-2-2' matched multiple (2) remote tracking branches```",5
"Avoid importing DAGs during clean DB installation (#18450)One of our migration (adding max_tries) creates the DagBag andeffectively parses all the DAGS while performing the migraiions.While this is needed in case you migrate from an older version ofthe DB where you have some TaskInstances requiring migration, foran empty database, there are no TaskInstance entities in the DBand parsing of all the DAGs is not needed.Actually it can even be harmful. If you attempt to run AirflowMigration on an empty dataase and your DAGs will be erroneous,your migration will break in the middle and you have noclue what caused it = see #18408 for an example of such failure.This PR skips DagBag import when there are no TaskInstance inthe database to migrate.Fixes: #18449",0
Allow users to submit bugs for 2.2.0b2 (#18452),0
"Remove unnecessary css state colors (#18461)The true source of truth for state colors somes from `settings.py` and is exposed to the frontend through the webserver. The css  state colors are redundant and cause confusion on the source of truth. Really, they are left over from when colors were not defined by `settings.py` (going back to the first airflow commit).Also, cleaned up some indentation for making the legends in the tree and graph html filesCloses #17303",2
"Fix task group tooltip (#18406)The task group tooltip wasn't just not loading the right info, but it wasn't rendering at all. This PR fixes that.Also:- add `deferred` as a group node status- show all possible status summaries in the group tooltip but only show them if the count is >0",1
"Add fixing of ownership also for sources in Breeze (#18464)In some cases files created inside the container on linux insources are owned by root - so far we cleaned only some directoriesthat we knew were touched by Airflow, but in case of branchswitching some folders/files might also got created as ownedby root.This PR adds AIRFLOW_SOURCES and ""dags"" folders to the listof folders to fix ownership of.This might lead to slightly longer exit time when exitingfrom Breeze, but since we are running it only when the host isLinux and the fixung is rather performant - with explicitly findingfiles and dirs owned by root, this should not be a problem.It would be much bigger problem on MacOS,Windows due to slow filesystem,but on MacOS it is not needed as there ownership mapping is doneusing the filesystem itself.",5
Cope with `@task.docker` decorated function not returning anything (#18463),1
"Bring back version to docker-compose files (#18472)Even if version is only for information and should not beused, apparently it's removal causes some problems with differentdocker versions.",2
"Revert ""Fix task instance url in webserver utils"" and fix properly (#18444)* Revert ""Fix task instance url in webserver utils (#18418)""This reverts commit d50cfd150de327a72136ec7e5f5abe48045ec164.* Update airflow/www/utils.py* Update airflow/www/utils.pyCo-authored-by: Tzu-ping Chung <tp@astronomer.io>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",5
Restore 'filename' to template_fields (#18466),2
Add index to the dataset name to have separate dataset for each example DAG (#18459)Co-authored-by: Dmytro Khimich <khimich@google.com>,2
Correctly select ``DagRun.execution_date`` from db (#18421),5
Added Zalando to ``INTHEWILD.md`` (#18480),1
Remove redundant ``session.commit()`` in migration (#18453)`session.commit()` is redundant as we won't have anything to commit.,4
"Update best practices around deleting a task (#18483)""Never delete a task from a DAG."" is just a wrong recommendation. I have updated the guide.",5
Doc: Fix example timetable for AIP-39 howto (#18475)The wonderful QA team caught me doing stupid things. This fixes that. Plus:* Adding `dag_id` and `start_date` to the example DAG. This was also suggested by the QA team.* A bunch of tests making sure the example timetable is providing a sane schedule so I have less chances getting more mistakes merged 😛,7
Docs: Fix grammar in ``docs/apache-airflow/start/docker.rst`` (#18484)Fixed grammatical issues  in ``docs/apache-airflow/start/docker.rst``,2
"Chart: warn when webserver secret key isn't set (#18306)We will start warning chart admins when they deploy with the defaultrandom webserver secret key, as it can lead to unnecessary restarts ofthe Airflow components.",2
Proper handling of Account URL custom conn field in AzureBatchHook (#18456)* Proper handling of Account URL custom conn field,1
Add start date to ``trigger_dagrun`` operator (#18226)closes: #18082,1
Pin ``google-cloud-dataproc`` to ``<2.6.0`` due to removal of ``v1beta2`` client (#18486)Re: #18485The removal of the v1beta2 client from the google-cloud-dataproc library in release 2.6.0 makes dataproc operators unusable.We can get around this temporarily by pinning the installation of the library to the previous version.In a follow-up PR I can update all of the dataproc integrations to use the stable dataproc_v1 client.,5
Implement test to detect actions that are missing required security decorator. (#18467),1
"Don't use flash for ""same-page"" UI messages. (#18462)The flash is designed for setting a message on one page and then showingit after a redirect, so in the case of these UI warnings they were beingshown twice due to an early redirect.I could have chosen to fix this by moving the checks to after the`reset_tags` redirect, but it _also_ felt wrong to me to have HTML inthe view, so I have chosen to move it in to the template where itbelongs.To (marginally) reduce boilerplate I have created `message()` ""macro""(Jinja macro, a.k.a. function; not to be confused with what Airflowtemplates call a macro, but is in fact just a template global) thathandles the formatting for messages.",0
"Add more informative messages when rebuilding the image (#18496)Reviewed and updated the messages printed when image needed tobe rebuild. Some messages were unclear or duplicatd. I removedall the ambiguities and added some colors. Also there wasapparently the case that automated rebuild on commit did not takeinto account the check of whethere image needs to be pulled, thatcould lead to longer rebuild times in this case.",1
"Increase timeouts for setup/teardown in pytest (#18505)For some unknown reason recently some tests started to failon setup timout exceeding 20 seconds. Those tests have NO setupso it likely a side-effect of other tests (for example with someprocess cleanup that takes more time/CPU.This change attempts to fix it by simply increasing the timeoutto 60 seconds. If it works - fine, but if not, it will be indicationthat we have a deeper problem to fix.",0
"Add missing email type of connection (#18502)The email configuration of Airflow became somwhat hybrid - partof the configuration is in the config file and part (authentication)in connection. However there is no dedicated connection type touse for email, and user got confused - they did not realize theycould use any connection, and this was really not intuitive.This PR adds email connection type as built-in and updates thedocumentation of email configuration to use that connectiontype for smtp/sendgrid (and use the aws one for aws email)Fixes: #18495",0
Add ok.ru into the list of airflow users (#18507),1
Add Pathstream to INTHEWILD.md (#18508),1
Add missing type of tests to breeze. (#18504),3
Chart: 1.2.0 changelog (#18510),4
Rename AzureDataLakeStorage to ADLS (#18493)* Rename AzureDataLakeStorage to ADLS,5
Improve guidance to users telling them what to do on import timeout (#18478)* Improve guidance to users telling them what to do on import timeoutThe message about import timeut does not explain an average userwhat the root cause is. The updated one links to the docuementationon how to reduce tpo-level Python code overhead and reducecomplexity of their DAGs.,2
Add setting up using VS Code (#15631),1
18400 chart custom pod annotations (#18481),2
Improve error message for BranchPythonOperator when no task_id to follow (#18471)* Improve error message for BranchPythonOperator when no task_id to follow,1
Removing redundant relabeling of password conn field (#18386),4
Add Dcard to the list of companies using Airflow (#18521),1
Fixed naming in the Spark Connection Extra field (#18469)Changed the naming of the Extra fields from spark_home/spark_binary/deploy_mode to spark-home/spark-binary/deploy-mode to match what is actually in the codebase,4
"Added notification to solve ""docker-credential-service-error"" (#18524)",2
Inherit `AirflowNotFound` from `connextion.NotFound` for better readability (#18523),1
Add Slack operators how-to guide (#18525),1
Add guide for Apache Druid operators (#18527),1
Fix running helm tests in parallel (#18533)When Helm tests are run in parallel in one machine they mightoverride each-other's cache if they are run at exactly the sametime (and one of the caches might be gone)This led to an intermittent failures of those tests.This PR sets different temporary directory for each helm test run,1
"Improve isolation of MSSQL files (#18538)The MSSQL tests sometimes failed with ""unhealthy"" status. The reaso forthat was that our cleanup did not properly clean hidden files I think.We were potentially reusing the bind volumes for MSSQL tests, but itturned out that we did not remove hidden files from that volume and thatlikely caused problems accross 2017/2019 versions when the volumes werereused.This change implements two-fold protection against such case:* the volumes have different path for different MSSQL versions* hidden files are also removed",4
Remove workaround for docker-compose-failures (#18539)Long time ago we had unknown docker-compose failures that returned254 exit code. This has long been improved by decreasing memorypressure for CI dockers and healthiness checks and is nolonger needed.,2
Fix intermittent orphan test (#18530)The orphan scheduler test fails intermittently likely due tothe way how pytest runs child processes. The fix is to run thistest in a fork and pass the result of the test back to theparent process. This way we can be pretty sure that the processhas no extra children.,3
Excludes rightfullhy unlicensed files from chart from RAT check (#18547),2
Updating miscellaneous provider DAGs to use TaskFlow API where applicable (#18278),1
Fix kubernetes engine system test (#18548)- Add 'in_cluster=False' argument to 'GKEStartPodOperator'- Change order of activating service account and setting project,1
Add package filter info to Breeze build docs (#18550),2
"Fix ``DetachedInstanceError`` when dag_run attrs are accessed from ti (#18499)Loading a taskinstance doesn't load the corresponding dag_run withthe effect that when the dag_run attr is accessed from the ti it givesa DetachedInstanceError, dag_run is not bound to a session.This PR fixes it by making an extra query to get the dagrun using therelationship between the two objects",1
Updating the Elasticsearch example DAG to use the TaskFlow API (#18565),1
"Allow core Triggerer loops to yield control (#18552)In the case of having several hundred triggers, the core triggerercreation/deletion loops would block the main thread for several hundredmilliseconds and bring the event loop to a halt. This change allows themto yield control after every trigger they process, preventing this.",1
Doc: Fix typo in Triggerer docs (#18560),2
"Fetch PR labels from API for Build Images workflow (#18572)The event payload in GitHub does contain PR labels, but they are ""fixed""at the time the original event is triggered, meaning that if you re-runthe Build Image job at a later date (say after the ""full tests needed""label has been automatically applies) it will still see the same values.",3
"Give MSSQL container more time to start up (#18476)The MSSQL jobs on CI fail from time-to-time with a message from dockercompose saying ""the container is unhealthy"" -- this should give us moretime to start up before failing.Thankfully for the way docker healthchecks work if the container doesstart up and pass healthcheck while still in the start_period we don'thave to wait for the full period to elapse before continuing",4
Fix flaky redis tests (#18537)This test very rarely fails with sqlite on CI. Just retrying shouldget rid of the problem.,0
"Fix rendering nested task fields (#18516)When we are referring ``{{task}}` in jinja templates we can alsorefer some of the fields, which are templated. We are notable to solve all the problems with such rendering (specificallyrecursive rendering of the fields used in JINJA templating mightbe problematic. Currently whether you see original, or renderedfield depends solely on the sequence in templated_fields.However that would not even explain the rendering problemdescribed in #13559 where kwargs were defined after opargs andthe rendering of opargs **should** work. It turned out thatthe problem was with a change introduced in #8805 which madethe context effectively holds a DIFFERENT task than the currentone. Context held an original task, and the curren task wasactually a locked copy of it (to allow resolving upstreamargs before locking). As a result, any changes done byrendering templates were not visible in the task accessedvia {{ task }} jinja variable.This change replaces the the task stored in context with thesame copy that is then used later during execution so thatat least the ""sequential"" rendering works and templatedfields which are 'earlier' in the list of templated fieldscan be used (and render correctly) in the following fields.Fixes: #13559",0
Adding `task_group` to the BaseOperator docstring (#18564),2
"Allow airflow standard images to run in openshift utilising the official helm chart #18136 (#18147)This pull request adds the parameter rbac.createSCCRoleBinding.When enabled, a new RoleBinding object will be created targeting the various serviceAccounts utilised by deployments and pods.The roleBinding will target the system:openshift:scc:anyuid cluster role which allows for pods to start with any arbitrary uid.The ideal solution would also add the possibility of removing the security contexts altogether, however this would not be possible due to a number of pod templates setting different uids.I will investigate the possibility to add the option of removing the security contexts so that openshift can then set its own uid as intended.As it is, the pods will start and work as expected, utilising the predefined uids set in values file or the default image uid as set during build.The change is not intrusive and should work in the current workflow as is. The option is also set to false by default in order to not impact any existing setups.",1
Change location of bucket creation for Datastore (#18569)Co-authored-by: Dmytro Khimich <khimich@google.com>,5
Update s3_list.py (#18561)removed inappropriate character `{` from the error message,0
Move FABs base Security Manager into Airflow. (#16647)* Move FABs base Security Manager into Airflow.* Remove AbstractSecurityManager class.* Remove import of BaseManager class.* Add licenses* Add reference to FAB creator Daniel Vaz Gaspar.* Correct argument name in test.* Fix missing colon in docblock.* Add line to docblock so doc build passes.* Add __init__.py files to fab_security directories.,2
Operator help code optimisation (#18571),1
Add in Bidnamic to INTHEWILD.md (#18589),1
"Influxdb Hook (#17068)* Added transfer operator for bigquery to mssql and tests* Update airflow/providers/google/cloud/transfers/bigquery_to_mssql.pyCo-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* Update airflow/providers/google/cloud/transfers/bigquery_to_mssql.pyCode change based on review comments.Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>* Added comment in bigquery to mssql transfer* Added influx operator files* Delete bigquery_to_mssql.py* Update test_bigquery_to_bigquery.py* Delete test_bigquery_to_mssql.py* [14168] Added functions to create organization, bucket and run query to influxdb.* [14168] Added file that was accidentally deleted.* [14168] Added support to write Point.* [14168] Added Influx provider support* [14168] Added Influx provider support* [14168] Added test case for InfluxDBHook* [14168] Added test case for InfluxDBHook* [14168] Fixed test case for InfluxDBHook* [14168] Fixed test case for InfluxDBHook* [14168] Added example DAG for influxDBHook* [14168] Changed org_id to org_name for clarity.* [14168] Removed README, fixed influxDBHook class name in provider.yml* [14168] Static code check fixes.* [14168] fixed license header influxdb.rst for connections* [14168] fixed flak tests and order of requirements - influxdb* [14168] Renamed provider.yaml from provider.yml* [14168] fixed imports in influxdb.py* [14168] Renamed function for passing flak8 tests.* [14168] Fixed influxDB connection rst* [14168] Fixed order in CONTRIBUTING.rst and spelling_wordlist.txt* [14168] ADded operators/influxdb.rst to satisfy pre commit check* [14168] fixed docs error influxdb* [14168] fixed example DAG path for influxDB* Update airflow/providers/influxdb/hooks/influxdb.pyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Rebase from master* Fixed PR review comments* Fixed influxdb documentation* Fixed documentation* Added empty doc for InfluxDB operator to workaround build-docs error* Fixed table for influxdb dependency* Fixed connection-type in influxdb provider* Added init to example_dags* Added influxdb to airflow_providers_bug_report* Added missing init file in influxdb hook* Added missing init in tests/influxdb* Removed link in toc tree for influxdb* Moved influxdb tests from operators to hooks, added tests* Added more tests for influxdb hook* Added commits.rst and installing-providers-from-sources.rst for influxdb* Added commits to toctree of index* Added pandas dependency to influxdbCo-authored-by: Kanthi <kanthi@Kanthis-MacBook-Air.local>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Kanthi Subramanian <subkanthi@gmail.ccom>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",5
Add Macquarie Group to the list of companies using Airflow (#18591),1
Fixing flaky test that fails when run with other test that leave a trace (#18586),3
Small typo in JdbcOperator (#18593),5
Only show the task modal if it is a valid instance (#18570)* Only show the task modal if it is a valid instance* clean up try number* add not allowed tooltip,1
"Fix errors upgrading from Airflow 1.10.15 (#18573)If either of those checks failed the session was left in a transactionwith a failure so couldn't be used for future tests.And the second check (conn_type_null) _always_ failed on 1.10.15,because even thought the table existed, the `description` column whichthe model expects hasn't yet been added (because we are running this premigration!) so the fix there is to just select `conn_id` column.",0
"De-dup and reorganize render template tests (#18432)- test_render_template_fields_native_envs duplicated the existing  test_retest_render_template_with_native_envs- Testing dag settings (user_defined_*) was testing the behaviour of  Jinja2. The test has bee streamlined and moved to test_dag (where the  jinija env is created)- Tests of the undefined behaviour have been removed as it is just  testing Jinja behaviour, moved to just testing property of env in  test_dag.- Removed neededless DAG creation- Used BaseOperator instead of DummyOperator- Used existing MockOperator class instead of file-local CustomOp",2
Bugfix: dag_bag.get_dag should not raise exception (#18554)`get_dag` raising exception is breaking many parts of the codebase.The usage in code suggests that it should return None if a dag is notfound. There are about 30 usages expecting it to return None if a dagis not found. A missing dag errors out in the UI instead of returninga message that DAG is missing.This PR returns None when a dag is not found in SerializedDagModel instead of raising an exception,2
"Fix part of Google system tests (#18494)* Fix CloudBuildExampleDagsSystemTest* Fix DataCatalog tests* Fix Dataprep test* Fix GCS test* Fix Dataflow test* Fix GC Functions test* Fix Cloud SQL tests* Fix Data Fusion test* Change env var in Dataprep system test* Fix config for dataflow system tests* Add unit test for datafusion* Fix Data Fusion utest, add optional parameter to data fusion pipeline state sensor* Update code with black suggestions* Fix flake8 issue* Revert changes to example_cloud_sql.py according to review feedbackCo-authored-by: Lukasz Wyszomirski <wyszomirski@google.com>",5
Fix helm chart links in source install guide (#18588)The links to the binary helm chart in the source install guides are broken.,2
Chart docs: Update webserver secret key reference configuration (#18595),5
Add helm chart 1.2.0 to chart bug issue template (#18609),0
Updating the InfluxDB example DAG to use the TaskFlow API (#18596),1
Static start_date and default arg cleanup for misc. provider example DAGs (#18597),2
Fix spelling of execution. (#18594),0
Spelling fix (#18606),0
fix setting task nodes class names (#18607)The stroke colour and tooltips were not setcorrectly for some tasks (depending on the ordering of `g.nodes`).,1
Add in before_send config option to sentry integration (#18261),1
Check docker-compose version in breeze (#18536)We started to use some newer features in docker-compose of breeze.They are new enough (May 2020) that older versions ofdocker-compose do not work with them.This PR checks if the docker-compose is >= 1.29,2
Improve how UI handles datetimes (#18611),5
Update helm chart release docs (#18612),2
"Fix helm chart unittests on public runners (#18553)The helm tests are now regularly taking longer than 25 minutes on publicGitHub Actions workers, so we will increase the timeout.",1
Refresh credentials for long-running pods on EKS (#17951),1
Change label from area:docs to kind:documentation (#18624)We should not use area:docs label.https://github.com/apache/airflow/blob/main/ISSUE_TRIAGE_PROCESS.rst#labelsI'll remove the label in the upcoming days.,4
Bugfix: Don't warn on using ``LocalExecutor`` (#18625)The webserver showed the following error when using ``LocalExecutor``:```Do not use SequentialExecutor in production. Click here for more information.```,5
Fix `retry_exponential_backoff` divide by zero error when retry delay is zero (#17003),1
Add multiple roles when creating users (#18617),1
Fix typo in comments (#18626)Looks like it was added in (#18533),1
fix exception string of BranchPythonOperator (#18623),1
"Revert ""Fix intermittent orphan test (#18530)"" (#18631)This reverts commit 387c43f625e379a0de8e3527a98833eb5f62d3bf.",4
"Combine changelog entires for AIPs 39 and 40 in to a single row (#18633)We had a number of PRs for both of these AIPs, including a few ""bug fix""ones that don't really make sense on their own as they are part of a newfeature.I have changed it to combine all of the PRs that are part of these AIPsin to one line each.I also removed a few duplicated entries, or ones that are only touchingproviders.",1
Adding a default conn ID value for Apache Cassandra sensors (#18620),1
Added VLMedia to INTHEWILD (#18636),1
Updating ADX conn docs to reflect new custom fields (#18132),1
Update documentation for September providers release (#18613),1
"Fix flaky test `test_set_dag_runs_action` (#18618)We don't really care about the order of the TI's, so compare setsinstead.",1
Add default weight rule configuration option (#18627)* Add default weight rule configuration option,5
Update changelog for changes since 2.2.0b2 (#18639)This updates the changelog for commits up to and including 840ea3efb,4
Fix typo change GitHyb to GitHub (#18640),4
Retry deadlocked transactions on deleting old rendered task fields (#18616)The query that deletes rendered old rendered task fields for MySQLcan occasionally deadlock because it is unncesssary complex witha subquery (due to features missing in MySQL). This changeadds DB retries to get rid of the deadlock (as is therecommended practice for MySQL).Fixes: #18512* Update airflow/models/renderedtifields.pyCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>* Apply suggestions from code reviewCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Add muldelete action to TaskInstanceModelView (#18438),4
"Add region to Snowflake URI. (#18650)Without adding the AWS region to the URL, SQLAlchemy engines created byAirflow can't write dataframes to snowflake using pd_writer. This PRfixes this.",0
Make AirflowDateTimePickerWidget a required field (#18602)The UI breaks when a search field sends a null datetime. This fixes it,0
"Fix section formatting in the ""Customizing the UI"" docs page (#18658)",2
Remove empty doc from influxdb provider (#18647),1
Fix stop_airflow typos in CONTRIBUTORS_QUICK_START.rst (#18656),2
"Patch `utcnow` in retry delay test (#18343)* Remove sleep from retry delay test* Change to use freezegun* Approximate smallest timedelta without numpy* Use pre-defined smallest timedeltaIt turns out `datetime.datetime.resolution` is already defined as the smallest possible timedelta (one microsecond). Attempting to create a timedelta with microseconds in the interval (0.5, 1) simply results in microseconds being rounded up to 1.",1
"Remove cargo-culted local in-page ToCs (#18668)This pattern of having a local table of contents seems to have beencargo-culted to all of the providers, and other pages, but our Sphinxtheme already has the lage contents on the right hand side -- this isjust leading to duplication.(Not to mention that for the majority of cases the page is so short thatyou can easily see everytine at once anyway.)I have left it in a few places -- for instance the mainconfigurations-ref.rst I have kept it.",5
"Fix error on triggering a dag that doesn't exist using dagrun_conf (#18655)We currently show a nice error message when the trigger button is clickedbut clicking on trigger DAG w/conf for a dag that doesn't exist anymore,takes us to enter configuration. When you eventually trigger, you get errormessages.This PR fixes it",0
Support all Unix wildcards in S3KeySensor (#18211)* Support all Unix wildcards in S3KeySensorThis addresses issue 15538 by expanding the regular expression to include all possible wildcard input to fnmatch* Added tests for s3 and s3_key,3
Check the allowed values for the logging level (#18651),2
update azure cosmos version (#18663),5
Added Viscovery to the list of companies using Apache Airflow (#18683),1
"Revert ""update azure cosmos version (#18663)"" (#18694)This reverts commit 10421c693199eeea2c1ea54844319080fd6f7153.",4
Correcting text in core extenders docs (#18661),2
Expanding docs on client auth for AzureKeyVaultBackend (#18659),2
Typo in docs/apache-airflow/installation/index.rst (#18689),2
Add max_ingestion_time to DruidOperator docstring (#18693),2
Add formatDateTime back into ti log (#18700)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,2
Simplify strings previously split across lines (#18679),5
"Fix compatibility issues with docker-compose 2 (#18725)Docker-compose 2 on linux introduced a number of problems andincompatibilities:* version is printed in different format* -f flag was failing after --log-level info was passed* environment variables were not correctly passed with _docker.env  with missing valuesAll those worked fine in 1.29.2 and even in MacOS version ofdocker-compose 2 (after raising and fixing some bugs by usdocker/compose-cli#1917This PR:* fixes version regexp* removes --log-level info* adds copy of _docker.env with var=""${var} assignments specifically  for docker-compose (duplicated variables but this works as a  workaround)",1
"Fix eager-upgrade builds for Airflow main. (#18719)For 5 days now Airflow's main did not build with eager upgradefor Python 3.7+. Unfortunately PIP did not give us enough cluesof what happend, so we had to investigated it and found outthat google-ads 14.0.1 bumped google-python-core-api dependencyto above 2.0.0 and we have limits in place as some other librariesare not compatible with it.See:https://github.com/apache/airflow/issues/18705#issuecomment-933746150Fix is to limit google-ads.",0
Add example DAG using TimeDeltaSensorAsync (#18728),1
Dockerfile: Fix env variable typo ``_AIRFLOW_WWW_USER_LASTNME`` (#18727)Env Variable Fix: _AIRFLOW_WWW_USER_LASTNME ---> _AIRFLOW_WWW_USER_LASTNAMEcloses #18716,1
Add ``DaskExecutor`` queue handling change to ``UPDATING.md`` (#18720)Add a note about the behavior change introduced in #16829.,4
update azure cosmos to latest version (#18695),3
Ensure task_instance exists before running update on its state(REST API) (#18642)Error when the task instance does not existCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
Renaming variables to be consistent with code logic (#18685),2
"Handle timetable exception in ``DAG.next_dagrun_info`` (#18729)For now, the exception is simply logged and the DAG not being scheduled.Currently, an incorrectly implemented timetable may crash the scheduler process entirely due to uncaught exceptions. This PR adds exception handlers around those calls. A failed infer_manual_data_interval() will cause the manual DAG run being skipped, and a failed next_dagrun_info() will cause the DAG run to not happen, and the DAG not being scheduled anymore (because None is set on DagModel.next_dagrun) until the DAG file is modified.For now, the exception is simply logged. In the future we'll add a new db model similar to ImportError and hold these errors and display them on the web UI. This new model class will also be designed to incorporate ImportError eventually.",2
"Properly set start_date for cleared tasks (#18708)This PR ensures that when we clear task instances, the start_date for the dagrun is set correctly.If the dag_run_state is queued we should set the start_date to None otherwise start_date should beset to timezone.utcnowAlso, in the clear task instances API endpoint, the state of the dagrun is being set to RUNNING onclear. This PR addresses it too.",1
Pretty print diff for order checks (#18686),5
update minimum version of sshtunnel to 0.3.2 (#18684),5
"Add google-ads limitation to eager upgrade (#18741)Since we released google provider without upper limitfor google-ads, the eager upgrade for PyPI providers has tohave this extra limit added in order not to hang when tryingto generate eager constraints.Related to: #18719",1
Add additional dependency for postgres extra for amazon provider (#18737),1
Fix changelog for Azure Provider (#18736)One of the commits has wrong description (Initial Commit) and hasbeen removed from the changelog by mistake.This PR adds it back,1
"Updates link to ""stable"" URLs for providers ""installing from sources"" (#18735)",1
Rename ``processor_poll_interval`` to ``scheduler_idle_sleep_time`` (#18704)`[scheduler] processor_poll_interval` setting in `airflow.cfg` has been renamed to `[scheduler] scheduler_idle_sleep_time`for better understanding.,1
"Fix typos in IMAGES.rst, README_RELEASE_PROVIDER_PACKAGES.md and REFRESHING_CI_CACHE.md (#18751)",1
"Don't ignore legacy `concurrency` dag parameter (#18730)Currently, even if legacy `concurrency` dag parameter is specified, it is always ignored because `max_active_tasks` is always initialized from `core.max_active_tasks_per_dag`.In our case this caused unexpected throttling of the task concurrency on production and performance issues.`concurrency` parameter should always be used, if provided, as this preserves backward compatibility for users performing the migration.Tested locally:```DAG(    dag_id='xxx',...    concurrency=1,)```Before fix:```{scheduler_job.py:413} INFO - DAG xxx has 1/16 running and queued tasks```After fix:```{scheduler_job.py:413} INFO - DAG xxx has 1/1 running and queued tasks```Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).",5
"Adds an s3 list prefixes operator (#17145)- Adds an operator to return a list of prefixes from an S3 bucket- Updates `list_prefixes()` unit test to assert on a nested dir with a prefix variable- Removes duplicate calls to `list_keys()` that were in the `test_list_prefixes()` unit test (likely a copy/paste boo boo?)There are two suggestion from [this conversation](https://github.com/apache/airflow/pull/8464) that I have not included here:1. Combine or otherwise simplify `s3_list_keys()` and `s3_list_prefixes()` into one - this makes sense to me but I don't quite know how people tend to use these operators or if there is a valid argument for keeping them separate. 2. Combining all the s3 operators into one file like [gcs.py](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/gcs.py) - this also makes sense to me, but it's not consistent with the other AWS operators. Might be worth opening a new issue to refactor them all if we want to go in this direction?Issue Link: #[8448](https://github.com/apache/airflow/issues/8448)",0
"Fix ""Test"" connection button when app not mounted at `/` (#18750)If the app is at /airflow requesting /api/v1/... isn't going to work :)",1
"Remove AIRFLOW_GID from Docker images (#18747)The AIRFLOW_GID parameter was in the images for historical reasons,however for a long time we recommend everyone to use GID=0 in orderto make it possible to run the image with Arbitrary UID. Settingdifferent group than 0 has NO VALUE actually. You can stilloverride the group of user when starting the container, so the onlyreal difference is that the ""airflow"" unmodifiable files such aspython code belong to different group, which has no real value.You can still use whatever group you want for mounted files andmodifiable resources. Airflow Docker image will work perfectly finewhen the main group of the user is 0 (and we also have to rememberthat if the user belongs to other groups in the host, it will alsobelong to those group inside the container, AIRFLOW_GID has onlyinfluence on primary group of that user IN-CONTAINER (not outsideof it).Removing AIRFLOW_GID seems like best choice for Airflow 2.2.Fixes: #18709",0
"Remove eagerloading when querying for TI (#18706)Since we now have ti.dag_run lazy loaded, no need to still have eagerloading on TIs",2
"Chart: Mount DAGs in triggerer (#18753)We need to mount the DAGs in the triggerer, as triggers could come fromthe DAGs themselves.",2
ECSOperator: airflow exception on edge case when cloudwatch log stream is not found (#18733),2
"Small improvements for Airflow UI (#18715)I slightly improved some small UI elements that were a bit off. Changes:- Removed `btn-sm` class that made the buttons next to each DAG on DAGs list not centered perfectly.- Changed everywhere the spelling of DAGs (made the last letter lowercase in 'DAGS' word)- Changed the 'Conn Type' and 'Conn Id' to 'Connection Type' Connection Id' (not sure why this was abbreviated everywhere, but if I am not aware of something, let me know and I can revert this change)- Fixed strange use of punctuation marks inside a view when trying to mark DAG run as failed- Changed font size inside DAG run circle from 8 to 9. It makes it a lot more readable and it can still fit even 4-digit numbers. I think we can even go with 10, but then 4-digit numbers slightly overlay the circle (who have 4-digit runs though?).Here I highlighted with red color the old font size and misaligned icons inside button. The green highlights the elements after changes.![image](https://user-images.githubusercontent.com/7412964/135896512-7b44b5d0-9cda-42d5-a8ea-3151c0dde2a1.png)If there are any other places that I missed or there is something that shouldn't be changed or maybe the change should be reflected in some places that I missed - let me know!",4
"Coerce datetime to pendulum for timetable (#18522)`Timetable.infer_manual_data_interval()` expects a `pendulum.DateTime`, but `marshmallow_sqlalchemy.auto_field()` only automatically decodes to `datetime.datetime`. So we perform one additional coersion before passing the value on.",4
Fix Pendulum 1.x references in documentation (#18766),2
Make REST API patch user endpoint work the same way as the UI (#18757)Username is updateable in the UI and already exists error is alsoraised when trying to update with username/email that already exists.This PR ensures that the REST API works the same way,1
"Amazon SQS Example (#18760)* Amazon SQS ExampleAdded a doc entry and example for using the SQS Publish Operator* Changed Amazon SQS Example to Task Flow API and added ref in sqs.pyThe example now uses the TaskFlow API to pass results from the Python functions, and per the build error is now referred to in the operator source code.",1
Quarantine iest_no_orphan_process_will_be_left (#18778)This test fails too often. Quarantining it for now.This is captured in #18777,5
Move docker decorator example dag to docker provider (#18739)This example dag errors out during startup when we set AIRFLOW__CORE__EXAMPLE_DAGS=True and dockerprovider is not installed.,2
Bump pre-commits: `black` and `pyupgrade` (#18782)```https://github.com/asottile/pyupgrade : v2.25.1 -> v2.29.0.https://github.com/psf/black: 21.8b0 -> 21.9b0.```,5
"Always run at least one backfill from BackfillJob (#18742)This restores the behavior around this prior to AIP-39 implementation. It is arguably not correct, but nobody ever complained about it (and they have to the new behavior), so we should meet user expectations.Close #18473",1
"Change the color for deferred status to mediumpurple (#18414)Changes the color for the deferred status to something more distinct, mediumpurple.![image](https://user-images.githubusercontent.com/13177948/134234580-11d1cec0-d9ad-45e4-9577-e9066b829363.png)![image](https://user-images.githubusercontent.com/13177948/134234619-04851b97-5a26-4ecb-baa8-dbf3cd7270b0.png)closes: #18245",5
Fix deprecated default for ``fab_logging_level`` to ``WARNING`` (#18783)`WARN` is deprecated in favor of `WARNING`,2
Adds CeleryKubernetesExecutor check to AirflowBaseView.run (#18441)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Open src and dst in binary for samba copy (#18752),5
AwsGlueJobOperator: add run_job_kwargs to Glue job run (#16796)* add run_job_kwargs to glue job run* add run_kwargs to hook and operator tests,3
"Amazon Athena Example (#18785)This example, and associated documentation, shows how to use Amazon Athena to read a table from a CSV file in an S3 bucket, read from that table, and clean up all resources.",4
Adds Hacktoberfest label to participate in Hacktoberfest 2021 (#18781),1
"Fix bug that backfill job fail to run when there are tasks run into reschedue state (#17305)Backfill job fails to run when there are tasks run into rescheduling state. The error log as follows in issue #13322```Traceback (most recent call last):  File ""/opt/conda/bin/airflow"", line 8, in <module>    sys.exit(main())  File ""/opt/conda/lib/python3.8/site-packages/airflow/__main__.py"", line 40, in main    args.func(args)  File ""/opt/conda/lib/python3.8/site-packages/airflow/cli/cli_parser.py"", line 48, in command    return func(*args, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/utils/cli.py"", line 89, in wrapper    return f(*args, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/cli/commands/dag_command.py"", line 103, in dag_backfill    dag.run(  File ""/opt/conda/lib/python3.8/site-packages/airflow/models/dag.py"", line 1701, in run    job.run()  File ""/opt/conda/lib/python3.8/site-packages/airflow/jobs/base_job.py"", line 237, in run    self._execute()  File ""/opt/conda/lib/python3.8/site-packages/airflow/utils/session.py"", line 65, in wrapper    return func(*args, session=session, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/jobs/backfill_job.py"", line 799, in _execute    self._execute_for_run_dates(  File ""/opt/conda/lib/python3.8/site-packages/airflow/utils/session.py"", line 62, in wrapper    return func(*args, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/jobs/backfill_job.py"", line 722, in _execute_for_run_dates    processed_dag_run_dates = self._process_backfill_task_instances(  File ""/opt/conda/lib/python3.8/site-packages/airflow/utils/session.py"", line 62, in wrapper    return func(*args, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/jobs/backfill_job.py"", line 620, in _process_backfill_task_instances    self._update_counters(ti_status=ti_status)  File ""/opt/conda/lib/python3.8/site-packages/airflow/utils/session.py"", line 65, in wrapper    return func(*args, session=session, **kwargs)  File ""/opt/conda/lib/python3.8/site-packages/airflow/jobs/backfill_job.py"", line 211, in _update_counters    ti_status.running.pop(key)KeyError: TaskInstanceKey(dag_id='dag_id', task_id='task_name', execution_date=datetime.datetime(2020, 12, 15, 0, 0, tzinfo=Timezone('UTC')), try_number=2)```The root cause is that the field `try_number` doesn't Increase when the task runs into rescheduling state, but there is a reduce operation on `try_number`. Currently, I can't think out a good ut to test it, only post the code here to help the one who is affected by it to solve the problem.",0
UI: Fix alignment of Delete Button (#18795)This got broken in #18715,4
Properly handle verify parameter in TrinoHook (#18791),1
Stabilize flaky test_extra_operator_links_not_loaded_in_scheduler_loop (#18796)This test fails intermittently where _run_scheduler_loop does notdo what it was supposed to do in tests. We run relevant schedulingmethods instead and flush session in order to make sure thatthe relevant part of scheduling happened.This should get rid of flakiness of that test,3
Dataflow Operators - use project and location from job in on_kill method. (#18699)Reason why we need this is because we can have situation where project_id is set to None but we define it in the dataflow_default_options. Job will start normally without error but in case when we decide to mark running task to different state we will get a error that the job does not exits.,0
Stabilize flaky test_do_schedule_max_active_runs_dag_timed_out (#18531)Some of the executions of this test return dagrun in Queuedrather than Running state.This test changes approach to get more control over thescheduling methods and only get to SCHEDULED staterather than run the whole scheduler loop.,1
"Backport fix to allow pickling of Loggers to Python 3.6 (#18798)When sending objects around via multiprocessing (on Python 3.6 or lower)it would fail if that object contained a Logger object.To fix that we have ""backported"" the change in Python 3.7 to make Loggerobjects be pickled ""by name"". (In Python 3.7 the change adds`__reduce__` methods on to the Logger and RootLogger objects, but herewe achieve it `copyreg` stdlib module so we don't monkeypatchanything.)This mainly applies to using Kubernetes client >12 (which is notcurrently possible as we restrict that version) but this adds supportfor it anywhere it might happen inside Python 3.6.",1
Docs: Move part of timetable guide to concepts (#18786),4
Update changelog with more changes since 2.2.0b2 (#18780),4
Fix flaky test_external_task_marker_cyclic_deep test (#18802)The test failed very rarely with this error:``` >       assert task_instance.state == state  E       AssertionError: assert None == <TaskInstanceState.SUCCESS: 'success'>  E        +  where None = <TaskInstance: dag_3.task_b_3 manual__2015-01-01T00:00:00+00:00 [None]>.state```The change makes sure that the ti instance is merged with thelatest DB changes when the assert runs.,1
Update instructions to create Airflow release (#18809),1
Accept custom run ID in ``TriggerDagRunOperator`` (#18788)Fix #17438,0
Update changelog to the latest (#18811)Adds recent changes from main too,4
Enable FTPToS3Operator to transfer several files (#17937),2
Add standard hook fields to pagerdutyHook to make hook show up in UI (#18763),1
Install only devel packages (#18815)Fixes #15770 unblocking `./breeze initialize-local-virtualenv` on Mac OSX,5
"Ensure that dag_id, run_id and execution_date are non-null on DagRun (#18804)These _should_ be non-nullable, and are always created as such. Withoutthis it was possible that someone had manually edited it which causedproblems with the TaskInstance FK migration not applying correctly.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
AwsGlueJobOperator: add wait_for_completion to Glue job run (#18814),1
Add unittest for #17305 (#18806),3
Remove unnecessary string concatenations in AirflowException messages (#18817),4
Add SalesforceApexRestOperator (#18819),1
Update ``dagbag_size`` documentation (#18824)Closes #10431,2
Add emr cluster link (#18691),2
Update documentation about bundle extras (#18828),2
"Fix wrong Postgres search_path set up instructions (#17600)In the set up instructions for Postgres database backend, the instructed command for setting search_path to the correct schema lacks `public` and `utility`, which prevented SqlAlchemy to find all those required schemas.",1
Enable AWS Secrets Manager backend to retrieve conns using different fields (#18764),1
Add AWS Fargate profile support (#18645),1
Prepare documentation for RC2 Amazon Provider release for September (#18830),1
Add last items to changelog before 2.2.0rc1 (#18837),4
Removed duplicated code on S3ToRedshiftOperator (#18671),1
"Add RedshiftSQLHook, RedshiftSQLOperator (#18447)",1
Fixing tests that leave traces (users) (#18690),1
Remove duplicate code on dbapi hook (#18821),1
Duplicate Connection: Added logic to query if a connection id exists before creating one (#18161),1
Added Parimatch Tech into userlist (#18848),1
"Handle leading slash in samba path (#18847)Fix issue that occurs when the path to a file on a samba share has aslash prepended to it, then the `SambaHook` will treat the path as thehost instead likely resulting trying to connect to the wrong samba host.",0
add more type hints in CeleryKubernetesExecutor (#18219),1
"Increase timeout of the job pushing to GitHub registry (#18856)Building and pushing image to GitHub Registry might take more than10 minutes, depending on the ""CI Build step"" - it can takeshorter or longer, depending on whether the change in Dockerfile,setup.py, setup.cfg.The jobs occassionally fail with 10 minute limit. Changing it to40 minutes seems much more reasonable.",4
"Quarantine maintain_hear_rate test (#18860)Depending on the circumstances, this test might show verydifferent timings (and very wrong ones). Example:```  >       assert time_end - time_start < job1.heartrate  E       assert (1633800366.161033 - 1633800360.9558177) < 1.0  E        +  where 1.0 = <airflow.jobs.local_task_job.LocalTaskJob object at 0x7f1faf784e80>.heartrate```This test should likely remain in quarantine for ever.You will still be able to run it locally, but running it in CImakes little sense.",1
Update bug template to 2.2.0rc1 (#18861),0
"Fix occassional deadloc on MSSQL test DagMaker cleanup (#18857)Occasionally our tests in CI for MsSQL failed with deadlock oncleaning SerilizedDag table. After closer inspection, thedeadlock happened in the test dag_maker cleanup() code.This PR fixes it by attempting to retry the cleaning in caseof deadlock.",4
Attempt to fix flaky timeout test (#18862)The test_do_schedule_max_active_runs_dag_timed_out testfails occasionally with task state where it is inqueuing rather than running state:```  >       assert run2.state == State.RUNNING  E       AssertionError: assert 'queued' == <TaskInstance...NG: 'running'>  E         - running  E         + queued```The most likey reason was that sesion was not flushed aftertask state has been set to Failed and the subsequentquery that queues subsequent runs did not see that there areno more active dag_runs running.,1
"Check min docker version (#18869)We have recently introduced a few features (--pull flag for example)in our docker commands, that require newer version of Docker(the `--pull` flag was introduced in 20.10.0). This PR adds checkif the docker version is at least 20.10.0.",2
"Strips suffixes from docker version (#18871)This version will strip any + alpha postfixes from docker version.Seems that CodeQL uses azure-specific docker version and theversion suffix is added folowing `+`. This change will strip suffixbut also will not fail docker version check in case of similarerror, it will continue running with a warning if version cannotbe retrieved.",2
fix get_connections deprecation warn in hivemetastore hook (#18854),1
Add MSSQL variables to dc_ci script (#18873)The dc_ci script is generated in `.build` folder so that youcan easily run docker compose command with multiple combineddocker-compose files and reproduce manually what breeze does.Useful for debugging. The MSSQL_* variables were missing fromthe script so MSSQL setup could not be easily debugged.,0
Remove extra postgres dependency from AWS Provider (#18844)* Remove extra prostgres dependency* Removed postgres cross dependency on aws provider,1
Adds Bwtech and Inter Platform Inc. to the list of companies using Apache Airflow (#18876)Co-authored-by: Guilherme da Silva Goncalves <guilherme.goncalves@bancointer.com.br>,1
Change the contact person for Clover (#18884),4
"Ignore License check in ``airflow/www/static/robots.txt`` (#18886)The License Header was missing but since the file is a simplistic version, based on https://www.apache.org/legal/src-headers.html#faq-exceptions we don't need license header for it.",2
"Adds back documentation about context usage in Python/@task (#18868)There were many questions recently along the line of""How do I access context from TaskFlow task"". Surprisingly,the paragraph about accessing current context was removed fromthe ""Concepts"" (where it was there for Airflow 2.0.0) but wasnever added to the ""TaskFlow Tutorial"" where it actually belongs.Also Python Operator's description about passing contextvariables as kwargs have been removed when `provide_context`parameter was removed (it was only present in the docstringof `provide_context` and you could likely deduce this behaviourfrom several examples, but it was not mentioned anywhere.This PR adds the description with examples to the Python operatoras well as adds similar description in TaskFlow tutorial, includingthe possibility of using `get_current_context` deep down thestack to retrieve the context variables even if they are notpassed via kwargs.",4
Fix failing static check (#18890)https://pypi.org/project/flake8/ released a new version which broke main. This fixes it,0
Fix failing static check (#18891)Try 2 to fix failing check,0
"Group PATCH DAGrun together with other DAGRun endpoints (#18885)Co-authored-by: Bas Harenslak <bas@astronomer.io>The endpoint to modify DAGRuns added in https://github.com/apache/airflow/pull/17839 is given a new tag ""UpdateDagRunState"", but I think it belongs together with the other DAGRun endpoints, which this PR does.WRT the 2.2 release: this PR only changes the grouping, the endpoint itself remains the same, so IMO this is fine for the 2.2.1 release.![image](https://user-images.githubusercontent.com/6249654/136782577-120f7b4d-7b70-49c8-9a1f-f70d368ad9b2.png)",1
"Doc: Add Callbacks Section to Logging & Monitoring (#18842)Airflow users often ask for an easy way to monitor the success or failure of tasks and/or DAGs. The use of callbacks, such as on_success_callback, helps users monitor DAG runs in an easily configurable way.This document provides an overview of available callbacks, linked to the relevant section in the Tasks document. It also provides a simple example of calling a function on any task failure in the DAG, as well as a separate function that is called upon the success of the last task in the DAG.",2
refactor connection tests (#18881),3
Release 2.2.0 (#18892),5
breeze setup-autocomplete zshrc reload (#18893),1
Update S3PrefixSensor to support checking multiple prefixes within a bucket (#18807),0
Fix occasional cleartask failures (#18859)The cleartask tests occasionally failed due to not consistentsequence in which task clearing was performed.The query did not have ordering and sometimes the tasks werereturned in different order than expected.,0
"Workaround docker-compose-v2 env passing (#18887)* Workaround docker-compose-v2 env passingDocker Compose v2 has environment parsing broken in many ways.Until this is fixed, we cannot use env files, instead we mustset all the variables directly, because parsing variables withoutvalues or parsing variables which have empty values is brokenin several ways. Some of the issues are closed but not released,and until this is fixed, some extra code duplication and explicitlysetting all default variables to """" when needed should solve theproblem for both Docker-Compose v1 and Docker-Compose v2",2
Remove deprecated usage of init_role() from API (#18820),5
Chart: Update default Airflow version to `2.2.0` (#18898),5
Update version added field in config after 2.2.0 release (#18899)* Update version added field in config after 2.2.0 release* fixup! Update version added field in config after 2.2.0 release,5
Fix comparision of docker versions (#18902)In some shells the comparable string with version was too long.The number leading with 0 was interpreted as octal number andit had too many digits for octal number to handle.This change;1) decreases the length of the string by using 3-digit numbers2) strips leading 0s during comparision making comparision work   in decimal,1
"Remove adding of ""test-run"" variables to dc_ci script (#18903)The RUN_*TEST variables are not part of the environmentso they are not set when the dc_ci is generated they areoverridden by Breeze when particular commands are executed.Therefore we should not hard-code those values in dc_ci script(this is useful for debugging to have the script but it is onlythere for environment configuration)",5
Remove the docker timeout workaround (#18872),1
"Stabilize occasional test_logging_head_error_request failures (#18858)Sometimes, when the test is executed in parallel it managed toexecute the test just 4 times instead of 5 before the timeout and thetest failed. This PR stabilizes the test by verifying if just twofirst times were executed. There is no need to check the exactnumber of retries - we just have to see that we tried at least2 times and that the task eventually timed-out.",3
Updating explicit arg example in TaskFlow API tutorial doc (#18907),2
"Handle occasional segfaults despites tests succeding (#18863)Sometimes Pytest fail with segfault despite seemingly all testssucceeded. It manifests with exit code 139In such case we now check if the junitxml is created, whetherit contains some errors/failures lines and whether all of themshow ""0"". If so - we pretend nothing happened.",0
"Fix --github-image-id flag for Breeze (#18882)When we moved to github registry, the --github-image-id flag wasbroken as it had pulled the ""latest"" image when run right afterpulling the tagged image (and it run that image instead).This change fixes it and uses GITHUB_PULL_IMAGE_TAG (latest if notspecified) everywhere where the image is used for running.This flag is not persistent so it is not persistent.",1
Decrease likelihood of memory issue in CI (#18852)This PR attempts to decrease the likelihood of memory issues forCI for non-committers. The MSSQL and MYSQL Provider and Integrationtests when run together with other tests in parallel (for MSSQL evenstandalone) might cause memory problems (143 or 137 exit code).This PR changes the approach slightly for low-memory conditions:1) MSSQL - both Integration and Providers tests are skipped   entirely (they will be run in High-Mem case so we will see if   there are any problems anyway)2) MySQL - both Integration and Providers tests are run separately   which will lead to slightly longer test runs but likely this   will save us from the occasional memory issues.,0
Make a separate hook for interacting with the Pagerduty Events API (#18784),1
Chart: Make PgBouncer cmd/args configurable (#18910),5
"Fix occasional external task sensor tests (#18853)Occassionally the sensor tests fail with assertion wherestate seems to be None. This might be caused by```      def assert_ti_state_equal(task_instance, state):          """"""          Assert state of task_instances equals the given state.          """"""          task_instance.refresh_from_db()  >       assert task_instance.state == state  E       AssertionError: assert None == <TaskInstanceState.SUCCESS: 'success'>  E        +  where None = <TaskI$anstance: dag_1.task_b_1 manual__2015-01-01T00:00:00+00:00 [None]>.state```Turned out it was because the task instance fields fromdagrun.taskinstance relationship could be returned in differentorder so some of the dependencies were not met for some of thetasks when later task was returned before earlier one.Deterministic sorting according to task_id solved the problem.",0
"Add information about keepalives for managed Postgres (#18850)The managed Postgres DBs often kill the connection aftersome time of inactivity, so if you are using Airflow with thoseyou need to configure keepalives.This PR adds description on how to do it. See #18846",1
Add more information to PodLauncher timeout error (#17953),0
"Revert ""Fix --github-image-id flag for Breeze (#18882)"" (#18923)This reverts commit ce0b64e895a247fde9d177a044ba16b1b99a9596.",4
Bump chart version to `1.3.0-dev` (#18919),2
Fix typos ``build.rst`` (#18935),2
"Chart: Use python 3.7 by default; support disabling triggerer (#18920)The triggerer is only supported in Python 3.7, so use that as thedefault image. For users who do not want to update Python versions,allow the triggerer component to be disabled.",1
"Doc: grammar check - remove a word (#18914)Remove ""the"" from this line:""Airflow defines the some Jinja filters that can be used to format values.""",1
Clean test_user once assert is complete (#18865)* Clean test_user once assert is complete* Remove redundant try block* Add docstring,2
Updating core example DAGs to use TaskFlow API where applicable (#18562),1
Switch default version of Python to 3.7 (#18922),5
"Don't bake ENV and _cmd into tmp config for non-sudo (#18772)If we are running tasks via sudo then AIRFLOW__ config env vars won't bevisible anymore (without them showing up in `ps`) and we likely mightnot have permission to run the _cmd's specified to find the passwords.But if we are running as the same user then there is no need to ""bake""those options in to the temporary config file -- if the operator decidedthey didn't want those values appearing in a config file on disk, thenlets do our best to respect that.Note: this commit originally appears in 2019 but a critical piece wasmissing, meaning that the secrets/envs were still actually appearing.",2
Fix occassionally failing timeout test (#18947)The test failed someties with:```  >       assert run2.state == State.RUNNING  E       AssertionError: assert 'queued' == <TaskInstance...NG: 'running'>  E         - running  E         + queued```,1
Update modules_management.rst (#18948)Fix typo,2
Add more type hints to PodLauncher (#18928),1
Adding feature in bash operator to append the user defined env variable to system env variable (#18944),5
Check python version before starting triggerer (#18926),5
"Add ``semver`` to devel deps (#18818)We have a new dev script, `validate_version_added_fields_in_config.py`,that uses it.",1
Refactor SSHOperator so a subclass can run many commands (#10874) (#17378),1
Test kubernetes refresh config (#18563)* Fixed flak8 errors* Removed unused import* Fixed pre-commit errors* Moved return value to mock.patch for test_refresh_config* Fixed flake8 errors,0
"Fix XCom.delete error in Airflow 2.2.0 (#18956)In Airflow 2.2.0 XCom.delete causes error, by trying to update dag_run table dag_id and execution_date columns to NULLs.sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) null value in column ""dag_id"" violates not-null constraint[SQL: UPDATE dag_run SET dag_id=%(dag_id)s, execution_date=%(execution_date)s WHERE dag_run.id = %(dag_run_id)s][parameters: {'dag_id': None, 'execution_date': None, 'dag_run_id': 2409}]Setting passive_deletes to the string value ‘all’ will disable the “nulling out”",4
Changes related to PR #16634 (#18644),4
Add description about `not a directory` errors in WSL2 (#18976),0
"Fix --github-image-id flag for Breeze (#18882) (#18946)When we moved to github registry, the --github-image-id flag wasbroken as it had pulled the ""latest"" image when run right afterpulling the tagged image (and it run that image instead).This change fixes it and uses GITHUB_PULL_IMAGE_TAG (latest if notspecified) everywhere where the image is used for running.This flag is not persistent so it is not persistent.",1
Align the default version with Facebook business SDK (#18883),5
Sentry before send fallback (#18980),1
Row lock TI query in SchedulerJob._process_executor_events (#18975)Using multiple schedulers causes Deadlock in _process_executor_events.This PR fixes it.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,0
Fix typo in ``tutorial.rst`` (#18983),2
"Try to move ""dangling"" rows in upgradedb (#18953)",5
"Adds Pendulum 1.x -> 2.x upgrade documentation (#18955)closes: #18634Adds documentation about the upgrade from Pendulum `1.x` to `2.x` as discussed in the issue.  Assumptions that were made:- Most of the Pendulum changes are already documented in the official Pendulum docs.Added the following: - Mention the upgrade from `1.x` to `2.x`- Added an example of a code snippet that will now throw errors- Added link to official pendulum `2.x` docs that discuss the changes from `1.x` to `2.x`The macros documentation as mentioned in the issue were actually pointing to the updated Pendulum documentation, so no changes were added for the same. For instance, consider the link for the macro [prev_execution_date](https://pendulum.eustace.io/docs/#introduction)",5
"Add pre-commit hook for common misspelling check in files (#18964)This PR adds codespell to the pre-commit hooks. This will specifically helpus a bit in resolving sphinx errors.From the project page:It does not check for word membership in a complete dictionary, but instead looks for a set of common misspellings.Therefore it should catch errors like ""adn"", but it will not catch ""adnasdfasdf"".This also means it shouldn't generate false-positives when you use a niche term it doesn't know about.This means the sphinx errors are not solved completely.",0
BugFix: Null execution date on insert to ``task_fail`` violating NOT NULL (#18979)Fixes #18943 null exec date on insert to task_failThe dag_run property isn't populated by refresh_from_db or when this iscalled from the failure handler when reaping zombies. This resulted inan IntegrityError violating the NOT NULL constraint on task_fail,0
Add decription on how you can customize image entrypoint (#18915),1
Update CSV ingest code for tutorial (#18960),5
Handle case of nonexistent file when preparing file path queue (#18998),2
MySQLToS3Operator add support for parquet format (#18755)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Replace default api_version of FacebookAdsReportToGcsOperator (#18996),1
"CI: Increase parallel test timeout for Helm Chart tests (#18993)The helm tests are now regularly taking right around 35 minutes on publicGitHub Actions workers, so we will increase the timeout.",1
Fix BigQueryToMsSqlOperator documentation (#18995)Co-authored-by: Aaron Mangum <aaron.mangum@iherb.com>,2
"CLI: Fail ``backfill`` command before loading DAGs if missing args (#18994)I was looking through some of the CLI code last week trying to improve the speed of `airflow user` commands and I noticed this small issue. If neither the `start_date` or `end_date` argument is provided then the command will fail, but it will first parse all of the DAGs which can take up to several minutes in large deployments. Now the command will fail faster, allowing the user to adjust their command and retry.",1
"Chart: Increase default liveness probe timeout (#19003)In practice the liveness probe can regularly take longer than 5 seconds.  10 seems like a better default. Because it takes double the time, we can reduce the check frequency so that we do not waste as many CPU cycles.  And to keep the max downtime to 5 minutes, we reduce number of failed checks to 5.",0
Fix typos in ``CHANGELOG.txt`` (#19012),5
Upgrade old DAG/task param format when deserializing from the DB (#18986),5
Doc: Restoring additional context in Slack operators how-to guide (#18985)A recent update to the Slack example DAG removed some context of using operators that users may find useful.  Some of the args were moved to `default_args` to simplify authoring of the DAG but these args disappeared from the Slack operator how-to guide as a result.  This PR should be a happy middle ground between example DAG enhancement and how-to guide showcase of the operators.,1
Minor grammar tweaks in docs (#19013),2
"Add pandas requirements for providers that use pandas (#18997)As we removed pandas as core airflow requirement, the providersthat need it should get pandas explicitlyly as installationrequirements.Fixes: #18901",0
"Skip updating constraints when only datetime changes (#19023)After adding datetime to generated constraints it became possiblethat constraints remained the same but generated constraint fileschanged (because of the date time). It was a rare occurencebecause we rarely had ""all-green"" build in `main`, but since wemanaged to fix most of the flaky tests it became more probable(and happened already several times).This PR ignores comment files when comparing generated constraintsand commits should only happen if something else than generatedcomment changes.",4
Remove unnecessary string concatenations in AirflowException in s3_to_hive.py (#19026),4
Use google cloud credentials when executing beam command in subprocess (#18992),1
More f-strings (#18855),5
Removing redundant max_tis_per_query initialisation on SchedulerJob (#19020),5
"Workaround libstdcpp TLS error (#19010)Workaround https://github.com/apache/airflow/issues/17546 issue with/usr/lib/x86_64-linux-gnu/libstdc++.so.6: cannot allocate memory instatic TLS block.We do not yet a more ""correct"" solution to the problem but in order toavoid raising new issues by users of the prod image, we implement theworkaround now.The side effect of this is slightly (in the range of 100s ofmilliseconds) slower load for any binary started and a little memoryused for Heap allocated by initialization of libstdc++.This overhead is not happening for binaries that already linkdynamically libstdc++.",2
Google provider catch invalid secret name (#18790),1
Rename execution date in forms and tables (#19063),5
Document `hdfs_namenode_principal` for HDFS connections (#18987)* Document `hdfs_namenode_principal` for HDFS connections,2
Fix: Add taskgroup tooltip to graph view (#19083),1
Relax packaging requirement (#19087),1
Add docker-compose explanation to conn localhost (#19076),2
Allow Param to support a default value of ``None`` (#19034),1
Add dataroots to Airflow users (#19074),1
Rename trigger page label to Logical Date (#19061),5
Updating TaskGroup unit test to handle new tooltip (#19089),1
Adding dag_id_pattern parameter to the /dags endpoint (#18924),2
"Ensure task state doesn't change when marked as failed/success/skipped (#19095)Currently, when a dagrun is marked as success in the UI, the expectedsigterm is sent but the task continues running, changing the dagrun to runningbefore eventually failing. Same as well when marked as failed.Also, queued tasks continue running even when the dagrun was failedThe same thing happens when a dagrun times out. The tasks that are markedskipped, starts again, and then fails.This PR fixes these issues",0
"Change `ds`, `ts`, etc. back to use logical date (#19088)",5
Fix wrong commands in docs/breeze,2
"Don't install SQLAlchemy/Pendulum adapters for other DBs (#18745)This stops the MySQL libs being imported ""unnecessarily"" when Postgresis in use -- and there have been a few confusing reports of the mysqlclient libs causing problems in rare cases, so lets avoid the import ifwe can.",2
Update taskinstance REST API schema to include dag_run_id field (#19105)This PR adds dag_run_id field to taskinstance schema,2
Static start_date and default arg cleanup for Microsoft providers example DAGs (#19062),2
Prevent scheduler crash when serialized dag is missing (#19113)Scheduler._send_dag_callbacks_to_processor calls dag_run.get_dag whichraises exception. This PR changes to calling dagbag.get_dag and changingScheduler._send_dag_callbacks_to_processor args to accept dag instead of dag_run.,2
"Fix catchup by limiting queued dagrun creation using max_active_runs (#18897)Currently, when catchup is True, we create a lot of dagruns limited bymax_queued_runs_per_dag setting. This is not efficient as some dagruns takeslonger to run.This PR brings back the old behavior of not creating dagruns once max_active_runsis reached thereby solving the catchup issue.Now, the dagruns appears as though they were created in running state",1
[Minor] Fix padding on home page (#19025)* [Minor] fix padding on home page* fix lint* Move filters and pagination out from dags-table-wrap* - fix last row of a table - col span should be 10 instead of 9* add bottom padding* Update airflow/www/static/css/dags.cssCo-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>,2
Hide tooltip when next run is none (#19112)* Hide Next Run when it is NoneCheck for `is not none` instead of `is defined` when showing the next run tooltip* check next_dagrun is defined and not none,2
Add Pinterest to Airflow users list (#19117),1
Warn about unsupported Python 3.10 (#19060)Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
"Update ""Release Airflow"" doc (#19111)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
Upgrade the Dataproc package to 3.0.0 and migrate from v1beta2 to v1 api (#18879),5
Remove distutils usages for Python 3.10 (#19064),4
Move away from legacy importlib.resources API (#19091),2
Update docstring to let users use `node_selector` (#19057),1
Add sensor default timeout config (#19119),5
Update virtualenv guide to use correct constraints file (#19141)The constraints file that was used in the example code snippetsis a constraints file for released versions (e.g.:airflow/constraints-main/constraints-3.6.txt). Which was leading to longdependency resolution times as well as dependency version conflicts whenusing the latest source from main. The correct constraints to use in thiscase is for example: airflow/constraints-main/constraints-source-providers-3.6.txt,5
Fixup string concatenations (#19099),0
Chart: Fix `extraEnvFrom` examples (#19144),0
Fix queued dag runs changes catchup=False behaviour (#19130),4
Consolidate method names between Airflow Security Manager and FAB default (#18726)* Replace add_view_menu with create_resource.* Replace add_permission_view_menu with create_permission.* Replace find_permission_view_menu with get_permission.* Replace find_view_menu with get_resource.* Replace get_all_view_menu with get_all_resources.* Replace find_permission with get_action.* Replace del_permission_view_menu with delete_permission.* Replace del_view_menu with delete_resource.* Replace del_permission with delete_action.* Replace add_permission_role with add_permission_to_role.* Replace find_permissions_view_menu with get_resource_permissions.* Replace self.del_permission_role wth remove_permission_from_role.* Replace exist_permission_on_roles with permission_exists_in_one_or_more_roles.* Replace find_roles_permission_view_menus with filter_roles_by_perm_with_action.* Replace get_db_role_permissions with get_role_permissions_from_db.* Replace add_permission with create_action.* Remove unused exist_permission_on_view function.* Rename local variables.* Remove sqla model names from SecurityManager base class.* Update names in add_permissions_view.* Flake8.* Use updated perm names.* Use updated perm names for local vars.* Use black.* Reorder import statements.* Remove accidental renaming of BaseSecurityManager.roles.,4
Crerate TI context with data interval compat layer (#19148),5
Add test for interval timetable catchup=False (#19145),3
add detailed information to logging when a dag or a task finishes. (#19097)* add detailed information to logging when a dag or a task finishes.* make logging of start_date/end_date ISO format to be consist* fix pre-commit* use only %s in new logging statements to gracefully handle when certain variables are None* fix precommit* use self._state instead of get_state().  add computation for dag run duration based on start_date and end_date* make linter happy with format* fix typo* put back missing reference to _stateCo-authored-by: Daniel Imberman <daniel.imberman@gmail.com>,2
"Navigate directly to DAG when selecting from search typeahead list (#18991)When searching for a DAG, we are given a typeahead list of matching DAGs. When selecting a DAG from the typeahead list, the previous behavior was to execute a search query on that dag_id. Instead, we now go directly to that DAG.",2
Replacing non-attribute template_fields for BigQueryToMsSqlOperator (#19052)* Replacing non-attribute template_fields for BigQueryToMsSqlOperator* Updating source_project_dataset_table arg in example DAG,2
Update INTWILD.md to add Retailink (#19115),2
Added Wayfair to the list of companies using Apache Airflow (#19162),1
"Adds Sprylab as user ""in the wild"" (#19159)",1
Add TriggererJob to jobs check command (#19179),1
Allow specifying extras when using breeze initialize_local_virtualenv (#19178),5
Remove incorrect type comment in Swagger2Specification._set_defaults classmethod (#19065),1
Fix breeze docker version parsing (#19182),2
Add migration job resources (#19175)* Add migration job resources* Add change notes* Fix test* Linter* Update chart/values.schema.json* Update chart/values.schema.jsonCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>* LinterCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Fix airflow jobs check cmd for TriggererJob (#19185),0
"Faster PostgreSQL db migration to Airflow 2.2 (#19166)Bigger Airflow databases can take a long time to migrate the database,particularly if they have a lot of task instances. On PostgreSQL, creating anew table is much faster than updating the existing table.",5
Clear ti.next_method and ti.next_kwargs on task finish (#19183),5
Add role export/import to cli tools (#18916)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Add test_connection method for Snowflake Hook (#19041)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add a fix on Ubuntu 20.04 for mysql_config not found error (#19184),0
Add DagRun.logical_date as a property (#19198),5
Fix static checks (#19200),0
Moving the example tag a little bit up to include the part where you specify the snowflake_conn_id (#19180),4
Allow Airflow UI to create worker pod via Clear > Run (#18272),1
Update to correctly resolve 'PostgresOperator' (#19212)Changed ``airflow.providers.postgres.operator.postgres.PostgresOperator`` to ``airflow.providers.postgres.operators.postgres.PostgresOperator``,1
Fix Unexpected commit error in schedulerjob (#19213),0
Fix bug in Dataflow hook when no jobs are returned (#18981),1
Removes hard-coding of /tmp filesystem cache (#19208)Filesystem cache has been hard-coded to /tmp. This would notwork for Windows but (more importantly) it does not work in casethe user has no access to /tmp directory and TMPDIR is set toa different value.Fixes: #19206,0
Chart: Add labels to jobs created by cleanup pods (#19225)Problem:Currently pods created by cleanup jobs doesn't add labels similar to metadata.labels.This prevents us to use network policies to block / allow to communication to internal servicesHow does this PR fix the problem above:This PR adds ability to utilise existing labels and allow to use new labels from values.yaml,1
Doc: Improve tutorial documentation and code (#19186)1. Added instructions on adding postgres connection.2. Modified proper SQL syntax.3. Remove redundant lines when writing to CSV.4. Added QUOTE argument for copy_expert,1
"Fix hard-coded /tmp directory in CloudSQL Hook (#19229)The /tmp directory is not ""guaranteed"" to be THE /tmp one. Thereare cases where TMPDIR or other env variables are set to overrideit (for example when user has no write access to /tmp dir) andwe should respect that. The gettempdir() will produce the righttemporary directory based on the env variables set in the system.",5
Update dataflow.py (#19231),5
Dev: Clarify file naming in release verification doc (#19233),2
"Move validation of templated input params to run after the context init (#19048)* Fix #14682, move input params validation into `execute()`* Adjust tests for LocalFilesystemToS3Operator",5
Added sas_token var to BlobServiceClient return. Updated tests (#19234),3
Create CommandExecutor for raising an exception in case of error during cmd execution (#17651),0
Add release date for when an endpoint/field is added in the REST API (#19203),1
"Fix release check script (#19238)There have been some changes to the filename conventions over time  and the release check script was not updated to reflect this.  This PR fixes the script and tries to simplify it a little bit.  In particular, the regex approach used previously was broken by the removal of the `-bin` identifier.  It is easy enough to simply compute all the expected files exactly and look for them, so that is what we do here",2
"sqlite_default has been hard-coded to /tmp, usegettempdir instead, (#19255)respecting tempdir enviroment variables.See #19208, #19229.",1
Edit permalinks in OpenApi description file (#19244),2
"Add value to ""namespaceId"" of query (#19163)",1
SFTP hook to prefer the SSH paramiko key over the key file path (#18988),2
"Rename variable `serialize_dag` to `serialized_dag` (#19265)The expression `serialize_dag` represents an action, but the variable is actually a dag that has been _serialized_.",2
Update BigQueryCreateExternalTableOperator doc and parameters (#18676),2
Added theScore to the list of companies using Apache Airflow (#19253)* Added theScore to the list of companies using Apache Airflow* Add space between Org name and GitHub handleCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,0
#19223 add mongo_db param to MongoSensor (#19276),2
Support query timeout as an argument in CassandraToGCSOperator (#18927)Support query timeout as an argument in CassandraToGCSOperator (#18927),1
Modify doc contributing (#19124),2
Grammar mistake (#19283),5
Fix MySQL db migration with default encoding/collation (#19268),5
"pyenv related docs added, warning message in breeze initialize-local-virtualenv command (#19100)",5
docs: reorder imports in tutorials 🎨 (#19035)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,2
Updating Apache HDFS title in index doc (#19169),2
Add support of placement in the DockerSwarmOperator (#18990),2
Add explicit session parameter in PoolSlotsAvailableDep (#18875),2
Fix task instance modal in gantt view (#19258),0
"Update BaseOperator type hints for retry_delay, max_retry_delay, dag. (#19142)",2
Remove redundant Union of one item (#19297),4
Typo fix in TESTING.rst (#19216),3
Fixing ses email backend (#18042),0
"Skip triggerer in 'breeze start-airflow' if on 3.6 (#19305)The triggerer does not work on 3.6, so there's no point showing a deadpane.",1
Clarify rat test guidance in release check docs (#19296)* Clarify rat test guidance in release check docsIn the context of the rat tests there are two binaries that need unpacking -- the rat test jar and the airflow release.  This change clarifies the references to the airflow binaries.,4
Expanding ``.output`` operator property information in TaskFlow tutorial doc (#19214),2
Use fab models (#19121)* Use FAB models.* Use FAB models.* Remove incorrect conversions to new permission naming scheme.* Fix missing FAB renames.* Remove unused FAB compatibility fixes in models.py.* Remove additional uses of view_menu.* Move airflow/www imports from global to function local.,1
"Add 2.2.1 to readme, changelog, updating, etc (#19318)",5
"change pyenv url readme, more specific (#19319)",4
Correctly handle get_pty attribute if command passed as XComArg or template (#19323),4
Prepare documentation for October Provider's release (#19321),1
Switch default Python version to 3.7 (#19317)Continuation of https://github.com/apache/airflow/pull/18922This case was missed and is needed to have correct image tagged for Dockerfiles,2
Fix various typos (#19316),2
"Add missing parameter documentation for ""timetable"" (#19282)",2
add mongo_db param to function doc string (#19280),2
Pre commit spellcheck fix (#19301),0
More friendly output of the airflow plugins command + add timetables (#19298),1
Chart: add resources for cleanup and createuser jobs (#19263),1
Add support of `path` parameter for GCloud Storage Transfer Service operators (#17446)Co-authored-by: ekarimovDH <evgenii.karimov@deliveryhero.com>,4
Remove duplicated entries in changelog (#19331)Amazon provider had wrong tag set (2.3.0 pointed to rc1 insteadof rc2) which resulted in duplicated entries in changelog.This PR fixes it,0
"Update known warnings for Python 3.7 (#19333)After seting 3.7 the default (#19317) the warning printed byPython during importing all providers (specifically apache beam)has slightly changed. Apparently collections.abc warning wasa bit more ""scary"" - warning that it's 3.9 not 3.10 where theold collection imports will stop working (Note that actuallythis did not happen even in 3.10, apparently)This PR fixes the ""known"" warning message to match it but alsoa separate PR (https://github.com/apache/beam/pull/15850) wasopened to Beam to get rid of the warnings altogether.Also seems 'dns` stopped generating this warning so I removed itand in case warnings are generated, they are printed outside ofthe folded group, so that it's immediately visible.",2
"Temporarily remove mypy checks to stop PRs from failing (#19345)After we moved to Python 3.7 as default, it had a ripple effectthat MyPy checks started failing. We aim to fix itpermanently in #19334 but this needs a bit more changes, so forthe moment we skip the checks.",4
"Chore: Use enum for ""__var"" and ""__type"" members (#19303)",1
Doc: Small clarification of base executor docstring (#19336),2
Docs: Fix typo in ``dag-run.rst`` (#19340),2
"Clarify dag-not-found error message (#19338)In this context, what's really happening is, we can't find the dag.  From a userperspective, when you encounter this error, 'could not find the dag' isa more intuitive representation of the problem than 'could not find the dag_id'.",2
Dags-in-image pod template example should not have dag mounts (#19337),2
Chart: Update default Airflow version to 2.2.1 (#19326),5
Modernize dockerfiles builds (#19327)* Modernize dockerfiles builds* fixup! Modernize dockerfiles builds* fixup! fixup! Modernize dockerfiles builds* fixup! fixup! Modernize dockerfiles builds* fixup! fixup! fixup! Modernize dockerfiles buildsCo-authored-by: mik-laj <mik-laj@example.org>,2
"Bugfix: Check next run exists before reading data interval (#19307)Fix #19304, and also an issue on scheduling a DAG's first-ever run introduced in #18897. We could fix it outside this function, but if `next_dagrun` is None, the next run's data interval is supposed to be None in the first place, so checking inside this function just makes sense.closes https://github.com/apache/airflow/issues/19343closes https://github.com/apache/airflow/issues/19304",0
Enable mouse mode by default in start_airflow tmux session (#19325),1
Removes unused state transitions to handle auto-changing view permissions. (#19153)* Remove unused FAB state transitions.* Remove unused FAB constant.,1
Make scripts/in_container/check_environment.sh Google Shell Guide Compliant (#19350),1
Fix hidden tooltip position (#19261)Only apply a large z-index when the tooltip is supposed to be display.,0
"Add Note to SLA regarding schedule_interval (#19173)This document entry is intended to make explicit the requirement that SLAs will only be triggered for tasks in that are part of a scheduled DAG run.Manually triggering DAGs with schedule_interval of None causes the error```  File ""/home/airflow/.local/lib/python3.6/site-packages/airflow/dag_processing/processor.py"", line 411, in manage_slas    while dttm < timezone.utcnow():TypeError: '<' not supported between instances of 'NoneType' and 'datetime.datetime'```And manually triggering DAGs with a valid schedule_interval do not produce tasks that can invoke an SLA Miss.  Only scheduled DAGs will check tasks for SLA misses.",2
Tests: Refactor ``LoggingCommandExecutor`` to use subprocess devnull (#19354)Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>,1
"Only mark SchedulerJobs as failed, not any jobs (#19375)In `adopt_or_reset_orphaned_tasks`, we set any SchedulerJobs that havefailed `scheduler_health_check_threshold` to failed, however a missingcondition was allowing that timeout to apply to all jobs, not just SchedulerJobs.This is because polymorphic identity isn't included for `update()`:https://docs.sqlalchemy.org/en/13/orm/query.html#sqlalchemy.orm.query.Query.updateSo if we had any running LocalTaskJobs that, for whatever reason, aren'theartbeating faster than `scheduler_health_check_threshold`, their stategets set to failed and they subsequently exit with a log line similar to:    State of this instance has been externally set to scheduled. Terminating instance.Note that the state it is set to can be different (e.g. queued orup_for_retry) simply depending on how quickly the scheduler hasprogressed that task_instance again.",1
Fix PostgresHook import in tutorial (#19374),2
Fix S3ToRedshiftOperator (#19358),1
Check if job object is None before calling .is_alive() (#19380)Co-authored-by: Jonathan Fernandes <jfernandes@virtela.net>,5
Cleanup of start_date and default arg use for Apache example DAGs (#18657),2
"Fix message on ""Mark as"" confirmation page (#19363)In an earlier refactor I created a macro called `message` which""stomped"" on the variable of the same name set in the view, meaning thepage shows `<Macro message>` instead of the string we meant to set.This ""fixes"" it by using a less-likely-to-clash name for the macro (andfixing the typo in `dismissible` parameter.)",2
Improve Kubernetes Executor docs (#19339),2
Fix Toggle Wrap on DAG code page (#19211)Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,2
Adding more Apache Airflow versions (#19392),1
Fix downgrade for a DB Migration (#19390)The downgrade was not working because of the issues fixed in this PR,0
Doc: Fix typos in variable and comments (#19349)Fix typos in comments and funtion's parameter:necssary - necessarydeserialised - deserialized,2
"Use ``execution_date`` to check for existing ``DagRun`` for ``TriggerDagRunOperator`` (#18968)A small suggestion to change `DagRun.find` in `trigger_dag` to use `execution_date` as a parameter rather than `run_id`.I feel it would be better to use this rather than `run_id` as a parameter since using `run_id` will miss out checking for a scheduled run that ran at the same `execution_date` and throw the error below when it tries to create a new run with the same `execution_date`:```sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""dag_run_dag_id_execution_date_key""```There is a constraint in `dag_run` called `dag_run_dag_id_execution_date_key` which can be found [here](https://github.com/apache/airflow/blob/c4f5233cd10ae03ee69fba861c8a6fa64e1f8a71/airflow/models/dagrun.py#L103).",2
"Ensure ``catchup=False`` is used in example dags (#19396)- Along with the effort to move to static start times, use catchup=False  to avoid users inadvertently causing a large backlog of tasks.",2
"Task should fail immediately when pod is unprocessable (#19359)When pod has invalid requirements, e.g. resource limit < resource request,the kubernetes api may return ""Unprocessable Entity"".  In this scenario,the kubernetes executor should fail the task immediately, rather than setit to be attempted againcloses https://github.com/apache/airflow/issues/19320",0
fix SagemakerProcessingOperator ThrottlingException (#19195)fix SagemakerProcessingOperator ThrottlingException (#19195),1
Fix --disable-mssql-client-installation error (#19295)* Fix --disable-mssql-client-installation error* Add flag to documentation* Fix documentation with hook,1
"Simplify ""invalid TI state"" message (#19029)Currently in the web UI on task instance details page, if a task is in the ""up for retry"" statewe will see this message:""Task is in the up_for_retry state which is not a valid state for execution. The task must be cleared in order to be run.""This might suggest to the user ""you need to clear this in order for this task to run"".  But this is not true.  But it is not simple to make it clearer, because this function is used for a lot of different scenarios.  For example, checking for ""schedulable"" tasks, and ""queueable"" tasks. So not only would we need to keep track of which states actually require user intervention, but the desired action (e.g. queue vs schedule vs run).So I think the best course of action is to simplify this to say only what is always true, and that is just the state.",1
Fix field relabeling when switching between conn types (#19411),0
Add documentation for RC2 release of Amazon provider for October (#19413),1
Add DruidOperator template_fields_renderers fields (#19420),1
"Fix typos in warnings, docstrings, exceptions (#19424)",2
Chart: Use strict k8s schemas for template validation (#19379),5
"Fix moving of dangling TaskInstance rows for SQL Server (#19425)SQL server uses a different syntax for creating a table from a select tothe other DBs we support.And to make the ""where_query"" reusable across all DBs (SQL Serverdoesn't support `WHERE (col1,col2) IN ...`) the delete has beenre-written too.",4
Chart: Allow disabling the Helm hooks in the helm chart (#18776),2
Fix log timezone in task log view (#19342) (#19401),2
Allow specifying kerberos keytab in the chart (#19054),2
"Fix serialization of Params with set data type (#19267)This is a solution for https://github.com/apache/airflow/issues/19096Previously, the serialization of params did not run the param value through the `_serialize` function, resulting in non-json-serializable dictionaries.  This manifested when a user, for example, tried to use params with a default value of type `set`.Here we change the logic to run the param value through the serialization process.  And I add a test for the `set` case.closes https://github.com/apache/airflow/issues/19096",0
Add Changelog for Airflow Chart 1.3.0 (#19417),2
Fix Serialization when``relativedelta`` is passed as ``schedule_interval``  (#19418)Also add relativedelta to timetable test cases to ensure this does notregress.Fix #19416,0
Fix bug when checking for existence of a Variable (#19395)`check_for_write_conflict` was a `staticmethod` but for some reason it was ignored,5
"Change the name of link to ASF downloads (#19441)The ASF used to use mirrors to distribute their software, howeverrecently they changed to use CDN. The mechanism might change inthe future (even if currently CDN is used the ASF 'mirrors' pageand closer.lua script provide a fully ASF-controlled mechanism toswitch to the right mechanism, however technically speaking thecurrent solution is not 'mirrors' but it is CDN, therefore it makessense to rename it to generic downloads.",1
Clean-up of google cloud example dags (#19436)- Use start start_date- Use catchup=False- Tidy up the chaining of tasks in some cases,5
use DefaultAzureCredential if login not provided for Data Factory (#19079),5
Decouple name randomization from name kwarg (#19398),5
"Update Databricks operators to match latest version of API 2.0 (#19443)This includes:- added support for `jar_params` in the `DatabricksRunNowOperator`- added support for `pipeline_task` in the `DatabricksSubmitRunOperator`Also, added documentation for `DatabricksRunNowOperator`this fixes #10786",0
Fix Daylight Saving Time issue with test case (#19456),3
Doc: Fix the parameter name 'deploy-mode' in spark.rst (#19403) (#19404),2
Updates version of airflow in docker examples (#19455)Unfortunately those cannot be updated at release time as theyneed released version of Airflow to run.,1
MySQLToS3Operator  actually allow writing parquet files to s3. (#19094)* Update mysql_to_s3.py* Actually fix it,0
removed unnecessary specifity and complication in contrib quickstart (#19430),4
Add dataproc metastore operators (#18945),1
"Authentication with AAD tokens in Databricks provider (#19335)* Authentication with AAD tokens in Databricks providerMany organizations don't allow to use personal access tokens, and instead force to usenative platform authentication.  This PR adds the possibility to authenticate to AzureDatabricks workspaces using the Azure Active Directory tokens generated from Azure ServicePrincipal's ID and secret.",1
Fix mismatch between docs and Azure Data Factory Hook (#19442),1
"Change to correct type in KubernetesPodOperator (#19459)A tiny fix to the type hinting, configmaps is expected to be list and not string (as can be seen in line 242)",5
FAB still requires WTForms < 3.0 (#19466),1
Add a proper example to patch DAG (#19465)Add an example based on the workaround described in https://github.com/Redocly/redoc/issues/1238,2
Fix whitespace error causing failing graphviz test (#19472)The latest graphviz module subtly changed the output in anon-significant way that was causing test failures.,0
Fix Cloud SQL system tests (#19014)- Add creation of Fine-grained bucket for ACLs- Add patching of environment variables- Add unique postfix to instances names,0
Remove `host` from hidden fields in `WasbHook` (#19475),1
"Add missing ""end_date"" to hash components (#19281)",5
Do not crash with stacktrace when task instance is missing (#19478)When task instance was missing (because old dag runs did not haveit) trying to run the missing task from the UI resulted instacktrace rather than nice error message.Fixes: #19477,0
"Add DAG file processing description to Scheduler Concepts (#18954)One of the challenges for Airflow users it to understand how the Airflow Scheduler turns Python code into DAGs.  Often users will be confused as to why their DAGs don't show up in the user interface, or they'll see DAG processing errors and not know why.This documentation entry is intended to clarify the process to new and experienced users alike, by providing a concise description of the process, along with a diagram of the flow and links to key configuration values that affect the way Airflow processes DAGs.",2
Fix typos in Hive transfer operator docstrings (#19474),2
Clarify guidance re trust of keys in release docs (#19480)* Clarify guidance re trust of keys in release docs1. Kaxil's key referenced in the docs is expired.  I update with the current key.2. keys.openpgp.org no longer seems to be set as the default (at least it was not on my machine).  So I update the key import to specify this server i.e.3. clarify language concerning the remote key servers* fix spelling,0
Update description of release process for adding new major release (#19483),1
Update Databricks API from 2.0 to 2.1 (#19412),5
Bump chart version to 1.4.0-dev (#19485),2
Update helm chart release docs (#19494),2
"Fix docker ""after entrypoint"" custom script example (#19495)",1
Clarify that .asf.yml and codecov.yml should be changed in main (#19496),4
"``KubernetesExecutor`` should default to template image if used (#19484)Currently, the user must specify image and tag in airflow.cfg, even when they are using a pod template file.  If the pod template file specifies an image and tag, the user should not be forced to also specify this in airflow.cfg.",5
Minor grammar and sentence flow corrections in pip installation docs (#19468)* Minor grammar and sentence flow corrections in pip installation docs (#19468),2
Optimizes running tests for public GitHub Runners. (#19512)We started to get more (and almost consistent) OOM failures whenwe tried to run all tests in parallel for the public GitHubrunners. This could previously happen for Providers and Integrationtests but it started to happen for Core tests.This PR optimizes this to also make Core tests sequentially runand refactors the code to make it much more readable and easy tounderstand what's going on there.,1
Update deferring.rst (#19509)add missing ')' in code example,1
Dev: Update Airflow versions in issue templates (#19521),0
Update Operators and Hooks doc to reflect latest (#19501),3
"Do not check for S3 key before attempting download (#19504)S3Hook.download_file first checks object existence then downloads.This resolves creds 2 times.  We don't need to check existence.Just ask for the key and if it's not there you'll know from the error.And if it is there, you'll only have resolved creds once.Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",0
"Improve message and documentation around moved data (#19453)* Improve message and documentation around moved dataIn Airflow 2.2.2 we introduced a fix in #18953 where the corrupteddata was moved to a separate table. However some of our users(rightly) might not have the context. We've never had anythinglike that before, so the users who treat Airflow DB asblack-boxes might get confused on what the error means and whatthey should do in this case.You can see it in #19440 converted into discussion #19444 and #19421indicate that the message is a bit unclear for users. This PR attempts toimprove that it adds `upgrading` section to our documentation and have themessage link to it so that rather than asking questions in the issues,users can find context and answers what they should do in our docs.It also guides the users who treat Airflow DB as ""black-box"" on how theycan use their tools and airflow db shell to fix the problem.",0
Disable test code coverage for PRs (#19523)The test code coverage took a lot of memory (2-3GB) when coretests were running (specifically test_kubernetes_executor.py) andthe memory was kept for the duration of whole test.This caused intermittent memory issues on public GitHub runners.The change only uses test coverage for `main` builds where we havea lot of memory available.,3
Fix DAG docstrings (#19531),2
Pass custom_headers to send_email and send_email_smtp (#19009)Co-authored-by: Przemyslaw Piorkowski <przemyslaw.piorkowski@polpc06179.allegrogroup.internal>,4
Resurrect python openapi client generator (#19155),5
"Fix missing dagruns when ``catchup=True`` (#19528)There's a bug that when the max_active_runs is reached, run dates could skip.This PR fixes itCloses: #19461",0
Define datetime and StringID column types centrally in migrations (#19408)We have various flavours of the code all over the place in manymigration files -- which leads to duplication and things not being insync.This pulls them once in to a central location.,2
Copy AppBuilder Base class verbatim from FAB to Airflow (with attribution) (#19322)* Copy FABs AppBuilder class directly into Airflow.* Add doc indicating copyright.* Remove unused FAB constant.* Reorder import statements.,2
Remove duplicate get_connection in SnowflakeHook (#19543),1
Remove remaining `pylint: disable` comments (#19541),5
Adding support for using ``client_type`` API for interacting with EC2 and support filters (#9011)1. Adding support for client_type API for accessing EC2 in EC2Hook2. Adding support for using filters when accessing EC2s (only for client_type API),1
"Show if an executor supports ``/run`` via duck-typing (#18787)This avoids repeated isinstance() checks in the view, which is bothmessy and less extendable.",1
Fix typo in Changelog (#19551),4
Dev: Update Airflow versions in issue templates (#19521) (#19550)Add 2.2.2rc1 across the board to track issues,0
Do not require all extras for SalesforceHook (#19530),1
"Upload KinD logs on cancell too (#19554)If the job times out, it is ""cancelled"", rather than failed, which meansthat the logs were not uploaded.This will likely also catch a few cases where the job is cancelled cosof another push to the branch/PR, but it's better to have too many logsthan not enough to debug problems",0
New Tree View (#18675)Add react to existing ui and build a new tree view that can properly handle task groups.Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>Co-authored-by: Sam Wheating <samwheating@gmail.com>,0
Fix typo on ``necessary`` word (#19565),2
Small formatting tweak to GCP SSH conn doc (#19562)The newline in the bulleted list was causing a line item to become a bolded blockquote.,1
Minor touch up for async docs (#19539),2
"Fix IntegrityError in `DagFileProcessor.manage_slas` (#19553)The DagFileProcessor.manage_slas does not consider if an SlaMiss already exists inDB while inserting slas.If an SLA for a task is missed and recorded, on checking SLA again, this taskcomes up again if there's no recent run of the task and we try to insertthe record into the SlaMiss table again, this results in Integrity error.This PR fixes that by avoiding insert if the record already existsCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@apache.org>",0
Improve documentation for tasks run command (#19580)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Update Azure modules to comply with AIP-21 (#19431),5
Databricks jobs 2.1 (#19544),5
"Databricks: allow to specify PAT in Password field (#19585)Currently, the PAT is specified in the extra section, where it'svisible, so it's less secure. This PR allows to put PAT into Passwordfield where it will be masked when editing the existing connection",2
Add support in GCP connection for reading key from Secret Manager (#19164),1
Add hdfs requirement for hdfs provider (#19540),1
Cleanup of start_date and default arg use for Amazon example DAGs (#19237),2
Clean-up of google cloud example dags - batch 2 (#19527)- Use static start_date- Use catchup=False- Tidy up the chaining of tasks in some cases- Remove unnecessary specification of default conn ids,4
Add hook_params in BaseSqlOperator (#18718),1
Fix typo: `parsed_results` -> `parse_results` (#19588),2
Add script to generate issue for status of testing of the rc (#19247),3
"Add 2.2.2 to readme, changelog, updating, etc (#19600)",5
Stop polling when Webserver doesn't start up in Kube tests (#19598),3
Fix some Changelog entries (#19604)Some changelog entries were not formatted correctly,4
"Improve automation for docker image release (#19573)The ""latest"" tags for docker images were not applied in recentreleases - mainly because the process of doing it was not followed,but this was also not obvious as preparing the rc image and finalimages was different - the latest images required extra manualstep.This PR modifies the ""image preparation"" script to ask a questionwhether the latest images should be tagged when non-RC buildis being prepared.Fixes: #19569",0
Add how-to Guide for MSSQL operators (#19470),1
#16691 Providing more information in docs for DataprocCreateCluster operator migration (#19446),1
Add upgrade note on execution_date -> run_id (#19593),1
"Catch AccessDeniedException in AWS Secrets Manager Backend (#19324)When a Secrets Manager call fails due to the secret not being accessible to the IAM principal, rather than just not existing, it reports AccessDeniedException instead of ResourceNotFoundException.This can happen e.g. if a cluster maintainer wants to limit an airflow instance to secrets that are tagged with a specific resource.  Today, this results in an uncaught exception.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",0
update tree data fetching (#19605)- add `base_date` to refresh api request- sort runs only on the webserver- add test for auto-refresh stop,3
Add test_connection method for sftp hook (#19609),1
Fix incorrect mocking in SFTPHook tests (#19617),3
Declare data interval fields as serializable (#19616),5
Remove reference to deprecated operator in example_dataproc (#19619),5
Remove unnecessary connection form customizations in Azure (#19595),4
Add Helm Chart 1.3.0 as an option in airflow_helmchart_bug_report.yml (#19621),5
Move scripts for prod image preparation to dev (#19623)The script belongs to dev. Also it had `echo` debug commands leftafter testing that are removed now.,4
Update docs about releasing providersk (#19549),1
"Disable yarn-dev in start-airflow command (#19626)When you run start-airflow, by default it also run `yarn dev`command, however if you've never built assets before, yarn devis very slow first time and the webserver started before the distfolder was even created which caused asset-less airflow experience.There was another race condition even if you did build theassets before. If you run start-airflow on MacOS or Windows whenthe filesystem was slow, there could be a case that yarn devcleaned up the dist folder while webserver was starting andit could lead again to asset-less experience if you were unlucky.Also running `yarn dev` has the side effect of removing the checksumfile which is used to see if any of the assets changed and whetherthey need recompilation. As the result after running `start-airflow`you always got the warning that the assets need recompilation.This PR disables automated start of `yarn dev` and suggests to runit manually instead if there is a need for dynamic assetrecompilation. Also when `start-airflow` is run and we are startingairflow from sources rather than PyPI, asset compilation isexecuted if the checksum is missing or does not match the sourcefiles.Related to: #19566",2
Fix helm chart 1.3.0 changelog (#19632),4
Update INTHEWILD.md (#19636),5
Fix failing CI phase with unhealthy container issue (#19633)Fix failing CI phase with unhealthy container issue* Add post cleanup* Pin pinot to stable version* Pin grafana to stable version  Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,4
Ensure the example DAGs are all working (#19355)Some of the example DAGs are not working and some are a repeat ofwhat we have in another DAG. This PR aims to update the examplesso we can only have working DAGs in the examples,2
Add example SLA DAG (#19563)This PR adds example SLA DAG,2
Improve `airflow-github` dev script (#19631),1
Minimze production js files (#19658)Add TerserPlugin to minimize js files for production builds,2
Cleanup dynamic `start_date` use for miscellaneous Google example DAGs (#19400)* Cleanup dynamic start_date use for misc Google example DAGs* Updating task dependencies based on XComArgs,5
Misc. documentation typos and language improvements (#19599),1
Clean up dynamic `start_date` values from docs (#19607),2
"Fix dumping container logs on error (#19645)When we optimized tests for memory use we added cleanup of allcontainers after each test suite. Unfortunately it causeddumping container logs to stop working because this dumping wasdone only only when the script was exiting.This PR moves dumping container logs to between the test run andcleanup, so that we can see the logs when there is a test failure.Related to: #19633 where the logs were not dumped and it made theanalysis much more difficult.",2
"Revert ""Copy AppBuilder Base class verbatim from FAB to Airflow (with attribution) (#19322)"" (#19661)This reverts commit 37a12e9c278209d7e8ea914012a31a91a6c6ccff.",4
Configurable logging of XCOM value in PythonOperator (#19378),1
Update Airflow release guide (#19663),5
Fix `airflow db check-migrations` (#19597),5
"Add more complete instruction for reproducing failed integration tests (#19646)When integration tests are failing, breeze prints the exactreproduction step to recreate the same environment. However whenintegration tests were enabled it missed the --integrationflags that were necessary to enable the integrations.This PR adds the --integration flags to the instructions and alsoadds the comment that Kerberos integration currently does not workwith docker-compose v2.",2
Fix argument error in AzureContainerInstancesOperator (#19668),1
Fix CI tests so they correctly fail in case of error! (#19678),0
"Add back-compat to db migrations for helm chart < 1.4 (#19677)Prior to 1.4 of the helm-chart we ""inlined""/hard-coded thewait-for-migrations command (as a `python -c` command) and that versiondidn't correctly initialize the alembic context, meaning that anythingtriggering a context.get_bind() at migration version import time causedan error.This fixes that problem by:- Making StringID and TIMESTAMP lazy objects, so that just importing  them at the top level doesn't trigger the problem- Stop calling it at the top level where it doesn't matter -- we have a  few migrations that create a ""copy"" of TaskInstance/DagRun model to do  an update, and those have been changed to    a. just use `String()` (as the exact type doesn't matter for UDPATE)    b. to only define the PK+columns being updated, as that is enough",5
Use built-in check-migrations command for Airflow>=2 in helm chart (#19676),2
Fix the link to the Graph view in the Tree view DAGRun modal (#19679)Co-authored-by: Bas Harenslak <bas@astronomer.io>,2
Removal debug code that causes success result state for CI tests (#19682)Co-authored-by: Mammadov <khalid.mammadov@prudential.co.uk>,3
"Fix broken KubeExecutor tests (#19680)We had deleted the example_kubernetes_executor_config dag and put it allin the sinle example_kubernetes_executor, but the tests had been brokenfor a while that we didn't notice.",3
"Fix: Do not render undefined graph edges (#19684)* Fix: Do not render undefined graph edgesA user had an issue where a `targetId` was undefined and that caused the whole graph view to crash. Instead, we should check for the source and target before rendering the edge.* move all checks to one line",4
Clean-up of google cloud example dags - batch 3 (#19664)- Use static start_date- Use catchup=False- Tidy up the chaining of tasks in some cases,5
Chart: Remove unnecessary pod_template_file defaults (#19690),2
Support impersonation_chain parameter in the GKEStartPodOperator (#19518),1
Add FAB base class and set import_name explicitly. (#19667)* Add FAB base class and set import_name explicitly.* Fix linter errors caused by FAB code* Update airflow/www/extensions/init_appbuilder.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/www/extensions/init_appbuilder.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>,1
"Fix failures with recent moto library 2.2.15 (#19693)The recent moto library is more picky about parameterspassed to it:* when you are sending too old logs they are rejected* when you are passing cloud formation template they are parsed  and validated for correctnessOur tests had artifficial values for those, which caused failureswith the recent moto version.This PR provides realistic values in tests to pass moto validation",5
Fix badly merged impersonation in GKEPodOperator (#19696)The #19518 was merged while we had false-positive test resultsdue to testing memory optmisation in CI - test failures wentunnoticed for the change.This PR fixes the problem (both in tests and in the code) andadds more tests to cover all scenarios,3
"Fix speed of yarn installation (#19697)The --network-concurrency=1 is very slow and even if this hasbeen added in #17293 to battle connection refused, it slows regularbuilds far too much.There is a new optimisation in progress that should significantlyreduce the yarn installations on kind-cluster deploy: #19210 andit should solve the problem much better.",1
"Coalesce `extra` params to None in KubernetesHook (#19694)Coalesce `extra` params to None in KubernetesHookWhen using UI form widgets FAB provides a empty string by default for every param.  This turns out to make a difference sometimes.  E.g. in this hook, we decide what to do depending on whether the param `is not None` -- and if you've created the connection in the UI, even though you didn't supply a value for this param, it will be `not None`In this case, I do not think this is a breaking change because e.g. if `kubeconfig_path` is empty string then loading it should fail.  This should just allow better functioning of the hook.",1
"Rework webserver cli tests to not retest Gunicorn (#19712)* Rework webserver cli tests to not retest GunicornWe were testing the webserver behaviour by actually invoking gunicorn -this is both slow, and testing code that isn't ours.Instead just test that we invoke gunicorn with the expected args",3
Chart: Update default Airflow version to 2.2.2 (#19603),5
"Speed up webserver boot time by delaying provider initialization (#19709)* Speed up webserver boot time by delaying provider initializationThis drops the time to first request from 37s to 20s by making thefollowing changes:- Don't pre-load the app when not in daemon mode.  The purpose of the call to `cached_app()` was to ensure that any  errors are reported on the terminal before it is detached to make  failures more obvious to the user (which is a good feature).  However the comment about ""pre-warm the cache"" was incorrect and did  not happen -- since we run gunicorn by spawning a whole new process  it doesn't share any state from the current python interpreter.- Don't load/initialize providers when only importing airflow.www.views  As it was written it would load the providers hook's at import time.  This changes it through a combination of cached properties and the  existing `init_connection_form` function.  (`extra_fields` is not set as a cached_property because of how FAB  works -- it iterates over all attributes of the class looking for  methods/routes and then looks at properties on it, meaning it would  still access the property too early)",5
"Speed up webserver start up in Kube tests (#19710)Thanks to a previous change to not load provider hooks too early we cantake advantage of the ""preload-app"" feature of Gunicorn to load theapplication once in the main gunicorn process before the workers areforked off.This change makes the webserver start up (time to serving first request)go from 20s to 5s.(The reason we don't just do this blindly everywhere is that it wouldmean plugins are loaded at start only, and is a change in behaviour. Butin tests this is fine.)",3
Added namespace as a template field in the KPO. (#19718),1
Fix task instance api cannot list task instances with None state (#19487)* Fix task instance api cannot list task instances with None stateThe task instance state can be None and in the API we accept `none` for null state.This PR fixes this issue by converting the `none` to None and improving the queryso that the DB can get this state.,1
"refactor: f-string and comment update (#19721)_ Update the f-string for the case when pool_name_length exceed, for coherent and consistent of the file._ Fix the docstring grammar  """"""Create a pool with a given parameters."""""" -> """"""Create a pool with given parameters.""""""",2
Remove duplicate line call in CI (#19728),4
Fix extra links url in tree view (#19714)The extra links was not fully plumbed through after the recent refactorof the tree view in #18675,4
Fix log endpoint for same task (#19672)Co-authored-by: huan.15 <huan.15@kakaocorp.com>,2
The Version should be 2.2.2 instead of 2.2.0.dev0 (#19738)As during the build of Dockerfile you will be receiving error at 254th i.e the link that is mentioned with the airflow version is not valid,2
"Databricks - allow Azure SP authentication on other Azure clouds (#19722)* Databricks - allow Azure SP authentication on other cloudsWhen other Azure clouds are used (US GovCloud, China, ...) otherauthentication endpoints should be used.  This PR allows to overwritethe authentication endpoint when using other clouds",1
Added wait mechanizm to the DataprocJobSensor to avoid 509 errors when Job is not available (#19740),0
Add DAG run details page (#19705)* Add new view for DAG Run details* Use already imported wwwutils package* Wrap datetimes in HTML time elementCo-authored-by: Bas Harenslak <bas@astronomer.io>,5
"Disclaimer in Kubernetes executor pod template (#19686)This came in when upgrading from 2.1.4 to 2.2.x, and using non-default dag folder.Because of https://github.com/apache/airflow/issues/8061all dags stopped working.Add a note that the examples for pod template are not final,and require more configuration variables to be passed explicitly.",4
docs: remove `self` parameter in the example pytest (#19763),3
Checking event.status.container_statuses before filtering (#19713),1
"Lower the recommended disk space requirements (#19775)The recommended disk space requirements for Breeze were set to40GB which is way to high (and our Public Runners do not have thatmuch of a disk space - this generated false warnings).Lowering it to 20GB should be quite enough for most ""casual"" users.",1
Run Other tests sequentially for Public GitHub runners (#19766)The Other tests take a lot of memory (> 1GB when tests ofwebserver are running). This causes OOM issues for Public GitHubrunners when those tests are run in parallel to other tests.This PR add Other to sequentially run tests which will make surethey are not run in parallel with any other tests.,3
"Add option to run PRs on public runners by maintainers. (#19772)When `use public runners` label is applied to a PR, that PR willrun on Public Runners even if it is created by the maintainer.",1
Doc: Fix absolute Doc link (#19780),2
Fix function name in example timetable (#19735),1
Add retagging images accross repos (#19778)Useful to refresh cache images to a different repository - inorder to speed up builds there.,1
Fix duplicate changelog entries (#19759)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,4
Adjust built-in base_aws methods to avoid Deprecation warnings (#19725)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Sync committers in ci config for self-hosted runners (#19786),1
Move PostgreSQL to be the first prod db listed (#19790),5
Correct table alignment in CI doc (#19794),2
"Revert ""Adjust built-in base_aws methods to avoid Deprecation warnings (#19725)"" (#19791)This reverts commit 4be04143a5f7e246127e942bf1d73abcd22ce189.",4
Add note to restart runners when updating committers (#19795)Also link to a script that can generate the list of committers,2
"Clarify Helm behaviour when it comes to loading default connections (#19708)Users find it surprising that our Helm Chart does not load thedefault connections. This is deliberate choice because Helm Chartshould not manage the database content and the default connectionsare just ""quick start"" type of data, but this should be explicitlystated in the documentaiton of Helm's configuration.This PR fixes it. See also discussion in #19688",0
Tests for Docker images in Python (#19737),2
"Fix OOM error in tests when using public Github Runners. (#19809)There was a side effect caused by th TestStandardRunnertest that caused broken logging configuration,which in turn created OutOfMemory condition for our PublicGitHubRunners.The problem was that the test overrode the configuration oflogging with some simple test configuration, but never restoredthe default configuration, which resulted in airflow.processorlogger that was created before contain empty handlers. Sincethe airflow.processor logger has ""propagate"" set to False,empty handlers normally cause a lastResort handler call, whichby default redirects everything to Stderr and this is whathappened in DagFile Processor tests. However, DagFileProcessoruses stderr_redirect which replaces sys.stderr with providedstream. In this case however the stream set (StreamLogWriter)redirected the output to ""airflow.processor"" logger - which inturn (as last resort) redirected everything to sys.stderr whichin turn redirected everything to ""airflow.processor"" logger etc.This resulted in:* OOM condition in Public GitHub Runners* DagFileProcessor failing with exceeded recursion depth when  there was enough memory to get there.The condition was triggered by two preceding tests:1) First test_plugins_manger.py initialized logging for   airflow.processor and stored it in logging manager2) The TestStandardTaskRunner test applied simpler configuration   but the way configure() works - it did not remove the   ""airflow.processor"" logger, but it REMOVED all handlers   registered for it - and never restored the default configuration3) The DagFileProcessor logs caused infinite recursionThe fix is two-fold:* the TestStandardTaskRunner restores default config after test* the DagFileProcessor sets default config before starting",5
Upload provider distribution artifacts during CI (#19807),1
"Fixes failure of image building (#19813)The read command introduced in #19737 returned non zer error codeon encountering EOF, and our bash script fail on that. This PRmakes sure that the return code is not taken into account for thatcommand.",1
Bring back Core and Other tests to be run in parallel (#19812)After merging #19809 we can very likely come back to parallelrunning of Core and Other tests as we separated them outthinking that the parallel runs were the cause of the problems.Those tests should be perfectly fine to run in parallel now.,1
Update docs to reflect that changes changes to the base_log_folder require updating other configs (#19793)* Update config.ymlUpdate docs to reflect that changes changes to the base_log_folder require updating other configs,5
Databricks: add more methods to represent run state information (#19723),5
hdfs provider: restore HA support for webhdfs (#19711),1
hive provider: restore HA support for metastore (#19777),1
fix bug of SparkSql Operator log  going to infinite loop. (#19449),5
add kubernetes 1.21 support (#19557),1
Update BranchSQLOperator doc string (#19715),2
Introduce DagRun action to change state to queued. (#19353),4
Cast macro datetime string inputs explicitly (#19592)This coerce template variables to strings to work around a limitation inlazy_object_proxy when used against built-in types.Some tests against macro functions are added to ensure macro functionsall work against lazy object proxy objects and avoid regression in thefuture.,1
"Fix PATH export in breeze tmux sessions (#19818)- The PATH which is exported in the CI Dockerfile was not making it'sway into the tmux session, as it was being overwritten by /etc/profile",2
Use hyphen instead of underscore to match other artifacts (#19820),1
Clean up ``default_args`` usage in docs (#19803)This PR aligns `default_args` usage within docs to updates that have been made to example DAGs across the board. The main types of updates include:- Removing `start_date` from being declared in `default_args`.- Removing the pattern of declaring `default_args` separately from the `DAG()` object.- Updating `default_args` values to more relevant examples.- Replace `DummyOperator` with another operator to make some other `default_args` updates relevant and applicable.,5
Fix GCS system tests (#19227)- Add '**/tmp/**' path to names of creating files- Add separate methods and fixtures for creating\deleting required files- Add `resource` key which disables uniform bucket level access to **GCSCreateBucketOperator**,1
Fix typo (#19826),2
Fix example code in Doc (#19824),2
"Avoid using Proxy in subscript type alias (#19830)Python's pickle module (or typing?) seems to have a bug whenlazy_object_proxy.Proxy is used in a subscript type (such as Union).pickle would somehow incorrectly try to look up the Proxy type inbuiltin during pickling, causing an exception.Since this global variable being pickled is only a type alias and notreally functionally significant, we can work around the bug by simplynot introducing that alias in the first place.",0
Fix typo in docs link (#19837),2
"Amazon provider remove deprecation, second try (#19815)",1
Add hook_params in SqlSensor using the latest changes from PR #18718. (#18431),4
Add missing description field to Pool schema(REST API) (#19841)The description field is missing in pool schema,1
Rewrite image building tests to Python (#19819),3
updates pipeline_timeout CloudDataFusionStartPipelineOperator (#18773),5
19489 - Pass client_encoding for postgres connections (#19827),4
Create dataproc serverless spark batches operator (#19248),1
Remove redundant parentheses (#19846),4
Added parentheses (#19853) (#19854),1
"Workaround occasional deadlocks with MSSQL (#19856)We already have a mechanism to retry operations that could resultin temporary deadlocks - this have been helpful with handling MySQLdeadlocks - however similar problems occur occasionally in MSSQL andthere we get a DBAPIError rather than OperationalError:`sqlalchemy.exc.DBAPIError: (pyodbc.Error) ('40001', '[40001][Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Transaction(Process ID 55) was deadlocked on lock resources with another processand has been chosen as the deadlock victim. Rerun the transaction.(1205) (SQLExecDirectW)');This PR adds DBAPIError to the list of errors that are handledby `run_with_db_retries` to mitigate such occasional deadlocks.",5
Renamed Connection.get_hook parameter to make it the same as in SqlSensor and SqlOperator. (#19849),1
"Add information about supported OS-es for Apache Airflow (#19855)While answering some Stack Overflow questions I found and wasgenerally very surprised that we do not mention POSIX complianceand ""only use Linux for production"" in our prerequisites.This PR adds a note about it - both in GitHub and in our installationprerequisites.",1
Update `default_args` value in example_functions DAG from str to int (#19865),2
Move `bucket_name` validation out of `__init__` in Google Marketing Platform operators (#19383),1
"Do not create dagruns for DAGs with import errors  (#19367)An active dag can suddenly have import errors as a result of the DAG file being changed. Currently, wedo not consider this before creating dagruns.This PR adds the has_import_errors to dagmodel so dags with import errors are not sent to the scheduler tocreate dagrunsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",2
"Fix race condition when running mssql tests (#19863)There is a race condition where initialization of Airlfow DBfor mssql might be executed when the server is started but it isnot yet initialized with a model db needed to create airflow db.In such case mssql database intialization will fail as it willnot be able to obtain a locl on the `model` database. The errorin the mssqlsetup container will be similar to:```Msg 1807, Level 16, State 3, Server d2888dd467fe, Line 20Could not obtain exclusive lock on database 'model'. Retry the operation later.Msg 1802, Level 16, State 4, Server d2888dd467fe, Line 20CREATE DATABASE failed. Some file names listed could not be created. Check related errors.Msg 5011, Level 14, State 5, Server d2888dd467fe, Line 21User does not have permission to alter database 'airflow', the database does not exist, or the database is not in a state that allows access checks.Msg 5069, Level 16, State 1, Server d2888dd467fe, Line 21ALTER DATABASE statement failed.```This PR alters the setup job to try to create airflow dbseveral times and wait a second before every retry.",1
"Revert ""Added parentheses (#19853) (#19854)"" (#19872)This reverts commit cee9a3063e319f87f0377d14494935561c353609.",4
Fix broken anchors markdown files (#19847),2
[19458] Added column duration to DAG runs view (#19482),1
"Reduce logs from imported/vendored FAB class (#19875)When we imported/vendored this class from FAB, we unintentionallychanged the logger name it was using, which caused more logs to appear(we configured the default level for FAB to warning, but that PR meant ""FAB""used a different logger so info messages were showing up.)",5
Improve various docstrings in Apache Hive providers (#19866),1
"Relax timetable clas validation (#19878)Airflow regularly reloads sys.modules, which makes type identitycomparison unreliable, because a class would obtain a second, differentidentity in the interpreter when imported after a reload.This makes validation difficult because there isn't really a way totell whether two class objects are indeed ""the same"". But this check isonly for sanity to begin with, so the best we can do is to drop thecheck entirely ans trust the Plugin Manager is doing its job correctly.",4
Add cli command for 'airflow dags reserialize` (#19471),2
Fix `multiNamespaceMode` docs to also cover KPO (#19879),2
"Avoid littering postgres server logs with ""could not obtain lock"" with HA schedulers (#19842)* Remove magic constants from global DB locksAnd `create_global_lock` isn't anything to do with session, so I havemoved it to utils.db instead.And as part of this I changed the lock id that `airflow db reset` usesto share one with `airflow db upgrade` -- there's no point blockingupgrade if a reset is going to clobber it halfway through.* Avoid littering Postgres server logs with ""could not obtain lock""If you are running multiple schedulers on PostgreSQL, it is likely thatsooner or later you will have one scheduler fail the race to enter thecritical section (which is fine, and expected).However this can end up spamming the DB logs with errors like this:```Nov 26 14:08:48 sinope postgres[709953]: 2021-11-26 14:08:48.672 GMT [709953] ERROR:  could not obtain lock on row in relation ""slot_pool""Nov 26 14:08:48 sinope postgres[709953]: 2021-11-26 14:08:48.672 GMT [709953] STATEMENT:  SELECT slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slotsNov 26 14:08:48 sinope postgres[709953]:         FROM slot_pool FOR UPDATE NOWAITNov 26 14:08:49 sinope postgres[709954]: 2021-11-26 14:08:49.730 GMT [709954] ERROR:  could not obtain lock on row in relation ""slot_pool""Nov 26 14:08:49 sinope postgres[709954]: 2021-11-26 14:08:49.730 GMT [709954] STATEMENT:  SELECT slot_pool.pool AS slot_pool_pool, slot_pool.slots AS slot_pool_slotsNov 26 14:08:49 sinope postgres[709954]:         FROM slot_pool FOR UPDATE NOWAIT```If you are really unlucky that can end up happening over and over andover again.So to avoid this error, for PostgreSQL only, we first try to acquire an""advisory lock"" (advisory because it's up to the application to respectit), and if we cannot raise an error _like_ would have happened from the`FOR UPDATE NOWAIT`.(We still obtain the exclusive log on the pool rows so that the rows are locked.)* Use same db lock for initdb and upgradedbThey do conceptually very similar things and when one is running theother shouldn't either. (This is unlikely to ever be hit in practice)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",1
Add influxdb operator (#19356),1
Run KubernetesPodOperator tests on any executor (#19810)Previously these tests were restricted to run only on KubernetesExecutor. But there is no reasonwe cannot run them on other executors.,1
"Add a short chapter focusing on adapting secret format for connections (#19859)* Add a short chapter focusing on adapting secret format for connectionsAs a result of discussion in #19857, I propose to add this shortchapter to respond to anticipated need of organisations to keep theconnections in format that is not Airflow-exclusive. I think it wouldbe good to explicitly state what is the Airflow approach in this case(i.e. either using existing capabilities of secret backends whenthey are there  - for example in AWS - or rolling your own backend,possibly by extending the community provided ones if the flexibilityis not implemented by the community provided backend.",1
Enable task run setting to be able reinitialise (#19845),5
Skip masking airflow password for tests (#19858),3
"Dynamically enable ""Test Connection"" button by connection type (#19792)Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",3
Adjust trimmed_pod_id and replace '.' with '-' (#19036)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Move class_permission_name to mixin so it applies to all auths (#18749),4
Add tests for docker-compose quick start (#19874),2
Update documentation for November 2021 provider's release (#19882),1
"Initial commit for new Breeze project (#19867)It includes:* proposal for initial ADRs (Architecture Decision records)  where we will keep decision records about both - Breeze2 and CI* scaffolding for the new breeze command including command line,  pre-commit checks, automated tests in CI and requirements",1
"feat: add Gitpod online workspaces support for the Apache Airflow (#19756)* starts the workspace with ./breeze -y* opens another terminal with bash* add documentation for opening Gitpod workspace, creating a branch,  making changes* also, the instructions about setting up and working with `breeze`* add workaround for setting PIP_USER=no variable",1
"Clarify behaviour of test_backfill_depends_on_past (#19862)* Clarify behaviour of test_backfill_depends_on_pastIt used to be that the backfill job deadlocked when a task had""depends_on_past"" set. This was because it depended on past,non-existing and not succesful task.You needed to set `ignore_first_depends_on_past' set in order toavoid the deadlock. This test was quarantined but I thinkthe behaviour has changed.It looks like currently Airflow behaves differently: when checkingdependencies, it actually retrieves the previous dag-run andif there is none available it does not deadlock no matter ifdepends_on_past is set. That makes much more sense and likelywas fixed during the recent changes involved in timetableimplementation.This PR updates the tests to show that backfill will behaveproperly, regardless from `ignore_first_depends_on_past`.Fixes: #14755",0
Update production Helm guide database section to use k8s secret (#19892)Co-authored-by: Bas Harenslak <bas@astronomer.io>,1
Capitalize names in docs (#19893)Co-authored-by: Bas Harenslak <bas@astronomer.io>,2
Convert www.security test suite to pytest and remove residuals (#18961)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,4
Re-enables MyPy in non-failure mode (#19890)This PR enables mypy back as pre-commit for local changes afterthe #19317 switched to Python 3.7 but also it separates outmypy to a separate non-failing step in CI.In the CI we will be able to see remaining mypy errors.This will allow us to gradually fix all the mypy errors and enablemypy back when we got all the problems fixed.,0
Update doc reference links (#19909),2
Context class handles deprecation (#19886),0
Add more filtering options for TI's in the UI (#19910),1
"Fix ""Top level Python Code"" links in best practices doc (#19913)",2
"Fix labels used to find queued KubeExecutor pods (#19904)We need to use the job_id used to queue the TI, not the currentschedulers job_id. These can differ naturally with HA schedulers andwith scheduler restarts (clearing ""queued but not launched TIs"" happensbefore adoption).",1
Correctly capitalize names and abbreviations in docs (#19908)Co-authored-by: Bas Harenslak <bas@astronomer.io>,2
BigQueryHook fix typo in run_load doc string (#19924),2
Validate DagRun state is valid on assignment (#19898),2
Move to watchtower 2.0.1 (#19907)- This version of watchtower contains patches that fixes #15279  where empty log lines would crash Watchtower.,2
Fix possible reference to undeclared variable (#19933),0
Fixed MyPy type issues for cli and api_connection tests (#19948),3
Simple class to make attribute reference obvious (#19982),1
Ensure Spark driver response is valid before setting UNKNOWN status (#19978),1
Clean up incorrect class names of Google system tests (#19956),3
hive provider: minor doc fixes (#19980),0
Better ``pod_template_file`` examples (#19691)Removes extra noise and actually mounts ``airflow.cfg``.,5
Remove deprecated template_fields from GoogleDriveToGCSOperator (#19991)This prevented `op.dry_run()` from working.,1
Fixed MyPy issues in tests decorators and hooks (#19996),1
"Fix race condition when starting DagProcessorAgent (#19935)As described in detail in #19860, there was a race condition instarting and terminating DagProcessorAgent that caused us a lotof headeaches with flaky test_scheduler_job failures on our CIand after long investigation, it turned out to be a racecondition. Not very likely, but possible to happen in production.The race condition involved starting DagProcessorAgent viamultiprocessing, where the first action of the agent was changingthe process GID to be the same as PID. If the DagProcessorAgentwas terminated quickly (on a busy system) before the processcould change the GID, the `reap_process_group` that was supposedto kill the whole group, was failing and the DagProcessorAgentremained running.This problem revealed a wrong behaviour of Airflow in some edgeconditions when 'spawn' mode was used for starting the DAG processorDetails are described in #19934, but this problem will have to besolved differently (avoiding ORM reinitialization during DAGprocessor starting).This change also moves the tests for `spawn` method out fromtest_scheduler_job.py (it was a remnant of old Airlfow and itdid not really test what it was supposed to test). Instead testswere added for different spawn modes and killing the processoragent in both spawn and ""default"" mode.",1
Breeze doc tmux instruction (#20006)* breeze setup-autocomplete zshrc reload* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* change assignment operator to comparison* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* added a new word into spelling exception* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* recommended changes done* warning message about airflow_home and source code in same path -- initialize-local-virtualenv command* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* added a new word into spelling exception* modified the instruction with link to pyenv README* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* modified the instruction with link to pyenv README* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* pyenv issue fix -macOS Big Sur* pyenv issue fix -macOS Big Sur* warning message about airflow_home and source code in same path -- initialize-local-virtualenv commandfixed the static code checks - except spelling mistake of 'Sur' which is the recent macOS version name* modified the instruction with link to pyenv README* change in tmux session related documentation,2
Allow DockerOperator's image to be templated (#19997),2
Add state details to EMR container failure reason (#19579),0
Fix moved data migration check for MySQL when replcation is used (#19999),1
Work around change in GH Actions concurrency expression evaluation (#20023),4
"Revert ""Work around change in GH Actions concurrency expression evaluation (#20023)"" (#20025)This reverts commit 42c46842f415da6bca09fa83d636ef1751243692.",4
Chart: Fix Helm Hooks Weight for K8s Jobs (#20018)https://github.com/apache/airflow/pull/18776 introduced a bug where it changed the Helm Hook weight for Create User job from 2 to 1. It needs to be 2 as we want to run migrations first even before create-user-job,1
"Fix ``breeze kind-cluster shell`` (#20015)This was failing with the following:```/Users/kaxilnaik/Documents/GitHub/astronomer/airflow/scripts/ci/kubernetes/ci_run_kubernetes_tests.sh: line 102: constraints[@]: unbound variableExporting logs for cluster ""airflow-python-3.7-v1.20.2"" to:/tmp/kind_logs_2021-12-03_0_0```and was caused by https://github.com/apache/airflow/pull/17290",1
Fix db downgrades (#19994)Downgrading from 2.2.0 wasn't working on Postgres or MySQL.,1
Fix mypy typing issues for airflow.executors (#20017),0
Update `CODEOWNERS` (#20027)I am not looking at the APIs that frequent so removing myself from CODEOWNERS !,4
Add support to specify kernel name in PapermillOperator (#20035),1
Allow using default celery command group with executors subclassed from Celery-based executors. (#18189),1
Databricks hook: fix expiration time check (#20036)There was a logical error in the check of expiration time that couldlead to authentication failures when executing long-running jobs,1
"Move setgid as the first command executed in forked task runner (#20040)The runner setgid command was executed after importing several airflowimports, which - when executed for the first time could take quitesome time (possibly even few seconds). The setgid command should bedone as soon as possible, in case of any errors in the import, itwould fail and the setgid could be never set.Also this caused the test_start_and_terminate test to fail in CIbecause the imports could take arbitrary long time (depending onparallel tests and whether the imported modules were alreadyloaded in the process so setting the gid could be set after morethan 0.5 seconds.This change fixes it twofold:* setgid is moved to be first instruction to be executed (also  signal handling was moved to before the potentially long  imports)* the test was fixed to wait actively and only fail after the  timeout of 1s (which should not happen before of the fix above)Additionally the test was using `task test` command rather than task run,and in some circumstances when you tried to run it locally,when FORK was disabled (MacOS) the same test could fail witha different error because --error-file flag is not defined for`task test` command but it is automatically added by the runner.The task command has been changed to `run'Fixing this tests caused occasional test_on_kill failurewhich suffered from similar problem and had similar sleepimplemented.Thanks to that the test will be usually faster as no significant delayswill be introduced.",3
Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Fix Py SDK version (#20046),0
Strict schema for k8s objects for values.yaml (#19181),5
"Fix flaky on_kill (#20054)The previous fix in #20040 improved forked tests but also causedinstability in the ""on_kill"" test for standard task runner.This PR fixes the instability by signalling when the task startedrather than waiting for fixed amount of time and it adds betterdiagnostics for the test.",3
"Increase timeouts even longer for on_kill test (#20056)Seems that when the system is busy, the timeouts we had towait for tasks to start were a bit to short. Increasing them.Related to #20054",1
Fixing failing quarantined test cases in test_task_command (#19864),3
Add requirements.txt description (#20048),5
Add sensor for AWS Batch (#19850) (#19885)* Add sensor for AWS Batch (#19850)Adds a sensor implementation to ask for the status of anAWS Batch job. The sensor will enable DAGs to wait for thebatch job to reach a terminal state before proceeding to thedownstream tasks.,2
Refactor DatabricksHook (#19835),5
Add show dag dependencies feature to CLI (#19985),2
Inrease length of the email and username (#19932),1
Fixing mypy issues inside tests model (#20026),3
Add xcom clearing behaviour on task retries (#19968)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Remove postgres 9.6 support (#19987),1
Fixing MyPy issues in testa/jobs (#19998),3
Added example DAG for MSSQL to Google Cloud Storage (GCS) (#19873),2
Fix mypy errors in Microsoft Azure provider (#19923),1
Bug fix in AWS glue operator when specifying the WorkerType & NumberOfWorkers (#19787),1
update upper bound for MarkupSafe (#19953)Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,5
Chart: Rename kerberos-keytab secret file (#20064),2
Order bug report fields by importance (#19990)* Order bug report fields by importance* Move Airflow version dropdown to top,4
"Add params config, in_cluster, and cluster_context to KubernetesHook (#19695)Here we add params, cluster_context, config_file, and in_cluster to KubernetesHook.  We also make `conn_id` optional.Mainly this is needed in order to enable the hook's optional use with KubernetesPodOperator, where currently you don't  _need_ to specify a k8s conn_id but can instead  just provide a combination of these three params.Additionally, k8s hook could naturally be used where there's already a config file in the default location, or from within a k8s cluster, in which case there's no value in setting up an airflow connection.",1
Add migration-wait-timeout to the helm chart (#20069)The `airflow db check-migrations` command is missing a migration wait timeoutvalue in the helm chart resulting in unexpected failures at times.This PR adds a default timeout to the commandCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Fix trino hook tests: change int to enum (#20082)Trino hook tests were failing because the mock assert method now appears to use an Enum instead of int.,1
"Removes InputRequired validation with azure extra (#20084)The change #19923 introduced (accidentally) a required validationfor an azure extra, which caused an error in the connectionUI when the connection could not be added because the field wasmissing.This failed the test_crate_connection fail.This change removes back the validation.",5
Add Qubole how to documentation (#20058)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
"Remove get_readable_dags and get_editable_dags, and get_accessible_dags. (#19961)",2
Fix failing main. (#20094),0
Update documentation for RC2 release of November Databricks Provider (#20086),1
Simplify helm install command in chart index (#20089)* Simplify helm install command in chart indexHelm allows use to install (or upgrade) and create namespace (if not exists) in a single command.  Using this approach makes the doc just a tiny bit cleaner. And it's a more convenient command since the same  one can be used for installing _and_ upgrading.* Update docs/helm-chart/index.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Correct set-up-database.rst (#20090),5
Type-annotate SkipMixin and BaseXCom (#20011),5
Add test case for views connections muldelete function (#19727),1
Modernize tests for SnowflakeHook (#20095),1
"Add ``autocommit`` to ``OracleHook``  (#20085)It's been [supported since version 4.3.2](https://cx-oracle.readthedocs.io/en/latest/release_notes.html#version-4-3-2-august-2007) – released in August, 2007.The `insert_rows` and `bulk_insert_rows` methods have been updated to explicitly disable autocommit using the attribute interface (although that is presumably the default) – since in `insert_rows` this was previously already done (via the cursor).",1
Add template fields to neo4j operator (#20043),1
Helper for provide_session-decorated functions (#20104)* Helper for provide_session-decorated functions* Apply NEW_SESSION trick on XCom,1
Adds retry on taskinstance retrieval lock (#20030)Fixes: #19832Co-authored-by: Jaroslaw Potiuk <jarek@Jaroslaws-MacBook-Pro.local>,0
"Improve handling edge-cases in airlfow.models by applying mypy (#20000)* Fix many of the mypy typing issues in airflow.models.dagAnd to fix these, I needed to fix a few other mistakes that areused/called by DAG's methods* Fix timetable-related typing errors in dag.pyAlso moved the sentinel value implementation to a utils module. Thisshould be useful when fixing typing issues in other modules.* Add note about assert allowed inside a TYPE_CHECKING conditional* Fix docs build of airflow.models.dagrun* Apply NEW_SESSION to dag, dagrun, ti and operator.subdagCo-authored-by: Tzu-ping Chung <tp@astronomer.io>",2
"Change ParamsDict to a MutableMapping subclass (#20019)This is a little more work, and while it has no change in behaviour itdoes make Mypy happier -- previously it was complaining about update'ssignature not matching dict _or_ MutableMapping'sCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",5
"Fix Airflow version for a db migration (#20115)The migration won't be making it into a bugfix release, so mark it forthe next feature release, 2.3.0.",0
"Fix infinite recursion on redact log (#20039)* Fix infinite recursion on redact logWhen redact warning log on ""unredactable"" item is printed, thelog entered an infinite recursion, because the item was attemptedto be redacted again in the log.This PR converts the item to str() - in the worst case the strconverstion will fail and raise exception - but this will be aboutright - but it will not attempt to redact the item again.Fixes: #19816* Update airflow/utils/log/secrets_masker.py",2
Fix mypy errors reported by tests/serializsation/ (#20117),3
Remove deprecated method call (blob.download_as_string) (#20091),4
Fix mypy tests providers part 2 (#20111),1
"Lift off upper bound for MarkupSafe (#20113)Per discussion and guidance from #19753, opening this PR for review. Based on if all the tests pass, this could be reviewed further. Resolves #19761.",0
Remove unused code in SnowflakeHook (#20107),1
Remove legacy hack for TaskInstance without DagRun (#20108),2
Update a link in the Google provider documentation (#20126)Replace a the Poland specific link to Google Workplace with a more general link in apache-airflow-providers-google documentation,2
Providers facebook hook multiple account (#19377)Multiple Account ID Support for ads.py,1
Unhide changelog entry for databricks (#20128),5
Fix mypy issues in www/views.py (#20096)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,0
"Fix typo in MySQL Database creation code (Set up DB docs)  (#20102)* Fix typo in MySQL Database creation codeCharacter set `utf8` is an alias for `utf8mb3`, see docs linked below. This means collation should be set to `utf8mb3_unicode_ci`.Using `COLLATE utf8mb4_unicode_ci` (current code) throws the following error:`ERROR 1253 (42000): COLLATION 'utf8mb4_unicode_ci' is not valid for CHARACTER SET 'utf8'`",1
Forward decorated function type to provide_session reusult (#20131),1
Type (ignore) airflow.compat (#20136),5
Upload provider artifacts before install/test step (#20137)The reasoning is that uploading is rather quick and sometimes you'll want to download the artifact regardless of whether tests pass or fail.,0
fixing #19028 by moving chown to use sudo (#20114)* fixing #19028 by having chown be in a sudo call* removing unused import* trying to clean up a test* combine sudo chown calls* force exception when chown fails* Update tests/task/task_runner/test_base_task_runner.py* Fix tests* Fix formattingCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,0
Fix log link in gantt view (#20121),2
"Log only when Zombies exists (#20118)The following feels like there were non-zero jobs that were zombies while there weren't```[2021-12-07 20:54:35,502] {manager.py:1051} INFO - Finding 'running' jobs without a recent heartbeat[2021-12-07 20:54:35,502] {manager.py:1055} INFO - Failing jobs without heartbeat after 2021-12-07 20:49:35.502841+00:00[2021-12-07 20:54:45,541] {manager.py:1051} INFO - Finding 'running' jobs without a recent heartbeat[2021-12-07 20:54:45,542] {manager.py:1055} INFO - Failing jobs without heartbeat after 2021-12-07 20:49:45.542170+00:00```This changes so that logs only exists when we have more than 0 zombies",2
Fix missing dot (#20141)Co-authored-by: Bas Harenslak <bas@astronomer.io>,0
"Fix grammar and typos in ""Logging for Tasks"" guide (#20146)",2
Added function in AWSAthenaHook to get s3 output query results file URI  (#20124),2
"Check and run migration in commands if necessary (#18439)This PR adds db initialization and upgrade in webserver startup.When the webserver is started, we check the migrations and decide whetherto initialize the database or upgrade it.",5
CI - OpenID Connect authorication to AWS (#19894)* CI - OpenID Connect authorication to AWS* fixup! CI - OpenID Connect authorication to AWS* Update ci.ymlCo-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
Extend config window on UI (#20052)* Extend config window on UI* Use min and max height logic,2
Update CODEOWNERS (#20152)The Changes are self-explanatory in the diff :),4
"Deferrable operators doc clarification (#20150)The language ""when two tasks defer based on the same trigger"" is a bit confusing. Many tasks can reuse the same trigger class.  But two tasks can't defer using the same trigger _instance_. I think what's important to call out here, and what I try to make clearer, is that the exact same instance of the trigger may have multiple copies of itself running.Additionally I clarify cleanup is not _only_ called ""when this happens"" (that is, when trigger is ""suddenly removed""), but called every time the trigger instance exits, no matter the reason.",4
Bump minimum required ``alembic`` version (#20153)Related to https://github.com/apache/airflow/pull/18453#issuecomment-989314399`1.5.0` was yanked so `>=1.5.1` is safe and we already have `1.7.5` in constraints-main,0
Fix mypy for cli package (#19912),0
Fix mypy checks for api_connexion (#19911)Part of #19891,0
"Fix TriggerDagRunOperator extra link (#19410)The extra link provided by the operator was previously using theexecution date of the triggering dag, not the triggered dag. Store theexecution date of the triggered dag in xcom so that it can be read backlater within the webserver when the link is being created.",1
"Update Sphinx and Sphinx-AutoAPI (#20079)We were stuck on an old version of Sphinx AutoAPI for a long while asmore recent versions wouldn't build Airflow's docs, but that seems tohave finally been resolved.We can remove the run_patched_sphinx.py as that was included insphinx-autoapi 1.1* Fix doc rendering glitch in Google provider utils* Remove duplicated link from cncf-kubernetes provider index",1
"Revert ""CI - OpenID Connect authorication to AWS (#19894)"" (#20173)This reverts commit 9083ecd928a0baba43e369e2c65225e092a275ca.",4
Organize EC2 classes in Amazon provider (#20157)* Organize EC2 classes in Amazon provider,1
"Mypy fixes to DagRun, TaskInstance, and db utils (#20163)",5
Log provider import errors as debug warnings (#20172),2
Update contributing guide to use typed session arg (#20181),1
"Update minimum sphinx versions after upgrading sphinx-autoapi (#20170)* Allow point releases of AutoAPI 1.8 (I used with 1.8.4 in all my testing)* Require at least Sphinx v4  A few things got deprecated in Sphinx 4, and as this dep is only for  us building docs we can pick and choose what we like without impacting  users, so lets stay up-to-date.",5
Remove db call from `DatabricksHook.__init__()` (#20180),5
"Only execute TIs of running DagRuns (#20182)Since we can no longer have TIs without DagRun, we canalso, stop executing TIs if the DagRun is not in a RUNNING state.Less work for the Scheduler",1
"Better confirmation prompts (#20183)Make our confirmation prompts more consistent, and optionally support adefault choice.",1
Change log level for Zombie detection messages (#20204)I missed it in https://github.com/apache/airflow/pull/20118,2
Add clear logging to tasks killed due to a Dagrun timeout (#19950)When a DagRun exceeds its `dagrun_timeout` value a few things happen: - The run is marked as `failed` - All unfinished tasks are marked as `skipped` (which causes running tasks to be SIGTERM'd) - A line is logged in the scheduler logs: `INFO: Run $RUN_NUMBER of $DAG_ID has timed-out` This has caused some confusion amongst users as its hard to tell why running tasks were killed without either:1) Cross-referencing the `dagrun_timeout` value with the execution time2) Reading the scheduler logs. This PR adds additional messaging into the task logs when it can be inferred that the task was killed due to a DagRun timeout. I'm not super happy with this implementation (as it kind of duplicates the timeout logic to infer that a timeout occurred) and would really appreciate some advice on other ways we can improve the clarity around timeouts. I'll add some tests for this once I get some feedback on the initial approach.,5
Prepare docs for provider's RC2 release (#20205),1
Add 2.2.3rc1 to issue templates (#20209),0
Finalised Datastore documentation (#20138)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Add Google Cloud Tasks how-to documentation (#20145),2
"Update docs/tools for releasing core Airflow (#20211)When building the ""testing status"" issue, don't include things skippedon the changelog or doc-only changes.Also, don't add skipped changelog entries in the changelog.",4
ShortCircuitOperator push XCom by returnung python_callable result (#20071),1
Add some type hints for Hive providers (#20210),1
Fix mypy neo4j and influxdb (#20189),5
Fix MyPy errors in google.cloud.sensors (#20228)Part of #19891,0
Increase limit of time for constraint job (#20230)Depending on how many changes are there from the previous imagesbuilding images for constrainst (with eager upgrade) might takemore time than 10 minutes which causes constraints generationjob to fail.This change increases the limit to 25 minutes.,1
Add optional location to bigquery data transfer service (#15088) (#20221),5
Fix MyPy Errors for SFTP provider (#20242),1
Fix MyPy Errors for Samba provider (#20243),1
Fix MyPy Errors for Presto provider (#20244),1
Sanity check for MySQL's TIMESTAMP column (#19821),5
Organize Step Function classes in Amazon provider (#20158)* 20139 - organize aws step_function,1
Exclude snowflake-sqlalchemy v1.2.5 (#20245),5
Docs for multiple pool slots (#20257)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Fix MyPy Errors for Tableau provider (#20240)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Support insecure mode in SnowflakeHook (#20106)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
#16692 show schedule_interval/timetable description in UI (#16931)Co-authored-by: Ryan Hamilton <ryan@ryanahamilton.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Organize S3 Classes in Amazon Provider (#20167)* Task: Organize S3 Classes in Amazon Provider,1
Fix MyPy errors for google/marketing_platform and suite (#20227)Part of #19891,0
Remove Integration tests from MSSQL on Public Runners (#20231)The Integration tests with MSSQL often fail on PublicRunners without a reason. The database becomes inaccessible andno logs are explaining what's going on. Its very likely howeverthat this is a memory-related issue (Integration tests take alot of memory as they run a lot of extra containers.Those tests will eventually run on Self-hosted runner after mergeand they are also run for Postgres/MySQL/SQlite so there is noneed to run them also for MSSQL if it causes random failures.,0
"Restore stability and unquarantine all test_scheduler_job tests (#19860)* Restore stability and unquarantine all test_scheduler_job testsThe scheduler job tests were pretty flaky and some of them werequarantined already (especially the query count). This PR improvesthe stability in the following ways:* clean the database between tests for TestSchedulerJob to avoid  side effects* forces UTC timezone in tests where date missed timezone specs* updates number of queries expected in the query count tests* stabilizes the sequence of retrieval of tasks in case tests  depended on it* adds more stack trace levels (5) to compare where extra  methods were called.* increase number of scheduler runs where it was needed* add session.flush() where it was missing* add requirement to have serialized dags ready when needed* increase dagruns number to process where we could have  some ""too slow"" tests comparing to fast processing of  dag runs.Hopefully:* Fixes: #18777* Fixes: #17291* Fixes: #17224* Fixes: #15255* Fixes: #15085* Update tests/jobs/test_scheduler_job.pyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",3
Fix MyPy errors in `dev/*` (#20261),0
Add RedshiftResumeClusterOperator and RedshiftPauseClusterOperator (#19665)These operators provide the ability to pause and resume a redshift cluster.,1
Fix MyPY errors for google.cloud.example_dags (#20232)Part of #19891,2
Fix MyPy Errors for SSH provider (#20241),1
Add types to api_connexion (#20187),1
Fix MyPy Errors for HTTP provider. (#20246)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Fix MyPy Errors for Snowflake provider. (#20212),1
Properly implement DAG param dict copying (#20216),2
Fix various MyPy issues in airflow/www folder (part 1) (#20120),0
Chart: PgBouncer service enhancements (#19749)* Added support for extraVolumes and extraVolumeMounts* Added separate sslmode for metrics exporter* Fixed condition for pgbouncer volume mounts,0
Add method 'callproc' on Oracle hook (#20072),1
Add linter for values.schema.json (#20193),5
"Use Viewer role as example public role (#19215)Currently the sample public role is the `Public` role.  But this role provids no access and still shows the login modal, which makes it seem like the config is not working at all.More intuitive would be to use `Viewer` role in example in the default config because you can at least view the dags.",2
Fix mypy errors in `tests/utils` (#20093),3
Fix MyPy Errors for Apache Druid provider. (#20270),1
Fix MyPy Errors for Apache Drill provider. (#20268),1
"Chart: properly quote namespace names (#20266)Without this, one cannot use namespaces whose names consist of only numbers.",1
MyPy cant follow the logic for dynamic atribute existence (#20274),2
MyPy airflow/timetables fix (#20275),0
Lazy Jinja2 context (#20217)Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Fix MyPy errors in `scripts/in_container` (#20280),0
Fix MyPy errors in `docs/exts` (#20281),2
Ignore the issue (#20049),0
Fix mypy docker provider (#20235),1
Fix mypy spark hooks (#20290),1
"YandexCloud provider: Support new Yandex SDK features: log_group_id, user-agent, maven packages (#20103)",1
"Speed up Helm Upgrade tests (#20289)The Helm Upgrade tests took a long time on Public Runners. Thisis in part because we were running tests before and after upgrade,but we do not need to run them before the upgrade, simply becausethose tests are already run elsewhere.Also increased the timeout for the Upgrade Job - just in caseit will still not be enough",1
Fix mypy errors in airflow utils (#19914)Part of #19891,0
Fix MyPy Errors for Databricks provider. (#20265),1
Fix mypy airbyte provider errors (#20271),0
"Fix race condition when flake checks run in parallel (#20294)The Flake checks run in parallel and when you had an image whichrequired rebuild, it performed additional check on whether theimage needs build or ""pull+build"". When it was run in parallela temporary file containing hash of the remote image could beoverwritten and emptied while another process was reading itwhich resulted in error when running flake command.This has been changed - the files are now stored in a temporaryfiles - unique to each of the processes running in parallel andthe file in question is moved as an atomic operation so it willnever become empty.",4
Fix MyPy errors for google.cloud.transfers (#20229)Part of #19891,0
Fix mypy typing for airflow/models and their tests (#20272)The re-ordering of setting attributes in BaseOperator is because_something_ about that function (throwing the exceptions?) causes mypyto think that BaseOperator objects could be missing those attributes,1
Making SFTPHook's constructor consistent with its superclass SSHHook (#20164),1
Organize Dms classes in Amazon provider (#20156)Organize Dms classes in Amazon provider,1
Warn without tracebacks when example_dags are missing deps (#20295),2
UI: Update duration column for better human readability (#20112)This PR came out of the review for: #19921. There was a review for adding an updated view for displaying the duration column in a more readable way.,5
fix(dag-dependencies): fix arrow styling (#20303),0
Add 2.2.3rc2 to issue templates (#20310),0
Fix mypy issues in airflow/jobs (#20298),0
"Add and use `exactly_one` helper (#20184)The XOR operator `^` is not very readable, only works with True / False, and only works with two values.  E.g. if you need to test ""exactly one of a, b, or c"", you cannot do  a ^ b ^ c.  I add an ""exactly_one"" boolean helper to address these shortcomings.",1
Fix mypy providers (#20190),1
Correctly send timing metrics when using dogstatsd (fix schedule_delay metric) (#19973),0
Fix typo (#20314)Build should be built.,2
Fix MyPy errors in leveldb (#20222)Part of #19891,5
"Deprecate passing execution_date to XCom methods (#19825)As part of AIP-39 (released in 2.2) we added `run_id` parameters to XCommethods, and this changes it so that passing by run_id is therecommended approach.A future PR will change the columns on the xcom table to store run_id(instead/as well as exeuction_date) but that will be for 2.3, where asthis change can be backported to 2.2.x* Clean up XCom run_id addition* Add XCom tests for include_prior_dates* Fix compatibility passing run_id to XCom* Let Kub Pod tests run with in-memory DAG run* Add tests for XCom.set() and XCom.clear()Both the ""modern"" run_id and deprecated execution_date approaches.* Convert get_lineage to use run_id instead of execution_date* Exempt 'picklable' from spelling* Note in UPGRADING.md on XCom exec date deprecationCo-authored-by: Tzu-ping Chung <tp@astronomer.io>",5
Fix MyPy Errors for Apache Beam (and Dataflow) provider. (#20301),1
"Rename TaskMixin to DependencyMixin (#20297)It was used on things that weren't tasks (such as XComArg andEdgeModifier) so the name was in-correct, and I want to disambiguatethis from a concept I need to add for AIP-42 (Dynamic task mapping) ofthings that actually _are_ Tasks or Task-like objects.",1
"Add docs about ``.airflowignore`` (#20311)This was deleted  in an [earlier refactor](https://github.com/apache/airflow/pull/15444/files) (see `concepts.rst`).  This PR brings it back.  I added it under the ""DAGs"" section because even though it's file-based and not dag-based, excluding files that define dags is the most likely use case for this feature (I think).",1
Chart: Fix flower restarts on update (#20316),5
"Chart: Allow ingress multiple hostnames w/diff secrets (#18542)This allows the ingress to specify multiple hosts with multipledifferent secrets, rather than being contrainted to one secret for allhosts specified.",1
"Deprecate smart sensors (#20151)Smart sensors are being replaced with Deferrable Operators. As they weremarked as an early-access feature, we can remove them before Airflow 3.",4
Fix MyPy errors for google.cloud.tasks (#20233)Part of #19891,0
Fix missing get_backup method for Dataproc Metastore (#20326)Found during #19891 fixing (yay! MyPy actually found some real errors).The `get_backup` method was missing in Dataproc Metastoreimplementation.,5
Fix cached_property MyPy declaration and related MyPy errors (#20226)Part of #19891,0
Adds missing mypy types (#20324)This PR adds a few missing type stub packages that we have but sofar MyPy did not complain about lack of those.Added by `mypy --install-types` command.Part of #19891,1
Chart: Include Datadog example in production guide (#17996)This is to enable exporting of logs to a Datadog agent running in the same cluster.,1
"Chart: Add extra containers, volumes and volume mounts for jobs (#18808)",1
Fix MyPy Errors for Apache Sqoop provider. (#20304),1
Only list linked issues once in release issues (#20299),0
"Bugfix: Deepcopying Kubernetes Secrets attributes causing issues (#20318)Encountered a nasty bug where somebody basically implemented their own KubernetesPodSensor, which failed after more than one attempt when using mode=""poke"" + a volume + a secret.Root cause turned out to be in `secret.attach_to_pod()`. In here, a volume and volumemount is created to mount the secret. A deepcopy() is made of the given Pod spec. In order to avoid appending to None, there is this line: `cp_pod.spec.volumes = pod.spec.volumes or []`. In case a volume is set on the Pod spec, a reference is created to the original pod spec volumes, which in turn was a reference to `self.volumes`. As a result, each secret resulted in a volume added to `self.volumes`, which resulted in an error when running the sensor a second time because the secret volume was already mounted during the first sensor attempt.This PR references the deepcopied object instead, and creates a new list if pod.spec.volumes is None.Co-authored-by: Bas Harenslak <bas@astronomer.io>",1
Doc: Fix incorrect filename references (#20277)Minor typo corrections. I changed the filenames in the example folder structure instead of the later references to be consistent with the other examples in the documentation.,2
"Add specific warning when Task asks for more slots than pool defined with (#20178)In cases where task asks for more pool slots than the total number it has the scheduler generates the following warning:`[2021-12-09 19:37:51,949] {scheduler_job.py:407} INFO - Not executing <TaskInstance: a_pool.my_task scheduled__2021-12-09T19:08:00+00:0│172.18.0.10 [scheduled]> since it requires 4 slots but there are 2 open slots in the pool my_pool. ` https://github.com/apache/airflow/blob/985bb06ba57ab67bb218ca9ca7549a81bea88f87/airflow/jobs/scheduler_job.py#L401-L408However this message is very confusing as the issue is not with open slots but with the total slots of the pool. Waiting for slots will not resolve the problem. To make the task run there must be an action from the user:1.  User to increase the total number of slots in the Pool2. User need to change the task code to requests less slots.This PR add specific log notice for this case.",2
Rename DataSync Hook and Operator (#20328),1
ECSOperator: fix KeyError on missing exitCode (#20264)* ECSOperator: fix KeyError on missing exitCode* [tests] add unit test for ECSOperator initialization failure* check that exit code is not included in the context,0
Organize EMR classes in Amazon provider (#20160)Organize EMR classes in Amazon provider (#20160),1
"Deprecate some functions in the experimental API (#19931)This PR seeks to deprecate some functions in the experimental API.Some of the deprecated functions are only used in the experimental REST API,others that are valid are being moved out of the experimental package.",4
Fix MyPy Errors for dataproc package (#20327)Part of #19891,5
Enable local Breeze script and `pipx` to be used for breeze bootstrap (#19992),1
Add hour and minute to time format on x-axis of all charts using nvd3.lineChart (#20002)* Add hour and minute to time format on x-axis in Landing Times* Make labels of x-axis more readable on line charts* Add month and date on x-axisCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Son Hyoungwoo <hwoo.son@navercorp.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,5
Fix grammar mistakes (#20341),0
Update .mailmap (#20343),5
Correct typo (#20345),2
Showing approximate time until next dag_run in Airflow  (#20273)* changes added* 19098 - approx time added in home page as well.* 19098 - approx time added in home page as well.* 19098 - approx time added in home page as well.* 19098 - approx time added in home page as well.* 19098 - Next Run is used without strong,1
"Chart docs: Add ""Customizing Workers"" page (#20331)This adds a page explaining how to customize workers, including the best situations I can think of to require a custom pod_template_file.closes: #16833",2
"Add deprecation warnining for non-json-serializable params (#20174)Following vote by lazy consensus (https://lists.apache.org/thread/whqbbo3gh84s7ggn7968mqlk4d41x7zl), we deprecate non-json-serializable params with removal planned for Airflow 3.0.",4
Fix Volume/VolumeMount KPO DeprecationWarning (#19726),2
Fix mypy in  providers/salesforce (#20325),1
"Fix mypy for providers: elasticsearch, oracle, yandex (#20344)",1
Move source_objects datatype check out of GCSToBigQueryOperator.__init__ (#20347),5
Update breeze setup instruction (#20352)use $(brew --prefix) to get the path to gnu-getoptthe prefix in newer version homebrew in Apple Silicon changed,4
"Split redshift sql and cluster objects (#20276)The first redshift hook was for managing the cluster itself.  Later a hook for _using_ the cluster (e.g. running sql statements) was added to the same module.  Better to separate these into distinct modules redshift_sql and redshift_cluster.  Here we split the redshift modules for operators, hooks, and sensors.",1
Fixing MyPy issues inside airflow/secrets and airflow/security (#20330),0
Standardize Amazon SNS naming (#20368),5
Standardize AWS Kinesis/Firehose naming (#20362),5
Standardize AWS Lambda naming (#20365),5
Fix MyPy errors in chart/tests (#20364),3
Typing fixes needed to deprecation warning fixes (#20376),0
Fix typo in docs (#20371)Minor typo in the task decorator documentation,2
Fix deprecation messages after splitting redshift modules (#20366)We recently split redshift into redshift_sql and redshift_cluster. Somehow I screwed up the module paths in the deprecation messages.,0
Standardize AWS CloudFormation naming (#20357)* Standardize AWS CloudFormation naming,5
"Chart: Add support for securityContext (#18249)This adds the ability to set both a global `securityContext` and `securityContext` by deployment,allowing for greater flexibility in configuring how Airflow is run.",1
Remove unnecssary logging in experimental API (#20356)The `execution_data` does not need to be passed to log. We send enough details to the API user in the response.,1
Change default python version in docker image docs (#20389),2
Fix MyPy Errors for Alibaba provider. (#20393),1
"Limit Snowflake connector to< 2.7.2 (#20395)The Snowflake connector 2.7.2 requires pyarrow to be >=6.0.0(but it has no ""install_requires"" for it - it checks itdynamically and prints warning when imported.We should limit the provider until apache-beam will remove thepyarrow < 6.0.0 limitation.",4
Fix mypy for exasol and facebook hooks (#20291),1
Add operator link to monitor Azure Data Factory pipeline runs (#20207),1
Fix remaining MyPy errors in Google Provider (#20358),1
Standardize Amazon SES naming (#20367),5
Fix MyPy errors in airflow/api and airflow/api_connexion (#20390),0
Add aws_conn_id to DynamoDBToS3Operator (#20363),5
Fix mypy errors in airflow/kubernetes (#20399),0
Fixing MyPy issues inside airflow/serialization (#20306),0
Standardize AWS Glue naming (#20372),5
Fixing MyPy issues in www fab_security manager (#20214),0
Fix mypy tests providers - part 1 (#20057),1
Fix MyPy Errors for Qubole provider. (#20319)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Fix mypy errors in providers/amazon/aws/operators (#20401),1
Fix mypy errors in aws/sensors (#20402),0
Fix mypy errors in aws/transfers (#20403),0
Update upgrading.rst with detailed code example of how to resolve post-upgrade warning (#19993),2
"Fix parsing of Cloudwatch log group arn containing slashes (#14667) (#19700)When the log group arn contains slashes, the urlparse function parsesthe group name as part of the 'path' instead of including it as part ofthe 'netloc'. This fix concatenates both the 'netloc' and 'path' fieldstogether to use as the group arn. For group names without slashes, thefunctionality remains the same as the group name is still parsed as partof the 'netloc' field and the 'path' field will be empty.",1
Standardize AWS EKS naming (#20354)* Standardize AWS EKS naming,5
Remove duplicate typing definition in CLI cheat-sheet command (#20415),5
Added windows extensions (#16110),1
Reduce deprecation warnings from www (#20378),2
Try to fix deprecation warnings from distutils update (#20420),5
Log filename template records (#20165),2
Standardize AWS Athena naming (#20305)* Rename Athena Operator* Rename Athena Hook,1
Add pre-commit that checks credentials are not persisted in CI (#20430)For security reason we should not persist credentials on checkingout code during GitHub actions. This pre-commit prevents thisfrom happening.,1
"Allows to disable built-in secret variables individually in chart (#18974)Some of the secret variables are pre-defined in our chart and theyare defined/hard-coded via environment variables. Howeverthis makes it impossible to set those values from _CMD or _SECRETvariables, because Airflow treats the `basic` env variables aspriority.This PR adds the capability of disabling the secret variablesindividually by values.yaml parameters.Fixes: #16684",2
Clean up JenkinsJobTriggerOperator (#19019)* Remove dead code path from JenkinsJobTriggerOperator* Match type to default arg* Remove literal_eval* !fixup Remove ast import* Remain comp* Update jenkins_job_trigger.pyCo-authored-by: Bin Huang <binh@stripe.com>,5
Un-ignore DeprecationWarning (#20322),2
move emr_container hook (#20375)* move emr_container hook,1
Also track task_log_prefix_template changes (#20435),4
Organize Sagemaker classes in Amazon provider (#20370)Organize Sagemaker classes in Amazon provider (#20370),1
"Add ADR describing reasoning why we build images and security of it (#20407)* Add ADR describing reasoning why we build images and security of itThe ADRs document the decision why Docker imaages are used as commonenvironment for CI and development environment, and also why buildingimages should be done in a secure way.",1
Allow setting port in IMAP Connection (#20440),1
"fix(standalone): Remove hardcoded Webserver port (#20429)Port 8080 is the default port for webserver (https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html?highlight=webserver#webserver). By setting it here again explicitly, we forbid users to override it using AIRFLOW__WEBSERVER__WEB_SERVER_PORT. Removing it IMO is not a breaking change, since it will still default to 8080.",4
Fix mypy tests in tests/providers/amazon/aws/sensors (#20431)Fix mypy tests in tests/providers/amazon/aws/sensors,3
Fix MyPy for Google Bigquery (#20329)Part of #19891,0
"Add possibility to ignore common deprecated message (#20444)There are some cases where deprecation of a commonly useddependency causes deprecation message in multiple dependencies.This happened in December 2021 with distutils deprecation.The disutil deprecation started to appear as new versions ofmultiple packages were released.This change adds such ""common"" deprecation messages that shouldbe filtered out independently where they were generated.",5
Fix mypy errors for google.cloud_build (#20234)Part of #19891,0
Add exiting on error in prod image script (#20447)The script did not fail but continued on error which might haveresulted in one or more images missing.Adding `set -e` fixes it.,0
Update CODEOWNERS for the Helm chart (#20451),2
Fix mypy errors in airflow/operators (#20404),1
Airflow 2.2.3 has been released (#20448),5
Chart: Use 2.2.3 as default Airflow version (#20450),1
"rewrite opsgenie alert hook with official python sdk, related issue #18641 (#20263)* rewrite opsgenie alert hook with official python sdk",1
Fixes docstring for PubSubCreateSubscriptionOperator (#20237),1
Add timeout parameter to DruidOperator (#19984)* PK | Add timeout parameter to druid operator so it can be used in hookCo-authored-by: Piotr Kmita <piotr.kmita@zalando.de>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Add custom pip.conf to docker-context-files (#20445),2
Update INTHEWILD.md (#20452)Add Narrativa as a Company that uses Airflow. Thank you so much for such a great product guys!,1
Ensure Tableau connection is active to access wait_for_state (#20433),2
Organize Opsgenie provider classes (#20454)* organize Opsgenie provider classes,1
Azure: New sftp to wasb operator (#18877)* Azure: New sftp to wasb operatorCo-authored-by: Guilherme da Silva Goncalves <guilherme.goncalves@bancointer.com.br>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>,1
Update hashicorp-vault.rst (#20348)* Update hashicorp-vault.rstCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Checks if the user running Breeze has permissions to run docker cmd (#20462),2
"Update the ""releasing Airflow"" docs (#20456)",2
"Fix backwards compatibility issue in AWS provider's _get_credentials (#20463)The #19815 change introduced backwards incompatibility forthe _get_credentials method - which is a centerpiece of AWSprovider and is likely to be overwritten by the user who wantfor example inject auditing or other credentials-related custombeheviours when interfacing with AWS even if the method isprotected.The change added default for region, which caused signatureincompatibility with such derived classes. Unfortunately, wealready released 2.5.0 provider with this change. We had toyank it and in order to avoid adding backwards-incompatible3.0.0 release we are going to release 2.5.1 with this changeincluded.Fixes: #20457",4
Fix MyPy issues in Core airflow packages (#20425)Part of #19891,0
Chart: Fix extra secrets/configmaps labels (#20464),5
Support regional GKE cluster (#18966),1
Fix mypy tests/providers/amazon/aws/operators (#20434),3
Add autoflake precommit to automatically remove unused code (#20466),1
Fixing MyPy issues inside providers/microsoft (#20409),1
Removes Python 3.6 support (#20467)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add config to warn public deployment exposure in UI (#18557)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>Co-authored-by: Shakaib Khan <shakaibkhan@Shakaibs-MacBook-Pro.local>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>,1
Remove unneeded FAB REST API endpoints (#20487),4
Bump PyJWT from `<2` to `<3` (#20490)* Bump pyjwt from `<2` to `<3`* Update setup.cfg,5
Add Travix to INTHEWILD.md  (#20494)Co-authored-by: bozturk <bozturk@travix.com>,1
Add `OpsgenieCloseAlertOperator` (#20488),1
Add licence headers also to .pyi files (#20513),2
Fix: pin pymongo < 4.0.0 (#20511),0
Increase time limit for Helm chart unit tests (#20525)Sometimes the helm chart unit tests exceed the allocated timefor the job for Public Runners by a small margin. (9X% testssuccessful).This change increases the limit.,1
Update connection object to ``cached_property`` in ``DatabricksHook`` (#20526),5
Remove ``execution_date`` label when get cleanup pods list (#20417),4
Use Python3.7+ syntax in pyupgrade (#20501)* Use Python3.7+ syntax in pyupgrade* Update test_dataflow.py* Update test_dataflow.py* Update test_dataflow.py,5
Improvements for `SnowflakeHook.get_sqlalchemy_engine`  (#20509),1
Rename `OpsgenieAlertOperator` to `OpsgenieCreateAlertOperator` (#20514),1
Add `wait_for_termination` argument for Databricks Operators (#20536),1
Chart: Add custom labels for ingresses/PVCs (#20535),1
"Avoid calling DAG.following_schedule() for TaskInstance.get_template_context() (#20486)This can use a more modern mechanism since get_template_context() hasenough context (namely, the current data interval).",5
Remove `host` as an instance attr in `DatabricksHook` (#20540),5
Removes unnecessary --upgrade option from our examples (#20537),4
20496 fix port standalone mode (#20505),0
Remove PyJWT upper bound from Dockerfile (#20503),2
Reinstate `region` to `default_args` for Alibaba example DAGs (#20423),2
avoid deprecation warnings in BigQuery transfer operators (#20502),1
Set default logger in logging Mixin (#20355)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
switch to follow_redirects on httpx.get call in CloudSQL provider (#20239)* switch to follow_redirects on httpx.get call in CloudSQL provider* sense for parameter as suggested in review,2
"Docs: Note that `remote_log_conn_id` is only used for reading logs, not writing them (#20499)",2
Databricks: fix verification of Managed Identity (#20550),0
Fix passing the gzip compression parameter on sftp_to_gcs. (#20553),2
Fix a package name import error (#20519) (#20519)Signed-off-by: wanlce <who@foxmail.com>Co-authored-by: wanlce <who@foxmail.com>,0
Add support to replace S3 file on MySqlToS3Operator (#20506),1
Bump croniter from `<1.1` to `<1.2` (#20489),5
"Simplify ``KubernetesPodOperator`` (#19572)This refactor has the following goals:* a simpler, easier-to-follow execute method; less logic and more readable method calls* remove reliance on mutation of pod and namespace instance attributes    - the `self.pod` attribute is no longer referenced or mutated anywhere outside of `execute`    - the only reason we need the `pod` attribute at all is for `on_kill`* reduce code duplication* improve method names for greater transparency",1
"Use local definitions for k8s schema validation (#20544)Instead of using remote schemas to validate helm chart values, we willvendor in the definitions locally so the chart can be used withoutusing outbound connections.There is a precommit hook that will ensure only the definitions we useare included in our schema file, as there are quite a few of them alltogether otherwise.",1
"Fix MyPy errors in Google Cloud (again) (#20469)Part of #19891The .py additions are to handle ""default_args"" passed inexamples. Currently some of the obligatory parameters are(correctly) passed as default_args. We have no goodmechanism yet to handle it properly for MyPy (it wouldrequire to add a custom MyPy plugin to handle it)We have no better way to handle it for now.",0
"Fix MyPy errors in Apache Providers (#20422)Part of #19891The .pyi additions are to handle ""default_args"" passed inexamples. Currently some of the obligatory parameters are(correctly) passed as default_args. We have no goodmechanism yet to handle it properly for MyPy (it wouldrequire to add a custom MyPy plugin to handle it)We have no better way to handle it for now.",0
Add script to generate chart changelog annotations (#20555),4
Fix MyPy errors for Airflow decorators (#20034)Related: #19891,0
Add ArtifactHUB annotations for docs and screenshots (#20558)This will provide a link out to our docs and screenshots in theArtifactHUB UI.,2
Use isort on pyi files (#20556),2
Chart: Support elasticsearch connection scheme (#20564),1
Use typed Context EVERYWHERE (#20565)Part of #19891,1
Chart: Fix network policy issue for webserver and flowerui (#20199),0
Fix static checks for isort failing in stub files (#20568)The change to add pre-commit on isort-ing .pyi files crossed withmerge of already approved and not isorted files.Part of #19891,2
Reword section covering the envvar secrets in chart docs (#20566),2
Fix mypy aws example dags (#20497),2
Fixe static checks on few other not sorted stub files (#20572),2
Fix slow DAG deletion due to missing ``dag_id`` index for job table (#20282)Fixes #20249,0
Fix big query to mssql/mysql transfer issues (#20001),0
Change download_video parameter to resourceName (#20528)Added mock for xcom_push support. Restored old way to provideresourse_name received from get_sdf_download_operation. Renamedoperation to operation_state for clarity.,1
Fix Google Mypy Dataproc errors (#20570)Part of #19891,0
Fix template_fields type to have MyPy friendly Sequence type (#20571)Part of #19891,0
Fix Google mlengine MyPy errors (#20569)Part of #19891,0
Clarify docstring for ``build_pod_request_obj`` in K8s providers (#20574)It's unclear what was referred to by `pod` because there isn't a `pod` parameter. I attempt to replace with clearer (and accurate) information.,5
Fixing MyPy issues inside tests/providers/amazon (#20561),3
Cleanup pending pods (#20438),4
Doc: Update Supported column for 1.10.x series (#20592)1.10.x is EOL,1
Fix mypy apache kylin operators (#20595),1
Fix mypy errors in asana example dags (#20593),2
Fix mypy facebook ads hooks (#20589),1
"Rename ``PodLauncher`` to ``PodManager`` (#20576)The name PodLauncher may have been appropriate at one time. But currently the PodLauncher class is used for much more than ""launching"" pods.  It's a more comprehensive helper class for creating, monitoring, and otherwise interacting with pods. For this reason PodManager is a more representative name and since we're doing a major release we can take the opportunity to update the name.",5
Update SlackWebhookHook docstring (#20061)'webhook_token' in 'extra' is deprecated recommend setting token in password field,4
Generate version documentation from single source of truth (#20594)We used to maintain supported versions separately in the docsand it led to discrepancies. Now we have single source of truth whichis used to generate it automatically with pre-commits,1
"Fix mypy databricks operator (#20598)* [16185] Added LocalKubernetesExecutor to breeze supported executors* Revert ""[16185] Added LocalKubernetesExecutor to breeze supported executors""This reverts commit a1c532eacfeddcbefaa3e565a0522e25315286c4.* Fixed mypy errors in databricks/operators",5
Fix mypy errors in apache/drill/operators and /apache/pig/operators (#20597),1
Fix mypy errors in postgres/hooks and postgres/operators (#20600),1
Fix mypy errors in google/cloud/operators/stackdriver (#20601),1
"Move pod_mutation_hook call from PodManager to KubernetesPodOperator (#20596)Previously, in KubernetesPodOperator, the invocation of the pod mutation hook occurredwithin the call to PodManager.run_pod_async.  So, `run_pod_async` would not quite runthe pod you asked it to run, but would mutate it first.With this change, `run_pod_async` runs exactly the pod you request, and the pod returnedby `build_pod_request_obj` is actually the pod you request.",5
Implement dry_run for KubernetesPodOperator (#20573)Calling task.dry_run() will print out the kubectl manifest for the pod that would be created (excluding labels that are derived from the task instance context).,1
Fix mypy errors in amazon aws transfer (#20590),0
Delete pods by default in KubernetesPodOperator (#20575)We change the default for `is_delete_operator_pod` to `True`.  For subclasses `GKEStartPodOperator` and `EksPodOperator` we do not _yet_ change the default since we may not want to do a major release in those providers.  Instead we identify when the parameter is not set and emit a deprecation warning to notify users of the impending change.,4
Fix JSON formatting in cassandra example (#20605)New Sphinx highlighter does not like spaces in json after ':' and itfails main builds because of that.,1
Add known warning generated by snowflake new version (#20604)The new snowflake library version generates a different warningmessage as they bumped pyarrow version used. This PR adds thewarning to known warnings.,2
Chart: Add type to extra secrets param (#20599)Description: allows users to specify they type of secret they are adding when adding extra secrets. Previously we were just defaulting to Opaque.,1
Even more typing in operators (template_fields/ext) (#20608)Part of #19891There were few more places where I missed adding Sequencetyping - including examples (also converted to tuples) andalso template_ext. Also in a few places iterable was left,1
Fix setting of project ID in ``provide_authorized_gcloud`` (#20428)fixes: #20426 and change in logic introduced (seemingly accidentally) in 2fadf3c,2
Fix MyPy issues in ``airflow/jobs`` (#20612)Part of https://github.com/apache/airflow/issues/19891,0
Fix mypy errors in Google Cloud provider (#20611)Part of #19891Another attempt to clean-up all MyPy errors in Google Provider.,1
"Fix MyPy issues in ``airflow/macros`` (#20613)* Fix MyPy issues in ``airflow/macros``**Before**:```root@57b2ac1779ad:/opt/airflow# mypy --namespace-packages airflow/macrosairflow/macros/__init__.py:83: error: Argument 1 to ""diff_for_humans"" of ""DateTime"" has incompatible type ""Optional[datetime]""; expected ""Optional[DateTime]""        return pendulum.instance(dt).diff_for_humans(since)                                                     ^Found 1 error in 1 file (checked 2 source files)```**After**:```root@57b2ac1779ad:/opt/airflow# mypy --namespace-packages airflow/macrosSuccess: no issues found in 2 source files```* Update __init__.py",5
Update documentation for provider December 2021 release (#20523),1
Fix K8S changelog to be PyPI-compatible (#20614),4
Fix chart elasticsearch default port 80 to 9200. (#20616)* Elasticsearch uses rather 9200 than 80 as default.,1
Fix mypy apache beam operators (#20610),1
Fix flaky dask executor test (#20620)The dask_executor test was failing occasionally on busy systemsand seems that it was caused by being throttled by other tests.The 30 seconds timeout in the test seems to be too short. Changedit to 120 seconds by default (it's just a timeout so it doesnot really impact speed of execution of the tests but it givesthe test extra time to complete in case it is throttled.Timeout was added to assert method so that we can control itindividually in different tests.,3
Add twine check for provider packages (#20619)Twine (which we use to upload packages to PyPI) has theability to run checks of packages before uploading them.This allows to detect cases like when we are using forbiddendirectives in README.rst (which delayed slightly preparing theDecember 2021 provider packages and resulted in #20614With this PR Twine check will be run for all packages in CIbefore we even attempt to merge such change that could breakthem.,4
"Fix empty excludes in selective checks (#20622)After removing Python 3.6, some of the excludes have beenempty - which caused error when evaluating combinations to runim main or when FULL_TESTS were needed.This PR brings sane excludes",3
"Fix generation of ""Status provider"" issue (#20621)The script for generating issue for ""Provider status"" and releaseprocess did not work well when only subset of providers were released.The issue was generated including some already released packageseven if they were not released in recent batch of providers (if therewas not even a doc change since last release, the package was consideredas being released again).This PR fixes it by adding a flag that only considers packages thatare present in dist folder (which matches the process of releasemanager)The process has also been updated with more accurate description ofthe steps to take - including manual execution of the script ratherthan using Breeze (Breeze is not neede for this script).",1
Update pre-commit hooks (#20623),1
Fix flaky sensor test (#20617)The test on a busy system could be flaky - the second callcould have not fired.But this is perfectly ok as we only check the first call which willalways happen.,5
"Generate constraints in PRs when upgrading dependencies (#20624)The constraints generation was only happening in push/scheduledruns, but sometimes it is useful to check what constraints wouldbe generated even in the PRs that change setup.py/setup.cfgThe change causes constraint generation also in the PRs and onlypushing the updated constraints is not executed in PRs.",5
Fix mypy errors in airflow/utils/ (#20482),0
Fix Constraints failure in PRs (#20631)The #20624 broke PRs that are changing setup.py.This PR fixes it.,0
upgrade celery 5.2.3 (#19703),5
Bump flask-appbuilder to >=3.3.4 (#20628),5
update breeze2 on windows (#20148),5
Update manage-dags-files.rst to fix some inconsistencies (#20630),0
Add hook for integrating with Google Calendar (#20542)Add GoogleCalendarHook to integrate with Google Calendar.,1
Fix incorrect arguments (#20638),0
Docs: Changed macros to correct classes and modules (#20637)closes: #20545Fixed docs for time and random macros as the reference to what they are was incorrect.,2
"Improve documentation on ``Params`` (#20567)I think that this doc could be improved by adding examples of how to reference the params in your dag. (Also, the current example code causes this: #20559.)While trying to find the right place to work a few reference examples in, I ended up rewriting quite a lot of it.Let me know if you think that this is an improvement.I haven't yet figured out how to build this and view it locally, and I'd want to do that as a sanity check before merging it, but I figured get feedback on what I've written before I do that.",5
Fxing MyPy issues inside airflow/providers/qubole (#20625),1
"Prevent exponential memory growth in Tasks with custom logging handler  (#20541)If the user code adds a log handler to write to the standard out/err, therewill be an infinite loop of logging messages. The `StreamLogWriter._buffer` willbe doubled in each iteration, and soon use up all the memory.This change doesn't stop the infinite loop; it merely changes the growth of`StreamLogWriter._buffer` from exponential to linear, which will also lead toruntime error (stack overflow). However, that is much easier to debug (see theinfinite loop) than the out-of-memory error (no call stack).",0
"Update Kubernetes library version (#18797)Previously we pinned this version as v12 as a change to Kube libraryinternals meant v1.Pod objects now have a logger object inside them, andcouldn't be pickled on Python 3.6.To fix that we have ""backported"" the change in Python 3.7 to make Loggerobjects be pickled ""by name"". (In Python 3.7 the change adds`__reduce__` methods on to the Logger and RootLogger objects, but herewe achieve it `copyreg` stdlib module so we don't monkeypatchanything.)This fix is also applied in to airflow core in a separate commit, but wealso apply it here in the provider so that cncf.kubernetes clientlibrary can be updated but still used with older versions of Airflowthat don't have this fix in.",0
[New] - Ci free space python (#20200),1
Add a button to set all tasks to skipped (#20455)* Added Set Skipped button to tasks* Cleanup* PR changes; wrong function layout; tested* Cleanup* Update more typing-friendly enumCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>* Import for TaskInstanceState and tested locallyCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,3
"Fix precedence of affinity, nodeSelector, and tolerations (#20641)",5
Standardize AWS ECS naming (#20332)* Rename ECS Hook and Operator,1
Add filter by state in DagRun REST API (List Dag Runs) (#20485),1
bugfix: deferred tasks does not cancel when DAG is marked fail (#20649),0
Better error when param value has unexpected type (#20648),2
Docs: Clarify ``sentry_on`` value is not quoted with example (#20639)Clarify the value for ``sentry_on`` is not quoted by providing an example.,1
Don't import Celery or Kubernetes at top level of CLI parser (#20675)The change to validate the subclassed executors mistakenly _required_ both celeryand kubernetes libraries installed to run `airflow`. Oopsie,1
"Revert ""[New] - Ci free space python (#20200)"" (#20684)This reverts commit 27fcd7e0be42dec8d5a68fb591239c4dbb0092f5.",5
Disabled edit button in task instances list view page (#20659),2
Add ``standalone_admin_password.txt`` to ``.gitignore`` (#20685)Co-authored-by: Bas Harenslak <bas@astronomer.io>,5
Fix ECSProtocol compat shim inheritance (#20669),0
Refactor vertica_to_mysql to make it more 'mypy' friendly (#20618)Part of #19891MyPy was confused by the logic in this method (and so humans couldbe) because there were some implicit relations between bulk_loadand tmpfle. This refector makes the bulk_load and non-bulk loadseparate (extracting common parts) and more obvious.Thanks MyPy for flagging this one.,4
rebase conflict resolved (#20338),0
Use original task's `start_date` if a task continues after deferral (#20062),5
"Allow Viewing DagRuns and TIs if a user has DAG ""read"" perms (#20663)This was updated in Airflow 2.2.0 via https://github.com/apache/airflow/pull/16634 which restricts a user to even views the DagRuns and TI records if they don't have ""edit"" permissions on DAG even though it has ""read"" permissions.The Behaviour seems inconsistent as a User can still view those from the Graph and Tree View of the DAG.And since we have got `@action_has_dag_edit_access` on all the Actions like Delete/Clear etc the approach in this PR is better as when a user will try to perform any actions from the List Dag Run view like deleting the record it will give an Access Denied error.",0
"Rename `to_delete` to `to_cancel` in TriggerRunner (#20658)The queue's purpose is to track triggers that need to be canceled. The language `to_delete` was a bit confusing because for one it does not actually delete them but cancel them.  The deletion work is actually in `cleanup_finished_triggers`.  It seems that this method will usually not do anything and it's only for cancelling triggers that are currently running but for whatever reason no longer should be.  E.g. when a task is killed and therefore the trigger is no longer needed, or some multi-triggerer scenarios.  So putting cancel in the name also highlights that this is about stopping running triggers, not e.g. purging completed ones.",1
"Move CoreToContrib to ""Always"" category and rename to Deprecation (#20691)This test touches a lot of classes - especially providers,and as such it should be always run. Also it's not any moreonly about core to contib but about general deprecations.",1
Fix missing f-string and spelling in DB migration errors (#20696)Co-authored-by: Bas Harenslak <bas@astronomer.io>,0
Remove Python 2 from our images (#20680),4
Cleaner output of docker image building scripts (#20679),2
Add context var hook to inject more env vars (#20361)Co-authored-by: Ping Zhang <ping.zhang@airbnb.com>,1
Fix import check for nested providers. (#20686),1
"Increase default livenessProbe timeout (#20698)This increases the default livenessProbe timeout to 20 seconds, givingmore leeway for less powerful or busier instances.",1
"Map and Partial DAG authoring interface for Dynamic Task Mapping (#19965)* Make DAGNode a proper Abstract Base Class* Prevent mapping an already mapped Task/TaskGroupAlso prevent calls like .partial(...).partial(...). It is uncertainwhether these kinds of repeated partial/map calls have utility, so let'sdisable them entirely for now to simplify implementation. We can alwaysadd them if they are proven useful.Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
Standardize DynamoDB naming (#20360)* Standardize DynamoDB naming,5
"Revert ""Fix import check for nested providers. (#20686)"" (#20706)This reverts commit d2675bbd324ee8306431907f5d19edd0db4de8bd.",5
Fix mypy in providers/grpc and providers/imap (#20651),1
Modify Swagger documentation to align with PR #20485 (#20697),2
fix deprecation messages for SFTPHook (#20692)* fix deprecation messages for SFTPHook,1
Make native environment Airflow-flavoured like sandbox (#20704),5
"Be build -> built, and a stray space (#20703)",5
Fix naming convention for sdist provider packages (#20711),1
"Fix MyPy issues in ``airflow/providers/amazon/aws/transfers`` (#20708)We were using ``cast`` redundantly.Before:```root@de8d91a123ec:/opt/airflow# mypy --namespace-packages airflow/providers/amazon/aws/transfersairflow/providers/amazon/aws/transfers/gcs_to_s3.py:185: error: Redundant cast to ""bytes""                        cast(bytes, file_bytes), key=dest_key, replace=self.replace, acl_policy=self.s3_acl_policy                        ^Found 1 error in 1 file (checked 18 source files)```After:```root@de8d91a123ec:/opt/airflow# mypy --namespace-packages airflow/providers/amazon/aws/transfersSuccess: no issues found in 18 source files```",2
Fix MyPy errors for Amazon DMS in hooks and operator (#20710),1
Fix MyPy issues in the experimental api (#20713)Added new sessions as default variable for the method,1
Fix MyPy in Amazon provider for Sagemaker operator (#20715),1
Fix MyPy issues in ``tests/executors`` (#20714),3
Fix MyPy issues with ``test_snowflake.py`` (#20716),3
Fix MyPy issues in AWS Sensors (#20717),0
Update operators.rst (#20640),1
"Revert ""Cleaner output of docker image building scripts (#20679)"" (#20721)This reverts commit fb8780013227a20462878b7f9286d083630c0bc2.",4
Standardize AWS Redshift naming (#20374)* Standardize AWS Redshift naming,5
"Add color to pytest tests on CI (#20723)While GitHub CI supports coloured output, the programs cannotdetect it properly because the output is redirected. Similarlyas in all other cases we force the color output.",1
Add `helm dependency update` step to chart INSTALL (#20702),2
Add Roles from Azure OAUTH Response in security manager as it is currently not able map any AD roles to airflow ones (#20707),1
Standardize AWS Batch naming (#20369),5
"Fix duplicate trigger creation race condition (#20699)The process for queueing up a trigger, for execution by the TriggerRunner, is handled by the TriggerJob's `load_triggers` method.  It fetches the triggers that should be running according to the database, checks if they are running and if not it adds them to `TriggerRunner.to_create`.  The problem is tha there's a small window of time between the moment a trigger (upon termination) is purged from the `TriggerRunner.triggers` set,  and the time that the database is updated to reflect the trigger's doneness.  If `TriggerJob.load_triggers` runs during this window, the trigger will be added back to the `TriggerRunner.to_create` set and it will run again.To resolve this what we do here is, before adding a trigger to the `to_create` queue, instead of comparing against the ""running"" triggers, we compare against all triggers known to the TriggerRunner instance.  When triggers move out of the `triggers` set they move into other data structures such as `events` and `failed_triggers` and `to_cancel`.  So we union all of these and only create those triggers which the database indicates should exist _and_ which are know already being handled (whatever state they may be in) by the TriggerRunner instance.",1
Chart: 1.4.0 changelog (#20661),4
Modernize usage of PIP in Airflow images (#20726)* remove PIP_INSTALL_USER variable* upgrade PIP to 21.3.1* remove AIRFLOW_INSTALL_USER_FLAG as it is not needed* remove spurious usage of --upgrade flag for PIP* add better diagnostics during the build for PIP location and versionSeparated out from #20238,1
Oracle Provider: Fix handling of bindvars with no parameters (#20720)This fixes a bug in the new callproc method where an exception will be raised when no parameters are provided – as in None.,1
Update ``CODEOWNERS`` for dev folder (#20749)I currently won't have that much capacity to review Breeze related changes that are in `/dev/` folder.,4
#16037 Templated requirements.txt in Python operators (#17349)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
Update python version to 3.7 contributing.rst (#20751),5
"Snowflake Provider: Improve tests for Snowflake Hook (#20745)This PR improves `test_run_storing_query_ids_extra` test for Snowflake Hook.- Redundant testing of `Snowflake.get_conn` (This is already tested in `test_get_conn_should_call_connect`)- The expected values were derived from input, so you can't easily fail the test ! I have passed expected results and input separately- We were passing duplicate values in `query_ids`. I have fixed that by changing the code to only access `cur.sfqid` once in a variable and reuse that variable for logging and returning.Co-Authored-By: bharanidharan14 <94612827+bharanidharan14@users.noreply.github.com>",1
Bugfix: ``SFTPHook`` does not respect ``ssh_conn_id`` arg (#20756)closes https://github.com/apache/airflow/issues/20735,0
"Speed up creation of DagRun for large DAGs (5k+ tasks) by 25-130% (#20722)* Speed up creation of DagRun for large DAGs (5k+ tasks) by 15-40%This uses the ""bulk"" operation API of SQLAlchemy to get a big speedup. Due to the `task_instance_mutation_hook` we still need to keepactual TaskInstance objects around.For postgresql we have enabled to ""batch operation helpers""[1] whichmakes it even faster. The default page sizes are chosen somewhatrandomly based on the SQLA docs.To make these options configurable I have added (and used here and inKubeConfig) a new `getjson` option to AirflowConfigParser class.Postgresql is over 77% faster with bulk_save_objects:Before:```number_of_tis=1 mean=0.004397215199423954 per=0.004397215199423954 times=[0.009390181003254838, 0.002814065999700688, 0.00284132499655243, 0.0036120269942330196, 0.0033284770033787936]number_of_tis=10 mean=0.008078816600027494 per=0.0008078816600027494 times=[0.011014281000825576, 0.008476420000079088, 0.00741832799394615, 0.006857775995740667, 0.006627278009545989]number_of_tis=50 mean=0.01927847799670417 per=0.00038556955993408336 times=[0.02556803499464877, 0.01935569499619305, 0.01662322599440813, 0.01840184700267855, 0.01644358699559234]number_of_tis=100 mean=0.03301511880126782 per=0.00033015118801267817 times=[0.04117956099798903, 0.030890661000739783, 0.03007458901265636, 0.03125198099587578, 0.03167880199907813]number_of_tis=500 mean=0.15320950179593637 per=0.0003064190035918727 times=[0.20054609200451523, 0.14052859699586406, 0.14509809199080337, 0.1365471329918364, 0.1433275949966628]number_of_tis=1000 mean=0.2929377429973101 per=0.0002929377429973101 times=[0.3517978919990128, 0.2807794280088274, 0.2806490379880415, 0.27710555399244186, 0.27435680299822707]number_of_tis=3000 mean=0.9935687056015012 per=0.00033118956853383374 times=[1.2047388390055858, 0.8248025969951414, 0.8685875020019012, 0.9017027500085533, 1.1680118399963249]number_of_tis=5000 mean=1.5349355740036117 per=0.00030698711480072236 times=[1.8663743910001358, 1.5182018500054255, 1.5446484510030132, 1.3932801040064078, 1.3521730740030762]number_of_tis=10000 mean=3.7448632712010292 per=0.0003744863271201029 times=[4.135914924001554, 3.4411147559876554, 3.526543836007477, 3.7195197630062466, 3.9012230770022143]number_of_tis=15000 mean=6.3099766838044165 per=0.00042066511225362775 times=[6.552250057997298, 6.1369703890086384, 6.8749958210100885, 6.067943914007628, 5.917723236998427]number_of_tis=20000 mean=8.317583500797628 per=0.00041587917503988143 times=[8.720249108009739, 8.0188543760014, 8.328030352990027, 8.398350054994808, 8.122433611992165]```When using bulk_save_objects:```number_of_tis=20000 mean=4.678154367001843 per=0.00023390771835009216 times=[4.465847548010061, 4.571855771995615, 4.749505186002352, 4.724330568002188, 4.8792327609990025]```MySQL is only 10-15% faster (and a lot noisier)Before:```number_of_tis=1 mean=0.006164804595755413 per=0.006164804595755413 times=[0.013516580002033152, 0.00427598599344492, 0.004508020996581763, 0.004067091998877004, 0.004456343987840228]number_of_tis=10 mean=0.007822793803643435 per=0.0007822793803643434 times=[0.0081135170039488, 0.00719467100861948, 0.009007985994685441, 0.00758794900320936, 0.007209846007754095]number_of_tis=50 mean=0.020377356800599954 per=0.00040754713601199905 times=[0.02612382399092894, 0.018950315003166907, 0.019109474000288174, 0.018008680999628268, 0.019694490008987486]number_of_tis=100 mean=0.040682651600218375 per=0.00040682651600218374 times=[0.05449078499805182, 0.037430580996442586, 0.039291110006161034, 0.03625023599306587, 0.035950546007370576]number_of_tis=500 mean=0.18646696420037187 per=0.00037293392840074375 times=[0.24278165798750706, 0.17090376401029062, 0.1837275660072919, 0.16893767600413412, 0.1659841569926357]number_of_tis=1000 mean=0.5903461098030676 per=0.0005903461098030675 times=[0.6001852740009781, 0.5642872750031529, 0.686630773008801, 0.5578094649972627, 0.5428177620051429]number_of_tis=3000 mean=1.9076304554007948 per=0.0006358768184669316 times=[2.042052763994434, 2.1137778090051142, 1.7461599689995637, 1.7260139089921722, 1.9101478260126896]number_of_tis=5000 mean=2.9185905692051164 per=0.0005837181138410233 times=[2.9221124830073677, 3.2889883980096783, 2.7569778940087417, 2.973596281008213, 2.651277789991582]number_of_tis=10000 mean=8.880191986600403 per=0.0008880191986600403 times=[7.3548113360011484, 9.13715232499817, 9.568511486999341, 8.80206210000324, 9.538422685000114]number_of_tis=15000 mean=15.426499317999696 per=0.0010284332878666464 times=[14.944712879005237, 15.38737604500784, 15.409629273999599, 15.852925243991194, 15.53785314799461]number_of_tis=20000 mean=20.579332908798825 per=0.0010289666454399414 times=[20.362008597003296, 19.878823954990366, 20.73281196100288, 20.837948996995692, 21.085071034001885]```After:```number_of_tis=20000 mean=18.36637533060275 per=0.0009183187665301375 times=[17.728908119010157, 18.62269214099797, 18.936747477011522, 17.74613195299753, 18.797396962996572]```[1]: https://docs.sqlalchemy.org/en/13/dialects/postgresql.html#psycopg2-batch-mode* Use bulk_insert_mappings for even more speed where possible.It gives us an extra speed up over bulk_save_objects, but we can'tuse it when the task_instance_mutation_hook does anything, as that hookneeds an actual object.So _when_ we know that hook won't do anything we switch in toinsert_mappings mode.New speeds (vs baseline, not vs bulk_save_objects) when usingbulk_insert_mappingsPostgreSQL now 130% faster:```number_of_tis=1 mean=0.028053103599813767 per=0.028053103599813767 times=[0.03762496300623752, 0.02637488600157667, 0.025065611000172794, 0.024561002996051684, 0.026639054995030165]number_of_tis=10 mean=0.02647183560184203 per=0.002647183560184203 times=[0.02698062499985099, 0.026417658998980187, 0.027347976007149555, 0.025797458001761697, 0.025815460001467727]number_of_tis=50 mean=0.03149963079486042 per=0.0006299926158972085 times=[0.03810671299288515, 0.03055680700344965, 0.029733988994848914, 0.03016914198815357, 0.02893150299496483]number_of_tis=100 mean=0.033998635396710594 per=0.0003399863539671059 times=[0.0351028829900315, 0.03299884400621522, 0.03358584298985079, 0.03295094799250364, 0.03535465900495183]number_of_tis=500 mean=0.07903424859978259 per=0.00015806849719956516 times=[0.08279920800123364, 0.08588568199775182, 0.07312070899934042, 0.07360191999759991, 0.07976372400298715]number_of_tis=1000 mean=0.12571056479937398 per=0.00012571056479937398 times=[0.12573593499837443, 0.12141938100103289, 0.12616568499652203, 0.12907471299695317, 0.12615711000398733]number_of_tis=3000 mean=0.36025245799683037 per=0.00012008415266561012 times=[0.36071603700111154, 0.3470657339930767, 0.3373015969991684, 0.3337128989951452, 0.42246602299564984]number_of_tis=5000 mean=0.6916533229988999 per=0.00013833066459977998 times=[0.9647149289958179, 0.6451378140045563, 0.5970188640058041, 0.5849326960014878, 0.6664623119868338]number_of_tis=10000 mean=2.071472014003666 per=0.00020714720140036663 times=[2.957865878008306, 1.9388906149979448, 1.766649461002089, 1.8647991580073722, 1.8291549580026185]number_of_tis=15000 mean=2.866650845797267 per=0.00019111005638648446 times=[3.3783503199956613, 2.657773957995232, 2.707275656008278, 2.7875704979960574, 2.802283796991105]number_of_tis=20000 mean=3.5886989389982773 per=0.00017943494694991387 times=[3.969436354993377, 3.436962780993781, 3.9078941010084236, 3.6387251569976797, 2.9904763009981252]```MySQL is (only) 27% faster:```number_of_tis=1 mean=0.035956257799989545 per=0.035956257799989545 times=[0.03932315899874084, 0.03545605999534018, 0.03535486999317072, 0.034727805003058165, 0.03491939500963781]number_of_tis=10 mean=0.036957260797498746 per=0.0036957260797498745 times=[0.040442515004542656, 0.0379129799985094, 0.03494819799379911, 0.03562593398964964, 0.03585667700099293]number_of_tis=50 mean=0.04745422120031435 per=0.0009490844240062871 times=[0.06965546800347511, 0.04221734800375998, 0.04038520700123627, 0.040363031992455944, 0.04465005100064445]number_of_tis=100 mean=0.0528092162014218 per=0.000528092162014218 times=[0.06113427500531543, 0.04883724599494599, 0.05276876600692049, 0.047688748003565706, 0.05361704599636141]number_of_tis=500 mean=0.16223246100416872 per=0.0003244649220083374 times=[0.24469116200634744, 0.1407806619972689, 0.14792052800476085, 0.14703868801007047, 0.13073126500239596]number_of_tis=1000 mean=0.285728433605982 per=0.00028572843360598197 times=[0.3230128890136257, 0.27035739900020417, 0.3003890450054314, 0.2638379510026425, 0.2710448840080062]number_of_tis=3000 mean=1.1824120475997915 per=0.0003941373491999305 times=[1.3103130240051541, 1.286688863998279, 1.1455156929878285, 1.1072918410063721, 1.062250816001324]number_of_tis=5000 mean=1.9416745471942705 per=0.0003883349094388541 times=[2.3746965279860888, 1.9103765429899795, 2.0542518720030785, 1.7706374429981224, 1.598410349994083]number_of_tis=10000 mean=5.059874459402636 per=0.0005059874459402636 times=[5.431018351999228, 5.262124675995437, 5.174487816999317, 4.423381198008428, 5.008360254010768]number_of_tis=15000 mean=9.717965700797503 per=0.0006478643800531668 times=[7.884617075993447, 9.466949063993525, 10.005758297003922, 10.105231182998978, 11.127272883997648]number_of_tis=20000 mean=16.2008618004038 per=0.00081004309002019 times=[14.645835625007749, 16.304637463006657, 16.255490412993822, 16.830263861003914, 16.968081640006858]```",1
Allow depending to a @task_group as a whole (#20671),1
Rewrite DAG run retrieval in task command (#20737),1
Add documentation for an ad-hoc release of 2 providers (#20765),1
Rename amazon EMR hook name (#20767)Co-authored-by: vinit payal <vinit@tribes.ai>,5
"Uses airflow user for build segment of docker image (#20744)PIP produces a warning when root user is used to run pip install.This is done for a good reason - because installing PIP this wayclashes with a number of distro-managed python packages.The warning cannot be disabled even if our use case is legitimateas has been extensively discussed inhttps://github.com/pypa/pip/issues/10556.However, the advice given by the warning is a bit misleading - itsuggests to use virtualenv, but since this is considered a bad practicefor container building and because we need to create virtualenvsdynamically inside the image, using virtualenv is a bad solution for us.It's been attempted in #19189 and failed.Instead we create an airflow user and use PIP_USER=""true"" whichinstalls all dependencies in build segment to ~/.local folderfrom where we can copy it to the main image.That get rids of the warning and at the same time allows us tokeep the best practices of building the images.",1
Standardize AWS SQS classes names (#20732)* Standardize AWS SQS classes names,5
Breeze2 autocomplete options (#20066),5
"Fix MyPy Errors for providers: Tableau, CNCF, Apache (#20654)",1
Fix mypy in providers/aws/hooks (#20353),1
Delay the creation of ssh proxy until get_conn() (#20474) (#20474),1
Compare taskgroup and subdag (#20700),2
"Send SLA callback to processor when DagRun has completed (#20683)* Send SLA callback to processor when DagRun has completedCurrently, sla callbacks are sent every time a dagrun is examined. This causes slacallbacks to be run too often and cause processors to timeout at times.Also deleted dags are not recreated when there are many slas.This PR addresses this by sending SLA callbacks to processor when a dagrun completes",2
Add roles to create_user test (#20773)Flask App Builder 3.4.3 made role and conf_password obligatorywhen creating user:https://github.com/dpgaspar/Flask-AppBuilder/pull/1758Our test for user creation did not have the role set (though theUI used it and the cient also allows to set it).This change adds the `roles` in our tests to enable upgradeto FAB 3.4.3 for the CI (currently tests in main fail because ofthe test failure),0
"Cleaner output for Docker image building (#20747)There was some ""junk"" output generated by the scripts that areused in Airflow image building. The junk has been cleaned up sothat no unnecessary warnings are generated.This change includes:* making sure that when everything is fine, there are no  warnnings generated by PROD docker build proces* making sure that when CI image is build the only remaining  warning is ""Using root"" - this warning cannot be silenced  https://github.com/pypa/pip/issues/10556 and instead  in CI build we explain in green that this is invalid warning* the ""scripted"" steps of docker build have nicely blue headers  that visually separate steps of building the iamge and give  more information on what's going on* the current way of printing ouput will play very nicely with  BUILDKIT UI where Blue color indicates progress in buildingSeparated out from #20238",5
Refactor ti clear next method kwargs tests (#19194),3
Fix task execution process in ``CeleryExecutor`` docs (#20783),2
Add Coinbase to the list of companies using Apache Airflow (#20784)Co-authored-by: mingshi <mingshi.wang@coinbase.com>,1
Enhance `multiple_outputs` inference of dict typing (#19608),5
Update metric name in documentation (#20764),2
Make ``delete_pod`` change more prominent in K8s changelog (#20753),4
Bump chart version to 1.5.0-dev (#20789),2
Python3 requisite start local (#20777),5
Avoid unintentional data loss when deleting DAGs (#20758),2
"Optimize dockerfiles for local rebuilds (#20238)When you build dockerfiles locally for development the layerinvalidation could happen earlier than you wanted - some of thevariables (like COMMIT_SHA) were affecting the cache of Dockerin the way that they forced either invalidation of the pre-cachedpackages installed or forced to recreate assets when they werenot touched.Similarly when no webpack/yarn/packages/static are modified,the node asset compilation should not happen. It makesno sense to compile all the assets on docker rebuild whennone of the www files changed.In case of CI build we can also separate node modulespreparation and asset compilation, because node modulesshould remain in the image anyway for incremental changes.Fixes: #20259This PR improves the experience of iterating over docker imagebuilding by decreasing unnecesary layer invalidations.",5
"refactoring the code - preventing circular import error, moving out common code to different file (#20748)",2
"Config command to toggle asciiart, cheatsheet",5
"Reduce test_utils.mock_operators to only that which is reusued (#20812)There was a lot in tests.test_utils.mock_* that was only used by theHive provider -- all of that has been removed and put intests.providers.apache.hiveAnything else that was only used by a single test has been moved in tothat specific test(The main driver for this was to remove the import of HiveOperator frommock_operators to make it easier to run ""core"" tests without needing_all_ of the extras installed.)",3
Test util `env_vars` to take arbitrary env vars (#20818)Currently it assumes that you will only use this for config settings (which you can already do with `conf_vars`).We should allow any kind of env var so that for example it could be used to patch an airflow conn or any other env var (which is sort of what is advertised in the function name anyway).,1
Fix task instances iteration in a pool to prevent blocking (#20816),0
Update exception docstrings to follow PEP 257 recommendations (#20811)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,5
Add Context stub to Airflow packages (#20817)We agreed that context.pyi is useful to be included in theairflow package as it will help our users with autocomplete andverification of the custom operators.,1
"Add ""use_ssl"" option to IMAP connection (#20441)",1
"Better signing instructions for helm chart releases (#20796)The primary change here is using the [helm gpg](https://github.com/technosophos/helm-gpg) plugin to sign the chart instead of using the default sign/verify in helm. This allows more modern GPG features to be used, e.g. a smartcard, instead of relying on a GnuPG v1 binary keyring containing the private key.",5
Fix airflow trigger cli (#20781)Co-authored-by: mingshi <mingshi.wang@coinbase.com>,0
Move tests out of test core that are duplicated/should live elsewhere (#20826)- Testing BashOperator lives in tests.operators.test_bash- CheckOperator and ValueCheckOperator already tested in  tests.operators.test.sql (using the non-deprecated names)- on_failure_callback already tested in test_local_task_job- SqliteOperator already tested in the sqlite provider- PythonOperator already extensively tested in  tests.operators.test_python- trigger_rule tested in test_baseoperator- Testing the task context was partially covvered already.,3
Integrate Breeze2 documentation together (#20836),1
Add Helm Chart 1.4.0 in airflow_helmchart_bug_report.yml (#20828),5
Fix grammatical error in Variable.get docstring (#20837)Co-authored-by: PApostol <50751110+PApostol@users.noreply.github.com>,1
Speedup liveness probe for scheduler and triggerer (#20833)Liveness probe is submitted through `/entrypoint` which by default runs `airflow db check`.This can be slow.We can disable by setting `CONNECTION_CHECK_MAX_COUNT=0`.,1
Doc: Added an enum param example (#20841)More examples makes it easier to compare our docs with the json-schema docs and figure out how they work together.I ended up doing something similar to this in my code and figured I'd contribute an example.Co-authored-by: Matt Rixman <MatrixManAtYrService@users.noreply.github.com>,1
static code check doc fix (#20844),0
Explain stub files are introduced for Mypy errors in examples (#20827),0
"Fix remaining mypy issues in ""core"" Airflow (#20795)Co-authored-by: Josh Fell <josh.d.fell@astronomer.io>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>",0
Fix flaky templatized call (#20843),0
"AwsAthenaOperator: do not generate ``client_request_token`` if not provided (#20854)- According to AWS doc (link provided below), boto3 will autopopulated ClientRequestToken is not provided.  So it's not necessary to generate it explicitly here- The current logic means a UUID will be generated when the DAG is being parsed into a DAG object.  When we check this DAG object, we will see a ""random"" client_request_token  This may not be desired (for example in the product my team is running, this is causing issue).AWS Doc link: https://boto3.amazonaws.com/v1/documentation/api/1.18.0/reference/services/athena.html?highlight=start_query_execution#Athena.Client.start_query_execution",2
"Add Listener Plugin API that tracks TaskInstance state changes (#20443)This adds new Plugin API - ""listeners"". It enables plugin authors to write[pluggy hook implementation][1] that will be called on certain formalized extensionpoints. To differentiate between current Airflow extension points, likeplugins, and current Airflow hooks, implementations of those hooks are calledlisteners.The API is ment to be called across all dags, and all operators - in contrastto current on_success_callback, pre_execute and related family which are meantto provide callbacks for particular dag authors, or operator creators.pluggy mechanism enables us to execute multiple, or none, listeners thatimplement particular extension point, so that users can use multiple listenersseamlessly.In this PR, three such extension points are added. When TaskInstance's state ischanged to RUNNING, on_task_instance_running hook is called. On changetoSUCCESS on_task_instance_success is called, similarly on FAILEDon_task_instance_failed is called.Actual notification mechanism is be implemented using [SQLAlchemy’s eventsmechanism][2]. This ensures that plugins will get every change of state,regardless of where in the codebase it happened, and not require manualannotation of TI state changes across the codebase.To make sure that this change is not affecting performance, running thismechanism on scheduler is disabled by default. The SQLAlchemy event mechanismis also not affected by default - the event listener is only added if we haveany plugin which actually provides any listener.[1]: https://pluggy.readthedocs.io/en/stable/[2]: https://docs.sqlalchemy.org/en/13/orm/session_events.html#after-flushSigned-off-by: Maciej Obuchowski <obuchowski.maciej@gmail.com>",2
"Fix Scheduler crash when executing task instances of missing DAG (#20349)When executing task instances, we do not check if the dag is missing inthe dagbag. This PR fixes it by ignoring task instances if we can't findthe dag in serialized dag tableCloses: #20099",2
Update ``INTHEWILD.md`` to add Breezeline (#20855)Add Breezeline (formerly Atlantic Broadband) to INTHEWILD.mdWe use it for building pipelines to ingest into BigQuery and automate many SFTP processes!,1
Docs: Wordings fix in CONTRIBUTING doc (#20861),2
Add TaskMap and TaskInstance.map_id (#20286)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Add ElasticSearch log_id_template to tracking model (#20822),2
Fix MyPy issues in ``airflow/decorators`` and ``airflow/models`` (#20859)Fix MyPy issues in ``airflow/decorators`` and ``airflow/models``,0
"Fix MyPy issues in AWS Sensors (#20863)Part of https://github.com/apache/airflow/issues/19891Before:```root@12e9fcfb4678:/opt/airflow# mypy --namespace-packages  airflow/providers/amazon/aws/sensorsairflow/providers/amazon/aws/sensors/s3.py:182: error: Incompatible types in assignment (expression has type ""function"", variable has type ""Callable[..., bool]"")  [assignment]            check_fn: Callable[..., bool] = self.check_fn_user if self.check_fn_user is not None else self.check_fn                                            ^Found 1 error in 1 file (checked 33 source files)```After:```root@12e9fcfb4678:/opt/airflow# mypy --namespace-packages  airflow/providers/amazon/aws/sensorsSuccess: no issues found in 33 source files```",2
Fixing flaky task that fails in first check ocasionally (#19132),0
Use `DagRun.run_id` instead of `execution_date` when updating state of TIs(UI & REST API) (#18724)We can now use run_id as well as execution_date to update statesof task instancesCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Handle stuck queued tasks in Celery for db backend(#19769)Move the state of stuck queued tasks in Celery to Scheduled so thatthe Scheduler can queue them again. Only applies to DatabaseBackend,5
Fix failing main (#20871)When I merged #18724 the jobs ran successfully but it's now failing in main.This PR fixes itCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
Unpin ``cattrs`` (#20872)This was pinned because of issue mentioned in https://github.com/apache/airflow/issues/16172 . However this was fixed in 1.8.0 of cattrs by https://github.com/python-attrs/cattrs/issues/151Changelog entry - https://cattrs.readthedocs.io/en/latest/history.html#id9,3
"AWS: Adds support for optional kwargs in the EKS Operators (#20819)closes: https://github.com/apache/airflow/issues/19641TL;DR:   BaseOperator will throw an exception if it gets an unexpected parameter so this is used as a workaround.Adds the option to include an operator-specific kwargs parameter to the three ""Create"" operators in the EKS module as seen in other Operators.  Includes unit testing and some formatting adjustments to related files to bring them in line with formatting in other modules.",2
"Fix Vault Hook default connection name (#20792)HashiCorp Vault connection uses `imap_default` as default connection name, which seems to be a copy/paste error.",0
"Add ""Greater/Smaller than or Equal"" to filters in the browse views (#20602) (#20798)",1
"Unpin ``argcomplete`` and ``colorlog`` (#20878)For Both of these libraries, the only breaking changes are dropping support for old Python version.- https://github.com/kislyuk/argcomplete/blob/develop/Changes.rst- https://github.com/borntyping/python-colorlog/releases",2
Remove redundant ``dataclass`` dependency (#20879)We had to install `dataclasses` because it was not available in Python <3.7This is no longer required as `main` is Python 3.7 only.,1
Chart: fix extra containers docs and remove krb excess if (#20787),4
Remove code duplication in the test suite test_views_acl.py (#20887),3
Only validate Params when DAG is triggered (#20802),2
Add downgrade to some FAB migrations (#20874)There are some FAB migrations that don't have downgrades.This PR fixes it,0
add entry in release readme to update milestone in Issues (#20890),0
Delete irrelevant test file (#20904)This test file is not needed anymore as the file it tests has moved,4
Switch to new MySQL public key (#20912)MySQL changed key used to sign their apt packages. This causeddocker building failing for prod images as MySQL could not beinstalled.New Public Key is used instead.Fixes: #20911,0
Switch map_index back to use server_default (#20902),1
Uses CI images built in previous step to prepare PROD image (#20889),1
"Serialize mapped tasks and task groups (#20743)* Basic serialization support for MappedOperators* Serialization support for TaskGroups* Make downstream_task_ids a normal writeable propertyIt simplifies a few things.We also deal with (and test) the old name when deserializing* Remove task_id from kwargs only in MappedOperatorSince `task_id` is handled speically in the serialization ofMappedOperators, we don't want it duplicated in to the partial_kwargs.Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
Fix task ID deduplication in @task_group (#20870),0
Rewrite the task decorator as a composition (#20868),5
Check free space python version for Breeze2 (#20701),5
"Switch to non-vendored latest connexion library (#20910)The `connexion` library has been vendored in because of requestslibrary that used to have non-optional chardet LGPL dependency,however requests library had since released a version (which wehelped to provide and convince the requests maintainer to merge)where chardet is an optional dependency (with mandatorycharset_normalizer). This means that we do not need to vendor-inconnexion any more.Also connexion after being somewhat abandoned, has been""revived"" and there are active community now that maintains it -they released several new versions since 2.7.0 we used (2.10.0 isnow the latest version, so we can upgrade to that version instead)",3
Fix a test case inside tests/models that leaves a trace in the DB (#20881),5
Setting up python uses default python version (#20926)There were a few places where default python version was notused where Python's breeze environment was set up in CI.This PR fixes it.,0
Fixing MyPy issues inside tests providers google cloud hooks (#20895),1
"Fix errors thrown by some versions of Bash v4 (#20932)In some versions of Bash V4, accessing an array that's only been inittedbut contains no values can throw an error (specifically if -u is set).So wrap access in an if check first.See more details here: https://stackoverflow.com/a/58261136/1055702",1
Introduce notification_sent to SlaMiss view (#20923),5
"Correctly specify overloads for TaskFlow API for type-hinting (#20933)The position of `@overload` matters -- at least to MyPy, and allfunction overloads have to be next to each other.I also removed the overload for `@task.docker`, as that one _requires_at least an image to be passed, so isn't valid with no parameters",2
"Remove all ""fake"" stub files (#20936)In order to improve MyPy check for example files we added fakestub files, it was supposed to be temporary solution and so itis. Adding fake stubs creates more problems than it solves.Instead - we ignore [call-arg] type of MyPy error for the examplefiles affected (those that are using default_args).Part of #19891",1
"Switch to 'buildkit' to build Airflow images (#20664)The ""buildkit"" is much more modern docker build mechanism and supportsmultiarchitecture builds which makes it suitable for our future ARMsupport, it also has nicer UI and much more sophisticated cachingmechanisms as well as supports better multi-segment builds.BuildKit has been promoted to official for quite a while and it israther stable now. Also we can now install BuildKit Plugin to dockerthat add capabilities of building and managin cache using dedicatedbuilders (previously BuildKit cache was managed using rathercomplex external tools).This gives us an opportunity to vastlysimplify our build scripts, because it has now much more robust cachingmechanism than the old docker build (which forced us to pull imagesbefore using them as cache).We had a lot of complexity involved in efficient cachingbut with BuildKit all that can be vastly simplified and we canget rid of:  * keeping base python images in our registry  * keeping build segments for prod image in our registry  * keeping manifest images in our registry  * deciding when to pull or pull&build image (not needed now, we can    always build image with --cache-from and buildkit will pull cached    layers as needed  * building the image when performing pre-commit (rather than that    we simply encourage users to rebuild the image via breeze command)  * pulling the images before building  * separate 'build' cache kept in our registry (not needed any more    as buildkit allows to keep cache for all segments of multi-segmented    build in a single cache  * the nice animated tty UI of buildkit eliminates the need of manual    spinner  * and a number of other complexities.Depends on #20238",1
Quelch deprecation warning in tests (#20900),3
Obtain lock for update when scheduling tis (#20894)Not sure how we get deadlock at this method even with one schedulerso I'm suggesting we lock it for update,5
"Fix cancelling of Pull Request builds when image build fails (#20939)When image build fails, the pull request that triggered it shouldbe cancelled. The #15944 introduced rewrite of the GitHub actionscode but by mistake it also introduced a failure in cancellingthe PR workflow by missing pipeline to jq.In most cases it did not matter, but it cause ""wait for images""in PRs to run far longer than they should be.This PR restores cancelling feature.",1
Add encoding parameter to `GCSToLocalFilesystemOperator` to fix #20901 (#20919)* Fixes #20901Adds encoding parameter to `GCSToLocalFilesystemOperator` that is used to decode `file_bytes` into a serializable string for XCom,2
"Verify enough resources for breeze (#20763)Verify resources, memory, cpus and disk for Docker in Python.",2
Allow using Markup in page title in Webserver (#20888)Set `instance_name_has_markup = True` in `airflow.cfg` to render `instance_name` with Markup.Closes #20877.,5
"Revert ""Verify enough resources for breeze (#20763)"" (#20948)This reverts commit 75755d7f65fb06c6e2e74f805b877774bfa7fcda.",4
Properly style code blocks in links (#20938),2
Fix new buildkit builds on MacOS (#20963)MacOS requires array variables to be declared on top level(Bash3).This PR changes BUILD_COMMAND variable to be declared at top level.,4
"Revert ""Fix cancelling of Pull Request builds when image build fails (#20939)"" (#20964)This reverts commit b171e03924fba92924162563f606d25f0d75351e.",4
Mount `airflow.cfg` in wait-for-airflow-migrations containers (#20609),5
Set dependencies in MappedOperator via XComArgs (#20931)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>,1
Return to the same place when triggering a DAG (#20955),2
Fixing MyPy issue inside providers IMAP hooks (#20968),1
Add Allo-Media/Uh!ive to the list of companies using Airflow (#20977),1
Update base python image to be Python 3.7 by default (#20978)Default base python image should be set to Python 3.7 as wedropped Python 3.6 support.,1
Docs: fix a typo in a recent change (#20861) (#20983),4
"Remove un-needed deps/version requirements (#20979)This is in an aim to reduce the search space pip will hve to look atwhen backtracking- iso8601 hasn't been in use since 2017(!)- pyjwt has been fixed upstream (flask-jwt-extended, via FAB), so we  don't need to require it ourselves- wtforms and python3-openid issues have been fixed in FAB 3.4.0, so lets just update to that  version",5
Add Dunzo to list of users who use Airflow (Astronomer) (#20990),1
Add `--map-index` parameter to task CLI commands (#20980),2
"Add extra sync when adding executable flag to installation scripts (#20987)Seems that when AUFS is a backing storage for Docker, changingthe script to executable and executing it right after during thebuild phase might cause an error: 'text file busy'https://github.com/moby/moby/issues/13594Workaround for that is to add extra `sync` command after changingthe executable flag to make sure that the filesystem change haspropageted to the underlying AUFS storage.This PR adds the sync and also makes sure that both CI And PRODimage use same formatting, executable bits and `&&` betweencommands rather than `;`. The `&&` is better to separate thecommands because it will not continue with execution steps in thesame bash command after previous command fails. This causedconfusion as to what is the reason for docker build failure.The problem was raised in the #20971 discussion.",0
"Rename params to cloudformation_parameter in CloudFormation operators. (#20989)The CloudFormationCreateStackOperator andCloudFormationDeleteStackOperator used ``params`` as one of theconstructor arguments, however this name clashes with params argument``params`` field which is processed differently in Airflow 2.2.  The``params`` parameter has been renamed to ``cloudformation_parameters``to make it non-ambiguous.",1
Fixing MyPy issue inside tests providers microsoft wasb systems (#20967),5
Verify enough resources for Docker (#20957),2
"Get rid of upload coverage warnings (#20994)Because of lack of memory for public runners, we only runcoverage on our tests in direct push builds in main. However itwe still attempted to upload partial coverage results asartifacts in regular PRs even if the coverage files were missing.This generated a lot of warnings in CI jobs (luckliy those warningsare not easily visible).This PR remove upload attempts on non-main builds in Airflow.",4
"Remove `:type` lines now sphinx-autoapi supports typehints (#20951)* Remove `:type` lines now sphinx-autoapi supports typehintsSince we have no updated sphinx-autoapi to a more recent version itsupports showing type hints in the documentation, so we don't need tohave the type hints _and_ the `:type` lines -- which is good, as theones in the doc strings are easy to get out of date!The following settings have been set:`autodoc_typehints = 'description'` -- show types in description (whereprevious `:type` used to show up)`autodoc_typehints_description_target = 'documented'` -- only link totypes that are documented. (Without this we have some missing returntypes that aren't documented, and aren't linked to in our current pythonAPI docs, so this caused a build failure)`autodoc_typehints_format = 'short'` -- Shorten type hints wherepossible, i.e. `StringIO` instead of `io.StringIO`* Add argument type names to local spelling dictionaryNow that we are using the type hints in the docs, sphinxcontrib-spellingpicks them up as words to be checked, so we have to ignore them.I've chosen to add the provider specific ones to local dictionary filesrather than the global, as for example, `mgmt` is an error in mostplaces, but not in some of the Azure provider.",1
Move some base_aws logging from info to debug level (#20858),0
"Update refreshing constraints instructions (#21001)After changing to buildx, instructions to refresh constraintsshould not include --local-cache, because buildx efficientlycaches rebuilds anyway and if you have not build ""upgrade""image before locally, --local-cache is not useful.",1
"Update md5 information about image after waiting (#21000)When ""wait_for_image"" was called, the information that the imagewas built (including the information about md5 hashes of importantfiles) had not been stored locally. It was only stored whenimage was pulled by ""prepare_image"". Constraints job useswait for image, because it runs in parallel for all python versions.With recent changes, the flag that image had never been built,automatically generates image build - unnecessarily, because theimage is already pulled by ""wait_for_image"".It made almost no difference for ""regular builds"" because the imageis rebuilt from cache (which is now very quick) but in case ofPRs that change setup.py it caused image rebuild. Additionally ifthis iamge has conflicting requirement, it would cause build failure.The change simply registers the fact that image is ""ok"" so thatno attempt to rebuild image happens when constraints are generated.",4
Use actual classes instead of dictionary (#20922),1
Remove a few stray `:type`s in docs (#21014)I also noticed a couple of `:rtype` that should have been `:type` soI've removed those too.Sadly we can't remove `:rtype` en-mass yet as Sphinx doesn't pick up the returntype from type hints.,4
Fix last google provider MyPy errors (#21010)Part of #19891,0
Fix session usage in ``/rendered-k8s`` view (#21006)We can't commit the session too early because later functions need thatsession to fetch related objects.Fix #20534.,0
Fix all Amazon Provider MyPy errors (#20935)Part of #19891,0
Fix grammar in ``dags.rst`` (#20988)grammar correction,2
Squelch more deprecation warnings (#21003),2
Warn if Python changes are not rebased (#21016)We are introducing this change temporary to make sure that Pythonchanges will not accidentally introduce new MyPy errors after wemerge them.This check might be disabled ~week after we enable MyPy as failingcheck.,0
Fix import path for `SageMakerHook` in `airflow/contrib/sensors/sagemaker_training` (#20930),5
Fixed tests failing on Python 3.8 (#21022)The change #21003 broke TestDeprecation class tests by removingTestCase and leaving self.skipTest.This change replaces self.skipTest with pytest.skipTest everywhere.,3
Remove last missed snakebite skip (#21025),4
Set X-Frame-Options header to DENY only if X_FRAME_ENABLED is set to true. (#19491),1
fix: cloudwatch logs fetch logic (#20814),2
Hide version selector for non-versioned packages (#21041),5
Add image labels required by ArtifactHub (#21040),1
Update tutorial.rst (#21043)Updated bad link,2
Remove Python 3.6 from `main` in README.md (#21042),2
Temporary limit Pandas version (#21045)This is likely only for couple of days to avoid test failuresin `main`. When the 3.4.4 version of Flask Builder getsreleased we should be able to relax the limit as it will allowus to migrate to sqlalchemy 1.4,1
Update v1.yaml (#21024),5
Make timeout Optional for wait_for_operation (#20981),1
"Revert ""Send SLA callback to processor when DagRun has completed"" (#20997)It turns out that while processing time for dags with slas were solved,the sla misses are not being recorded.",2
Insrease timeout for occasionally failing Dask test (#21051),3
Fix running airflow dags test <dag_id> <execution_dt> results in error when run twice (#21031)related: #21023,1
Create a generic operator SqlToS3Operator and deprecate the MySqlToS3Operator.  (#20807),1
"fix: Update custom connection field processing (#20883)* fix: Update custom connection field processingFixes issue where custom connectionfields are not updated because `extra` field is in form and has previous values, overriding custom field values.Adds portion of connection form tests to test functionality.",1
Explicitly set pytest asyncio mode to 'strict' (#20995),3
Docs: Fix TriggerDagRunOperator docstring (#20985)to be consistent with the code right below it.,2
"Refresh Breeze documentation (#21044)* Refresh Breeze documentationIn preparation for switching to the new Breeze, the documentationof Breeze needs to be reviewed and updated.This PR updates the docs to be a bit better structured and containmore relevant information (and removes/replaces some commands thathave been removed/added)There are suprisingly little changes, actually. Seems that Breezematured and stabilized a lot.",4
build documentation breeze (#20886),2
name mismatch (#21055),5
"Revert ""Obtain lock for update when scheduling tis (#20894)"" (#21048)This reverts commit 0e4a057cd8f704a51376d9694c0114eb2ced64ef.The change was not really effective. It did not change thebehaviour of the UPDATE query. The ""with_row_lock"" clauseadds ""FOR UPDATE"" clause to SELECT queries but it issilently ignored by SQLAlchemy for UPDATE queries becauseUPDATE queries obtain row locks automatically.As the result, this change did not change Airflow Behaviourbut added ""noise"" around the query.",1
"Refactor dangling row check to use SQLA queries (#19808)This is a prepaoratory refactor to have the move dangling rowspre-upgrade check make better use of the SQLA Queries -- this is neededbecause in a future PR we will add a check for dangling XCom rows, andthat will need to conditionally join against DagRun to getexecution_date (depending on if it is run pre- or post-2.2).This has been tested with Postgres 9.6, SQLite, MSSQL 2017 and MySQL 5.7codespell didn't like `froms` as it thinks it is a typo of forms, andmost other cases it would be, except here. Codespell doesn't currentlyhave a method of ignoring a _single_ line without ignoring the wordeverywhere (which we don't want to do) so I have to ignore the exact_line_. Sad panda",2
Implement enough interface for MappedOperator to be baggable (#20945),1
Add a link to the DAG model in the Python API reference (#21060),2
"Logs in to Github Registry when preparing cache (#21069)Whe we are preparing cache on CI, we should login to theGitHub registry (using GITHUB_TOKEN) in order for --cache-toto be able to push images.",1
"Untangle airflow/decorators/__init__.py[i] names (#21056)This by large distches the confusing ""factory"" thing, and use""collection"" to name the type of '@task'. This should hopefullyclarify things up a bit.",1
(Re)fix Dangling rows moving with MySQL+Replication (#21061)Splitting this CREATE TABLE AS SELECT query into two queries (CREATETABLE LIKE .. followed by INSERT INTO) because the former doesn't playnicely with MySQL when replication is enabled,0
Fix last remaining MyPy errors (#21020)Closes: #19891,0
Remove `:type` directives from `SqlToS3Operator` (#21079),1
"Add documentation and release policy on ""latest"" constraints (#21093)",3
Extend dataproc example dag (#21091),2
Improved instructions for custom image build with docker compose (#21052)* Create build.rst* Update docs/docker-stack/build.rstCo-authored-by: Jarek Potiuk <jarek@potiuk.com>* fix doc buildCo-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Update logging-tasks.rst (#21088),2
Add max_map_size to limit XCom task mapping size (#20976),1
Add constraint to ensure task map length >= 0 (#21115),1
"Function to expand mapped tasks in to multiple ""real"" TIs (#21019)Mark unmapped ti as SKIPPED if upstream map is emptyCo-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
batch as templated field in DataprocCreateBatchOperator (#20905),5
"Grant pod log reader to triggerer serviceaccount (#21111)In order to monitor a pod from a trigger (e.g. for use with a deferring k8s pod operator), we need some read permissions on the triggerer service account.",1
Improve handling of string type and non-attribute `template_fields` (#21054),1
"Add back legacy .piprc customization for pip (#21124)This change brings back backwards compatibility to using .piprcto customize Airflow Image. Some older vrsions of pip used .piprc(even though documentation about is difficult to find now) and weused to support this option. With #20445, we changed to use(fully documented) ``pip.conf`` option, however if someone used.piprc before to customize their image, this change would break it.The PR brings back also the .piprc option to the image (even ifit is not really clear whether current and future versions of pipwill support it.",1
"Add dev tool to review and classify cherry-picked commits (#21032)Until we have Towncrier, this is a useful tool to classify commitsto one of three categories (in v*-test) branches1) a/add - add to milestone2) d/doc - doc-only change3) e/excluded - change that is skipped from changelog (dev tools)This is done via label and milestone assignment.We can also skip the PR or quit.Information about the PR is nicely printed including its currentlabels and URL that allows to quickly review the PR in question.",1
Added test_examples_of_prod_image_building in python (#21097),3
Do not set `TaskInstance.max_tries` in `refresh_from_task` (#21018),1
Add note in UPDATING re non-JSON-serializable params deprecation (#21135),2
Fix liveness probe speedup for scheduler and triggerer (#21108)PR https://github.com/apache/airflow/pull/20833 tried to speed up the liveness probe by setting variable CONNECTION_CHECK_MAX_COUNT=0 which disables a connectivity check in `/entrypoint` (which turns out to be slow).Unfortunately the approach taken doesn't work; we have to use `sh -c exec` instead.,1
Add wall clock time to next scheduled run log message,2
Removed duplicated dag_run join in Dag.get_task_instances() (#20591)Co-authored-by: hubert-pietron <hubert.pietron95@gmail.com>,2
Fix 'airflow dags backfill --reset-dagruns' errors when run twiceCo-authored-by: uplsh <uplsh@linecorp.com>,1
Ensure clear_task_instances sets valid run state (#21116),1
Better multiple_outputs inferral for @task.python (#20800),5
"Add optional features in providers. (#21074)Some features in providers can be optional, depending on thepresence of some libraries. Since Providers Manager triesto import the right classes that are exposed via providers itshould not - in this case - log warning message for thoseoptional features. Previously, all ImportErrors were turned intodebug log but now we only turn them in debug log when creatorof the provider deliberately raisedan AirflowOptionalProviderFeatureException.Instructions on how to raise such exception in the way to keepbackwards compatibility were updated in proider's documentation.Fixes: #20709",2
Return slack api call response in slack_hook (#21107),1
Update `version_added` for `[email] from_email` (#21138),1
"Ensure `on_task_instance_running` listener can get at task (#21157)When we added TaskListener API. It's contract promises to pass TaskInstanceobject to listener plugin. However, what happens is not 100% true- the object being passed is one that maps to current SQLAlchemy session.`check_and_change_state_before_execution` operates on detached TaskInstanceobject, then merges it to current session. Since there is no attached object inthe SQLAlchemy identity map, SQLAlchemy creates it, and it's this object that'sbeing passed to the SQLAlchemy event listeners.The problem with that is that when creating new SQLAlchemy object, SQLAlchemytakes care about setting only database-mapped fields. The ones that are purelyon the python side, like task aren't being set on the new object.This manually sets task on the new SQLAlchemy object, so that`on_task_instance_running` receives a TaskInstance with `task` field set.Signed-off-by: Maciej Obuchowski <obuchowski.maciej@gmail.com>",1
Static check in Breeze2 (#20848),5
Add possibility to create user in the Remote User mode (#19963),1
Chart: minor doc fixes (#21189),0
"Remove ""remember-last-answer"" from Breeze/CI (#21186)This feature had been removed when recent BUILDX improvementswere added. This PR removes the remnants of it.Follow up after #20664",4
Create CustomJob and Datasets operators for Vertex AI service (#20077),1
Clarify default Python rules after dropping 3.6 (#21180),4
Fix #21096: Support boolean in extra__snowflake__insecure_mode (#21155),1
Fix async_mode config key (#21179),5
Adding GitHub provider (#21076),1
Cloudsql import links fix. (#21199),0
Update CODEOWNERS for the chart docs and k8s provider (#21202),1
"Revert ""Create CustomJob and Datasets operators for Vertex AI service (#20077)"" (#21203)This reverts commit 640c0b67631c5f2c8ee866b0726fa7a8a452cd3c.",4
Use DAG run ID in /run form (#21146),1
[SQSSensor] Add opt-in to disable auto-delete messages (#21159)Co-authored-by: TungHoang <st.hoang@jellysmack.com>,4
"Remove up-to-date checks for Python (#21208)The up-to-date check for Python run for ~week so all the PRsraised in the last week will need to be rebased to account forMyPy changes. Hopefully there will be no more PRs from before,that have not been rebased (we had one serious MyPy problem forthe #20077 change that was approved before the up-to-date checkerwas enabled in #21016",0
Alleviate import warning for `EmrClusterLink` in deprecated AWS module (#21195),2
Docstring fixes (#21200),0
A trigger might use a connection; make sure we mask passwords (#21207),4
"Avoid changing executable bits in docker scripts (#21211)There are various problems with executable bits in scripts usedin different environments in docker builds:* depending on the umask of the Linux host system, group bits  might be set or not - this might lead to Docker cache  invalidation* on Windows host systems, when file is copied to Docker, the  executable bits might be lost when the file is copied to  docker context* when AUFS is used as backing storage changing executable bit  might lead to crashes if there is no extra sync* changing executable bit of the script leads to actual change  of the cache while building, which also might produce  cache invalidationAs the result we cannot rely on the executable bits of the scriptsand cannot change them in the image either.This change removes executable bits from the scripts for group andother, and executes all the docker scripts via `bash` command.",2
Fix documentation errors in apache-airflow/lineage.rst (#21158),0
"Replaces the usage of postgres:// with postgresql:// (#21205)After releasing 3.4.4 we can finally migrate to SQLAlchemy 1.4,however SQLAlchemy 1.4 removed the use of postgres:// as validspecification for Postgres DB SQL (leaving only postgresql://)Due to that we need to change:* postgres provider to return postgresql:// with get_db_uri()* fix a number of tests that expected postgres://We cannot do much if someone uses postgres:// specification.Technically it might be seen as breaking change, but this is notan airflow breaking change and users could still use SQLAlchemy1.3 to keep the old prefix, so we can introduce this changein Airflow without raising the major version.Details in the [SQLAlchemy Changelog](https://docs.sqlalchemy.org/en/14/changelog/changelog_14.html#change-3687655465c25a39b968b4f5f6e9170b).",4
Fixed Tuple required in most recent black version (#21215)The 22.1.0 release of Black as of 29 Jan 2022 requires Tuplein target_version_option_callback.,1
Fixed tuple also in provider packages (#21216)The 22.1.0 release of Black as of 29 Jan 2022 requires Tuplein target_version_option_callback.Follow up after #21215,1
[INTHEWILD] Update EBANX company users (#21220),1
Actually fix tuple and bool checks for black 22.1.0 (#21221)Previous two fixes in #21215 and #21216 did not really fix theproblem introduced by Black 22.1.0 (they could not as they werewrong). This change was actually tested with the new black andshould fix it finally.,0
"Optimize Airflow images for cross-version caching (#21217)Airflow produces images for multiple versions. Sometimes werelease several images for different versions close to one anotherwhen base Python image is the same. In those cases it would begreat if images for different Airflow versions shared as muchas possible of image layers - the ""base"" dependencies usuallydo not change between versions so two images with the same Pythonversion could only start differring at the moment we start toinstall Airflow for the first time.This change moves AIRFLOW_VERSION ARG declaration and ENV variableas late as possible, so that earlier layers in the Docker images are thesame for different Airflow image versions as long as the base Pythonimage is the same.This should decrease significantly used space if someone switchesbetween two Airflow versions in the same system (for testingor when doing migration). It also should speed up cacherefreshing and rebuilding and decrease overall size of the cacheused by multiple Airflow Images (especially when we are closeto release (like we are now with Airlfow 2.2.4)",1
"Remove check for tuple in test_security (#21228)SQLAlchemy 1.4 does not produce iterator of ""real"" Tuples,it returns iterator of Rows. Rows are not Tuples so instancecheck will fail for them, however for all practical purposethey behave as Tuples (even comparing them with Tuples of thesame content produce an equality). They also behave like dict,but this is a different story.The test started to fail in SQLAlchemy 1.4 because it containedassert for the returned set entry to be Tuple, but it also containedthe actual check for length and whether the expected Tuple is inthe set, so assert for instance is pretty redundant.",3
"Simplify pull vs. build scenario in CI (#21219)We used to have two options in our CI - one was to build theimages in the ""build"" step and pull them in ""ci/tests"" steps, theother was to build the images in ""ci/tests"" steps as a backup,in case the build/ci workflows has some problems. It's likely beena year or more when we did not have to use the other workflow andsince switching to GitHub Registry as cache it became extremelystable, which means that we can remove the option where theimages might be build ""on the spot"" in the CI/tests steps.That involves few clarifications:* we can split `prepare` scripts into separate `pull` and `build`  ones* the steps with `Prepare` name (which was - either pull or build)  can use proper name (either Pull or Build)* GITHUB_REGISTRY_WAIT_FOR_IMAGE variable and corresponding  BUILD_IMAGES_OVERRIDE and debug secret to enable ""build in CI""  flow can be removed* documentation is updatedThis will also allow to easier implement the equivalent stepsin the ""CI & Breeze"" override project, as the steps will be nowpossible to replace with corresponding breeze's build-image/pull_image functions and calls.",1
"Remove Shebang in docker scripts (#21224)Now that all the scripts in docker build are using direct bashcalling, shebang on those scripts is not needed (and harmful).Some of the IDEs and pre-commits will insist on making the filesas executable (which just happened in one of the previous commits)and this - depending on the system umask setting - might add theexecutable bit set for the owner and for group or just the owner.Removing both - executable bit and shebang will make sure therewill be no temptation to add the executable bit (thus the executablebit will not trigger cache invalidation)",5
"Add bash to compile assets in case PROD image is build from sources (#21234)The #21219 removed all the shebangs from scripts and that wasfine, but there was one case (only for local build and cacherefresh) where production image build from sources executedthe compile_www_assets.sh script directly.This change fixes it.",0
Add ADRs about using root user and database volumes in Breeze (#21209),5
TaskGroup decorator example DAG: Fix TaskGroup dependencies (#21240),0
"Add explicit image push to cache preparation (#21229)When BUILDKIT cache is prepared, you cannot (yet) at the same timeto ""load"" the image to local docker engine and ""push"" it to theregistry. You need to push it separately.This change adds separate ""push"" step when buildx cache is prepared.",1
Fix trigger dag redirect from task instance log view (#21239),2
Add missed deprecations for cncf (#20031),1
Add a retry with wait interval for SSH operator #14489 (#19981),1
Add ShortCircuitOperator configurability for respecting downstream trigger rules (#20044)* Add short-circuit mode handling,1
Use Identity instead of Seqence in SQLAlchemy 1.4 for MSSQL (#21238)Primary columns in MsSQL use IDENTITY keyword to autoincrement. UsingSequence for those fields used to be allowed in SQLAlchemy 1.3 (andessentially ignored if only name was specified).See https://docs.sqlalchemy.org/en/14/dialects/mssql.html:  Changed in version 1.4: Removed the ability to use a Sequence  object to modify IDENTITY characteristics. Sequence objects  now only manipulate true T-SQL SEQUENCE types.,1
Update debug.rst docs (#21246),2
"Streamlines image-based pre-commit tests (#21226)This PR simplifies and streamlines the pre-commits that requireCI and BATS docker images to run. This is the result ofupcoming CI/Breze rewrite from Bash to Python and recentimprovements in image caching.The following simplifications and deletion are implemented:* BATS tests (amd all related code) are completely removed from both  Host and container we are removing Bash so those won't be needed* The 'build' test that prepared CI image if needed has been  removed (see below)* The 'mypy' and 'flake' images that require CI image, instead of  attempting to rebuild the image, will run on whatever image they  have, but they will warn that the image needs to be rebuilt  at earliest convenience.* In case the CI image has never been built locally, it will  be automaticaly pulled from ghcr.io latest image built on CI  without asking the user* Documentation is updated",5
Split out confusing path combination logic to separate method (#21247)* Split out confusing path combination logic to separate method* Fix argument type,0
Log trigger status only if at least one is running (#21191),1
"Remove unused ""airflow_2"" test marker (#21249)This hasn't every really been used.",1
Log context only for default method (#21244)There's no reason for us to log the context when a task resumes.,2
PSRP improvements (#19806),1
Fixes Docker xcom functionality (#21175)* Fixes Docker xcom functionality,1
fןס Broken link in api.rst (#21165)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Update version to 2.2.4 for things in that release (#21196),5
"Update CONTRIBUTORS_QUICK_START.rst, TESTING.rst(#21140)",3
Fix insecure_mode parameter formatting in Snowflake conn doc (#21256),2
"Fix building documentation for PSRP (#21260)The PSRP docs building fails in main/commiter PRs after merging #19806 because the documentation needs to be rebuilt one more time. In parallel builds,  sequence of building the documentation matters. In main and the commiter PRs the ""apache-airflow"" documentation does not complete buildbefore misrosoft psrp is built, and the psrp docs usesreference to the airlfow ""concept"" documentation and thelink is not existing yet in the inventory for apache-airflow.This results in a race condition where PSRP documentation failswith `toctree contains reference to nonexisting document`The error has not been included in the list of errors thatare ""eligible for rebuild"" and the race condition was triggered.This change adds the error to errors ""eligible for rebuild"" andmakes PSRP documentation build succeed on retry.",1
Add banner_timeout feature to SSH Hook/Operator (#21262)Recently ssh tests in CI started to fail intermittently withError reading SSH protocol banner error. This error is raisedwhen SSH server is slow to start (which might happen forexample when there is not enough entropy to generate keys)This can be mitigated by adding banner_timeout.,1
"Update local venv doc to use 3.7 as base version (#21288)The instructions link to constraints files that were removed in16e0625, as the examples were all 3.6 based. Also includes commentsto illustrate that the 3.7 in the URL is the python version, as itcould also be interpreted as the version of the file.",2
Remove TODO from Snowflake docs (#21280),2
Refactor operator links to not create ad hoc TaskInstances (#21285),1
Limit SQLAlchemy until MSSQL datetime bug is fixed (#21272)* Limit SQLAlchemy until MSSQL datetime bug is fixedSQL Alchemy 1.4.10 introduces a bug where for PyODBC driver UTCDateTimefields get wrongly converted as string and fail to be converted back todatetime. It was supposed to be fixed inhttps://github.com/sqlalchemy/sqlalchemy/issues/6366 (released in1.4.10) but apparently our case is different.Opened https://github.com/sqlalchemy/sqlalchemy/issues/7660 to track it,0
Augment xcom docs (#20755),2
"Fix relationship join bug in FAB/SecurityManager with SQLA 1.4 (#21296)This is fixed in SQLA 1.4.19, but the fix makes the intent clearer hereanyway.",1
Docs: Fix task order in overview example (#21282),0
Update stat_name_handler documentation (#21298)Previously stat_name_handler was under the scheduler section of theconfiguration but it was moved to the metrics section since 2.0.0.,4
Update recipe for Google Cloud SDK (#21268),5
"Fix tests for mssql after SQLA 1.4 upgrade (#21303)The way SQLA 1.4 constructed the query then `exeuction_date.in_([])`changed, and as a result it started failing.But we don't even need to ask the database in this case, as we know itwon't return any rows.",5
Fix `oauth_whitelists` in BaseSecurityManager (#21308),0
Update error docs to include before_send option (#21275)https://github.com/apache/airflow/pull/18261 Added support for the `before_send` option when initializing the Sentry SDK in airflow. This patch updates the documentation to reflect this change.,4
Chart: Update git-sync to v3.4.0 (#21309),5
Fix mismatch in generated run_id and logical date of DAG run (#18707)Co-authored-by: Tzu-ping Chung <tp@astronomer.io>Co-authored-by: Jed Cunningham <jedcunningham@apache.org>,1
"Update boto3 to latest (#21311)boto3's latest release is `1.20.48`, but is currently constrained to be `<1.19.0`. There was no reason given for pinning to `<1.19.0` in https://github.com/apache/airflow/pull/18389, and that PR was just bumping the pinning from `<1.18.0` to `<1.19.0`. Most other packages that are preemptively constrained to be less than a certain release are pinned to be less than a _major_ release. This PR updates boto3 to follow that convention.The `<1.19.0` constraint is in conflict with `awswrangler`'s latest release. https://github.com/awslabs/aws-data-wrangler/blob/e8cba42922626b6dfe4ea50c0e1498a3be9def79/pyproject.toml#L31A discussion of another Airflow user encountering this issue, and @potiuk 's recommendation to try this PR, is here: https://github.com/apache/airflow/discussions/20340",1
Added My Money Bank to the list of companies using Apache Airflow (#21306)Co-authored-by: Guillaume Renard <guillaume.renard@mymoneybank.com>,1
"Make `airflow dags test` be able to execute Mapped Tasks (#21210)* Make `airflow dags test` be able to execute Mapped TasksIn order to do this there were two steps required:- The BackfillJob needs to know about mapped tasks, both to expand them,  and in order to update it's TI tracking- The DebugExecutor needed to ""unmap"" the mapped task to get the real  operator backI was testing this with the following dag:```from airflow import DAGfrom airflow.decorators import taskfrom airflow.operators.python import PythonOperatorimport pendulum@taskdef make_list():    return list(map(lambda a: f'echo ""{a!r}""', [1, 2, {'a': 'b'}]))def consumer(*args):     print(repr(args))with DAG(dag_id='maptest', start_date=pendulum.DateTime(2022, 1, 18)) as dag:    PythonOperator(task_id='consumer', python_callable=consumer).map(op_args=make_list())```It can't ""unmap"" decorated operators successfully yet, so we're usingold-school PythonOperatorWe also just pass the whole value to the operator, not just the currentmapping value(s)* Always have a `task_group` property on DAGNodesAnd since TaskGroup is a DAGNode, we don't need to store parent groupdirectly anymore -- it'll already be stored* Add ""integation"" tests for running mapped tasks via BackfillJob* Only show ""Map Index"" in Backfill report when relevantCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",1
Fix BigQuery system test (#21320),3
"Simplify fab has access lookup (#19294)* Use FAB models.* Remove incorrect conversions to new permission naming scheme.* Fix missing FAB renames.* Remove unused FAB compatibility fixes in models.py.* Set perms directly on user objects.* Set perms properties on User model.* Rename missed old naming scheme conversion.* Remove unused imports.* Remove unused imports.* Remeve get_user_roles() method.* Make permissions eagerload.* Remove unused imports.* Clarify query params.* Modify sort logic so MSSQL passes.* Add text modifier to order_by values.* Remove calls to get_*_dags.* Add back execution_date* Add back comma to match rest of file.* Remove unused permission functions.* Fix failing tests.* Pass user object to current_app.appbuilder.sm.has_all_dags_access.* Remove attempts to fix query.* Update the api_connexion query builders.* Add typing.* Apply sorts directly to model objects.* Apply sorts directly to model objects.* Standardize custom sort code.* Code review* Augment xcom docs (#20755)* Fix relationship join bug in FAB/SecurityManager with SQLA 1.4 (#21296)This is fixed in SQLA 1.4.19, but the fix makes the intent clearer hereanyway.* Docs: Fix task order in overview example (#21282)* Update stat_name_handler documentation (#21298)Previously stat_name_handler was under the scheduler section of theconfiguration but it was moved to the metrics section since 2.0.0.* Update recipe for Google Cloud SDK (#21268)* Use FAB models.* Remove incorrect conversions to new permission naming scheme.* Fix missing FAB renames.* Remove unused FAB compatibility fixes in models.py.* Set perms directly on user objects.* Set perms properties on User model.* Rename missed old naming scheme conversion.* Remove unused imports.* Remove unused imports.* Remeve get_user_roles() method.* Make permissions eagerload.* Remove unused imports.* Clarify query params.* Modify sort logic so MSSQL passes.* Add text modifier to order_by values.* Remove calls to get_*_dags.* Add back execution_date* Add back comma to match rest of file.* Remove unused permission functions.* Fix failing tests.* Pass user object to current_app.appbuilder.sm.has_all_dags_access.* Remove attempts to fix query.* Update the api_connexion query builders.* Add typing.* Apply sorts directly to model objects.* Apply sorts directly to model objects.* Standardize custom sort code.* Make sure joined fields prefetch.* Dont use cached_property, since its only on > 3.8.Co-authored-by: Ash Berlin-Taylor <ash@apache.org>Co-authored-by: Lewis John McGibbney <lewis.mcgibbney@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Lucia Kasman <38845383+luciakasman@users.noreply.github.com>Co-authored-by: Fran Sánchez <fj-sanchez@users.noreply.github.com>Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>",1
"Refactor SSH tests to not use SSH server in operator tests (#21326)This required a slight refactor to the SSHOperator (moving`exec_ssh_client_command` ""down"" in to the Hook) but the SSH _Operator_tests now just use stubbing, and the only place that connects to a realSSH server is the one test of `test_exec_ssh_client_command` in SSHHook.This is both better structured, and hopefully produces less (or ideallyno) random failures in our tests",3
Chart: support for priorityClassName (#20794),1
Replace tenacity retry with flaky marker for ssh hook test (#21334),3
Add more SQL template fields renderers (#21237),1
Fix the hyperlink between anchors (#21337),2
Add how-to Guide for WinRM operators (#21344),1
Add documentation for January 2021 providers release (#21257),1
Fix md formatting of UPDATING.md (#21347),5
"Fix the incorrect scheduling time for the first run of dag (#21011)When Catchup_by_default is set to false and start_date in the DAG is theprevious day, the first schedule time for this DAG may be incorrectCo-authored-by: wanlce <who@foxmail.com>",2
Fix to check if values are integer or float and convert accordingly. (#21277)This code will prevent the loss of data if the value is a float it will convert to float if it is not then int.  It will use pd.Float64Dtype() for floats instead of using the the pd.Int64Dtype(). Since there could be floating-point values in the array this will fix the exception for safely casting the array to data type.fixes error when using mysql_to_s3 (TypeError: cannot safely cast non-equivalent object to int64) #16919,0
Clarify ElasticsearchTaskHandler docstring (#21255)Previously it said 'logs are not indexed into ES' but what it meant was '_airflow_ does not index the logs for you'.,2
:bug: (BigQueryHook) fix compatibility with sqlalchemy engine (#19508),0
Support to upload file to Google Shared Drive (#21319),2
Update `ExternalTaskSensorLink` to handle templated `external_dag_id` (#21192),2
Bug fix in AWS glue operator related to num_of_dpus #19787 (#21353),1
Switch from zdesk to zenpy in ZendeskHook (#21349),1
"Emit ""logs not found"" message when ES logs appear to be missing (#21261)Current ES log handler will wait up to 5 minutes for logs to appear (or for _more_ logs to appear since last log message was emitted).  This produces undesirable behavior when the log message has been deleted from the elasticsearch cluster.  A user may wait a long time thinking that the logs are coming when they are not.To resolve this, if no logs whatsoever have been retrieved after 5 seconds of trying, we give up and emit a ""logs not  found"" message.If the task has only just started, this may be a ""false negative"", and we guide the user to refresh if they think that might be the case.",1
Update information about rebasing after enabling rebase in GitHub (#21340),0
"Filter celery stuck task query to exclude completed tasks (#21335)On testing #19769, it was reported that there was a spike in CPU usage https://github.com/apache/airflow/pull/19769#issuecomment-1029755436Hopefully, this will fix it",0
"Fix docs link for smart sensor deprecation (#21394)We are releasing the deprecation in version 2.2.4, not 2.3.0 likeoriginally planned.",2
Update example DAGs (#21372),2
"Avoid deadlock when rescheduling task (#21362)The scheduler job performs scheduling after locking the ""scheduled""DagRun row for writing. This should prevent from modifying DagRunand related task instances by another scheduler or ""mini-scheduler""run after task is completed.However there is apparently one more case where the DagRun is beinglocked by ""Task"" processes - namely when task throwsAirflowRescheduleException. In this case a new ""TaskReschedule""entity is inserted into the database and it also performs lockon the DagRun (because TaskReschedule has ""DagRun"" relationship.This PR modifies handling the AirflowRescheduleException to obtain thevery same DagRun lock before it attempts to insert TaskRescheduleentity.Seems that TaskReschedule is the only one that has this relationshipso likely all the misterious SchedulerJob deadlock cases weexperienced might be explained (and fixed) by this one.It is likely that this one:* Fixes: #16982* Fixes: #19957",0
[Oracle] Oracle Hook - automatically set current_schema when defined in Connection (#19084),1
Fix BigQueryDataTransferServiceHook.get_transfer_run() request parameter (#21293),2
Add conditional `template_fields_renderers` check for new SQL lexers (#21403),1
Add how-to guide for WebHDFS operators (#21393),1
Create CustomJob and Datasets operators for Vertex AI service (#21253),1
"Fix docker behaviour with byte lines returned (#21429)* Fix docker behaviour with byte lines returnedThe fix from #21175 did not actually fix the logging behaviourwith non-ascii characters returned by docker logs when xcompush was enabled. The problem is that DockerOperator usesdifferent methods to stream the logs as they come (using attachstream) and different method to retrieve the logs to actuallyreturn the Xcom value. The latter uses ""logs"" method of dockerclient. The tests have not caught it, because the two methodswere mocked in two different places.This PR uses the same ""stringify()"" function to convert both""logged"" logs and those that are pushed as xcom. Also addedtest for ""no lines returned"" case.Fixes: #19027* Update tests/providers/docker/operators/test_docker.pyCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
Add dev script to find core PRs missing categorization (#21432),1
Typing support for operator mapping functions (#21415),1
eks_hook log level fatal -> FATAL  (#21427),2
Fixed changelog for January 2022 (delayed) provider's release (#21439),1
Display better error message when importing providers has errors (#21182)When there are import errors in providers we printed errorsin a folded group which lead to poor discovery of those errors.Also with recent changes to Airflow main for upcoming 2.3 versionsome errors might become more common when developing providers.Specifically the way how to import Context in order to satisfyMyPy and keep Airflow 2.1 compatibility is not obvious.This change introduces helpful guideline to users adding newproviders:* moving errors outside of the folded group with imports* adding comment explaining what the errors are about* adding message about backwards compatibility in case  errors happen during 2.1.0 backwards-compatibility check* adding explanation and suggest a fix in the common Context  impport error,0
"Add pre-commit check for docstring param types (#21398)Param types can now be inferred by sphinx from type annotations, so we no longer need them in docstrings.This pre-commit check fails when type declarations are included in docstrings, and seems to do a reasonable job of not catching false positives.",2
"Improve speed to run `airflow` by 6x (#21438)By delaying expensive/slow imports to where they are needed, this gets`airflow` printing it's usage information in under 0.8s, down from almost3s which makes it feel much much snappier.By not loading BaseExecutor we can get down to <0.5s",1
add how-to guide for sqoop operator (#21424),1
Some refactoring work on scheduling code (#21414),1
Add missing statsd metric for failing SLA Callback notification (#20924),0
Never set DagRun.state to State.NONE (#21263),2
Allow blank lines after docstring (#21477),2
Fix typing of operator attrs for mypy (#21480),1
Added SNS example DAG and rst (#21475),2
Simplify fab has access lookup (#21482)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>,2
Rewrite decorated task mapping (#21328),5
Move Zombie detection to SchedulerJob (#21181),4
"Modernize DAG-related URL routes and rename ""tree"" to ""grid"" (#20730)Co-authored-by: Igor Kholopov <kholopovus@gmail.com>",2
Fix broken tests after URL restructure PR (#21489),3
Log memory usage in CgroupTaskRunner (#21481),1
Fix Resources __eq__ check (#21442),0
Update link in Postgres connection doc (#21490),2
Fix postgres hook import pipeline tutorial (#21491),2
Optionally raise an error if source file does not exist in GCSToGCSOperator (#21391),1
Update Zendesk example DAG to use TaskFlow API (#21411),1
Update docs/readme.rst (#21346),2
add how-to guide for pig operator (#21498),1
Chart: Fix elasticsearch URL when username/password are empty (#21222),4
"Fix bug incorrectly removing action from role, rather than permission. (#21483)* Fix bug incorrectly removing action from role, rather than permission.* Add permissions directly to FAB view classes.",1
"Enable asynchronous job submission in BigQuery hook (#21385)* Add nowait flag to the insert_job method* When nowait is True, the execution won't wait till the job results are available.* By default, the job execution will wait till job results are available.",1
update tutorial_etl_dag notes (#21503)* update tutorial_etl_dag notes,2
Set larger limit get_partitions_by_filter in HiveMetastoreHook (#21504),1
Support config worker_enable_remote_control for celery (#21507),0
[doc] Fix copy path in S3 to redshift (#21416),0
Use temporary file in GCSToS3Operator (#21295)* Use temporary file in GCSToS3Operator,1
[doc] Improve s3 operator example by adding task upload_keys (#21422),1
Google Cloud Composer opearators (#21251),5
Support mssql in airflow db shell (#21511)We currently do not support mssql shell in the DB. This would ease troubleshooting for mssql,5
"Fix checking against a baseline (#21444)When bulk releasing rcn for providers we should check if the same""full"" version was released and only release if it was not,We used to check against the rcN release, but this was only workingwhen we had rc1 previously released.When releasing rc2 in bulk, we now skip the packages that havethe same ""full release"" tag.",1
Added haodf to INTHEWILD.md (#21510)Added haodf to the list of companies using Apache Airflow,1
Only adds MySQL apt sources once (#21519)The MySQL apt sources were appended rather than overwrittenin the installation script. That resulted with double entry inCI image as we are installing both PROD and DEV dependenciesthere.This PR overrides the list rather that appends to it.,1
Straighten up MappedOperator hierarchy and typing (#21505),1
Fix key typo in `template_fields_renderers` for `HiveOperator` (#21525),1
improved backwards compatibility (#21524)Co-authored-by: Maximilian Mehnert <memax@live.de>,1
"fix all ""high"" npm vulnerabilities (#21526)",0
Don't track changes to task_log_prefix_template (#21516)It only gets used in the produced logs (and not to find logs) so wedon't care about historic values of it,2
add how-to guide for livy operator (#21529),1
Chart: Add annotations to cleanup pods (#21484),4
Prepare to switch to debian bullseye (#21522)In order to switch smoothly to Debian Bullseye we need to run afew more tests and see if the performance impact we observe isconsistent (the tests with Public Runners seems to be killed morefrequently there).This requires some preparatory work - namely add the capabilityof overriding the DEBIAN_VERSION via PR - because otherwiseBuild Images workflow that runs from main will build buster imagesuntil the change is actually merged.This change introduce this capability without switching toBullseye yet - it will unblock the actual PR that will make thechange from failing in PRs though.,0
Use compat data interval shim in log handlers (#21289),0
fixup! Prepare to switch to debian bullseye (#21522) (#21536),0
Add Audit Log View to Dag View (#20733),2
Added template_ext = ('.json') to databricks operators #18925 (#21530),1
"Add mssql-cli to devel extra in Airflow (#21520)The change #21511 added support for db shell for MSSQL. This changefollows up with adding mssql-cli adding to [devel] extra of airlfowso that it is automatically installed in Airflow Breeze CI image.The mssql-cli is a python command line and it has a number ofdependencies that clash with ""airflow's"" python dependencies,therefore we install the mssql-cli via pipx so that it gets itsown set of dependencies and virtualenv.",1
"Add support for BeamGoPipelineOperator (#20386)closes: https://github.com/apache/airflow/issues/20283In this PR:- [x]  Upgrade the minimum package requirement to 2.33.0 for apache-beam (first stable for beam go sdk)- [x]  Refactor `operators/beam.py` with an abstract `BeamBasePipelineOperator` class to factorize initialization and common code, also fixed mypy hook on ``BeamDataflowMixin``- [x] Add `BeamRunGoPipelineOperator` and `BeamHook.start_go_pipeline` (+tests)- [x]  Add `utils/go_module.py` to handle initialisation and dependency installation for a module. (+ tests)- [x]  Slightly modified `process_util` + tests to be able to handle an extra optional parameter `cwd`. (This way we can move to the module directory to build it)- [x]  Write docs",2
Add str to task_ids typing in BaseOperator.xcom_pull(#21541)Co-authored-by: Alexander Chen <alexchen@apple.com>,1
Fix bigquery-hook when no  engine_kwargs are passed,4
(providers_google) add a location check in bigquery (#19571),1
Clarify quick start step and link to Breeze doc in MySQL error (#21552),0
Add documentation for RC3 release of providers for Jan 2022 (#21553),1
Show task status only for running dags or only for the last finished dag (#21352)* Show task status only for running dags or only for the last finished dag* Brought the logic of getting task statistics into a separate function,1
"Doc: Remove extra 'of' (#21408)* Remove extra 'of'Just a double 'of', removes the second one.* removing 'of' from link and adding back after link",2
Branch python operator decorator (#20860),1
Add statistic calculation for Provider's testing. (#21564)This script allows to calculate some basic stats of testing forprovider releases.,1
Standardize approach to dependencies (#21356)Approach to dependencies we had (especially with regards toupper bounds) was pretty random so far. This PR attempts todescribe the rules discussed in the devlist discussion - includingreview and update of all dependencies to match the policies.,5
Dispose unused connection pool (#21565),1
"Remove unnecessary/stale comments (#21572)We certainly don't need a stale and incorrect 'Last Updated' comments,and likely also don't need the original author either, so remove them.Both can be found via git history.",4
Get log events after sleep to get all logs (#21574),2
"Fix multi query scenario in bigquery example DAG (#21575)- The Google Bigquery Jobs api can accept multiple queries passed in  a string separated by a semicolon. When queries are passed in a list,  then it ends up executing first query only.",4
"Add option to compress Serialized dag data (#21332)The uncompressed dag data size can be very large for large DAGs. In our prod db, the dag size can be up to `514MB`.Adding this optional feature to compress the dag data. It reduces the size from `514MB` to `44MB`.By default, `compress_serialized_dags` is `False`.",2
Fix mypy issues in `example_twitter_dag` (#21571),2
Change logging level details of connection info in `get_connection()` (#21162),1
Fixed PostgresToGCSOperator fail on empty resultset for use_server_side_cursor=True (#21307)Fixed issue #20007,0
"Change default log filename template to include map_index (#21495)* Change default log filename template to include map_indexWith the recently added LogTemplate mechanism old TIs will still use theformat they had at creation time (with the change here to ensure that wecreate a LogTemplate row for the just-upgraded-in-place) so the logs canstill be viewed in the UIAnd since it was now getting quite ""deep"" I have chosen to ""label"" thecomponents in the ""hive partition style""* Make ti.set_context work for MappedOperatorsThe `except AttributeError` was _also_ catching more than just ""thishandler doesn't have a set_context attribute"", but also errors fromcalling that function which lead to hard-to-track-down errors (missinginlets/outlets)I have also changed `get_template_context` to a side-effect-freefunction, so it no longer mutates task.params!* Change default template to put run_id before task_idAnd instead of having to duplicate the default config value in toconfiguration.py for the update process, change it to get the newdefault value out of the loaded default_airflow.cfg* Fix smart sensor loggingIt has been broken in main for a while (it works fine in 2.2.x series),but because we were catching _all_ AttributeErrors we never noticed.* Update Elasticsearch logging config to include map_indexSince the log_id is never really visible to users I have taken theeasier approach of just always including the map_index, even forunmapped tasks.",1
Fix race condition between triggerer and scheduler (#21316),0
Type TaskInstance.task to Operator and call unmap() when needed (#21563),1
"Tree view: Highlight dag run on hover (#21476)* vertical highlighting when hover on group* full horizontal/vertical crosshairs* fix ref, js only classname, linting* calculate vertical bar height* improve vertical highlight calculation* update package.json",5
Fix Mypy errors in Kylin example (#21589),0
"Fix EcsOperatorError, so it can be loaded from a picklefile (#21441)",2
Add a session backend to store session data in the database (#21478)Co-authored-by: Jed Cunningham <jedcunningham@apache.org>,5
Simplify trigger cancel button (#21591)Co-authored-by: Jed Cunningham <jedcunningham@apache.org>,2
Fix postgres part of pipeline example of tutorial (#21586),0
"S3KeySensor to use S3Hook url parser (#21500)Rather than relying on the poke method to parse the attrs we should either do it in `__init__` or in a cached property.Since it's a pretty insignifigant computation let's  just do it in `__init__`.And anyway, if the params are bad then best to get a warning before deploying your code.",2
Update test connection functionality to use custom form fields (#21330),1
Normalize *_conn_id parameters in BigQuery sensors (#21430)It fixes deprecation warning in `BigQueryHook` because of `bigquery_conn_id` parameter usage from sensors code.Note: similar change for BigQuery operators was already performed in commit 042a9ba2c285772fcc2208847198c2e2c31d4424,1
Dataproc metastore assets (#21267),1
Datafusion assets (#21518),1
Pass Trino hook params to DbApiHook (#21479),5
Fix test_clear_multiple_external_task_marker timing out (#21343)closes: #11443,3
Updated Databricks docs for correct jobs 2.1 API and links (#21494),2
Extract ClientInfo to module level (#21554),5
Add note about Variable precedence with env vars (#21568)This PR updates some documentation regarding setting Airflow Variables using environment variables. Environment variables take precedence over variables defined in the UI/metastore based on this default search path list: https://github.dev/apache/airflow/blob/7864693e43c40fd8f0914c05f7e196a007d16d50/airflow/secrets/__init__.py#L29-L30,5
Add how-to guide for hive operator (#21590),1
Added retries to LivyHook #19384  (#21550),1
"Implement multiple API auth backends (#21472)* Implement API auth through session* Expand API auth to multiple backendsAs part of AIP-42, the auth_backend setting is expanded toauth_backends, and on an API request each is tried one after the otheruntil one succeeds. A new auth backend of session is added that willvalidate against the signed-in user in the case where requests are madevia JavaScript from the UI.",1
Action log on Browse Views (#21569),2
"Adds ADRs resulting from discussions during internship (#21549)Those ADRs describe some of the context of Docker optimisationtechniques we used - why we used them, what was the rationaleand solution applied.",1
Reorder migrations to include bugfix in 2.2.4 (#21598),0
Switch XCom implementation to use run_id (#20975),1
Add GoogleCalendarToGCSOperator (#20769),1
Don't check if `py` DAG files are zipped during parsing (#21538),2
Alembic migration filename should match revision id (#21621)This is the one mismatch we have.,2
Support generating SQL script for upgrades (#20962)This PR attempts to add support for generating sql scripts for upgrade.Example command:`airflow db upgrade --revision-range e8d98d8ss99:78daisdu38d``airflow db upgrade --range 2.0.0:2.2.3`,5
Delay creation of dag_run table object (#21622)In order for the `alembic history` command to work properly we need to defer creation of the dag_run Table object until we're inside upgrade / downgrade.,2
Support different timeout value for dag file parsing (#21501),2
Chart: add envFrom to the flower deployment (#21401),1
Update test example in CONTRIBUTORS_QUICK_START.rst (#21620)Co-authored-by: Adam Paslawski <adampaslawski@mail.utoronto.ca>,3
Add GCSToPrestoOperator (#21084),1
Adding Cover Genius to INTHEWILD.md (#21637)Co-authored-by: Lihan Li <lihan@covergenius.com>,1
"Fix bug in Breeze2 auto-complete setup with root files modified (#21636)There was a bug with ""/"" added when files were modified bythe autocomplete.Fixes: #21163",0
Move `confirm` script into `scripts/tools` (#21626),5
Dataflow Assets (#21639),1
"Remove wrapper scripts from static_checks folder (#21643)The wrapper scripts in static checks were remnant of old approachof running the static checks. They were there for those whodid not want to use pre-commit and they provided extra parametersthat could be passed and variables that could be used andprovided more verbose output. This is however all possible withthe `pre-commit` command and we are all used to run pre-commitand we even have missing auto-complete for those in Breezeso there is no need to keep the wrappers around.The static-checks folder remains now only for the real""static check"" actions that are run directly from CI:* running all static checks* running only basic static checks* running www lint* running ui lintThose scripts are directly referred from the ci.yml worflow,thus the ""scripts/ci/static-checks"" is now back to what it wasorignally used for.",1
"Add params dag_id, task_id etc to XCom.serialize_value (#19505)When implementing a custom XCom backend, in order to store XCom objects organized by dag_id, run_id etc, we need to pass those params to `serialize_value`.",2
Upgrade Postgres client in Docker images (#21631),2
added explaining concept of logical date in DAG run docs (#21433),2
Adding missing login provider related methods from Flask-Appbuilder (#21294),1
"Clarify pendulum use in timezone cases (#21646)It is important to use Pendulum in case timezone is used - becausethere are a number of limitations coming from using stdlibtimezone implementation.However our documentation was not very clear about it, especiallysome examples shown using standard datetime in DAGs which couldmislead our users to continue using datetime if they use timezone.This PR clarifies and stresses the use of pendulum is necessarywhen timezone is used. Also it points to the documentationin case serialization throws error about not using Pendulumso that the users can learn about the reasoning.This is the first part of the change - the follow up will bechanging all provider examples to also use timezone andpendulum explicitly.See also #20070",1
Pin Markupsafe until we are able to upgrade Flask/Jinja (#21664)Markupsafe 2.1.0 breaks with error: import name 'soft_unicode' from 'markupsafe'.This should be removed when either this issue is closed:https://github.com/pallets/markupsafe/issues/284or when we will be able to upgrade JINJA to newer version (currentlylimited due to Flask and Flask Application Builder),1
"Deprecate helper utility `days_ago` (#21653)This helper function is not all that helpful, introduces confusion (e.g. which timezone? how to handle DST?), and results in a ""moving"" start date for dags.Vote thread: https://lists.apache.org/thread/qfqjb8m3v834yc8mxo1oqtjddhp9sggk",2
Update best-practices.rst (#21679)I'm not sure how to view this in staging but I think I got the syntax right? I mimicked another one that referred to the top level code header.,5
Fix Amazon SES emailer signature (#21681)Amazon SES Had a wrong signature to become a mailer in Airflowintroduced in #18042. As the result setting SES emailer asemail backend resulted in:```TypeError: send_email() missing 1 required positional argument:'html_content'```Fixes: #21671,0
Increase timeout for constraints (#21680)The constraints building takes longer than 25 minutesin public runners. This only happens when you modify setup.py sothis should not affect regular PRs.,1
Add celery_logging_level (#21506),2
Update EKS sample DAGs and docs (#21523)* Update EKS sample DAGs to new standards,1
Update MySqlOperator example dag (#21434),2
Convert our internal tools to use rich_click (#21689)Rich_click provides out-of-the-box beautifying of the click commandline tools to provide 'rich' help output.This PR changes our internal tools (including the new Breeze2in-progress effort to use rich click to make our help nicer.,1
"Add extra information about time synchronization needed (#21685)When you have several machines running Airflow and their timeis not synchronized, you might get very weird behaviour - some ofthe log retrieval actions might randomly return ""forbidden"" errorbecause of expiring token. This is extremely difficult todiagnose and figure out (some of our users spent days oninvestigating that). Explicitly stating the requirement in theforbidden error and whenever secret_key parameter ismentioned, should help our users to diagnose it more easily(and save maintainers from unnecessary questions and discussionsin Slack :))",1
"Refactor TriggerRule & WeightRule classes to inherit from Enum (#21264)closes: #19905related: #5302,#18627Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",4
Log exception in local executor (#21667),2
Simply imports in dev github script (#21702),2
[de]serialize resources on task correctly (#21445),5
Add Auto ML operators for Vertex AI service (#21470),1
Fix import in unit test example (#21703),3
Fix assertion to check actual content (#21678)Fix the assertions to check the actual contents of the dictionary instead of checking only for existence.,3
"Run mapped tasks via the normal Scheduler (#21614)* Create an end-to-end test for running a DAG via the scheduler.This is important as there are a lot of moving parts in mapped DAGs andwe want to make sure that, somewhere, the DAG runs to completion.This is marked as a long-running test as it could be :) It is certainlymore than a ""unit"" test at any rate!* Expand mapped tasks in the SchedulerTechnically this is done insideDagRun.task_instance_scheduling_decisions, but the only place that iscurrently called is the SchedulerThe way we are getting `upstream_ti` to pass to expand_mapped_task isall sorts of wrong and will need fixing, I think the interface for thatmethod is wrong and the mapped task should be responsible for findingthe right upstream TI itself.* Test that DagRun can cope with expanding a task resulting in SKIPPING",2
Bump upper bound version of jsonschema to 5.0 (#21712)The upper bound is necessary because of connexion upper bound on jsonshema,5
Implement mapped value unpacking (#21641),5
Extend documentation for states of DAGs & tasks and update trigger rules docs (#21382),2
Replaced hql references to sql in TrinoHook and PrestoHook (#21630)* Replaced hql references to sql in TrinoHook and PrestoHook,1
Fix failing tests on main due to merging incompatible changes (#21733),4
Introduce 'Callbacks Sink' classes for sending callbacks (#21301),5
Fix graph autorefresh on page load (#21736)* fix auto refresh check on page load* minor code cleanup* remove new line,1
Fix stray order_by(TaskInstance.execution_date) (#21705),5
2.2.4 has been released (#21744),5
"Fix some migrations (#21670)In the xcom migration, there's a bad join. The clauses need to be wrapped in and_.  And in both, for sqlite we need to temporarily suspend FK enforcement before dropping the tables.",4
Dev:`constraints-latest` needs to be force-pushed (#21746),1
Add support for custom command and args in jobs (#20864),1
"Simplify chart docs for configuring Airflow (#21747)This simplifies the docs page for configuring Airflow. Instead ofshowing an (out of date) complete list of default configs, which don'tneed to be set, only show the simple config override required for theexample.This also rewords a few sections for better clarity.",1
Remove python 3.5 reference from README (#21749)Airflow 1.10.x has been EOL long enough now that we don't need to keepsections specific to it in the main README any longer.,4
Chart: Default to Airflow 2.2.4 (#21745),2
Fix triggerer --capacity parameter (#21753),2
Add Dataproc assets/links (#21756)Co-authored-by: Wojciech Januszek <januszek@google.com>,2
Fix the triggerer capacity test (#21760)Commit 9076b67 changed the triggerer logic to use int not string.,1
"Don't show alembic info logs at the start of every cli command (#21758)We recently merged a change where we check if DB migrationsare pending before running the main command, but this had theside-effect of showing these two log lines from alembic:```[2022-02-22 18:06:01,995] {{migration.py:201}} INFO - Context impl PostgresqlImpl.[2022-02-22 18:06:01,995] {{migration.py:204}} INFO - Will assume transactional DDL.```Which is a) not useful information to a user, and b) ""pollutes"" theoutput if a command was producing JSON or some other structured format",5
"Change the default auth backend to session (#21640)* Change default backendAs part of AIP-42, change the default auth backend to validate using the session,so that the UI can use the API. If auth_backends has been set to a non-defaultvalue, include the session in the list of backends.* When updating a deprecated config value from env, set it back to envOtherwise this means the config seen by an execed sub-process would bedifferent (and wrong, taking neither the configured env var value, northe new default, but instead just what is in the config file!)* Remove the chart auth_backends settingCo-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
(AzureCosmosDBHook) Update to latest Cosmos API (#21514)* Bumping the ms azure cosmos providers to work with the 4.x azure python sdk apiCo-authored-by: gatewoodb <ben@everythingisbroken.net>,5
"Correct a couple grammatical errors in docs (#21750)Just reading through the docs as we implement Airflow on our end, saw a couple additions that could be made.",1
Add Paxful to INTHEWILD.md (#21766),1
Add `2.2.4` to db migrations map (#21777),5
Fix max_active_runs=1 not scheduling runs when min_file_process_interval is high (#21413)The finished dagrun was still being seen as running when we call dag.get_num_active_runsbecause the session was not flushed. This PR fixes it,0
Correctly handle multiple '=' in LocalFileSystem secrets. (#21694),5
Rewrite taskflow-mapping argument validation (#21759),5
Fix bigquery_dts parameter docstring typo (#21786),2
"Add --platform as parameter of image building (#21695)Building image for now shoudl be forced to linux/amd64 as this isthe only supported platform. When using BUILDKIT, the defaultplatform depends on the OS/processor, but until we implementmulti-platform images we force them to linux/amd64.Setting it as parameter of docker build instead of env variablesis more explicit and allows to copy&paste the whole commandto reproduce it outside of breeze when verbose is usedindependently if you are on Linux, MacOS Intel/ARM.",1
Upgrade and record elasticsearch log_id_template changes (#21734),4
Rename operator mapping map() to apply() (#21754),1
Restore image rendering in AWS Secrets Manager Backend doc (#21772),2
Use Pendulum's built-in UTC object (#21732)Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
Make sure emphasis in UPDATING in .md is consistent (#21804)There is a new rule in markdownlint which has been violated inmain when new version of pre-commits is installed introduced inthe #21734,1
REST API: add rendered fields in task instance. (#21741)Make task instance rendered template fields available in the REST API.Co-authored-by: Mocheng Guo <mocheng.guo@airbnb.com>,1
"Use DB where possible for quicker ``airflow dag`` subcommands (#21793)Some of the subcommands here don't actually need a full dag, so there isno point paying the (possibly long) time to parse a dagfile if we couldgo directly to the DB instead.Closes #21450",5
Fix test CreateUserJobTest::test_should_disable_default_helm_hooks (#21776)The test was asserting against a wrong attribute when disablingHelm's hooksThese hooks are required for a proper synchronizationwith Kubernetes jobs when deploying with ArgoCD,1
"Add another way to dynamically generate DAGs to docs (#21297)Also, move dynamic DAG generation cases from ""best practices"" to ""how to"" section",2
Attempt to upgrede to 22.0.3 version of `pip` (#21818)There was a recent version of `pip` - 22.0 line that had quite a numberof teething problems. Hopefully all teething problems were solvedalready and we should be able to upgrade to teh newer version.This might (maybe) help with solving some of the backtrackingissues that started to appear during the last few days.,0
Make DbApiHook use get_uri from Connection (#21764)DBApi has its own get_uri method which does not dealwith quoting properly and neither with empty passwords.Connection also has a get_uri method that deals properlywith the above issues.This also fixes issues with RFC compliancy.,0
"Remove 8 year old screenshots (#21819)These still show the project being called ""Flux"" -- so they aredefinitely not needed anymore :)",5
fix param rendering in docs of SparkSubmitHook (#21788),1
Add step to add releases to the committee report helper (#21812),1
Fix typo in chart docs (#21814),2
Show Task Map Index in  task instance table (#21774)* add map_index to /taskinstance table* use markup for blank value,1
Fix doc - replace decreasing by increasing (#21805),1
"Update TaskFlow tutorial doc to show how to pass ""operator-level"" args. (#21446)* Remove invalid arg from snippet in TaskFlow tutorial doc* Add `retries` to a different code snippet",1
"Limits GitHub3.py in order to avoid backtracking (#21824)* Limits GitHub3.py in order to avoid backtrackingGithub3 version 3.1.2 requires PyJWT>=2.3.0 which clashes with Flask AppBuilder where PyJWT is <2.0.0 Actually GitHub3.1.0 already introducedPyJWT>=2.3.0 but so far `pip` was able to resolve it without gettinginto a long backtracking loop and figure out that github3 3.0.0 versionis the right version similarly limiting it to 3.1.2 causes pip not toenter the backtracking loop. Apparently when there Are 3 versions withPyJWT>=2.3.0 (3.1.0, 3.1.1 an 3.1.2) pip enters into backtrack loop andfails to resolve that github3 3.0.0 is the right version to use.This limitation could be removed if PyJWT limitation < 2.0.0 is droppedfrom FAB or when pip resolution is improved to handle the case, Theissue which describes this PIP behaviour and hopefully allowing toimprove it is tracked in https://github.com/pypa/pip/issues/10924.",0
"Switch to GitHub-rendered Mermaid diagrams (#21682)We used mermaid diagrams in a few places and had pre-commitscript to generate images out of them, but since GitHubintroduced rendering of mermaid diagrams in javascript, we cannow switch to markdown files with embedded mermaid markup.",2
extends typing-extensions to be installed with python 3.8+ #21566 (#21567),2
Remove types from KPO docstring (#21826)These are no longer the right types and now they are inferred correctlyanyways.,5
Describe policy about support for OS versions used. (#21697),1
Added Hook for Amazon RDS. Added `boto3_stub` library for autocomplete. (#20642),1
"Add `db clean` CLI command for purging old data (#20838)CLI command to delete old rows from airflow metadata database.Notes:* Must supply ""purge before date"".* Can optionally provide table list.* Dry run will only print the number of rows meeting criteria.* If not dry run, will require the user to confirm before deleting.",4
Add Github integration steps for committers (#21834),1
"Fix assignment of unassigned triggers (#21770)Previously, the query returned no alive triggerers which resultedin all triggers to be assigned to the current triggerer. This worksfine, despite the logic bug, in the case where there's a singletriggerer. But with multiple triggerers, concurrent iterations ofthe TriggerJob loop would bounce trigger ownership to whicheverloop ran last.Addresses https://github.com/apache/airflow/issues/21616",0
Truncate stack trace to DAG user code for exceptions raised during execution (#20731),1
Bug Fix - S3DeleteObjectsOperator will try and delete all keys (#21458)If a templated rendered empty list of key is passed into S3DeleteObjectsOperatorwithout any prefix it will deleted everything on the bucket,4
Add ALL_SKIPPED trigger rule (#21662),1
Databricks: add support for triggering jobs by name (#21663),1
Fix oracle test connection (#21699),3
Removed 'request.referrer' from views.py  (#21751),4
Add GCSToTrinoOperator (#21704),1
Fix handling cached parameters for Breeze2 (#21849)* if parameter is not passed and not in cache - default is used* if parameter is passed as --<parameter> it is stored in cache  and used* if parameter is not passed and in cache - it is retrieved from  cache and used.,1
Add SageMakerDeleteModelOperator (#21673)* Implement SagemakerDeleteModelOperator,4
Fix logging JDBC SQL error when task fails (#21540),0
Databricks SQL operators (#21363),1
Add unit tests to infer `multiple_outputs` when invoking decorator `__call__` (#21773),5
Minor `pre-commit` config clean up (#21794),4
Airflow `db downgrade` cli command (#21596),5
Add dbt Cloud provider (#20998),1
Suppress hook warnings from the Bigquery transfers (#20119),2
Add 'method' to attributes in HttpSensor. (#21831),1
Fix DAG date range bug (#20507),0
Fix handling of empty (None) tags in `bulk_write_to_db` (#21757),5
Quick Update GCS Presto (#21855),5
Add JSON output on SqlToS3Operator (#21779),1
Add 'Show record' option for variables (#21342),1
Configurable AWS Session Factory (#21778),5
"Fix mixup up shadowed ImportError in db_cleanup (#21862)TableConfig requires models.ImportError, while the try/excepton line 128 requires the builtin ImportErrorThis was causing errors when running with k8 scheduler,without having celery packages installed.",1
update value from database to celery workers (#21859)Co-authored-by: huan.15 <huan.15@kakaocorp.com>,1
"Allow to switch easily between Bullseye and Buster debian versions (#21546)We are preparing to switch from Buster to Bullseye and this isthe second change that is needed (following #21522). This changeallows to choose whether we want to use Buster or Bullseye imagesas a base. We need to be able to choose, because:1) we want to keep backwards compatibility and continue our   users to build Buster-base images2) we cannot yet fully switch to Bullseye because MsSQL's odbc   driver does not yet support Bullseye and we reached out to   mysql maintainers to learn about their plans to make the   decision on when and how we are going to support Bullseye and   MSSQL.   Details of this discussion are in:   https://github.com/MicrosoftDocs/sql-docs/issues/7255#issuecomment-1037097131This PR adds the capability of choosing the DEBIAN_VERSION inBreeze when building images but does not yet switch from Buster toBullseye",1
"Switch to Debian 11 (bullseye) as base for our dockerfiles (#21378)Debian 11 Bullseye have been released some time ago as the newLTS Debian release and already all our dependencies (includingMySQL and MSSQL ODBC drivers) caught up with it so we can finallymigrate to it.This change switches base images to bullsey for our Dockerfilesas well as for Redis image we are using in CI.The relevant packages have been updated to include thatand documentation have been updated.Examples of ours also are updated to use ""bullseye"" rather thanbuster.Closes: #18190Closes: #18279",1
"Revert ""Switch to Debian 11 (bullseye) as base for our dockerfiles (#21378)"" (#21874)This reverts commit 5d89dea56843d7b76d5e308e373ba16ecbcffa77.The issue is not a random IO timeout -- it's a problem with the file in the repo.Reverting this right now as all PRs are failing :(",0
Update to latest version in Dockerfile (and add instructions) (#21876),1
Log traceback in trigger excs (#21213),2
"Make Grid and and Graph view work with task mapping (#21740)* Expand mapped tasks in the SchedulerTechnically this is done insideDagRun.task_instance_scheduling_decisions, but the only place that iscurrently called is the SchedulerThe way we are getting `upstream_ti` to pass to expand_mapped_task isall sorts of wrong and will need fixing, I think the interface for thatmethod is wrong and the mapped task should be responsible for findingthe right upstream TI itself.* make UI and tree work with mapped tasks* add graph tooltip and map count* simplify node label redraw logic* add utils.js and map_index to /taskInstances* use TaskInstanceState instead of strings* move map_index on /taskinstance to separate PR* check to use Task or Tasks* remove `no_status` and use TaskInstanceStateCo-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
Fix UPDATING section on SqlAlchemy 1.4 scheme changes (#21887),4
"Update databricks.rst (#21886)The method outlined in the current doc results in a 403 error, which can be avoided by following Databricks' documentation on this topic: https://kb.databricks.com/dev-tools/invalid-access-token-airflow.html. The changes suggested here reflect the Databricks doc.",2
"Fix the Type Hints in ``RedshiftSQLOperator`` (#21885)`sql` accepts `Union[str, Iterable[str]],` not `Optional[Union[Dict, Iterable]],`",1
Allow templates in more DataprocUpdateClusterOperator fields (#21865),5
Autogenerate migration reference doc (#21601)* document airflow version in each alembic migration module and use this to autogen the doc* update each migration module to have the same description used in migration ref (so it can be used in autogen),1
Fix filesystem sensor for directories (#21729)Fix walking through wildcarded directory in `FileSensor.poke` method,2
add celery.task_timeout_error metric (#21602)* log celery task id to correlate logs* add celery.task_timeout_error metric,0
Feature: Add invoke lambda function operator (#21686),1
"Update modules_management.rst (#21889)need to add packages in setup.py, otherwise the package can not be found",1
"Switch to Debian 11 (bullseye) as base for our dockerfiles (#21378) (#21875)Debian 11 Bullseye have been released some time ago as the newLTS Debian release and already all our dependencies (includingMySQL and MSSQL ODBC drivers) caught up with it so we can finallymigrate to it.This change switches base images to bullsey for our Dockerfilesas well as for Redis image we are using in CI.The relevant packages have been updated to include thatand documentation have been updated.Examples of ours also are updated to use ""bullseye"" rather thanbuster.Closes: #18190Closes: #18279",1
Fix TaskDecorator type hints (#21881),0
"Change BaseOperatorLink interface to take a ti_key, not a datetime (#21798)",5
Limits Yandexcloud to 0.145 to unblock main failing tests. (#21899)The yandexcloud 0.145 broke logging feature of the client libraryand it fails yandex provider tests.https://github.com/yandex-cloud/python-sdk/issues/47,3
"Fix Kubernetes example with wrong operator casing (#21898)The Kubernetes example of ours had bad casing in ""In"" operator(""in"") and recent kubernetes client library started to fail importingthe example).This PR fixes case for the operator.",1
EdgeModifier refactoring (#21404),4
"Deprecate non-JSON conn.extra (#21816)Connection extra field is generally assumed to be JSON but we don't actually require it.  Here we deprecate non-JSON extra so that in 3.0 we can require it.  Further, we require that it not just be any json but must also parse as dict, because a string value such as '""hi""' or '[1,2,3]' is json, but a very bad practice.",5
Limit yandexcloud to < 0.142 (#21903)It turned out the yandexcloud bugfix attempt from #21899 was notsuccessful. The bug was present already in 0.142,0
"Fix handling some None parameters in kubernetes 23 libs. (#21905)Kubernetes 23.* is more picky when it comes to values passed toPod Generator - it requires:* imagePullPolicy* dnsPolicy* restartPolicyto be not None.We are fixing it in the way, that we simply skip setting thoseif they are None.",1
Make project_id argument optional in all dataproc operators (#21866)`DataprocHook` handles `project_id=None` in `@GoogleBaseHook.fallback_to_default_project_id` decorator. In some operators `project_id` was already optional. Make it optional for all.,1
Update list of non-core files/paths (#21907),2
Add-showing-runtime-error-feature-to-DatabricksSubmitRunOperator (#21709),5
"Refactor KubernetesPodOperator tests (#21911)This re-works many of the KubePodOp tests to just _real_DAGs/DagRun/TaskInstance rather than faked ones, and splits out some ofthe tests that checked 3 or 4 ""concepts"" in to multiple separate testfunctions",3
Add Python 3.9 support to Hive (#21893)Hive support for Python 3.9 has been removed in #15515 but clouderareleased new ssl 0.3.1 library version to support it and we shouldbe able to get Hive provider working for Python 3.9 too.Fixes: #21891,0
"Chart: move updating note about config removal (#21919)This was accidentally placed in the 1.4.0 section, which is alreadyreleased. Move it to the 1.5.0 section where it should be.",4
Unpin ``pandas-gbq`` and remove unused code (#21915)* Unpin ``pandas-gbq`` and remove unused code`BigQueryPandasConnector` was previously used by `BigqueryHook.get_pandas_df`. This was fixed in https://github.com/apache/airflow/commit/ad308ea441372f2b44b4292c3779eb745f2ed48c (**in 2018**). However we forgot to remove `BigQueryPandasConnector` which is age-old code (2016) and used private methods.2016 code - https://github.com/apache/airflow/pull/1452/files,2
Unpin `google-cloud-memcache` (#21912),5
Add `test_connection` method to `AzureDataFactoryHook` (#21924),5
py files doesn't have to be checked is_zipfiles in refresh_dag (#21926),2
Fix incorrect data provided to tries & landing times charts (#21928),2
"Ensure deps is set, convert BaseSensorOperator to classvar (#21815)",1
Only allow mapping against return value XCom (#21930),1
Fix single backticks in rst changelogs (#21922),4
Add map_index to pods launched by KubernetesExecutor (#21871)I also did a slight drive-by-refactor (sorry!) to rename `queued_tasksand `task` inside `clear_not_launched_queued_tasks` to `queued_tis` and`ti` to reflect what they are.,4
Add map_index label to mapped KubernetesPodOperator (#21916),1
update freshworks company users (#21932),1
Update ECS sample DAG and Docs to new standards (#21828),1
Chart: 1.5.0 changelog (#21906)Add changelog for chart version 1.5.0.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
"Rename `xcom.dagrun_id` to `xcom.dag_run_id` (#21806)We use snake case for tables and columns in the metadata database.  So references (in the database) to `dag_run.id` should be `dag_run_id`.  Historically though, in many places in the codebase we used the expression ""dag run id"" to refer to the `run_id` attribtue of the DagRun model.  So here we try to disambiguate this where necessary by using `run_id` to refer to DagRun.run_id and `dag_run_id` to refer to `DagRun.id`.One area where I did not make any changes is in the REST API which exposes parameter `dag_run_id` to refer to the `run_id` field in DagRun.  Since this param is user-facing, it's OK for it to be disconnected from the internal code references and certainly from database object naming.  We can consider renaming to `run_id` in the next major API release.  Also did not change the code in `views.py`.Squashed commits:* rename IN_MEMORY_DAGRUN_ID* rename ARG_EXECUTION_DATE_OR_RUN_ID* rename DagrunIdDep* fake_dagrun_id* rename dagrun_id to dag_run_id* clarify docstrings / comments* rename dag_run_id to run_id in dag model* _set_dag_run_state param dag_run_id -> run_id",1
"Exit with appropriate message for shell in PROD image in breeze (#21848)While Breeze allows to build production image as well as the CIimage, it is not really meant to be used to run production image.Unlike CI image, which requires many parameters to pass to dockerand docker-compose commands, most of the docker commands for PROD imageare already possible and easy to be executed with regular dockercommand.When someone tries to execute the PROD image they will get appropriateerror message and link to the documentation explaining how torun commands in production image.Fixes: #21723",0
Make container creation configurable when uploading files via WasbHook (#20510),1
Add RedshiftDataHook (#19137)Use the AWS `redshift-data` API to interact with AWS redshift clustersCo-authored-by: john-jac <jacnjoh@amazon.com>,5
Add docs for `db upgrade` / `db downgrade` (#21879),5
"Add CI jobs and tooling to aid with tracking backtracking pip issues (#21825)* Add CI jobs and tooling to aid with tracking backtracking pip issuesThis is a follow-up after investigation done with failing mainbuilds (resulting in #21824 and tracked inhttps://github.com/pypa/pip/issues/10924)Handling failure of image building has been also improved as partof the PR. Instead of separate ""cancel"" job (which did not reallywork anyway) we build and push empty images instead and theempty images are handled in the ""wait for images"" jobwith appropriate message.* Update dev/breeze/src/airflow_ci/find_newer_dependencies.pyCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>",1
Enhance magic methods on XComArg for UX (#21882)* Enhance magic methods on XComArg for UXImplementing Python's magic methods for purposes not matching theiroriginal design can have unwanted consequences due to defaultinteractions between those methods. This implements a custom __iter__ sothe custom __getitem__ does not trigger the default __iter__ behaviorand result in bad run-time behavior.,1
enter the shell breeze2 environment (#21145),5
"Proposed policy for Provider's minimum supported version. (#21696)This is the proposed policy, where we explain when by defaultwe increase the minimum supported version for the providers werelease from the community.",1
Add detailed email docs for Sendgrid (#21958)Improved documentation for Sendgrid emailFixes #20419,0
Dev: Update script used in generating issues for status of testing of RC (#21950),3
Fix typo in docs (#21964),2
Remove unnecessary loggings from offline sql generation command (#21962)I found a way to remove unnecessary loggings that comes from validating revisions in the offlinemigration command. Also cleaned up some codes,4
"Change KubePodOperator labels from exeuction_date to run_id (#21960)Now that execution_date isn't the PK for TaskInstance we should replaceit with run_id.There is on backwards compatibility concern here, as these labels areonly needed for re-attaching to a container, and the way we get therun_id is compatible back to 2.1",1
"More explicit mapped argument validation (#21933)* More explicit mapped argument validationInstead of always using MagicMock to validate mapped arguments, thisimplements a more sophisticated protocol that allows an operator toimplement a 'validate_mapped_arguments' to provide custom validationlogic. If an operator just wants to use __init__ for validation,however, they can set a flag 'mapped_arguments_validated_by_init' to getthe behavior easily. (This does *not* use MagicMock, however, since anycustom validation logic should be able to handle those on its own).The 'validate_mapped_arguments' flag is currently only set onPythonOperator. It can likely be used on a lot more operators down theroad.* Add flag to distinguish a validation-only initThere's just too much magic during a task's initialization that tries toadd it into the dependency graph. This flag is needed to work around allthat, I think.",1
Ensure that `airflow dags tests` works for mapped DAGs (#21969),2
Add compat shim for SQLAlchemy to avoid warnings (#21959),2
replace extra links value (#21971),2
"Run inclusive language check on CHANGELOG (#21980)We should run our inclusive language check on our changelog. We onlyhave a single failure when we do, ironically from when we put thecheck in place.",0
"Fix changelog/updating typos (#21979)Some of these are just British vs American spelling differences andnames, others are true typos.",2
Rename 'S3' hook name to 'Amazon S3' (#21988)PR to produce more intuitive and organized names for Amazon hooks.,1
Change the storage of frame to use threadLocal rather than Dict (#21993)There is a very probable WeakKeyDict bug in Python standardlibrary (to be confirmed and investigated further) thatmanifests itself in a very rare failure of thetest_stacktrace_on_failure_starts_with_task_execute_methodThis turned out to be related to an unexpected behaviour(and most likely a bug - to be confirmed) of WeakKeyDictwhen you have potentially two different objects with thesame `equals` and `hash` values added to the sameWeakKeyDict as keys.More info on similar report (but raised for a bit differentreason) bug in Python can be found here:https://bugs.python.org/issue44140We submitted a PR to fix the problem found https://github.com/python/cpython/pull/31685,0
Add test to run DB downgrade in the CI (#21273)This attempts to add db upgrade/downgrade test to the CI,3
"Simplify tests for CLI connections commands (#21983)The tests (particularly with the export subcommand) were doing a fair bit of unnecessary mocking. In particular they mocked builtins.open. But there's no good reason that we can't actually write out the file and read the file in the assert (which is a much better way to test this). There were also a number of assertions re splittext which are not really relevant.I suspect the reason that the author did this is because the file wasn't closed, so when you read the value from the exported file (if the process has not been exited) then the buffer may not be fully written to disk. I fix this by closing the file after writing the connections.As part of this change I also update to use current pytest techniques for parameterization and temp files.",2
"Add per-DAG delete permissions (#21938)This PR adds per-DAG delete permissions and extends the sync-perms subcommand to add delete permissions to the database for all existing DAGs (it does not, however, grant any of the new DAG delete permissions to any roles).",4
"Don't validate that Params are JSON when NOTSET (#22000)If params are NOTSET, then JSON validation will of course fail.",0
Minor cleanup on Celery config (#21880)This rewords the description and changes the type of``worker_enable_remote_control`` and sets a default for``worker_prefetch_multiplier`` instead of relying on fallback.,1
Add brief examples of integration test dags you might want (#22009),2
Upgrade `moto` library to version 3.0 (#22005)Update `moto` library to version 3.0,5
Local kubernetes executor (#19729),5
Resolve mypy issue in athena example dag (#22020)* Resolve mypy issue in athena example dag* fixup! Resolve mypy issue in athena example dag,2
Updates FTPHook provider to have test_connection (#21997)* Updates FTP provider to have test_connectionCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Add autodetect arg to external table creation in GCSToBigQueryOperator (#21944)* Added autodetect parameter in external table creation in GCSToBigQueryOperator,1
Fix typo in docs (#22024),2
"Remove back the limitation of Yandex as a bug in 0.142 is fixed (#22023)The original bug in Yandex causing test failures has beenfixed, so we are removing the upper-bound limit for it.",4
"Fix failing main after merging SFTP hook test connection (#22026)The #21997 implemented test_connection method in FTP hook, butwe were using FTP hook to actually test .... what happenswhen the test_connection method was missing :)Unfortunately the test for it was not run because the ""core""tests are skipped in case only provider change :(.This PR switches the test_connection missing test to use GRPC hook(it does not have the test_connection method) and moves the wholetest_connection.py to ""always"" category of tests that arealways executed - there are more tests there that have someassumptions on the existing connections and having the testsexecuted ""always"" makes sense to avoid similar problems in thefuture.",0
Added AWS RDS operators (#20907),1
Added AWS RDS sensors (#21231),1
Fixing bug when roles list is empty (#18590),0
added docker network_mode options (#21986),1
Removes limitations from Dask dependencies (#22017)Dask dependencies were holding us back - when it comes to upgradingsomoe of the packages (for example apache-beam and looker - in googleprovider). This PR removes the limitations but with a twist.* Dask tests stop working. We reach out to the Dask Team to fix them  but since a very old version of `distributed` library was used  the Dask team is called for help to fix those* The typing-extensions library was limited by `distributed` but it  seems that version 4.0.0+ breaks kubernetes tests,3
Move S3ToRedshiftOperator documentation to transfer dir (#21975),2
Default args type check (#21809),5
Cleanup RedshiftSQLOperator documentation (#21976),2
Switch oss hook tests in alibaba-provider to use Mocks (17617) (#21992),1
Change default python executable to python3 for docker decorator (#21973)Added support to pass python executable(python3) to Docker decorator.,2
retry on very specific eni provision failures (#22002),0
Add docs and sample dags for AWS Batch (#22010),2
"Dev: Allow easy repacking of Providers (#22018)* Dev: Allow easy repacking of ProvidersThis PR adds `--skip-tag-check` to easily repackage a provider by running:```rm -rf provider_packages/airflowcp -r airflow provider_packagesmkdir distpython dev/provider_packages/prepare_provider_packages.py generate-setup-files --skip-tag-check ""cncf.kubernetes""python dev/provider_packages/prepare_provider_packages.py build-provider-packages --skip-tag-check ""cncf.kubernetes""```This allow us to repackage providers without using Docker containers/breeze or git.",2
Fix mypy errors resulting from typing updates in marshmallow (#22044)Marshmallow library has now better typing and mypy detecteda potential problem with messages not always being dict inValidationError. Switched to normalized_messages to fix it.,0
Update limits of dependencies after `dask` test disabling (#22046)Some of the tests failed previously with typing extensions above 4.This PR attempts to relax the limit and check if the problemsstill appear.Also new tests (S3) started to fail when a new `responses` libraryversion has been released today.So this change also add limits to the responses library in orderto make sure the tests pass.Issue https://github.com/getsentry/responses/issues/511 has beenopened to raise it to `responses` library maintainers.,0
"missing quotes (#21990)Missing quotes throws the following error:```airflow:- env.0.value: Invalid type. Expected: string, given: boolean```",0
"Add information on DAG pausing/deactivation/deletion (#22025)Many of our users do not understand how DAG deactivation, deletionworks - it's quite straightforward for us, who understand howscheduler, serialization and refresh works, but we have not reallydocumented everywhere what the differences are between pausing,deactivation and actual deletion of the metadata.It has been distributed in a few places (API documentation, commentswhen you issued a ""delete"" UI action, but I found that there is nosingle place where it is described.This PR adds documentation in ""DAG"" page about it - explainingwhat happens in those different states and instructing the userto follow -> remove dag file -> wait for deactivation -> deletepattern if they want to remove a DAG.Related: #21864",2
Add Looker PDT operators (#20882),1
Update best-practices.rst (#22053),5
Fix spelling (#22054),0
Add documentation for Feb Providers release (#22056)This provider's release is pretty special as we releaseall providers with Python 3.10 support.,1
"DB upgrade is required when updating Airflow (#22061)Just strengthen the language that it is ""required"", not ""recommended"" torun `airflow db upgrade` when upgrading Airflow versions.",5
Add sample dag and doc for RedshiftToS3Operator (#22060),1
Allow for uploading metadata with GCS Hook Upload (#22058),1
Clean up the `pre-commit` config (#22052),5
Dynamo to S3 Sample DAG and Docs (#21920),2
"Add example config of sql_alchemy_connect_args (#22045)I saw a few people in slack confused about how this setting is applied. After verifying this behavior for my own deployment, I'm including it in the docs.",2
fixes query status polling logic (#21423),2
Add template fields to DynamoDBToS3Operator (#22080),5
Add new options to DatabricksCopyIntoOperator (#22076)This includes:* `encryption` - to specify encryption options for a given location* `credential` - to specify authentication options for a given location* `validate` - to control validation of schema & data,5
Store callbacks in database if standalone_dag_processor config is True. (#21731),5
Add Mapped task instance endpoint (#21965)Add an API endpoint that supplies details of a mapped task instance.,1
refactors polling logic for athena queries (#21488),2
Bug-fix GCSToS3Operator (#22071),1
Use yaml safe load (#22085),1
Improve guidelines for contributors for provider testing (#22087)The instructions on how to test providers are improvedand added to an issue template.,0
"Revert ""Use yaml safe load (#22085)"" (#22089)This reverts commit 7f4935bab36c41d5927610e38c46a30da2b80906.",4
"Fixed dask executor and tests (#22027) Fixed dask executor and tests, distributed package does not ship with tests folder and the certificates, added certificates to certs folder",1
Fix/21994 liveness probe (#22041),0
Update changelog for helm chart 1.5.0 (#22090),2
Update release date on changelog (#22094),4
Use yaml safe load (#22091),1
Pause auto-refresh when page is hidden (#21904)* pause autorefresh when page is hidden* fix tests* use react query and simplify webAPI vars* use initRefresh instead of always starting refresh,5
Add `list-import-errors` to `airflow dags` command (#22084)This will help users to see the dags with import error and enable scriptsprocess the output,0
Add guide for DataprocInstantiateInlineWorkflowTemplateOperator (#22062),5
EMR on EKS Sample DAG and Docs Update (#22095),5
Upgrade PIP to 22.0.4 (just released) (#22081),5
Add new map_index field to log message (#22098),2
"Show import error for 'airflow dags list' CLI command (#21991)When there's an import error in a dag, the dag doesn't show up in the list of dagsand there's no indication that there's an import error.This PR fixes that",0
bump sphinx-jinja (#22101),5
"If uploading task logs to S3 fails, retry once (#21981)",1
Fix typo in AWS doc (#22097),2
Fix spelling (#22107),0
bump moto version (#22099),5
Replace build image bash script  in CI with build image in python (#22008)Use Breeze2 build image python on CI,1
Rename task-mapping trigger to 'expand' (#22106),5
"Add Codespaces support (#22082)* Add Codespaces supportCodespaces is (soon to be GA) feature that allows to startdevelopment environment on a virtual machine directly from theGitHub UI or from VSCode connected with GitHub account.This PR adds configuration that allows to start codespacesenvironment (based on Breeze's docker compose environment)in the basic form that allows the users to run basic unittests.It does not replace Breeze/CI but for most users who wantto just run a few tests, make sure that all dependenciesand plugins are installed so that the users can get theVSCode autocomplete and execution of the tests possible.This is a great way to begin your journey with Airflow - whilethe Codespace machines are not very powerful and the integrationtests might require more resources, the ""no-setup""environment and no need to have powerful machine is greatfor contributors who cannot afford powerful developmentworkstations.* Update CONTRIBUTING.rstCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>* Update scripts/ci/libraries/_build_images.shCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>* Update CONTRIBUTING.rstCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",5
"Add map_index to RenderedTaskInstanceFields (#22004)In order to do this ""properly"" we have also migrated it from usingexecution_date to run_id in its PK",1
"Require SQLAlchemy 1.4 (#22114)Some of the exceptions have moved between 1.3 and 1.4(`sqlalchemy.orm.exc import MultipleResultsFound` becomes `fromsqlalchemy.exc import MultipleResultsFound`)Since we only ever test with the ""latest"" version we should update therequirement to be tighter to ease upgrades.(Without this the UI/API will fail to start)",0
Update logging-tasks.rst (#22116)Fix PYTHONPATH Environment Variable name,0
"Use ""dev"" from main in ""build-images"" workflow (#22119)",1
Allow searching/filtering Browse Task Instances view by map_index (#22117),1
Don't try to create automigration for celery tables (#22120),1
Fix RTIF test against the apply-expand (#22121),3
Add webserver PodDisruptionBudget (#21735),1
"Add support for ARM platform (#22127)This support is mostly for the developers, not for CI full chain yet.It has several limitations:* no MySQL client support* no MsSQL client support* no CI tests yetWhat is implemented:* automated detection of ARM/AMD architecture when building and  running breeze* automated cache refresh on CI for ARM/AMDCurrently only development (ghcr.io) images are supported for ARM.Fixes: #18849Fixes: #17494Relates to: #15635The images published in DockerHub for now are AMD64 only. We willrun development with M1 images for some time and later we willlikely make our DockerHub images multi-platform as well.Also Hadolint does not have ARM images yet so we had to disable itand we should re-enable it back after the support is added.See https://github.com/hadolint/hadolint/issues/411",0
Rename wrongly named script for pushing prod images (#22139),0
Switches backend to sqlite when building images (#22140)The backend is now checked at execution so we should notset it to postgres when postgres DB is not started.,5
"Fix location of --push directive for prod images (#22142)Previous #22127 had `--push` added in a wrong place.It should only be added when cache is being built and itwas added always, with resulted in authentication error aslogin has not been performed before the --push.",2
Add more template fields to `DbtCloudJobRunOperator` (#22126),5
Pass explicit overrides in `DbtCloudJobRunOperator` to `DbtCloudHook` (#22136),5
Explicitly adding read permission to Docker entrypoint. (#22135)Co-authored-by: Len Budney <Len.Budney@grantstreet.com>,1
Update cloud_composer.rst (#22124),5
Add queue button to click-on-DagRun interface. (#21555)* Initial implementation of adding Queue button to DagRun interface* Implement the test cases* FIX Add all required MyPy ignores* FIX import* Update airflow/www/views.pyFIX DocumentationCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* update modal UICo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,5
Reduce rerendering of TaskName component (#22156),5
Reduce rerendering of component (#22161)Memoize component with a custom compare function to reduce rerenders,1
"Don't try to expand mapped tasks if they can't run (#22155)This lead to the case where we tried to expand a mapped task when itsupstream was failed, but we hadn't yet set the (unexpanded) mapped TI toUPSTREAM_FAILED.In order to support this behaviour we need to add a new ""ignore"" flag toDepContext -- not the best pattern, but we should follow it for now",1
Order filenames for migrations (#22168)Sometimes you want to quickly find recent migrations.  This makes it easier,1
Bugfix for retrying on provision failuers(#22137),0
Conditional skipping of provider tag when it exists (#22172),1
Fix ordering of migration filenames (#22169),2
Use jobs check command for liveness probe check in airflow 2 (#22143)This PR removes the current use of python code for liveness probe checkcommands in the Scheduler & Triggerer deploymentsCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
additional information in the ECSOperator around support of launch_type=EXTERNAL (#22093)* additional information in the ECSOperator around support of launch_type=EXTERNAL,1
Fix RedshiftDataOperator and update doc (#22157),2
Add pip_install_options to PythonVirtualenvOperator (#22158),1
Switch unit tests for oss operator and oss sensor in alibaba provider to use mocks (#17617) (#22178),1
Add oss_task_handler into alibaba-provider and enable remote logging to OSS (#21785),2
Improve StatusBox rendering in Grid view (#22170)* reduce grid task instance rerenders* fix test,3
"Add map_index to XCom model and interface (#22112)* Add map_index to XCom primary keyThis is not actually stored correctly yet. We still need to fix the XCominterface.* Add map_index to XCom interfaceThis adds an additional (optional) map_index argument to XCom'sget/set/clear interface so mapped task instances can push to thecorrect entries, and have them pulled correctly by a downstream.To make the XCom interface easier to use for common scenarios, aconvenience method get_value is added to take a TaskInstanceKey thatautomatically performs argument unpacking and call get_one underneath.This is not done as a get_one overload to simplify the implementationand typing.",1
"Update links to new Grid/Graph view instead of relying on redirects (#22167)Recently we changed the URL strucutre for Graph and Grid (nee Tree)views, but there were still a few places that used the old names (whichstill work thanks to the redirects we have in place.)This changes the front end to use the new URLs directly.A few of the places the URL is generated/tweaked via JS, so we needed apass a ""fake"" dag_id -- `$dag_id` to the `url_for()` helper (otherwiseit would blow up) and then replace it directly in the string from JS.",2
Show DagModel details. (#21868)* Show DagModel details.* FIX Invalid extension of list* FIX Add more items to the excluded list* FIX Add more items to the excluded list,1
Remove Snowflake limits (#22181)The limits of snowflake can be removed as the conditions forall limits are fulfilled now.This is the last thing to remove before we can enable Python 3.10,0
Use relation in TI join to RTIF (#22159)Did not include map index; replaced with relation.,1
"Avoid trying to kill container when it did not succeed for Docker (#22145)When container cannot be created in Docker Operator andexception is raised, the on_kill method attempts to kill thecontainer but since it is None, it fails with another exception```    self.cli.stop(self.container['Id'])    TypeError: 'NoneType' object is not subscriptable```and the original exception is swallowed.This PR avoids stopping the container if it has not been created.",1
"Remove some really old Airflow 1.10 compatibility shims (#22187)We used some Airflow 1.10 compatibility shims in systemtests and Breeze's entrypoints, in order to be able to run1.10 still if needed, but the need for that is completely gonefor months now. Time to remove it has long been overdue.",4
"Remove extra initialization overhead for mypy/flake (#22183)The #22127 change introduced a change how execution of dockercommnds was done (due to LD_PRELOAD change) and they started touse entrypoint_ci, however this caused undesired effect of runningdatabase initialization and printing extra lines which was not neededand cluttered the output.This PR introduced SKIP_ENVIRONMENT_INITIALIZATION that (if set to true)skips the entire initialization of the entrypoint_ci",1
Prepare for Python 3.10 adding (#22075)We need to merge this change to `main` first before weenable Python 3.10 in order to make sure our build-imageworkflow is ready to build 3.10 images.,1
Remove warnings from kubernetes tests (#22190)There are warnings generated during K8S tests that there is agroup access to kubeconfig file.This change makes the kubeconfig only accessible to user.,1
Bump chart version to 1.6.0-dev (#22192)Bumping the version to 1.6.0-dev since 1.5.0 is now released,2
Move last chart test to `tests` (#22198)This test was added after the rest of the tests were moved into `tests/charts`.,3
"Update instructions of doc updates when removing providers from release (#22199)When removing providers from release it might be that we remmovejust added provider due to bugs found, in which case more detailewdinstructions on what should be removed from prepared dodcumentationis needed.Related to #22197",4
Fix typo in mongo.rst (#22211),2
"Update ``GKEDeleteClusterOperator`, ``GKECreateClusterOperator`` docstrings (#22212)updated misleading docstrings to use Kubernetes rather than Compute as this takes Kubernetes engine zone as location parameters",2
fix contributing guide image master -> main (#22216)Co-authored-by: aiden <wuai@garena.com>,0
"Make code snippet appear again (#22163)Due to some erroneous extra dots, the reStructuredText got mangled and the code snippet was no longer visible.",1
"SFTP docs add example “extras” field using ``key_file``, ``private_key`` and ``host_key`` (#20757)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",1
Support Uploading Bigger Files to Google Drive (#22179)Add `chunk_size` & `resumable` as parameters to `upload_file` methodChange the default `chunk_size` to a clear representation & fix documentation typo,2
AWS RDS integration fixes (#22125)Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>,5
Skip some tests for Databricks from running on Python 3.10 (#22221)This is a temporary measure until the Databricks SQL libraryis released in Python 3.10 - compatible version.Related to: #22220,5
Change the default `chunk_size` to a clear representation & add documentation (#22222),2
Databricks hook - retry on HTTP Status 429 as well (#21852)* Databricks hook - retry on HTTP Status 429 as wellthis fixes #21559* Reimplement retries using tenacityit's now uses exponential backoff by default,1
"Add sample dags and update doc for RedshiftClusterSensor, RedshiftPauseClusterOperator and RedshiftResumeClusterOperator (#22128)",1
Issue 20453 - Fixes the test_http and test_sheets assert calls only (#22104),3
Add REST API endpoint for bulk update of DAGs (#19758)Added endpoint for bulk update of DAGs in the airflow stable API,2
Add the new Airflow Trove Classifier to setup.cfg (#22241)We have new Trove Classifiers in PyPI for Apache Airflow:https://github.com/pypa/trove-classifiers/pull/87This PR adds it for Airflow. The next release of Providers willadd the classifiers for providers.,1
"Use variable for vote end date for helm chart release email (#22239)The current URL is auto escaped in zsh with backslashes and instead of manually editing thisurl, I propose to automate it with environment variables that are not auto escaped by zshAlso, added a link to the created issue for testing the release in the email body",3
Remove celery from intersphinx mapping temporarily (#22254),4
Fix assert_queries_count margin to be 0 by default (#22249)The margin is really only needed for more complex queries. Formost normal queries we should look at the exact value (i.e.margin should be 0 by default),3
use different logger to avoid duplicate log entry (#22256)https://github.com/apache/airflow/pull/22137#issuecomment-1067060405,0
"Protect against accidental misuse of XCom.get_value() (#22244)The XCom.get_value has been added in 2.3.0 and while there arecases it should be used in the providers when task are mapped,in order to keep compatibility with earlier versions of Airlfow,the XCom.get_value() should only be used when ti_key is not None.We check for the construct used in community providers automaticallyand also add a documentation for users who would like to usedynamic task mapping featuers in their own providers.",1
Add Dataplex operators (#20377),1
"Add vertical scrolling to grid view (#22134)* Add vertical scrolling to grid view* combine x and y scrolling* fix zindexes* comment and linting* store width as var, move side labels* lint, remove unused var",1
EMR Sample DAG and Docs Update (#22189),5
"Soften the wording in issue templates (#22200)* Soften the wording in issue templatesMaybe this is my non-native English, but I often feel a bit strangewhen reading the issues raised by our users whe they start thedescription with:* ""I expected this and that""For me - this comes from the ""What you expected to happen"" headingwe have and I have a feeling this might lead to bad wording of theissues which can lead to unnecessary tensions. ""Expectation"" isa bit too strong of a word for the Open-Source project because theremight be many reasons why the ""expectation"" is not fulfilled andthe user might not be aware, so they might ""Expect"" something ina sense of ""demanding"" it, when there are good reasons things arehow they are.In most cases people do not read the description or hint weprovided - they stop short by reading the header and base theiranswer on the header.I personally perceive that when you start ""I expected this and this"",it feels like ""This is obvious it should be like this"".I think changing the heading to ""What you think should happen""might be much better and has much less of the ""demand"" in it.Eventually it might lead to people understanding better that whatthey raise here is more of a ""wish"" rather than ""expectation"" andthat this presents their personal thinking rather than ""generalexpectation"".Making sure in the heading wording that the raised issue is a bit""softer"" might also set the tone for the follow-up conversation.* Update .github/ISSUE_TEMPLATE/airflow_bug_report.ymlCo-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>",5
"Add documentation for Classifier release for March 2022 (#22226)This is a bulk-release of all providers - it includessome providers that actually changed, but most ofthe providers are just adding the two classifiers:* Framework :: Apache Airflow* Framework :: Apache Airflow :: Provider",1
Pause autorefresh if scheduler isn't running (#22151)* pause autorefresh if scheduler isn't running* fix test* update jinja if statement,5
"Addressed some issues in the tutorial mentioned in discussion #22233 (#22236)* Streamlined the tutorial, repaired incorrect logic in the merge, and addressed some concerns in duscussion # 22233Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>",1
Add support for private key in connection for Snowflake (#22266),1
Fixed issue generation to always include last release (#22265),0
"Enhance `db upgrade` args (#22102)Make `db upgrade` args more like `db downgrade`.```usage: airflow db upgrade [-h] [--from-revision FROM_REVISION] [--from-version FROM_VERSION] [-r REVISION]                          [-s] [-n VERSION]Upgrade the schema of the metadata database. To print but not execute commands, use option ``--show-sql-only``. If using options ``--from-revision`` or ``--from-version``, you must also use ``--show-sql-only``, because if actually *running* migrations, we should only migrate from the *current* revision.optional arguments:  -h, --help            show this help message and exit  --from-revision FROM_REVISION                        (Optional) If generating sql, may supply a *from* revision  --from-version FROM_VERSION                        (Optional) If generating sql, may supply a *from* version  -r REVISION, --revision REVISION                        (Optional) The airflow revision to upgrade to. Note: must provide either `--revision` or `--version`.  -s, --show-sql-only   Don't actually run migrations; just print out sql scripts for offline migration. Required if using either `--from-version` or `--from-version`.  -n VERSION, --version VERSION                        (Optional) The airflow version to upgrade to. Note: must provide either `--revision` or `--version`.```",1
[FIX] typo doc of gcs operator (#22290),1
Set queued_dttm when submitting task to directly to executor (#22259),1
add a few more fields to the taskinstance finished log message (#22262),2
"Filter out default configs when overrides exist. (#21539)* Filter out default configs when overrides exist.When sending configs to Airflow workers we materialize a temp config file. In #18772 a feature was added so that `_cmd` generated secrets are not written to the files in some cases instead favoring maintaining the raw `_cmd` settings. Unfortunately during materializing of the configs via `as_dict()` Airflow defaults are generated and materialized as well including defaults for the non `_cmd` versions of some settings. And due to Airflow setting precedence stating bare versions of settings winning over `_cmd` versions it results in `_cmd` settings being discarded:https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.htmlThis change checks `_cmd`, env, and secrets when materializing configs via `as_dict()` so that if the bare versions of the values is exactly the same as Airflow defaults and we have ""hidden"" / special versions of these configs that are trying to be set we remove the bare versions so that the correct version can be used.Fixes: #20092Related to: #18772 #4050",0
"Fail with error when extending image with pip run as root (#22292)The production docker image installs airflow with --userflag for `airflow` user and all subsequent image extensionshould be done using `airflow` user. It is very easy however,to run `pip` as root user when you switched temporarily to theroot user for `apt` installation.This PR makes the accidental `root` user run `pip` fail witherror and redirection to the documentation where it is explainedto the users that they should use `airflow` user for `pip` withexamples.Fixes: #22250",0
"Add arguments to filter list: start_after_key, from_datetime, to_datetime, object_filter callable (#22231)Implemented as discussed in [closed PR](https://github.com/apache/airflow/pull/19018).Add more filter options to list_keys of S3Hook- `start_after_key`: should return only keys greater than this key- `from_datetime`: should return only keys with LastModified attr greater than this equal `from_datetime`.- `to_datetime`: should return only keys with LastModified attr less than this `to_datetime`.- `object_filter`: Function callable that receives the list of the S3 objects, `from_datetime` and `to_datetime` and returns the List of the matched key.Add test for the added argument to `list_keys`.closes: #16627",1
Masking extras in GET /connections/<connection> endpoint (#22227)Masking extras in GET /connections/<connection> endpoint,1
Operator for updating Databricks Repos (#22278)* initial version of ReposUpdate operator,1
Added Quick Algorithm Analytics to the list of companies using Apache Airflow (#22279),1
"Enable JSON serialization for connections (#19857)Previously in general we could only store connections in the Airflow URI format.  With this change we can serialize as JSON.  The Airflow URI format can be very tricky to work with and although we have for some time had a convenience method Connection.get_uri, using JSON is just simpler.",5
Add test to run offline SQL generation in the CI (#22177)This ensures our offline migrations command continue to work,1
add an option for run id in the ui trigger screen (#21851),2
Disable default_pool delete on web ui (#21658),4
Add generic connection type (#22310)See https://github.com/apache/airflow/discussions/20350.,1
Remove RefreshConfiguration workaround for K8s token refreshing (#20759)A workaround was added (https://github.com/apache/airflow/pull/5731) to handle the refreshing of EKS tokens.  It was necessary because of an upstream bug.  It has since been fixed (https://github.com/kubernetes-client/python-base/commit/70b78cd8488068c014b6d762a0c8d358273865b4) and released in v21.7.0 (https://github.com/kubernetes-client/python/blob/master/CHANGELOG.md#v2170).,4
Add recipe for BeamRunGoPipelineOperator (#22296),1
"Remove incorrect deprecation warning in secrets backend (#22326)When the no value is found with `get_conn_value`, the warning was being triggered, even though `get_conn_value` was implemented and just returned no value (cus there wasn't one).Now we make the logic a little tighter and only raise the dep warning when `get_conn_value` not implemented, which is what we intended to do in the first place.",1
"Stronger language about Docker Compose customizability (#22304)* Stronger language about Docker Compose customizabilityDespite our warnings, our users continue treating the DockerCompose that we exposed as something that should be easy toextend and customize for their own needs, yet they continueto struggle with some basic behaviour of containers, Docker Composeand how they interact. This results in vast space of potentialproblems as Docker Compose gives the user a false premise ofsomething that ""just works"" where it requires quite a deepunderstanding on how it works.When you get things wrong with Docker Compose, you often end upwith extremely confusing messages, that might suggest that theproblem is with Airflow, but really the problem is with how usersinteract with their custom Docker images, registries, pulling,networking, mounting volumes and plenty other things.While this is the same with Kubernetes and Helm Chart, Helm Chart makesit infinitely easier to customize in declarative way (this is whatour values.yaml does) and anything that has not been foreseen by HelmChart developers is ""hard"" by definition.Docker Compose makes no such distinction. You really can't make DockerCompose customizable by configuration, and any customization in itrequires modifying the compose file and for people who do not knowwhat they are doing will eventually lead to errors that they are notable to diagnose and leads to creation of ""Airlfow isssues"", where theyshould be brought to ""Docker Compose"" issues.Example of that is here: https://github.com/apache/airflow/discussions/22301where there are at least two issues that are not reproducible withoutknowing in detail what the user has done, how the image was buildand distributed, and how the docker-compose installation interactedwith them. This leads to a terrible distraction for supportingusers of Airflow as the issues are really Docker Compose issues andAirflow maintainers should not be involved in solving those.This PR adds a bit stronger language and statement about the scopeand customizability of the Quick Start Docker Compose of ours. Notonly mentioning ""Lack of Production Readiness"" but also theresponsibility of the user to understand and diagnose docker composeerrors on their own and setting expectations that issues with DockerCompose running should be directed elsewhere.* Update docs/apache-airflow/start/docker.rst* Update docs/apache-airflow/start/docker.rstCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>* Update docs/apache-airflow/start/docker.rstCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>",1
adds ability to pass config params to postgres operator (#21551),1
Add back celery intersphinx mapping (#22370)We had disabled this previously in (#22254) but now the website is up on a different domain as listed in https://github.com/celery/celeryproject/issues/51#issuecomment-1072248499,0
"Fix broken links to celery documentation (#22364)The celery documentation have been moved from https://docs.celeryproject.org/ to https://docs.celeryq.dev/. The old links now refer to a 404 error page, the new links to the actual documentation.",2
"Patch sql_alchemy_conn if old postgres scheme used (#22333)Prior to SqlAlchemy 1.4 the correct scheme for postgres was `postgres+psycopg2` but as of 1.4 it is `postgresql`.  Airflow 2.3 updates SqlAlchemy to 1.4 so unless we patch the config for users (or they update their URIs), upgrading to 2.3 will break.",4
fix deprecation warning in test_default_views.py (#22346),3
Add dataflow_default_options to templated_fields (#22367),5
"Update sample dag and doc for S3CreateBucketOperator, S3PutBucketTaggingOperator, S3GetBucketTaggingOperator, S3DeleteBucketTaggingOperator, S3DeleteBucketOperator (#22312)",4
Correctly handle task_id mangling during unmapping (#22355),0
"Reduce DB load incurred by Stale DAG deactivation (#21399)Deactivating stale DAGs periodically in bulkBy moving this logic into the DagFileProcessorManager and running it across all processed file periodically, we can prevent the use of un-indexed queries.The basic logic is that we can look at the last processed time of a file (for a given processor) and compare that to the last_parsed_time of an entry in the dag table. If the file has been processed significantly more recently than the DAG has been updated, then its safe to assume that the DAG is missing and can be marked inactive.",2
Add documentation on specifying a DB schema. (#22347)* Add documentation on specifying a DB schema.From request - https://github.com/apache/airflow/issues/17374#issuecomment-1060019956Co-authored-by: Nick Shook <nick.shook@apple.com>,1
Add description on the vendoring process we use (#22204)We need to vendor in cgroupspy library in order to make Airflowcompatible with Python 3.10 (see #22050) so this is the right timeto make our vendoring process more organized.I based in parts on the readme described by `bleach` package.,1
"Add cgroupspy to _vendor folder (#22206)This is just importing existing cgroupspy library withoutcode modifications. In the next step we will modify the cgroupspycode to work from the new location, then we will fix it toimplement Python 3.10 compatibility and finally we will changeairflow to use the vendored package instead of the originalpackage.This is part of the effort needed to implement Python 3.10compatibility: #22050",1
Expose try_number in airflow vars (#22297),1
Fix skipping non-GCS located jars (#22302)* Fix #21989 indentation. A test is added to confirm job is executed on DataFlow with local jar file.Co-authored-by: Kyaw <kyawtuns@gmail.com>,2
Apply import fixes to vendored cgroupspy library (#22207)This commit applies import fixes to vendored cgroupspy libraryso that the library uses the vendored version internally.This commit will need to be re-applied if we upgrade thevendored cgroupspy library.This is part of the effort needed to implement Python 3.10compatibility: #22050,1
Fix mistakenly added install_requires for all providers (#22382)The TroveClassifiers change #22226 - by mistake - added thegitpython and wheel for all providers.This could have been avoided (and noticed) if we split the changefrom doc generation. So as a learning I separate a fix to onlyfix the problem.Fix #22380,0
Fix python 3.10 support in vendored in cgroupspy (#22208)The cgrouppspy is not Python 3.10 compliant due to Iterablebeing imported directly from collections.This is captured in https://github.com/cloudsigma/cgroupspy/issues/13We fix it in our vendored-in version of cgroupspy untilhopefully new version of it is released.This is part of the effort needed to implement Python 3.10compatibility: #22050,1
Add map_index support to all task instance-related views (#22272)Co-authored-by: Ash Berlin-Taylor <ash@apache.org>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,1
Add fk between xcom and task instance (#22334),1
Make date picker label visible in trigger dag view (#22379),2
Add timeout and retry to the BigQueryInsertJobOperator (#22395),1
Add `LocalFilesystemToGoogleDriveOperator` (#22219)*  Add `LocalFilesystemToGoogleDriveOperator`Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Tomek Urbaszek <turbaszek@gmail.com>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>,1
Update airflow_helmchart_bug_report.yml (#22400),5
Check if there are instances before adding a count (#22399),1
Fix grid+calendar page titles (#22401),0
add issue template update to helm chart release doc (#22402),2
Update doc and sample dag for S3ToSFTPOperator and SFTPToS3Operator (#22313),1
Correct `multiple_outputs` param descriptions mentioning lists/tuples (#22371),2
Remove pandas upper limit now that SQLA is 1.4+ (#22162),4
Update base sensor operator to support XCOM return value (#20656)Co-authored-by: mingshi <mingshi.wang@coinbase.com>,1
[FEATURE] add 1.22 1.23 K8S support (#21902),1
adding `on_execute_callback` to callbacks docs (#22362)* adding on_execute_callback`on_execute_callback` is not listed but is an available callback to use. Could not find a docs page to link the way that the rest of them have.* Update docs/apache-airflow/logging-monitoring/callbacks.rstCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update docs/apache-airflow/logging-monitoring/callbacks.rstCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Switched cgroupspy to vendored version (#22209)This is part of the effort needed to implement Python 3.10compatibility: #22050,5
Add docs and example dag for AWS Glue (#22295),2
"Add Python 3.10 support (#22050)Python 3.10 support has been long missing because a number of ourdependencies had problems with it. It seems that last problemsremaining should be fixed now, and we should be able to geta proper Python 3.10 support.Closes: #19059",1
Add default connection for redshift (#22263),1
"Limit Docutils to make our documentation pretty again (#22420)Docutils 0.17.0 which was upgraded during sphinx-jinja upgradebreaks our documentation by converting `<div class=""section"">` into`<section>`. This in turn adds a lot of whitespace separation andmake our documentation next to unusable.Until we fix it in our documentation we limit it to be < 0.17.0.",2
Fix incorrect datetime details (DagRun views) (#21357),2
Add `extraVolumeMounts` to flower (#22414),1
Allow to except_skip None on BranchPythonOperator (#20411)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
"ImapAttachmentToS3Operator: fix it, update sample dag and update doc (#22351)",2
"Fix Tasks getting stuck in scheduled state (#19747)The scheduler_job can get stuck in a state, where it is not able to queue new tasks. It will get out of this state on its own, but the time taken depends on the runtime of current tasks - this could be several hours or even days.If the scheduler can't queue any tasks because of different concurrency limits (per pool, dag or task), then on next iterations of the scheduler loop it will try to queue the same tasks. Meanwhile there could be some scheduled tasks with lower priority_weight that could be queued, but they will remain waiting.The proposed solution is to keep track of dag and task ids, that are concurrecy limited and then repeat the query with these dags and tasks filtered out.Co-authored-by: Tanel Kiis <tanel.kiis@reach-u.com>",2
Increase docker compose test wait time (#22476),3
"Fix ""run_id"" k8s and elasticsearch compatibility with Airflow 2.1 (#22385)The execution_date -> run_id change (#21960) attempted to make itAirflow 2.1 backwards-compatible, but the problem is that inAirflo2 2.1 retrieving `run_id` attribute of TaskInstance throwsAttributeError rather than returns None. It turns out that whenyou have a field defined in an ORM model, it will never throwAtributeError (even if you delete the attribute it will returnNone.Accesising `run_id` with getattr raisesAttributeError in Airflow 2.1 (because there TaskInstance has norun_id defined).This PR adds automated pre-commit to check if other providershave not suffered (and will not suffer) the same problem.",0
Update version added for `deactivate_stale_dags_interval` config (#22478),5
Add documentation for bugfix release of Providers (#22383)This is another out-of-band release of Providers after wefound out that the #22226 wrongly added gitpython and wheelas dependencies for all providers.,1
Allow migration jobs and init containers to be optional (#22195),5
"Optionally not follow logs in KPO pod_manager (#22412)When writing an async KPO, you want to be able to read logs up to the current moment and exit (i.e. and not follow the logs).  Additionally you want to be able to resume from a particular moment in time.  That's what this PR enables.",0
Only recursively check XComArg if arg is operator (#22359),1
Fix to `CloudBuildRunBuildTriggerOperator` fails to find build id. (#22419)* Fix CloudBuildRunBuildTriggerOperator: 'property' object has no attribute 'build' #22398,5
Fix `download_media` url in `GoogleDisplayVideo360SDFtoGCSOperator` (#22479),1
Dev: Update K8s-KIND version to 0.12.0 (#22424)using the default K8S image versions of the KIND releasehttps://github.com/kubernetes-sigs/kind/releases/tag/v0.12.0,1
"Add DOCKER_CONTEXT_FILES arg to PROD image build (#22494)In order to build PROD image with inlined scripts, we need to mergea change to `main` to pass DOCKER_CONTEXT_FILES arg as parameter.Related to #22492",2
Fix the docstrings (#22497)I think PubSubHook is using gRPC but not REST.,1
Add tool to bulk-create issues. (#22462),0
Add `JenkinsBuildSensor` (#22421)* Add `JenkinsBuildSensor`Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
update smart sensor docs and minor fix on is_smart_sensor_compatible() (#22386),0
Fix spelling (#22486),0
Remove coerce_datetime usage from GCSTimeSpanFileTransformOperator (#22501),2
Bring back python 3.6 possibility for Airlfow 2.2.5 builds (#22511),5
Add allowed 3.6 version for 2.2.5 release (#22513),1
"Add map_index and run_id to TaskFail (#22260)TaskFail entities always belong to a TaskInstance.  The PK for TaskInstance has changed, so we need to update TaskFail to have the new columns.",1
Update secrets backends to use get_conn_value instead of get_conn_uri (#22348)In #19857 we enabled storing connections as JSON instead of URI and renamed get_conn_uri to get_conn_value to be consistent with this change.  The method get_conn_uri is now deprecated and should warn when used.,1
Fix failing Breeze2 tests after adding 3.6 version in main (#22514),1
"Make sure finalizers are not skipped during exception handling (#22475)There was a bug in our fork handling that prevented finalizerfor objects kept in frame of unhandled excepion. Previously,the os._exit() was called in finally clause, and if there wasan unhandled exception raised, the exception was still kept inmemory while finally clause was processed together with theobjects in the same stack frame as the thrown exception thatcould lead to those objects not being finalized.This is mitigated by moving the os._exit() outside of the finallyclause.Fixes: #22404",0
Pass X-Presto-Client-Info in presto hook (#22416),1
Remove back 3.6 to unblock PRs (#22516),4
Add ignore_first_depends_on_past for scheduled jobs (#22491),1
Disable connection pool for celery worker (#22493),1
"Fix mocking the right method in secret backend test (#22524)The #22348 introduced a change on how connections are retrievedfrom secret backends, but one of the tests has not beenchanged to follow.This fixes failing main.",0
Use logger to print message during task execution. (#22488),2
New design of system tests (#22311)Migrate BigQuery system tests to new design (See AIP-47 for details),1
"Replace timedelta.max with year long timdelta in test_manager (#22527)Timedelta.max used in tests is not realistic and in somecircumstances, when it is added to date, it might causedate OverflowError. Using long (but not 999999999 days long)timedelta solves the problem.",0
Check and disallow a relative path for sqlite (#22530),1
"Update black precommit (#22521)Use latest version of black, drop py36, and add py310.",1
Add check for coerce_datetime to Airflow 2-1 compatibility check (#22499)Checking coerce_datetime presence (and also consolidating all2.1 checks in a single pre-commit check).,5
Added support to override auth_type in auth_file in pgbouncer helm configuration (#21999)* Move auth_type and auth_file to values for pgbouncer* [19654] Added tests for auth_type and auth_file in pgbouncer configuration* Update chart/values.schema.jsonCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
"Optimize direct push workflows in GitHub Actions (#22542)When the build is run via direct push to apache airflow repowe do not need to run two separate workflows. The ""push"" workflowis never a ""pull request from fork"" so it should havethe capability to build and push images to registry.This allows the committers to make direct push requests to run PRsthat are actually running the build without having to mergebuild-image.yml first.This is cool because committers can simply push a branch to apacheand test if it works with some build image changes that otherwisewould require to push to `main` of an apache-airflow fork.Another advantage is that merge and schedule builds do not run twoseparate workflows - both building the image and running tests is donein the same workflow (and the build-image workflow is not started)This saves some build time on ""wait for CI images"" and""wait for PROD images"" jobs - because in merge builds andschedule builds they only start after the images are actuallybuilt.",7
Pass X-Trino-Client-Info in trino hook (#22535),1
PostgresToGoogleCloudStorageOperator - BigQuery schema type for time zone naive fields (#22536)* Change default type for BigQuery for timezone naive objects,4
Add Notifications of build failures (#22552)There is new feature to receive notifications about build failuresfor git repos. It is supposed to only send info about changedstatus -> failed/succeeded or succeeded-> failed transition.Trying it out.,1
Add doc and sample dag for S3ToFTPOperator and FTPToS3Operator (#22534),1
SalesforceToS3Operator: update sample dag and doc (#22489),2
"Create Endpoint and Model Service, Batch Prediction and Hyperparameter Tuning Jobs operators for Vertex AI service (#22088)",1
Remove references to deprecated operators/params in PubSub operators (#22519),1
"Converts Dockerfiles to be standalone (#22492)This change is one of the biggest optimizations to the Dockerfilesthat from the very beginning was a goal, but it has been enabledby switching to buildkit and recent relase of support forthe 1.4 dockerfile syntax. This syntax introduced two features:* heredocs* links for COPY commandsBoth changes allows to solve multiple problems:* COPY for build scripts suffer from permission problems. Depending  on umask setting of the host, the scripts could have different  group permissions and invalidate docker cache. Inlining the  scripts (automatically by pre-commit) gets rid of the problem  completely* COPY --link allows to optimize and parallelize builds for  Dockerfile.ci embedded source code. This should speed up  not only building the images locally but also it will allow  to use more efficiently cache for the CI builds (in case no  source code change, the builds will use pre-cached layers from  the cache more efficiently (and in parallel)* The PROD Dockerfile is now completely standalone. You do not  need to have any folders or files to build Airlfow image. At  the same time the versatility and support for multiple ways  on how you can build the image (as described in  https://airflow.apache.org/docs/docker-stack/build.html is  maintained (this was a goal from the very beginning of the  PROD Dockerfile but it was not easily achievable - heredocs  allow to inline scripts that are used for the build and the  pre-commits will make sure that there is one source of truth  and nicely editable scripts for both PROD and CI Dockerfile.The last point is really cool, because it allows our users tobuild custom dockerfiles without checking out the code ofAirflow, it is enough to download the latest releasedDockerfile and they can easily build the image.Overall - this change will vastly optimize build speed forboth PROD and CI images in multiple scenarios.",4
"Optimize Multiplatform cache builds (#22258)This PR starts an ARM EC2 instance and forwards socket via SSHso that it can be used by docker buildx build with airflow_cachemulti-platform builders.The instance is created only when ""main"" build reaches the cachebuild job/step. The instances run for maximum 50 minutes andthen self-terminate, also the instance is killed when the jobeither succeeds or fails.",0
"Fix entire DAG stops when one task has end_date (#20920)related #19917 , #20471",5
Fix bugs about timezone change (#22525)* Fix typo* Use currentTarget,1
GoogleApiToS3Operator: update sample dag and doc (#22507),2
Add doc and example dag for AWS CloudFormation Operators (#22533),1
Refactor: BigQuery to GCS Operator (#22506),1
More operators for Databricks Repos (#22422),5
"Add max line length setting to .editorconfig (#22540)Right now, there is no explicit settings for max line width, so oftenerror is only detected when pre-commit hook is executed.  Addingexplicit setting helps to mitigate this issue earlier",0
Add a link to Databricks Job Run (#22541)It will be easier for users/admins to go to the specific run ofDatabricks Job,5
Issue 20453 google common cloud fixes part 1 (#22213),0
PowerShell Remoting fail on non-zero exitcode (#22503),0
Fix indentation of the new notification entry (#22567),1
Fixed sequence of starting an ARM instance (#22564)The ARM instance should be started before cache build :facepalm:The #22258 introduced optimized cache builds but the sequenceof steps was wrong :(,0
"Use Airflow.Base.metadata in FAB models (#22353)Since FAB models are now in airflow, it makes sense to monitor changesin them. Therefore we use Airflow.models.base.Base.metadata for FAB models",5
"Stop crashing when empty logs are received from kubernetes client (#22566)It seems that in some circumstances, the K8S client might returnempty logs even if ""timestamps"" options is specified.That should not happen in general, but apparently it does insome cases and leads to task being killed.Rather than killing the tasks we should log it as an error(on top of trying to find out why and preventing it fromhappening - also to be able to gather more information anddiagnosis on when it happens).Related to: #21605",5
Add timeout parameter to `DockerOperator` (#22502),2
enable optional subPath for dags volume mount (#22323),2
Fix very strange (likely a bug) error in mypy (#22581)Mypy Started to return a strange error in one of the scripts:```dev/breeze/src/airflow_ci/find_newer_dependencies.py:142: error: <nothing> notcallable  [misc]        main()        ^```This is likely a bug.,0
Fix caching for image builds (#22568)It turns out that the --link breakes remote caching strategyfor multi-staging builds. From test it seems thatthis strategy works best:* inline scripts in separate stage* removal of --link from COPY steps* keeping registry cache type* adding mode=max for cacheCombination of those two seems to work best to makecaching works.,1
Update check-health.rst (#22372)* Update check-health.rstCorrecting URL to flower (Celery worker monitoring tool) documentation* Update docs/apache-airflow/logging-monitoring/check-health.rstCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Jarek Potiuk <jarek@potiuk.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
add newer_than parameter to SFTP sensor (#21655) (#22377)Co-authored-by: paslawsk <Adampaslawski@gmail.com>,2
Switch azure provider to azure-keyvault-secrets (#22557)The only dependency actually used in the microsoft-azure providerappears to be `azure-keyvault-secrets`.  This merge updates theprovider to depend only on that package and not the full`azure-keyvault` metapackage.  This has the potential addedbenefits of allowing that package to be constrained morespecifically and also removing dependence on a metapackage thatis causing problem for packagers (on conda-forge in specific).,0
More explicit messages for pools and exceptions (#22569),5
Add production Build Image in the new Breeze (#21956),1
"Use timetable to generate planned days for current year (#22055)* Use timetable to generate planned days for current year* Slightly bigger and less bright dot, add legend",1
"Suppress import errors for providers from sources (#22579)When we are running airflow locally with providers installed from sources, often many providers will be discovered which we haven't installed the deps for.  This generally results in a very large amount of traceback logging, which has a very negative effect on usefulness of terminal output.  Here we suppress this error logging for providers that are installed from sources.",1
"Update regexp classifying changes for providers (#22593)The regexp we had classified airflow/providers_manager asprovider change which prevented the test running when only thatfile was modified. This change fixes the regexp to onlycount files in ""providers"" subdirectories as provider changes.",4
Specify Alembic revision to help text for db upgrade (#22577),5
Fix type of upgrade_to_newer_dependencies parameter (#22597),2
"Update our approach for executor-bound dependencies (#22573)Kubernetes and Celery are both providers and part of the core.The dependencies for both are added via ""extras"" which makes them""soft"" limits and in case of serious dependency bumps this mightend up with a mess (as we experienced with bumping min K8Slibrary version from 11.0.0 to 22.* (resulting in yanking 4versions of `cncf.kubernetes` provider.After this learning, we approach K8S and Celery dependencies a bitdifferently than any other dependencies.* for Celery and K8S (and Dask but this is rather an afterhought)  we do not strip-off the dependencies from the extra (so for  example [cncf.kubernetes] extra will have dependencies on  both 'apache-airflow-providers-cncf-kubernetes' as well as  directly on kubernetes library* We add upper-bound limits for both Celery and Kubernetes to prevent  from accidental upgrades. Both Celery and Kubernetes Python library  follow SemVer, and they are crucial components of Airlfow so they  both squarely fit our ""do not upper-bound"" exceptions.* We also add a rule that whenever dependency upper-bound limit is  raised, we should also make sure that additional testing is done  and appropriate `apache-airflow` lower-bound limit is added for  the `apache-airflow-providers-cncf-kubernetes` and  `apache-airflow-providers-celery` providers.As part of this change we also had to fix two issues:* the image was needlesly rebuilt during constraint generation as  we already have the image and we even warn that it should  be built before we run constraint generation* after this change, the currently released, unyanked cncf.kubernetes  provider cannot be installed with airflow, because it has  conflicting requirements for kubernetes library (provider has  <11 and airflow has > 22.7). Therefore during constraint  generation with PyPI providers we install providers from PyPI, we  explicitly install the yanked 3.1.2 version. This should be  removed after we release the next K8S provider version.That should protect our users in all scenarios where they mightunknowingly attempt to upgrade Kubernetes or Celery to incompatibleversion.Related to: #22560, #21727",1
"Fix providers manager tests (#22592)PR #22579 suppressed provider import log warnings when running from sources. For whatever reason, CI didn't run test_providers_manager.py so I didn't catch that changes were needed there.",4
Fix black version to support click 8.0.0 (#22598)The new click causes black to fail,0
Update FAB to latest released from 3.4 line (3.4.5) (#22596)We checked that the changes introduced between 3.4.4 and 3.4.5do not require from us to change the vendored-in security manager.,4
"Log traceback only on ``DEBUG`` for KPO logs read interruption (#22595)Logging the traceback after every disconnect is a little overly scary when this is an expected occurrence for long-running pods, and this can create false alarm for users.  Here we reduce noise a bit while allowing users to troubleshoot if desired by changing the log level to debug.",0
"Fix typo in the Pull Request ""build image"" if. (#22585)In case Pull request is run in the ""apache/airflow"" repository,image building happens in the ci.yml workflow as of #22542.However there was a typo in the build-image workflow that made theimages build in both - BuildImage and CI workflow.",1
Doc: Update description for executor-bound dependencies (#22601)These suggestions were missed in https://github.com/apache/airflow/pull/22573- https://github.com/apache/airflow/pull/22573/files#r837754616- https://github.com/apache/airflow/pull/22573/files#r837755144- https://github.com/apache/airflow/pull/22573/files#r837755355,2
Doc: Explicitly specify Py 3.10 will be supported from Airflow 2.3.0 (#22602)Based on the feedback in https://github.com/apache/airflow/issues/19059#issuecomment-1080597936 -- this PR makes it explicit,1
Fix grammar in issue templates (#22611),0
Fix link to generating constraints (#22615),2
"Final fix for image caching (#22618)This is - I hope - final fix for our image caching. I fought withit for quite a while and it turned out that I was fighting withbuidkit bug when preparing a multiplatform image.I think I finally got it under control, the main problem was thatwhen we prepared multi-platform image, seems like the image thatwas prepared ""last"" was overriding the cache that was preparedby the other platform build. This cause numerous problems withinvestigating it, because sometimes it looked like it worked,when the ""other"" platform was faster to build and someties itdid not when it was slower. It almost drove me mad.It looks like however, that the inline cache works better inthis case (and I got it repetetively working) so finally we mightgo back to faster builds on CI and fast pull for ./breeze.I also opened an issue at buildkit, hoping that they will be ableto fix it:https://github.com/moby/buildkit/issues/2758",0
Fix failing static check (#22621),0
Update modal buttons for mapped task instances (#22578)* Fix task_type typo* Update mapped task instance modal buttons* remove extra runId var,1
Fix failing exec in Breeze (#22623),0
Fix /rendered-templates for mapped operator (#22396),1
Rewrite ti.xcom_pull() to consider map indexes (#22609),5
Bump black version for other pre-commit hooks (#22599)This PR/commit bumps the black version used in other pre-commit hooks as a follow up of https://github.com/apache/airflow/pull/22598,1
Fix: Add extra headers to all livy API requests instead of only to post_batch (#22510),1
"Use ""terminate-instances"" instead of ""stop-instances"" (#22636)The spot instances need to be terminated and cannot be stopped.",1
Do not log the hook connection details even at DEBUG level (#22627),0
Add doc for LocalFilesystemToS3Operator (#22574),5
"Add more fields to REST API get DAG(dags/dag_id) endpoint (#22637)The DagModel columns have increased since this endpoint was created. This PR improves theendpoint by including all the missing fields of the DagModel on the endpoint.This update also touched on DAGDetails schema because it inherits from DAGSchema.In a future PR, when more details would be added to the DAGDetails endpoint, we couldseparate it from the DAGSchema. They are related but not really the same. One is adatabase object while the other is not",5
Set `webhook_endpoint` as templated field in `DiscordWebhookOperator`(#22570),1
"Further improves image caching for Breeze (#22625)In order to allow ""big"" rebuilds we remove the image tagbefore rebuilding, to make sure that the remote and not local imageis used as source of cache. This is consequence of using inlinedcache.Files are added to the Python Base image image - the baselayers will be shared with other stages but we will make surethat the script stages are different for different platforms.Also - when there is no image at all we fail pre-commit. Thisshould handle the situation when we tried to build the image andstopped it in-between.Hadolint released a new version of their checker - with supportfor the new Dockerfile buildkit features and native support forARM so we are enabling it back.See https://github.com/hadolint/hadolint/pull/803Finally we still need some files that we cannot inline, becausethey are only needed for source build and they are too longto inline (yarn.lock for example). In order to keep the cacheworking for all umasks we need to bring back group fixing.",0
Add API endpoint to list mapped task instances (#22341)* Refactor to remove unnecessary duplication* Add API endpoint to get mapped task instances* Add search parameters* Add ordering by map_index or state* Properly test for empty results vs 404* Reminder for the 3 states of map_index==-1,3
Better description for UPGRADE_TO_NEWER_DEPENDENCIES parameter (#22644),2
adding check system test in breeze2 (#22645),3
removed unnecessary dependence on bcrypt (#22498)Co-authored-by: Matt Rixman <MatrixManAtYrService@users.noreply.github.com>,1
Only hide rendered button for mapped summaries (#22633),5
"Fix passing string to upgrade-to-newer-dependencies (#22649)This change fixes passing upgrade-to-newer-dependenciesparameter as string. The previous fix (#22597) did not actuallyfix handling of the parameter passed and ""false"" passed as stringwas considered as ""true"" :)This should speed UP most CI builds immensely.",4
"If k8s is not installed, add if condition to exclude DAG(local_kubernetes_executor) (#22556)",2
Make sure build-prod-image works in the new Breeze2 (#22660)Some defaults were missing for manual build of PROD images.,1
Fixing task status for non-running and non-committed tasks  (#22410),1
Add doc and sample dag for MongoToS3Operator (#22575),1
Refactor `DatabricksJobRunLink` to not create ad hoc TaskInstances (#22571),1
"Fixes ScheduleInterval spec (#22635)In the schedule interval, the data can actually match more than one schema and the `__type` should decide which type of the interval it actually matches. Fixes https://github.com/apache/airflow-client-go/issues/20",0
Add links for BigQuery Data Transfer (#22280),5
"Consistent DB upgrade/downgrade arguments (#22537)This is a follow up to #22102, and be forewarned, this might be a bikeshed. If this gets contentious at all, I'll just close it and move on.I think it's a little bit easier for users to have consistent flags/arguments for the `airflow db upgrade` and `airflow db downgrade` commands. This PR just tweaks the argument processing to expect `--to-revision` and `--to-version` instead of `--revision` and `--version`, respectively.That change makes the arguments to those commands more consistent with the `--from-revision` and `--from-version` arguments. Doing so also avoids overloading the `--version` flag, which is usually a flag that prints out the version information of the command itself (eg: Airflow's version, which is available via `airflow version`).An argument against this change is that the `--to-...` arguments can be understood to be implied, like this:```bashairflow db upgrade --from-version 10.15.8  # Upgrade from 10.15.8 to the current Airflow version```and this means that you do not necessarily need to always specify the `--to-...` arguments. By having both `--to-` and `--from-` arguments, users might think that they always need to specify both a `--to-` and `--from-` argument.I also fixed an unrelated grammar typo, which corrects the grammar used to log the operation.",2
Adds HiveToDynamoDB Transfer Sample DAG and Docs (#22517),2
Fix file permissions in all build commands (#22656)Moves group permission cleanup to build command in new Breeze2.Seems that we should fix group permissions in all build commands.That will shave extra few minutes on most builds where theymodify sources. Also the output from the build will be a bitcleaner by removing come of the console outputs.,4
Add doc and sample dag for SqlToS3Operator (#22603)* Add doc and sample dag for SqlToS3Operator* Update doc,2
Remove dag parsing from db init command (#22531),5
"Add details drawer to Grid View (#22123)* make UI and tree work with mapped tasksbasic slide drawerreformat grid background colorsimprove rendering and add selected dag runfix hover and extra propswitch from drawer to details sectionadd tooltip info to detailsuse APImake side panel collapsible, useTasks,dag run actionsdag run actions w/ react-querytask instance linkstask actionsremove modalsadjust panel width and use status colorminor details stylingadd duration to tooltipsadd last scheduling decision and fix tests* move ref and selection to providers* fix test with mock providers* update TI and DR buttons* download logs and external logs* add extra links to TI details* download log bug fixes* fix extra links, hide local TZ if UTC,* confirm mark task failed/success* Update confirm modals for runs and tasks- async/await on mutations instead of useeffect- add confirmation for run actions* Fix dialog scrolling* Code cleanup and fix task clear* Fix task/run label, dialog focus, dag details overflow, panel open/close* Add timezone provider* Fix TimezoneEvent import* Improve button UX- Remove details panel title- Add button to reset root- Make ""More Details"" buttons more specific- Specify timezone as DAG timezone* autorefresh dag run details* auto-refresh task instance details* revert useTreeData changesNone of these changes were relevant to this PR. Better to be done separately.* Address PR feedback- useState vs useDisclosure- Remove extraneous elements- Copy changes- Wire up params for runTask- Breadcrumb padding* Handle task/run action sideeffects by separating autorefresh and treeData hooks* Clean up views.py endpoints- Pass 'Accept' headers for json returns- Consolidate more endpoints to return json or redirect* pass request as arg* remove request as arg* Anticipate when the 'Accept' header is not present* Fix argument count errors* Replace hard coded urls* Replace hard coded urls in react components* Update filter upstream link* Split TaskInstance details component* Fix undefined variables in tests* init_api_connexion in tests- add readme-  rename context providers to avoid confusion with Airflow Providers* Fix url params, hide last item breadcrumb links* Update task run failed copy* Fix taskinstance/list buttonsCo-authored-by: Tzu-ping Chung <tp@astronomer.io>",0
Add waiting for ARM instance docker connection (#22672)One of the recent rebases lost waiting for the connection on SSH,2
Standardize the `pre-commit` config (#22674),5
Add possibility to create users in LDAP mode (#22619)* Add possibility to create users in LDAP mode,1
Prepare Build Cache in Breeze2 (#22344),5
"Reset warnings.showwarning on interpreter shutdown (#22677)Since our custom showwarning hook lazy-imports Rich, it may not be ableto execute correctly if a warning is emitted after the interpreter hasalready cleaned up the import system. This adds an atexit hook torestore the original showwarning hook, so that warnings emitted afterthe clean-up phase are logged plainly without using additional modules.",1
Refine 'task not mapped' warning (#22678),2
Standardize the `pre-commit` config (#22686)Minor sorting of files/folder/terms,2
allow annotations on helm dag pvc (#22261),2
Make grid view aware of DAG pause/unpause (#22690)* Grid view responds to pausing a DAG* Fix tests,3
Fix StatsD casing and helm chart StatsD comments (#22610),2
Add dag-processor cli command (#22305),2
Revert Handle stuck queued tasks in Celery for db backend (#21556)Revert the stuck queued task solution,4
Fix typo (#22706),2
"Changed self-destruct log level to ERROR (#22703)The log level was set to `warning` but it raises and `AirflowException`. I believe it should be `error` level. Code snippet:```pythonif ti.state == State.RUNNING:            fqdn = get_hostname()            same_hostname = fqdn == ti.hostname            if not same_hostname:    -->          self.log.warning(                    ""The recorded hostname %s does not match this instance's hostname %s"",                    ti.hostname,                    fqdn,                )                raise AirflowException(""Hostname of job runner does not match"")```",1
support for continue backfill on failures (#22697),0
Add jenkins connection doc (#22682)* add jenkins connection doc* adding doc to menuhttps://github.com/apache/airflow/pull/22682#issuecomment-1085728836,0
Adding ArangoDB Provider (#22548)* Adding ArangoDB Provider,1
"Fix Breeze2 autocomplete (#22695)Breeze2 autocomplete did not work because we were using some oldway of adding it (via click-complete). Since then click hasnative (and very well working) autocomplete support without anyexternal dependencies needed. It cannnot automatically generatethe completions but it is not needed either, because we canstore generated completion scripts in our repo.We also move some imports to local and catch rich_click importerror to minimize dependencies needed to get autocompleteworking. Setup-autocomplete install (and upgrade if neededclick in case it needs to be used by autocomplete script.Fixes: #21164",0
Builders for ARM images should be cleaned before/after for reuse (#22694),1
"Prepare Breeze2 for prime time :) (#22713)This is a review and clean-up for all the parameters andcommands for Breeze2 in order to prepare it for beingused by the contribugors.There are various small fixes here and there, removalof duplicated code, refactoring and moving code aroundas well as cleanup and review all the parameters usedfor all implemented commands.The parameters, default values and their behaviours wereupdated to match ""new"" life of Breeze rather than oldone.Some improvements are made to the autocomplete andclick help messages printed.  Full list of choices isalways displayed, parameters are groups according totheir target audience, and they were sorted accordingto importance and frequency of use.Various messages have been colourised according to theirmeaning - warnings as yellow, errors as red andinformational messages as bright_blue.The `dry-run` option has been added to just show whatwould have been run without actually running somepotentially ""write"" commands (read commands are stillexecuted) so that you can easily verify and manuallycopy and execute the commands with option to modifythem before. The `dry_run` and `verbose` options arenow used for all commands.The ""main"" command now runs ""shell"" by default similarlyas the original Breeze.All ""shortcut"" parameters have been standardized - i.ecommon options (verbose/dry run/help) have one and allcommon flags that are likely to be used often have anassigned shortcute.The ""stop"" and ""cleanup"" command have been addedas they are necessary for average user to complete theregular usage cycle.Documentation for all the important methods have beenupdated.",5
"Cleanup of build-image scripts (#22687)This PR re-arranges tags int GitHub actions to be more explicit inthe pull/build/push steps on which tag of the image is currentlypulled/pushed/built.Previously the actual configuration of those steps was determinedby combining ""top"", ""job"" and ""step"" variables, but that gavelittle insight on what combinations of evn variables was used inthe step eventually. After this change, in all the pull/push/buildsteps you can see what environment is used for that build.Also, since we often build and push and then pull the same image inthe same workflow, the PUSH tag was sometimes used in PULLvariable as input. We change it now by introducing single`IMAGE_TAG_FOR_THE_BUILD` which will be used as PUSH and PULLtag as needed.This one also fixes failure of PROD cache builds which did notpull the right CI image.",0
extra condition to check returned object not None (#22608),5
Print configuration on scheduler startup. (#22588),5
2.2.5 has been released (#22722),5
retry commit on MySQL deadlocks during backfill (#22696),1
Remove references to `rbac = True` from docs (#22725),2
Fix starting dag processor when running as a daemon (#22720)+ move reading [scheduler]standalone_dag_processor outside of the loopSeehttps://github.com/apache/airflow/pull/22305#discussion_r841004257,2
Add autodetect arg in BQCreateExternalTable Operator (#22710)* Add autodetect parameter* Update docstring* Update google provider documentation,2
"Properly setup Breeze2 completion in bash if it is first completion (#22726)In case you've never setup bash completion in Bash you'd missthe .bash_completion file and autocompletion setup in Breeze2would fail.In case the file is missing we skip backing it up and create itnow. Also installing click is done before modifying the scripts,this way we always install latest click version for the userwhen running setup-autocomplete and the 'source' instructionsare printed last so the user will not miss it.",1
Chart: Default to Airflow 2.2.5 (#22724),2
Modify transfer operators to handle more data (#22495)* Modify transfer operators to handle more dataThis addresses an issue where large data imports can result in fillingall available disk space and cause the task to fail.Previously all data would be written out to disk before any was uploadedto GCS. Now each data chunk is written to GCS and immediately freed.,5
"List Mapped Instances in Task Details Panel (#22691)* Add table of mapped instances to grid view* Autorefresh, table width, remove extraneous code* Switch to icon buttons* Fix mapped instances url* Only show dates when they exist and are final",5
Docs: Fix example usage for `AzureCosmosDocumentSensor` (#22735),2
Unvendor cgroupspy (#22736)The 0.2.2 release of `cgroupspy` makes it Python 3.10 compatible.We can remove vendoring done as of #22209 #22208 #2207 #22206Discussion and links:* https://github.com/cloudsigma/cgroupspy/pull/14,2
Assert run_as_user in test_localtaskjob_heartbeat (#22743),3
log backfill exceptions to sentry (#22704),1
Fix wrong reference in tracking-user-activity.rst (#22745),1
Better verification of Localexecutor's parallelism option (#22711),1
Show tasks in grid view based on topological sort. (#22741)This takes the existing topological sort that existed on a DAG and movesit down to TaskGroup.In order to do this (and not have duplicated sort) the existing sort onDAG is re-implemented on top of the new method.This also surfaced a tiny bug in deserialize_task_group where theSerializedTaskGroup did not have `dag` set -- it didn't cause anyproblems until now but was needed to call `upstream_list` on aSerializedTaskGroup object.,0
Improve Grid view selection rerenders (#22742)* Improve selection rerenders* use data attribute instead of check for color,5
"Check for missing dagrun should know version (#22752)When we apply the pre-upgrade check for tables with records missing a matching dagrun record, we create a temp table, and we name the temp table with a suffix including the version where the migration appears.  Tables added to the check after Airflow 2.2 were using the wrong version number.",0
"Fix state and try number for failed mapped tasks (#22757)Without this change the details drawer thinks there are two attemptsrather than one for failed tasks, and a mapped task in UPSTREAM_FAILEDhas no_status rather than upstream_failed.",0
Remove extraneous treeData fields (#22763),5
Move tests under correct test class (#22768),3
Consider mapped upstream in TriggerRuleDep (#22583),5
Remove task group mapping for now (#22518),4
Add map_index to Log model (#22274),2
remove json parse for gantt chart (#22780),2
Add summary of runs to dag details (#22766)* Add basic cross-run info to dag details* Fix linting* add start times and states* Simplify metadata details* change Text to Heading,4
Prevent meta name clash for task instances (#22783),2
Pickle dag exception string fix (#22760),0
No need to load whole ti in current_state (#22764)Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Correctly apply defaults to mapped task flow (#22683)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
"API: Fix deprecation warning due to using query.value (#22775)When using sqlalchemy 1.4, there's a deprecation warning at the task logging:SADeprecationWarning: Query.value() is deprecated and will be removedin a future release.  Please use Query.with_entities() in combinationwith Query.scalar() (deprecated since: 1.4)This PR fixes it",0
"Fail ``LocalFilesystemToGCSOperator`` if src does not exist (#22772)Fix #22705.Fail LocalFilesystemToGCSOperator if the src file does not exist`src` argument of LocalFilesystemToGCSOperator accept either list of source file path or a single source file path as a string. In the case of a single source file path we are using [glob](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/transfers/local_to_gcs.py#L111) to parse the file path and glob return empty list if file path does not exist. In the next step, we iterate on this list and call [hook api](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/transfers/local_to_gcs.py#L123) to update the file since the list is empty control is not going inside loop and task is succeeding even if the source file is not available.Change - Raise an exception if [filepath](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/transfers/local_to_gcs.py#L111) list is emptyAfter this change below task will fail if example-text.txt does not exist```upload_file = LocalFilesystemToGCSOperator(        task_id=""upload_file"",        src=""example-text.txt"",        dst=DESTINATION_FILE_LOCATION,        bucket=BUCKET_NAME,    )```",2
"Fix ``SQLALchemy`` warning about conflicting relationships (#22786)Running airflow commands print out warnings about conflicting relationships.SAWarning: relationship 'DagRun.serialized_dag' will copy column serialized_dag.dag_id to column dag_run.dag_id, which conflicts with relationship(s): 'RenderedTaskInstanceFields.dag_run' (copies rendered_task_instance_fields.dag_id to dag_run.dag_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=Trueshould be applied to one or more if they are read-only. For the less common case that foreignkey constraints are partially overlapping, the orm.foreign() annotation can be used toisolate the columns that should be written towards.   The 'overlaps' parameter may be used to remove this warning. (Background on this error at: http://sqlalche.me/e/14/qzyx)/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/relationships.py:3463 SAWarning:relationship 'SerializedDagModel.dag_runs' will copy column serialized_dag.dag_id tocolumn dag_run.dag_id, which conflicts with relationship(s): 'RenderedTaskInstanceFields.dag_run'(copies rendered_task_instance_fields.dag_id to dag_run.dag_id). If this is not the intention, consider if these relationships should be linked with back_populates, or if viewonly=True should be applied to one or more if they are read-only. For the less common case that foreignkey constraints are partially overlapping, the orm.foreign() annotation can be used to isolatethe columns that should be written towards.   The 'overlaps' parameter may be used to remove this warning. (Background on this error at: http://sqlalche.me/e/14/qzyx)Since the RenderedTaskInstanceFields.dag_run is only used in loading up the execution_date, we mark it asviewonly which fixes the above warning",2
Serialize mapped operator expansion kwargs logic (#22792)This is needed because we need to be able to access the correctexpansion kwargs from the serialized mapped operator in the scheduler.,1
"Move CTAS logic into a CTAS function in db pre-upgrade (#22791)Will be reusing this in later PRs that need to do the same thing, but doing in separate PR for ease of review.",1
Switch to `pipx` as the only installation Breeze2 method (#22740)Switching Breeze2 to only use `pipx` for installation of Breeze2due to problems it might cause for autocompletion if entrypointis not avaiable on PATH.,1
"Fix sqlalchemy warning about coercing subquery for use in IN() (#22788)On task log, when using postgres db, you would see a warning:WARNING - /usr/local/lib/python3.7/site-packages/sqlalchemy/sql/coercions.py:521SAWarning: Coercing Subquery object into a select() for use in IN();please pass a select() construct explicitlyThis PR fixes it",0
"Fix processor cleanup on DagFileProcessorManager (#22685)* Fix processor cleanupReferences to processors weren't being cleaned up afterkilling them in the event of a timeout. This lead toa crash caused by an unhandled exception when trying toread from a closed end of a pipe.* Reap the zombie when killing the processorWhen calling `_kill_process()` we're generatingzombies which weren't being `wait()`ed for. Thisled to a process leak we fix by just calling`waitpid()` on the appropriate PIDs.* Reap resulting zombies in a safe wayAccording to @potiuk's and @malthe's input, the waywe were reaping the zombies could cause some racy andunwanted situations. As seen on the discussion over at`https://bugs.python.org/issue42558` we can safelyreap the spawned zombies with the changes we haveintroduced.* Explain why we are actively waitingAs suggested by @potiuk explaining why we chose to actively wait on an scenario such as this one can indeed be useful for anybody taking a look at the code some time from now...Co-authored-by: Jarek Potiuk <jarek@potiuk.com>* Fix small typo and triling whitespaceAfter accepting the changes proposed on the PRwe found a small typo (we make those on a daily basis)and a trailing whitespace we though was nice to delete.Hope we made the right choice!* Fix call to `poll()`We were calling `poll()` through the `_process` attributeand, as shown on the static checks triggered by GitHub,it's not defined for the `BaseProcess` class. We insteadhave to call `poll()` through `BaseProcess`'s `_popen`attribute.* Fix processor cleanupReferences to processors weren't being cleaned up afterkilling them in the event of a timeout. This lead toa crash caused by an unhandled exception when trying toread from a closed end of a pipe.* Reap the zombie when killing the processorWhen calling `_kill_process()` we're generatingzombies which weren't being `wait()`ed for. Thisled to a process leak we fix by just calling`waitpid()` on the appropriate PIDs.* Reap resulting zombies in a safe wayAccording to @potiuk's and @malthe's input, the waywe were reaping the zombies could cause some racy andunwanted situations. As seen on the discussion over at`https://bugs.python.org/issue42558` we can safelyreap the spawned zombies with the changes we haveintroduced.* Explain why we are actively waitingAs suggested by @potiuk explaining why we chose to actively wait on an scenario such as this one can indeed be useful for anybody taking a look at the code some time from now...Co-authored-by: Jarek Potiuk <jarek@potiuk.com>* Fix small typo and triling whitespaceAfter accepting the changes proposed on the PRwe found a small typo (we make those on a daily basis)and a trailing whitespace we though was nice to delete.Hope we made the right choice!* Fix call to `poll()`We were calling `poll()` through the `_process` attributeand, as shown on the static checks triggered by GitHub,it's not defined for the `BaseProcess` class. We insteadhave to call `poll()` through `BaseProcess`'s `_popen`attribute.* Prevent static check from failingAfter reading through `multiprocessing`'s implementation wereally didn't know why the static check on line `239` wasfailing: the process should contain a `_popen` attribute...That's when we found line `223` and discovered the trailing`# type: ignore` comment. After reading up on it we foundthat it instructs *MyPy* not to statically check that veryline. Given we're having trouble with the exact same attributewe decided to include the same directive for the static checker.Hope we made the right call!* Fix test for `_kill_timed_out_processors()`We hadn't updated the tests for the method whosebody we've altered. This caused the tests to failwhen trying to retrieve a processor's *waitable*,a property similar to a *file descriptor* inUNIX-like systems. We have added a mock property tothe `processor` and we've also updated the `manager`'sattributes so as to faithfully recreate the state ofthe data sctructures at a moment when a `processor`is to be terminated.Please note the `assertions` at the end are meant tocheck we reach the `manager`'s expected state. We havechosen to check the number of processor's against anexplicit value because we're defining `manager._processors`explicitly within the test. On the other hand, `manager.waitables`can have a different length depending on the call to`DagFileProcessorManager`'s `__init__()`. In this test theexpected initial length is `1` given we're passing `MagicMock()`as the `signal_conn` when instantiating the manager. However,if this were to be changed the tests would 'inexplicably' fail.Instead of checking `manager.waitables`' length against a hardcodedvalue we decided to instead compare it to its initial lengthso as to emphasize we're interested in the change in length, notits absolute value.* Fix `black` checks and `mock` decoratorsOne of the methods we are to mock required a ratherlong `@mock.patch` decorator which didn't pass thechecks made by `black` on the precommit hooks. Ontop of that, we messed up the ordering of the`@mock.patch` decorators which meant we didn'tset them up properly. This manifested as a `KeyError`on the method we're currently testing. O_oCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",3
Pass custom headers through in SES email backend (#22667),4
Bump axios from 0.21.1 to 0.21.2 in /airflow/ui (#22797)Bumps [axios](https://github.com/axios/axios) from 0.21.1 to 0.21.2.- [Release notes](https://github.com/axios/axios/releases)- [Changelog](https://github.com/axios/axios/blob/master/CHANGELOG.md)- [Commits](https://github.com/axios/axios/compare/v0.21.1...v0.21.2)---updated-dependencies:- dependency-name: axios  dependency-type: direct:production...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump minimist from 1.2.5 to 1.2.6 in /airflow/ui (#22799)Bumps [minimist](https://github.com/substack/minimist) from 1.2.5 to 1.2.6.- [Release notes](https://github.com/substack/minimist/releases)- [Commits](https://github.com/substack/minimist/compare/1.2.5...1.2.6)---updated-dependencies:- dependency-name: minimist  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Expand mapped tasks at DagRun.Veriy_integrity (#22679)Create the necessary task instances for a mapped task at dagrun.verify_integrityCo-authored-by: Ash Berlin-Taylor <ash@apache.org>,2
Fix `email_on_failure` with `render_template_as_native_obj` (#22770)Co-authored-by: andyhuang <andyhuang@mirrormedia.mg>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,0
Correctly interpolate pool name in PoolSlotsAvailableDep statues (#22807),5
"Don't show irrelevant/duplicated/""internal"" Task attrs in UI (#22812)For example, showing `Log Logger <airflow.models.mappedoperator.MappedOperator (INFO)>` isn't useful.",1
"Make ElasticSearch Provider compatible for Airflow<2.3 (#22814)`ti.map_index` is not released yet and even once it is released in 2.3, we still want this provider to be backwards compatible, this fixes it.",0
Check if map_index is accidentally used in providers. (#22817),1
Give up on trying to recreate task_id logic (#22794),2
Correctly fetch logs for mapped task instances (#22818)We weren't passing the map_index param down to the server,2
Bump minimist from 1.2.5 to 1.2.6 in /airflow/www (#22798)Bumps [minimist](https://github.com/substack/minimist) from 1.2.5 to 1.2.6.- [Release notes](https://github.com/substack/minimist/releases)- [Commits](https://github.com/substack/minimist/compare/1.2.5...1.2.6)---updated-dependencies:- dependency-name: minimist  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump url-parse from 1.5.1 to 1.5.10 in /airflow/ui (#22822)Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.10.- [Release notes](https://github.com/unshiftio/url-parse/releases)- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.10)---updated-dependencies:- dependency-name: url-parse  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Disable SLAs for mapped operators (#22641)When trying to update SLA logic to handle mapped operators we discovered some odd behavior and decided to defer adding support for SLAs with mapped tasks.,1
Bump nanoid from 3.1.23 to 3.3.2 in /airflow/www (#22803)Bumps [nanoid](https://github.com/ai/nanoid) from 3.1.23 to 3.3.2.- [Release notes](https://github.com/ai/nanoid/releases)- [Changelog](https://github.com/ai/nanoid/blob/main/CHANGELOG.md)- [Commits](https://github.com/ai/nanoid/compare/3.1.23...3.3.2)---updated-dependencies:- dependency-name: nanoid  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump postcss from 7.0.35 to 7.0.39 in /airflow/ui (#22831)Bumps [postcss](https://github.com/postcss/postcss) from 7.0.35 to 7.0.39.- [Release notes](https://github.com/postcss/postcss/releases)- [Changelog](https://github.com/postcss/postcss/blob/7.0.39/CHANGELOG.md)- [Commits](https://github.com/postcss/postcss/compare/7.0.35...7.0.39)---updated-dependencies:- dependency-name: postcss  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Prepare mid-April provider documentation. (#22819),2
Bump prismjs from 1.26.0 to 1.27.0 in /airflow/www (#22823)Bumps [prismjs](https://github.com/PrismJS/prism) from 1.26.0 to 1.27.0.- [Release notes](https://github.com/PrismJS/prism/releases)- [Changelog](https://github.com/PrismJS/prism/blob/master/CHANGELOG.md)- [Commits](https://github.com/PrismJS/prism/compare/v1.26.0...v1.27.0)---updated-dependencies:- dependency-name: prismjs  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Add 2.2.5 to revision heads map (#22841)This is necessary to allow for downgrade to 2.2.5,1
Temporarily disable task_fail pre-upgrade duplicates check (#22839)I am reworking it to actually move the rows but for now we can disable it.,4
Support conf param override for backfill runs (#22837)Co-authored-by: Dmirty Suvorov <dmitry.suvorov@scribd.com>,1
Disable foreign keys on sqlite when modifying dag_run (#22848)If we do not disable FKs then it has the side effect of deleting all task instances.,4
"Revert ""Print configuration on scheduler startup. (#22588)"" (#22851)This reverts commit 78586b45a0f6007ab6b94c35b33790a944856e5e.",4
Bring back limits on branches/tags builds in Airlfow repo (#22855)The change #22542 accidentally removed limit on branchesthat trigger direct push workflows in CI.Currently the builds are also triggered when a new TAG is pushednot only when new branch is created and this is quite too muchespecially when we push multiple tags for providers :(,1
Docs: `remote_log_conn_id` can also be used to write logs (#22844),2
"Add more fields to REST API dags/dag_id/details endpoint (#22756)Added more fields to the DAG details endpoint, which is the endpoint forgetting DAG `object` details",2
fix message in prepare_provider_packages.py (#22856),1
Add example DAG for demonstrating usage of GCS sensors (#22808)Following GCS Sensors examples are provided as part of the change:1. GCSUploadSessionCompleteSensor2. GCSObjectUpdateSensorThe commit does the following:1. Delete the newly created top level example_gcs.py as it was a   wrong place for the sensors2. Add the intended sensors of the PR to the existing example_gcs.py file   located in airflow/cloud/example_dags directory,2
Support log download in task log view (#22804)* Add download button to ti_log view* Use native anchor tag for ti_log download button* Replace regex with data attribute* Update airflow/www/static/js/ti_log.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,2
Better handle auto-refresh errors (#22840),0
"Add XComArg to lazy-imported list of Airflow module (#22862)In writing the docs for Dynamic Task Mapping (AIP-42) I noticed thatthere are some cases where users need to use XComArg directly, and itdidn't feel right to make the import things from `airflow.models`.And I've now refactored the lazy import to be ""data-driven"" as threeblocks of almost identical code was my limit.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: D. Ferruzzi <ferruzzi@amazon.com>",1
"Add `pre-commit` to check that `REVISION_HEADS_MAP` is up-to-date (#22860)This PR adds a `pre-commit` to make sure that the `REVISION_HEADS_MAP` isup-to-date in any release. Since this feature will be out in 2.3.0 and is very importantto update it for releases, not just main, this would be helpful",5
Add securityContext config for Redis to helm chart (#22182)Co-authored-by: Jed Cunningham <jedcunningham@apache.org>,2
Support dag serialization with custom ti_deps rules (#22698),2
Fixed backfill interference with scheduler (#22701)Co-authored-by: Dmirty Suvorov <dmitry.suvorov@scribd.com>,1
"Events Timetable (#22332)This Timetable will be widely useful for timing based on sporting events, planned communication campaigns,and other schedules that are arbitrary and irregular but predictable.",1
"Speed up `has_access` decorator by ~200ms (#22858)Using the ORM to create all the Role, Permission, Action and Resourceobjects, only to throw them all away _on every request_ is slow. Andsince we are now using the API more and more in the UI it's starting toget noticeableThis changes the `user.perm` property to issue a custom query thatreturns the tuple of action_name, permission_name we want, bypassing theORM object inflation entirely, and since `user.roles` isn't needed inmost requests we no longer eagerly load that.* Fix testsCaching issues that only crop up in tests (but not ever a problem in therequest life cycle of webserver",0
Fix pre-upgrade check for rows dangling w.r.t. dag_run (#22850)Some migrations for 2.3.0 add keys to TI and DR.We have a check that purges any rows that can't be mapped to a DR.But it doesn't work correctly. It does DELETE FROM USING dag_run; but this is an inner join so if the dag run isn't there the rows won't be deleted.Instead we can do DELETE FROM WHERE NOT EXISTS. This has a happy side effect of letting us remove some dialect-specific code.This PR does not add a check for dangling w.r.t. TI -- that is deferred for a later PR.,1
Fix Grid view font sizing (#22866),0
Cleanup dup code now that k8s provider requires 2.3.0+ (#22845),1
"Support for sorting DAGs in the web UI (#22671)* Add sort + small test* clean code* Remove useless forgotten macro, fix nullslast for mysql* Changes following code review* Remove nullslast* Changes desc syntax",4
Bump moment from 2.29.1 to 2.29.2 in /airflow/www (#22873)Bumps [moment](https://github.com/moment/moment) from 2.29.1 to 2.29.2.- [Release notes](https://github.com/moment/moment/releases)- [Changelog](https://github.com/moment/moment/blob/develop/CHANGELOG.md)- [Commits](https://github.com/moment/moment/compare/2.29.1...2.29.2)---updated-dependencies:- dependency-name: moment  dependency-type: direct:development...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Adjust DAG/TI details panel (#22877)* Clean up Dag details* Clean up TI details* Add mapped task count,1
Support unknown backends in entrypoint_prod.sh (#22883),1
Fix new MyPy errors in main (#22884)Those MyPe errors are side effect of some new dependencies.,1
Databricks: Correctly handle HTTP exception (#22885)Exception for non-existent repo wasn't correctly handled for DatabricksRepos operations,5
"Databricks SQL operators are now Python 3.10 compatible (#22886)New version of databricks-sql-connector fixes incompatibility withPython 3.10, so rollig back #22221, and bumping dependency.This closes #22220",0
"Replace old Breeze with Python based implementation (#22880)Over the last few months together with Outreachy internswe rewrote the most important functionality of the old Bash-basedBreeze with Python Based implementation.We approached it in systematic way with capturing all our decisionsin the ADR format (dev/breeze/docs) and implementing the partsthat are used on a daily basis by the users. Breeze2 as it wascalled is ready for Prime-Time with the users so we are swappingout the old breeze wiht the new one.The old `breeze` has been moved to `breeze-legacy` and we willgradually parts of it that are already migrated to Python and provenwhile continue rewriting the parts that are missing (mostly themaintainer tools) and replacing the remaining CI shell scripts withthe new `breeze` commands.We also need to make sure that there is no accidental top-levelimport for extra packages added in the future, because peoplewho installed breeze before will not have it - so we havea pre-commit that checks if breeze.py can be parsed and--help executed with just rich and click installed.This PR:* moves `breeze` to `breeze-legacy`* moves `Breeze2` to `breeze`* updates documentation and screenshots where applicable* explains old vs. new breeze in documentation* adds protection so that no accidental top-level import is  added to breeze.py and files imported from thereFixes: #22827",0
"Remove unnecessary python 3.6 conditionals (#20549)Since Python 3.7 is now the lowest supported version, we no longer needto have conditionals to support 3.6.",1
Push CI image using new Python Breeze (#22888)Fixes: #22821,0
"Fix ""force_answers"" parameter to be ""yes"" rather than ""true"" (#22891)",2
MSSQLToGCSOperator fails: datetime is not JSON Serializable (#22882)* Handle date and time convert_type for MSSQLToGCSOperator,1
update INTHEWILD (#22896),5
"make operator's execution_timeout configurable (#22389)* make operator's execution_timeout configurableBy this commit, execution_timeout attribute of theoperators is now configurable globally via airflow.cfg.* The default value is still `None`. Users are expected todefine a positive integer value to be passed into timedelta objectto set timeout in terms of seconds by default, via configuration.* If the key is missing or is set to a non-positive value, then it isconsidered as `None`.* Added `gettimedelta` method to be used in abstractoperatorto get timedelta or None type object. The method raises exceptionfor the values that are not convertible to integer and/or the valuestoo large to be converted to C int.* Sample config cases are added into unit tests.Closes #18578* raise error for non-positive execution_timeout* By this commit, error raises for the values <= 0instead of using fallback value* Updated unit tests* include OverflowError error message in exceptionTo be more clear to the user, added relevant error messageinto to AirflowConfigException.* rename default_execution_timeoutThis parameter specifies the tasks' execution timeout,so all configuration and variable names are now contains`task` in it.* update `version_added` for execution_timeout* update execution_timeout descriptionfixed the description of default_task_execution_timeoutbased on the recent changes* update inline comment for non-positive value check* update `gettimedelta` docstring* allow non-positive values in gettimedeltaBefore this commit, gettimedelta method was preventinguser to provide non-positive values. Now it is totally up tousers to provide a sensible value for this configurationCo-authored-by: sercan.sagman <sercan.sagman@inventanalytics.com>",5
Add test case for clearTaskInstance call with invalid Task IDs. (#22894),3
"Allow using mapped upstream's aggregated XCom (#22849)This needs two changes. First, when the upstream pushes the return valueto XCom, we need to identify that the pushed value is not used on itsown, but only aggregated with other return values from other mapped taskinstances. Fortunately, this is actually the only possible case rightnow, since we have not implemented support for depending on individualreturn values from a mapped task (aka nested mapping). So we insteadskip recording any TaskMap metadata from a mapped task to avoid theproblem altogether.The second change is for when the downstream task is expanded. Since thetask depends on the mapped upstream as a whole, we should not useTaskMap from the upstream (which corresponds to individual taskinstances, as mentioned above), but the XComs pushed by every instanceof the mapped task. Again, since we don't nested mapping now, we can cutcorners and simply check whether the upstream is mapped or not to decidewhat to do, and leave further logic to the future.Co-authored-by: Ash Berlin-Taylor <ash@apache.org>",2
Handle invalid JSON metadata in get_logs_with_metadata endpoint. (#22898),5
Add concept doc for Dynamic Task Mapping (#22867)Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>Co-authored-by: Jed Cunningham <jedcunningham@apache.org>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Catch error in Breeze when docker is not running (#22901),1
"Update mapped task UX (#22911)* Add Xcom button, hide map index actions, disabled run* Allow bulk mapped task actions* Remove table selection for now* fix linting error* Fix copy and isDisabled",0
Add SmoothOperator (#22813)Easter is coming so I just came with idea of an easter egg.,1
typo in BREEZE.rst (#22919),2
implements #22859 - Add .sql as templatable extension (#22920),1
Fix bug where dynamically mapped tasks got set to REMOVED (#22909)* Fix bug where dynamically mapped tasks got set to REMOVEDThis mostly affects backfil/`airflow tasks test`.,3
Remove installation instructions from Breeze's cheatsheet (#22923)The cheatsheet is displayed only after Breeze is installed so itmakes no sense to display installation instructions in thecheathsheet.,1
Move the database configuration to a new section (#22284)Co-authored-by: gitstart-airflow <gitstart@users.noreply.github.com>Co-authored-by: GitStart <1501599+gitstart@users.noreply.github.com>Co-authored-by: Egbosi Kelechi <egbosikelechi@gmail.com>,1
Fix changelog spelling (#22926),4
Deprecate `S3PrefixSensor` and `S3KeySizeSensor` in favor of `S3KeySensor` (#22737)Deprecate `S3PrefixSensor` and `S3KeySizeSensor` in favor of `S3KeySensor` Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
Move database config move note to `main` section (#22929)This was accidentally added to the `2.2.4` notes instead of `main`.,1
Use full version string for deprecated config (#22930),5
Do not clear XCom when resuming from deferral (#22932),5
Add `parameters` to templated fields in `OracleOperator` (#22857),1
Reuse reflect_tables helper in db.py (#22922),5
Fix airflow version in migration 587bdf053233 (#22935),0
"Fix column names in ""moved"" tables created pre-upgrade (#22937)I inadvertently prefixed all the column names with the table name.  This fixes by adding column aliases explicitly.",1
cache and typo fix (#22876),0
"Call mapped_dependants only on the original task (#22904)* Add literal expands in test DAGs* Call mapped_dependants only on the original taskWe've made change on this in the scheduler, but need to match it inthe BackfillJob.",4
"Ensure that mapped TIs in BackfillJob have a start_date (#22946)Since BackfillJob is not at all like the scheduler it handles thingsdifferently, and mapped TIs were ending up with a null start date whenexecuting!",5
Delete old Spark Application in SparkKubernetesOperator (#21092)* Delete previous SparkApp in Kubernetes+ KubernetesHook: adding delete_custom_object+ SparkKubernetesOperator: extract name from k8yaml and delete if exists+ Update SparkKubernetesOperator docstring* Delete previous SparkApp in Kubernetes+ KubernetesHook: adding delete_custom_object+ SparkKubernetesOperator: extract name from k8yaml and delete if exists+ Update SparkKubernetesOperator docstring,2
"Priority order tasks even when using pools (#22483)When picking tasks to queue, the scheduler_job groups candidate task instancesby their pools and picks tasks for queueing for each ""pool group"".This way tasks with lower priority could be queued before tasks with higherpriority. This is demostrated in new UT`test_find_executable_task_instances_order_priority_with_pools` - before thischange `dummy3` and `dummy1` are queued instead of `dummy3` and `dummy2`.Co-authored-by: Tanel Kiis <tanel.kiis@reach-u.com>",4
Helm support for LocalKubernetesExecutor (#22388),1
Remove badly merged conflict for BREEZE.rst (#22953)Missed the conflict when merging #22876. Github hidessuch big changes by default :(,4
Fix regression in pool metrics (#22939)Co-authored-by: Tanel Kiis <tanel.kiis@reach-u.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,0
"Ensure that BackfillJob re-runs existing mapped task instances (#22952)* Ensure that BackfillJob re-runs existing mapped task instances`expand_mapped_task` only returns _new_ TaskInstances, so if a backfilljob was run for a dag that already existed the mapped task would neverbe executed.* Fix tests for changed interfaceThis interface also isn't the best, and we could probably do withrefactoring it. Not now though",4
Remove Grid labels and differentiate runs from tasks more (#22950),1
Remove duplicate Apache License line in `ci.yml` (#22960)Remove duplicate line !,4
"Bug Fix for `apache-airflow-providers-jenkins` `JenkinsJobTriggerOperator` (#22802)* bugfix for when polling for the created job, if fail to get job info it should not fail the task, instead it should continue polling until reaches the max allowed polling tries",1
Hide pagination when data is a single page (#22963),5
"Fix screenshot generation for dumb terminal (#22962)When dumb terminal is set when screenshot image is generated, the terminal width is decreased to 80 and screenshots are changing.We force 256 color xterm during screenshot generation.",2
Deprecate `DummyOperator` in favor of `EmptyOperator` (#22832)* Deprecate `DummyOperator` in favor of `EmptyOperator`,1
Refactor airbyte provider tests to use assert_has_calls (#22951)* refactor airbyte provider tests to use assert_has_callsCo-authored-by: gitstart-airflow <gitstart@users.noreply.github.com>,1
Allow DagParam to hold falsy values (#22964),2
Fix select * query xcom push for BigQueryGetDataOperator (#22936)Use in instead of get for conditinal check,1
Add template support for external_task_ids. (#22809),1
"Support glob syntax in ``.airflowignore`` files (#21392) (#22051)A new configuration parameter ""CORE_IGNORE_FILE_SYNTAX"" is added toallow patterns in .airflowignore files to be interpreted as eitherregular expressions (the default) or glob expressions as found in.gitignore files. This allows users to use patterns they will befamiliar with from tools such as git, helm and docker.Glob expressions support wildcard matches (""*"", ""?"") within a directoryas well as character classes (""[0-9]""). In addition, zero or moredirectories can be matched using ""**"". Patterns can be negated byprefixing a ""!"" at the beginning of the pattern.The ""fnmatch"" library in core Python does not produce patterns that arefully compliant with the kind of patterns that users will be used tofrom gitignore or dockerignore files, so the globs are parsed usingthe pathspec package from PyPI.To aid with debugging ignorefile patterns a more helpful errormessage is emitted in the logs for invalid patterns, which arenow skipped rather than causing a hard-to-read scheduler stack trace.closes: #21392",1
"Add max width to task group tooltips (#22978)Add a max width to the tooltip css class to make long task group descriptions wrap.Fixes: https://github.com/apache/airflow/issues/22912Before:<img width=""1680"" alt=""Screen Shot 2022-04-13 at 9 12 54 AM"" src=""https://user-images.githubusercontent.com/4600967/163188297-e7369d02-bb87-42de-8827-4de0884a9e62.png"">After:<img width=""612"" alt=""Screen Shot 2022-04-13 at 9 11 52 AM"" src=""https://user-images.githubusercontent.com/4600967/163188319-7ede8211-25e4-4f9d-8edf-a0ed1a7c47b6.png"">",1
Prepare for RC2 release of March Databricks provider (#22979),1
Update tree doc references to grid (#22966)* Update tree doc references to grid* Update docs/apache-airflow/ui.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* Update docs/apache-airflow/ui.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* revert Chart changes. move main grid view* Update docs/apache-airflow/ui.rstCo-authored-by: Tzu-ping Chung <tp@astronomer.io>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
Add no_status to state priority list (#22985),1
Correctly show rendered templates for mapped task instances. (#22984),2
"Support importing connections from files with "".yml"" extension (#22872)* Add .yml to list of import-able secret file extensions",2
"Fail if task does not push XCom for downstream (#22954)The task can already fail if an XCom is pushed but unmappable. Thisextends the check to cover cases where the task returns None, or doesnot push at all (i.e. do_xcom_push=False).",0
Add a clipboard button to grid details (#22988)* Add a clipboard button to grid details* rename otherProps* Remove Box,4
"Drop ""airflow moved"" tables in command `db reset` (#22990)The `db reset` command does not currently drop the ""temporary"" tables we create when purging bad rows as part of an upgrade.But to truly reset the db, we need to remove them.",4
es new system tests (#22811),3
Fix trigger event payload is not persisted in db (#22944)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
Fix tooltip for mapped tasks (#22994)* Fix tooltip for mapped tasks* Only show count when it exists* Handle singular task,0
Replace usage of `DummyOperator` with `EmptyOperator` (#22974)* Replace usage of `DummyOperator` with `EmptyOperator`,1
migrate system test gcs_to_bigquery into new design (#22753),1
formatting fix (#22688),0
"Add option `--skip-init` to db reset command (#22989)This is useful when doing testing, if we want to clear out the tables but no re-initialize the database, e.g. because we plan to only initialize it to a certain revision with `db upgrade --to-version`.",5
Base run details header on interval start (#22999)Backup on executionDate for dags that don't have a data interval (ie: from a migration),5
Add doc and sample dag for S3CopyObjectOperator and S3DeleteObjectsOperator (#22959),4
Add k8s container's error message in airflow exception (#22871),0
Skip log template sync if table doesn't exist (#22993)When doing partial or offline upgrades the log template table (which is added in 2.3.0) may not exist.  We should skip synchrization (and save the user some error logging) in that case.,2
"Add dangling rows check for TaskInstance references (#22924)We are adding some foreign keys in 2.3.0 so we want make it more likely that migration succeeds by detecting FK violations and moving the records out of the table before creating the FK.  We already had a check for ""missing"" dag runs, but this adds a check for TaskInstance.  In most cases we replace the ""missing dag run"" check with a ""missing TI"" check since from 2.2.0 a TI implies the existence of a DR anyway.",1
"Resolve XComArgs before trying to unmap MappedOperators (#22975)Many operators do some type validation inside `__init__`(DateTimeSensor for instance -- which requires a str or a datetime)which then fail when mapped as they get an XComArg instead.To fix this we have had to change the order we unmap and resolvetemplates:- first we get the unmapping kwargs, we resolve expansion/mapping args  in that- Then we create the operator (this should fix the constructor getting  XComArg problem)- Then we render templates, but only for values that _weren't_ expanded  alreadyUnmapping the task early in LocalTaskJob causes problems, and it's justnot needed as it is (correctly) unmapped insideTaskInstance._execute_task_with_callbacks call to`self.render_templates()`",0
"Update ImportError items instead of deleting and recreating them (#22928)* Update ImportError items instead of deleting and recreating themEach time a dag with import error is parsed, the ImportError record is deletedand a new record is created. For example, say I have two dags with import errors,initially, the import error id will be dag_1:import_error.id=1, dag2:import_error.id=2.In the next dag parsing, the import error will increase. dag_1:import_error.id=3,dag_2:import_error.id=4 and it continues like that.This makes it impossible for the get import error REST API endpoint to be consistentThis PR fixes this issue by updating the existing record and creating a new one if no recordexists",1
Give useful repr to _LazyXComAccess class (#23002)When doing an xcom_pull on a join/reduce task we return an object oftype _LazyXComAccess -- this gives it a slightly more useful repr incase a user prints it.(I do this by using attr to define the class as it builds a repr for mefor free),1
"Note that value received in reduce is not a list (#23006)To avoid clogging, the aggregated value from an upstream mapped task isa lazy access proxy. This should work as expected (similar to a list) inmost situations, but let's add a note to clarify it's not really a listto avoid potential user confusion.",5
"Don't add planned tasks for legacy DAG runs (#23007)Technically we could still infer planned runs even from a legacy DAG run(i.e. run without an explicit data interval), but that's probably notworth it. This will ""fix"" itself after that DAG is run one time against2.2 anyway.",1
Fix download logs from Grid/graph view (#23009)There Checking for metadata to be falsy before JSON-decoding it doesn'tmake sense.And since 99%+ of times we don't need this value I have made it optionaland don't pass it in the front end,4
Change trigger dropdown left position (#23013),4
Fix some weird English in `create_dagrun` exceptions (#23003),2
Fix cancel_on_kill after execution timeout for DataprocSubmitJobOperator (#22955)Synchronous tasks killed by execution timeout weren't canceleddue to wrong assignment of job_id property.,5
Change `ExternalTaskMarker` to `EmptyOperator` (#23010),1
"Fix TaskFail queries in views after run_id migration (#23008)Two problems here:1. TaskFail no longer has a executin_date property -- switch to run_id2. We weren't joining to DagRun correctly, meaning we'd end up with a   cross-product effect(? Something weird anyway)Co-authored-by: Karthikeyan Singaravelan <tir.karthi@gmail.com>",2
Make sure all mapped nodes are updated. (#23019),5
"Purge duplicates from TaskFail prior to 2.3 upgrade (#22769)We're adding a PK to TaskFail in 2.3 so we wanted to check and move dupes out of the table prior to upgrade.  Why do we do it in pre-upgrade check instead of in the migration? (1) historical reasons, we initially only ""checked"" and failed with this type of thing but then realized we needed to actualy move the rows out but did want to move the code (2) will take a little more thought concering how to implement with `--sql-mode` given that we don't know until we run the migrations whether those `_airflow_moved` tables will need to be created and if we create them unnecessarily then many users will get unneeded warnings in the web UI (we warn and tell users to deal with these tables if they are created).",1
Add `2.3.0b1` to issue template (#23024),0
"Update gcs_to_local.rst (#23026)Change:`to upload a file to GCS.` -> `to download a file from GCS.`Reason:`GCSToLocalFilesystemOperator` downloads a file from GCS to your local file system. There is a different operator, `LocalFilesystemToGCSOperator`, which can upload files from your local filesystem to GCS.",5
Breeze is installed via pipx in CI (#23023)The new Breeze needs to be installed via pipx in CI - except whenit is unit-tested. Also it needs to be added to PATH.This change adds a very small script that is used in all placeswhere Breeze needs to be installed.,1
Upgrade to support Google Ads v10 (#22965),1
Include message in graph errors (#23021),0
Bump async from 2.6.3 to 2.6.4 in /airflow/ui (#23034)Bumps [async](https://github.com/caolan/async) from 2.6.3 to 2.6.4.- [Release notes](https://github.com/caolan/async/releases)- [Changelog](https://github.com/caolan/async/blob/v2.6.4/CHANGELOG.md)- [Commits](https://github.com/caolan/async/compare/v2.6.3...v2.6.4)---updated-dependencies:- dependency-name: async  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Allow re-use of decorated tasks (#22941)This opens up the possibility of using one decorated taskin different dag files.Take for example the below task:- common.py@task(task_id='hello')def hello():    print('Hello')defined in a file and called in different dag files using different task ids:- dag_file1.py:from common import hello@dag()def mydag():    for i in range(3):        hello.override(task_id=f'myhellotask_{i}')()- dag_file2.py:from common import hello@dag():def mydag2():    for i in range(3):        hello.override(task_id=f'welcome_message_{i}')()They would all run with different task ids,1
Fix tests using has_calls to use assert_has_calls. (#23001),3
Fix Grid autoscroll with ResizeObserver (#23022),0
Default side panel open vs closed (#23039),5
Revert disabling run task button (#23038),1
Show map_index in states-for-dag-run (#23030),2
Make presto and trino compatible with airflow 2.1 (#23061),1
Add SnowSQL installation script to Breeze (#23065),1
"KubernetesPodOperator should patch ""already checked"" always (#22734)When not configured to delete pods, at end of task execution the current behavior is to patch the pod as ""already checked"", but only if pod not successful.  We should also patch when successful so it isn't ""reattached"" to after a task clear.",4
Improve speed of `dag.partial_subset` by not deep-copying TaskGroup (#23088)This resulted in the _entire_ dag being copied over and over many times.For a task with 500 dags this takes the time of this function down from60s(!) to just over 1s.,1
Protect against using try_number from context in provider (#23069)This is a (temporary and not perfect - until we figure out a better way)protection against using constructs and interfaces which are notpresent in Airflow 2.1 in providers.Follow-up after #23059,1
Correct default conn ID in WASB connection doc (#23057),2
"Add migration to update DAG default_view (#23091)The scheduler would make this change anyway when it next parses the dag,but up until that point the webserver won't be able to display the homepage with that dag visible.This migration also helps with the new downgrade feature.",1
"Ensure TaskMap only checks ""relevant"" dependencies (#23053)When looking for ""mapped dependants"" of a task, we only want a task ifit not only is a direct downstream of the task, but also it actually""uses"" the task's pushed XCom for task mapping. So we need to peek intothe mapped downstream task's expansion kwargs, and only count it as amapped dependant if the upstream is referenced there.",1
fix link to dbt docs by removing extra h (#23086),4
Fix typo in scheduler_job.py (#23095),2
Improve Graph view task actions for Dynamic Tasks (#23064)* Include state in mapped task dropdown* Rearrange task action IA* List Instances button all runs vs current run,1
"Fix artifact for MyPy checks (#23094)Occasionally MyPy detects errors which have not been detectedbefore. This is likely caused by having too many files passedto MyPY. If the number of files to pass to MyPy is too big,pre-commit will automatically split the list of files intoseveral ""mypy"" commands. If we are unlucky the list of fileswill cause MyPy to detect slightly different errors.We split the mypy checks to be run separately for airflow coreand airflow providers to limit the list of files to be shorter.We are also preparing for splitting off providers so this isgood idea in general.",1
"Fix moto/pyparsing issue. (#23096)Since we use Glue support we need the glue extra to get the correct depsinstalled, namely pyparsing >= 3",1
Allow offline upgrade with no options (#23093)User should be able to do `airflow db upgrade --show-sql-only` when upgradeing to latest revision.,3
Make Grid and Graph buttons consistent. (#23097),1
"Add server default for map_index in Log table (#23056)When logging CLI actions we insert a record into the Log table.  But for 2.3 we add column map_index to Log, and if the Log model expects map_index to be there the insert will fail and a warning will be emitted.We can avoid the error and warning by adding a server_default of NULL on map_index in Log. I choose NULL instead of -1 because generally speaking map_index doesn't make sense for Log tables.",2
"Fix MyPy errors in dev folder (#23100)The files in dev folder were not thoroughly checked before withMyPY - they were added to MyPy check for all files andapparently they were not really verified in isolation.This PR fixes all the problems left and separated mypy checks tobe run separately for ""dev"" folder.",1
Add `S3CreateObjectOperator` (#22758)* Add S3CreateObjectOperatorCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Meaningful error mssage in resolve_template_files (#23027)When fails to get source,1
Fix KPO to have hyphen instead of period (#22982),0
Replace changelog/updating with release notes and towncrier now (#22003),5
"Make copy button blue (#23120)* Make copy button blueOur actions are blue, copy button is an action, therefore it should be blue as well* Bump fontsize",1
"Switch bitnami images in tests to ""standard"" ones (#23122)We aren't using anything special in those images, and all of bitnami'simages on quay.io seem to have disappeared.",1
"Support clearing and updating state of individual mapped task instances (#22958)* Allow marking/clearing mapped taskinstances from the UI* Refactor to straighten up types* Accept multiple map_index param from front endThis allows setting multiple instances of the same task to SUCCESS orFAILED in one request. This is translated to multiple task specifiertuples (task_id, map_index) when passed to set_state().Also made some drive-through improvements adding types and clean someformatting up.* Introduce tuple_().in_() shim for MSSQL compatCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",4
Improve logging of optional provider features messages (#23037)The optional provider features are now better detected and weare just logging an info message in case some missing importsare detected during provider importing hooks.Fixes: #23033,0
Replace `DummyOperator` usage in test_zip.zip and test_zip_invalid_cron.zip (#23123),3
Change ComputeSSH to throw provider import error instead paramiko (#23035)The paramiko import should be done after ssh provider to properlydetect it as an optional Google Provider feature.Part of: #23033,1
"Simplify Task exception trackback truncation to never warn (#23121)There are a few ways we can get an exception before ever making it touser code, and if that happens _also_ warning about ""this shouldn'thappen"" is obfuscating the original error.Before:```[2022-04-20, 14:48:44 BST]  1103834 QueuedLocalWorker-3 {{airflow.models.taskinstance.TaskInstance taskinstance.py:1865}} WARNING - We expected to get frame set in local storage but it was not. Please report this as an issue with full logs at https://github.com/apache/airflow/issues/newTraceback (most recent call last):  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 1442, in _run_raw_task    self._execute_task_with_callbacks(context, test_mode)  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 1546, in _execute_task_with_callbacks    task_orig = self.render_templates(context=context)  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 2210, in render_templates    rendered_task = self.task.render_template_fields(context)  File ""/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py"", line 724, in render_template_fields    unmapped_task = self.unmap(unmap_kwargs=kwargs)  File ""/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py"", line 510, in unmap    op = self.operator_class(**unmap_kwargs, _airflow_from_mapped=True)  File ""/home/ash/code/airflow/airflow/airflow/models/baseoperator.py"", line 390, in apply_defaults    result = func(self, **kwargs, default_args=default_args)  File ""/home/ash/code/airflow/airflow/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 259, in __init__    self.name = self._set_name(name)  File ""/home/ash/code/airflow/airflow/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 442, in _set_name    raise AirflowException(""`name` is required unless `pod_template_file` or `full_pod_spec` is set"")airflow.exceptions.AirflowException: `name` is required unless `pod_template_file` or `full_pod_spec` is setDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 1863, in get_truncated_error_traceback    execution_frame = _TASK_EXECUTION_FRAME_LOCAL_STORAGE.frameAttributeError: '_thread._local' object has no attribute 'frame'[2022-04-20, 14:48:44 BST]  1103834 QueuedLocalWorker-3 {{airflow.models.taskinstance.TaskInstance taskinstance.py:1896}} ERROR - Task failed with exceptionTraceback (most recent call last):  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 1442, in _run_raw_task    self._execute_task_with_callbacks(context, test_mode)  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 1546, in _execute_task_with_callbacks    task_orig = self.render_templates(context=context)  File ""/home/ash/code/airflow/airflow/airflow/models/taskinstance.py"", line 2210, in render_templates    rendered_task = self.task.render_template_fields(context)  File ""/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py"", line 724, in render_template_fields    unmapped_task = self.unmap(unmap_kwargs=kwargs)  File ""/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py"", line 510, in unmap    op = self.operator_class(**unmap_kwargs, _airflow_from_mapped=True)  File ""/home/ash/code/airflow/airflow/airflow/models/baseoperator.py"", line 390, in apply_defaults    result = func(self, **kwargs, default_args=default_args)  File ""/home/ash/code/airflow/airflow/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 259, in __init__    self.name = self._set_name(name)  File ""/home/ash/code/airflow/airflow/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 442, in _set_name    raise AirflowException(""`name` is required unless `pod_template_file` or `full_pod_spec` is set"")airflow.exceptions.AirflowException: `name` is required unless `pod_template_file` or `full_pod_spec` is set```This refactors the approach we use to not need thread local as we alwaystruncate to the same place, but instead to just walk the traceback andexamine the code object until we find the one we want.",1
"Ensure that we don't schedule all mapped TIs when one is cleared (#23130)We had missed a query and when setting TIs to scheduled state we weremistakenly not filtering on map_index, meaning that if you cleared asingle mapped TI it would re-run _all_ of them for that dagrun",2
"When expanding a task end up skipping it, ensure we don't deadlock the DagRun (#23134)* When expanding a task end up skipping it, ensure we don't deadlock the DagRun* Use separate flag to bubble expansion infoInstead of overloading changed_tis for detecting zero-lengthtask-mapping expansion, this instead *not* mark the SKIPPED ti as readyat all, and use a separate flag to trigger re-calculation in thescheduler to pick up the task state update.This may be ever so slightly slower, but should be much easier to makesense of.Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
add script to initialise virtualenv (#22971)Co-authored-by: Jarek Potiuk <jarek@potiuk.com>,5
Task actions UI for individual mapped instances (#23127),2
"Fix TI failure handling when task cannot be unmapped. (#23119)At first glance this looks like a lot of un-related changed, but it isall related to handling errors in unmapping:- Ensure that SimpleTaskInstance (and thus the Zombie callback) knows  about map_index, and simplify the code for SimpleTaskInstance -- no  need for properties, just attributes works.- Be able to create a TaskFail from a TI, not a Task.  This is so that we can create the TaskFail with the mapped task so we  can delay unmapping the task in TI.handle_failure as long as possible.- Change email_alert and get_email_subject_content to take the task so  we can pass the unmapped Task around.",4
Fix timezone display for logs on UI (#23075),2
"KubernetesHook should try incluster first when not otherwise configured (#23126)Currently when K8s hook receives no configuration (e.g. incluster vs config file content vs config file path) the default client generation process will try to load the kube config in the default location.  This is inconsistent with airflow core's behavior in kubernetes executor and kubernetes pod operator (in_cluster=True is the default with those).To make k8s hook's behavior consistent, we can first try incluster, then if that fails, try default kubeconfig.  This should be safe to do.  The kubernetes client will check for 2 environment variables that an in-cluster environment should have and if it doesn't find them, it will raise ConfigException (see here: https://github.com/kubernetes-client/python/blob/1271465acdb80bf174c50564a384fd6898635ea6/kubernetes/base/config/incluster_config.py#L60-L62).  If ConfigException is raised, K8s hook will fall back to looking for the default config.",5
"Change `[api] auth_backends` to be comma separated (#23138)I could infer that this config accepted more than 1 value, but it wasn'tobvious how it was split. This makes it match our other multi-valueconfig options, plus adds it to the docs.This also fixes the docs link referenced in config.",5
Fix error handling in Grid view (#23152),0
Add isLoading to ConfirmDialog (#23155),5
"Fix false warnings re non-JSON extra params (#23157)We were validating JSON-serializability on the fernet-encoded value, which of course won't be JSON-serializable!",5
Add support for multiple codespace configuration (#23158)Today GitHub released support for multiple codespaceconfigurations. This PR adds mysql and sqliteconfigurations on top of the existing postgresone.,5
"recipes documentation: update airflow version (#23148)Not sure if there's a way to generate the version automatically, seems like there's something like that [here](https://github.com/apache/airflow/blob/main/docs/docker-stack/docker-examples/extending/add-providers/Dockerfile#L18) that finally generates the latest version [here](https://airflow.apache.org/docs/docker-stack/build.html#example-of-upgrading-airflow-provider-packages).",2
"Initialize finished counter at zero (#23080)Sets initial count of task finished state to zero.This enables acquiring the rate from zero to one(particularly useful if you want to alert on any failures).We're using the Prometheus statsd-exporter. Since countersare usually used with a PromQL function like `rate`, it's importantthat counters are initialized at zero, otherwise when a taskfinishes the rate function will not have a previous value to comparethe state count to.For example, what we'd like to do:```sum by (dag_id, task_id) (rate(airflow_ti_finish{state='failed'}[1h])) >0```This tells us the failure rate of tasks over time.What I've tried to do instead to ensure the metric captures the changefrom zero to one:```(sum by (dag_id, task_id) (rate(airflow_ti_finish{state='failed'}[1h])) > 0) or sum by (dag_id, task_id) (airflow_ti_finish{state='failed'} != 0 unless (airflow_ti_finish{state='failed'} offset 1m))```Two useful posts on this subject:https://www.robustperception.io/why-predeclare-metricshttps://www.section.io/blog/beware-prometheus-counters-that-do-not-begin-at-zero/Co-authored-by: Bill Franklin <b.franklin@mwam.com>",2
"Fallback Provider's doc URL to ""Documentation"" meta-data (#23012)When Airflow displays provider's Doc URLs it builds theURL to documentation for community providers but for third partyproviders it was wrong.With this change:* if provider already has Documentation standard Project-URL  metadata, Airflow will use it* if provider is an ""apache-airflow"" one, it will dynamically  build the URL from provider info* if neither of two is available it will direct user to the  place in documentation where requirements for custom providers  are explainedFixes: ##22248",0
Clarify guidance on folder locations for newsfragments (#23170),1
"Fix deprecated and updated env var config handling (#23137)Configs that both are have a deprecated ancestor and need to modified,for example `auth_backend` -> `auth_backends` + `session` backend, werebroken when the old key was used via an env var.`get` looks for the deprecated key also and finds it, leading to theunmodified value being used instead of the modified one. Removing theold env var to avoid this issue.We also start setting the new value in an env var so subprocesses see it.This surfaced a bug with `log_filename_template`. We now set the`parsed` value in the modified value, as it ends up in an env var so themodification works properly in subprocesses also.",1
"Fix TaskInstance actions with upstream/downstream (#23153)* Fix Clear+upstream/downstreamWhen we added clearing individual mapped tasks, we unfortunately brokethe up/down stream featureThis was because when passing task_id/task_id+map_index down thatlimited it to _just_ that task_id.So we need to change it to also support the up/downstream we need to addthose tasks to the list we pass on, meaning that we have to supporttask_id and (task_id,map_index) tuples in the same `task_id` list.* Fix Mark Success/FailureSimilar problems as clear.Note we changed the eager loading of DagRun (which is also the default)in to an explicit lazy load to avoid a needless join on to DR for thisfunction.Co-authored-by: Tzu-ping Chung <tp@astronomer.io>",1
Fix typo in dbt Cloud provider description (#23179),1
"Use new Breese for building, pulling and verifying the images. (#23104)We have the new Python-based breeze and we want to replace allthe functionality used from the old Breeze with the new one.The most important is image management. It is used in manyplaces and this PR replaces all the places and removes imagebuilidng, pushing, pulling to use the new Breeze everywhere.That includes:* Building and pushing CI image on CI* Building and pushing PROD image on CI which includes:  * building Airflow packages  * building Provider packages  * waiting for images in parallel (both CI and and PROD)  * veifying the images (both CI and PROD)All those commands have been moved to breeze.py and that requiredsome modification in the Breeze parameter handling - mainly relatedto adding spaces when help was displayed because long list ofpackages looked very bad in help output.It's been easier to implement it in one big PR as usingimage building and pulling was deeply embedded in many scripts.With this change all the scripts in CI use directly breezeand combine using command line parameters with evn variablesdirectly in the job that execute the breeze commands whichmakes it much easier to understand what is going on andrepeat it locally using Breeze.Fixes: #22825Fixes: #23077Fixes: #23076Fixes: #22829Fixes: #22828Fixes: #22826Fixes: #20961Fixes: #23102Fixes: #21098",0
`S3Hook`: fix `load_bytes` docstring (#23182),2
"Fix and improve consistency of checking command return code (#23189)This is an aftermath of #23104 after switchig to docs buildingby breeze, failure of build documentation did not trigger failureof the docs build (but it did trigger main failure of pushingthe documentation).This change improves and simplifies the return code processing andpropagation in the commands executed by breeze - thanks to commonreturncode, stdout, stderr available in both CompletedProcessand CalledProcessError and returning fake CompletedProcess in dry_runmode, we can also satisfy MyPy type check by returning non-optionalUnion of those two types which simplifies returncode processing.This change fixes the error in the docs (lack of empty lines beforeauto-generated extras).All commands have been reviewed to see if the returncode iscorrectly handled where needed.",0
Use the new breeze in CI static checks (#23187)* breeze static-checks is used in CI* static-checks command is removed from ./breeze-legacy* old bash pre-commit for static-checks is removed* names are verified for existence in .pre-commit-config.yaml* the names are used to generate documentation in STATIC_CODE_CHECKS.rst* there is no more need to update STATIC_CODE_CHECKS.rst or  breeze-complete when new static check is added.* The .pre-commmit-config.yaml file is a single source of truth for list  of static checks.Fixes: #21099,0
"Fix constraint generation on CI (#23194)This is another small aftermath after the #23104 - this could notbe tested during PRs because generate-constraints only run inmain in apache/airflow repo and a problem crept in that I haveforgotten to add --run-in-parallel for those breeze commands,which resulted in missing the python version to generateconstraints for.This change adds --run-in-parallell and list of python versionsto work on so that generate constraints might start work again.",1
Add cleanup of docker-context-files (#23197)Seems that some files can be left in the repositories of ours. Thisprevents empty prod images to build propoerly.This change adds --cleanup-docker-context files emptybuild-prod-image on CI,2
Force installing Breeze on CI (#23196)Since our environment is re-used between runs it might be thatdifferent version of Breeze has been installed in the cachepreviously. This change force-installs breeze every time thejob is started to make sure current Breeze version is used.,1
Fix main failure after moto upgrade (#23200)The moto library 3.1.6 extracted MotoAPI to dedicated module(How about SemVer?).https://github.com/spulec/moto/pull/5055This broke our S3/CloudWatch tests.This PR bumps minimum version of Moto to 3.1.6 and switches tothe new module when importing the API.,2
"Unify all ""breeze"" tools under breeze sub-commands (#23193)Experimentally we've separated some CI-only tools like freespaceand find-newer-dependencies as separate scripts (similarly toinitialize-virtualenv) but contrary to the initialize, our toolscan and should be run in Breeze's virtualenv.However we are just in the middle of using breeze commands inthe CI context, the commands are not ""alien"" any more andthey seem to fit the same pattern as other commands.Since we now have the possibility of grouping sub-commmands in separatelogical groups, it makes perfect sense to bring the tools to thebreeze's sub-commands - mostly because of the auto-completesupport and familiarity of using breeze commands in CI.This PR brings both freespace and find-newer-dependencies toolsalready used in CI to become breeze sub-commands. It alsoadds ""fix-ownership"" and ""resource-check"" commands, so that we can runowership fixing on Linux machines (and adds fixing ownership asmandatory stop in all breeze-run jobs in CI and that userscan run resource-check locally.This is because thgere were some cases after introducing Breeze thatsome root-owned left-overs prevented to checkout code by therunner on self-hosted runners.This was likely due to resource-check that was run withoutPYTHONDONTWRITEBYTECODE=1 variable - this variable have beenadded now to execution of the python script.",1
Remove fix-ownership after upload coverage and extra cache (#23203)There is no Breeze installed in upload coverage so fix-ownershipis not needed (and harmful because breeze is missing :))Also extra cache for ~/.local caused overriding of breezeinstallation and it should be removed.,4
"Add cleanup of repository in case there are some leftovers (#23201)This change protects against accidental left-overs (root-owned)that might be created in the repository, in case a job isre-run on a self-hosted runnner from a previous build.",1
"Fix static-checks actually work after Breeze migration (#23202)Static checks were not really ""enabled' after migration since #23193This PR fixes it.",0
Further improvement of Databricks Jobs operators (#23199)This PR includes following changes:* Document missed parameters for `DatabricksSubmitRunOperator` and  `DatabricksRunNowOperator`* Add support for new parameters in `DatabricksRunNowOperator`:  `python_named_parameters` and `idempotency_token`* Rework documentation for both operators based on the feedback from  another PR,5
nIcer handling of cached Breeze parametersThe cached parameters in Python Breeze were largely based onBash implementation. They did the job but required prettycumbersome synchronization of cached values with parameterspassed and it was easy to forget about this as you had todo it sepearately in each method that had potentiallycacheable parameters.In this PR we take advantage of the Click class hiarchy and theirextendability. We've already extended Click Choice parameterto be much better formatted for long list of choices but we takethis a bit further now with adding new type of parameter thatcan cache the values between runs.This has multiple advantages:* we do not have to remember about synchroniation - parameters  automatically read/write their values from cache as they are  used* we can automatically set parameters to default when wrong  value is passed. This is nice as user does not have to  re-run the command because the values are corrected  on-the-flight.* the parameters can have their default values displayed in  help screen (we use sentinel default that we can use to  detect if value was passed from parameter or taken from default)* the parameters can also have their <current> values marked in  list of choices - so the user in the help screen can see not  only the default but also which value is currently selected  and will be used if you do not pass any parameter.,2
"Split breeze commands across multiple filesThe new breeze.py commands grew into separated groups of commands andbreeze.py became far too big.This change is a refactor to split breeze.py into separated groups ofcommands.No new functionality is added, just refactoring to separate files.Fixes: #23204",2
"Fix pushing image cacheAfter converting to `breeze` commands, pushing cache started to failas the image tag was used (but for cache we always build and pushusing 'latest' images.",3
Add longer timeout for execution for flaky lambda invoke test,3
Remove duplicated py37 in dev/breeze/pyproject.toml,5
"When marking future tasks, ensure we don't touch other mapped TIs (#23177)We had a logic bug where if you selected ""Map Index 1"" to mark assuccess, the other mapped TIs of that run would get cleared too.This was because the `partial_dag.clear` was picking up the othermapped instances of the task.Co-authored-by: Jed Cunningham <jedcunningham@apache.org>",2
"Fix renamed .README.md file in clean/check for docker-context-filesThe README.md has been renamed to .README.md in docker-context-filesto make it less prone to accidental deletion, but breeze commandswere not updated to account for that change.",4
Use JiraHook instead of JiraOperator for JiraSensor,1
Fix dag_id extraction for dag level access checks in web ui (#23015)Properly extract dag_id from post form or json body for dag level accesspermissions.Added test case for dag level access.Fixed test_success_fail_for_read_only_task_instance_access to succeeddue to the right reasons.,3
added uninstall apache-airflow-breeze with pipx,1
added spaces for code-block in .rst file,2
added bold to commands,1
Update doc for DAG file processing (#23209)We can now run the ``DagFileProcessorProcess`` in a separate process and it's not fully documentedCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Update Param example code to add a default (#23212)A param without a default value causes import error. Updated this example tobe runnable.,1
Catch `ParamValidationError` in view when triggering a DAG (#23217)dag.create_dagrun can raise `ParamValidationError`. We should capture itwhen manually triggering a dag to avoid breaking the UI,4
Add MSSQL link to contributing quick start.,2
Add MSSQL link to breeze visuals.py and breeze-legacy,2
Allow extra to be nullable in connection payload as per schema(REST API). (#23183),1
Add `endpoint_id` arg to `google.cloud.operators.vertex_ai.CreateEndpointOperator`,1
Add the new parameter to the docstring,2
Add the format of Vertex Endpoint ID to the docstrings,2
Update the TEST_ENDPOINT_ID to use the valid format,1
`GoogleDriveToGCSOperator`: Remove `destination_bucket` and `destination_object`,4
Migrate Datastore system tests to new design (AIP-47)Change-Id: Ibc6f0a03a0c6fb374de85d74a7ac62cf6fa55bec,4
Support serviceAccount attr for dataflow in the Apache beam,5
Add doc and example dag for AWS Step Functions Operators,1
Add links for Cloud Datastore operators,1
Fix pre-commit check,0
Change CloudDatastoreExportEntitiesLink to StorageLink,2
Update unit tests for Datastore operators,1
"Ensure state is updated on task actions (#23221)* Ensure state is updated on task actionsWhen the DAG was paused or autorefresh was off, there was no change when performing a task action. Making it look like it didn't work.Fix: remove the `enabled` option to make sure that anytime the query is invalidated it does refetch data. But also, to prevent a refetch on load with `staleTime`* fix react tests",3
Remove `GCSObjectsWtihPrefixExistenceSensor`,0
Use map_index when clearing not launched tasks in k8s (#23224),1
Create Dataproc operators for GKE,1
Create system test for K8s and dataproc operators,1
Update spelling_wordlist for docs,2
Remove run_in_gke_cluster flag,1
Fix static checks,0
Update system tests,3
Mock project_id for test_delete_cluster_error unit test,3
image building documentation: adding new provider example,1
code review fixes,0
fix Build docs failure,0
Fix X-Frame enabled behaviourThe #19491 incorrectly changed condition on assigning theX-Frame-Options header DENY. It actually was not possible to setthe DENY header.,1
Add doc and sample dag for S3FileTransformOperator,2
Add location support to BigQueryDataTransferServiceTransferRunSensor.,5
"minor callable fix (#23151)On line 132, changes mention of callable from `execution_time` to `execution_timeout` (as I believe is intended here)",4
Add sample dag and doc for S3KeysUnchangedSensor,4
"Fix ""Chain not supported for different length Iterable""",1
"Enable use of custom conn extra fields without prefix (#22607)Previously, connection ""extra"" fields which were added as custom fields in thewebserver connection form had to be named with prefix `extra__<conn_type>__`.This was because custom fields are registered globally on the connection view model,so the prefix was necessary to prevent collisions.But the prefix is ugly and cumbersome in the `extra` field.  So now what we do isadd this prefix when defining the field internally in the model, and strip it whensaving the connection.This doesn't change any providers -- each of those will have to be updated in order to use no-prefix custom fields, with special care to handle backcompat.",0
Add RedshiftCreateClusterOperator,1
DatabricksSqlOperator - switch to databricks-sql-connector 2.x,5
Update to the released version of DBSQL connectorAlso added additional parameters for further customization of connectionif it's required,1
Address review comments,1
Remove unneeded --pip-args from pipx install (#23238),4
Fix doc build failure on main (#23240),0
Update list of non-core files (#23236),2
"`LookerStartPdtBuildOperator`, `LookerCheckPdtBuildSensor` : fix empty materialization id handling (#23025)* fix empty materialization id handling",0
"Remove deprecated `params` from google operators (#23230)* `GoogleDisplayVideo360CreateReportOperator`, `FacebookAdsReportToGcsOperator`: remove `params`",2
`PubSubPullSensor`: Remove `project` and `return_immediately` (#23231)* `PubSubPullSensor`: Remove `project` and `return_immediately`,4
"Bring back deprecated security manager functions (#23243)We need to deprecate these, not remove them, to keep from breaking ourpublic api.",4
"Fix tasks being wrongly skipped by schedule_after_task_execution (#23181)In the reproducing example, once branch finishes, it creates a partial_dagwhich includes `task_a`, `task_b` and `task_d` (but does not include `task_c`because it's not downstream of `branch`). Looking at only this partial_dag, the""mini scheduler"" determines that task_d can be skipped because its onlyupstream task in partial_dag `task_a` is in skipped state. This happens in`DagRun._get_ready_tis()` when calling `st.are_dependencies_met()`.",2
Dataproc : remove `location` in favor of `region` (#23250),4
"In DAG dependency detector, use class type instead of class name, 2nd attempt (#21706)",1
Use inherited 'trigger_tasks' method (#23016)This is a follow-up to #21316 which did not take into account that CeleryExecutoroverrides trigger_tasks and thus would ignore if a task was already running.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,1
Add new committers to allowed list for self-hosted GH runners (#23253),1
Validate conn_type and values for extra_field_name_mapping (#23241),5
"`BigtableCreateInstanceOperator` & `BigtableHook.create_instance` Remove `replica_cluster_id`, `replica_cluster_zone`. (#23251)",4
Create links for Biqtable operators (#23164),1
* `CloudDatastoreImportEntitiesOperator` : Remove `xcom_push`. Please use `BaseOperator.do_xcom_push` (#23252)* `CloudDatastoreExportEntitiesOperator` : Remove `xcom_push`. Please use `BaseOperator.do_xcom_push`,1
"Ensure that DAG calendar view creates the right kind of DateTime objects (#23255)This view has entirely too much logic in it, and it all needs to bemoved somewhere else.",4
Calculate duration in UI (#23259)* Calculate duration in UI* calculate mapped instance duration too,2
Handle undefined data interval in grid runs (#23265),1
"Unify context parameter names for Production image building (#23267)The parameter names for installing airflow from PyPI orcontext were pretty confusing. This change unifies andshortens names of the parameters, so that it is easier toreason about them.If INSTALL_PACKAGES_FROM_CONTEXT is set then the image willinstall packages from docker-context-files (for airflow andproviders it will use constraints as if it was a normalairflow installation).By default Airflow is installed from PyPi or source codedepending on the installation method, but if you alsoset AIRFLOW_IS_IN_CONTEXT = true, it means that Airflow andproviders are already present in the docker-context-filesand no extra installation of Airlfow and Providers isperformed from PyPI or sources.",1
Fix left-over function in breeze-legacy (#23276)Fixes: #23272,0
Remove use of static link in REST API test (#23278),3
add missing docstring in `BigQueryHook.create_empty_table` (#23270),1
Restore Breeze while preparing the images (#23281)When you prepare the release images you clean git repoand it removes Breeze's .egg-info files. Also when you createa new branch you might simply not have the breeze imageavailable which will not allow you to build airflow packages.This adds two steps to release process:* reinstalling Breeze* pulling the Breeze CI image,1
Remove deprecated parameters from BigQueryHook: (#23269)* `BigQueryHook.create_empty_table` Remove `num_retries`. Please use `retry`.* `BigQueryHook.run_grant_dataset_view_access` Remove `source_project`. Please use `project_id`.,1
Add `v2-3-stable` and `v2-3-test` to main (#23277),3
Fix typo in retag_images.py (#23280),2
Remove deprecated parameters from PubSub operators: (#23261)* `PubSubCreateTopicOperator`: Remove `project`. Please use `project_id`* `PubSubCreateSubscriptionOperator`: Remove `topic_project`. Please use `project_id`* `PubSubCreateSubscriptionOperator`: Remove `subscription_project`. Please use `subscription_project_id`* `PubSubDeleteTopicOperator`: Remove `project`. Please use `project_id`* `PubSubDeleteSubscriptionOperator`: Remove `project`. Please use `project_id`* `PubSubPublishMessageOperator`: Remove `project`. Please use `project_id`,1
Remove custom signal handling in Triggerer (#23274)There is a bug in CPython (fixed in March 2022 but not yet released) thatmakes async.io handle SIGTERM improperly by using async unsafefunctions and hanging the triggerer receive SIGPIPE while handlingSIGTERN/SIGINT and deadlocking itself. Until the bug is handledwe should rather rely on standard handling of the signals rather thanadding our own signal handlers. Seems that even if our signal handlerjust run exit(0) - it caused a race condition that led to the hanging.More details:   * https://bugs.python.org/issue39622   * https://github.com/python/cpython/issues/83803Fixes: #19260,0
`CloudBuildCreateBuildOperator`: Remove deprecated `body` parameter (#23263)* `CloudBuildCreateBuildOperator`: Remove deprecated `body` parameter* `CloudBuildCreateBuildOperator`: Remove `body`. Please use `build`,1
Add 2.3.0rc1 to issue templates (#23288),0
We weren't checking .jsx files for licenses and missed one (#23289),2
"Revert ""EdgeModifier refactoring (#21404)"" (#23291)This reverts commit ace8c6e942ff5554639801468b971915b7c0e9b9.Fixes apache/airflow#23285This dag breaks with a cycle as group.end has itself as a downstream otherwise:```pythonfrom pendulum import datetimefrom airflow.decorators import dag, task, task_groupfrom airflow.utils.edgemodifier import Label@taskdef begin():    ...@taskdef end():    ...@dag(start_date=datetime(2022, 1, 1), schedule_interval=None)def task_groups_with_edge_labels():    @task_group    def group():        begin() >> end()    group()_ = task_groups_with_edge_labels()```",5
Add missing licenses and update `.rat-excludes` (#23296),5
Fix retrieval of the right branch in pre-commits (#23297),0
"Update CONTRIBUTORS_QUICK_START.rst with note on pyenv for Mac M1 (#23305)The Contributor's Quick Start [recommends pyenv](https://github.com/apache/airflow/blob/main/CONTRIBUTORS_QUICK_START.rst#pyenv-and-setting-up-virtual-env) to manage environments when developing on Airflow. There are [lots of issues trying to get pyenv to work on M1 the Mac M1 chip](https://www.google.com/search?q=pyenv+m1+mac+site:stackoverflow.com&client=firefox-b-1-d&channel=nus5&sa=X&ved=2ahUKEwjVqMyUhrX3AhVVRTABHc8vB_YQrQIoBHoECAsQBQ&biw=1744&bih=942), so it might be worth including a note on a pyenv alternative.",1
Add `2.3.0rc2` to issue templates (#23298),0
Fix typos in README.md and airflow_doc_issue_report.yml (#23294),5
Fix empty image preparation (#23304)Empty image preparation failed in CI because it was impossible tobuild an empty image without buildkit. This change sets DOCKER_BUILDKITvariable for empty image build which make it always use the buildkit.,1
Migrate gcs to new system tests design (#22778),3
"Add DAG cycle test for Label use within Task Groups (#23300)To guard against any future regressions, this PR adds a unit test toensure that a DAG cycle is not detected when using Labels alongsidetasks contained in Task Groups.",1
Use <Time /> in Mapped Instance table (#23313)The start/end dates for a mapped instance weren't updating when a user changed their timezone. Using <Time /> fixes that,0
Add is_mapped field to Task response. (#23319)* Add is_mapped field to Task response.* Add is_mapped to schema file and add test for GetTasks.,1
Fix HiveToMySqlOperator's wrong docstring (#23316)Replace `metastore_conn_id` with `hiveserver2_conn_id`,2
"update processor to fix broken download URLs (#23299)merging because it has the ""Ok to merge"" and all non-skipped tests passed!",4
`DatastoreHook`: Remove `datastore_conn_id` (#23323)* `DatastoreHook`: Remove `datastore_conn_id`,5
Add tags inside try block. (#21784),1
Unify style of communication with the users for Breeze. (#23311)Fixes: #22906,0
"`GCSFileTransformOperator`: New templated fields `source_object`, `destination_object` (#23328)* `GCSFileTransformOperator`: New templated fields `source_object`, `destination_object`* Add `source_object`, `destination_object` as templated fields* fix docstringcloses: #23327",2
Don't show grid actions if server would reject with permission denied (#23332)* Add edit permission check for grid actions* Remove if wrapper for meta tag* Use dag.can_edit,2
Use run_id for ti.mark_success_url (#23330),1
Update Airflow Release Doc (#23322)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Fix update user auth stats (#23314),1
"Remove confusion about upgrade-to-newer-dependencies breeze param (#23334)The ""upgrade-to-newer-dependencies"" in the image can take ""false""""true"" and <RANDOM> value (the RANDOM value is used to alwaysinvalidate docker cache).This has been carried to Breeze command line, but this was a sourceof major confusion as the name of the parameter suggest bool value.The change modifies the parameter to be flag, and when the flagis set the parameter is set to random during image building.That should help with recent bug when image was always rebuiltwithout checking if it should be rebuilt.",0
resolving conflict (#23052),5
Hide some task instance attributes (#23338),2
"Cleaner default output when breeze starts (#23341)There was a bit of noise printed when Breeze started:* information about branch/python/image/backend used* information about actions performed (like fixing permissions)* information that docke image build is not needed* warnings about missing variablesThis PR marks all the messages as ""info"" and only prints themwhen --verbose flag is used and it adds default values for thevariables that generated warnings.",2
"Google provider: Remove `bigquery_conn_id`, `google_cloud_storage_conn_id` (#23326)* `bigquery_conn_id` is removed. Please use `gcp_conn_id`.  affected classes:  `BigQueryCheckOperator`  `BigQueryCreateEmptyDatasetOperator`  `BigQueryDeleteDatasetOperator`  `BigQueryDeleteTableOperator`  `BigQueryExecuteQueryOperator`  `BigQueryGetDataOperator`  `BigQueryHook`  `BigQueryIntervalCheckOperator`  `BigQueryTableExistenceSensor`  `BigQueryTablePartitionExistenceSensor`  `BigQueryToBigQueryOperator`  `BigQueryToGCSOperator`  `BigQueryUpdateTableSchemaOperator`  `BigQueryUpsertTableOperator`  `BigQueryValueCheckOperator`  `GCSToBigQueryOperator`* `google_cloud_storage_conn_id` is removed. Please use `gcp_conn_id`.  affected classes:  `ADLSToGCSOperator`  `BaseSQLToGCSOperator`  `CassandraToGCSOperator`  `GCSBucketCreateAclEntryOperator`  `GCSCreateBucketOperator`  `GCSDeleteObjectsOperator`  `GCSHook`  `GCSListObjectsOperator`  `GCSObjectCreateAclEntryOperator`  `GCSToBigQueryOperator`  `GCSToGCSOperator`  `GCSToLocalFilesystemOperator`  `LocalFilesystemToGCSOperator`",5
"Fix connection test button (#23345)The connection test button was always disabled if any of your hooks hadimport errors, for example because of a missing module. This handlesthat scenario.",0
Store grid view selection in url params (#23290)* Add url params for dag_run_id and task_id* Persist other search params* simplify useSelection* delete extra params* remove API change,4
Fix regeneration of breeze screenshots (#23344),2
Dataproc: Remove default value of `region` (#23350)* `region` parameter has no default value.  affected functions/classes:  `DataprocHook.cancel_job`  `DataprocCreateClusterOperator`  `DataprocJobBaseOperator`  * `DataprocJobBaseOperator`: order of parameters has changed,4
`S3ToGCSOperator`: Remove `dest_gcs_conn_id` (#23348),4
Remove redundant docstring in `BigQueryUpdateTableSchemaOperator` (#23349),5
Organize Tableau classes (#23353)* Organize Tableau classes,2
Allow for LOGGING_LEVEL=DEBUG (#23360),2
"Fix broken task instance link in xcom list (#23367)* Fix broken task instance link in xcom listAdd execution date back to the xcom list to be able to pass to the `task_instance_link()` function.Long term, we should swap out the execution_date param for run_id* Make execution date a search column",5
Fix mssql in the new Breeze (#23368)The new Breeze did not use conditionally debian version to runMsSQL docker compose.This PR fixes it,0
"Fix ``KubernetesPodOperator`` with `KubernetesExecutor`` on 2.3.0 (#23371)KubernetesPodOperator was mistakenly trying to reattach to it'sKubernetesExecutor worker, where it would get stuck watching itself forlogs. We will properly filter for KPO's only, and ignoreKubernetesExecutor workers for good measure.",1
Allow back script_location in Glue to be None (#23357),1
"When exec fails in breeze we do not print stack-trace (#23342)When you run exec and breeze is not running, there was a stacktrace printed rather than straightforward error message.This fixes it - stacktrace is only printed now when verbose isused. If not just error message is printed.",0
Prepare documentation for cncf.kubernetes 4.0.1 release (#23374),2
Add missing --for-production parameter for new breeze docs building (#23376),2
Add doc notes for keyword-only args for `expand()` and `partial()` (#23373),2
fix cli `airflow dags show` for mapped operator (#23339),1
Fix duplicated Kubernetes DeprecationWarnings (#23302),2
Add YANKED to yanked releases of the cncf.kubernetes (#23378),1
Bigquery assets (#23165),1
Override pool for TaskInstance when pool is passed from cli. (#23258),4
Few fixes in the providers release doc (#23382),2
"Update multiplatform doc image tagging to use regctl (#23383)You cannot use 'docker tag' to move multiplatform image, insteadwe should use regctl.",1
Fix deriving of PyPI branch from airflow version (#23380)The regexp expression for deriving right branch from version wasmissing . for version numbers.,0
Clarify `reattach_on_restart` behavior (#23377),5
Update missing `version_added` in config.yml (#23387),5
`2.3.0` has been released (#23385),5
Chart: Update default airflow version to `2.3.0` (#23386),5
Add missing steps to release process (#23384),1
`DataprocHook`: Remove deprecated function `submit` (#23389),1
Fix attempting to reattach in `ECSOperator` (#23370)* Updated ecs_task_id on reattaching,5
add auto refresh to dags home page (#22900)* add auto refresh to dags home page* fix lint errors* fix lint* stop refresh when page is not focused or no active dag runs. change css for layout* remove margin for refresh switch* Update airflow/www/static/css/main.cssCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* fix text case* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>* add comment on refresh interval* refactor last dag run handler date updateCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,5
Fix `version_added` for `[sensors] default_timeout` (#23388),1
Allow multiline text in private key field for Snowflake (#23066),1
Clarify 2.3.0 kubernetes min version is about library not cluster (#23398)It was not clear from the release notes that the minimum versionof the kubernetes was about the library rather than cluster version.Also README was not updated with min versions of Kubernetes clusterfor 2.3.0 version.,5
Add better description for Breeze customization (#23397),1
Remove tagging of `constraint-x-y` branch from release process (#23399),4
Ignore some files/directory when releasing source code (#23325),2
"Improve handling of entry and exit to common Breeze commands (#23395)This PR improves handling of both entry and exit to commonBreeze commands:* at entry all common commands check if rebuild of image is needed* when you exit and there is an error from shell commands, rather  than printing stack trace an error message is printed",0
Validate DAG owner to be a string (#23359)non-string values raise `AttributeError` as `task.owner.lower` is called with `task.owner` not being a string and the error is not passed as import error failing silently. Raise explicit error will be helpful to the user.closes: #23343related: #23343,1
Cleanup Google provider CHANGELOG.rst (#23390),4
"Fix ""breeze-legacy"" after building images was removed (#23404)The `breeze-legacy` stopped working after building images wereremoved as few parameters were still checked for allowed valuesbut they were missing,This PR fixes it by removing the parameters.",2
"Mark image as refreshed when pulled on CI (#23410)One of the recent changes in Breeze (#23395) caused unnecessaryrebuilding of image when ""build-docs"" is run. On CI we buildimage once and reuse it. However in case of Buld docs we missedinformation that the image is ""fresh"" and we started rebuildingit. This change marks the image as ""refreshed"" when it is pulledwith `--tag-as-latest` flag (which happens in CI).",3
Improve react www tests (#23329)* Add shared test wrapper & treeData placeholder* remove console log* move testUtils to /utils* change unnamed export,4
Refactor code references from tree to grid (#23254),4
Add fields to dagrun endpoint (#23440)* Add below fields to dagrun endpoint :* data_interval_start* data_interval_end* last_scheduling_decision* run_type* Refactor hardcoded dates with constants.,5
Add backward compatibility for core__sql_alchemy_conn__cmd (#23441),1
"Improve verbose output of Breeze (#23446)When you add --verbose or --dry-run options to breeze it willprint the commands it is executing (or is supposed to in dry-runmode). The output contains environment variables as theyoften contain crucial information to execute the command (forexample in docker-compose run commands it contains COMPOSE_FILEvariable which is the list of compose files that are used). Thisis done in a fashion that you can copy the whole command andexecute it, but it very unfriendly for visual inspection asall the variables were printed in one line and in semi-randomorder and also the variables contained often all system variablesset by the shell before.This change keeps the proerty of ""we can copy&paste the commandand run it"" but it improves the visual aspect of it:1) each env variable is kept in one line2) first all system variables are printed and then variables that   were specifically added for this command3) variables in each group are sorted alphabetically which helps   in finding the variable you are looking for when you visually   inspect the output.",1
[FIX] remove python 3.6 (#23409),4
Fix code-snippets in google provider (#23438),1
Add support for topologySpreadConstraints to Helm Chart (#22712),2
Support annotations on volumeClaimTemplates (#23433),1
Fix `check_files.py` to work on new minor releases (#23287),1
Optimize 2.3.0 pre-upgrade check queries (#23458)We have to check for rows that are missing either corresponding TI or DR and move them out of table before adding FKs.  We were doing correlation in the JOIN condition but it appears postgres does *not* like this so here we move correlation to WHERE.,4
Docs: Python 3.10 is now supported (#23457),1
Unify approach for user questions asked in Breeze (#23335)This change documents and unifies the approach we've taken forthe user inut handling when it comes to confirmation questions.,5
Show warning if '/' is used in a DAG run ID (#23106),1
"Move non-opencontainer labeling of the image to breeze from Dockerfile (#23379)* Extract ""extra"" labeling of the image to breeze from DockerfileFixes: #21046* Add more ArtifictHub-specific labelsCo-authored-by: Kamil Breguła <kamilbregula@apache.org>",1
Bump pre-commit hook versions (#22887),1
Add Python 3.10 trove classifier (#23464),1
Remove remaining Python3.6 references (#23474),5
Remove color change for highly nested groups (#23482),4
"Ensure the messages from migration job show up early (#23479)The default for python is to buffer stdout, which means that log linesmight now show up in the output straight away (until a certain number oflines or number of bytes of output have been written) -- this isespecially problematic if the pre-migration checks taking a long time asit makes it look like it has hung",1
Visually distinguish task group summarys (#23488)Bold task groups names and darken their bottom row border.,5
Fix literal cross product expansion (#23434),0
TextToSpeech assets & system tests migration (AIP-47) (#23247),3
Add support for timezone as string in cron interval timetable (#23279),1
CloudTasks assets & system tests migration (AIP-47) (#23282),3
Adds resultBackendSecretName warning in Helm production docs (#23307),2
Add doc and example dag for Amazon SQS Operators (#23312),1
Add Stackdriver assets and migrate system tests to AIP-47 (#23320)Change-Id: I6f751e6576f57a89a5145aeb05f506da8a22b379Co-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,4
"Use kubernetes queue in kubernetes hybrid executors (#23048)When using ""hybrid"" executors (`CeleryKubernetesExecutor` or `LocalKubernetesExecutor`),then the `clear_not_launched_queued_tasks` mechnism in the `KubernetesExecutor` canreset the queued tasks, that were given to the other executor. `KuberneterExecutor` should limit itself to the configured queue when working in the""hybrid"" mode.",1
Changed word 'the' instead 'his' (#23493),4
Replace DummyOperator references in docs (#23502),2
Expand/collapse all groups (#23487)* Add expand/collapse all groups button to Grid* add tests* add comments* Switch to 2 icon buttonsDisable buttons if all groups are expanded or collapsed* Update localStorage key,5
Move tests command in new breeze (#23445),1
Fix cassandra to 3.0.25 (#23522)fix cassandra to 3.0.25 as latest 3.0 (3.0.26) does not start cleanly,4
Add `OpsgenieDeleteAlertOperator` (#23405)* Add `OpsgenieDeleteAlertOperator`,4
"Only count bad refs when `moved` table exists (#23491)This keeps the logic to fail without upgrading when (A) there are bad rows and(B) the ""moved"" table already exists. But we optimize so that we don't countthe bad rows unless the ""moved"" table is there. Previously we counted always,but the first time a user attempts upgrade, the tables won't be there sothere's no point in counting.Instead what we do is skip right to the CTAS, creating the _airflow_movedtables. If there aren't any rows in the ""moved"" table, then we delete the tableimmediately.Also included here is a delete optimization, where we join to the moved tableinstead of running the not exists query again.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
"Change approach to finding bad rows to LEFT OUTER JOIN. (#23528)Rather than sub-selects (two for count, or one for the CREATE TABLE).For a _large_ database (27m TaskInstances, 2m DagRuns) this takes thetime from 10minutes to around 3 minutes per table (we have 3) down to 3minutes per table. (All times on Postgres.)Before:```sqlCREATE TABLE _airflow_moved__2_3__dangling__rendered_task_instance_fields ASSELECT  rendered_task_instance_fields.dag_id AS dag_id,  rendered_task_instance_fields.task_id AS task_id,  rendered_task_instance_fields.execution_date AS execution_date,  rendered_task_instance_fields.rendered_fields AS rendered_fields,  rendered_task_instance_fields.k8s_pod_yaml AS k8s_pod_yaml +FROM  rendered_task_instance_fieldsWHERE  NOT (    EXISTS (      SELECT        1      FROM        task_instance        JOIN dag_run ON dag_run.dag_id = task_instance.dag_id        AND dag_run.run_id = task_instance.run_id      WHERE        rendered_task_instance_fields.dag_id = task_instance.dag_id        AND rendered_task_instance_fields.task_id = task_instance.task_id        AND rendered_task_instance_fields.execution_date = dag_run.execution_date    )  )```After:```sqlCREATE TABLE _airflow_moved__2_3__dangling__rendered_task_instance_fields ASSELECT  rendered_task_instance_fields.dag_id AS dag_id,  rendered_task_instance_fields.task_id AS task_id,  rendered_task_instance_fields.execution_date AS execution_date,  rendered_task_instance_fields.rendered_fields AS rendered_fields,  rendered_task_instance_fields.k8s_pod_yaml AS k8s_pod_yaml +FROM  rendered_task_instance_fields  LEFT OUTER JOIN dag_run ON rendered_task_instance_fields.dag_id = dag_run.dag_id  AND rendered_task_instance_fields.execution_date = dag_run.execution_date  LEFT OUTER JOIN task_instance ON dag_run.dag_id = task_instance.dag_id  AND dag_run.run_id = task_instance.run_id  AND rendered_task_instance_fields.task_id = task_instance.task_idWHERE  task_instance.dag_id IS NULL  OR dag_run.dag_id IS NULL;```",2
Update docs Amazon Glacier Docs (#23372),2
TrinoHook add authentication via JWT token and Impersonation  (#23116)* added trino authentication via JWT token and impersonation* added test cases for jwt verification in trino* added documenation for trino hook,1
"Seperate provider verification as standalone breeze command (#23454)This is another step in simplifying and converting to Python all ofthe CI/local development tooling.This PR separates out verification of providers as a separatebreeze command `verify-provider-packages`. It was previously part of""prepare_provider_packages.py"" but it has been nowextracted to a separate in-container python file and it waswrapped with breeze's `verify-provider-packages` command.No longer provider verification is run with ""preparing provider docs""nor ""preparing provider packages"" - it's a standaline command.This command is also used in CI now to run the tests:* all provider packages are built and created on CI together with  airflow version* the packages are installed inside the CI image and providers are  verified* the 2.1 version of Airflow is installed together with all 2.1  - compatible providers and provider verification is run there too.This all is much simpler now - we got rediof some 500 lines of bashcode again in favour of breeze python code.Fixes: #23430",0
Replace `pytest.mark.xfail` in Postgres tests (#23541),3
"Fix accidental including of providers in airflow package (#23552)The change #23454 accidentally remove INSTALL_PROVIDERS_FROM_SOURCESsetting to ""false"" which resulted in airflow package containing allproviders. This has been caught by our tests (but it was onlyvisible after merging)This PR brings the variable back.",3
Update the Athena Sample DAG and Docs (#23428)* Update the Athena Sample DAG and Docs,2
Change chart annotation generator to use RELEASE_NOTES (#23549),1
Fix LocalFilesystemToS3Operator and S3CreateObjectOperator to support full s3:// style keys (#23180)* Fix LocalFilesystemToS3Operator and S3CreateObjectOperator.Support full s3:// style keys* Fix spelling error,0
"Add logging in to Github Registry for breeze pull (#23551)All of the Airlfow Images are Public in ghcr.io but default settingfor iamges is ""private"" and when users want to build CI workflowsin their forks, had to manually change their images to Public, sothat ci.yml workflow can pull the images prepared in the build-imagesworkflow.This PR adds logging in for `breeze pull` command when GITHUB_TOKENis available, also the workflow gets packages: read permissions.This way ci should works in forks of users without any action fromuser except first-time workflow enabling.",0
"Add IPV6 form of the address in cassandra status check (#23537)This PR fixes problem introduced in 3.0.26 of cassandra image whichadds square brackets around IP address regardless of its type.The problem was workarounded by pinning cassandra to 3.0.25 inthe ##23522 as a quick fix, but this one introducec permanent,future-proof solution.Based on discussion in https://issues.apache.org/jira/browse/CASSANDRA-17612Fixes: #23523",0
"Refactor Breeze to group related methods and classes together (#23556)This change refactors Breeze classes to more consistent approach.* The ""commands"" package only contains commands* All Parameters (BuildCi, BuildProd, BuildDoc, Shell) are now  in ""params"" package* Required/Optional Build args are now members of the  BuildCiParams, BuildProdParams which makes the params  much more self-contained..* All utils are in ""utils"" packageThis helps with avoiding circular imports (all utios are nowstandalone and do not use any of the commands.Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>",1
"Fix _PIP_ADDITIONAL_REQUIREMENTS case for docker-compose (#23517)Recent versions of Airflow do not allow to run `pip install` asroot but the `init` job runs as root so when the variable_PIP_ADDITIONAL_REQUIREMENTS is set, the init container fails.This PR forces _PIP_ADDITIONAL_REQUIREMENTS to be empty for the initjob.",5
Add slim images to release process (#23391)This PR adds slim images to release process of Airflow.Those images are small as they do not contain any extras.Fixes: #20849,0
Tests for provider code structure (#23351)Improved test for code structure that can be re-used among various providders.,1
Fix GCSToGCSOperator ignores replace parameter when there is no wildcard (#23340),2
Move dag_processing.processor_timeouts to counters section (#23393),2
wasb hook: user defaultAzureCredentials instead of managedIdentity (#23394)Co-authored-by: Sanjay Pillai <sanjaypillai11 [at] gmail.com>,1
Amazon Sagemaker Sample DAG and docs update (#23256),5
Opsgenie: Fix `close_alert` to properly send `kwargs` (#23442),0
Fix `PostgresToGCSOperator` does not allow nested JSON (#23063)* Avoid double json.dumps for json data export in PostgresToGCSOperator.* Fix CI,0
Fix conn close error on retrieving log events (#23470)related: [#23469] (https://github.com/apache/airflow/issues/23469).,0
"Temporarily pin xmltodict to 0.12.0 to fix main failure (#23577)The xmltodict 0,13.0 breaks some tests and likely 0.13.0 is buggyas the error is ValueError: Malformatted input.We pin it to 0.12.0 to fix the main failing.Related: #23576",0
Update dags.rst (#23579)Update missing bracket,5
Fix dag-processor fetch metabase config (#23575),5
"tHe output of commands of Breeze are only generated when they change (#23570)Previously we generated output of all the commands from Breeze always,hoping that they will be the same, but rich already had two changesin the format of the SVG files which made the output different andbreaking our PRs.Temporarily we pinned rich to fix the output, but better solution isto get the hash of all the configuration options and see if it changed,and only run generation when it did. This way we keep automatedgeneration on pre-commit but we are protected from accidental changeof the output.We also remove the rich limits and regenerated all svg files to onesgenerated by 12.4.0. Also found a way to run the check if we shouldrun generation at all in pre-commit without prior installing breeze.Fixes: #22908",0
Fixed option name in Breeze description (#23582),0
Add support for queued state in DagRun update endpoint. (#23481),5
"Fix scheduler crash when expanding with mapped task that returned none (#23486)When task is expanded from a mapped task that returned no value, itcrashes the scheduler. This PR fixes it by first checking if there'sa return value from the mapped task, if no returned value, then errorin the task itself instead of crashing the scheduler",0
Add `device_requests` parameter to `DockerOperator` (#23554)* Expose device_requests to DockerOperatorCo-authored-by: Tedi Papajorgji <tedi.papajorgji@hotmail.com>,2
Pools with negative open slots should not block other pools (#23143),5
Fix `PythonVirtualenvOperator` templated_fields (#23559)* Fix `PythonVirtualenvOperator` templated_fieldsThe `PythonVirtualenvOperator` templated_fields override `PythonOperator` templated_fields which caused functionality not to work as expected.fixes: https://github.com/apache/airflow/issues/23557,0
Fix broken dagrun links when many runs start at the same time (#23462)* Load requested dagrun even when there are many dagruns at (almost) the same time* Fix code formatting issues,0
Add default 'aws_conn_id' to SageMaker Operators #21808 (#23515),1
Update sample dag and doc for Datasync (#23511),5
Clean up in-line f-string concatenation (#23591),4
"Apply specific ID collation to root_dag_id too (#23536)In certain databases there is a need to set the collation for ID fieldslike dag_id or task_id to something different than the database default.This is because in MySQL with utf8mb4 the index size becomes too big forthe MySQL limits. In past pull requests this was handled[#7570](https://github.com/apache/airflow/pull/7570),[#17729](https://github.com/apache/airflow/pull/17729), but theroot_dag_id field on the dag model was missed. Since this field is usedto join with the dag_id in various other models ([andself-referentially](https://github.com/apache/airflow/blob/451c7cbc42a83a180c4362693508ed33dd1d1dab/airflow/models/dag.py#L2766)),it also needs to have the same collation as other ID fields.This can be seen by running `airflow db reset` before and after applyingthis change while also specifying `sql_engine_collation_for_ids` in theconfiguration.Other related PRs[#19408](https://github.com/apache/airflow/pull/19408)",5
Add doc and sample dag for EC2 (#23547),2
Helm chart 1.6.0rc1 (#23548),2
Add sample dag and doc for S3ListOperator (#23449)* Add sample dag and doc for S3ListOperator* Fix doc,2
19943 Grid view status filters (#23392)* Move tree filtering inside react and add some filters* Move filters from context to utils* Fix tests for useTreeData* Fix last tests.* Add tests for useFilters* Refact to use existing SimpleStatus component* Additional fix after rebase.* Update following bbovenzi code review* Update following code review* Fix tests.* Fix page flickering issues from react-query* Fix side panel and small changes.* Use default_dag_run_display_number in the filter options* Handle timezone* Fix flaky testCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,3
"Improve caching for multi-platform images. (#23562)This is another attempt to improve caching performance formulti-platform images as the previous ones were undermined by abug in buildx multiplatform cache-to implementattion that causedthe image cache to be overwritten between platforms,when multiple images were build.The bug is created for the buildx behaviour athttps://github.com/docker/buildx/issues/1044 and until it is fixedwe have to prpare separate caches for each platform and push themto separate tags.That adds a bit overhead on the building step, but for now it isthe simplest way we can workaround the bug if we do not want tomanually manipulate manifests and images.",0
Use inclusive words in apache airflow project (#23090),1
Add exception to catch single line private keys (#23043),1
Add sample dag and doc for S3ListPrefixesOperator (#23448)* Add sample dag and doc for S3ListPrefixesOperator* Fix static checks,0
Update min requirements for rich to 12.4.1 (#23604),1
Add exportContext.offload flag to CLOUD_SQL_EXPORT_VALIDATION. (#23614),5
"Make Breeze help generation indepdent from having breeze installed (#23612)Generation of Breeze help requires breeze to be installed. Howeverif you have locally installed breeze with different dependenciesand did not run self-upgrade, the results of generation of theimages might be different (for example when different richversion is used). This change works in the way that:* you do not have to have breeze installed at all to make it work* it always upgrades to latest breeze when it is not installed* but this only happens when you actually modified some breeze code",3
Add Quicksight create ingestion Hook and Operator (#21863)* Add Quicksight create ingestion Hook and OperatorCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Add slim images to docker-stack docs index (#23601),2
Fixed Kubernetes Operator large xcom content Defect  (#23490),1
[FEATURE] google provider - split GkeStartPodOperator execute (#23518),1
Implement send_callback method for CeleryKubernetesExecutor and LocalKubernetesExecutor (#23617),5
"Fix: Exception when parsing log #20966 (#23301)* UnicodeDecodeError: 'utf-8' codec can't decode byte 0xXX in position X: invalid start byte  File ""/opt/work/python395/lib/python3.9/site-packages/airflow/hooks/subprocess.py"", line 89, in run_command    line = raw_line.decode(output_encoding).rstrip()            # raw_line ==  b'\x00\x00\x00\x11\xa9\x01\n'UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa9 in position 4: invalid start byte* Update subprocess.py* Update subprocess.py* Fix:  Exception when parsing log #20966* Fix:  Exception when parsing log #20966  Another alternative is: try-catch it. e.g.```            line = ''            for raw_line in iter(self.sub_process.stdout.readline, b''):                try:                    line = raw_line.decode(output_encoding).rstrip()                except UnicodeDecodeError as err:                    print(err, output_encoding, raw_line)                self.log.info(""%s"", line)```* Create test_subprocess.sh* Update test_subprocess.py* Added shell directive and license to test_subprocess.sh* Distinguish between raw and decoded lines as suggested by @uranusjr* simplify testCo-authored-by: muhua <microhuang@live.com>",3
Make provider doc preparation a bit more fun :) (#23629)Previously you had to manually add versions when changelog wasmodified. But why not to get a bit more fun and get the versionsbumped automatically based on your assesment when reviewing theprovideers rather than after looking at the generated changelog.,4
"Prevent KubernetesJobWatcher getting stuck on resource too old (#23521)* Prevent KubernetesJobWatcher getting stuck on resource too oldIf the watch fails because ""resource too old"" theKubernetesJobWatcher should not retry with the same resource versionas that will end up in loop where there is no progress.* Reset ResourceVersion().resource_version to 0",1
[FEATURE] update K8S-KIND to 0.13.0 (#23636),5
[FEATURE] add K8S 1.24 support (#23637),1
Fix typo issue (#23633),0
"Fix assuming ""Feature"" answer on CI when generating docs (#23640)We have now different answers posisble when generating docs, andfor testing we assume we answered randomly during the generationof documentation.",2
Simplify flash message for _airflow_moved tables (#23635)Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Add index for event column in log table (#23625),2
"Don't run pre-migration checks for downgrade (#23634)These checks are only make sense for upgrades.  Generally they exist to resolve referential integrity issues etc before adding constraints.  In the downgrade context, we generally only remove constraints, so it's a non-issue.",0
Added postgres 14 to support versions(including breeze) (#23506)* Added postgres 14 to support versions(including breeze),1
"Add `RedshiftDeleteClusterOperator` support (#23563)Add support for `RedshiftDeleteClusterOperator`. This will help to clean resources using airflow operators when needed. In the current implementation, By default, I'm waiting until the cluster is completely removed to return immediately without waiting set `wait_for_completion` param to False- Add operator class- Add basic unit test- Add an example task- Add relevant documentation",2
"Added kubernetes version (1.24) in README.md(for Main version(dev)), … (#23649)* Added kubernetes version (1.24) in README.md(for Main version(dev)), accidentally removed in merge cnflict.* Update README.mdCo-authored-by: Jarek Potiuk <jarek@potiuk.com>",2
Fixed test and remove pytest.mark.xfail for test_exc_tb (#23650),3
Fix k8s pod.execute randomly stuck indefinitely by logs consumption (#23497) (#23618),2
[FEATURE] google provider - BigQueryInsertJobOperator log query (#23648),2
Rename cluster_policy to task_policy (#23468)* Rename cluster_policy to task_policy* rename task_policy as example_task_policy.,5
"Revert ""Fix k8s pod.execute randomly stuck indefinitely by logs consumption (#23497) (#23618)"" (#23656)This reverts commit ee342b85b97649e2e29fcf83f439279b68f1b4d4.",4
Prepare provider documentation 2022.05.11 (#23631)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
AIP45 Remove dag parsing in airflow run local (#21877),1
remove `--` in `./breeze build-docs` command (#23671),2
Synchronize support for Postgres and K8S in docs (#23673)We just added support for Postgres 14 and K8S 1.24 and since wedid not have any changes to support either in main we are bringingthe support to 2.3 line as well.This documentation syncs all remaining places where it should beupdated.,5
Migrate Dataproc to new system tests design (#22777),3
Add wildcard possibility to `package-filter` parametere (#23672)the glob parameters (for example `apache-airflow-providers-*`) didnot work because only fixed list of parameters was allowed.This PR converts the package-filter parameter to stop verifying thevalue passed - so autocomplete continues to work but you shouldstill be able to use glob.It also removes few places where the parameters were used with`--` separator.,1
"Replace ""absolute()"" with ""resolve()"" in pathlib objects (#23675)TIL that absolute() is an undocumented in Pathlib and that weshould use resolve() instead.So this is it.",0
Upgrade `pip` to latest released 22.1.0 version (#23665)We are finally able to get rid of the annoying false-positivewarnings and we have finally a chance on having warning-freeinstallation during docker builds.,2
"Shorten max pre-commit hook name length (#23677)When names are too long, pre-commit output looks very ugly and takes up 2x lines. Here I reduce max length just a little bit further so that pre-commit output renders properly on a macbook pro 16"" with terminal window splitting screen horizontally.",2
remove stale serialized dags (#22917),2
"Move around overflow, position and padding (#23044)",1
Fix expand/collapse all buttons (#23590)* communicate via customevents* Handle open group logic in wrapper* fix tests* Make grid action buttons sticky* Add default toggle fn* fix splitting task id by '.'* fix missing dagrun ids,2
Update doc and sample dag for Quicksight (#23653),2
Use func.count to count rows (#23657),1
"Add git_source to DatabricksSubmitRunOperator (#23620)The existing `DatabricksSubmitRunOperator` is extended with the support for the `git_source` parameter which allows users to run notebook tasks from files committed to git repositories.If specified, any notebook task that is part of the payload will clone the repository and check out the commit, tag, or the tip of the specified branch. This is an alternative to dev repos ([docs](https://docs.databricks.com/repos/index.html)) where the checkout/update would have to be triggered manually.Public documentation for the feature available here: https://docs.databricks.com/dev-tools/api/latest/jobs.html (NB: as noted in the docs, the feature is currently in public preview).",2
Disable Flower by default from docker-compose (#23685),2
Fix property name in breeze Shell Params (#23696)The rename from #23562 missed few shell_parms usage where italso should be replaced.,2
Clarify that bundle extras should not be used for PyPi installs (#23697)The bundle extras we have are only used for development and theyshould not be used to install airflow from PyPI. This updateto documentation clarifies it.Closes: #23692,2
"Add environment check and build image check for more Breeze commands (#23687)Several commands of Breeze depends on docker, docker composebeing available as well as breeze image. They will workfine if you ""just"" built the image but they might benefitfrom the image being rebuilt (to make sure all latestdependencies are installed in the image). The common checksdone in ""shell"" command for that are now extracted to commonutils and run as first thing in those commands that need it.",1
Add UI tests for /utils and /components (#23456)* Add UI tests for /utils and /components* add test for Table* Address PR feedback* Fix window prompt var* Fix TaskName test from rebase* fix lint errors,0
Add slim image to docs/docker-stack/README.md (#23710),2
Use profiles to disable flower in docker-compose (#23709),2
Ensure execution_timeout as timedelta (#23655),5
Handle invalid date parsing in webserver views. (#23161)* Handle invalid date from query parameters in views.* Add tests.* Use common parsing helper.* Add type hint.* Remove unwanted error check.* Fix extra_links endpoint.,2
Add fields to CLOUD_SQL_EXPORT_VALIDATION. (#23724),5
Add doc and sample dag for GCSToS3Operator (#23730),1
Fix grid details header text overlap (#23728)Move top margin to each breadcrumb component to make sure that there is no overlap when the header wraps with long names.,1
Add version to migration prefix (#23564)We don't really need the alembic revision id in the filename.  having version instead is much more useful.  having both of them takes up too much space.,1
Add typing for airflow/configuration.py (#23716)* Add typing for airflow/configuration.pyThe configuraiton.py did not have typing information and it madeit rather difficult to reason about it-especially that it wenta few changes in the past that made it rather complex tounderstand.This PR adds typing information all over the configuration file,2
Remove titles from link buttons (#23736),2
Disable flower in chart by default (#23737),2
Add AWS project structure tests (re: AIP-47) (#23630),3
Speech To Text assets & system tests migration (AIP-47) (#23643)Co-authored-by: Wojciech Januszek <januszek@google.com>,3
Add 'reschedule' to the serialized fields for the BaseSensorOperator (#23674)fix #23411,0
Updated MongoDB logo (#23746)As per https://www.mongodb.com/brand-resources,5
Fix broken main branch (#23751)main branch is broken since https://github.com/apache/airflow/pull/23630 needed rebase before mergeas https://github.com/apache/airflow/pull/23730 added the missing example dag,2
Allow more parameters to be piped through via execute_in_subprocess (#23286),2
Increase timeout for Helm Chart executor upgrade tests (#23759),3
Fix task log is not captured (#23684)when StandardTaskRunner runs tasks with execIssue: https://github.com/apache/airflow/issues/23540,0
Helm chart 1.6.0rc2 (#23754),2
Fix doc description of [core] parallelism config setting (#23768),1
Change `Github` to `GitHub` (#23764),4
"Add tagging image as latest for CI image wait (#23775)The ""wait for image"" step lacked --tag-as-latest which made thesubsequent ""fix-ownership"" step run sometimes far longer thanneeded - because it rebuilt the image for fix-ownership case.Also the ""fix-ownership"" command has been changed to just pullthe image if one is missing locally rather than build. Thiscommand might be run in an environment where the image is missingor any other image was build (for example in jobs where an imagewas build for different Python version) in this case the commandwill simply use whatever Python version is available (it doesnot matter), or in case no image is available, it will pull the imageas the last resort.",1
"Fix auto upstream dep when expanding non-templated field (#23771)If you tried to expand via xcom into a non-templated field withoutexplicitly setting the upstream task dependency, the scheduler wouldcrash because the upstream task dependency wasn't being setautomatically. It was being set only for templated fields, but now we doit for both.",1
clearer method name in scheduler_job.py (#23702),5
Fallback to parse dag_file when no dag in the db (#23738),5
cleanup usage of `get_connections()`` from test suite (#23757)The function is deprecated and raises warnings https://github.com/apache/airflow/pull/10192Replacing the usage with `get_connection()`,1
"Maintain grid view selection on filtering upstream (#23779)* Maintain grid selection on filter upstreamThe grid view selection was being cleared when clicking ""Filter Upstream"". The selection should persist.Also, added a left margin to the ""Reset root"" button* fix linting",0
"Fix ``SqliteHook`` compatibility with SQLAlchemy engine (#23790)Same as https://github.com/apache/airflow/pull/19508 but for Sqlite as described in https://docs.sqlalchemy.org/en/14/dialects/sqlite.html#connect-strings to be able to create a Sqlalchemy engine from the URI itself.Without this, it currently fails with the following error due to how we create URI in Connections. An absolute path is denoted by starting with a slash, means you need four slashes:```url = sqlite://%2Ftmp%2Fsqlite.db    def create_connect_args(self, url):        if url.username or url.password or url.host or url.port:>           raise exc.ArgumentError(                ""Invalid SQLite URL: %s\n""                ""Valid SQLite URL forms are:\n""                "" sqlite:///:memory: (or, sqlite://)\n""                "" sqlite:///relative/path/to/file.db\n""                "" sqlite:////absolute/path/to/file.db"" % (url,)            )E           sqlalchemy.exc.ArgumentError: Invalid SQLite URL: sqlite://%2Ftmp%2Fsqlite.dbE           Valid SQLite URL forms are:E            sqlite:///:memory: (or, sqlite://)E            sqlite:///relative/path/to/file.dbE            sqlite:////absolute/path/to/file.db```",5
"Fix python version used for cache preparaation (#23785)Cache preparation on CI used default (Python 3.7) version of theimage. It had an influence on time of ""full build needed"" only andfor users who wanted to build breeze image for Python versiondifferent than default Python 3.7.It had no big influence on ""main"" builds"" because in main we arebuild images with ""upgrade-to-newer-dependencies"" which takeslonger anyway.",1
Add `dttm` searchable field in audit log (#23794),2
"Further speed up fixing ownership in CI (#23782)After #23775 I noticed that there is yet another small improvementarea in the CI buld speed. Currently build-ci-image builds and pushonly ""commit-tagged"" images, but ""fix-ownership"" requiresthe ""latest"" image to run.This PR adds --tag-as-latest option also to build-image andbuild-prod-image commands - similarly as for the pull-image andpull-prod-image. This will retag the ""commit"" images as latest in thebuild-ci-images step and allow to save 1m on pulling the latest imagebefore fix-ownership (bringing it back to 1s overhead)",0
Modify db clean to also catch the ProgrammingError exception (#23699),0
Update the DMS Sample DAG and Docs (#23681),2
"postgres_operator_howto_guide.rst (#23789)Saying ""**the** PostgreSQL database"" confused me. I thought it was implying that a user could/should connect to the airflow metadata db",5
Support host_name on Datadog provider (#23784)This is required to use other Datadog tenants like app.datadoghq.eu,5
Cloud SQL assets & system tests migration (AIP-47) (#23583),3
Unbreak main after missing classes were added (#23819),1
Fix python version command (#23818),0
update CloudSqlInstanceImportOperator to CloudSQLImportInstanceOperator (#23800),2
Reformat the whole AWS documentation (#23810),2
Fix error when SnowflakeHook take empty list in `sql` param (#23767),2
Grid data: do not load all mapped instances (#23813)* only get necessary task instances* add comment* encode_ti -> get_task_summary,1
Fix regression in ignoring symlinks (#23535),5
[Issue#22846] allow option to encode or not encode UUID when uploading from Cassandra to GCS (#23766),1
Fix provider import error matching (#23825),0
Fix secrets rendered in UI when task is not executed. (#22754),0
"Fix retrieval of deprecated non-config values (#23723)It turned out that deprecation of config values did not work asintended. While deprecation worked fine when the value was specifiedin configuration value it did not work when `run_as_user` was used.In those cases the ""as_dict"" option was used to generate temporaryconfiguratin and this temporary configuration contained default valuefor the new configuration value - for example it caused thatthe generated temporary value contained:```[database]sql_alchemy_conn=sqlite:///{AIRFLOW_HOME}/airflow.db```Even if the deprecated `core/sql_alchemy_conn` was set (and nonew `database/sql_alchemy_conn` was set at the same time.This effectively rendered the old installation that did not convertto the new ""database"" configuration not working for run_as_user, becausethe tasks run with ""run_as_user"" used wrong, empty sqlite databaseinstaead of the one configured for Airflow.Also during adding tests, it turned out that the mechanism was alsonot working as intended before - in case `_CMD` or `_SECRET` were usedas environment variables rather than configuration. In those casesboth _CMD and _SECRET should be evaluated during as_dict() evaluation,because the ""run_as_user"" might have not enough permission to run thecommand or retrieve secret. The _cmd and _secret variables were onlyevaluated during as_dict() when they were in the config file (notethat this only happens when include_cmd, include_env, include_secretare set to True).The changes implemented in this PR fix both problems:* the _CMD and _SECRET env vars are evaluated during as_dict when the  respective include_* is set* the defaults are only set for the values that have deprecations  in case the deprecations have no values set in either of the ways:    * in config file    * in env variable    * in _cmd (via config file or env variable)    * in _secret (via config file or env variable)Fixes: #23679",0
"Automatically reschedule stalled queued tasks in CeleryExecutor (v2) (#23690)Celery can lose tasks on worker shutdown, causing airflow to just wait on themindefinitely (may be related to celery/celery#7266). This PR expands the""stalled tasks"" functionality which is already in place for adopted tasks, andadds the ability to apply it to all tasks such that these lost/hung tasks canbe automatically recovered and queued up again.",1
"Document fix for broken elasticsearch logs with 2.3.0+ upgrade (#23821)In certain upgrade paths, Airflow isn't given an opportunity to trackthe old `log_id_template`, so document the fix for folks who run intotrouble.",1
Add tool to automaticaly update status of AIP-47 issues. (#23745),0
"Self upgrade when refreshing images (#23686)When you have two branches, you should sefl-upgrade breeze to makesure you use the version that is tied with your branch.Usually we have two active branches - main and the last releasedline, so switching between then is not unlikely for maintainers.",1
Exclude missing tasks from the gantt view (#23627)* Exclude missing tasks from the gantt viewStops the gantt view from crashing if a task no longer existsin a DAG but there are TaskInstances for that task.* Fix tests,3
Don't use the root logger in KPO _suppress function (#23835),1
Update Production Guide for Helm Chart docs (#23836)Explain that db initialization is not necessary if using the helm chart.,2
Helm chart 1.6.0 is released; bump chart version to 1.7.0-dev (#23840),2
"Add missing ""airflow-constraints-reference"" parameter (#23844)The build commands were missing ""airflow-constraints-reference""parameter and it always defaulted to constraints-main",2
"Better fix for constraint-reference (#23845)The previous fix (#23844) broke main on package verificationas the package verification used the same parameter that was set toempty.This change rmeoves some remnant from the ""bash"" version wherewe had to check if variable was empty and also making the ""constraint""parameters accepting default values from the current branch to be usedalso for build commands.",1
Mask sensitive values for not-yet-running TIs (#23807)Alternative approach to #22754.  Resolves  #22738.,0
Add limit for JPype1 (#23847)The JPype1 limit has to be introduced because otherwise the 1.4.0JPype1 breaks our ARM builds. The 1.4.0 did not release the sdistversion of the package. This made our cache refresh job to failas 1.4.0 version cannot be installed on ARM image.The issue is captured inhttps://github.com/jpype-project/jpype/issues/1069,0
"Add ""no-issue-needed"" rule directly in CONTRIBUTING.rst (#23802)The rule was not really explained directly where you'd expect it,it was hidden deeply in ""triage"" process where many contributorswould not even get to.This PR adds appropriate explanation and also explains thatdiscussions is the preferred way to discuss things in Airflowrather than issues.",0
Handler parameter from `JdbcOperator` to `JdbcHook.run` (#23817),5
Doc: Add column names for DB Migration Reference (#23853)Before the automation: https://airflow.apache.org/docs/apache-airflow/2.2.5/migrations-ref.htmlCurrently (with missing column names): https://airflow.apache.org/docs/apache-airflow/2.3.0/migrations-ref.html,2
Fix exception trying to display moved table warnings (#23837)If you still have an old dangling table from the 2.2 migration thiswould fail. Make it more resilient and cope with both styles of movedtable name,4
Update sample dag and doc for RDS (#23651),2
"Fix DataprocJobBaseOperator not being compatible with dotted names (#23439). (#23791)* job_name parameter is now sanitized, replacing dots by underscores.",2
Upgrade `pip` to 22.1.1 version (just released) (#23854),5
"Add better feedback to Breeze users about expected action timing (#23827)There are a few actions in Breeze that might take more or less timewhen invoked. This is mostly when you need to upgrade Breeze orupdate to latest version of the image because some dependedncieswere added or image was modified.While we have improved significantly the waiting time involvednow (and caching problems have been fixed to make it as fastpossible), there are still a few situations that you need to havea good connectivity and a little time to run the upgrade. Whichis often not something you would like to loose your time on ina number of cases when you need to do things fast.Usually Breeeze does not force the user to perform such longactions - it allows to continue without doing them (either bytimeout or by letting user answer ""no"" to question asked.Previously Breeze have not informed the user about the exepctedtime of running such operation, but with this change it tellswhat is the expected delay - thus allowing the user to makeinformed action whether they want to run the upgrade or not.",1
Fix UnboundLocalError when sql is empty list in DbApiHook (#23816),5
Fix UnboundLocalError when sql is empty list in DatabricksSqlHook (#23815),5
Add number of node params only for single-node cluster in RedshiftCreateClusterOperator (#23839),1
Sql to gcs with exclude columns (#23695),5
Add support for associating  custom tags to job runs submitted via EmrContainerOperator (#23769)Co-authored-by: Sandeep Kadyan <sandeep.kadyan@publicissapient.com>,1
Add Deferrable Databricks operators (#19736),1
Fix Amazon EKS example DAG raises warning during Imports (#23849)Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
Fix databricks tests (#23856),3
Add __wrapped__ property to _TaskDecorator (#23830)Co-authored-by: Sanjay Pillai <sanjaypillai11 [at] gmail.com>,5
"Highlight task states by hovering on legend row (#23678)* Rework the legend row and add the hover effect.* Move horevedTaskState to state and fix merge conflicts.* Add tests.* Order of item in the LegendRow, add no_status support",1
Clean up f-strings in logging calls (#23597),2
update K8S-KIND to 0.14.0 (#23859),5
Replaced all days_ago functions with datetime functions (#23237)Co-authored-by: Dev232001 <thedevhooda@gmail.com>,1
Add clear DagRun endpoint. (#23451),2
Ignore the DeprecationWarning in test_days_ago (#23875)Co-authored-by: alexkru <alexkru@wix.com>,3
"Speed up Breeze experience on Mac OS (#23866)This change should significantly speed up Breeze experience (andespecially iterating over a change in Breeze for MacOS users -independently if you are using x86 or arm architecture.The problem with MacOS with docker is particularly slow filesystemused to map sources from Host to Docker VM. It is particularly badwhen there are multiple small files involved.The improvement come from two areas:* removing duplicate pycache cleaning* moving MyPy cache to docker volumeWhen entering breeze we are - just in case - cleaning .pyc and__pychache__ files potentially generated outside of the dockercontainer - this is particularly useful if you use local IDEand you do not have bytecode generation disabled (we have itdisabled in Breeze). Generating python bytecode might lead tovarious problems when you are switching branches and Pythonversions, so for Breeze development where the files changeoften anyway, disabling them and removing when they are foundis important. This happens at entering breeze and it might takea second or two depending if you have locally generated.It could happen that __init script was called twice (depending whichscript was called - therefore the time could be double the onethat was actually needed. Also if you ever generated providerpackages, the time could be much longer, because node_modulesgenerated in provider sources were not excluded from searching(and on MacOS it takes a LOT of time).This also led to duplicate time of exit as the initialization codeinstalled traps that were also run twice. The traps however wererather fast so had no negative influence on performance.The change adds a guard so that initialization is only ever executedonce.Second part of the change is moving the cache of mypy to a dockervolume rather than being used from local source folder (defaultwhen complete sources are mounted). We were already using selectivemount to make sure MacOS filesystem slowness affects us in minimalway - but with this change, the cache will be stored in dockervolume that does not suffer from the same problems as mountingvolumes from host. The Docker volume is preserved until the`docker stop` command is run - which means that iterating overa change should be WAY faster now - observed speed-up were around5x speedups for MyPy pre-commit.",4
Add default task retry delay config (#23861),5
"Move MappedOperator tests to mirror code location (#23884)At some point during the development of AIP-42 we moved the code forMappedOperator out of baseoperator.py to mappedoperator.py, but wedidn't move the tests at the same time",3
"Enable clicking on DAG owner in autocomplete dropdown (#23804)PR#18991 introduced directly navigating to a DAG when selecting onefrom the typeahead search results. Unfortunately, the search resultsalso includes DAG owner names, and selecting one of those navigates toa DAG with that name, which almost certainly doesn't exist.This extends the autocompletion endpoint to return the type of result,and adjusts the typeahead selection to use this to know which way tonavigate.",1
Document LocalKubernetesExecutor support in chart (#23876),2
Avoid extra questions in `breeze build image` command. (#23898)Fixes: #23867,0
Update INTHEWILD.md (#23892),5
Split contributor's quick start into separate guides. (#23762)The foldable parts were not good. They made links not to work aswell as they were not too discoverable.Fixes: #23174,0
Avoid printing exception when exiting tests command (#23897)Fixes: #23868,0
"Move string arg evals to `execute()` in `EksCreateClusterOperator` (#23877)Currently there are string-value evaluations of `compute`, `nodegroup_role_arn`,  and `fargate_pod_execution_role_arn` args in the constructor of `EksCreateClusterOperator`.  These args are all listed as a template fields so it's entirely possible that the value(s) passed in to the operator is a Jinja expression or an `XComArg`. Either of these value types could cause a false-negative `ValueError` (in the case of unsupported `compute` values) or a `false-positive` (in the the cases of explicit checks for the *arn values) since the values themselves have not been rendered.This PR moves the evaluations of these args to the `execute()` scope.",4
Update .readthedocs.yml (#23903)String instead of Int see https://docs.readthedocs.io/en/stable/config-file/v2.html,5
"Make --file command in static-checks autocomplete file name (#23896)The --verbose and --dry-dun commands caused n --files command to failand the flag was ""artifficial"" -it was equivalent to bool flag.the actual files were taken  from arguments.This PR fixes it by turning the arguments into multiple ``--file``commands  - each with its own completioin for local files.",2
Chart: Update default airflow version to `2.3.1` (#23913),5
Fix Breeze documentation typo (#23919),2
Update environments documentation links (#23920),2
`2.3.1` has been released (#23912),5
Make CI and PROD image builds consistent (#23841)Simple refactoring to make the jobs more consistent.,1
Alphabetizes two tables (#23923)The rest of the page has consistently alphabetized tables. This commit fixes three `extras` that were not alphabetized.,0
"Use ""remote"" pod when patching KPO pod as ""checked"" (#23676)When patching as ""checked"", we have to use the current version of the pod otherwise we may get an error when trying to patch it, e.g.:```Operation cannot be fulfilled on pods \""test-kubernetes-pod-db9eedb7885c40099dd40cd4edc62415\"": the object has been modified; please apply your changes to the latest version and try again""```This error would not cause a failure of the task, since errors in `cleanup` are suppressed.  However, it would fail to patch.I believe one scenario when the pod may be updated is when retrieving xcom, since the sidecar is terminated after extracting the value.Concerning some changes in the tests re the ""already_checked"" label, it was added to a few ""expected pods"" recently, when we changed it to patch even in the case of a successful pod.Since we are changing the ""patch"" code to patch with the latest read on the pod that we have (i.e. using the `remote_pod` variable), and no longer the pod object stored on `k.pod`, the label no longer shows up in those tests (that's because in k.pod isn't actually a read of the remote pod, but just happens to get mutated in the patch function before it is used to actually patch the pod).Further, since the `remote_pod` is a local variable, we can't observe it in tests.  So we have to read the pod using k8s api. _But_, our ""find pod"" function excludes ""already checked"" pods!  So we have to make this configurable.So, now we have a proper integration test for the ""already_checked"" behavior (there was already a unit test).",3
Clarify manual merging of PR in release doc (#23928)It was not clear to me what this really means,2
"Fix broken main (#23940)main breaks with`Traceback:  /usr/local/lib/python3.7/importlib/__init__.py:127: in import_module      return _bootstrap._gcd_import(name[level:], package, level)  tests/providers/amazon/aws/hooks/test_cloud_formation.py:31: in <module>      class TestCloudFormationHook(unittest.TestCase):  tests/providers/amazon/aws/hooks/test_cloud_formation.py:67: in TestCloudFormationHook      @mock_cloudformation  /usr/local/lib/python3.7/site-packages/moto/__init__.py:30: in f      module = importlib.import_module(module_name, ""moto"")  /usr/local/lib/python3.7/importlib/__init__.py:127: in import_module      return _bootstrap._gcd_import(name[level:], package, level)  /usr/local/lib/python3.7/site-packages/moto/cloudformation/__init__.py:1: in <module>      from .models import cloudformation_backends  /usr/local/lib/python3.7/site-packages/moto/cloudformation/models.py:18: in <module>      from .parsing import ResourceMap, OutputMap  /usr/local/lib/python3.7/site-packages/moto/cloudformation/parsing.py:17: in <module>      from moto.apigateway import models  # noqa  # pylint: disable=all  /usr/local/lib/python3.7/site-packages/moto/apigateway/__init__.py:1: in <module>      from .models import apigateway_backends  /usr/local/lib/python3.7/site-packages/moto/apigateway/models.py:9: in <module>      from openapi_spec_validator import validate_spec  E   ModuleNotFoundError: No module named 'openapi_spec_validator'  `  Fix is already in placed in moto https://github.com/spulec/moto/pull/5165 but version 3.1.11 wasn't released yet",0
Update INSTALL_PROVIDERS_FROM_SOURCES instructions. (#23938),1
Add typing to Azure Cosmos Client Hook (#23941)New release of Azure Cosmos library has added typing informationand it broke main builds with mypy verification.,5
Remove redundant register exit signals in `dag-processor` command (#23886),2
"Disable rebase workflow (#23943)The change of the release workflow in #23928 removed the reasonwhy we should have rebase workflow possible. We only needed todo rebase when we merged test branch into stable branch andsince we are doing it manually, there is no more reeason tohave it in the GitHub UI.",3
Prevent UI from crashing if grid task instances are null (#23939)* UI fix for null task instances* improve tests without global vars* fix test data,5
Grid fix details button truncated and small UI tweaks (#23934)* Show details button and wrap on LegendRow.* Update following brent review* Fix display on small width* Rotate icon for a 'ReadLess' effect,0
"Fix and speed up grid view (#23947)This fetches all TIs for a given task across dag runs, leading tosignifincatly faster response times. It also fixes a bug where Noneswere being passed to the UI when a new task was added to a DAG withexiting runs.",1
Removes duplicate code block (#23952)There's are two code blocks with identical text in the helm-chart docs. This commit removes one of them.,4
Update dep for databricks #23917 (#23927),5
Use '--subdir' argument value for standalong dag processor. (#23864),2
"Revert ""Add limit for JPype1 (#23847)"" (#23953)This turned out to be mistake in manual submission. Fixedon JPype1 side.This reverts commit 3699be49b24ef5a0a8d8de81a149af2c5a7dc206.",4
Faster grid view (#23951),5
Disallow calling expand with no arguments (#23463),1
[FEATURE] KPO use K8S hook (#22086),1
"Add cascade to `dag_tag` to `dag` foreignkey (#23444)Bulk delete does not work if the cascade behaviour of a foreignkeyis set on python side(relationship configuration). To allow bulk delete of dagswe need to setup cascade deletion in the DB.The warning on query.delete athttps://docs.sqlalchemy.org/en/14/orm/session_basics.html#selecting-a-synchronization-strategystated that:The operations do not offer in-Python cascading of relationships - it is assumed that ON UPDATE CASCADE and/or ON DELETE CASCADE is configured for any foreign key references which require it, otherwise the database may emit an integrity violation if foreign key references are being enforced.Another alternative is avoiding bulk delete of dags but I prefer we support bulk deletes.This will break offline sql generation for mssql(already broken before now :) ). Also, since there's only one foreign keyin `dag_tag` table, I assume that the foreign key would be named `dag_tag_ibfk_1` in `mysql`. Thisavoided having to query the db for the name.The foreignkey is explicitly named now, would be easy for future upgrades",5
DagFileProcessorManager: Start a new process group only if current process not a session leader (#23872),1
Introduce `flake8-implicit-str-concat` plugin to static checks (#23873),2
Fix UnboundLocalError when sql is empty list in ExasolHook (#23812),1
"Fix inverted section levels in best-practices.rst (#23968)This PR fixes inverted levels in the sections added to the ""Best Practices"" document in #21879.",2
Add support to specify language name in PapermillOperator (#23916)* Add support to specify language name in PapermillOperator* Replace getattr() with simple attribute access,1
[23945] Icons in grid view for different dag types (#23970),2
Helm logo no longer a link (#23977),2
Fix links in documentation (#23975)* fix links* added right link to breeze,2
"Add TaskInstance State 'REMOVED' to finished states and success states (#23797)Now that we support dynamic task mapping, we should have the 'REMOVED'state of task instances as a finished state becausefor dynamic tasks with a removed task instance, the dagrun would be stuck inrunning state if 'REMOVED' state is not in finished states.",5
Remove `xcom_push` from `DockerOperator` (#23981),2
Fix missing shorthand for docker buildx rm -f (#23984)Latest version of buildx removed -f as shorthand for --force flag.,1
"use explicit --mount with types of mounts rather than --volume flags (#23982)The --volume flag is an old style of specifying mounts used by docker,the newer and more explicit version is --mount where you have tospecify type, source, destination in the form of key/value pairs.This is more explicit and avoids some guesswork when volumes aremounted (for example seems that on WSL2 volume name might beguessed as path wrongly). The change explicitly specifies whichof the mounts are bind mounts and which are volume mounts.Another nice side effect of this change is that when source ismissing, docker will not automatically create directories with themissing name but it will fail. This is nicer because before itled to creating directories when they were missing (for example.bash_aliases and similar). This allows us to avoid some cleanupsto account for those files being created - instead we simplyskip those mounts if the file/folder does not exist.",2
Force colors in yarn test output in CI (#23986),3
"Fix breeze failures when there is no buildx installed on Mac (#23988)If you have no buildx plugin installed on Mac (for example whenyou use colima instead of Docker Desktop) the breeze check wasfailing - but buildx in fact is not needed to run typical breezecommands, and breeze already has support for it - it was justwrongly handled.",0
"Replace generation of docker volumes to be done from python (#23985)The pre-commit to generate docker volumes in docker composefile is now written in Python and it also uses the newer ""volume:""syntax to define the volumes mounted in the docker-compose.",2
"Replace `use_task_execution_date` with `use_task_logical_date` (#23983)* Replace `use_task_execution_date` with `use_task_logical_date`We have some operators/sensors that use `*_execution_date` as the class parameters. This PR deprecate the usage of these parameters and replace it with `logical_date`.There is no change in functionality, under the hood the functionality already uses `logical_date` this is just about the parameters name as exposed to the users.",1
Remove pinning for xmltodict (#23992)We have now moto 3.1.9+ in constraints so we should remove the limit.Fixes: #23576,0
"Remove fixing cncf.kubernetes provider when generating constraints (#23994)When we yanked cncf.kubernetes provider, we pinned 3.1.2temporarily for provider generation. This removes the pinning aswe are already at 4.0.2 version",4
"Add better diagnostics capabilities for pre-commits run via CI image (#23980)The pre-commits that require CI image run docker command underthe hood that is highly optimized for performance (only mountsfiles that are necessary to be mounted) - in order to improveperformance on Mac OS and make sure that artifacts are not leftin the source code of Airflow.However that makes the command slightly more difficult to debugbecause they generate dynamically the docker command used,including the volumens that should be mounted when the dockercommand is run.This PR adds better diagnostics to the pre-commit scriptsallowing VERBOSE=""true"" and DRY_RUN=""true"" variables that canhelp with diagnosing problems such as running the scripts onWSL2.It also fixes a few documentation bugs that have been missedafter changing names of the image-related static checks andthanks to separating the common code to utility functionit allows to set SKIP_IMAGE_PRE_COMMITS variable to truewhich will skip running all pre-commit checks that requirebreeze image to be available locally.",1
"Disable fail-fast on pushing images to docker cache (#24005)There is an issue with pushing cache to docker registry thatis connected to containerd bug but started to appear morefrequently recently (as evidenced for example byhttps://github.community/t/buildx-failed-with-error-cannot-reuse-body-request-must-be-retried/253178). The issue is still open in containerd:https://github.com/containerd/containerd/issues/5978.Until it if fixed, we disable fail-fast on pushing cacheso that even if it happens, we just have to re-run that singlepython version that actually failed. Currently there is a muchlower chance of success because all 4 build have to succeed.",1
"Add automated retries on retryable condition for building images in CI (#24006)There is a flakiness in pushing cache images to ghcr.io, thereforewe want to add automated retries when the images fail intermittently.The root cause of the problem is tracked in containerd:https://github.com/containerd/containerd/issues/5978",0
Ensure @contextmanager decorates generator func (#23103),5
"Revert ""Add automated retries on retryable condition for building images in CI (#24006)"" (#24016)This reverts commit 7cf0e43b70eb1c57a90ee7e2ff14b03487ffb018.",4
Cleanup `BranchDayOfWeekOperator` example dag (#24007)* Cleanup BranchDayOfWeekOperator example dagThere is no need for `dag=dag` when using context manager.,1
Added missing project_id to the wait_for_job (#24020),1
"Only run separate per-platform build when preparing build cache (#24023)Apparently pushing multi-platform images when building cache on CIhas some problems recently, connected with ghcr.io being morevulnerable to race condition described in this issue:https://github.com/containerd/containerd/issues/5978Apparently when two, different platform layers are pushed aboutthe same time to ghcr.io, the error""cannot reuse body, request must be retried"" is generated.However we actually do not even need to build the multiplatformlatest images because as of recently we have separate cache for eachplatform, and the ghcr.io/:latest images are not used any morenot even for docker builds. We we always build images rather thanpull and we use --from-cache for that - specific per platform. The onlyimage pulling we do is when we pull the :COMMIT_HASH images in CI- butthose are single-platform images (amd64) and even if we add tests forarm, they will have different tag.Hopefully we can still build release images without causing therace condition too frequently - this is more likely because whenwe build images for cache we use machines with different performancecharacteristics and the same layers are pushed at different timesfrom different platforms.",1
"Preparing buildx cache is allowed without --push-image flag (#24028)The previous version of buildx cache preparation implied --push-imageflag, but now this is completely separated (we do not push image,we just prepare cache), so when mutli-platform buildx preparation isrun we should also allow the cache to run without --push-image flag.",1
"Add partition related methods to GlueCatalogHook: (#23857)* ""get_partition"" to retrieve a Partition* ""create_partition"" to create a Partition",1
Adds foldable CI group for command output (#24026),1
"Add foldable groups in CI outputs in commands that need it (#24035)This is follow-up after #24026 which added capability of selectivelydeciding for each breeze command, whether the output of the commandshould be ""foldable"" group. All CI output has been reviewed, andthe commands which ""need"" it were identified.This also fixes a problem introduced there - that the command itselfwas not ""foldable"" group itself.",0
Increase size of ARM build instance (#24036)Our ARM cache builds started to hang recently at yarn prod step.The most likely reason are limited resources we had for the ARMinstance to run the docker build - it was rather small instancewith 2GB RAM and it is likely not nearly enought to cope withrecent changes related to Grid View where we likely need muchmore memory during the yarn build step.This change increases the instance memory to 8 GB (c6g.xlarge).Also this instance type gives 70% cost saving and has very lowprobability of being evicted (it's not in high demand in OhioRegion of AWS.Also the AMI used is refreshed with latest software (docker),2
Remove unused [github_enterprise] from ref docs (#24033),2
Add enum validation for [webserver]analytics_tool (#24032),5
Support impersonation service account parameter for Dataflow runner (#23961),1
Fix closing connection dbapi.get_pandas_df (#23452),5
Light Refactor and Clean-up AWS Provider (#23907),1
Removing magic numbers from exceptions (#23997)* Removing magic numbers from exceptions* Running pre-commit,1
Upgrade to pip 22.1.2 (#24043)Pip has been upgraded to version 22.1.2 12 minutes ago. Time tocatch up.,5
Shaves-off about 3 minutes from usage of ARM instances on CI (#24052)Preparing airflow packages and provider packages does notneed to be done on ARM and actually the ARM instance is idlewhile they are prepared during cache building.This change moves preparation of the packages to beforethe ARM instance is started which saves about 3 minutes of ARMinstance time.,4
"SSL Bucket, Light Logic Refactor and Docstring Update for Alibaba Provider (#23891)",1
"Use KubernetesHook to create api client in KubernetesPodOperator (#20578)Add support for k8s hook in KPO; use it always (even when no conn id); continue to consider the core k8s settings that KPO already takes into account but emit deprecation warning about them.KPO historically takes into account a few settings from core airflow cfg (e.g. verify ssl, tcp keepalive, context, config file, and in_cluster). So to use the hook to generate the client, somehow the hook has to take these settings into account. But we don't want the hook to consider these settings in general.  So we read them in KPO and if necessary patch the hook and warn.",1
Re-add --force-build flag (#24061)After #24052 we also need to add --force-build flag as forPython 3.7 rebuilding CI cache would have been silently ignored asno image building would be needed,1
Fix grid view for mapped tasks (#24059),0
Fix StatD timing metric units (#21106)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,0
Drop Python 3.6 compatibility objects/modules (#24048),4
Remove hack from BigQuery DTS hook (#23887),1
Spanner assets & system tests migration (AIP-47) (#23957),3
Run the `check_migration` loop at least once (#24068)This is broken since 2.3.0. that's if a user specifies a migration_timeoutof 0 then no migration is run at all.,1
Bump eventsource from 1.0.7 to 1.1.1 in /airflow/ui (#24062)Bumps [eventsource](https://github.com/EventSource/eventsource) from 1.0.7 to 1.1.1.- [Release notes](https://github.com/EventSource/eventsource/releases)- [Changelog](https://github.com/EventSource/eventsource/blob/master/HISTORY.md)- [Commits](https://github.com/EventSource/eventsource/compare/v1.0.7...v1.1.1)---updated-dependencies:- dependency-name: eventsource  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
"Remove certifi limitations from eager upgrade limits (#23995)The certifi limitation was introduced to keep snowflake happy whileperforming eager upgrade because it added limits on certifi. Howeverseems like it is not limitation any more in latest versions ofsnowflake python connector, so we can safely remove it from here.The only remaining limit is dill but this one still holds.",5
fix style of example block (#24078),0
Handle occasional deadlocks in trigger with retries (#24071)Fixes: #23639,0
"Adds Pura Scents, edits The Dyrt (#24086)",2
Migrate Yandex example DAGs to new design AIP-47 (#24082)closes: #22470,1
set color to operators in cloud_sql.py (#24000),1
"Migrate HTTP example DAGs to new design AIP-47 (#23991)closes: #22448 , #22431",1
Make expand() error vague so it's not misleading (#24018),0
"Use github for postgres chart index (#24089)Bitnami's CloudFront CDN is seemingly having issues, so point at githubdirect instead until it is resolved.",0
Fix the link to google workplace (#24080),1
Bring MappedOperator members in sync with BaseOperator (#24034),1
Add note about Docker volume remount issues in WSL 2 (#24094),0
Convert Athena Sample DAG to System Test (#24058),3
Self-update pre-commit to latest versions (#24106),3
"Temporarily fix bitnami index problem (#24112)We started to experience ""Internal Error"" when installingHelm chart and apperently bitnami ""solved"" the problem byremoving from their index software older than 6 months(!).This makes our CI fail but It is much worse. Thisrenders all our charts useless for people to installThis is terribly wrong, and I raised this in the issuehere:https://github.com/bitnami/charts/issues/10539#issuecomment-1144869092",2
"Fix small typos in static code checks doc (#24113)- Trivial typo fix in the command to run static checks on the last commit- Update ""run all tests"" to ""run all checks"" where applicable for consistency",1
Really workaround bitnami chart problem (#24115)The original fix in #24112 did not work due to:* not updated lock* EOL characters at the end of multiline long URLThis PR fixes it.,0
Reduce grid view API calls (#24083)* Reduce API calls from /grid- Separate /grid_data from /grid- Remove need for formatData- Increase default query stale time to prevent extra fetches- Fix useTask query keys* consolidate grid data functions* fix www teststest grid_data instead of /grid,5
Removing magic status code numbers from api_connecxion (#24050),4
"Do not support MSSQL less than v2017 in code (#24095)Our experimental support for MSSQL starts from v2017(in README.md) butwe still support 2000 & 2005 in code.This PR removes this support, allowing us to use mssql.DATETIME2 in allMSSQL DB.",5
Rename Permissions to Permission Pairs. (#24065),5
"Note that yarn dev needs webserver in debug mode (#24119)* Note that yarn dev needs webserver -d* Update CONTRIBUTING.rstCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* Use -D* Revert ""Use -D""This reverts commit 94d63adcf36aac13f5d94c2d4cd651907d833794.Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>",1
fixing SSHHook bug when using allow_host_key_change param (#24116),2
"Adds mssql volumes to ""all"" backends selection (#24123)The ""stop"" command of Breeze uses ""all"" backend to remove allvolumes - but mssql has special approach where the volumesdefined depend on the filesystem used and we need to add thespecific docker-compose files to list of files used whenwe use stop command.",1
Breeze must create `hooks\` and `dags\` directories for bind mounts (#24122)  Now that breeze uses --mount instead of --volume (the former of which  does not create missing mount dirs like the latter does see docs here:  https://docs.docker.com/storage/bind-mounts/#differences-between--v-and---mount-behavior)  we need to create these directories explicitly.,1
AIP-47 | Migrate Trino example DAGs to new design (#24118),1
"Update production-deployment.rst (#24121)The sql_alchemy_conn option is in the database section, not the core section.  Simple typo fix.",0
Migrate Zendesk example DAGs to new design #22471 (#24129),1
Migrate JDBC example DAGs to new design #22450 (#24137),1
Migrate Jenkins example DAGs to new design #22451 (#24138),1
Migrate Microsoft example DAGs to new design #22452 - mssql (#24139),1
Migrate MySQL example DAGs to new design #22453 (#24142),1
Migrate Opsgenie example DAGs to new design #22455 (#24144),1
Migrate Presto example DAGs to new design #22459 (#24145),1
Migrate Plexus example DAGs to new design #22457 (#24147),1
Migrate SQLite example DAGs to new design #22461 (#24150),1
Migrate Telegram example DAGs to new design #22468 (#24126),1
AIP-47 - Migrate Tableau DAGs to new design (#24125),1
Migrate Salesforce example DAGs to new design #22463 (#24127),1
Update credentials when using ADC in Compute Engine (#23773),1
Improve Windows development compatibility for breeze (#24098),1
Migrate Asana example DAGs to new design #22440 (#24131),1
Migrate Neo4j example DAGs to new design #22454 (#24143),1
Workflows assets & system tests migration (AIP-47) (#24105)* Workflows assets & system tests migration (AIP-47)Co-authored-by: Wojciech Januszek <januszek@google.com>,3
Add disabled_algorithms as an extra parameter for SSH connections (#24090),2
Migrate Postgres example DAGs to new design #22458 (#24148)* Migrate Postgres example DAGs to new design #22458* Fix static checks,0
Migrate Snowflake system tests to new design #22434 (#24151)* Migrate Snowflake system tests to new design #22434* Fix flake8,0
Migrate Qubole example DAGs to new design #22460 (#24149)* Migrate Qubole example DAGs to new design #22460,1
Migrate Microsoft example DAGs to new design #22452 - azure (#24141)* Migrate Microsoft example DAGs to new design #22452 - azure,1
Migrate Microsoft example DAGs to new design #22452 - winrm (#24140)* Migrate Microsoft example DAGs to new design #22452 - winrm* Fix static checks,0
Migrate Influx example DAGs to new design #22449 (#24136)* Migrate Influx example DAGs to new design #22449* Fix static checks,0
Migrate DingTalk example DAGs to new design #22443 (#24133)* Migrate DingTalk example DAGs to new design #22443,1
Migrate Cncf.Kubernetes example DAGs to new design #22441 (#24132)* Migrate Cncf.Kubernetes example DAGs to new design #22441,1
Migrate Alibaba example DAGs to new design #22437 (#24130)* Migrate Alibaba example DAGs to new design #22437,1
Pass connection extra parameters to wasb BlobServiceClient (#24154),2
fix BigQueryInsertJobOperator (#24165),1
Migrate Singularity example DAGs to new design #22464 (#24128),1
Better summary of status of AIP-47 (#24169)Result is here: https://github.com/apache/airflow/issues/24168,0
Remove old Athena Sample DAG (#24170),2
removed old files (#24172),2
Chart: Default to Airflow 2.3.2 (#24184),2
Update 'rich' to latest version across the board. (#24186)That Also includes regenerating the breeze output images.,3
Fix BigQuery system tests (#24013)* Change execution_date to data_interval_start in BigQueryInsertJobOperator job_idChange-Id: Ie1f3bba701169ceb2b39d693da320564de145c0c* Change jinja template path to relative pathChange-Id: I6cced215124f69e9f4edf8ac08bb71d3ec3c8afcCo-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,4
`2.3.2` has been released (#24182),5
Add verification step to image release process (#24177),1
Added impersonation_chain for DataflowStartFlexTemplateOperator and DataflowStartSqlJobOperator (#24046),5
Add key_secret_project_id parameter which specifies a project with KeyFile (#23930),2
Add built-in Extrenal Link for ExternalTaskMarker operator (#23964),1
fix: DatabricksSubmitRunOperator and DatabricksRunNowOperator cannot define .json as template_ext (#23622) (#23641),5
fix: StepFunctionHook ignores explicit set `region_name` (#23976),1
"Remove `GithubOperator` use in  `GithubSensor.__init__()`` (#24214)The constructor for `GithubSensor` was instantiating `GitHubOperator` to use its `execute()` method as the driver for the result of the sensor's `poke()` logic. However, this could yield a `DuplicateTaskIdFound` when used in DAGs.This PR updates the `GithubSensor` to use the `GithubHook` instead.",1
Mac M1 postgress and doc fix (#24200),0
AIP-47 - Migrate dbt DAGs to new design #22472 (#24202),1
AIP-47 - Migrate databricks DAGs to new design #22442 (#24203),1
AIP-47 - Migrate hive DAGs to new design #22439 (#24204),1
AIP-47 - Migrate kylin DAGs to new design #22439 (#24205),1
AIP-47 - Migrate drill DAGs to new design #22439 (#24206),1
AIP-47 - Migrate druid DAGs to new design #22439 (#24207),1
AIP-47 - Migrate cassandra DAGs to new design #22439 (#24209),1
AIP-47 - Migrate spark DAGs to new design #22439 (#24210),1
AIP-47 - Migrate apache pig DAGs to new design #22439 (#24212),1
Migrate GitHub example DAGs to new design #22446 (#24134),1
"Remove warnings when starting breeze (#24183)Breeze when started produced three warnings that were harmless,but we should fix them to remove ""false positives"".",4
AIP-47 - Migrate livy DAGs to new design #22439 (#24208),1
Remove escaping which is wrong in latest rich version (#24217)Latest rich makes escaping not needed for extra `[` needed inMarkdown URLs.,1
"Parse error for task added to multiple groups (#23071)This raises an exception if a task already belonging to a task group(including added to a DAG, since such task is automatically added to theDAG's root task group).Also, according to the issue response, manually calling TaskGroup.add()is not considered a supported way to add a task to group. So ameta-marker is added to the function docstring to prevent it fromshowing up in documentation and users from trying to use it.",1
Fix xfail test in test_scheduler.py (#23731),3
Migrate Papermill example DAGs to new design #22456 (#24146),1
Migrate Asana system tests to new design AIP-47 (#24226)closes: #22428related: #22440,1
Migrate Microsoft system tests to new design AIP-47 (#24225)closes: #22432related: #22452,1
Migrate CNCF system tests to new design AIP-47 (#24224)closes: #22429related: #22441,1
Migrate Postgres system tests to new design (#24223)closes: #22433related: #22458,1
AIP-47 - Migrate beam DAGs to new design #22439 (#24211)* AIP-47 - Migrate beam DAGs to new design #22439,1
Add explanatory note for contributors about updating Changelog (#24229),4
Fix backwards-compatibility introduced by fixing mypy problems (#24230)There was a backwards-incompatibility introduced by #23716 intwo providers by using get_mandatory_value config method.This PR corrects that backwards compatibility and updates 2.1compatibility pre-commit to check for forbidden usage ofget_mandatory_value.,5
Bump moto version (#24222)* Bump moto versionversion 3.1.10 broke main but the issue was fixed since in motorelated: https://github.com/spulec/moto/pull/5165* fix moto,0
Add `PrestoToSlackOperator` (#23979)* Add `PrestoToSlackOperator`Adding the funcitonality to run a single query against presto and send the result as slack message.Similar to `SnowflakeToSlackOperator`,1
Fix BigQuery Sensors system test (#24245)Co-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,3
"adding AWS_DEFAULT_REGION to the docs, boto3 expects this to be in the env variables (#24181)",2
Unify return_code interface for task runner (#24093),1
Update dbt.py (#24218),5
Fix GCSToGCSOperator cannot copy a single file/folder without copying other files/folders with that prefix (#24039),0
Adding fnmatch type regex to SFTPSensor (#24084),1
docs: amazon-provider retry modes (#23906),1
Cloud Storage assets & StorageLink update (#23865)Co-authored-by: Wojciech Januszek <januszek@google.com>,5
Fix useTasks crash on error (#24152)* Prevent UI from crashing on Get API error* add test* don't show API errors in test logs* use setMinutes inline,1
"Refactor GlueJobHook get_or_create_glue_job method. (#24215)When invoked, create_job takes into account the provided 'Command' argument instead of having it hardcoded.",1
Fix delete_cluster no use TriggerRule.ALL_DONE (#24213)related: #24082,1
docker new system test (#23167),3
chore: Refactoring and Cleaning Apache Providers (#24219),1
Fix await_container_completion condition (#23883),0
Migrate Apache Beam system tests to new design AIP-47 (#24256)closes: #22427,1
Migrate Apache Beam system tests to new design #22427 (#24241),1
"Migrate Google leveldb system tests to new design AIP-47 (#24255)related: #22447, #22430",1
Add param docs to KubernetesHook and KubernetesPodOperator (#23955) (#24054),1
Enable dbt Cloud provider to interact with single tenant instances (#24264)* Enable provider to interact with single tenant* Define single tenant arg on Operator* Add test for single tenant endpoint* Enable provider to interact with single tenant* Define single tenant arg on Operator* Add test for single tenant endpoint* Code linting from black* Code linting from black* Pass tenant to dbtCloudHook in DbtCloudGetJobRunArtifactOperator class* Make Tenant a connection-level setting* Remove tenant arg from Operator* Make tenant connection-level param that defaults to 'cloud'* Remove tenant param from sensor* Remove leftover param string from hook* Update airflow/providers/dbt/cloud/hooks/dbt.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Parameterize test_init_hook to test single and multi tenant connections* Integrate test simplification suggestion* Add connection to TestDbtCloudJobRunSesnorCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>,1
Apply per-run log templates to log handlers (#24153),0
AIP-47 - Migrate google leveldb DAGs to new design ##22447 (#24233),1
"Fix choosing backend versions in breeze's command line (#24228)Choosing version of backend were broken when command line switcheswere used. The _VERSION variables were ""hard-coded"" to defaultsrather than taken from command line. This is a remnant of initialimplementation and converting the parameters to ""cacheable"" ones.While looking at the versions we also found that PARAM_NAME_FLAGis not used any more so we took the opportunity to remove it.",4
Fix link broken after #24082 (#24276)https://github.com/apache/airflow/pull/24082,2
Add command to regenerate breeze command output images (#24216),1
Make numpy effectively an optional dependency for Oracle provider (#24272)Better fix to #23132,0
Add SMAP Energy to list of companies using Airflow (#24268),1
fix command and typo (#24282),2
Update doc and sample dag for EMR Containers (#24087),2
scheduleinterval nullable true added in openapi (#24253),1
Check that edge nodes actually exist (#24166),5
Prepare docs for May 2022 provider's release (#24231)This documentation update also (following the rule agreed inhttps://github.com/apache/airflow/blob/main/README.md#support-for-providers)bumps mininimum supported version of Airflow for all providersto 2.2 and it constitutes a breaking change and major version bumpfor all providers.,1
pydocstyle D202 added (#24221),1
Update provider templates for new Airflow 2.2+ req (#24291)I imagine we could update this somewhat programmatically and/or add this update to instructions somewhere. Let me know what you think.,5
Update package description to remove double min-airflow specification (#24292),4
Airflow UI fix vulnerabilities - Prototype Pollution (#24201),0
Mention context variables and logging (#24304)* Mention context variables and logging* Fix static checks,0
Remove limit of presto-python-client version (#24305),4
Fix langauge override in papermill operator (#24301),1
Also mention airflow 2 only in readme template (#24296),5
"Fix permission issue for dag that has dot in name (#23510)How we determine if a DAG is a subdag in airflow.security.permissions.resource_name_for_dag is not right.If a dag_id contains a dot, the permission is not recorded correctly.The current solution makes a query every time we check for permission for dags that has a dot in the name. Not that I like it but I think it's better than other options I considered such as changing how we name dags for subdag. That's notgood in UX. Another option I considered was making a query when parsing, that's not good and it's avoidedby passing root_dag to resource_name_for_dagCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
"Check bag DAG schedule_interval match tiemtable (#23113)This guards against the DAG's timetable or schedule_interval from beingchanged after it's created. Validation is done by creating a timetableand check its summary matches schedule_interval. The logic is notbullet-proof, especially if a custom timetable does not provide a usefulsummary. But this is the best we can do.",1
fix: patches #24215. Won't raise KeyError when 'create_job_kwargs' contains the 'Command' key. (#24308),1
Fix D202 issue (#24322),0
Check for run_id for grid group summaries (#24327),1
Workaround job race bug on biguery to gcs transfer (#24330)Fixes: #24277,0
Update release notes for RC2 release of Providers for May 2022 (#24307)Also updates links to example dags to work properlyfollowing #24331,1
Add typescript (#24337),1
"Add scripts that provide good links to example dags (#24348)The documentation generated used ""main"" in the URL of theexample DAGs.The generation of the links have been fixed in the #24307, but thisPR adds a tool that has been used to fix existing links in generateddocumentation resulting in https://github.com/apache/airflow-site/pull/610Fixes: #24331",0
Optimize calendar view for cron scheduled DAGs (#24262),2
Added small health check server and endpoint in scheduler and updated… (#23905)Co-authored-by: Kamil Breguła <mik-laj@users.noreply.github.com>,1
fix 2.3.2 release date. (#24370),5
Fix: `emr_conn_id` should be optional in `EmrCreateJobFlowOperator` (#24306)Closes: #24318,1
Refactor `DagRun.verify_integrity` (#24114)This refactoring became necessary as there's a necessity to add additional codeto the already exisiting code to handle mapped task immutability during run. The additionalcode would make this method difficult to read. Refactoring the code will aid understanding andhelp in debugging.,0
Add tests for the grid_data endpoint (#24375)The one fix/change here was to include the JSON content response here so that`resp.json` works in the test.,3
Rework contract of try_adopt_task_instances method (#23188)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <tp@astronomer.io>,1
Migrate Google calendar system test to new design AIP-47 (#24334)related: #22430,1
Fix S3KeySensor. See #24321 (#24378),0
Refactoring EmrClusterLink and add for other AWS EMR Operators (#24294),1
"Migrate Google sheets example DAG to new design AIP-47 (#24351)related: #22430, #22447",1
Convert SNS Sample DAG to System Test (#24384),3
"Remove bigquery example already migrated to AIP-47 (#24379)related: #22430, #22447, #22311",4
"Migrate Google azure_fileshare example DAG to new design AIP-47 (#24349)related: #22430, #22447",1
Add Ping Zhang to committers list (#24391)* Add Ping Zhang to committers list* P before Q,1
Migrate Zendesk example DAGs to new design AIP-47 (#24053)closes: #22471,1
Migrate Google calendar example DAG to new design AIP-47 (#24333)related: #22447,1
Update Dockerfile (#24397),2
"Better diagnostics for ARM for MySQL and MSSQL (#24185)Until we have debian suppor tof MySQL and MSSQL ARM, runnignthose on ARM platform is not supported. However error about itwas not clear (pulling docker image failed).This PR adds platform checking also in breeze and fails fastwithout even attempting to enter breeze shell when you are onARM and wants to run MsSQL or MySQL breeze shell.Also some errors with running different backend versions viabreeze have been removed.",4
Refresh list of committers (#24398),5
"Add Task Logs to Grid details panel (#24249)* WIP* Add Common LogLink component.* Split details in two columns.* Retrieve task log url from metadata.* Checkbox for requesting full logs, add tabs.* Persist tab preference into the local storage.* Task group tab fallback fix.* Simple LinkButton test for shared component.* Use codeblock component and auto scroll to bottom* Add checkbox for line wrapping toggle* Remove animation scroll into view, fix logs mapped tasks.* Add more tests.* Fix replace issue for certain tasks.* Add LogLink Internal test.",3
Mask secrets in stdout for 'airflow tasks test' (#24362)A stdout redirector is implemented to mask all values to stdout andredact any secrets in it with the secrets masker. This redirector isapplied to the 'airflow.task' logger.Co-authored-by: Alex Kennedy <alex.kennedy@astronomer.io>,2
misc: create new process group by `set_new_process_group` utility (#24371),1
Deprecate remaining occurrences of `bigquery_conn_id` in favor of `gcp_conn_id` (#24376)* Replace the remaining occurrences of bigquery_conn_id with gcp_conn_id* Deprecate remaining bigquery_conn_id usagesCo-authored-by: Kian <kian.eliasi@cafebazaar.ir>,5
Expose SQL to GCS Metadata (#24382),5
"DebugExecutor use ti.run() instead of ti._run_raw_task (#24357)The DebugExecutor previously executed tasks by calling the ""private""ti._run_raw_task(...) method instead of ti.run(...). But the lattercontains the logic to increase task instance try_numbers when running,thus tasks executed with the DebugExecutor were never getting theirtry_numbers increased and for rescheduled tasks this led to off-by-oneerrors (as the logic to reduce the try_number for the reschedule wasstill working while the increase was not).",1
Fix flaky order of returned dag runs (#24405)There was no ordering on a query returning dag_runs when it comesto grid view. This caused flaky tests but also it would havecaused problems with random reordering of reported dagruns in theUI (it seems).This change adds stable ordering on returned Dag Runs:* by dag_run_id (ascending) ascNo need to filter by map_index as there will be always max onereturned TI from each dag run,1
"The timeouts for Helm tests are far too small for ""full tests"" (#24408)When ""Full tests needed"" are run, then the Helm tests take farmore time because they are running mor combinations of executorand Python version. Such tests will timeout now.This PR increases the timeout",1
Fix HttpHook.run_with_advanced_retry document error (#24380)related: #9569,0
Close #24303 (#24394)Provide details on how to pull airflow image from a private repository,1
Add release cadence information to provider support in README (#24367)I noticed that we have not mentioned anywhere that release cadencefor providers is roughly monthly.,1
Add note on subtle logical date change in 2.2.0 (#24413),4
"Revert ""Remove custom signal handling in Triggerer (#23274)"" (#24390)This reverts commit 6bdbed6c43df3c5473b168a75c50e0139cc13e88.",5
Fix links to sources for examples (#24386)The links to example sources in exampleinclude have been broken in anumber of providers and they were additionally broken by AIP-47.This PR fixes it.Fixes: #23632Fixes: https://github.com/apache/airflow-site/issues/536,0
Improve grid rendering performance with a custom tooltip (#24417)* fully custom tooltip* use a customized chakra tooltip* update notice,5
Add salesforce_default to List Connection (#24347)* add salesforce_default to List Connection* add attributes to salesforce_default,1
Add Flux to chart gitops docs (#24288),2
Add Trakken to list of companies using Airflow (#24429)Co-authored-by: Ilias Troullinos <htrul18@gmail.com>,1
fix typo in google provider additional extras (#24431),1
Grid task logs filtering and local time (#24403)* Logs filtering + local datetime* Add tests* Reset fields on task change if not availables.* Update following code review.* Fix select width,0
Add AWS Batch & AWS CloudWatch Extra Links (#24406),2
Update Oracle library to latest version (#24311),3
Upgrade to react 18 and chakra 2 (#24430)* Upgrade to react 18 and chakra 2* upgrade emotion cache* fix framer-motion and (some) testing issues,0
Update errors.rst (#24412)Fix wording in some sentences,0
Vendor in the bitnami chart (#24395)After the Bitnami fiascohttps://github.com/bitnami/charts/issues/10539We lost trust in bitnami index being good and reliable sourceof charts. That's why we vendored-in the postgres chart needed forour Helm chart.Fixes: #24037,2
"Clarify ""significant"" newsfragment vs significant newsfragment _type_ (#24450)Previously was tiny bit ambiguous and could make it appear that any newsfragment type could have summary + body.  Also I remove the word ""simply"" just because it's a pet peeve of mine.",1
`TI.log_url` fix for `map_index` (#24335),0
Convert Cloudformation Sample DAG to System Test (#24447),3
"Add CI-friendly progress output for tests (#24236)This is the first step to run breeze tests in parallel in CI.This flag adds ""limited progress"" output when running testswhich means that the runnig tests will just print few lines withpercent progress and color status indication from last fewprogress lines of Pytest output, but when it completes, the whole output isprinted in a CI group - colored depending on status.The final version (wnen we implement parallel test execution) shouldalso defer writing the output to until all tests are completed, butthis should be a follow-up PR.",3
Docs: Fix default 2.2.5 log_id_template (#24455)I accidentally got the wrong default for 2.2.5 when documenting howto fix elasticsearch remote logging after upgrading to 2.3.0+.,2
"Don't crash scheduler if exec config has old k8s objects (#24117)From time to time k8s library objects change their attrs.  If executor config is stored with old version, and unpickled with new version, we can get attribute errors that can crash the scheduler (see https://github.com/apache/airflow/issues/23727).Here we update handling so that we fail the task but don't crash the scheduler.",0
First attempt to have CI-controlled process of releasing PROD image (#24433),5
Use insert_job in the BigQueryToGCPOpertor and adjust links (#24416)* Use insert_job in the BigQueryToGCPOpertor and adjust links,2
"Speed up grid_data endpoint by 10x (#24284)* Speed up grid_data endpoint by 10xThese changes make the endpoint go from almost 20s down to 1.5s and thechanges are two fold:1. Keep datetimes as objects for as long as possible   Previously we were converting start/end dates for a task group to a   string, and then in the parent parsing it back to a datetime to find   the min and max of all the child nodes.   The fix for that was to leave it as a datetime (or a   pendulum.DateTime technically) and use the existing   `AirflowJsonEncoder` class to ""correctly"" encode these objects on   output.2. Reduce the number of DB queries from 1 per task to 1.   The removed `get_task_summaries` function was called for each task,   and was making a query to the database to find info for the given   DagRuns.   The helper function now makes just a single DB query for all   tasks/runs and constructs a dict to efficiently look up the ti by   run_id.* Add support for mapped tasks in the grid data* Don't fail when not all tasks have a finish date.Note that this possibly has incorrect behaviour, in that the end_date ofa TaskGroup is set to the max of all the children's end dates, even ifsome are still running. (This is the existing behaviour and is notchanged or altered by this change - limiting it to just performancefixes)",0
Update release docs for Google and Oracle providers (#24461),1
"Revert ""Fix await_container_completion condition (#23883)"" (#24474)This reverts commit 42abbf0d61f94ec50026af0c0f95eb378e403042.",4
Remove framer-motion from custom tooltip (#24449)* remove framer-motion from custom tooltip* remove framer-motion type declaration file,2
Fix semver compare number for `jobs check` command (#24480)From what I can tell this command was added in 2.1.0 not 2.0.0.,1
"Use `get_hostname` instead of `socket.getfqdn` (#24260)We allow users to configure a different function to determine thehostname, so we should use that consistently when we need the hostname.",1
"Return empty dict if Pod JSON encoding fails (#24478)When UI unpickles executor_configs with outdated k8s objects it can run into the same issue as the scheduler does (see https://github.com/apache/airflow/issues/23727).Our JSON encoder therefore needs to suppress encoding errors arising from pod serialization, and fallback to a default value.",0
Small cleanup of ``get_current_context()`` chapter (#24482),1
"Improve production image release workflow (#24481)Few improvements:* direct link to workflow in the docs* ""green"" success screenshots (separate screenshot for rc)* job and step names contain both Airflow version and python version* running subsequent build with same version will cancel past  in-progress build (in case you quickly make constraint fis for  example and re-run)",1
Credit image used in tutorial.py (#24490)The image used in the example task documentation is [licensed under the Creative Commons license](https://xkcd.com/license.html). This change adds a mention of the author and a link to the license under the image.,2
Remove internet explorer support (#24495)* remove internet explorer support* remove from webpack,4
"Upgrade to webpack 5 (#24485)* update webpack-cli, eslint, stylelint, babel* revert stylelint changes* update more plugins* update to webpack 5* remove all resolutions",4
"Add indexes for CASCADE deletes for task_instance (#24488)When we add foreign keys with ON DELETE CASCADE, and we delete rows in the foreign table, the database needs to join back to the referencing table.  If there's no suitable index, then it can be slow to perform the deletes.",4
"Restore tooltip animation (#24503)* update webpack-cli, eslint, stylelint, babel* revert stylelint changes* update more plugins* update to webpack 5* restore animation with framer-motion v6",5
"Fix toast messages (#24505)* update webpack-cli, eslint, stylelint, babel* revert stylelint changes* update more plugins* update to webpack 5* use portalProps to properly style toast msgs",1
"Fix deprecated log_id_template value (#24506)We had the wrong old default in our deprecated value upgrading logic, sothe 2.2.5 default wasn't actually being upgraded.",2
"Seed log_template table (#24511)Seed the log_template table with the default values pre 2.3.0 so logretrieval still works post upgrade. This only worked previously if youhave the default in your config, now it works even if you don't.",1
Update ui.rst (#24514)Fix minor typos,2
Do not calculate grid root instances (#24528)* do not calculate grid root instances* fix tests,3
"Rename grid.css to chart.css (#24529)Given it was no longer used in the new grid view and only the chartpages, _and_ we want to have a new css file for the react gridcomponents it is better to rename this one now.",1
"Don't rely on current ORM structure for db clean command (#23574)For command DB clean, by not relying on the ORM models, we will be able to use the command even when the metadatabase is not yet upgraded to the version of Airflow you have installed.Additionally we archive all rows before deletion.",4
"Restore capability of reproducing CI failures using new Breeze (#24402)The old breeze-legacy used to have a possibility of very easyreproduction of CI failures by executing the right breeze commandthat contained the commit hash of the PR being tested. This hasbeen broken for some time after we migrated to the new breeze,but finally it was the time when it was needed again.This PR brings back the capability by:* addding --image-tag parameters for tests, shell and start-airflow  commands* if --image-tag is specified, then rather than building the  image, it is pulled using the specified hash* if --image-tag is specified, the local sources are not mounted  to breeze when started, but the sources already embedded in the  image are used (""skipped"" set for --mount-sources).* new ""removed"" command value is added to --mount-sources, it  causes breeze command to remove the sources from the image (it  is used when installing airflow during the tests for specified  version (it's automatically used when --use-airflow-version  is used).",1
Rename charts.css to chart.css (#24531)Just a missed rename.,2
Update `actual_file_to_check` with rendered `path` (#24451),2
Fix mapped task immutability after clear (#23667)We should be able to detect if the structure of mapped task has changedand verify the integrity.This PR ensures thisCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,4
Move LOAD_DEFAULT_CONNECTIONS env var to database config section in CI (#24536),5
"Get rid of TimedJSONWebSignatureSerializer (#24519)The TimedJSONWebSignatureSerializer has been deprecated from theitsdangerous library and they recommended to use dedicatedlibraries for it.https://github.com/pallets/itsdangerous/issues/129Since we are going to move to FAB 4+ with #22397 where newer version ofitsdangerous is used, we need to switch to another library.We are already using PyJWT so the choice is obvious.Additionally to switching, the following improvements were done:* the use of JWT claims has been fixed to follow JWT standard.  We were using ""iat"" header wrongly. The specification of JWT only  expects the header to be there and be valid UTC timestamp, but the  claim does not impact maturity of the signature - the signature  is valid if iat is in the future.  Instead ""nbf"" - ""not before"" claim should be used to verify if the  request is not coming from the future. We now require all claims  to be present in the request.* rather than using salt/signing_context we switched to standard  JWT ""audience"" claim (same end result)* we have now much better diagnostics on the server side of the  reason why request is forbidden - explicit error messages  are printed in server logs and details of the exception. This  is secure, we do not spill the information about the reason  to the client, it's only available in server logs, so there is  no risk attacker could use it.* the JWTSigner is ""use-agnostic"". We should be able to use the  same class for any other signatures (Internal API from AIP-44)  with just different audience* Short, 5 seconds default clock skew is allowed, to account for  systems that have ""almost"" synchronized time* more tests addded with proper time freezing testing both  expiry and immaturity of the requestThis change is not a breaking one because the JWT authenticationdetails are not ""public API"" - but in case someone reverse engineeredour claims and implemented their own log file retrieval, weshould add a change in our changelog - therefore newsfragmentis added.",1
Add imports to deferring code samples (#24544),2
"Add verification steps when releasing the images. (#24520)After the images are pushed in CI we are running the verificationof the AMD image now.This cannot be really done during building and pushing the image,because we are using multi-platform images using remote buildersso the image is not even available locally, so we need to actuallypull the images after they are built in order to verify them.This PR adds those features:* ability to pull images for verification with --pull-flag* ability to verify slim images (regular tests are skipped and  we only expect the preinstalled providers to be available* the steps to verify the images (both regular and slim) are  added to the workflow",1
Preserve original order of providers' connection extra fields in UI (#24425)* Preserve original order of providers' connection extra fields* Sort widgets before printed in cli command `airflow providers widgets`,1
Update flask-appbuilder authlib/oauth dependency (#24516)The dependency we have for flask-appbuilder oauth authentication(for github/google authentication) should follow the limitsthat flask-appbuilder current version has. We added authlib therebut apparently FAB currently limits authlib to <= 1.0 - we shouldfollow fab rather than have our own dependency here.This has been pointed out inhttps://github.com/dpgaspar/Flask-AppBuilder/issues/1861,0
Add DeprecationWarning for column_transformations parameter in AutoML (#24467),5
Fix bugs in URI constructor for MySQL connection (#24320)* Fix bugs in URI constructor for MySQL connection* Update unit tests,3
"Add missing types to FSHook (#24470)Fix mypy error :```error: Call to untyped function ""FSHook"" in typed context  [no-untyped-call]```When using :```pythonfs_hook = FSHook(fs_conn_id)```",1
fix: RedshiftDataHook and RdsHook not use cached connection (#24387),1
"Migrate Google gcs_to_sheets DAG to new design AIP-47 (#24501)related: #22430, #22447",1
GCSDeleteObjectsOperator empty prefix bug fix (#24353),0
Close the MySQL connections once operations are done. (#24508),5
"Migrate Google search_ads DAG to new design AIP-47 (#24298)related: #22447, #22430",1
Convert SQS Sample DAG to System Test (#24513),3
Convert sftp hook to use paramiko instead of pysftp (#24512),2
Make extra_args in S3Hook immutable between calls (#24527),1
Re-serialize all DAGs on 'airflow db upgrade' (#24518),5
"Migrate Google example DAG mysql_to_gcs to new design AIP-47 (#24540)related: #22447, #22430",1
Add TrinoOperator (#24415)* Add TrinoOperator* Enable multi query execution with TrinoOperator,1
Doc: Add hyperlinks to Github PRs for Release Notes (#24532)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
"Update description of installing providers separately from core (#24454)The description of how to install providers separately from thecore was wrong. It suggested installing main constraints, butin fact we are not able to give any guarantees when installingproviders this way so no constraints can guarantee consistentset of dependencies and the user is on their own in this case.This PR corrects the installation docs.",2
"Migrate Google example DAG oracle_to_gcs to new design AIP-47 (#24542)related: #22447, #22430",1
Fix the error caused by passing unused context in JiraSensor.poke (#23352)* Fix the error caused by passing unused context in JiraSensor.pokeThe sample ticket object used in the test is mocked more realistically and the original field_checker_func is used.,1
Clarify that users should not use Maria DB (#24556),5
Add `on_kill()` to kill Trino query if the task is killed (#24559)This PR is a follow up to PR #24415.It adds on_kill method to the TrinoOperator to kill Trino query if Airflow task is killed,1
"Change requests intersphinx inventory to 'stable' (#24575)It was using 'master', but the URL is no longer available. We could use'main', but 'stable' seems much more sensible.",1
Switch Markdown engine to markdown-it-py (#19702),5
Fix typo (#24568),2
"Added instructions on what to do if your command images are regenerated (#24581)In case there are conflicting changes to breeze command in severalPRs, you might get conflicting images printed. in such case youshould run `breeze regenerate-command-images`",1
"Use found pod for deletion in KubernetesPodOperator (#22092)Due to bad user configuration, it's possible that pod creation fails because pod with name already exists.  Then in cleanup, the pod that was already there is deleted.  When we use find_pod it looks up based on more than name, so we are confident if we found a pod there it's safe to delete.",4
"Switch to building images in parallell (#24580)In the new Breeze, switching to using parallelism is a ... breeze.This PR adds the capability of building the images in parallel in Breezelocally - for breeze command, but also uses this capability to build theimages in parallel in our CI. Our builds are always executed onpowerful, big machines with lots of CPU and docker run in memoryfilesystem with 32GB RAM, so it should be possible to run all builds inparallel on a single machine rather then spin off parallel machines torun the builds using the matrix strategy of Github Actions.Generally speaking - this will either speed up or get 4x cost saving forthe build steps for all the ""full test needed"" PRs as well as all themain builds.There are a number of savings and improvements we can achieve this way:1) less overhead for starting and runnning the machines2) seems that with the new buildkit, the parallel builds are not   suffering from some sequential locks (as it used to be, so   we are basically do the same job using 25% resources for building   the images.3) we will stop having random ""one image failed to build"" cases - they   will all either fail or succeed.4) Less checks in the output5) Production builds will additionally gain from single CI image   pulled in order to perform the preparation of the packages   and single package preparation step - it will save 4-5 minutes   per image.The disadvantage is a less clear output of such parallel build whereoutputs from multiple builds will be interleaved in one CI output.",1
Fix occasional test failures where pid matches part of another PID (#24584)Solves problems that intermittently fail our builds:```  '135' is contained here:    ith pid: 10135  ?            +++```,0
Add test_connection method to Trino hook (#24583),1
"Add DagWarning model, and a check for missing pools (#23317)We can use this to track (and surface to the user in the web UI) configuration problems that don't rise to the level of failing the dag parse, such tasks that reference nonexistent pools.",2
"Amazon appflow (#24057)* Add Amazon AppFlow hook.* Add Amazon AppFlow operators.* Add Amazon AppFlow examples.* Add Amazon Appflow docs.* Apply comments/docs patterns.* Removing the ""private"" attribute signal and more.* Fix task_ids for example_appflow.* Move datetime_to_epoch() to utils and more.* Fix the AppflowBaseOperator name.* Ignore AppflowBaseOperator during structure check.* test_short_circuit refactor.* Add get_airflow_version.* Update airflow/providers/amazon/aws/hooks/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Addressing Josh's requests.* Add cached_property to AppflowHook* Update airflow/providers/amazon/aws/hooks/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update airflow/providers/amazon/aws/operators/appflow.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>* Update Josh's comment.* Update cached_property import.* Fix mypy.Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>",1
Update providers to use functools compat for ``cached_property`` (#24582),5
fix TestLocalTaskJob tests (#24432),3
Remove special serde logic for mapped op_kwargs (#23860)Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>,1
Pattern parameter in S3ToSnowflakeOperator (#24571),1
Patch sql_alchemy_conn if old Postgres schemes used (#24569),1
Update breeze mypy check documentation (#24609),2
"Migrate jsx files that affect run/task selection to tsx (#24509)* convert all useSelection files to tsUpdate grid data ts, remove some anys* yarn, lint and tests* convert statusbox to ts* remove some anys, update instance tooltip* fix types* remove any, add comment for global vars* fix url selection and grid/task defaults* remove React.FC declarations* specify tsconfig file path* remove ts-loader",4
"Upgrade FAB to 4.1.1 (#24399)* Upgrade FAB to 4.1.1The Flask Application Builder have been updated recently tosupport a number of newer dependencies. This PR is theattempt to migrate FAB to newer version.This includes:* update setup.py and setup.cfg upper and lower bounds to  account for proper version of dependencies that  FAB < 4.0.0 was blocking from upgrade* added typed Flask application retrieval with a custom  application fields available for MyPy typing checks.* fix typing to account for typing hints added in multiple  upgraded libraries optional values and content of request  returned as Mapping* switch to PyJWT 2.* by using non-deprecated ""required"" claim as  list rather than separate fields* add possibiliyt to install providers without constraints  so that we could avoid errors on conflicting constraints when  upgrade-to-newer-dependencies is used* add pre-commit to check that 2.4+ only get_airflow_app is not  used in providers* avoid Bad Request in case the request sent to Flask 2.0 is not  JSon content type* switch imports of internal classes to direct packages  where classes are available rather than from ""airflow.models"" to  satisfy MyPY* synchronize changes of FAB Security Manager 4.1.1 with our copy  of the Security Manager.* add error handling for a few ""None"" cases detected by MyPY* corrected test cases that were broken by immutability of  Flask 2 objects and better escaping done by Flask 2* updated test cases to account for redirection to ""path"" rather  than full URL by Flask2Fixes: #22397* fixup! Upgrade FAB to 4.1.1",0
Fix recording console for new rich-click 1.5 (#24611),1
Add note about image regeneration in June 2022 (#24524)The note is about image changes after refreshing are now betterorganized (around date of the change) - this should be more usefulby the users who will look why their images have been refreshed.Related to: #24516,1
"Fix errors revealed on autoupgrade of breeze (#24612)Recent changes to Breeze cause it to fail in certain situations,especially at self-upgrade (which was generated by today'supgrade with rich-click).There were two problems:* docker volume inspect missed 'volume' and it caused sometimes  failures in CI* inputimeout dependency was missing after recent update to  pre-commit venvs",5
Fix usage of `click.get_terminal_size()` (#24616)We were ignoring mypy error instead of fixing it.click had removed `get_terminal_size` and recommend using `shutil.get_terminal_size`,1
Use sql_alchemy_conn for celery result backend when result_backend is not set (#24496),1
"Implement Azure Service Bus Queue Operators (#24038)Implemented Azure Service Bus Queue based Operator's to create queue, send message to the queue and receive message(list of message or batch message) and delete queue in azure service - Added `AzureServiceBusCreateQueueOperator`- Added `AzureServiceBusSendMessageOperator`- Added  `AzureServiceBusReceiveMessageOperator`- Added `AzureServiceBusDeleteQueueOperator`- Added Example DAG- Added Documentation- Added hooks and connection type in - provider yaml file- Added unit Test case, doc strings",2
"Lay the groundwork for migrating Airflow CLI to Rich+Click (#24590)This is the first PR of what will be a series of PRs breaking up #22613 into smaller, more reviewable chunks. The end result will be rewriting the existing `airflow` CLI to use Click instead of argparse. For motivation, please see #22708.This PR installs Click, adds constraints to Rich_Click so we can rely on some nice features in recent versions of that, adds a new barebones `airflow-ng` console script, and tweaks some CLI internals to be more flexible between argparse and Click.To see how this initial groundwork will be used by future PRs, see #22613, and to see how some of this will be used please see #24591.",1
Upgrade FAB to 4.1.2 (#24619),5
"We now need at least Flask-WTF 0.15 (#24621)We upgraded flask and werkzeug in #24399, and updated the constraints,but not everyone uses them (such as me in my local virtual environmentwhen developing) so the min version in setup.cfg has to match as well",5
update `task-generated mapping` example (#23424)Co-authored-by: James Timmins <jameshtimmins@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,5
docs: sqlalchemy link update (#24627)following to this change: https://github.com/apache/airflow/pull/22114,4
Limit azure-servicebus to not be used on ARM (#24635)Azure service bus uses uamqp which does not build for ARM architectureand we need to disable it as a dependency for ARM.,1
Add gcp_conn_id argument to GoogleDriveToLocalOperator (#24622)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
"Add --platform to breeze shell and start-airflow (#24626)This allows Breeze to run images of another platform, such as AMD64 onan ARM machine.",1
Add %z for %(asctime)s to fix timezone for logs on UI (#24373),2
ExternalTaskSensor respects soft_fail if the external task enters a failed_state (#23647)* Respecting soft_fail in ExternalTaskSensor when the upstream tasks are in the failed state (#19754)- Changed behaviour of sensor to as above to respect soft_fail- Added tests of new soft_fail behaviour (#19754)- Added newsfragment and improved sensor docstring,2
"Have consistent types between the ORM and the migration files (#24044)We currently don't compare column types between ORM and the migration files. Some columns in the migration files have different types from the same columns in the ORM.Here, I made effort to match the types in migration files with the types in ORM, using the migration files as the source of truth in most cases.I couldn't convert the MySQL VARCHAR collation in db(utf8_bin) to use the one in ORM(utf8mb3_bin). It seems it's not possible to convert a collation of an already existing column in MySQL.",1
Fix timestamp defaults for sensorinstance (#24638)Constant values were used where callables were intended.,1
"Allows to specify different Python base image when building images (#24634)Today Python released new images that seems to break some ofour dependencies (at least on M1/ARM). This PR adds a workaroundpossibility to add --python-image option to override thedefault, latest image with any other version released previouslyuntil we diagnose and fix the real issue.",0
Remove short version of colour flag (#24639),4
"Convert selective checks to Breeze Python (#24610)Instead of bash-based, complex logic script to perform PR selectivechecks we now integrated the whole logic into Breeze Python code.It is now much simplified, when it comes to algorithm. We'veimplemented simple rule-based decision tree. The rules describingthe decision tree are now are now much easierto reason about and they correspond one-to-one with the rulesthat are implemented in the code in rather straightforward way.The code is much simpler and diagnostics of the selective checkshas also been vastly improved:* The rule engine displays status of applying each rule and  explains (with yellow warning message what decision was made  and why. Informative messages are printed showing the resulting  output* List of files impacting the decision are also displayed* The names of ""ci file group"" and ""test type"" were aligned* Unit tests covering wide range of cases are added. Each test  describes what is the case they demonstrate* `breeze selective-checks` command that is used in CI can also  be used locally by just providing commit-ish reference of the  commit to check. This way you can very easily debug problems and  fix themFixes: #19971",0
Switch to new selective-checks in label-when-reviewed workflow (#24651)When #24610 was implemented I missed the label-when-reviewed workflow,1
Cleanup references to selective checks (#24649)Selective checks docs have been moved to breeze as part of #24610but some of the references were still left.This PR cleans it up.,4
"Remove misleading message from CI (#24650)The CI jobs do not usually have mypy_cache volume created andalmost all jobs print misleading ""mypy_cache_volume"" error atthe beginning.This is a noise - we are not interested in this stderr printedmessage - we are only interested in returncode from checkingif it exists.",0
"Remove selective checks from the ""release workflow"" (#24655)Missed that one too :(",1
"Migrate Google example DAG bigquery_transfer to new design AIP-47 (#24543)related: #22447, #22430",1
Change postgres auth method (#24640),4
Use target commit SHA for build image workflow (#24659)The build-image workflow should use TARGET_COMMIT_SHA as theselective check COMMIT_REF otherwise it might not buildimage when needed,1
"Handle ""workflow_run"" event properly in selective-check (#24656)Unfortunately testing workflow_run is a bit difficult becausethe changes are only effective after merging them.Fixing (hopefully) yet another mistake in the workflow run wherecommit hash was passed as event name (?)We are going to handle ""workflow_run"" as valid event type so thisshould now work without passing any event.",4
"Fix handling of GitHub Event types for new selective checks (#24665)One more finding after merging the selective checks in Python- I missed a case of ""pull_request_target"".Fixed and added more tests.",3
S3ToSnowflakeOperator: escape single quote in s3_keys (#24607),1
"Fix behaviour of build/pull after recent Breeze changes (#24657)The behaviour of Breeze after some recent changes related topulling and building images in parallel have been slightly broken.Nothing serious but slightly annoying behaviour:* when starting breeze shell, the image was attempted to be  build even if it was not needed (but cache efficiency made it fast  enough to not be too annoying (unless we updated to newer  python base image* breeze pull command for ""latest"" branch makes no sense any more -  we stopped pushing ""latest"" image to ghcr.io, we only push  cache and ""tagged"" images. We are now turning --image-tag as  required in ""pull_image"" and when someone specifies latest,  error and helpful message is printed* --force-build flag in ""shell-related-commands"" was not  properly propagated to build-image so it did not actually  force image building.All those problems are fixed now.",0
"Turn inability of creating mypy-cache into warning (#24677)The mypy cache creation might happen before checking if goodversion of docker is installed and it works, so we want to turnit into a warning rather than error when it is not possible tocreate one.",1
"Add Core SQL Provider (#24476)Adds operators, tests, and new and updated docs for a Core SQLProvider. This provider is made in favor of adding these operatorsto the existing SQL operators in core Airflow. The new providerwill allow for quicker development cycles.closes: #23874, #24422related: #23915",1
"Add dataset model (#24613)Add model for storing references to datasets, a fundamental component of AIP-48.",5
Pass worker annotations to generated pod_template_file (#24647),2
Updated Snowflake provider connection documentation (#24573)* Updated Snowflake provider connection documentation* Moved paragraph down.* Minor rephrasing to improve clarity,1
keep Log scrolling only to nearest div (#24689),2
TriggerDagRunOperator.operator_extra_links is attr (#24676)There's absolutely no reason this needs to be a property. And it cannotbe since we need to access this at the class level.,5
Remove commented out code (#24685),4
Clean up task decorator type hints and docstrings (#24667),2
Rename 'resources' arg in Kub op to k8s_resources (#24673),5
Clear next method when clearing TIs (#23929),5
Add `test_connection` method to Databricks hook (#24617)This PR enables the test button on the airflow UI for testing whether the Databricks connection works using the connection params filled by the user,1
`WebHDFSHook` Bugfix/optional port (#24550),0
Add batch option to `SqsSensor` (#24554)Add batch option to `SqsSensor`Co-authored-by: TungHoang <st.hoang@jellysmack.com>Co-authored-by: D. Ferruzzi <ferruzzi@amazon.com>,1
"Migrate Google example DAG s3_to_gcs to new design AIP-47 (#24641)related: #22447, #22430",1
"Remove ""Label when approved"" workflow (#24704)The labelling workflow has proven to be far less useful than wethought and some of the recent changes in selective checks madeit largely obsolete. The committers can still add ""full tests needed""label when they think it is needed and there is no need to labelthe PRs automatically for that (or any other reason).For quite a while this workflow is basically a useless noise.",1
don't try to render child rows for closed groups (#24637),1
"Add `airflow_kpo_in_cluster` label to KPO pods (#24658)This allows one to determine if the pod was created with in_cluster configor not, both on the k8s side and in pod_mutation_hooks.",1
Fix Grid vertical scrolling (#24684)* fix vertical scrolling* fix flex grow for panel open/close* add type checking* add duration axis component* remove details/grid width changesthis should be done in a separate PR,4
Remove `hook-class-names` from provider.yaml (#24702)* Remove `hook-class-names` from provider.yamlnow that providers>=2.2 there is no need for the hook-class-names any longer.refrence to https://github.com/apache/airflow/pull/17775* fix `airflow/provider.yaml.schema.json` and `provider_info.schema.json`* undo changes in `provider_info.schema.json`,5
fix document about response_check in HttpSensor (#24708),2
"Protect against using ""hook_params"" in ""get_hook"" until we move to 2.3+ (#24706)The ""hook_params"" method was added in Airflow 2.3 and we should notuse it in providers (yet).",1
"Add ARM image building for regular PRs (#24664)The image building for ARM is currently only done in the main buildonly to refresh cache, however there are sometimes cases whennew dependency (for example #24635) broke ARM image build and itwas only discovered after merge.This PR adds extra ARM-based build that should be run afterthe AMD64 build. It should not influence the depending steps,it should just signal failure of the PR if the ARM image cannotbe build.",0
Debounce status highlighting in Grid view (#24710)* Add delay / debounce to not always highlight tasks* fix linting* single delay variable at 200ms,0
Add AWS operators to create and delete RDS Database (#24099)* Add RdsCreateDbInstanceOperator* Add RdsDeleteDbInstanceOperator,5
Fix unnecessary check for ARM images (#24718)The ARM image build introduced in #24664 had problem with buildimage that was additionally checking for arm images which weremoved out to a spearate step,4
Script to filter candidates for PR of the month based on heuristics (#24654)This scripts proposes top candidates for PR of the monthbased on simple heuristics as discussed in the documenthttps://docs.google.com/document/d/1qO5FztgzJLccfvbagX8DLh1EwhFVD2nUqbw96fRJmQQ/edit?disco=AAAAZ-Ct0Bs&usp_dm=true,2
"Merge-friendly output of command hashes for breeze (#24711)We had just one hash generated from all commands in breeze and thatbasically meant that when there were two PRs on two differentcommands in Breeze, they resulted with merge conflict whichshould be solved with `breeze regenerate-command-images`.This change turns the hash output into a multi-command one - i.e.each command has its own hash, which will make it much moremerge-friendly - i.e. if two PRs will work on two different commandsthe rebase should result with merge rather than conflict.",5
Do not fail requeued TIs (#23846),0
"Grid task log, multi select and ts files migration. (#24623)* Multi select + ts files migration.* Fix conflicts and transfert tests to ts files* Add multi select on file source.* Update placeholder color* Update scrollIntoview.",5
Fix migration 0080_2_0_2 - Replace null values before setting column not null (#24585),1
Address all yarn test warnings (#24722)- remove outdated react-hooks library- add missing mock,1
"Use our yaml util in all providers (#24720)Our yaml util, which uses libyaml where possible, has been available incore since 2.0.2. Providers now require 2.2.0+, so we can safely use itnow.",1
`AirbyteHook` add cancel job option (#24593),1
Adding generic `SqlToSlackOperator` (#24663)* adding `SqlToSlackOperator`Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>,1
fix connection extra parameter `auth_mechanism` in `HiveMetastoreHook` and `HiveServer2Hook` (#24713)closes: https://github.com/apache/airflow/issues/24692,0
Upgrade more javascript files to typescript (#24715)* upgrade more js files to ts* upgrade more component files* fix linting issues,0
"Move provider dependencies to inside provider folders (#24672)The ``setup.py`` had ALWAUS contained provider dependencies,but this is really a remnant of Airlfow 1.10 where providerswere not separated out to subfolders of ""providers"".This change moves all the provider-specific dependenciesto provider.yaml where they are kept together with all otherprovider meta-data.Later, when we move providers out, we can move them toprovider specific setup.py files (or let provider-specificsetup.py files read them from provider.yaml) but this isnot something we want to do it now.The dependencies.json is now renamed to provider_dependencies.jsonand moved to ""airflow"" so tha it can be kept as part of thesources needed for sdist package to provide extras. Pre-commit stillgenerates the file as needed and it contains now both:* cross-provider-deps information which providers depend on each  other* deps - information what regular dependencies are needed for each  providerOn top of preparing to splitting providers it has the advantage,that there will be no more case where adding a dependency changefor provider will not run tests for that provider.",1
Fix grid date ticks (#24738)* fix date ticks* fix linting* fix LinkButton type error,0
Use LTS version of node for pre-commit checks (#24688),1
Update command from breeze to command (#24741),5
More typing and minor refactor for kubernetes (#24719),4
"Remove defaults from Airflow test config (#24263)* Remove defaults from Airflow test configAll of these removed values are the defaults (or in the case of the`dag_default_view`, the old default) and don't actually doanything being in this file.* Fix tests",3
"(GitHub CI) update most CI workflows (stale, build-images, etc) (#24705)* Update stale.yml* Update build-images.yml* Update release_dockerhub_image.yml* Update codeql-analysis.yml* Update ci.yml",5
Fix purge_inactive_dag_warnings filter (#24749),2
"Move fallible ti.task.dag assignment back inside try/except block (#24533) (#24592)* Move fallible ti.task.dag assignment back inside try/except blockIt looks like ti.task.dag was originally protected inside try/except,but was moved out at commit 7be87d* Remove unneeded variable annotationCo-authored-by: EJ Kreinar <ej@he360.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",4
Add support for KEDA HPA config to Helm Chart (#24220),2
Add warning to AIP-38 UI readme (#24764),2
Document built in Timetables (#23099),2
"Increase time to build the ARM image (#24765)Building ARM image is now done in parallel in case of CI buildsthat update dependencies. If the image is not refreshed, then it mighttake quite some time - more than the 50 minutes it's been allowed toThis PR increases both timeouts - the self-terminate timeout forthe ARM instance and timeout on the CI job that triggers it.",1
Update templates doc to mention `extras` and format Airflow Vars / Conns (#24735),2
docs: sqlalchemy link update (#24756),5
Add test_connection method to Azure WasbHook (#24771),1
"Fix selective checks to work for non-main-branch (#24769)The recent changes in selective checks introduced a few problemsin non-main branches:* constraints branch was not used by CI build in non-main branch  this was not a proble, (until it started to fail) because  other branches always work in ""upgradeDepencies"" case* default branch calculation did not account for the selective-check  comment that was run before default branch was retrieved. Therefore  we did not run build as from 2.3 branch* Some precomits should be skiped in non-main branch* Integration was also excluded from Provider Check* Two more commit outputs added:  * docs-filter -  depends whether you are with more packages  * skip-pre-commit - allows to dynamically choose which pre-commits    should be skipped",1
"Choose the right builder when pushing to ghcr.io registry for cache (#24780)When pushing cache we cannot use default builder because it hasno capability to push cache. We need to use airflow_cache for it,which has additionally the capacity of forwarding the buildsto a remote ARM instance in case cache is built for ARM imagestoo. Recently we added a new --builder flag to allow to choosethe builder, but our scripts and CI have not been modified toaccount for that (previously builder was chosen automatically butthat has proven to be limiting in some manual operations and italso allows to choose different names for builders in casesomeone wants to build their own pipeline of builds.",1
"Better diagnostics in case someone modified the dependencies file (#24763)The ""provider_dependencies.json"" file gets automatically generatedby pre-commit and in case it cannot be json-parsed, it will breakbuilding the CI image. This is a bit chicken-egg because onCI the image is needed to run pre-commits that could warn theuser this is the case (and the image fails build with a bitcryptic message).This PR improves the diagnostics:* it runs the pre-commit check before image building which will  fix the generated file (and will let the build run - only to  fail at the latest static-checks step* it prints warning to the user seeing pre-commit error to not  modify the file manually but let pre-commit do the job.",2
Restore Optional value of script_location (#24754)This fixes https://github.com/apache/airflow/issues/24753,0
Move Flask hook registration to end of file (#24776)Pylance is too clever here. It (correctly) detects the connect() call isa NoReturn and marks everything after it as unreachable. But this is notcorrect; Flask does trickery at runtime to make it not the case. Thesimplest workaround is to just move those registration code to the endof the file.,2
Align Black and blacken-docs configs (#24785),5
Update docstring in `SqlToSlackOperator` (#24759),1
Refactor DR.task_instance_scheduling_decisions (#24774),4
Force-remove container after DockerOperator execution (#23160)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>Co-authored-by: park.z <park.z@bybit.com>,1
Add workflow_dispatch to allowed events in selective_checks (#24807),1
"Make workflow dispatch"" trigger full build (all python versions) (#24808)",1
"Unified ""dash-name"" convention for outputs in ci workflows. (#24802)There were errors with retieving constraints branch caused byusing different convention for output names (sometimes dash,sometimes camelCase as suggested by most GitHub documents).The ""dash-name"" looks much better and is far more readable sowe shoud unify all internal outputs to follow it.During that rename some old, unused outputs were removed,also it turned out that the new selective-check canreplace previous ""dynamic outputs"" written in Bash as well.Additionally, the ""defaults"" are now retrieved via Python script, notbash script which will make it much more readable - both build_imagesand ci.yaml use it in the right place - before replacingthe scripts and dev with the version coming in from PR in caseof build_images.yaml.",1
Fix typos in PR template (#24796),2
"Update release process for Providers (#24680)This release process updates Provider's release approach and""mixed-governance"" model after the discussion and proposalhttps://lists.apache.org/thread/6ngq79df7op541gfwntspdtsvzlv1cr6",1
"Remove Tableau from Salesforce provider (#23747)* Remove Tableau from Salesforce providerTableau was split from Salesforce provider a year+ ago in https://github.com/apache/airflow/pull/14030We kept Tableau in Salesforce to preserve backward compatibility, it's time to remove the deprecated dependency.",4
"Prepare ARM images much faster and cheaper (#24813)Since we are now building ARM images in parallel, We need more powerfulmachines and we implemented in-memory docker, similarly as in our AMDinstances. The m6g.2xlarge  are quite a bit better than c6g.xlarge forour case:1) They have 8 vCPUs2) They have 32 GB memory (should be enough to build 4 ARM CI images3) Thye are Memory-optimised, and since docker is build in memory   the memory speed is the most important factorThis also allows to switch building all images (including cache)in parallel - so that we can have 1 job instead of 4 - similarlyas we have in case of regular AMD builds.Another advantage of it is that we loose far less time of theAMD instance which ""triggers"" the docker build, because this instancewill control 4 parallel builds at a time effectively, which willdecrease a lot of overhead connected with running the instance mostlyidle during the build (and since the builds will be generallyfaster, the overhead will be even smaller).",1
"Add ""generated"" folder to volumes mounted when ""MOUNT_SELECTED"" used (#24818)",1
Adding Docker context check for breeze (#24751),2
Remove `xcom_push` flag from `BashOperator` (#24824),1
Remove `xcom_push` flag from providers (#24823),1
"Migrate Google example DAG mssql_to_gcs to new design AIP-47 (#24541)related: #22447, #22430",1
perf(BigQuery): pass table_id as str type (#23141),4
"Serialize pod_override to JSON before pickling executor_config (#24356)* Serialize pod_override to JSON before pickling executor_configIf we unpickle a k8s object that was pickled under an earlier k8s library version, then the unpickled object may throw an error when to_dict is called.  To be more tolerant of version changes we convert to JSON using Airflow's serializer before pickling.",1
Add parameter to turn off SQL query logging (#24570),2
Add test_connection method to AWS hook (#24662),1
Datacatalog assets & system tests migration (AIP-47) (#24600),3
Update AWS Connection docs and deprecate some extras (#24670)* Update AWS Connection docs and deprecate some extras* Update docs and deprecated legacy local credentials file,2
Add configurable scheme for webserver probes (#22815)* Add configurable scheme for webserver probes- add scheme to webserver probes- update values schema- add unittest,3
Add support for Salesforce bulk api (#24473)* add support for Salesforce bulk api,1
Remove upper-binding for SQLAlchemy (#24819)There was a problem with custom classes for SQLAlchemy thatprevented it to work on MySQL. This PR removes the SQLAlchemyupper binding.This has been added as an issue in:https://github.com/sqlalchemy/sqlalchemy/issues/7660But apparently it's been fixed in one of the more recent SQLAlchemyreleases.,0
Replace all NBSP characters by whitespaces (#24797),5
Disable attrs state management on MappedOperator (#24772),1
`DockerOperator` fix cli.logs giving character array instead of string (#24726),2
Add cache_ok flag to sqlalchemy TypeDecorators. (#24499),1
"Remove ""bad characters"" from our codebase (#24841)* Remove ""bad characters"" from our codebaseWe had plenty of ""bad characters"" in our codebase that were notinvited and came here by accident. We want to get rid of those""bad characters"" once and for all.",1
"Fix cycle bug with attaching label to task group (#24847)The problem was specific to EdgeModifiers as they try to be""transparent"" to upstream/downstreamThe fix is to set track the upstream/downstream for the task groupbefore making any changes to the EdgeModifiers' relations -- otherwisethe roots of the TG were added as dependencies to themeslves!",1
improve grid date tick spacing (#24849),5
Removed duplicate 'this is' from docs (#24856),2
"Fix sequence of jobs for Basic Static checks (#24864)After recent changes to use Python for selective checks, wehave to checkout and install python in basic checks slightly later.It uses setup* files from Breeze to determine if cache needs tobe rebuild, so we need to checkout the code before setting upPython.",1
Send DAG timeout callbacks to processor outside of prohibit_commit (#24366),2
Update PR template (#24851),5
"Fix exception in mini task scheduler. (#24865)I introduced a bug in 2.3.0 as part of the dynamic task mapping workthat frequently made the mini scheduler fail for tasks involvingXComArgs.The fix is to alter the logic in BaseOperator's deepcopy to not set the`__instantiated` flag until all the other attributes are copied.For background the `__instantiated` flag is use so that when you do`task.some_attr = an_xcom_arg` the relationships are set appropriately,but since we are copying all the existing attributes we don't need to dothat, as the relationships will already be set!",1
Remove note about Python 3.10 support availability (#24861),1
"Fix docs build errors for Sphinx 5 (#24870)Sphinx 5 is a little bit more picky about docuementation linkerrors that Sphinx 4 had. Due to lack of job dependency (fixed nowvia #24866), we accidentally upgraded to Sphinx 5 in constraintsand it started to fail our builds. Fortunately the errors in docsare very few and we can easily fix it without reverting theconstraint change.",4
KubernetesExecutor: Use user-provided namespace from executor_config arg (#24342)* Always give precedence to pod from executor_config arg* Use only image and namespace values from pod_override_object,1
Add dependency of constraints on docs (#24866)The dependency between constraints and docs jobs in CI weremissing and it led to Sphinx 5 breaking our main builds becausethe constraints were updated even if the docs building failedThis change prevents similar case in the future - constraintswill not get updated if only docs build fails,0
update boring-cyborg.yml (#24872),5
Add Tabular provider (#23704),1
Add test_connection method to AzureFileShareHook (#24843),2
Add test_connection method to `GoogleBaseHook` (#24682)This PR adds test connection functionality to Google Cloud connection type in airflow UI,1
Small fixes on the TabularConnection (#24874),0
"Migrate Google example sql_to_sheets to new design AIP-47 (#24814)related: #22447, #22430",1
"Revert ""KubernetesExecutor: Use user-provided namespace from executor_config arg (#24342)"" (#24879)This reverts commit 1fe07e5cebac5e8a0b3fe7e88c65f6d2b0c2134d.",4
Improve version check for kubectl (#24882)kubectl 1.24.0 added a warning that broke the version detection.It leads to redownload kubectl everytime we run a breeze-legacy command.,1
"Add more selective provider tests (#24666)After implementing #24610 and few follow-up fixes, it is now easyto add more optimizations to our unit test execution in CI (andto give this capability back to our contributors).This PR adds capability of running tests for selected set ofproviders - not for the whole ""Providers"" group. You canspecify `--test-type ""Providers[airbyte,http]"" to only run testsfor the two selected providers.This is the step towards separating providers to separaterepositories, but it also allows to optimize the experience ofthe contributors developing only single provider changes (whichis vast majority of contributions).This also allows to optimize build and elapsed time needd to runtests for those PRs that only affects selected providers (again -vast majority of PRs).The CI selection of which provider tests is done now in SelectiveCheckcs - they are a bit smarter in just selecting the providersthat has been changed, they also check if there are any otherproviders that depend on it (we keep automatically updated bypre-commit dependencies.json file and this file determineswhich files should be run.",1
"Chart: Provision Standalone Dag Processor (#23711)Add Dag Processor templates and values to schema, at the momentthe standalone dag processor is disabled by default.",2
"Move all SQL classes to common-sql provider (#24836)The DBApiHook, SQLSensor are now part of the common.sql provider.",1
Add links for Google Kubernetes Engine operators (#24786)* Add links for Google Kubernetes Engine operators* Update unit tests for GKE operators,1
Upgrade FAB to 4.1.3 (#24884)no relevant changes found when comparing `airflow/www/fab_security` with FABsee https://github.com/dpgaspar/Flask-AppBuilder/compare/v4.1.2...v4.1.3,4
Add additional information to Airflow release doc (#24848)* Add additional information to Airflow release doc* Update dev/README_RELEASE_AIRFLOW.mdCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>* Update dev/README_RELEASE_AIRFLOW.mdCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>,1
Improve documentation to add path of Breeze in a Mac with Python (#24829)* Add path of Breeze in a Mac with Python,1
"Make sure that build-images workflow has permissions to read PR info (#24897)For a public repo this information is public so no extra permission isrequired, but just in case this workflow ends up against a private repowe need these permissions explicitly.",1
"Don't add init.py files to __pycache__ folders. (#24896)For instance, after running pytest I had these files created by thisstep:```Create missing init.py files in tests..................................................Failed- hook id: create-missing-init-py-files-tests- exit code: 1 Created /home/ash/code/airflow/airflow/tests/__pycache__/__init__.py Created /home/ash/code/airflow/airflow/tests/models/__pycache__/__init__.py Created /home/ash/code/airflow/airflow/tests/test_utils/__pycache__/__init__.py Created /home/ash/code/airflow/airflow/tests/test_utils/perf/__pycache__/__init__.py Created /home/ash/code/airflow/airflow/tests/test_utils/perf/perf_kit/__pycache__/__init__.py```(This change also stops the `os.walkdir` from needlessly recursing in totest_logs folder as well.)",3
Sync committers list with Apache project page (#24900),5
"Remove full provider's specification from TEST_TYPES (#24901)The new provider's selective tests introduce option to specifywhich providers should be tested (which is automaticallyused in CI). This works fine but in case when we removeProvider's tests from the list (MySQL/MsSQL) the removal wasdone only for the Provider's part, not for the detailed listof providers. That led to test attempting to run```[amazon,apache.hive,google,mysql,postgres]```for example.This PR fixes it by removing full Provider's specification.",1
Update PythonVirtualenvOperator Howto (#24782)* Update PythonVirtualenvOperator Howto,1
Add test connection functionality to `GithubHook` (#24903),1
Bind log server on worker to IPv6 address (#24755) (#24846),1
Add blocksize arg for ftp hook (#24860)Co-authored-by: Kevin George <“kevingeorge232@gmail.com”>,1
Convert Step Functions Example DAG to System Test (AIP-47) (#24643)* Add Amazon System Test Context BuilderThis factory class builds a task that will generate the environment idand fetch any variables at runtime and store them in xcom* Convert step functions example dag to system testMoved example_step_functions example dag AWS system test dir.Convert to AIP-47 standard.,3
"Get dataset-driven scheduling working (#24743)AIP-48 (https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-48+Data+Dependency+Management+and+Data+Driven+Scheduling) allows DAGs to reference upstream ""datasets"", and tasks to reference downstream datasets through outlets.  And when a DAG's upstream datasets have been updated, a dagrun will be triggered.This constitutes essentially the ""initial commit"" for AIP-48.  We're gonna have to make changes and iterate and tweak things here and there, but it's a starting point that implements the basic functionality of the AIP.",1
"Make sure builder is created as needed before warming up (#24875)In case of parallel cache building, we want to warm-up the builder and makesure the builder is created. It might or might not be created byin case of ARM builds (and then it will point to ARM builder) but incase we run AMD builds, we should attempt to check and re-create it ifneeded.",1
"Run ""fix-ownership"" with sudo rather than docker image if specified (#24871)The ""fix-ownership"" command uses our breeze docker image to fixownership of files generated by breeze. This is nice from the userperspective because there is no need to authenticate with passwordand - since we already have breeze image handy - it is fast andpainless. However in CI, there are often cases where ""fix-ownership""does not really have an image available and needs to pull one, whichtakes 1 minute. However the user in CI has sudoer capability withoutpassword, so we can use it to run fix-ownership with just plainsudo/chown.We switch all the CI jobs to ""USE_SUDO"" and also avoid runningfix-ownership on other systems that Linux, as the ownership offiles creaed from docker as root is only problem when the filesystemis directly mounted and used in the container (which happens only onLinux). MacOS and Windows have ""user-space"" filesystem that aremuch slower, but then they perform user-remapping on their ownand files created in container have automatically ownership ofthe user who runs the docker container command.",2
"Automatically detect if non-lazy logging interpolation is used (#24910)We used to have pylint check that was preventing it but sincewe have no pylint, we need to do some intelligent guessingbased on AST of the python code.",1
Bump moment from 2.29.3 to 2.29.4 in /airflow/www (#24885)Bumps [moment](https://github.com/moment/moment) from 2.29.3 to 2.29.4.- [Release notes](https://github.com/moment/moment/releases)- [Changelog](https://github.com/moment/moment/blob/develop/CHANGELOG.md)- [Commits](https://github.com/moment/moment/compare/2.29.3...2.29.4)---updated-dependencies:- dependency-name: moment  dependency-type: direct:development...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Refactor and fix AWS secret manager invalid exception (#24898),0
"Revert ""Add %z for %(asctime)s to fix timezone for logs on UI (#24373)"" (#24810)This reverts commit 7de050ceeb381fb7959b65acd7008e85b430c46f.",4
Fix tag link on dag detail page (#24918),2
"Fix cartesian join re dataset deps in update_state (#24925)We use the subquery to get distinct downstream dataset references for the dag (there could bemultiple tasks that touch the same dataset), then join to DDR to get the dags pointing tothose datasets.  The subuery avoids a many-to-many join.",5
"Fix zombie task handling with multiple schedulers (#24906)Each scheduler was looking at all running tasks for zombies, leading tomultiple schedulers handling the zombies. This causes problems withretries (e.g. being marked as FAILED instead of UP_FOR_RETRY) andcallbacks (e.g. `on_failure_callback` being called multiple times).When the second scheduler tries to determine if the task is able to be retried,and it's already in UP_FOR_RETRY (the first scheduler already finished),it sees the ""next"" try_number (as it's no longer running),which then leads it to be FAILED instead.The easy fix is to simply restrict each scheduler to its own TIs, asorphaned running TIs will be adopted anyways.",1
Convert RDS Event and Snapshot Sample DAGs to System Tests (#24932),3
Correct parameter typing in `SalesforceBulkOperator` (#24927),1
Small docstring clarification in _find_zombies (#24930),2
Adding ElasticserachPythonHook - ES Hook With The Python Client (#24895),1
add more descriptive REMOTE_BASE_LOG_FOLDER (#24935),2
Chart: minor clarifications to docs (#24929),2
Fix syntax in mysql setup documentation (#24893 (#24939)In the 'set-up-databases' documentation the code block was not working on newer versions of mssql. Updated the docs so that the query works on supported 2017 and 2019 versions of mssql,1
Fixing syntax issues in breeze documentation (#24940),2
Airflow `2.3.3` has been released (#24942),5
"Add `--clean-build` option for breeze build-docs (#24951)This option removes all previously generated docs files so thatbuild docs can run using clean state. Prevents cases where localinventory has been updated from local providers rather thanfrom released ones, breaking the docs building.",2
Add tenant specification to dbt Cloud conn doc (#24907),2
Fix error in seeding elastic `log_id` template (#24960)There was a mistake in seeding the elastic log id. We used hyphens instead of underscores,1
Rewrite the Airflow documentation home page (#24795)* Rewrite the Airflow home page* Rename home to overview* Ignore parameterization* Remove history since that can be read elsewhere* Link companies to inthewild.md* Process comment,2
"Add Kubernetes-related files to trigger helm tests (#24950)So far, when Kubernetes-related files ware changed, the Helm chartwere not run - which lead to some of the tests failing after merge.This change adds kubernetes related files in airlfow to list offiles that trigger Helm tets.",2
Added databricks_conn_id as templated field (#24945),5
"Revert ""Fix error in seeding elastic `log_id` template (#24960)"" (#24966)This reverts commit c97f0b3da09079778dc9317030cfacc5df01e88c.",4
Fix pid check (#24636),0
"Dataset event table (#24908)Part of AIP-48. Adds a table to record all dataset events, which are currently used to used to trigger dag runs, but which may in the future also contain information about the nature of the update which can be read by the downstream tasks working with the datasets.",5
Extends resolve_xcom_backend function level documentation (#24965)* # This is a combination of 2 commits.# This is the 1st commit message:Extends resolve_xcom_backend documentation# This is the commit message #2:properly stylize XCom* Extends resolve_xcom_backend documentationproperly stylize XComFinal docstring updateExtends resolve_xcom_backend documentationproperly stylize XComFinal docstring update* Stylistic changes,4
Only assert stuff for mypy when type checking (#24937),3
Adding podAnnotations to Redis statefulset (#23708),1
"Limit astroid version to < 2.12 (#24982)Astroid 2.12 released 9th of July breaks documentation buildingwith sphinx-autoapi.Issue about it has been opened inhttps://github.com/PyCQA/astroid/issues/1708Until it is fixed, we should limit astroid.",0
"Add TCP_KEEPALIVE option to http provider (#24967)Enabling TCP_KEEPALIVE allows to keep the idle connections openedwhen there are firewalls in-betweeen that close such connections.TCP_KEEPALIVE sends a ""no-data"" packet regularly (and expectsACK from the server). This should not be mistaken with Pythonrequests ""keep alive"" feature - which reuses opened HTTP connectionsif you perform several requests to the same server (both are namedkeep alive but they both mean two completely different things andare implemented at differen layers of the OSI network stack. TheRequests Keep Alive is Layer 7 (application) and TCP Keep Aliveis at Layer 4 (transport).The ""Requests"" keep alive is enabled by default in the requestslibrary while the TCP Keep Alive requires a TCPKeepAliveAdapterto be used (from requests_toolbelt library).Fixes: #21365",0
"Update set-up-database.rst (#24983)Add notice about MySQL's `NO_ZERO_DATE` mode, which could cause database operation error in some cases.",0
"Patch getfqdn with more resilient version (#24981)We keep on having repeated issue reports about non-matchinghostname of workers. This seems to be trceable to getfqdn methodof socket in Kubernetes that in some circumstances (race conditionwith netwrking setup when starting) can return different hostnameat different times.There seems to be a related issue in Python that has not beenresolved in more than 13 years (!)https://github.com/python/cpython/issues/49254The error seems to be related to the way how canonicalname isderived by getfqdn (it uses gethostbyaddr which sometimesprovides different name than canonical name (it returns thefirst DNS name resolved that contains ""."").We are fixing it in two ways:* instead of using gethostbyaddr we are using getadddrinfo with  AI_CANONNAME flag which (according to the docs):  https://man7.org/linux/man-pages/man3/getaddrinfo.3.html    If hints.ai_flags includes the AI_CANONNAME flag, then the    ai_canonname field of the first of the addrinfo structures in the    returned list is set to point to the official name of the host.* we are caching the name returned by first time retrieval per  interpreter. This way at least inside the same interpreter, the  name of the host should not change.",4
Move mapped kwargs introspection to separate type (#24971),4
"Add handling state of existing Dataproc batch (#24924)This change avoids Airflow marking tasks as 'Success' even if theexisting Batch is in a 'Failed' state. We check the various states,and ensure that the Airflow task state reflects the actual state ofthe Dataproc Batch.Co-authored-by: Daniel van der Ende <daniel.van.der.ende@mollie.com>",5
"Sort operator extra links (#24992)Today, every time webserver is restarted, the order of theoperator extra links are randomized due to Python sets beingunordered.This change will sort the links according to their name.No particular thought has been given to customizing this sortorder, except to make it consistent so that extra links alwaysappear at the same place for the users.",1
"Migrate Google firestore example to new design AIP-47 (#24830)related: #22447, #22430",1
"Migrate Google example gcs_to_gdrive to new design AIP-47 (#24949)related: #22447, #22430",1
Modify BigQueryCreateExternalTableOperator to use updated hook function (#24363)* Fixed BigQueryCreateExternalTableOperator and its unit test (#24160),3
D400 first line should end with period two files batch 01 (#24955),2
"Migrate Google ads example to new design AIP-47 (#24941)related: #22447, #22430",1
"Migrate Google sheets example to new design AIP-47 (#24975)related: #22447, #22430",1
"Add absolute paths for typescript files (#24978)* Add absolute paths for typescript filesUpdate jest, webpack and ts config files to allow for two aliased paths:`app` = `static/js``grid` = `static/js/grid`* remove extraneous path var",4
PubSub assets & system tests migration (AIP-47) (#24867),3
Update 2.3.3 date in release notes (#25004),5
"doc: reload pods when using the same DAG tag (#24576)Using the same tag for airflow dags cannot work as kubernetes will not refresh pods (scheduler, workers, webservers) if they are not updated.Then you need to trigger the reloading of those pods manually.",5
Implement Azure service bus subscription Operators (#24625)Implement Azure service bus subscription Operators- Added AzureServiceBusSubscriptionCreateOperator- Added AzureServiceBusSubscriptionDeleteOperator-example DAG- Added hooks for creating subscription and delete subscription- Added unit Test case,3
More typing in SchedulerJob and TaskInstance (#24912)* More typing in SchedulerJob and TaskInstance,2
"chore: fix typo (#25010)fix typo in bullet list under Completing our DAG from ""a"" to ""at""",2
Add read-only REST API endpoint for Datasets (#24696)Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>Co-authored-by: Jed Cunningham <jedcunningham@apache.org>,5
[DOC] update README 2.3.3 (#25016),5
"Old ""Core"" SQL database config used in breeze for old airflow versions (#25009)Airflow versions from before Airlfow 2.3 used ""CORE"" sectionfor SQL configuration and they need to add variablefrom the DATABASE one when Airflow version 2.2 and before isused in `--use-airflow-version` switch.",1
Only refresh active dags on dags page (#24770)* Only refresh active dags on home page* Make feedback updates* Replace function parameters with object* Fix eslint errors* Update airflow/www/static/js/dags.jsCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,2
Add %z for %(asctime)s to fix timezone for logs on UI (#24811),2
Less verbose logging in ssh operator (#24915)* Less verbose logging in ssh operator,1
Add `test_connection` method to AzureCosmosDBHook (#25018)* Add test_connection method to AzureCosmosDBHook,5
"Refactor js file structure (#25003)* Add absolute paths for typescript filesUpdate jest, webpack and ts config files to allow for two aliased paths:`app` = `static/js``grid` = `static/js/grid`* remove extraneous path var* refactor js file structureget ready for graph view in grid and other react pages like datasets and dag dependencies* move instancetooltip and flatten details folder* DRY babel config* remove extraneous eslint disable* unDRY babel* fix children props",0
"Show 'Dataset' as schedule interval for dataset-triggered dags (#25013)In the DAGs page, in place of schedule interval, show `Dataset` to indicate there is no ""schedule interval"" per se but this is what will determine the next run.  Update tooltip also (derived from dag_model.timetable_description). And, in the dag detail page, suppress the next run text entirely because it's not possible to say.",1
Add documentation for July 2022 Provider's release (#25030),1
Add project_id as a templated variable in two BQ operators (#24768),1
Chart: Default to Airflow 2.3.3 (#24947),2
Remove old snowflake system tests (#25026),3
Implement expand_kwargs() (#24989),5
"Stop failing image refreshing on ""warm-up"" failure (#25046)The ""warm-up"" buld for parallel images might fail when you runit locally, but this should be ignored, because it is really onlyneeded at the CI when we want to make sure that several parallelbuilds do not try to create the same buildx container.It's safe to ignore any failure at the warm-up stage.",0
Implement Azure Service Bus (Update and Receive) Subscription Operator (#25029)Implement Azure Service Bus (Update and Receive) Subscription Operator,1
"Add Datasets view (#25038)* dataset ui init* List datasets from API* remove security change* fix datasets_api meta, and linting* remove dag.html datasets api meta",5
"airflow/www/package.json: Add name, version fields. (#25065)These fields are required according to:https://docs.npmjs.com/creating-a-package-json-file#required-name-and-version-fields",5
Add `source_` prefix to DatasetEvent columns (#25068)This helps clarify that the TI key was the source of the event.,5
Explicitly list @dag arguments (#25044),2
Unify summary/description logic to timetable (#25045),2
Cleanup Dataset API variable name and typing (#25073),5
Skip mapping against mapped ti if it returns None (#25047),2
SageMaker system tests - Part 1 of 3 - Prep Work (AIP-47) (#25078)* Sagemaker Operator - Improve type hints and docstrings* SageMaker Operator Unit Test Improvements- Add explicit unit testing for integer_fields- Improve type hinting- Standardize some variable naming* Sagemaker Operators - Configure integer fields at runtime,1
"Update `check_files.py` to support many provider releases (#25077)This allows you to put many provider releases in `packages.txt` andcheck them all in one go. In fact, you can copy/paste the list of pypiurls from the vote email!",5
Added Clarity AI to INTHEWILD.md (#25091),1
Upgrade grid Table component to ts. (#25074),2
Upgrade ramaining context file to typescript. (#25096)* Upgrade timezone to typescript.* Update props type.,5
Add dataset events to dataset api (#25039),5
Fix invalidateQueries call (#25097),5
Upgrade utils files to typescript (#25089)* Migrate utils to typescript.* Use PropsWithChildren for typing props.,1
Migrate lambda sample dag to system test (AIP-47) (#24355)* Migrate lambda sample dag to system test* Use SystemTestContextBuilder to pass along data,5
Set grant type of the Tabular hook (#25099),1
"Migrate Google analytics example to new design AIP-47 (#25006)related: #22447, #22430",1
"Migrate Google campaign manager example to new design AIP-47 (#25069)related: #22447, #22430",1
Added logging to get better diagnosis of flaky backfill test (#25106),3
Sagemaker System Tests - Part 2 of 3 - example_sagemaker.py (#25079),1
Fix xcom_sidecar stuck problem (#24993),0
The 'depdendencies.json' file is removed (#25108)The dependencies.json was replaced by provider_dependencies.json ingenerated folder in #24836 but it missed deletion of theoriginal file. This PR removes the old file.,2
Migrate datasync sample dag to system tests (AIP-47) (#24354)* Migrate datasync sample dag to system tests (AIP-47)* Use SystemTestContextBuilder to pass along data,5
Add documentation for follow-up release for june providers (#25111)Co-authored-by: Vincent <97131062+vincbeck@users.noreply.github.com>,1
Update tutorial docs to include a definition of operators (#25012),1
Fix markdown error in Airflow Readme (#25120)The new markdown-lint is a bit more picky about headeers in markdown,1
"Avoid running build ARM image twice (#24858)In case of ""merge-run"" (i.e. when maintainer merges somethinto main or v2-*-test branch is pushed, the ARM instance does notneed to be built because in such case cache is built and pushedfor ARM image.",1
"Bump typing-extensions and mypy for ParamSpec (#25088)* Bump typing-extensions and mypy for ParamSpecI want to use them in some @task signature improvements. Mypy added thisin 0.950, but let's just bump to latest since why not.Changelog of typing-extensions is spotty before 4.0, but ParamSpec wasintroduced some time before that (likely some time in 2021), and itseems to be a reasonble minimum to bump to.For more about ParamSpec, read PEP 612: https://peps.python.org/pep-0612/",2
"Databricks: fix test_connection implementation (#25114)Original implementation has used the API that exists only on AWS, so it will be failing onAzure & GCP Databricks.  This PR uses another API that is available on all Databrickscloud platforms.",5
"Add .../dagRuns/DR-ID/upstreamDatasetEvents endpoint (#25080)Tells you, for a given dag run, the dataset events that are ""part of"" the dag run.  I.e. they were part of the collection of dataset events that contributed to the triggering of the dag run.  In practice we just query the events that occurred since the prev dag run.  We may make the relationship tighter, see https://github.com/apache/airflow/pull/24969.",1
"Migrate Google example gcs_to_sftp to new design AIP-47 (#25107)related: #22447, #22430",1
Convert the batch sample dag to system tests (AIP-47) (#24448),3
Added exception catching to send default email if template file raises any exception (#24943),2
"Makes changes to SqlToS3Operator method _fix_int_dtypes (#25083)Convert dataframe object columns to str, to avoid errors when converting from df to parquet.Renamed methods to remove old name:_fix_int_dtypes -> _fix_dtypestest_fix_int_dtypes -> test_fix_dtypesCo-authored-by: Paul Stanton <paul.stanton@selectquote.com>",3
Convert Glue Sample DAG to System Test (#25136),3
AIP-47 - Migrate Slack DAG to new design (#25137),1
Don't queue dagruns or create dataset events on skipped task instances (#25086),5
"Glue Job Driver logging (#25142)Adds an optional `verbose` boolean to Glue job operators and sensors which defaults to False.  If set true, then Glue job logs will be passed through to the Airflow task logs.",2
AIP-47 - Migrate redshift DAGs to new design #22438 (#24239),1
Adding Authentication to webhdfs sensor  (#25110),1
"Fix PR label detection in CI (#25148)Label detection for incoming PR had few bugs:* wrong name of output was used for Build Info* assumption in selective checks was that PR labels are  space separated, but they were really array formatted.* there was a $ typo in build-images.yamlThis PR fixes all that:* output name and typo is corrected* we use ast.literal_eval now to parse the PR labels.",1
Standardize AwsLambda (#25100)* Standardize AwsLambdaThe `aws_lambda.py` is not the standard file path. The `aws_` prefix is not needed.For the file name I used the same path as the hook.* fix deprecated_classes.py* update verify_providers.py,1
"Migrate Google example automl_vision to new design AIP-47 (#25152)related: #22447, #22430",1
"Speed up Kubernetes tests ~30% on main. (#25143)We should have enough resources - we were limiting them to 2 parallelruns (we assumed 4 CPUs are needed for full K8S tests). Howevermonitoring shows that ~20/30% memory and up to 4 CPUs are usedwhen 2 tests are running in parallel (on a big instance) but the testshave also some peaks that might cause more than 3 parallel tests to fail.We currently run 5 tests so increasing the limit to 3 for self-hostedrunners should actually speed it up quite a bit (generally speaking wehad to run 2 tests in parallel twice and 5th test was run as standalone.By increasing the limit to 3 we switch to 3x, 2x mode - which not onlywill safe 30% of the time, but also has a free ""slot"" for one moreK8S version.",1
"Disable provider packages building for non-main branch builds (#25056)* Disable provider packages building for non-main branch buildsWhen running the builds from non-main, we do not want to useprovider packages built locally, but we want to install those fromPyPI. This is achieved by skipping the step of building providersin case default branch is not main.During this change it was also discovered that we do not needAIRFLOW_FROM_CONTEXT flag to indicate whether airflow isamong those being installed from docker-context files. This isnow detected automatically and airflow is installed from Pypiif not installed from docker context files. Providersand airflow are automatically detected and installed if they arepresent.Installing pre-cached dependencies from GitHub is skipped incase docker-context-files are used.",1
"Retrieve airflow branch/constraints from env variables (#25053)We are using the ""main"" breeze to build even v2-3 images andit has to retrieve the branch and constraints not from thePython constants but from environment variables that areset by build-image.yaml. Otherwise ""main"" is used to push/pullimage and constraints.This pr changes the retrieval in build image to retrievebranch, constraints branch (and debian version) from envvariables if they are set.",1
"Migrate Google example compute_igm to new design AIP-47 (#25132)related: #22447, #22430",1
"Fix put-secret-value example (#25140)To set the secret string so it is considered a list of key/value pairs we need to send a JSON object with keys and values, not an array with and object per key/value. The command as it is this will throw an error on zsh as the JSON is not escaped. After escaping the json it will update the secret as plain text and the value will simply be a JSON string. This change remedies both of these issues.",0
"Speed up Kubernetes upgrade tests at least 2x (#25159)We do not need to run upgrade kubernetes tests for all pythonversions and all kubernetes versions. It is enough to run themfor the minimum and maximum versions. We already run all k8s testsin main for all executors, so they ""generally"" work, and itis very, very unlikely that if it works for Python 3.7, and 3.10,it will not work for 3.8 or 3.9 (similarly for K8S version).This will speed up the build at least 2 times.",1
"Update description of branch creation to include the new Breeze (#25155)* Update description of branch creation to include the new BreezeThe new Breeze has a different place where default branch andconstraints branch are kept.When we create a new ""release"" branch we should update it hereas well as in the bash legacy breeze (until the bash breeze hasstil some zombies around)",5
"Generate typescript types from rest API docs (#25123)* generate api types from swagger docs, add map_index to task instance api schema",1
No grid auto-refresh for backfill dag runs (#25042)* Update useGridData.ts* Update useGridData.test.js* Update useGridData.test.js,5
"Update index.rst (#25184)This release is 3.0.1, not 3.1.0 --right?",5
"Prefix the dags in dataset example dags with example_dataset_ (#25181)Having dag1, dag2 etc was not telling much about the different dataset dags",2
"Move javascript compilation to host (#25169)Previously, in order to keep consistent development environmentwe've compiled javascript in the CI image. However we can utilisepower of pre-commmit for setting the node environment for allcontributors automatically. Instead of compiling the javascriptin the image, we can compile it via pre-commit in the host.This can be done thanks to the new python breeze which is far moreflexible and can now add execution of compilation of the javascript whenneeded and using pre-commit environments.Thanks to that, we can vastly simplify the Dockerfiles and scripts thatare used to automatically build or signal that the assets needrecompilation. We can basically assume that the assets were preparedoutside of the image building (and breeze makes sure it happens)The changes:* node.js is not needed in images (neither PROD build nor CI)* no need for multiple asset compilation scripts. All is done  via pre-commit environment with `breeze compile-www-assets``  command* lint checks for UI do not need the docker image any more  (they are also based on pre-commit environment)* no more checks/warnings when you enter the image* start-airflow command builds the compilation before entering* prepare-airflow-package runs asset compilation before entering  docker airflow building.",2
Add revisionHistoryLimit to all deployments (#25059),1
Fix asset compilation via setup.py (#25201)Asset compilation via setup.py has been broken in #25169.This PR fixes it.,0
Add ts types generation to static checks (#25167),1
Fix missing space for breeze build-image hint command (#25204),0
Add Tessian to list of companies using Airflow (#25206),1
make MsSQL tests runnable on Python 3.8 (#25214)mysql lib does support newer version of python https://github.com/pymssql/pymssql/pull/659,1
"Migrate Google example bigquery_to_mssql to new design AIP-47 (#25174)related: #22447, #22430",1
call updateNodeLabels after expandGroup (#25217),5
make MsSQL tests runnable on Python 3.8 (2nd) (#25216)I missed the hive provider from the commit in https://github.com/apache/airflow/pull/25214,1
Also compile assets in non-main (#25220)Assets compilation when CI and PROD images are prepared shouldalso be run in non-main branch.,1
SQSPublishOperator should allow sending messages to a FIFO Queue (#25171),1
Show dataset readiness for the next run (#25141),1
Include missing mention of `external_executor_id` in `sql_engine_collation_for_ids` docs (#25197),2
Dataset details view (#25208)* dataset details view* Fix sorting and naming* update table tests* update comments,5
Convert RDS Export Sample DAG to System Test (AIP-47) (#25205)* Convert RDS Export Sample DAG to System Test* PR Fixes,0
Stop SLA callbacks gazumping other callbacks and DOS'ing the DagProcessorManager queue (#25147),2
"Upgrade to Pip 22.2 (#25218)The 22.2 version of `pip` has **just** been released. Looks likeit does not break neither constraints nor ""eager upgrade"" buildswhen we build images locally so it's safe to update to it in main.",5
Improve ElasticsearchTaskHandler (#21942)* Improve ElasticsearchTaskHandler:- use builtin logging.makeLogRecord instead of strange _ESJsonLogFmt- do not re-sort already sorted logs- apply ISO 8601 datetime format- fixed several found bugs,0
"Fix order of returned rows in a flaky test_outlets_dataset test (#25231)In Postgres especially (but in generaly in all databases, ifthere is no order specified in select query, the rows mightcome in random order. It depends on many factors.The test query here retrieved the dags without any order butexpected the list to be in specific order.This PR adds ordering - it also removes side-effects of thetest by using fixture that clears the datasets before and afterthe tests that rely/modify datasets - because othrwise failure ofone of the tests can create side effects that fail the othertests (this is what happened in this case)",3
Sagemaker System Tests - Part 3 of 3 - example_sagemaker_endpoint.py (AIP-47) (#25134)* Sagemaker System Tests - Part 3 of 3 - example_sagemaker_endpoint.py* PR Fixes* Fix failing static checks - unused import,2
"Common SQLCheckOperators Various Functionality Update (#25164)* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Move alias to proper query build statementThe table alias should be in the self.sql query build statementas that is where the table it needs to alias is defined.* Add batching to SQL Check OperatorsCommit adds a WHERE clause to the sql statement that allows forarbitrary batching in a given table.* Fix bug with multiple table checksWhen multiple table checks are given to the SQLTableCheckOperatorand at least one is not a fully aggregate statement, a GROUP BYclause was previously needed. This commit updates the operator touse the get_pandas_df() method instead of _get_first() to return apandas dataframe object that contains the check names and checkresults from the new style of query. The new style of query usesUNION ALL to run each test as its own SELECT statement, bypassingthe need to do a GROUP BY.* Update test failure logicChanged name of method from _get_failed_tests to _get_failed_checksto better match naming, and updated logic of the method to includean optional column param. The query in the column check operatoris removed from the failed test exception message, as it was onlyever showing the last query, instead of the relevant one(s). This isreplaced by the column, which will be more useful in debugging.* Add table alias to SQLTableCheckOperator queryWithout a table alias, the query does not run on Postgres andother databases. The alias is arbitrary and used only forproper query execution.* Fix formatting error in operator* Bug fixes and updates to test and operatorFixed bug in test where the dataframe column names did not matchthe operator's expected dataframe column names. Added more infoto the SQLColumnCheckOperator's batch arg. Fixed the location oftable aliasing in SQLTableCheckOperator.* Remove merge conflict lines* Rename parameter batch to partition_clauseGives a clearer name to the parameter and adds templating tothe SQLTableCheckOperator.* Fix typo in docstring* Reformat operator file",2
Dataset-triggered dag runs should have the right DagRunType (#25237),2
Add Databricks provider to boring cyborg (#25238),1
Unify DbApiHook.run() method with the methods which override it (#23971),5
"Add datasets to dag dependencies view (#25175)Also, remove config option `scheduler > dependency_detector`.  See discussion in mailing list vote: https://lists.apache.org/thread/sd84gvxghocnjjgxz4p69qrb1t6k08z5",5
Make tag fetching when preparing providers optional. (#25236)* Make tag fetching when preparing providers optional.Tag fetching is only needed to make sure we do not generatepackages that have already been generated. However this is justan optimisation for CI runs. There is no harm if the tags arenot refreshed and we generate the package again locally.Tag fetching might faile in various cases - for example whenyou are in corporate environment and require specific certificatesto be available or when you are working from a worktree.After this change fetching tag will produce warning when there isan error and instruction on how you can fetch tags manually.* Update dev/provider_packages/prepare_provider_packages.pyCo-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>,1
"Protect against the case when emulated Python is used on M1s (#25229)People might have an intel Python installed on M1s. This happenedalready. The result of it that they (unknowingly) suffer from10x slower speed of any Python code they use (even locally).This can happen in two cases:* you can run in ""arm"" terminal and your python might be  Intel based (for example when your environment is old or  managed before it was ready for ARM architecture) or when  you use some scientific package managers like anaconda that  still have not switched to ARM. In this case we  detect different architecture reported by Python and system.* you can run it in emulated terminal when your IDE has been  installed using different architecture. In this case, we detect  if rosetta emulation is running via sysctl command.This also impact Breeze because we are using Python architecturein order to determine which platform image should be used byBreeze.This change adds big, fat warning and 20 seconds of delay askingthe user if they REALLY want to run Breeze command using emulatedarchitecture. If they answer y - it will continue. If they do not answeror answer anything else, the command will error out.",0
convert TimeSensorAsync target_time to utc on call time (#25221),1
Fix BatchOperator links on wait_for_completion = True (#25228),2
Add test_connection to Azure Batch hook (#25235)* Add test_connection to Azure Batch hook* Apply review suggestions,1
Add override method to TaskGroupDecorator (#25160),1
fix comments (#25245),0
Implement map() semantic (#25085),5
Add werkzeug limitation until flask-login will handle it (#25270)* Add werkzeug limitation until flask-login will handle it* comments for afterwards,0
Remove support for FAB `APP_THEME` (#15277),1
add europcar to list of companies using Airflow (#25279)Co-authored-by: aninda bhattacharjee <aninda.bhattacharjee@europcar.com>,1
Disallow any dag tags longer than 100 char (#25196),2
Fix ExternalTaskSensor not working with dynamic task (#25215)* Make operator_extra_links a class variable* Make operator_extra_links a class variable of ExternalTaskMarker,2
"Avoid unnecessary error output when checking for emulated environment (#25289)The #25229 introduced check for emulated environment, butit introduced a warning being printed on Linux environment.This avoids printing the warning and also it stops running the checkoutside of MacOS (Darwin)",1
Migrate files to ts (#25267),2
"Add URI to dataset event response (#25250)Also, update the `extra` fields to return as JSON in the REST API.",5
Use tables in grid details panes (#25258),1
Rename `created_at` to `timestamp` in DatasetEvent (#25292)Timestamp seems more appropriate,5
"Add dev version of asset compilation (#25272)When we moved www asset compilation from container to the host in #25169,we removed yarn from the image and the old instructions are no longervalid. Instead of in-container yarn dev we expect nowto have yarn dev run in the host. It can be done either withpre-commit (if you have no node/yarn installed in your host, pre-commitwill automatically install both node and yarn in the right versions,or if you have yarn installed you can run it manually.The 'start-airflow' instructions now remove the in-containerinstructions and explain those two options you have.",4
[docs] Update DAG run to clarify when a DAG actually runs (#25290),1
Improve taskflow type hints with ParamSpec (#25173),2
add common-sql label to boring-cyborg.yml (#25298)labeling PR based on label https://github.com/apache/airflow/labels/provider%3ACommon-sql,1
"Delete redundant system test bigquery_to_bigquery (#25261)related: #22447, #22430",3
Improved telemetry for Databricks provider (#25115)* Improved telemetry for Databricks provider,1
"Migrate Google example natural_language to new design AIP-47 (#25262)related: #22447, #22430",1
Adjust heuristics for PR of the Month script (#25239),5
"Migrate Google example life_sciences to new design AIP-47 (#25264)related: #22447, #22430",1
Resolve and validate AWS Connection parameters in wrapper (#25256),2
Adds new warning to provider verification (#25310)There is a new warning generated when you import snowflake. This issomething we cannot do much about (and it is rather harmlessbecause it only impacts pyarrow functionality of snowflakeconnector).Adding it will silence provider check in main builds.,1
"Fix too long names of files and docker compose projects (#25301)When there is a change spanning multiple providers, the test typemight be very long ""Providers[google,microsoft.mssql......]"".Test type is used in generating directory/file names as well asin determining docker compose project names and such long name mightbe just ... too long. But at any point in time we only run oneProvider* test type, so we can simply truncate the ""[*]"" when weuse TEST_TYPE to determine dir and docker-compose name.This PR does exactly this.",2
Introduce sla_miss metric (#23402),5
"Fix Flask Login user setting for Flask 2.2 and Flask-Login 0.6.2 (#25318)The Google openid auth backend of ours had hard-coded way ofseting the current user which was not compatible with Flask 2.2With Flask-login 0.6.2 the user is stored in g module of flask, wherebefore, it was stored in _request_ctx_stack. Unforatunately thegoogle_openid rather than using _update_request_context_with_userset the user directly in context. In Flask-login 0.6.2 this stoppedworking.This change switches to use the _update_request_context_with_usermethod rather than directly storing user in context which works beforeand after the Flask-Login 0.6.2 change.",4
More improvements in the Databricks operators (#25260),1
Use simple Json in Dataset & DatasetEvent extra field (#25321)Replaces ExtendedJson with sqlalchemy_jsonfield type,5
Add __repr__ to ParamsDict class (#25305)Fixes #25295,0
Filter XCOM by key when calculating map lengths (#24530)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,5
Refactor test DAGs to unclutter example DAGs (#25327),2
Fix MsSQL failing on long list of providers. (#25338)The #25301 fixed too long names for files in case of long listof providers but there is one more place it needs to be fixed -in the volume name of MsSQL tests.,3
Allow to add extra flags passed to ``pip install`` when building images (#25337)This allows a bit more flexibility when building imagesas you can pass arbitrary command line switch to ``pip install``(for example --pre).,4
"Deprecate hql parameters and synchronize DBApiHook method APIs (#25299)* Deprecate hql parameters and synchronize DBApiHook method APIsVarious providers deriving from DbApi had some variations in somemethods that were derived from the common DbApi Hook. Mostly theywere about extra parameters added and hql parameter used instead ofsql. This prevents from really ""common"" approach in DbApiHook assome common sql operators rely on signatures being the same.This introduced breaking changes in a few providers - but thosebreaking changes are easy to fix and most have already beendeprecated.",0
Upgrade to latest `pip` version 22.2.1 released today (#25348),3
"Remove Werkzeug limitation after flask-login was fixed (#25291)The Werkzeug limitation removed in #25270 can be removed now, whenFlask-login is fixed and 0.6.2 version is released that supports it.",1
"Add possibility to specify command to run at Breeze entry via env var (#25288)Breeze environment can be easily customized by specifying your owninit.sh file in files/airflow-breeze-config/init.sh. However, thereare certain scenarios (for example GitHub Codespaces), where you cannotdo that, but you can easily specify an environment variable to setwhen you enter Breeze container - and you need to customize yourenvironment there, becasue you need to make your airflow codere-linked from Github default ""/workspace/airflow"" to ""/opt/airflow"").In such case it is much more convenient to provide the command to runvia environment variable and eval the command at entering the image.This PR implements such a possibility.",1
"Allow Legacy SqlSensor to use the common.sql providers (#25293)The legacy Airflow SqlSensor, rejects working wiht comon.sql providerseve if they are perfectly fine to use.While users could switch to the common.sql sensor (and itshould work fine). we should not force the users to switch to it.We are implementing ""fake"" class hierarchy in case the provideris installed on Airflow 2.3 and below.In case of Airflow 2.4+ importing the old DbApiHook will fail,because it will cause a circular import - in such case ournew DbApiHook will derive (as it was originally planned) fromBaseHook.But In case of Airflow 2.3 and below such import will succeedand we are using the original DbApiHook from airflow.hooks.dbapias base class - this way any ""common.sql"" hook on Airflow 2.3and below will also derive from the airlfow.hooks.dbapi.DbApiHook- thus it will be possible to use it by the original SqlSensor.Fixes: #25274",0
Remove useless logging line (#25347),2
List upstream dataset events (#25300)* add upstream dataset events to run details* show only task id link* fix timestamps and extra field* fix relative import path* improve dataset events table,5
Add dataset-triggered run type icon (#25244)* add dataset triggered run type* add comment* fix linting,0
Add `operator` and `has_outlet_datasets` to `/grid_data` (#25323)* Add `operator` and `has_outlet_datasets` to `/grid_data`* Fix static checks,0
Remove getTasks from Grid view (#25359)* Add `operator` and `has_outlet_datasets` to `/grid_data`* Remove getTasks from grid view* use getTask() util fn* add tests* fix rebaseCo-authored-by: Jed Cunningham <jedcunningham@apache.org>,0
Update operators.rst (#25358),1
Add `test_connection` method to AzureContainerInstanceHook (#25362),1
test(hooks/exasol): add test for no resultSet rtn type (#25277),1
Fix Vertex AI Custom Job training issue (#25367),0
Remove extraneous word in installation guide (#25371)Removes a single stray word from the installation instructions,4
updated documentation for databricks operator (#24599),1
"Remove unnecessary asset compilaton for prod images (#25374)When prod image is built, we install airflow from packages andasset compilation happens as part of the package preparation.There is no need whatsoever to repeat it here for PROD images(it is still needed for CI images though).",1
add downstream events to task instances (#25375),1
"Restore pushing CI image as latest to GHCR.io (#25380)We stopped pushing latest CI/PROD images to ghcr.io when we started torun multiplatform builds and switched to --cache-from option ofbuildx. Hoever there are still some workflows that might requirethe latest image (for example Codespaces) and generally speakingsomeone could just pull the image if they are curious.This PR adds back pushing the image (only ""linux/amd""during image cache preparation.",1
fix - resolve bash by absolute path (#25331)Co-authored-by: Matt Rixman <MatrixManAtYrService@users.noreply.github.com>,1
Convert ECS Fargate Sample DAG to System Test (#25316),3
fix: change disable_verify_ssl behaviour (#25023)The problem is that verify_ssl is overwritten by theconfiguration from the kube_config or load_incluster_config file.,2
Translate system tests migration (AIP-47) (#25340),3
Use newer kubernetes authentication method in internal vault client (#25351)This gets rid of the hvac deprecation warning about `auth_kubernetes`being removed in version 1.0 that is currently printed when using thisauthentication method.,1
Memorystore assets & system tests migration (AIP-47) (#25361),3
Check expand_kwargs() input type before unmapping (#25355),5
Change stdout and stderr access mode to append in commands (#25253)Co-authored-by: Iuhos Zoltán <iuhosz@ukatemi.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,4
"Add missing import in best-practices code example (#25391)* Add missing import in best-practices code exampleThis PR adds a missing import to the ""unit test for a custom operator"" code example. While this code example won't run on its own any way since `MyCustomOperator` isn't defined (and probably shouldn't be for simplicity), I noticed when applying this code to my own custom operator that an import for `DAG` was missing. This PR adds that back in, so the only missing import is for the user-added custom operator.",1
YandexCloud provider: Support new Yandex SDK features for DataProc (#25158),5
Fix datasets list page (#25382)* add link to dag dependencies from datasets* fix dataset selection,5
"Adjust limits when constructing cross-provider dependencies (#25364)When we are constructing cross-provider dependencies, we should adjustthe limits to account for some apache-airflow packages that are not yetreleased.The adjustment is to add "".*"" after the version number whendependency is lower-bound with `>=`. It's not explicitly mentioned inPEP 440 - it is only mentioned there that it works forequality, but since `>=` is also part of `equal` it alsoworks there.Example is a common-sql package that might be limited to `>=1.1.0`in google provider as part of the change, but it might not yet be releasedas it is being released together with the package it is needed by.We  need to adjust the version whenever in our providers werefer to other providers with >= install clause because  `--pre`flag in `pip` only allows to install direct pre-release (anddevelopment) dependencies but it does not modify requirements ofthose package to also include pre-releases as transitive dependency.Also, we need to remove the limits in `devel` dependencies becausethe packages are not yet released at the moment we use thosedependencies, so the limits in `devel` should be removed, allowingthe developers to install Airflow without those devel packages.Also a bug was found that would prevent to generatethe dependencies in case provider.yaml file only changed (badspecification of .pre-commit include)",4
"Move all ""old"" SQL operators to common.sql providers (#25350)Previously, in #24836 we moved Hooks and added some new operators to thecommon.sql package. Now we are salso moving the operatorsand sensors to common.sql.",1
"Add missing option when pushing latest images to cache :( (#25399)The #25380 re-introduced pushing image as latest on main successand (as unfortunately happens) since this is only testable aftermerge, a small bug crippled in making main build fail.This flag should fix the problem.",0
Add upstream and downstream to datasets api responses (#25390)This also renames the existing relationships on datasets to make thedirection explicit.,1
"Refactor DAG pages to be consistent (#25402)There were a few DAG pages that were inconsistent:- Missing error handling for non-existing dag_ids- Not passing the orm model for the templates use (most notably missing  the ""next run"" info in the header)",5
"Reword dag run ""upstream events"" wording (#25405)The language ""updates to"" doesn't make sense in this context.  Simplified a bit further, too.",1
Add `uri_pattern` query param to Get `/datasets` endpoint (#25411),5
Update INTHEWILD.md (#25417),5
Implement XComArg.zip(*xcom_args) (#25176),5
Create new databases from the ORM (#24156)This PR opens up to creating new databases from the ORM instead of going through the migration files.`airflow db init` creates the new db.Co-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,5
"Fix `airflow db reset` when dangling tables exist (#25441)If one of the ""dangling"" tables already existed in the DB, performing an`airflow db reset` would delete the tables, but it would then try and_re-create_ the table later. This was because the Table object was stillassociated with the Metadata object.The fix is to remove the it from Metadata once we have dropped it.",4
Making sure serialization tests runs on all example dags (#25447)while checking the failure of `TestStringifiedDAGs::test_serialization` in https://github.com/apache/airflow/runs/7608813338?check_suite_focus=true  (PR https://github.com/apache/airflow/pull/25280 )I noticed that we have:https://github.com/apache/airflow/blob/7d95bd9f416c9319f6b5c00058b0a1e3bd5bf805/tests/serialization/test_dag_serialization.py#L255-L256This test suppose to collect all example dags including providers but now in AIP-47 we move the example dags away from this path so the more we move the less coverage this test has.To solve this I added also the new paths,1
"Fix DAG audit_log route (#25415)This moves the audit_log route under /dags, like it's peers.",2
"Add option to mask sensitive data in UI configuration page (#25346)* Add option to mask sensitive data in UI configuration pageThis PR adds an option to mask sensitive data in the UI configuration page, making it possible to view the page with redated data",5
Bump terser from 5.10.0 to 5.14.2 in /airflow/www (#25172)Bumps [terser](https://github.com/terser/terser) from 5.10.0 to 5.14.2.- [Release notes](https://github.com/terser/terser/releases)- [Changelog](https://github.com/terser/terser/blob/master/CHANGELOG.md)- [Commits](https://github.com/terser/terser/commits)---updated-dependencies:- dependency-name: terser  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
"Enable ""upgrade_to_newer_dependencies"" when only provider.yaml change (#25464)When provider.yaml change and pre-commig has not be run to regenerategenerated/dependencies.json, it might happen that building CI imagewill fail due to conflicting dependencies so developer has nochance to find out that pre-commit needs to be also run dependenciesregenerated.This PR changes selective checks to also trigger the dependenciesupgrade when only provider.yaml changed.",4
Removed interfering force of index. (#25404),1
Add logging information in _fetch_from_ssm (#25401),5
"Move breeze commands to sub-commands (#25449)* mOve breeze commands to sub-commandsOriginally Python version of Breeze had only a handful of commands,while moving from Bash, but we are close to completion of thetransition and the number of commands grew quite a lot bringit iton par to the commands we used to have in the Bash version of Breeze.However, only a small subset of the commands is actually useful toaverage developer, most of the more advanced commands are used inspecific circumstances (release management, configuring Breezeonce or rebuilding the image or even runing non-interactive testsession are rarely used. Therefore it makes much more sense tosurface the common commands as the top-level commands and movethe less frequent commands to subcommands to move them out fromthe main help page.At the same time, the BREEZE.rst documentation got a little messyduring the move and this is the right time to structure it similarlyto breeze commands:* prerequisites and installation* first-time configuration* regular task description* more advanced tasks grupped in the same subcommands as in Breeze* diving deeper into details of Breeze implentation for those  who wish to understand how Breeze works under-the-hoodAliases for the common commands that users could already get used towere created, and deprecation warnings are printed in casee thosecommands are used (guiding the user to the new commands to use).Less frequently used options for shell command are still availablein `shell` but they have been removed from the default command toremove clutter. You can still used them by explicitly specifyingthe `shell` commnd.Configuration for rich click has been separated out from thecommand group implementation to separate packages. This allowsfor less problems with circular imports - none of the commandsare needed when rich-click configuration is being prepared, whichhappens before main `click` command is parsed, which allows torun imports of the ""code"" as late as possible.* Update manage-dags-files.rst",2
Update core example DAGs to use `@task.branch` decorator (#25242)* Update core example DAGs to use `@task.branch` decorator* fixup! Update core example DAGs to use `@task.branch` decoratorfixup! fixup! Update core example DAGs to use `@task.branch` decorator* fixup! fixup! Update core example DAGs to use `@task.branch` decorator* Update `override` return type anno + use DAG context manager* Update dedent for dataset docs* Use `__future__.annotations` where applicable* fixup! Use `__future__.annotations` where applicable,1
"Show upstream/downstreams from a dataset (#25403)* add links to upstream/downstream dags, fix typing* tooltip descriptions* Update airflow/www/static/js/datasets/Details.tsxCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>* remove created/updated at, separate links* update copyCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>",1
"Better warning and instructions in case someone runs emulated Python (#25482)It seems that the problem with runnig an emulated Python with rosettaon Macs M1/M2 might be more widespread. There are various scenarioswhere people could get the emulated Python:* Installing application with pre-bundled Intel Python (Intellj/PyCharm)* Installing older Python via anaconda* Transferring the configuration from Intel based Macs (yes, Apple  will NOT warn you when you are copying brew, python and other  binaries during the Apple-blessed ""Transfer my Mac"").This means that proably quite a number of people currently runsIntel, emulated version of Python on their shiny new M1s whichslows the Python (and thus Airflow) more than 10x.We alredy had a warning about it, but now it is more explicit, tellingthe user what exactly should be checked and how to rectify fromthe situation (basically nuking the development environment includingbrew and restarting from scratch is the best option you have).",1
"Migrate Google example automl_nl_text_extraction to new design AIP-47 (#25418)related: #22447, #22430",1
Use only public AwsHook's methods during IAM authorization (#25424),1
"Migrate Google example trino_to_gcs to new design AIP-47 (#25420)related: #22447, #22430",1
Remove `PrestoToSlackOperator` (#25425)Since we already going to release major version of Presto provider lets also remove deprecated operator,1
Enable Auto-incrementing Transform job name in SageMakerTransformOperator (#25263),1
removed recommendation for using 'token' as login in databricks connection when using auth via PAT (#25435)* removed recommendation for using 'token' as login in databricks connection when using auth via PATin the code the condition that decides whether to use PAT token is checking whether login is empty not whether it is 'token' https://github.com/apache/airflow/blob/main/airflow/providers/databricks/hooks/databricks_base.py#L421* added more detail to description of login options,2
Fix odbc hook sqlalchemy_scheme docstring (#25421),2
Bump terser from 4.8.0 to 4.8.1 in /airflow/ui (#25178)Bumps [terser](https://github.com/terser/terser) from 4.8.0 to 4.8.1.- [Release notes](https://github.com/terser/terser/releases)- [Changelog](https://github.com/terser/terser/blob/master/CHANGELOG.md)- [Commits](https://github.com/terser/terser/commits)---updated-dependencies:- dependency-name: terser  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Improve Airflow logging for operator Jinja template processing (#25452),1
Fix PostgresToGCSOperat bool dtype (#25475)When converting postgres boolean data type it was defaulting to theparquet parquet.string() data type. Pyarrow would then raise an errorwhen attempting to convert the boolean data type into its string.Changing the PostgresToGCSOperator class map `data_type` to convertpostgres boolean type to bigquery `BOOL` data type which then maps tothe parquet `pa.bool_()` data type when `_convert_parquet_schema` iscalled.,5
"Add and document description fields (#25370)* Add description to variable API resultsVariables can have descriptions, so that should be included in API results.",1
"First/last names can be empty (#25476)* First/last names can be emptyThe User schema restricts first and last names to not-null, and anempty string satisfies that requirement. The API insisted names haveat least one character, which caused errors when the database had anacceptably empty string.",5
"Fix Serialization error in TaskCallbackRequest (#25471)How we serialize `SimpleTaskInstance `in `TaskCallbackRequest` class leads to JSON serialization error when there's start_date or end_date in the task instance. Since there's always a start_date on tis, this would always fail.This PR aims to fix this through a new method on the SimpleTaskInstance that looks for start_date/end_date and converts them to isoformat for serialization.",5
Remove unused code in /grid endpoint (#25481),1
"Implement CronTriggerTimetable (#23662)Relates #15432 I also have gone through the above discussion and the related documents introduced there:- [AIP-39 Richer scheduler_interval](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-39+Richer+scheduler_interval)- [Scoping out a new feature for 2.1: improving schedule_interval](https://lists.apache.org/thread.html/rb4e004e68574e5fb77ee5b51f4fd5bfb4b3392d884c178bc767681bf%40%3Cdev.airflow.apache.org%3E)- [[DISCUSS][AIP-39] Richer (and pluggable) schedule_interval on DAGs](https://lists.apache.org/thread.html/rf8eeb06493681f48dc9e82ce605a9c3a930cfee0b4ca19462a4e55b3%40%3Cdev.airflow.apache.org%3E)The default behavior of [`CronDataIntervalTimetable`](https://github.com/apache/airflow/blob/2.2.5/airflow/timetables/interval.py#L116) is a little bit confusing - a DAG Run is triggered immediately after the DAG is enabled (unpaused), unlike normal cron's behavior.Hence I'd like to add another cron timetable which does not care about ""data interval"" and which starts a DAG Run at the start of the period.I know even without this PR, each Airflow user can [customize DAG scheduling with Timetables](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html#). However, I assume this PR worth adding to the main repository so that the users can use the timetable easier.",1
"Fix BaseSQLToGCSOperator approx_max_file_size_bytes (#25469)* Fix BaseSQLToGCSOperator approx_max_file_size_bytesWhen using the parquet file_format, using `tmp_file_handle.tell()`always points to the beginning of the file after the data has been savedand therefore is not a good indicator for the files current size.Save the current file pointer position and set the file pointer positionto `os.SEEK_END`. file_size is set to the new position, and the filepointer's position goes back to the saved position.Currently, after a parquet write operation the pointer is set to 0,and therefore, simply executing `tmp_file_handle.tell()` is notsufficient to determine the current size. This sequence is added toallow file splitting when the export format is set to parquet.",1
Note how DAG policy works with default_args (#24804)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
"Use 'python3' instead of 'python' in scripts (#25499)These scripts are run in the *host*, not a container, so they shouldreference 'python3' explicitly to make sure they get a useful Pythonversion. According to PEP 394, 'python' could refer to Python 2 in aglobal environment, so 'python3' should be preferred for scripts.",1
"Correctly handle output of the failed tasks (#25427)In the Jobs API 2.1, we can't call `get_run_output` on the top-level Run ID because it'snot supported by API - we need to call this function on specific sub-run of the job, evenif it consists of the single taskcloses: #25286",1
Convert Local to S3 example DAG to System Test (AIP-47) (#25345)* System test for local to s3 - without using fetch_variable* Update system test to use SystemTestContextBuilder* Make requested changes* Update local_to_s3.rst file to use the correct code reference.,1
Only load distribution of a name once (#25296),5
Limit Flask to <2.3 in the wake of 2.2 breaking our tests (#25511)Flask 2.2 added a few deprecations and made a few changes thatmade our tests stop working. Those were really test problems notreal application problems (there were no breaking changes in 2.2):* new deprecation warnings produced* Flask app cannot be reused in multiple tests* The way how session lifetime is calculated makes test fail  if freezegun is frozen using Pendulum datetime rather than the  stdlib onesThis is an early warning for the future as the deprecationwarnings make us aware that Flask 2.3 is breaking. So this PRfixes the 2.2 compatibility but at the same time limits Flask to< 2.3 with appropriate information when the limit can be removed.,4
Fix elasticsearch test config to avoid warning on deprecated template (#25520),2
Update to just-released `pip` 22.2.2 (#25519),5
"Simply json responses (#25518)Our json responses don't need to call jsonify explicitly, as flask willdo it for us. This also moves to returning a tuple to specify the statuscode.",4
Limit the list of versions in the issue template (#25480),0
AIP-47 - Migrate Airbyte DAGs to new design (#25135),1
Hide unused fields for Amazon Web Services connection (#25416),1
Apply flake8-logging-format changes to tests (#24931),3
Fix BigQueryInsertJobOperator cancel_on_kill (#25342),1
add description method in BigQueryCursor class (#25366)],1
"Correctly render `results_parser_callable` param in Qubole docs (#25514)Within the Python API docs for both `QuboleCheckOperator` and `QuboleValueCheckOperator`, the `results_parser_callable` parameter is improperly listed under a ""kwargs"" section. This could be confusing to users reading the docs as this is an explicit input to these operators rather part of catch-all keyword args.",1
Remove Smart Sensors (#25507)It was deprecated in 2.3 and since it is experimental we can now removeit -- Deferrable operators is a much more efficient pattern to achievethe same feature.,1
"Sync up plugin API schema and definition (#25524)Many of the entries should have been objects (dicts), not strings. Alsoremoved ""number"" since it is not present anyway and I don't even knowwhere it came from.",4
Do not convert boolean values to string in deep_string_coerce function (#25394),1
Add parsing context to DAG Parsing (#25161)Adds proper context in the form of context managers settingevironment variables to indicate whethere thedag file is parsed in context of DAG processor or Task Executionand allows to retrieve DAG_ID and TASK_ID easily.,2
Align Common SQL provider logo location (#25538)The integration logo directory for the Common SQL provider was still pointing to a `core/sql` directory which was the original name of the provider.A trivial reorganization of the location of the provider logo for docs to align with the new `common` convention.,1
"Validate newsfragment types (#25536)We should validate the a type is provided, and that it is a supported type.",1
Chart: custom labels for extrasecrets/configmaps (#25283),5
"Add ui endpoint to get ""next run datasets"" info (#25454)",5
"Coorect branch used for non-main builds (#25545)Shell parameters should determine the branch based on environmentvariables so that main version of Breeze can properly build imagesin build-images.yml workflow.While CI/PROD params were ok, Shell Param was always using mainand this failed PRs to v2-3-stable/test.This PR unifies branch selection to use env variables (which areproperly set in CI).",1
"Fix dag dependencies detection (#25521)Fix issue where we were creating duplicate deps elementss in the dag_dependencies node in serialized dag.Also added some test coverage for the ""dependency detector"".",3
Add optional data interval to CronTriggerTimetable (#25503),5
Make extra link work in UI (#25500),1
Further limit list of versions in issue template (#25542),0
Fix reducing mapped length of a mapped task at runtime after a clear (#25531)The previous fix on task immutability after a run did not fix a case where the task was removed at runtime when the literal is dynamic. This PR addreses it,1
Fix fetch_all_handler & db-api tests for it (#25430),3
Copy build packages on non-main (#25546)When non-main build is prepared (for example 2.3) we preparejust airflow package in CI but we do not prepare provider packages.We have not copied dist/files when building prod image but this meanswe did not copy the Airflow package either and the PROD build failedon non-main branch.This PR fixes it - packages are still copied after preparing in allbranches (in non-main this is only airflow package),0
Show dataset-triggered next run details (#25549)* add dataset-triggered next run details modal* use correct url* add tooltip telling users to click* use macro for modal* fix datasets url,5
set default task group in dag.add_task method (#25000)Signed-off-by: Hossein Torabi <blcksrx@gmail.com>,2
Possibility to document DAG with a separated markdown file (#25509),2
Fix MsSqlHook.get_uri: pymssql driver to scheme (25092) (#25185),1
Resolve Amazon Hook's `region_name` and `config` in wrapper (#25336),5
"Fix ""This Session's transaction has been rolled back"" (#25532)Accessing the run_id(self.run_id) on exception leads to error because sessions are invalidated on exception. Here we extract the run_id before handling the exception",1
Add right padding (#25554),1
Amazon provider package system test contributing guide (#25547),3
Add EMR Serverless Operators and Hooks (#25324),1
"Documentation on task mapping additions (#24489)* Documentation on task mapping additionsNone of these actually exist yet. These are the additions planned for2.5 on AIP-42 (task mapping), namely expand_kwargs(), map(), filter(using Airflow's existing mechanism), and zip().The examples for expand_kwargs() and zip() are pretty bad now.Suggestions are much welcomed.Let's discuss the interface in this patch's thread; once we settle onthe syntax and semantic, I'll start implementing them.* Detect common user error passing a task to map()The argument to map() must be a plain function, not a task. This hookchecks for that.",1
clear specific dag run TI (#23516),1
Fix S3Hook transfer config arguments validation (#25544)* Fix S3Hook transfer config arguments validation* Add proper exception type,1
"Ensure that zombie tasks for dags with errors get cleaned up (#25550)If there is a parse error in a DAG the zombie cleanup request never ran,which resulted in the TI never leaving running state and justcontinually being detected as a zombie.(Prior to AIP-45 landing, this bug/behaviour resulted in a DAG with aparse error never actually leaving the queued state.)The fix here is to _always_ make sure we run `ti.handle_failure` when weare given a request, even if we can't load the DAG. To _try_ and work aswell as we can, we try to load the serialized_dag if we can, but incases where we can't for whatever reason we also make sureTaskInstance.handle_failure is able to operate even when `self.task` isNone.",1
"Allow wildcarded CORS origins (#25553)'*' is a valid 'Access-Control-Allow-Origin' response, but was beingdropped as it failed to match the Origin header sent in requests.",0
Remove deprecated modules (#25543),4
Use meta value url for dataset links (#25551)* use macro for modal* fix datasets url* fix datasets urls,5
Adding support for owner links in the Dags view UI (#25280)* Adding support for owner links in the Dags view UICo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Update API & Python Client versions (#25562),5
feat: breeze - support compose v2 (#25563),1
System test for EMR Serverless  (#25559)* System test for EMR Serverless following the template in #24643 (AIP-47)* Remove example_emr_serverless.py from example_dags,2
"Cache the custom secrets backend so the same instance gets re-used (#25556)* Cache the custom secrets backend so the same instance gets re-usedFixes #25555This uses `functools.lru_cache` to re-use the same secrets backendinstance between the `conf` global when it loads configuration fromsecrets and uses outside the `configuration` module like variables andconnections. Previously, each fetch of a configuration value fromsecrets would use its own secrets backend instance.Also add unit test to confirm that only one secrets backend instancegets created.",1
"Do not declare a volume for sshKeySecret if dag persistence is enabled (#22913)* Do not declare a volume for sshKeySecret if dag persistence is enabledIn scheduler and triggerer components, git-sync-ssh-key volume was created evenwhen persistence is enabled. This PR fixes that and added testsin other components to avoid regression",3
"Move ""additional"" build args from required to optional in Breeze (#25567)The ""required"" build args in Breeze are replaced with empty`--build-arg arg=`. This is problematic if those parameters willhave default values set. We move them from required to optionalto skip the build args entirely when ""build"" command is run.",1
Correct compile assets command in tmux welcome message (#25570),1
Turn Airflow versions into a free-form field for Helm/Providers (#25564)* Turn Airflow versions in free-form field for Helm/Providers,1
Fix yamllint check with lines too long (#25573),0
Optimize log when using VerticaOperator (#25566),1
Add map index to task logs api (#25568),2
"Update AIP confluence link (#25571)Was renamed from ""Improvements"" to ""Improvement""",1
"Better behaviour for self-update of Breeze (#25572)Breeze self-upgrade behaved somewhat erratically:1) when executed manually as `self-upgrade` it printed ""repaat the   command"" - which made no sense2) whe triggered as part of an environment check (i.e. when breeze   version was different or installed from another workspaece) it   upgraded - and also printed ""repeat the command"" - which made   more sense but added a need to repeat the command.3) It asked the user to make the decision - but since reinstallation   is not very intrusive (just few seonds delay) it makes little sense   as reinstalling breeze would usually take less time than making   decisionThis change fixes it for both cases:1) there is no ""repat the command"" when you run self-upgrade command2) when self-upgrade is triggered as part of another command, the   original command is automatically re-run (with execvl - so   replacing previous process) after the self-upgrde completed.3) reinstalling breeze happens automatically in both cases without   asking the user.",1
Adding configurable fetch_all_handler for JdbcOperator (#25412),5
"Fix upgrade code for the dag_owner_attributes table (#25579)When running migrations on MySQL, the migration code for `dag_owner_attributes` fails with error:```1215 (HY000): Cannot add foreign key constraint```This is caused by the type incompatibility between table definition of the `dag_id` columnthat was declared as `sa.Column('dag_id', sa.String(length=250), nullable=False)`, but the`dag_id` in `dag` table is declared as `StringID()` that is mapped into UTF-8string (`dag_id VARCHAR(250) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL`).MySQL is sensitive in difference of the charset declarations and fails because of it.related: #25280",1
"Databricks: update user-agent string (#25578)Just after the previous PR was merged, Databricks dev ecosystem team came with formalguidance on the format of User Agent string for integrations.  This PR updates the UserAgent string to match it, and added a unit test for it",3
Get boto3.session.Session by appropriate method (#25569),1
Avoid requirement that AWS Secret Manager JSON values be urlencoded. (#25432)* Avoid requirement that AWS Secret Manager JSON values be urlencoded.,5
Add auth_type to LivyHook (#25183),1
Documentation update. Actualizing passing data options (#25577)* Actualizing passing data optionsTaskFlow API adds new pass data options,5
"Speed-up Python Helm Unit and Docker Compose tests by ~30% (#25583)The `pytest -n auto` switch does not give the accurate maximum numberof parallel tests we can run. It uses the number of CPUs not the numberof cores.The Helm tests utilise ~70% of available CPUs when the CPUS have dual cores,so we shoudl be able to gain 5 minutes (from 22 to 17) when Helmtests are run on self-hosted runners, and even more ~12 minutes(32 m instead of 45m) when run on Public Runner.The gains on docker-compose tests have not been measured yet but they arelikely similar.",3
Upgrate Kubernetes and Helm Charts used during our tests (#25582)The Kubernetes and Helm Charts need to get new patch level bump.Unfortunately the new Helm chart (correctly) refuses names ofreleases containing capital letters so we also needed to updateour tests to use lower-case release names.,1
Helm Unit testing use new Breeze command (#25581)* Helm Unit testing uses now new Breeze commandThe unit testing used bash scrips - now it uses breeze commandand we have nicer instructions describing the ways how you caniterate with the tests not only run complete set of tests.Closes: #25580,3
Remove migrations from up-to-date-checker (#25591)We have now better mechanisms to check if migrations are notadded in parallel (namely automated generation of documentationthat will conflict in case two migrations are added in parallel).We can safely remove the up-to-date checker now.,5
Add myself as code owner for AIP-42 related stuffs (#25600),1
Update Helm version in Breeze as well (#25589)Follow-up after #25582 - Helm version was still not upgraded inBreeze and Docker CI image.This PR fixes it.,0
refactor: Deprecate parameter 'host' as an extra attribute for the connection. Depreciation is happening in favor of 'endpoint_url' in extra. (#25494)The Connection.host column is discarded due to lack of semantics as it's easier and more intuitive to pass the whole url than filling its pieces in the UI.Update documentation stating 'host' is deprecated in favor of 'endpoint_url'. Also updated placeholders on the UI.Usage of Connection.host display a deprecation warning.,2
update-breeze-cmd-output is failing on main (#25612),0
DAG regex flag in backfill command (#23870),2
"Grid Logs, add visual indicator of selected attempt (#25611)",1
Add note on task_instance_mutation_hook usage (#25607)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Fix GCSListObjectsOperator docstring (#25614)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
"Move the old ./breeze script to scripts/tools/setup_breeze (#25584)We used to use Breeze via ./breeze script in the main airflowfolder, but it's already long enough time after new breezeintroduction to get-rid of it. However, the Bash script itselfis pretty useful to automate Breeze installation on POSIX-compliantOS-es, so turning it into an installation script seems like a goodidea.",1
Fix the errors raised when None is passed to template filters (#25593),4
"Don't mistakenly take a lock on DagRun via ti.refresh_from_fb (#25312)In 2.2.0 we made TI.dag_run be automatically join-loaded, which is finefor most cases, but for `refresh_from_db` we don't need that (we don'taccess anything under ti.dag_run) and it's possible that when`lock_for_update=True` is passed we are locking more than we want to and_might_ cause deadlocks.Even if it doesn't, selecting more than we need is wasteful.",1
Dataform operators (#25587),1
Add auto refresh to grid logs (#25621),2
Remove deprecated modules from Amazon provider package (#25609)* Remove deprecated modules from Amazon provider package,1
Set default wasb Azure http logging level to warning; fixes #16224 (#18896),0
Remove log groups in the lambda system test (#25626),3
"Refactor monolithic ECS Operator into Operators, Sensors, and a Hook (#25413)",1
Prepare docs for new providers release (August 2022) (#25618),1
Enable multiple query execution in RedshiftDataOperator (#25619)Enable RedshiftDataOperator to execute a batch of SQL using batch_execute_statement boto3 API.,1
Fix CHANGELOG for common.sql provider and add amazon commit (#25636)By mistake (overridden version in provider.yaml) the common.sqlprovider had missing release notes.This PR fixes it.,0
Consolidate to one `schedule` param (#25410)Deprecate params `schedule_interval` and `timetable` in favor of new param `schedule`.Dev list vote thread: https://lists.apache.org/thread/9mlhhsoxk4dkowxlltxdk7p1owk1ffxmDiscuss thread: https://lists.apache.org/thread/do76d17wmt64nc7nps0ld0x1tgmo944m,2
"Revert ""Remove support for FAB `APP_THEME` (#15277)"" (#25652)This reverts commit f634361cb7c1aa13deb098252784624a59339cad.This needs to wait until we are doing breaking changes for Airflow 3.",4
Clean up datasets UI and copy (#25646)* clean up datasets ui + copy* simplify dataset docs text* events -> updates,5
Remove useless statement in task_group_to_grid (#25654),1
"Fix mapped sensor with reschedule mode (#25594)There are two issues with mapped sensor with `reschedule` mode. First, the reschedule table is being populated with a default map_index of -1 even when the map_index is not -1. Secondly, MappedOperator does not have the `ReadyToReschedule` dependency.This PR is an attempt to fix thisCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",0
"Configurable umask to all deamonized processes. (#25664)We previously had this for just the `celery worker` subcommand, this PRextends it to anything that can run in daemon mode",1
Add semver and description of preparing rc2+ issue for providers (#25649)While releasing the providers two small problems were detected(as I recreated all my envs from scratch and released evenrc3 this time :)),1
Adding mysql index hint to use index on task_instance.state in critical section query (#25673),1
Databricks: Fix provider for Airflow 2.2.x (#25674)The problem was caused by using `ProviderInfo.is_source` field that was introduced only inAirflow 2.3.0.,5
"Fix `airflow-github needs-categorization` tool handling of renamed files (#25670)Renamed files are listed by github to start with `{old-filename} -> {new-filename}`.Currently, these are treated as core files. Here we search the renamed files if it starts with what should be ignored",2
"Use ParamSpec to replace ... in Callable (#25658)For whatever reason, this makes PyLance able to infer the wrappedfunction's signature, which is a nice improvement.",1
Two typing fixes (#25690),0
"Renamed up/downstream references to producing/consuming (#25688)We changed the UI to use these new terms already, but the code wasn'tchanged. These names are clearer to me (and others) so I have made thecode reflect them too.",4
Check for queued states for dags autorefresh (#25695),2
Help pip resolved to make better decisions on Pyarrow version (#25697)The pip resolver in `eager upgrade` mode on Python 3.10 decidesto downgrade Pyarrow to 5.0.0 which triggers numpy not beingcompatible with 3.10 - even if Pyarrow 6.* is perfectly fine.Adding the limit to eager upgrade directly helps the resolver tomake better decisions on it.,1
Bump cattrs version (#25689)Co-authored-by: Radek Tomšej <radek.tomsej@almamedia.com>,5
"Grid, fix toast for axios errors (#25703)",0
Upgrade MyPy to 0.971 (#25708)Recent upgrades of MyPy introduced some stability issues. Hopefullythis one will bring it back as there are few issues solved in itthat are likely the ones we see in our pre-commits.This version does not introduce any new issues reported by MyPy inour code so it is an easy upgrade.,0
Adding a parameter for exclusion of trashed files in GoogleDriveHook (#25675)* Adding a parameter for exclusion of trashed files to GoogleDriveHook,1
"INTHEWILD, add shopify (#25701)Since we've been using it for years and is core to our data org.",5
"Grid logs for mapped instances (#25610)* Add logs for mapped tasks* Mapped Task for download link, see more and paginated Table.* Handle refresh on select mapped task instance + other fix* Add useSelection tests for mapIndex* Stop propagation on mapped instance icon clicked* Add nav link for mapped task instances* Add Back to Dynamic Task Summary button* Move MappedInstances to its own tab* Update following code review* Simplifies the logic by using get_task_instance endpoint* Pull instance either from useGridData or useTaskInstance* Update test",3
"Fix SQL split string to include `;-less` statements (#25713)There was a bug in an incoming change to common-sql providerintroduced in #23971 where `;-less` statements were removedwhen ""split_statements"" flag was used. Since this flag is usedby default in Databricks statement, it introduced backwardsincompatible change.",4
Prepare documentation for RC4 release of providers (#25720),1
fix bug construction of Connection object in version 5.0.0rc3 (#25716),0
"Only excluded actually expanded fields from render (#25599)Previously we excluded all mapped kwargs from being rendered, whichcause issues for fields that are not actually rendered during expansion(i.e. those that are literals). This new implementation adds some extralogic to ensure we still include kwargs that still need rendering.",2
Allow to pass AIRFLOW_SOURCES env variable to ci entrypoint (#25709),1
make grid view group/mapped summary UI  more consistent (#25723),1
Added MySQL index hint to use ti_state on find_zombies query (#25725),1
Correct `json` arg help in `airflow variables set` command (#25726),1
Return None if an XComArg fails to resolve (#25661)Co-authored-by: wuzhijie <juzai123@Gmail.com>,0
Add groups in CI output for parallell tasks (#25702)Some of our tasks run in parallel in CI. Their output is now groupedone group per parallel task and printed at the end only.Progress is displayed while the tasks are executed.,1
"Separate ""public"" Dataset class from SQLA model (#25727)Importing all of SQLA can be very heavy weight (slowing import times),and it can also make it harder if libraries want to build on top of theDataset concept.Not to mention that the internal-API/db-isolation AIP should mean wedon't want to import any SQLA models directly in user code.",1
"Correctly link to Dag parsing context in docs (#25722)* Render docs for new dag-parsing contextThere were two problems: 1) the tag was incorrect (missing leadingcolon) so it didn't render as a link, and 2) The docs for the new modulewere being excluded.",1
Fix incorrect data interval alignment due to assumption on input time alignment (#22658)* Fix incorrect data_interval_start due to DAG's schedule changedThis PR fixes the incorrect first run of data_interval_start afterchanging the scheduling time.* Added _align_to_prev function for scheduling alignment after time change.* renamed _align to _align_to_next.Co-authored-by: wanlce <who@foxmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,4
Fixed never ending loop to in CreateWorkflowInvocation (#25737),1
Delete Old-style System Tests (#25655),3
"Remove missing extras in Airflow 2.2 check (#25738)The Airflow 2.2 check install airflow with predefined set of extras,but some of those extras were missing in 2.2 because the providerswere not existing yet then. There was also a 'dot' instead of comain the list of extras between apache.beam and apache.atlas.It did not have too bad effect, because those dependencies arepulled in when the providers get installed, but it did not correctlytest the apache.atlas and apache.beam upgrade scenarios - from theversions that were installed in 2.2 to the latest version in main.This PR fixes it.",0
"Implement Azure Service Bus Topic Create, Delete Operators (#25436)- Added Create Topic Operator- Added Test case- Added Example DAG- Added Doc for the operatorImplemented Azure service bus Topic Delete operator- Added Operator for Delete Topic Operator- Added example DAG",2
Fix aws cli installation script on ARM64 (#25699),0
Remove groundwork for Click CLI (#24793),1
"Move dataset dagrun creation to scheduler main (#24969)* Ensure no two processes will attempt to create the same dag run* Stamp the dataset events that are ""part of"" the dag runCo-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
Allow AWS Secrets Backends use AWS Connection capabilities (#25628),1
Improve docker documentation (#25735),2
Clarify filename_template deprecation message (#25749),2
Fix SystemsManagerParameterStoreBackend test (#25751)This has been failing since the backend moved use_ssl to a separatedattribute instead of bundling it in kwargs.,5
Operator name separate from class (#22834)It should be more helpful to display '@task.virtualenv' rather than'_PythonVirtualenvOperator' when looking at dag run details.,1
TESTING.rst: Fix typo in path (#25763)Critical because the path does not exist and results in error.    ERROR: file or directory not found: tests/chart,3
Add origin request args when triggering a run (#25729)* Add origin request args when triggering a run* Update airflow/www/templates/airflow/dag.htmlCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
Use cfg default_wrap value for grid logs (#25731)* Use cfg default_wrap value for grid logs* Use meta tag to pass default_wrap data to the grid,5
Update tutorial.rst (#25759)Command: `curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'` to download the docker-compose.yaml file was with wrong options.,0
Next run datasets tooltip preview (#25694)* create dataset triggered preview tooltip* share datasets data btwn tooltip+modal* add backup tooltip message,1
"The secrets tests are moved to ""always"" section of tests (#25760)The secrets tests are interacting with providers so changesin providers secrets implementation might break those tests.Moving them to ""always"" folder prevents them from acceptinga PR that breaks other PRs.Related: #25751 #25628",4
"Used many-to-many relation for finding attached dataset events. (#25758)The previous PR added this join table -- by using it we make theassociation of which events created a DagRun fixed at DagRun creationtime; previously if you deleted old DagRuns then the oldest remainingrun would ""collect"" all the oldest events.",5
"Fix outlet datasets not being detected on Mapped operators (#25767)The ""driving"" factor of this was confusion/inconsistency over whereoutlets are stored. For BaseOperator it was stored on `self._outlets`, (butat execution time the `@prepare_lineage` decorator would munge it andmove things in to `self.outlets`.But for MappedOperators a) this munging never took place, and b) itstored it on `self.outlets`.This PR removes the ambiguity and always stores inlets/outlets in the""public"" attribute.As a ""bonus"" change, this doesn't store empty lists in the serializedDAG for these two attributes",2
get dataset by uri instead of if in rest api (#25770)* get dataset by uri in rest api* encode uri in test,3
"Revert ""Help pip resolved to make better decisions on Pyarrow version (#25697)"" (#25777)This reverts commit 1b84048a15c108a210a0a8c809ae85532c667449.",4
Increase timeout for providers checks for 2.2 (#25779)Seems that we started to have problems with longer backtracking whenwe install new providers for Airflow 2.2. This is an attempt tostabilize it before we investigate the root cause.,1
"Update code examples from ""classic"" operators to taskflow (#25657)Co-authored-by: Josh Fell <48934154+josh-fell@users.noreply.github.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",1
Support `/` in variable get endpoint (#25774),1
Allow per-timetable ordering override in grid view (#25633),1
Add breeze parameter test-timeout to override pytest timeouts (#25766)* Add breeze parameter test-timeout to override pytest timeouts,3
Add custom_operator_name attr to `_BranchPythonDecoratedOperator` (#25783),1
Rotate session id during login (#25771),2
Help pip resolver make better decision on Pyarrow (#25791),1
"Add common-sql lower bound for common-sql (#25789)There are some implicit dependencies for common-sql 1.1.0 versionthat slipped through our understanding of cross-dependencies(learning for the future). We are bumping dependencies of all thepackages that depend on common-sql, to reflect that",1
Container specific extra environment variables (#24784),5
Add pre-commit hook for custom_operator_name (#25786),1
Remove skipping tests of Mssql for Python 3.8 (#25800)in https://github.com/apache/airflow/pull/25214 we missed the deprecation tests in always folder,3
Fail task if mapping upstream fails (#25757),0
Make GoogleBaseHook credentials functions public (#25785),1
Find cross-group tasks in iter_mapped_dependants (#25793),5
Let timetables control generated run_ids. (#25795),1
"Reduce `operator_name` dupe in serialised JSON (#25819)Often the operator_name will match the task_type, so this can beremoved from the JSON serialisation to save some bytes.",5
Remove depreciation warning when use default remote tasks logging handlers (#25764),0
"Set data_interval for dataset triggered runs to the range up ""upstream"" intervals (#25825)This won't be perfect for everyone, so the behaviour is controlled bythe timetable, not ""core"" scheduling logic.",2
"Don't use Pandas for SQLTableCheckOperator (#25822)Pandas is an optional extra for common-sql provider, so _forcing_ it fora query that is going to return a couple of rows is not a good idea",1
Bump undici from 5.8.0 to 5.9.1 in /airflow/www (#25801)Bumps [undici](https://github.com/nodejs/undici) from 5.8.0 to 5.9.1.- [Release notes](https://github.com/nodejs/undici/releases)- [Commits](https://github.com/nodejs/undici/compare/v5.8.0...v5.9.1)---updated-dependencies:- dependency-name: undici  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Fix (and test) SQLTableCheckOperator on postgresql (#25821),1
"Improve error handling/messaging around bucket exist check (#25805)S3Hook.check_for_bucket() method uses the boto3 s3 client method `head_bucket`to check for bucket existence. This client method does not work likemost boto3 APIs, it only returns a small subset of error codes.",0
Avoid circular import problems when instantiating AWS SM backend (#25810),0
"Add a way to import Airflow without side-effects (#25832)I know it's been a long-standing issue that it should be possible to import Airflow as a library without side-effects, and while I think the ultimate fix for this is to go through and steadily remove the need to call `settings.initalize()` in `__init__.py`, I am currently working on a project where I would really like to use Airflow without it doing strange things to logging, sys.path, or atexit.As such, this PR wraps the import side-effect in a simple environment variable check that code can use to disable the side-effects for now, while we slowly try and progress towards a ""cleaner"" solution. Without this, I am having to dynamically modify the source code of `__init__.py` in an import hook, and nobody wants that!I don't believe it's possible to write tests for this as Airflow is already imported when tests are running, but if you can think of a way, let me know and I'll have a go. The environment variable name is also up for consideration - I just picked something that looked a bit like a setting, but we can deliberately make it not look like that if we want.",1
Add Clarum to companies using Apache Airflow (#25814),1
Remove faulthandler_timeout (#25838),0
Use temporary directories for tests remote aws loggers (#25762)* Use temporary directories for tests remote aws loggers* replace tmpdir_factory by tmp_path_factory,2
Fix broken link to Trigger Rules (#25840)The link has been replaced by cross-referencing.,2
"Filter out non-editable apache-airflow from constraints. (#25847)The constraint files should not contain apache-airflow. So farapache-airflow has been automatically filtered out by the fact that ithas been an editable installation and we filtered out all editableand file installations from `pip freeze`.However as of ~12 August 2022, likely setuptools change triggereda slight behaviour change when `eager-upgrade` installation ofproviders from PyPi was run in the CI image. Previously airflowremained an editable install, but as of 12th of August, such aninstallation causes removal of editable airlfow install andreinstalls it in non-editable mode. While this change is somewhatconfusing, we have to protect against it and remove apache-airflowfrom `pip freeze` also in non-editable mode.",2
Workaround setuptools editable packages path issue (#25848)Setuptools 64.0.0 introduced change that broke paths of editablecli packages. This change makes CLIs installed via editablepackages to miss paths required to import code from the editablepackages themselves.Related to: https://github.com/pypa/setuptools/issues/3548,0
Support project_id argument in BigQueryGetDataOperator (#25782),5
Fix RDS system test (#25839),3
"Separate instruction to install OS dependencies in images (#25565)This change will allow to experiment with other base images(for example CentOS that our users highly demand) but alsoit has a few nice simplifications and improvements alongthe way:* no more runtime parameters for CI image (they only make  sense for PROD image)* no more support for Buster image (it is end of life in August)* Dockerfile has now less embedded default values (most of them  moved to inlined bash script)* configuration for yarn sources is removed (we do not need yarn  any more in our images)* additional pure-dev dependencies in CI image are passed through  ADDITIONAL_DEV_DEPS* dev installation does not remove installation cache, making  the CI image slightly bigger but easier for devel use - to install new  dependencies (no need for `apt-get update` before installation)* latest patchlevels of various tools we use for CI were bumped",1
Fix broken link for google cloud (#25642)dir structure has slightly changed - fixed the link to Google Cloud where different services and examples can be found.,2
"Wait for xcom sidecar container to start before sidecar exec (#25055)According to k8s's docs on pod phase [1], ""Running"" state means ""The Pod has been bound to a node,and all of the containers have been created. At least one container is still running,or is in the process of starting or restarting."".This means there is no guarantee that the xcom sidecar container will have beenrunning by the time the `extract_com` is used by KPO's `execute` even ifwe wait for pod to be a ""Running"" state - this further means if the base containerhas completed before the xcom sidecar container is running, the entireoperator fails because you cannot exec against a non-running container.Fix is to wait for the xcom sidecar container to be in the running statebefore we exec against it, which this commit implements.[1] - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase[2] - https://github.com/schattian/airflow/blob/86171ff43f8977021708cfa241da64f96c4c15e2/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py#L398",1
Discard semicolon stripping in SQL hook (#25855),1
Fix type annotations in SkipMixin (#25864),0
More DAG(schedule=...) improvements (#25648)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Databricks: fix provider name in the User-Agent string (#25873)`databricks-aiflow` -> `databricks-airflow` (:facepalm:),5
[CLEAN] support kubernetes 1.20 support (#25871),1
Add EndOfLife instructions to release procedure (#25877),1
pretty print KubernetesPodOperator rendered template env_vars (#25850)* pretty print env_vars,1
Bump dns-packet from 1.3.1 to 1.3.4 in /airflow/ui (#25806)Bumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.- [Release notes](https://github.com/mafintosh/dns-packet/releases)- [Changelog](https://github.com/mafintosh/dns-packet/blob/master/CHANGELOG.md)- [Commits](https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4)---updated-dependencies:- dependency-name: dns-packet  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump hosted-git-info from 2.8.8 to 2.8.9 in /airflow/ui (#25820)Bumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9.- [Release notes](https://github.com/npm/hosted-git-info/releases)- [Changelog](https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md)- [Commits](https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9)---updated-dependencies:- dependency-name: hosted-git-info  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump browserslist from 4.16.3 to 4.21.3 in /airflow/ui (#25807)Bumps [browserslist](https://github.com/browserslist/browserslist) from 4.16.3 to 4.21.3.- [Release notes](https://github.com/browserslist/browserslist/releases)- [Changelog](https://github.com/browserslist/browserslist/blob/main/CHANGELOG.md)- [Commits](https://github.com/browserslist/browserslist/compare/4.16.3...4.21.3)---updated-dependencies:- dependency-name: browserslist  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump tmpl from 1.0.4 to 1.0.5 in /airflow/ui (#25809)Bumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.- [Release notes](https://github.com/daaku/nodejs-tmpl/releases)- [Commits](https://github.com/daaku/nodejs-tmpl/commits/v1.0.5)---updated-dependencies:- dependency-name: tmpl  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Bump path-parse from 1.0.6 to 1.0.7 in /airflow/ui (#25803)Bumps [path-parse](https://github.com/jbgutierrez/path-parse) from 1.0.6 to 1.0.7.- [Release notes](https://github.com/jbgutierrez/path-parse/releases)- [Commits](https://github.com/jbgutierrez/path-parse/commits/v1.0.7)---updated-dependencies:- dependency-name: path-parse  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Remove mapped operator validation code (#25870),5
Bump ws from 6.2.1 to 6.2.2 in /airflow/ui (#25804)Bumps [ws](https://github.com/websockets/ws) from 6.2.1 to 6.2.2.- [Release notes](https://github.com/websockets/ws/releases)- [Commits](https://github.com/websockets/ws/compare/6.2.1...6.2.2)---updated-dependencies:- dependency-name: ws  dependency-type: indirect...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Add @task.kubernetes taskflow decorator (#25663)Co-authored-by: Kaxil Naik <kaxilnaik@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add support for TaskGroup in ExternalTaskSensor (#24902),1
Dataproc submit job operator async (#25302),1
Implement `EmrEksCreateClusterOperator` (#25816),1
"Improve logging of autput for Breeze commands in CI (#25860)This change organizes better and improves overal output of the CI breezecommands, in the way that it makes it far more useful.1. No more separated output for running command and it's output in   CI. They were displayed in separately foldable sections and it   has been somewhat misleading. Now, the folded section contain both   the command and its output.2. The heuristics to determine ""short"" name of a command has been   improved and the foldable groups for the command are more similar   to Github Actions runs (white color foldable) but also it has   pink automatically rich-highlighted main command that is run.3. CI groups (foldable components) will no longer overlap. Github   Actions does not support nested groups, so in case we start   group in another group, the first group is ""closed"" automatically.   With this change, if we start group inside of another group, the   original group is not closed (but also the new group is not started   and the nested command is not folded inside the main group.4. Progress output is much more consitent and nicer in case of   parallel runs. We have it nicely wrapped in rich rules, showing   how much time started with progress and single line of most   recent interesting output from the parallel operation. We have   ""smarter"" progress information implemented - showing much more   useful one-liner truncated and caloured status from each of the   running threads. Then all the individual outputs are nicer   summarized in the folded groups after all of the parallel runs   completed. No more spilling of the individual jobs into the   progress monitoring.5. When running the same CI parallel commands locally, the output   is adjusted to local terminal (no foldable groups and the output   from parallel jobs is only printed if the job fails by default.6. Hash calculation for Breeze images has been improved to not include   spurious dictionary entries.This change has been extracted from #25678 as it grew out of it originalscope.",4
Limit Google Protobuf for compatibility with biggtable client (#25886)The bigtable client does not work well with protobuf > 3.20.0 andit should be limited until we solve the problem.,0
The old breeze video and related assets are removed (#25884)The video and related assets are not valid in most cases any more.We discussed re-recording Breeze videe after we complete Pythonconversion.This PR was extracted out of #25678 as it outgrew it's initial scope,5
"Mark new urrlib3 1.26.12 as ""known deprecations"" (#25885)We check if there are no unknwn deprecations when we importproviders. The urllibe3 1.26.12 added new warnings.",2
[improvement] Audit log (#25856)Audit log made the webrowser crashes when the amountis to large to be dealed by the browser.This PR introduces the same patterm we use to query Dags,2
"Fix migration issues and tighten the CI upgrade/downgrade test (#25869)There was a typo that was not detected in the migration file which prevented upgrades.On fixing the typo, some other migration issues were detected.Apart from postgres, other dbs couldn't upgrade -> downgrade -> upgrade. This was partlybecause we now create new DB from the ORM which also uses a naming convention to create constraints.I have applied a fix, the side effect is that it breaks offline generation of sql for mysql. This is because it uses the new naming convention to create constraints when we run upgrade through the migration file. Inmigration file 0093, we dropped unique keys with names dag_id & dag_id_2 while with the namingconvention in place, these two now have a unique name across all db that's different from the nameswe use to drop the keys. Because of that, I have chosen to use sqlalchemy inspector to get the nameand drop them.",4
improve test coverage of SnsHook (#25881)* improve test coverage of SnsHook,1
Rewrite recursion when parsing DAG into iteration (#25898)This avoids a RecursionError when parsing a DAG with very deepdependencies that takes the interpreter over `sys.getrecursionlimit()`,5
Sql to GSC operators update docs for parquet format (#25878),2
"Remove implementation of ``__hash__`` method on ``DatasetEvent`` (#25889)It is not used (we know this because it's currently defined with attrs that do not exist (created_at). We could fix this, but it's somewhat questionable that dataset + timestamp is the ""right"" choice, so, since it's not used, better not to implement.",1
Added `Plaetos` to the list of companies using Apache Airflow (#25896)Added Plaetos to the list of companies using Apache Airflow,1
Use per-timetable ordering in grid UI (#25880)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add CamelCase to generated operations types (#25887),1
React tests should also run when only `www` change (#25908)We only run React tests when airflow/ui changed but we haveactually started to do more and more react in airflow/www.This caused a green build in #25880 for example even though itstarted to fail in `main`.We add www also to the react tests trigger,3
Create a pluggable DatasetEventManager (#25419)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,5
postgres provider: use non-binary psycopg2 (#25710),1
Add Cafebazaar to the list of companies using Airflow (#25906),1
Datasets graph (#25707)* Add dependencies graph to datasets page.Temporarily uses an exact uri lookup until get_dataset() updates.* improve node width calculation* dep data as separate endpoint* move data transformations to api hook* add center button* fix testCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>,1
Added labels to specific airflow components (#25031),1
"Remove postgres extra from Helm tests venv (#25921)We do not need postgres extra when running Helm tests. The testsstarted to fail because we switch to a binary postgres dependencyfor postgres provider ad it started to fail because of lack ofpgconfig, but we actually do not need the extra to run the testsso we can remove it.",4
Add Airflow specific warning classes (#25799)Adding `RemoveInAirflow3DeprecationWarning` and `AirflowProviderDeprecationWarning`Closes: https://github.com/apache/airflow/issues/22356,0
Fix AIRFLOW_EXTRAS in CI for 2.2 build (#25915),0
Remove implementation of `__eq__` method from DatasetEvent (#25907),5
Apache Airflow 2.3.4 has been released (#25913),5
Fix minor typos in README (#25936),2
Chart: Default to Airflow 2.3.4 (#25916),2
Add test for datasets reserving 'airflow' scheme (#25914),5
Remove `--type all` from breeze static checks docs (#25932),2
Update example dataset DAGs names (#25910)* Update example dataset DAGs namesThe terminology upstream/downstream in datasets was previously renamedto produces/consumes.,5
Prefer the local Quick Start in docs (#25888),2
fix broken auto-refresh (#25950)In PR #25042 the merged change included a bug that prevents the pagefrom auto-refreshing if there are anything other than manual runs on thepage. We also want to auto-refresh for scheduled runs.I've also added more complete unit tests for the expected behaviour.,3
Add redshift create cluster snapshot operator (#25857),1
"Add instructions on manually fixing MySQL Charset problems (#25938)Depending on the way how and when you MySQL DB was created, it mighthave problems with the right CHARACTER SET and COLLATION used.This PR attempts to describe a process that users can follow tofix such a problem.Also a chapter was added to recommend taking a backup beforethe migration.Based on discussions and user input from #25866, #24526Closes: #24526",1
"Improve cleanup of temporary files in CI (#25957)After recent change in Paralell execution, we start to haveinfrequent ""no space left on device"" message - likely caused bythe /tmp/ generated files clogging the filesystem from multipleruns. We could fix it by simply running cleanup after paralleljob always, but this is not good due to diagnostics neededwhen debugging parallel runs locally so we need to havea way to skip /tmp files deletion.This PR fixes the problem twofold:* cleanup breeze instructions which is run at the beginning of  every job cleans also /tmp file* the parallel jobs cleans after themselvs unless skipped.",4
"Properly check the existence of missing mapped TIs (#25788)The previous implementation of missing indexes was not correct. Missing indexeswere being checked every time that `task_instance_scheduling_decision` was called.The missing tasks should only be revised after expanding of last resort for mapped tasks have been done. If we find that a task is in schedulable state and has already been expanded, we revise its indexes and ensure they are complete. Missing indexes are marked as removed.This implementation allows the revision to be done in one placeCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",1
Fix dataset_event_manager resolution (#25943)Appears `__init__` is not invoked as part of `_run_raw_task` due to the way TI is refreshed from db.  Centralize dataset manager instantiation instead.,5
Fix unhashable issue with secrets.backend_kwargs and caching (#25970)Resolves #25968,0
Fix response schema for list-mapped-task-instance (#25965),0
"update areActiveRuns, fix states (#25962)",0
Glue system test cleanup (#25966),4
"Do not include mypy volume by default (#25958)MyPy volume was included by default in shell command, but it isnot needed and might lead to missing mypy-cache volume problem.It is useful for debugging mypy problems so it is still useful tohave it as an option of shell command.",1
DatabricksSubmitRunOperator dbt task support (#25623),1
Convert Quicksight Sample DAG to System Test (#25696),3
"Less hacky double-rendering prevention in mapped task (#25924)* Refactor _expand_mapped_kwargs to disallow None* Less hacky double-rendering preventionInstead of inventing a separate way to track what XComArg has beenrendered (due to task unmapping), we can actually track this withseen_oids. This makes the mechanism considerably less hacky.",1
"Add issue stats to PRotM score, enhance terminal output (#25741)",0
Add liveness probe to Celery workers (#25561),1
Fix dead link in MySQL Provider page (#25387),1
copy into snowflake from external stage (#25541),5
Update AWS system tests to use SystemTestContextBuilder (#25748),5
Fix AzureBatchOperator false negative task status (#25844),1
"Update code examples from ""classic"" operators to taskflow (#25845)",1
Make codespace run successfully (#25251),1
Added append_job_name parameter to DataflowTemplatedJobStartOperator (#25746),5
Cloud Build assets & system tests migration (AIP-47) (#25895),3
"Fix Flask deprecation warning (#25753)* Fix Flask deprecation warning`app.json_encoder` has been deprecated in favor of`app.json_provider_class`, and the former is being removed in Flask 2.3.* Bump min Flask version to 2.2* Don't pass datetime to JSON serializerAlso convert those parametrized calls to pytest. This helped me find thetest I was looking for.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",3
Fix incorrect env_variables keyword (#25792)Co-authored-by: yi.wu <yi.wu@shopee.com>,0
Add roles delete command to cli (#25854)closes: #15318,4
Google Cloud Tasks Sensor for queue being empty (#25622),5
"Fix placeholders in `TrinoHook`, `PrestoHook`, `SqliteHook` (#25939)",1
Fix gcs to sftp system test (#25934)gcs_to_sftp example was missing setup and teardown stages aftermigration to the new design.Change-Id: I68e29f49fb4e1ac69f419c70a9ac46d8b8442e3bCo-authored-by: Bartlomiej Hirsz <bartomiejh@google.com>,4
Adds a wait to prevent a race condition (#25967),1
Clean up the AWS System Test ContextBuilder (#25773)* Convert ContextBuilder to Variable class,3
feat: load host keys to save new host key (#25979)Co-authored-by: doiken <doiken@users.noreply.github.com>,1
Fix concept doc for dynamic task map (#26002),2
Add custom handler param in SnowflakeOperator (#25983),1
Fix EMR serverless system test (#25969),3
Use project_id to get authenticated client (#25984),1
Reorganize tutorials into a section (#25890),2
Raise an error on create bucket if use regional endpoint for us-east-1 and region not set (#25945),1
Add RedshiftDeleteClusterSnapshotOperator (#25975)* Add RedshiftDeleteClusterSnapshotOperator* Review feedback,5
Fix `EcsBaseOperator` and `EcsBaseSensor` arguments (#25989),1
Fix command dags list-runs argument dag_id (#25978)dag_id should be a positional argument.,2
Fix wrong link for taskflow tutorial (#26007),2
Fix legacy timetable schedule interval params (#25999),2
Rename DatasetDagRef to DagScheduleDatasetReference (#25920),5
"Replace javascript ""get-workflow-origin"" with Python (#26018)Intially this action had much bigger scope and retrieved PR also forbuild-images workflow. This Python breeze command replaces theuse of that action. Tests are added to cover the functionality.Thanks to that we can also combine several separate steps in theworkflows into one testable step.Closes: #25739",3
Remove --force from manual refresh image script (#26021)We have removed the `--force` flag from `self-upgrade` command butthe manual refresh script was not updated.,5
"Remove remaining fromJson in a few runs-on (#26022)The change #26018 introduced better way of calculating runs-onusing Breeze, but it missed a few cases where old mechanism usedfromJson (where it was not necessary and harmful actually).This fixes the problem.",0
Add `output` property to MappedOperator (#25604)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Fix runs-on array remaining specification (#26026)One more fix needed after #26018 and #26022,0
Include scheduled slots in pools view (#26006),5
Add conf parameter to CLI for airflow dags test (#25900)Co-authored-by: Schilling Christian (XC-AD/ETV5) <christian.schilling@bosch.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,3
"Fix BranchDateTimeOperator to be timezone-awreness-insensitive (#25944)* fIx BranchDateTimeOperator to be timezone-awreness-insensitiveThe BranchDateTimeOperator was sensitive to whether timezoneaware or timezone noive parameters were passed to it. Actuallyit worked a bit unpredictably - if use_task_logical_date wasused, the lower/upper ranges were supposed to be timezone aware,but when ""now()"" was used, the ranges were supposed to be timezonenaive. One of our examples has been broken because it wascomparing naive and aware datetime.This PR coerces all values to timezone aware Pendulum datetime usingthe timezone of the Dag, which makes it insensitive to whether theaware or naive ranges have been used.Also, we missed example in the howto showing logical date usage(and it was rather strange as logical date is the only reasonableusage of the operator - using utcnow() make the DAG essentiallynon-idempotent - it's result depends on when the task is run whichmight make sense in some cases but most of the time is somethingthat should be discouraged.The documentation has been updated to explain that.* Also use tz when converting non-Pendulum datetimePlus a better written docstring.Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
Update release_dockerhub_image.yml (#26033)Signed-off-by: sashashura <93376818+sashashura@users.noreply.github.com>Signed-off-by: sashashura <93376818+sashashura@users.noreply.github.com>,1
Use AsyncClient for Composer Operators in deferrable mode (#25951),1
"Updating deprecated configuration in examples (#26037)```/home/airflow/.local/lib/python3.9/site-packages/airflow/configuration.py:528 DeprecationWarning: The sql_alchemy_schema option in [core] has been moved to the sql_alchemy_schema option in [database] - the old setting has been used, but please update your config.```",5
"Upgrade API files to typescript (#25098)* Upgrade some of the migrations to ts.* Update types.* Update get type for useConfirmMarkTask.* Migrate useMarkFailedRun* migrate useMarkFailedTask, useMarkSuccessRun, useMarkSuccessTask* Migrate more* Use API types when possible* Migrate more* Add URLSearchParamsWrapper* Migrate more",2
Rename DatasetTaskRef to TaskOutletDatasetReference (#25919),5
Fix wrong deprecation warning for `S3ToSnowflakeOperator` (#26047),1
Hook into Mypy to get rid of those cast() (#26023),1
"Breeze help images are now consistently generated in pre-commit (#26032)Images generated for Breeze Help can slightly differ, however wewant them to be generated always consistently. Therefore the imagesare now (when using pre-commit) generated inside the CI image ratherthan locally.This change also introduces much more selective image generationwhich will make the image generation much faster in case youchange any of breeze code. Previously, any change in any breezecommand parameters triggered regeneration of the images.With this change, only those commands that get changed getregenerated (and information about which images need to be regeneratedis printed).This change removes one of the Breeze setup commands - the hashgeneration - as it is now done internally by the regenerate commandand does not need to be externalized (--check-only) flag providesa way how to check if breeze images need regeneration.",1
Replace SQL with Common SQL in pre commit (#26058)This pre-commit verify source/target provider for transfer exist.Now that we have `common.sql` provider there is no need to use SQL along with Common SQL,1
"Rename `DatasetEventManager` to `DatasetManager` (#26054)This manager class will end up dealing with more than just datasetevents over time, so rename it to be more general.",5
"Better error messsage for pre-common-sql providers (#26051)When you are using a common-sql provider functionality suchas SQLColumnCheckOperator with a provider from before common-sqlprovider was released, attempts to instatiate such provider fromcommmon-sql will fail because the Hooks in those providers derive fromthe old airflow.hooks.dbapi_hook.DbApiHook rather than fromairflow.providers.common.sql.hooks.sql.DbApiHook.We cannot do much about it, simply speaking the old providersshould be upgraded to a version that supports common.sql provider.This PR raises a message that explicitly states the error.Closes: #26046",0
clean up verify_providers.py (#26075)Some entries are not needed any more as the modules that generated them were removed in latest major release of Amazon provider,1
Change CI output to pr-labels (#26050)The output for pull requests was pullRequestLabels should bepr-labels (missing rename in #26018),4
"Fix schedule_interval in decorated dags (#26082)Using schedule_interval in decorated dag does not work at the moment. Theissue is that the schedule arg on the dag decorator has a default of None thus when wehave schedule_interval set and not schedule arg, the schedule arg resets the schedule_interval.The fix was to make the schedule arg same with what's in DAG __init__ as instructed",5
FIX Make items nullable for TaskInstance related endpoints to avoid API errors (#26076) FIX Add missing nullable items to TaskInstance related endpointsCo-authored-by: jorrick <jorrick.sleijster@adyen.com>,1
Deprecate jira provider in favor of atlassian.jira provider (#25930)* Deprecate jira provider in favor of atlassian.jira provider,1
Bump moment-timezone from 0.5.34 to 0.5.35 in /airflow/www (#26080)Bumps [moment-timezone](https://github.com/moment/moment-timezone) from 0.5.34 to 0.5.35.- [Release notes](https://github.com/moment/moment-timezone/releases)- [Changelog](https://github.com/moment/moment-timezone/blob/develop/changelog.md)- [Commits](https://github.com/moment/moment-timezone/compare/0.5.34...0.5.35)---updated-dependencies:- dependency-name: moment-timezone  dependency-type: direct:production...Signed-off-by: dependabot[bot] <support@github.com>Signed-off-by: dependabot[bot] <support@github.com>Co-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>,1
Refactor Slack API Hook and add Connection (#25852),1
"Run EmptyOperator tasks that have outlets (#26087)Like when they have callbacks, we should actually run EmptyOperatorsthat have outlets (e.g. datasets).",5
Add triggered dag runs to dataset events (#25961)* add triggered runs to dataset events* simplify dag run return schema* add tests for created_dagruns* fix other api tests* remove execution_date from created_dagruns* run_id and logical_date are not nullable,5
"Remove limitation to pull latest images. (#26086)The #25380 restored regular pushing of ""latest"" images to ghcr.io(after some period where we did not do it, because cachingis now solved differently - using separate ""cache"" images foreach of the architectures). We restored it because codespaceswere pulling the latest image rather than rebuilding the images usingcache. However we have not removed the protection against pullingthe latest image (because it could be old).This PR removes the - now unnecessary - protection.",4
Add missing schedule DAG parameter to AWS system tests (#26090),3
Add trigger rule tooltip (#26043)* Display operator trigger rule in graph view tooltip* Change order of trigger rule in tooltip* Fix linting errors in airflow/www* Fix additional lint errors in airflow/www/views.py* Wrap trigger rule display in if statement* Add null as 2nd argument to tiTooltip in gantt.jsAccomodates the additional argument for the tiTooltip object* Update airflow/www/static/js/task_instances.jsChange condition for determining if task.trigger_rule is presentCo-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>Co-authored-by: Brent Bovenzi <brent.bovenzi@gmail.com>,5
Change the template to use human readable task_instance description (#25960)The task_instance_key_str description is a better templateexample as a human-readable description of the task.,1
Fix display aws connection info (#26025),5
Additional mask aws credentials (#26014),1
"Improve the speed and stability of KPO tests (#26091)The KPO tests were occasionally failing in CI because of racecondition between delete from previous test and running thefollowing tests. Simply speaking the pods run via KPO wellnot immediately sometimes and the following test picked themtogether with the current pod and failed becasue of duplicatepods. There was even a tearDown Delay of 1 second introducedas workaround to combat this problem.This PR fixes it much better. Each POD in the KPO test getsunique label (derived from test name) and when executing thePOD this label is then used as selector, so that only the**just started** POD in current test are selected - not anydangling pod (being deleted) from the previous tests.This not only removes flakiness of the tests, but also removethe need of sleep, which speeds up the whole suite by whoooping33 seconds (there were 33 tests to run).",1
Implement expand_kwargs() against a literal list (#25925),2
Add pre-commit check for DAG and dag parity (#26083),2
Promote Operator.output more (#25617),1
Fix UI flash when triggering with dup logical date (#26094),5
Add group prefix to decorated mapped task (#26081),0
Only send an SlaCallbackRequest if the DAG is scheduled (#26089),2
"Don't fail DagRun when leaf mapped_task is SKIPPED (#25995)This one was a fun one to track down, and was only a problem when thescheduler ""expanded"" the first mapped task (making it SKIPPED).- The scheduler looks at `add_one` and marks it as SKIPPED.- `unfinished_tis` contains add_one_1, and `_are_premature_tis` changes  the state of `add_one__1`, (which just so happens to be a leaf task),  and the check on line 584 essentially gets confused, as no one  envisaged the states changing!In a reverse of how this normally plays out, if the mini_scheduler inthe LocalTaskJob was disabled then this example DAG would deadlock everytime. (Since that mini scheduler only operates on a partial DAG it can'tever change the whole DagRun state.)Co-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
use label instead of id for dynamic task labels in graph (#26108),1
Remove airflow/ui (#26113)* remove airflow/ui directory* fix mentions of /ui* update github ci yml file,2
Make execution_date optional for command `dags test` (#26111)* Make execution_date optional for command `dags test`,3
"Don't error when multiple tasks produce the same dataset (#26103)* Don't error when multiple tasks produce the same datasetPreviously this was ""racey"", so running multiple dags all updating thesame outlet dataset (or how I ran in to this: mapped tasks) would causesome of them to fail with a unique constraing violation.The fix has two paths, one generic and an optimized version forPostgres.The generic one is likely slightly slower, and uses the pattern that theSQLA docs have for exactly this case. To quote> This pattern is ideal for situations such as using PostgreSQL and> catching IntegrityError to detect duplicate rows; PostgreSQL normally> aborts the entire tranasction when such an error is raised, however when> using SAVEPOINT, the outer transaction is maintained. In the example> below a list of data is persisted into the database, with the occasional> ""duplicate primary key"" record skipped, without rolling back the entire> operation:However for PostgreSQL specifically, there is a better approach we cando: use it's `ON CONFLICT DO NOTHING` approach. This also allows us todo the whole process in a single SQL statement (vs 1 select + 1 insertper dataset for the slow path)",5
Add back default latest tag for pulling image (#26121)The `pull` commands of Breeze ci-image and prod-image got thepossibility of pulling latest image in #26086 but the --image-tagremained as required. This PR brings back the --image-tag to fallback to 'latest' for pull commands.,3
"Enable pushing early image cache to ghcr.io registry (#26119)In order to react quicker to changes in setup.py, setup.cfg andDockerfiles, we can push an early image cache to the ghcr.io cache.This build is done way before constraints are regenerated so it isusing the ""current"" constraints at the moment the merge happens.This should be safe - if the image fails to build (we ignore that)the cache will not be pushed, but if it will, then even if we mergea new commit quickly and the rest of the build is cancelled, thecache will be refreshed with new Dockerfile/setup.py/setup.cfg andthe subsequent builds will run much faster.",1
Add Azure synapse operator (#26038)This PR adds implementation for `AzureSynapseRunSparkBatchOperator` which allows Airflow users to submit Spark batch jobs on Azure Synapse Apache Spark Pools,1
Switch providers-init pre-commit to Python (#26125)Part of #26020,5
Convert Apache RAT licence check to Python (#26123)Part of #26020,5
Convert hadolint check into Python (#26124)Part of #26020,5
"Fix DagRun.start_date not set during backfill with --reset-dagruns True (#26135)When we run backfill on an existing runs, the start_date is lost during the clearingof the run and never set.This PR sets it",1
feat: kubernetes 1.25 support (#26139),1
"Handle updating non-overlapping requirements (#26122)So far when we attempted to upgrade requirements of a dependency withfixed requirement or when the new range was not overlapping with theold rangem we had to synchronize pushing constraints at the sametime as merging the setup.py/setup.cfg change - otherwise regularPR that did not have ""--upgrade-to-newer-dependencies"" derived,failed because of conflicting constraints.This PR solves the problem by attempting to rebuild the image again,this time with ""--upgrade-to-newer-dependencies"" set when thefirst attempt to build fail in CI.The effect of it should be that the image should not fail and let theregular PRs continue, while the constraints are being updated inmain.This should finally make the builds self-maintainable even if webump fixed constraints.",0
Convert Helm tests to use the new Python Breeeze (#25678)This PR converts the Helm tests to use the new Python Breeze.It has all the features of previous Kind breeze command and more.All the commands are now grupped under k8s group and they are veryeasy to use locally (even easier than the previous version).The CI part is also converted and simplified - i.e. the upgradetest is now much faster (only tests one upgrade per job and itruns withing the original Helm/Kubernetes tests jobs so it willnot have the cluster creation overhead.Most importantly - this is almost the last step before we can getrid of the old legacy breeze code and one that we can get rid ofthe `./breeze-legacy` script because all functionality from theold breeze has been moved to the Python version with this change.This removal allows us also to remove a lot of the common librarybash code that is not used any more anywhere - even in CI.The only change left is running regular tests in parallel.Closes: #23085,3
Replace INTHEWILD sorting with python (#26137)Part of #26020,5
"Remove --limit-progress-output from helm-test (#26144)It is not needed any more, as we do not run the testsin parallell and we have a better mechanism in parallel test.",3
Replace license setup.cfg file pre-commit with Python (#26140)Part of: #26020,2
"Make ``BaseSerialization.serialize`` ""public"" to other classes. (#26142)It is still not designed for use by DAG authors, but this says ""itsokay to use elsewhere in Airflow""",1
"Update zombie message to be more descriptive (#26141)Because the zombie detection method comes from the scheduler job, in a distributed environment it can be cumbersome to find the worker that actually had the problem that led to the zombie. In editing this, I also noticed that the zombie message had some redundant and unhelpful elements (e.g. printing the memory address of the SimpleTaskInstance) so I've cleaned it up.closes: #26067",4
Use proper default airflow_constraints_reference (#26148)In case image was built by `breeze` and not by `build` command.default value of the arg was wrong (empty) rather than defaultconstraint branch. That led to early cache invalidation and muchlonger image build than necessary.,5
Replace spelling wordlist sorting with Python (#26138)Part of #26020,5
"Fail ""early build cache"" if breeze shell command is not quick (#26150)This is a follow-up after #26148 where we found out that basicbreeze command took far more time than it should when useranswered ""yes"" to rebuild the image.This PR adds `--max-time` parameter to `breze shell` command whichforces ""exit"" to be executed as command and fails if the commanddoes not complete in time specified.This command is run in ""early build cache"" so failure there willnot fail the whole build (the job has ""continue on error"" set),but we will see the ""red"" status of the `main` workflow in casethe build takes longer than 120 seconds - indicating that thebuild takes too long.We only run it for `amd` images because if we have a problemthere, we will have similar problem in arm images.We make sure to clear the docker cache before the command is runto make sure that it runs in ""clean"" state.",4
"Fix saving hash on image build. (#26152)There was a case when image was build with breeze, when the hashof important files were not modified after image has been built becausethe ""latest"" tag was set and comparision did not include it.This PR fixes it.",0
Bump dep on common-sql to fix issue with SQLTableCheckOperator (#26143),1
"Prepare bug-fix release of providers out of band (#26109)* Prepare release of amazon provider out of bandThere is an circular import error that means 5.0.0 can't be used withthe SecretsManager - worth releaseing a fix for this now* Include common.sql, presto, sqlite and trino in the batch* Correctly set dep back to common-sql>=1.2.0* Postgres provider too",1
"Removed deprecated contrib files and replace them with PEP-562 getattr (#26153)The conrib modules are carried from Airflow 1.10 and we cannotremove the classes that were used there without bumping Airlfowto 3, however we can replace the files with dynamic attributeretrieval following PEP-562. This way we will have far less numberof files in contrib and users will not get auto-import andautocomplete when they will be developing their DAGs.This does not break compatibility but it providers much strongersignal to the users that they should switch to new classes:* they will not see the classes in autocomplete and autoimport  any more* it will seem that the classes are not existing in contrib until  they are used* if anyone performs static analysis on the DAGs, the analysis  will fail (and this is an intended behaviour)",0
"Remove cattrs from lineage processing. (#26134)Cattrs was used for two reasons:1. As a hacky way of forcing templated fields on classes2. As a way to store the outlets in XCom without needing pickle1 has been fixed in core for a while now and classes can have  ""template_fields"" properties (deeply)2 is now done by using a combo of BaseSerialization and `attr.asdict`",1
"Set template_fields on RDS operators (#26005)Related: #24099 #25952### SummaryThis PR sets the `template_fields` property on `RdsCreateDbInstanceOperator` and `RdsDeleteDbInstanceOperator` to allow users to programmatically change the database id, instance size, allocated storage, etc..It also replaces use of `@task` decorated functions with their appropriate operator in system tests.Co-authored-by: D. Ferruzzi <ferruzzi@amazon.com>",3
"Push provider tags as a single command, not one per provider (#26163)This makes this step much much quicker.Also don't hard-code the remote name as `apache` but find it from somecommon names (apache and origin for now)",1
Add url prefix setting for Celery Flower (#25986)This setting in airflow uses hyphen `--url-prefix` and the original param is `--url_prefix`,0
Fix backfill occassional deadlocking (#26161),0
"Implement ExternalPythonOperator (#25780)This Operator works very similarly to PythonVirtualenvOperator - butinstead of creating a virtualenv dynamically, it expects theenv to be available in the environment that Airlfow is run in.The PR adds not only the implemenat the operator, but alsodocuments the operator's use and adds best-practices chapterthat explains the differences between different ways how you canachieve separation of dependencies between different tasks. Thishas been a question added many times in by our users, so addingthis operator and outlining future aspects of AIP-46 and AIP-43that will make separate docker images another option is alsopart of this change.",4
Rewrite recursion into iteration (#26175)This helps to avoid RecursionError when viewing the graphview of a dag with many tasks,2
Support multiple DagProcessors parsing files from different locations. (#25935)* Separate dag processors* Introduce [scheduler]stalled_dags_update_timeout configuration option* Remove DagProcessorDirectory class and pass dag_directory as parameter* Rename dag_directory column to processor_subdir in CallbackRequestsCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>,2
Add subdir parameter to dags reserialize command (#26170),2
Allow setting TaskGroup tooltip via function docstring (#26028),2
Fixes SageMaker operator return values (#23628)Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,1
Add dataset read permission to viewer role (#26181),5
Show DAGs and Datasets menu links based on role permission (#26183)This allows users with an elevated public role (e.g. AUTH_ROLE_PUBLIC=Admin)to see the DAGs and Datasets menu links in the navbar.Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>,1
Add option of sending DAG parser logs to stdout. (#25754)* Add option of sending DAG parser logs to stdout.* Make long config line into two lines.* Reverse order of tests.* Update tests with new changes.,4
Fix typo of roles export command (#26195),2
"Fix edge detection iteration (#26188)In #26175, edge detection was changed to use iteration instead ofrecursion, however it wasn't keeping track of the parent tasks.This changes the set we iterate over to be `(task, {children})` pairs insteadof just adding all the children tasks themselves.",1
Don't throw an exception when a BQ cusor job has no schema (#26096),5
Re-configure/connect the ORM after forking to run a DAG processor (#26216),2
Add missing contrib classes to deprecated dictionaries (#26179)The #26153 missed the classes that were added by deriving a newclass from the imported ones. This was mostly about non-consistentspelling of class names.This PR fixes it and brings missing contrib classes back.,0
Add triggering dataset events to ti context (#26168)This allows downstream tasks to get details about the events thattriggered the dagrun.Co-authored-by: Jed Cunningham <jedcunningham@apache.org>Co-authored-by: Tzu-ping Chung <uranusjr@gmail.com>,2
"Automatically register DAGs that are used in a context manager (#23592)This has caused a bit of confusion to committers (missing off the `asdag`) and is just more user friendly, especially for dynamic dagsThis is on by default but can be disabled by passing`auto_register=False` to a DAG",2
FIX Incorrect typing information (#26077)* FIX Broken typing information* FIX missing pre-commit run* FIX Improve typing even more* FIX Improve imports and add note regarding import* FIX run pre-commitCo-authored-by: jorrick <jorrick.sleijster@adyen.com>,1
"Lazily import many modules to improve import speed (#24486)* Use sql-alchemy events to import_all_models at the ""right"" time* Move more ""slow"" imports out of top level* Don't force loading of all models early in CLI audit loggingCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>",2
"Move dag_edges and task_group_to_dict to corresponding util modules (#26212)The methods were implemented in ""view"" but they were used in otherplaces (in dot_renderer) so they conceptually belong to commonutil code. Having those in a wrong package (airflow/www) causedthe tests to pass because selective checks did nor realise thatchange in ""airflow/www"" also requires running Core tests.This PR moves the methods to ""airflow/utils"". The methodsare imported in the ""views"" module so even if someone used themfrom there, they will stil be available there, so the change isfully backwards compatible (even if those are not ""public""airflow API methods.Follow up after #26188",4
Remove remaining deprecated classes and replace them with PEP562 (#26167)This is a follow-up after #26513 - removal of all remainingdeprecated classes and replace them with PEP-562 dynamic attributeloading.,5
GCSToBigQueryOperator allow for schema_object in alternate GCS Bucket (#26190),1
"Update docs for data-aware scheduling (AKA ""datasets""/AIP-48) (#26208)Co-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>Co-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>Co-authored-by: Ash Berlin-Taylor <ash@apache.org>",1
"Ensure stale dataset references are removed (#25959)If a DAG or task previously referenced a dataset but no longer does, that reference should be removed from the DB the next time the dag is processed.",2
Mark serialization functions as internal (#26193),1
Make `execution_date_or_run_id` optional in `tasks test` command (#26114),3
"Cleanup core newsfragments (#26224)We accidentally ended up with one for a provider, and another was missingthe new auto-regsitration behavior.",1
"Fix faulty executor config serialization logic (#26191)The bind processor logic had the effect of applying serialization logic multiple times, which produced an incorrect serialization output.Resolves #26101.",0
Add ``@task.short_circuit`` TaskFlow decorator (#25752),1
"Undo secrets backend config caching (#26223)Revert ""Fix unhashable issue with secrets.backend_kwargs and caching (#25970)""",0
Fix TaskInstance.task not defined before handle_failure (#26040),0
"For worker log servers only bind to IPV6 when dual stack is available (#26222)This function only exists on Python 3.8, so Python 3.7 will always listen on IPv4 only.",1
"Add automatically generated ERD schema for the MetaData DB (#26217)On a popular :) request by the users, we've added thisautomatically generated ERD diagram of Airflow Database.It is automatically generated by pre-commit (only when themigrations change).",4
ProSiebenSat.1 added to INTHEWILD.md (#25378)* ProSiebenSat.1 added to INTHEWILD.mdCo-authored-by: Ash Berlin-Taylor <ash@apache.org>,1
"Allow to override version suffix in CI (#26230)The CI of ourse is using dev0 prefix to test packages and to buildPROD image, but when Airflow (in beta) has conflicting version, itfailed. This PR allows to override the version and only printswarning in this case rather than fail.",0
"Run mypy against provider tests with providers, not core (#26229)",1
Fix typo on rebuild warning message (#26231),2
Dataset doc update (#26232),5
"Prepare to release 2.4.0beta1 (#26228)As I like to have the branch point for a release as ""late"" as possible,I'm committing the version change to main, and then branching off.The release notes/changelog are _far_ from complete, but lets take whatnewsfragments we have",1
Branch setup for v2-4 (#26236),1
"Fix case when SHELL variable is not set in kubernetes tests (#26235)When SHELL variable is not set, kubernetes tests will fall backto using 'bash'",1
"Make breeze works with latest docker-compose (#26233)The latest docker-compose dropped an alias for `docker-compose` andwe need to detect it and use ""docker compose"" instead.",2
Add 2.4.0b1 to issue template (#26242),0
"Limit eager upgrade of protobuf library to < 4.21.0 (#26243)* Limit eager upgrade of protobuf library to < 4.21.0Until all the Google client libraries get upgraded to >= 2.0.0, we need tolimit the protobuf version.",1
Reformat 2.4.0 release notes (#26247)This makes them proper sections and reorders a couple of them as well.,1
"Skip overriding version from parameter with the one from tag (#26244)Since we are not always updating the version suffix in code(rc1/rc2/b1/b2), the tag specified via --tag-build prefix shouldoverride the one in code rathe than the other way round.So what's left now - we will just print warning if the suffix doesnot match.",0
Add deferrable big query operators and sensors (#26156)This PR donates the following big query deferrable operators and sensors developed in [astronomer-providers](https://github.com/astronomer/astronomer-providers) repo to apache airflow.- `BigQueryInsertJobAsyncOperator`   - `BigQueryCheckAsyncOperator`- `BigQueryGetDataAsyncOperator`- `BigQueryIntervalCheckAsyncOperator`- `BigQueryValueCheckAsyncOperator`- `BigQueryTableExistenceAsyncSensor`,1
Add better progress in CI for constraints generation. (#26253)Currently constraints generation is not really showing good progresswhile the packages are being removed/installed. This adds progressthat shows that something happens.,1
The PROD cache is pushed always in regular cache build step (#26254)In v2-4-test it turned out that PROD cache build was a bittoo limited - the cache has not been prepared if branch was notmain (copy&paste victim). This PR fixes this - cache is always buildin merge run regardless of the branch we are in.,1
D400 first line should end with period batch02 (#25268),5
Life Science assets & system tests migration (AIP-47) (#25548),3
feat(KubernetesPodOperator): Add support of container_security_context (#25530),1
Add more weekday operator and sensor examples #26071 (#26098),1
Add network_profile param in AzureContainerInstancesOperator (#26117),1
Move send_file method into SlackHook (#26118),1
Add RdsDbSensor to amazon provider package (#26003),1
"Fix 'from airflow import version' lazy import (#26239)This incorrectly accesses the version *attribute* in airflow.versioninstead of the airflow.version module itself, breaking compatibility.I actually rewrote the entire __getattr__ hook; much of the logic seemsto be quite redundant.",2
Update erd SHA to pass static checks (#26263)The cause of the failure was a PR race -- the PR that changed migrationswas opened and passing tests before the ERD sha was added.,1
Add the dag_id to AirflowDagCycleException message (#26204),2
"Don't blow up when a task produces a dataset that is not consumed. (#26257)If you have a dataset outlet on a task, and no DAG was recorded asconsuming that dataset it failed with a null value constraint violationin the db",5
"Respect ""common"" options value in breeze sub-commands. (#26264)This lets us run `breeze -v static-checks` and have the sub-command pickup the value from the parent context. If you specify a value for thesubcommand, that is used in place of the global one.(The fact that `-v` was allowed before the sub-command but had no effectcaused me lots of confusion)",5
"Make ""quick build"" actually test build time (#26251)* Make ""quick build"" actually test build timeThere was a ""string"" instead of array passed in case of --max-timecommand flag which caused an error - regardless of the time ittook to run the build, but it was also ignored because thejob was run within ""continue-on-error"" cache push job.This PR fixes it to use proper ""exit"" command and separates it outto a job where quick build failure will be noticed (not in PRs butin the merge-run)",7
Fix kubernetes tests (#26272),3
"Update CI documentation, renaming runs to ""Canary"" (#26151)For quite some time we did not have the name of the main buildsHowever more and more the ""main"" builds are used to provide earlywarnigns for some problems:* 3rd-party dependencies breaking our builds* our own dependencies breaking the constraints* building ARM images* building breeze images quickly* running complete matrix of tests* finding flaky testsSo effectively, those main builds are really ""Canary"" builds - whenthose builds are failing, they give us a chance to react quickly,without affecting the regular PR builds.This PR clarifies the meaning and reasoning for those buildsand introduces ""Canary"" name for them.During related documentation review, it also turned out that a numberof old environment variables are not used any more (after the breezechanging to Python) and this PR also removes them from documentationand removes the variables from all the scripts (including removal ofsome unused scripts)The new documentation also mentions somethign that we've learnedrecently - that in case you use Breeze in non-airflow workflows inGitHub Actions, you need to override the variables through commandline parameters rather than through environment variables, becauseGitHub actions treats GITHUB_* variables as immutable for security.",1
"Flush dataset events before queuing dagruns (#26276)When we go to schedule dagruns from the dataset dagrun queue, we assumethe events will happen before the queue records, which isn't the caseunless we explicitly flush them first. This ensures that dagruns areproperly related to their upstream dataset events.",5
Cloud Video Intelligence Operators assets & system tests migration (AIP-47) (#26132) Cloud Video Intelligence Operators assets & system tests migration (AIP-47),3
Properly build URL to retrieve logs independently from system (#26337)The previous way of building the path depended on the OS pathbut it was really used to build the URL so we should useurllib instead of os.path.join.,1
Athena and EMR operator max_retries mix-up fix (#25971)* Internally rename `max_tries` to `max_polling_attempts`. Add deprecation warning to inform users about naming change.Raise Exception if values of `max_tries` does not match value of `max_polling_attempts`.,4
